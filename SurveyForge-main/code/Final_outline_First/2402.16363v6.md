1. Introduction

2. Delve into LLM Inference and Deployment

3. Model Compression

4. Algorithmic Methods for Fast Decoding

5. Compiler/System Optimization

6. Hardware Optimization

7. Discussion

8. Conclusion