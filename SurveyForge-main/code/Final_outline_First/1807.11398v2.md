1. Introduction  
2. The Basic Preference-based Multi-Armed Bandit Problem  
3. Learning from Coherent Pairwise Comparisons  
4. Learning from Non-coherent Pairwise Comparisons  
5. Further Extensions  
6. Multi-Dueling Bandits  
7. Applications  
8. Summary and Perspectives  