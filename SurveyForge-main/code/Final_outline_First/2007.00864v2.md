I. Introduction  

II. Background: Need for Efficient Execution of ML Models on Hardware Accelerators  

III. Acceleration Opportunities Due to Compact Models and the Need for Special Support  

V. Encodings for Compressing Sparse Tensors  

VI. Extraction of Matching Data for Computations on Non-Zeros  

VII. Memory Management of Compressed Tensors  

VIII. Interconnects for Distributing Non-Zeros and Reducing Partial Outputs  

IX. PE Architecture Design  

X. Load Balancing of Effectual Computations  

XI. Write-Back and Post-Processing  

XII. Compiler Support  

XIII. Trends and Future Directions  

XIV. Related Work  

XV. Summary