Prologue: What this survey is and is not about
1 What is a neural network?
2 What is the use of a neural network?
3 Network training is an optimization problem
4 Solving the optimization problem
4.1 Gradient descent
4.2 Back propagation
4.3 Mini-batch stochastic gradient descent and back propagation
5 Activation functions
5.1 Desirable properties of activation functions
5.2 Popular choices of activation functions
6 Loss functions
7 Overfitting and regularization
7.1 Adding penalty terms
7.2 Early stopping
7.3 Dropout
8 Validation and hyper-parameter tuning
9 Procedure summary
10 Introduction
11 Density in polynomial approximation
11.1 Uniform convergence
11.2 Weierstrass approximation theorem
11.3 Bernstein polynomials
11.4 A constructive proof of Weierstrass theorem
11.5 Stone-Weierstrass theorem
12 Density of two-layer networks
12.1 Two-layer feed forward networks
12.2 Pinkus theorem
12.3 Proof sketch of Pinkus theorem
13 Convergence rate of approximation by two-layer networks
13.1 Target functions of interest
13.2 Sobolev spaces
13.3 A typical convergence rate result
13.4 A short discussion on curse of dimension
14 Complexity of deep ReLU networks
14.1 Target functions of interest
14.2 Main complexity result
14.3 Proof strategy
14.4 Averaged Taylor polynomial approximation
14.5 Partition of unity
14.6 Construction of a global polynomial approximant
14.7 Expressive power of ReLU networks: an intuitive example
14.8 Connected and standard ReLU networks
14.9 Approximating product of two numbers by ReLU networks
14.10 ReLU networks and the product of their output components
14.11 ReLU networks and the partition of unity
14.12 Approximating global polynomials by ReLU networks
14.13 Proof of Complexity Theorem
14.14 Further reading and nonlinear approximation
15 Introduction
16 Standard ReLU networks and free knot linear splines
16.1 Space of standard ReLU networks
16.2 Space of free knot linear splines
16.3 A comparison between one-layer ReLU networks and free knot linear splines
16.4 A comparison between deep ReLU networks and free knot linear splines
17 ReLU networks are at least as expressive as free knot linear splines
17.1 Special ReLU networks
17.2 Special ReLU networks that realize continuous piecewise linear functions
17.3 Proof of Theorem 1
18 The power of depth
18.2 A large class of functions with self-similarity