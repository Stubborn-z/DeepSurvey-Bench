1 Introduction  
2 Vector Representations of Words  
2.1 Static Word Embeddings  
2.2 Contextual Word Vectors in Recurrent Networks  
2.3 Contextual Representation in Transformers  
3 Measures of Syntactic Information  
4 Morphology and Syntax in Word Embeddings and Latent Vectors  
5 Syntax in Transformers' Attention Matrices  
6 Conclusion