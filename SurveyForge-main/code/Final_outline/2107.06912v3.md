1 INTRODUCTION

2 VISUAL ENCODING  
2.1 Global CNN Features  
2.2 Attention Over Grid of CNN Features  
2.3 Attention Over Visual Regions  
2.4 Graph-based Encoding  
2.5 Self-Attention Encoding  
2.6 Discussion

3 LANGUAGE MODELS  
3.1 LSTM-based Models  
3.2 Convolutional Language Models  
3.3 Transformer-based Architectures  
3.4 BERT-like Architectures  
3.5 Non-autoregressive Language Models  
3.6 Discussion

4 TRAINING STRATEGIES  
4.1 Cross-Entropy Loss  
4.2 Masked Language Model (MLM)  
4.3 Reinforcement Learning  
4.4 Large-scale Pre-Training

5 EVALUATION PROTOCOL  
5.1 Datasets  
5.2 Evaluation Metrics

6 EXPERIMENTAL EVALUATION

7 IMAGE CAPTIONING VARIANTS  
7.1 Dealing with the Lack of Training Data  
7.2 Focusing on the Visual Input  
7.3 Focusing on the Textual Output  
7.4 Addressing User Requirements

8 CONCLUSIONS AND FUTURE DIRECTIONS