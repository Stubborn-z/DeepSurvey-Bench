1. Introduction
2. Background
2.1. Task Definition
3. Evaluation Metrics
3.1. Quality-based Metrics
3.2. Latency-based Metrics
3.3. Loss Functions
4. Cascade vs. End-to-End
5. Data Issues
5.1. Augmentation
5.2. Pre-training
5.3. Self-training and Back-translation
5.4. Knowledge Distillation
6. Segmentation and Representation Learning
6.1. Segmentation Learning
6.2. Representation Learning
7. End-to-End ST Models
7.1. E2E ST Models based on Frameworks
7.2. ST Models based on the Nature of Available Data
7.3. Discussion
7.5. SOTA Performance of E2E ST Models on Low-Resource Languages
8. Deployment of E2E ST Models
9. Resources for ST
9.1. Datasets for ST Tasks
9.2. Toolkits for ST
10. Future Directions for Research
10.1. Cascade vs. End-to-End Models
10.2. ST on Code-Mix Data
10.3. Domain-Invariant Models
10.4. Discrepancy between Automatic and Human Evaluation
10.5. Handling Ambient Noise
10.6. Handling Multiple Speakers
10.7. Handling Speaker Diarization
10.9. Low-resource ST Datasets and Models
10.10. LLMs for ST tasks
11. Conclusion