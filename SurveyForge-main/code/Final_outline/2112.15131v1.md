1 Introduction

2 Background on Deep Learning and Resource-Efficiency  
2.1 Deep Learning Overview  
2.2 Resource Efficiency Metrics for Deep Learning  

3 Model-Level Resource-Efficient Techniques  
3.1 Weight Quantization  
3.2 Pruning  
3.3 Compact Convolution  
3.4 Knowledge Distillation  
3.5 Neural Architecture Search for Compressed Models  

4 Arithmetic-Level Resource-Efficient Techniques  
4.1 Number Formats for Deep Learning  
4.2 Arithmetic-Level Techniques for Inference  
4.3 Arithmetic-Level Techniques for Training  

5 Implementation-Level Resource-Efficient Techniques  
5.1 Leveraging Data Reuse from Convolution  
5.2 Leveraging Sparsity of Weights and Activations  
5.3 Leveraging Weight Repetition in Quantized DNNs  
5.4 Leveraging Innovative Technology  
5.5 Adaptive Compute Resource Assignment  

6 Interrelated Influences  
6.1 Influences of Model-Level Techniques on Arithmetic-Level Techniques  
6.2 Influences of Model-Level Techniques on Implementation-Level Techniques  
6.3 Influences of Arithmetic-Level Techniques on Implementation-Level Techniques  

7 Future Trend for Resource-Efficient Deep Learning  
7.1 Future Trend for Model-Level Resource-Efficient Techniques  
7.2 Future Trend for Arithmetic-Level Resource-Efficient Techniques  
7.3 Future Trend for Implementation-Level Resource-Efficient Techniques  

8 Conclusion