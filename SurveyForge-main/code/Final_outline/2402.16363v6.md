1. Introduction
2. Delve into LLM Inference and Deployment
2.1. LLM Inference
2.2. Roofline Model
2.3. LLM-Viewer
3. Model Compression
3.1. Quantization
3.2. Pruning
3.3. Knowledge Distillation
3.4. Factorization
4. Algorithmic Methods for Fast Decoding
4.1. Minimum Parameter Used Per Token Decoded
4.2. Maximum Tokens Decoded Per LLM Forward Propagation
5. Compiler/System Optimization
5.1. Operator Fusion
5.2. Memory Management and Workload Offloading
5.3. Parallel Serving
6. Hardware Optimization
6.1. Spatial Architecture
6.2. Processing in Memory
6.3. New Data Format
6.4. New Processing Element
7. Discussion
7.1. Reliability
7.2. Safety Alignment
7.3. OOD Generalization
7.4. Efficient Large Multimodal Models
7.5. Long Context Modeling
8. Conclusion