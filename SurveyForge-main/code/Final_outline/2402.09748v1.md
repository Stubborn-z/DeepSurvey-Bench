1 INTRODUCTION

2 PRELIMINARIES
2.1 Transformer
2.2 Medium/Large Language Models
2.3 Parameter-efficient Finetuning (PEFT)

3 QUANTIZATION
3.1 Basic Concepts
3.2 Quantization Methods for Medium-Size Language Models
3.3 Post-Training Quantization for LLMs
3.4 Quantization-Aware Training for LLMs
3.5 Other Topics for LLM Quantization

4 PRUNING
4.1 Basic Concepts
4.2 Pruning Methods for Medium-Size Language Models
4.3 Pruning Methods for LLMs
4.4 Other Topics for LLM pruning

5 KNOWLEDGE DISTILLATION
5.1 Basic Concepts
5.2 KD for Medium-Size Language Models
5.3 KD for Large Language Models

6 COMPACT ARCHITECTURE DESIGN
6.1 Efficient Attention
6.2 Neural Architecture Search

7 DYNAMIC NETWORKS
7.1 Mixture of Experts
7.2 Combining MoE with other efficient techniques

8 ACCELERATION FRAMEWORK
8.1 General Framework
8.2 Specialized Framework

9 CONCLUSIONS