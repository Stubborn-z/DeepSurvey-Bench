1. Introduction
1.1. Motivating Applications
1.2. The Multi-Armed Bandit Model

2. Considerations of Multi-Armed Bandits
2.1. Measures of Regret
2.2. Variance and Bounds of Regret
2.3. Stationarity of the Problem
2.4. Feedback Delay

3. Algorithms for the Application of Multi-Armed Bandits
3.1. Gittins index
3.2. ε-greedy
3.3. ε-first
3.4. UCB1
3.5. UCB2
3.6. UCB-Tuned
3.7. MOSS
3.8. KL-UCB
3.9. Bayes-UCB
3.10. POKER and price of knowledge
3.11. Thompson Sampling and Optimism
3.12. Best Empirical Sampled Average (BESA)

4. Complications of the Simple Stochastic Model
4.1. Adversarial Bandits
4.2. Contextual Bandits for Experiments with Covariates
4.3. Nonstationary Bandits for Ongoing Experimentation
4.4. Infinite- and Continuum-Armed Bandits for Continuous Spaces
4.5. Bandits with Multiple Plays

5. State of the Theory and Future Work