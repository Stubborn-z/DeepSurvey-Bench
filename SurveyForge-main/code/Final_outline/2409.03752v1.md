1 Introduction  
1.1 Structure of Our Work  
1.2 Comparison with Related Surveys  
1.3 Out-of-scope Topics  

2 Background  
2.1 Mathematical Representation of LLMs  
2.2 Glossary of Key Terms  

3 Overview of Special Attention Heads  
3.1 How Does Brain / Attention Head Think?  
3.2 Knowledge Recalling (KR)  
3.3 In-Context Identification (ICI)  
3.4 Latent Reasoning (LR)  
3.5 Expression Preparation (EP)  
3.6 How Attention Heads Work Together?  

4 Unveiling the Discovery of Attention Heads  
4.1 Modeling-Free  
4.2 Modeling-Required  

5 Evaluation  
5.1 Mechanism Exploration Evaluation  
5.2 Common Evaluation  

6 Additional Topics  
6.1 FFN Interpretability  
6.2 Machine Psychology  

7 Conclusion  
7.1 Limitations in Current Research  
7.2 Future Directions and Challenges