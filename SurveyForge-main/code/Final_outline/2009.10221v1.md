1. Introduction  
1.1. What are explainable and explained?  
1.2. Types of machine learning models  
1.3. Informal definitions  
1.4. Formal operational definitions  
1.5. Interpretability and granularity  

2. Foundations of Interpretability  
2.1. How interpretable are the current interpretable models?  
2.2. Domain specificity of interpretations  
2.3. User centricity of interpretations  
2.4. Types of interpretable models  
2.5. Using black-box models to explain black box models  

3. Overview of Visual Interpretability  
3.1. What is visual interpretability?  
3.2. Visual vs. non-visual methods for interpretability and why visual thinking  
3.3. Visual interpretation pre-dates formal interpretation  

4. Visual Discovery of ML Models  
4.1. Lossy and lossless approaches to visual discovery in n-D data  
4.2. Theoretical limitations  

5. General Line Coordinates (GLC)  
5.1. General Line Coordinates to convert n-D points to graphs  
5.2. Case studies  

6. Visual Methods for Traditional Machine Learning  
6.1. Visualizing association rules: matrix and parallel sets visualization for association rules  
6.2. Dataflow tracing in ML models: Decision Trees  
6.3. iForest: Interpreting Random Forests via Visual Analytics  
6.4. Tree Explainer for Tree Based Models  

7. Traditional Visual Methods for Model Understanding: PCA, t-SNE and related point-to-point methods  

8. Interpreting Deep Learning  
8.1. Understanding Deep Learning via Generalization Analysis  
8.2. Visual Explanations for DNN  
8.3. Rule-Based methods for Deep Learning  
8.4. Human in the Loop Explanations  
8.5. Understanding Generative Adversarial Networks GANs via Explanations  

9. Open Problems and Current Research Frontiers  
9.1. Evaluation and development of new visual methods  
9.2. Cross-Domain Pollination: Physics & domain-based methods  
9.3. Cross-Domain Pollination: Heatmap for non-image data  
9.4. Future Directions  

10. Conclusion