1 Introduction  
2 Search Strategy and Selection Criteria  
3 Background  
3.1 Attention  
3.2 Attention Mechanisms  
3.3 Position-wise Feed-Forward Network  
3.4 Residual Connections and Layer Normalization  
3.5 Positional Encodings  
3.6 Assembling a Transformer  
3.7 Computational Complexity of Transformer Attention  
3.8 Transformer Model Usage  
4 Mainstream Transformer-based Architectures  
4.1 Bidirectional Encoder Representations from Transformers (BERT)  
4.2 Vision Transformer (ViT)  
5 Large Language Models (LLMs)  
6 Transformers in NLP  
6.1 Clinical Word Embeddings  
6.2 Transformers for Clinical Information Extraction (IE)  
6.3 Neural Machine Translation (NMT)  
7 Transformers for Structured EHR Data  
7.1 Ontological Structure Learning  
7.2 Multi-modal Data Fusion  
7.3 Predicting Future Diagnoses using ICD Codes  
8 Transformers in Computer Vision  
8.1 Medical Image Segmentation  
8.2 Medical Image Registration  
8.3 Medical Image Captioning and Report Generation  
8.4 Visual Question Answering (VQA)  
8.5 Image Synthesis  
8.6 Image Reconstruction  
9 Transformers for Critical Care  
9.1 Predicting Long-Term Adverse Outcomes  
9.2 Surgical Instruction Generation  
10 Transformers for Social Media Data in Public Health  
10.1 Monitoring Adverse Drug Reactions (ADRs)  
10.2 Monitoring Depression  
10.3 Monitoring Diabetes  
10.4 Categorizing Vaccine Confidence  
10.5 Locating Disease Hotspot  
11 Monitoring Bio-Physical Signals  
11.1 Human Activity Recognition (HAR)  
11.2 Electroencephalograms (EEGs)  
11.3 Electrocardiograms (ECGs)  
12 Transformers for Biomolecular Sequences  
12.1 DNA  
12.2 Protein  
12.3 Molecular Drugs  
13 Discussion  
13.1 Interpretability and Explainability  
13.2 Environmental Impact  
13.3 Computational Costs  
13.4 Fairness and Bias  
13.5 AI Alignment  
13.6 Data Privacy and Data Sharing  
14 Conclusion