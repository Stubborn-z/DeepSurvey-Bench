Contents  
1 Introduction  
2 Related Work  
3 Organization of the Paper  
4 Philosophy of Scientific Explanations  
5 What and Why is Model Interpretability?  
5.1 How to Achieve Interpretability?  
5.2 Global vs. Local Interpretability  
5.3 Looking at the Model Interpretability Problem  
5.4 Important Terminology in Interpretability  
6 Taxonomy of Interpretability Methods  
6.1 Visualization Methods  
6.2 Distillation Methods  
6.3 Intrinsic Methods  
6.4 Counterfactual Explanations  
6.5 Influence Functions  
7 Axiomatic Properties of Attribution Methods  
8 Evaluation Approaches  
8.1 Sanity Checks for Interpretability Methods  
8.2 Evaluation Metrics  
8.3 Metrics for Ground-truth Datasets  
8.4 Metrics for Real Datasets  
8.5 Criticisms of Post hoc Interpretability  
9 Interpretable Neuroimaging  
9.1 Feature Engineering Approach to Neuroimaging  
9.2 Deep Learning Approach to Neuroimaging  
9.3 Transfer Learning in Neuroimaging  
9.4 Interpretability in Neuroimaging  
10 Review of Interpretability Methods in Neuroimaging  
10.1 Backpropagation Methods  
10.2 Perturbation-based Methods  
10.3 Counterfactual  
10.4 Distillation Methods  
10.5 Intrinsic Methods  
10.6 Feature Map Visualization  
11 The Usage Trend of Interpretability Methods  
12 Suggestions for Interpretable Models in Neuroimaging  
13 Conclusion