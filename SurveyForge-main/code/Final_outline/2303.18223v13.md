1 INTRODUCTION

2 OVERVIEW
2.1 Background for LLMs
2.2 Technical Evolution of GPT-series Models

3 RESOURCES OF LLMS
3.1 Publicly Available Model Checkpoints or APIs
3.2 Commonly Used Corpora for Pre-training
3.3 Commonly Used Datasets for Fine-tuning
3.4 Library Resource

4 PRE-TRAINING
4.1 Data Collection and Preparation
4.2 Architecture
4.3 Model Training

5 ADAPTATION OF LLMS
5.1 Instruction Tuning
5.2 Alignment Tuning
5.3 Parameter-Efficient Model Adaptation
5.4 Memory-Efficient Model Adaptation

6 UTILIZATION
6.1 Prompting
6.2 In-Context Learning
6.3 Chain-of-Thought Prompting
6.4 Planning for Complex Task Solving

7 CAPACITY AND EVALUATION
7.1 Basic Ability
7.2 Advanced Ability
7.3 Benchmarks and Evaluation Approaches
7.4 Empirical Evaluation

8 APPLICATIONS
8.1 LLM for Research Community
8.2 LLM for Specific Domains

9 CONCLUSION AND FUTURE DIRECTIONS