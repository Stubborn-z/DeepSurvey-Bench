1 Introduction
1.1 No peek rule
1.2 What needs to be protected
1.3 Computational Goals
2 No peek approaches for distributed deep learning
3 Federated Learning
3.1 Benefits
3.2 Limitations
3.3 Future Trends
4 Large Batch Synchronous SGD
4.1 Key Idea
4.2 Benefits
4.3 Limitations
4.4 Future Trends
5 Split Learning (SplitNN)
5.1 Key Idea
5.2 Benefits
5.3 Limitations
5.4 Future Trends
6 Methods to Further Reduce Leakage and Improve Efficiency
6.1 Obfuscation with Differential Privacy for NN
6.2 Homomorphic Encryption for NN
6.3 Multi-Party Computation (MPC) and Garbled Circuits
7 Comparison of resource efficiency across no peek distributed deep learning
8 Conclusion and Future Work