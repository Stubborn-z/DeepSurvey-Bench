I. INTRODUCTION  
A. Background  
B. Motivation: From Cloud LLMs to On-device LLMs to MEI LLMs  
C. Comparisons with Prior Surveys and Our Contributions  

II. PRELIMINARIES I: AN OVERVIEW OF LLMS AND MEI  
A. Large Language Models  
B. Mobile Edge Intelligence  
C. Lessons Learned for MEI4LLM  

III. PRELIMINARIES II: RESOURCE-EFFICIENT LLM TECHNIQUES  
A. Resource-efficient Inference  
B. Resource-efficient Fine-tuning  

IV. APPLICATION SCENARIOS  

V. AN OVERVIEW OF MEI4LLM  
A. AI-native Architecture  
B. Parameter-sharing LLM Caching and Delivery  
C. Distributed LLM Training (Fine-tuning)  
D. Distributed LLM Inference  

VI. EDGE CACHING AND DELIVERY FOR LLMS  
A. Edge LLM Caching  
B. Edge LLM Delivery  
C. Lessons Learned  

VII. EDGE TRAINING FOR LLMS  
A. Centralized Edge Learning  
B. Federated Edge Learning  
C. Split Learning  
D. Hierarchical Collaborative Learning  
E. Lessons Learned  

VIII. EDGE INFERENCE FOR LLMS  
A. Centralized Edge Inference  
B. Split inference  
C. Collaborative inference  
D. Lessons Learned  

IX. FURTHER RESEARCH OPPORTUNITIES  
A. Green Edge LLM  
B. Secure Edge LLM  

X. CONCLUSIONS