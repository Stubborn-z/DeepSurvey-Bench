1 Introduction  
2 Taxonomy of Applications  
3 Mathematical Formulation  
4 Architecture  
4.1 UNet  
4.2 Vision Transformer  
4.3 Cascaded Diffusion Models  
4.4 Latent Diffusion Models  
5 Temporal Dynamics  
5.1 Spatio-Temporal Attention Mechanisms  
5.2 Temporal Upsampling  
5.3 Structure Preservation  
6 Training & Evaluation  
6.1 Video Data Sets  
6.2 Image Data Sets  
6.3 Evaluation Metrics  
6.4 Benchmarks  
7 Video Generation  
7.1 Unconditional Generation & Text-to-Video  
7.2 Training-Free Models  
7.3 Personalized Generation  
7.4 Image-Conditioned Generation  
8 Video Completion & Long Video Generation  
8.1 Temporal Upsampling & Video Prediction  
8.2 Alternative Approaches  
9 Audio-Conditioned Synthesis  
9.1 Audio-Conditioned Generation & Editing  
9.2 Talking Head Generation  
10 Video Editing  
10.1 One-Shot Tuning Methods  
10.2 Depth-Conditioned Editing  
10.3 Pose-Conditioned Editing  
10.4 Leveraging Pre-Trained Video Generation Model for Video Editing  
10.5 Multi-Conditional Editing  
10.6 Other Approaches  
10.7 Video Restoration  
11 Video Diffusion Models for Intelligent Decision Making  
11.1 Representation Learning  
11.2 World Models  
11.3 Synthetic Training Data  
12 Outlook and Challenges  
13 Conclusion