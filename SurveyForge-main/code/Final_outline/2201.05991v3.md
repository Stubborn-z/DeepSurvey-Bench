1 INTRODUCTION

2 THE TRANSFORMER

3 INPUT PRE-PROCESSING
3.1 Embedding
3.2 Tokenization
3.3 Positional Embeddings (PE)
3.4 Discussion on input pre-processing

4 ARCHITECTURE
4.1 Efficient designs
4.2 Long-term (temporal) modeling
4.3 Multi-view approaches
4.4 Discussion on Architecture

5 TRAINING A TRANSFORMER
5.1 Training regime
5.2 Self-supervised pretext tasks
5.3 Discussion on training strategies

6 PERFORMANCE
6.1 Video classification
6.2 Comparison among state-of-the-art models
6.3 Discussion on performance

7 FINAL DISCUSSION
7.1 Generalization
7.2 Future work