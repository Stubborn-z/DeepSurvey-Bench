1 Introduction

2 Preliminaries of the Transformer  
2.1 Vanilla Transformer  
2.2 Input Encoding and Positional Encoding  
2.3 Multi-head Attention  
2.4 Feed-forward and Residual Network  

3 Taxonomy of Transformers in Time Series

4 Network Modifications for Time Series  
4.1 Positional Encoding  
4.2 Attention Module  
4.3 Architecture-based Attention Innovation  

5 Applications of Time Series Transformers  
5.1 Transformers in Forecasting  
5.2 Transformers in Anomaly Detection  
5.3 Transformers in Classification  

6 Experimental Evaluation and Discussion  
6.1 Robustness Analysis  
6.2 Model Size Analysis  
6.3 Seasonal-Trend Decomposition Analysis  

7 Future Research Opportunities  
7.1 Inductive Biases for Time Series Transformers  
7.2 Transformers and GNN for Time Series  
7.3 Pre-trained Transformers for Time Series  
7.4 Transformers with Architecture Level Variants  
7.5 Transformers with NAS for Time Series  

8 Conclusion