ACM Reference Format:

1 INTRODUCTION

2 BACKGROUND  
2.1 Large Language Models  
2.2 AI Model Evaluation  

3 WHAT TO EVALUATE  
3.1 Natural Language Processing Tasks  
3.2 Robustness, Ethic, Bias, and Trustworthiness  
3.3 Social Science  
3.4 Natural Science and Engineering  
3.5 Medical Applications  
3.6 Agent Applications  
3.7 Other Applications  

4 WHERE TO EVALUATE: DATASETS AND BENCHMARKS  
4.1 Benchmarks for General Tasks  
4.2 Benchmarks for Specific Downstream Tasks  
4.3 Benchmarks for Multi-modal Tasks  

5 HOW TO EVALUATE  
5.1 Automatic Evaluation  
5.2 Human Evaluation  

6 SUMMARY  
6.1 Task: Success and Failure Cases of LLMs  
6.2 Benchmark and Evaluation Protocol  

7 GRAND CHALLENGES AND OPPORTUNITIES FOR FUTURE RESEARCH  
7.1 Designing AGI Benchmarks  
7.2 Complete Behavioral Evaluation  
7.3 Robustness Evaluation  
7.4 Dynamic and Evolving Evaluation  
7.5 Principled and Trustworthy Evaluation  
7.6 Unified Evaluation that Supports All LLMs Tasks  
7.7 Beyond Evaluation: LLMs Enhancement  

8 CONCLUSION