1 Introduction  
3 Preliminaries  
4 Transformers  
4.1 Input Layer  
4.2 Hidden Layers  
4.3 Networks and Output Layers  
4.4 Uniformity and Precision  
4.5 Summary  
5 Languages and Language Classes  
5.1 Automata and Classes L, NL, P  
5.3 Logic  
5.4 Relationships  
6 Current Results  
6.1 Decoders with Intermediate Steps  
6.2 Leftmost-hard/Rightmost-hard Attention  
6.3 Average-hard and Softmax Attention  
7 Conclusions