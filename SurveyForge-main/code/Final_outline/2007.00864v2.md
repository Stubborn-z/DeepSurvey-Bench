I. Introduction  

II. Background: Need for Efficient Execution of ML Models on Hardware Accelerators  
A. Domain-Specific Machine Learning Models  
B. Hardware Accelerators for Machine Learning  
C. Need for Further Efficient Execution  

III. Acceleration Opportunities Due to Compact Models and the Need for Special Support  
A. Opportunities Due to Sparse Tensors  
B. Opportunities Due to Size-Reduced Tensors  
C. Opportunities Due to Quantized Tensors  

V. Encodings for Compressing Sparse Tensors  
A. Encoding Formats and Implications  

VI. Extraction of Matching Data for Computations on Non-Zeros  
A. Non-Zero Detection and Extraction Mechanisms  
B. Centralized vs. Distributed Management  

VII. Memory Management of Compressed Tensors  
A. Leveraging Data Reuse Opportunities  
B. Hiding Miss Latency Behind Computations  
C. Management of Multi-Bank Memory  
D. Reusing Intermediate Tensors  
E. Techniques for Further Energy-Efficiency  

VIII. Interconnects for Distributing Non-Zeros and Reducing Partial Outputs  
A. Mechanisms for Distribution of Operands  
B. Mechanisms for Reduction of Partial Outputs  

IX. PE Architecture Design  
A. Functional Units  
B. Dataflow Mechanisms  
C. Leveraging Value Similarity  

X. Load Balancing of Effectual Computations  
A. Sources and Impact of Imbalance  
B. Software Directed Load Balance  
C. Load Balancing with Hardware Structures  

XI. Write-Back and Post-Processing  
A. Write-Back from PEs  
B. Data Assembling  
C. Data Layout Transformations  

XII. Compiler Support  
A. Intermediate Representations  
B. Support for Sparse Tensors  
C. Compiler Optimizations  

XIII. Trends and Future Directions  
B. Design Tools and Frameworks  
C. Accelerating Training of ML Models  
D. Applying Techniques for Sparsity to Other Domains  

XIV. Related Work  

XV. Summary