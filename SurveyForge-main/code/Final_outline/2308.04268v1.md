1 INTRODUCTION

2 DISTILLATION OBJECTIVES  
2.1 Knowledge Compression  
2.2 Knowledge Expansion  
2.3 Knowledge Adaptation  
2.4 Knowledge Enhancement  
2.5 Comparison Analysis  

3 KNOWLEDGE FORMULATION  
3.1 Knowledge Construction  
3.2 Knowledge Optimization  

4 DISTILLATION WITH REPRESENTATIVE LEARNING ALGORITHMS  
4.1 Multi-Teacher Distillation  
4.2 Graph-Based Distillation  
4.3 Federated Distillation  
4.4 Cross-Modal Distillation  

5 DISTILLATION SCHEMES  
5.1 Online Distillation  
5.2 Self-Distillation  

6 APPLICATIONS  
6.1 Classification Purpose  
6.2 Recognition Purpose  
6.3 Generation Purpose  
6.4 Ranking Purpose  
6.5 Regression Purpose  

7 OPPORTUNITIES AND FUTURE WORKS  
7.1 Teacher-Student Architecture Design  
7.2 Knowledge Quality  
7.3 Theoretical Understandings of Regression-Based Knowledge Learning  

8 CONCLUSION