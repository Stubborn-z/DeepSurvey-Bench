1 INTRODUCTION  
2 BACKGROUND  
2.1 Transformer-based LLMs  
2.2 LLM Training Workloads Characteristics  
2.3 LLM Training Challenges  
2.4 Related Survey  
3 INFRASTRUCTURE FOR LLM TRAINING  
3.1 AI Accelerators  
3.2 Network Infrastructure  
3.3 Storage  
3.4 Scheduling  
4 PARALLELISM SCHEMES FOR LLM TRAINING  
4.1 Hybrid Parallelism  
4.2 Auto Parallelism  
4.3 Heterogeneous Parallelism  
5 COMPUTATION OPTIMIZATIONS  
5.1 Operator Optimizations  
5.2 Mixed-precision Training  
6 MEMORY OPTIMIZATIONS  
6.1 Activation Re-computation  
6.2 Redundancy Reduction  
6.3 Defragmentation  
6.4 Offloading  
7 COMMUNICATION OPTIMIZATIONS  
7.1 Collective Communication  
7.2 Communication Scheduling  
7.3 In-Network Aggregation  
8 FAULT TOLERANCE  
8.1 LLM Failure Analysis  
8.2 Anomaly Detection  
8.3 Checkpoint-Based Recovery  
8.4 Checkpoint-Free Recovery  
9 CONCLUSION AND OUTLOOKS