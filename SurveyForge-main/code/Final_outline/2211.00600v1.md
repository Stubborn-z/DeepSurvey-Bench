1 Introduction
1.1 Motivation
1.2 Terminology Unification
1.2.1 Imitation Learning
1.2.2 Interactive Machine Learning
1.2.3 Interactive Imitation Learning
1.3 Other Surveys and Outline
2 Theoretical Background
2.1 Decision Theory
2.1.1 Markov Decision Process (MDP)
2.1.2 Sequential Decision-Making Problem
2.2 Interactive Imitation Learning
3 Modalities of Interaction
3.1 Human Feedback in Evaluative Space
3.1.1 Learning from Human Reinforcements
3.1.2 Learning from Human Preference
3.2 Human Feedback in Transition (State-Action) Space
3.2.1 Learning from Human Absolute Corrections
3.2.2 Learning from Human Relative Corrections
3.3 Discussion
4 Behavior Representations Learned from Interactions
4.1 Direct Policy Learning (Actions)
4.2 Learning Desired State Transition/Dynamics
4.3 Learning Reward and Objective Functions
4.4 Discussion
5.1 Task Features Learning
5.2 Object Affordances
5.3 Forward and Inverse Transition Models
5.4 Confidence, Novelty and Risk Models
5.5 Human Models for Feedback Interpretation
5.6 Discussion
6 Model Representations (Function Approximation)
6.1 Linear Models
6.2 Gaussian Process
6.3 Gaussian Mixture Model
6.4 Support Vector Machine
6.5 Neural Networks
6.6 Movement-Conditioned Models
6.7 Discussion
7 On/Off Policy Learning
7.1 Online and Offline Learning
7.2 On-policy and Off-policy Learning
7.3 On-Policy/Off-Policy Learning in Imitation Learning
7.4 Discussion
8 Reinforcement Learning with Human-in-the-Loop
8.1 Other related approaches
8.2 Historical perspective
8.3 Reinforcement Learning with Human-in-the-Loop approaches
8.4 Discussion
9 Interfaces
9.1 Human-to-Robot Interfaces
9.2 Robot-to-Human Interfaces
9.3 Interface Design
9.4 Discussion
10 User Studies in IIL
10.1 Study Setup
10.2 Evaluation Methods
10.3 Discussion
11 Benchmarks and Applications
11.1 Applications
11.2 Datasets
11.3 Benchmarks
11.4 Discussion
12 Research Challenges and Opportunities
13 Conclusion