1 INTRODUCTION

2 RELATED WORKS
2.1 Neural Machine Translation
2.2 Pretraining with Unsupervised Feature-based Approaches
2.3 Korean NLP Benchmarks

3 KOREAN PLM ARCHITECTURES
3.1 Encoder-Centric Models
3.2 Decoder-Centric Models
3.3 Seq2Seq-Centric Models

4 EXPERIMENT
4.1 Single Sentence Tasks
4.2 Multiple Sentence and Agent Tasks

5 CONCLUSION