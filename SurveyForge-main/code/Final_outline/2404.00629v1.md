1 Introduction  
2 Background  
2.1 Terminology  
2.2 Related Work  
3 Risk Taxonomy  
4 Attack  
4.1 Attack Strategies by Language Model Capabilities  
4.2 Attack Searchers  
5 Evaluation  
5.1 Attack Evaluation  
5.2 Defense Evaluation  
5.3 Evaluators  
5.4 Benchmarks  
6 Safeguard  
6.1 Training-time Defense  
6.2 Inference-time Defense  
7 Multi-modal Model Red Teaming  
7.1 Text-to-Image Model Attack  
7.2 Vision Language Model Attack  
7.3 Benchmarks  
7.4 Safeguards  
8 LLM-based Application Red Teaming  
8.1 Application Scenarios and Risks  
8.2 Attack Methods  
8.3 Defense  
8.4 Evaluation  
9 Future Direction  
9.1 Systematic Exploration  
9.2 Evaluation  
9.3 Defense  
9.4 LLM Applications  
10 Conclusion