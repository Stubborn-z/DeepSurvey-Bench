1 Introduction  
2 Data  
2.1 Filtering  
2.2 Active Learning  
2.3 Curriculum Learning  
2.4 Estimating Data Quality  
3 Model Design  
3.1 Improving Attention in Transformers  
3.2 Sparse Modeling  
3.3 Parameter Efficiency  
3.4 Retrieval-Augmented Models  
3.5 Model Design Considerations  
4 Pre-training  
4.1 Optimization Objective  
4.2 Pre-training Considerations  
5 Fine-tuning  
5.1 Parameter-Efficient Fine-Tuning  
5.2 Multi-Task and Zero-Shot Learning  
5.3 Prompting  
5.4 Fine-Tuning Considerations  
6 Inference and Compression  
6.1 Pruning  
6.2 Knowledge Distillation  
6.3 Quantization  
6.4 Inference Considerations  
7 Hardware Utilization  
7.1 Reducing Optimizer Memory  
7.2 Specialized Hardware  
7.3 Co-design  
7.4 Edge Devices  
7.5 Hardware Considerations  
8 Evaluating Efficiency  
8.1 Evaluation Measures  
9 Model Selection  
9.1 Hyperparameter Search  
9.2 Hyperparameter Transfer  
9.3 Model Selection Considerations  
10 Conclusion