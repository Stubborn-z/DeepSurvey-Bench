1 INTRODUCTION

2 BACKGROUND  
2.1 3D Representations  
2.2 Large Language Model (LLM)  
2.3 2D Vision-Language Models  
2.4 Vision Foundation Models (VFMs)  

3 TASKS AND METRICS  
3.1 3D Captioning  
3.2 3D Grounding  
3.3 3D Conversation  
3.4 3D Embodied Agents  
3.5 Text-to-3D Generation  

4 3D TASKS WITH LLMS  
4.1 How do LLMs process 3D scene information?  
4.2 LLMs for Enhancing 3D Task Performance  
4.3 LLMs for 3D Multi-Task Learning  
4.4 LLMs as 3D Multi-Modal Interfaces  
4.5 LLMs for Embodied Agents  
4.6 LLMs for 3D Generation  

5 3D TASKS WITH VLMS  
5.1 Open-Vocabulary 3D Scene Understanding  
5.2 Text-Driven 3D Generation  
5.3 End-to-End Architectures for 3D Vision & Language  

6 DATASETS  

7 CHALLENGES AND OPPORTUNITIES

8 CONCLUSION