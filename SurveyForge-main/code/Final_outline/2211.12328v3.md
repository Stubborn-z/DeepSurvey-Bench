1 Introduction
2 Background
3 Multimodal representation learning
3.1 Text representation
3.2 Visual representation
3.3 Sequential models for VL tasks
3.4 Multimodal Transformers
3.5 Image generation
3.6 Evaluation metrics for VL models
4 Graphs
4.1 Knowledge Graphs
4.2 Graph representation
5 Knowledge
5.1 Types of knowledge sources
5.2 Widely used Knowledge graphs
6 Multimodal Tasks with knowledge
7 Knowledge in Visual Question Answering (K-VQA)
7.1 Datasets
7.2 Methods
7.3 Evaluation
8 Knowledge in Visual Word Sense Disambiguation (K-VWSD)
8.2 Methods
8.3 Evaluation
9 Knowledge in Visual Reasoning (K-VR)
9.1 Datasets
9.2 Methods
9.3 Evaluation
10 Knowledge in Visual Commonsense Reasoning (K-VCR)
10.1 Datasets
10.2 Methods
10.3 Evaluation
11 Knowledge in Image Captioning (K-IC)
11.1 Datasets
11.2 Methods
11.3 Evaluation
12 Knowledge in Visual Dialog (K-VD)
12.1 Datasets
12.2 Methods
12.3 Evaluation
13 Knowledge in Visual Storytelling (K-VIST)
13.1 Datasets
13.2 Methods
13.3 Evaluation
14 Knowledge in Image Generation
14.1 Datasets
14.2 Methods
14.3 Metrics
15 Multi-task transformers with knowledge
15.1 Methods
15.2 Evaluation
16 The future of knowledge in VL
16.1 Explainability and biases
16.2 Exploitation and integration of more knowledge senses
16.3 Zero-shot learning
16.4 Language Models as Knowledge Bases
16.5 Datasets
16.6 Knowledge-enhanced generative tasks
16.7 The need for multi-task learners
17 Conclusion