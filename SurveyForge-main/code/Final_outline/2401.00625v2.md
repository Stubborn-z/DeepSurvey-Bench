1 Introduction
1.1 Related work
1.2 Outline
2 Preliminary and Taxonomy
2.1 Preliminaries
2.2 Proposed Taxonomy
3 LLM Architecture Design
3.1 Efficient Transformer Architecture
3.2 Non-transformer Architecture
4 LLM Pre-training
4.1 Memory Efficiency
4.2 Data Efficiency
5 LLM Fine-tuning
5.1 Parameter-efficient Fine-tuning
5.2 Full-Parameter Fine-tuning
6 LLM Inference
6.1 Model Compression
6.2 Dynamic Acceleration
7 System Design
7.1 Deployment Optimization
7.2 Support Infrastructure
7.3 Other Systems
8 Technique Categorization by Resources
8.1 Computation Efficiency
8.2 Memory Efficiency
8.3 Energy Efficiency
8.4 Financial Cost Efficiency
8.5 Network Communication Efficiency
9 Benchmark and Evaluation Metrics
9.1 Evaluation Metrics
9.2 Benchmarks
10 Open Challenges and Future Directions
10.1 Managing Resource Type Disagreements
10.2 Combining Techniques for Resource Efficiency
10.3 Standardized and Unified Evaluation
10.4 Explainability and Robustness
10.5 AutoML for Resource-efficient LLMs
10.6 Edge Computing with LLMs
10.7 Theoretical Insights into Scaling Laws
11 Conclusion