Contents
1 Introduction
2 Why LLM Alignment?
2.1 Social and Ethical Risks of LLMs
2.2 Potential Risks Associated with Advanced LLMs
3 What is LLM Alignment?
3.1 Origins of AI Alignment
3.2 Research Landscape and Ingredients of AI Alignment
3.3 Related Concepts
3.4 From AI Alignment to LLM Alignment
4 Outer Alignment
4.1 Major Goals Specified in Outer Alignment of LLMs
4.2 Overview of Approaches to Outer Alignment
4.3 Non-recursive Oversight
4.4 Scalable Oversight
5 Inner Alignment
5.1 Inner Alignment Failures
5.2 Inner Alignment Methodology
5.3 Empirical Experiment Proposals for Inner Alignment
6 Mechanistic Interpretability
7 Attacks on Aligned Language Models
7.1 Privacy Attacks
7.2 Backdoor Attacks
7.3 Adversarial Attacks
8 Alignment Evaluation
8.1 Factuality Evaluation
8.2 Ethics Evaluation
8.3 Toxicity Evaluation
8.4 Stereotype and Bias Evaluation
8.5 General Evaluation
9 Future Directions and Discussions
9.1 Theoretical Research for LLM Alignment
9.2 Scalable Oversight
9.3 Empirical Research into Deceptive Alignment
9.4 Automated LLM Alignment
9.5 Explainability and Transparency
9.6 Dynamic Evaluation of LLM Alignment via Adversarial Attacks
9.7 Field Building of LLM Alignment: Bridging between LLM and AI Alignment Community
10 Conclusion