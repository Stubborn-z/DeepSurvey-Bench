1 Introduction

2 Background on large language models  
2.1 Components of the transformer architecture  
2.2 Types of LLMs  
2.3 Training of large language models  

3 Digital forgetting  
3.1 Motivations for digital forgetting  
3.2 Types of digital forgetting  
3.3 Requirements of digital forgetting  

4 Approaches to digital forgetting in LLM  

5 Survey on unlearning in LLMs  
5.1 Global weight modification  
5.2 Local weight modification  
5.3 Architecture modification  
5.4 Input/output modification  

6 Evaluation of unlearning in LLMs  
6.1 Datasets  
6.2 Models  
6.3 Metrics and attacks to evaluate forgetting  
6.4 Retaining evaluation  
6.5 Runtime evaluation  

7 Challenges and potential solutions  
7.1 Guarantees of forgetting  
7.2 Retaining of model utility  
7.3 Generalization of unlearning  
7.4 Runtime and scalability  
7.5 Evaluation  
7.6 When can each method be used?  
7.7 Black-box access scenario  
7.8 Reconciling effectiveness, utility, and efficiency  

8 Conclusions