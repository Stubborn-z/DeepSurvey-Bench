1 Introduction  
2 Preliminaries  
2.1 Table Structure  
2.2 Cell Information  
2.3 Key Differences Between Table and Text  
2.4 Existing Large Table Corpus  
3 Model  
3.1 Tabular Sequence Serialization  
3.2 Input Featurization and Embedding  
3.3 Encoder and Decoder Architecture  
3.4 Structure-based Attention  
3.5 Model Efficiency  
4 Pre-training Objectives  
4.1 Denoising Autoencoder Objectives  
4.2 Task-specific Objectives  
5 Downstream Tasks  
6 Conclusion and Discussion