Keywords: transformers, attention, state-space models, capsule networks, survey, review, deep learning, architectures, layers

1 Introduction

2 Overview

3 Transformers  
3.1 Classical and State-of-the-Art Transformer Architectures  
3.2 Trends and Alternatives to Transformers  

4 Loss Functions and Optimization  
4.1 Loss Functions  
4.2 Regularization  
4.3 Optimization  

5 Self, Semi-Supervised and Contrastive Learning  

6 Architectures and Layers  
6.1 Activation  
6.2 Skip Connections  
6.3 Normalization  
6.4 Attention  
6.5 Graph Neural Networks  

7 Discussion  

8 Conclusions