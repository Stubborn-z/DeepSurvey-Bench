1 Introduction  
2 Pretext Tasks  
2.1 Color Transformation  
2.2 Geometric Transformation  
2.3 Context-Based  
2.4 View Prediction (Cross modal-based)  
2.5 Identifying the Right Pretext Task  
2.6 Pretext Tasks in NLP  
3 Architectures  
3.1 End-to-End Learning  
3.2 Using a Memory Bank  
3.3 Using a Momentum Encoder  
3.4 Clustering Feature Representations  
4 Encoders  
5 Training  
6 Downstream Tasks  
6.1 Visualizing Kernels and Feature Maps  
6.2 Nearest Neighbor Retrieval  
7 Benchmarks  
8 Contrastive Learning in NLP  
9 Discussions and Future Directions  
9.1 Lack of Theoretical Foundation  
9.2 Selection of Data Augmentation and Pretext Tasks  
9.3 Proper Negative Sampling during Training  
9.4 Dataset Biases  
10 Conclusion