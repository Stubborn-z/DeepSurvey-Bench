1 INTRODUCTION  
2 PRELIMINARIES  
2.1 Denoising Diffusion Probabilistic Models  
2.2 Text-to-Image Diffusion Models  
2.3 Taxonomy  
3 HOW TO CONTROL TEXT-TO-IMAGE DIFFUSION MODELS WITH NOVEL CONDITIONS  
3.1 Conditional Score Prediction  
3.2 Condition-Guided Score Estimation  
4 CONTROLLABLE TEXT-TO-IMAGE GENERATION WITH SPECIFIC CONDITIONS  
4.1 Personalization  
4.2 Spatial Control  
4.3 Advanced Text-Conditioned Generation  
4.4 In-Context Generation  
4.5 Brain-Guided Generation  
4.6 Sound-Guided Generation  
4.7 Text Rendering  
5 CONTROLLABLE GENERATION WITH MULTIPLE CONDITIONS  
5.1 Joint Training  
5.2 Continual Learning  
5.3 Weight Fusion  
5.4 Attention-based Integration  
5.5 Guidance Composition  
6 UNIVERSAL CONTROLLABLE TEXT-TO-IMAGE GENERATION  
6.1 Universal Conditional Score Prediction Framework  
6.2 Universal Condition-Guided Score Estimation  
7 APPLICATIONS  
7.1 Image Manipulation  
7.2 Image Completion and Inpainting  
7.3 Image Composition  
7.4 Text/Image-to-3D Generation  
8 CONCLUSION