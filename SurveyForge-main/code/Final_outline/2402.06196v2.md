I. INTRODUCTION
II. LARGE LANGUAGE MODELS
A. Early Pre-trained Neural Language Models
C. Other Representative LLMs
III. HOW LLMS ARE BUILT
A. Dominant LLM Architectures
B. Data Cleaning
C. Tokenizations
E. Model Pre-training
F. Fine-tuning and Instruction Tuning
G. Alignment
H. Decoding Strategies
I. Cost-Effective Training/Inference/Adaptation/Compression
IV. HOW LLMS ARE USED AND AUGMENTED
A. LLM limitations
B. Using LLMs: Prompt Design and Engineering
C. Augmenting LLMs through external knowledge - RAG
D. Using External Tools
E. LLM Agents
V. POPULAR DATASETS FOR LLMS
VI. PROMINENT LLMS’ PERFORMANCE ON BENCHMARKS
A. Popular Metrics for Evaluating LLMs
B. LLMs’ Performance on Different Tasks
VII. CHALLENGES AND FUTURE DIRECTIONS
A. Smaller and more efficient Language Models
B. New Post-attention Architectural Paradigms
C. Multi-modal Models
D. Improved LLM Usage and Augmentation techniques
E. Security and Ethical/Responsible AI
VIII. CONCLUSION