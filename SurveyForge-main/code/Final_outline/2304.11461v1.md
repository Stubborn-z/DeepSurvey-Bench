1. Introduction  
2. Recurrent Neural Network  
2.1. Dynamical System  
2.2. Parameter Sharing  
2.3. Backpropagation Through Time (BPTT)  
3. Gradient Vanishing or Explosion in Long-term Dependencies  
3.1. Close-to-identity Weight Matrix  
3.2. Long Delays  
3.3. Leaky Units  
3.4. Echo State Networks  
3.5. Other Methods  
4. Long Short-Term Memory Network  
4.1. LSTM Gates and Cells  
4.2. History and Variants of LSTM  
4.3. Gated Recurrent Units (GRU)  
5. Bidirectional RNN and LSTM  
5.1. Justification of Bidirectional Processing  
5.2. Bidirectional RNN  
5.3. Bidirectional LSTM  
5.4. Embeddings from Language Model (ELMo)  
6. Conclusion