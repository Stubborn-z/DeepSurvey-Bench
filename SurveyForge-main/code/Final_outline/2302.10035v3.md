1 Introduction  
2 Background  
2.1 Conventional Deep Learning  
2.2 Pre-training in Natural Language Processing  
2.3 Pre-training in Computer Vision  
2.4 Pre-training in Audio and Speech  
3 Multi-Modal Pre-training  
3.1 Task Definition and Key Challenges  
3.2 Advantages of MM-PTMs  
3.3 Pre-training Data  
3.4 Pre-training Objectives  
3.5 Pre-training Network Architecture  
3.6 Pre-training using Knowledge  
3.7 Characteristics of Different Pre-trained Big Models  
4 Downstream Tasks  
4.1 Generative Tasks  
4.2 Classification Tasks  
4.3 Regression Tasks  
4.4 Prompt Learning  
5 Experimental Analysis  
5.1 Model Parameters and Training Information  
5.2 Performance on Representative Downstream Tasks  
6 Research Directions  
7 Conclusion