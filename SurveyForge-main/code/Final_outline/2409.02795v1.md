1. Introduction

3. The Unified View of Preference Learning for LLM

4. Preference Data  
4.1. On-policy Data Collection  
4.2. Off-policy Data Collection

5. Feedbacks  
5.1. Direct Feedback  
5.2. Model-based Feedback

6. Algorithms  
6.1. Point-wise Methods  
6.2. Pair-wise Contrasts  
6.3. List-wise Contrasts  
6.4. Training-Free Alignment

7. Evaluation  
7.1. Rule-based Evaluation  
7.2. LLM-based Evaluation  

8. Future Directions

9. Conclusion