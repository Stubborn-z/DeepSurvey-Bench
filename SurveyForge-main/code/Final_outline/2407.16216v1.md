1 Introduction

2 Categorical Outline  
2.1 Reward Model  
2.2 Feedback  
2.3 Reinforcement Learning (RL)  
2.4 Optimization  

3 Individual Paper Reviews in Detail  
3.1 RLHF/PPO  
3.2 RLAIF  
3.3 Direct Human Preference Optimization  
3.4 Token-level DPO  
3.5 Iterative/Online DPO  
3.6 Binary Feedback  
3.7 Merge SFT and Alignment  
3.8 Length Control DPO and Reference Free DPO  
3.9 Listwise Preference Optimization  
3.10 Negative Preference Optimization  
3.11 Nash Learning  
3.12 Beyond Reverse KL Divergence  
3.13 Comparison of Different Methods  

4 Future Directions  
4.1 General Tasks for Alignment Evaluation  
4.2 Apply Implicit Reward Models, Listwise Preference and Nash Learning to Larger Scale LMs  
4.3 Experiments on Binary Feedbacks  
4.4 Experiments on Helpful AI Feedback  
4.5 Speeding up Nash Learning  
4.6 Termination of Iterative/Online Learning  
4.7 Simplify SFT + Alignment