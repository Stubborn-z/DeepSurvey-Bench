1 Introduction

2 Literature Review

3 Classical Activation Functions  
3.1 Binary Activation Function  
3.2 Sigmoid Family of Activation Functions  
3.3 Class of Sigmoid-Weighted Linear Units  
3.4 Gated Linear Unit (GLU)  
3.5 Softmax  
3.6 Rectified Linear Function (ReLU)  
3.7 Maxsig  
3.8 Square-Based Activation Functions  
3.9 Square-Root-Based Activation Function (SQRT)  
3.10 Bent Identity  
3.11 Mishra Activation Function  
3.12 Saha-Bora Activation Function (SBAF)  
3.13 Logarithmic Activation Function  
3.14 Symexp  
3.15 Scaled Polynomial Constant Unit (SPOCU)  
3.16 Polynomial Universal Activation Function (PUAF)  
3.17 Softplus  
3.18 Parametric Softplus (PSoftplus)  
3.19 Rand Softplus (RSP)  
3.20 Aranda-Ordaz  
3.21 Bi-Firing Activation Function (bfire)  
3.22 Bounded Bi-Firing Activation Function (bbfire)  
3.23 Piecewise Mexican-Hat Activation Function (PMAF)  
3.24 Piecewise Radial Basis Function (PRBF)  
3.25 Comb-H-Sine  
3.26 Modified Arcsinh  
3.27 Hyper-Sinh  
3.28 Arctid  
3.29 Sine  
3.30 Cosine  
3.31 Cosid  
3.32 Sinp  
3.33 Growing Cosine Unit (GCU)  
3.34 Amplifying Sine Unit (ASU)  
3.35 Sinc  
3.36 Decaying Sine Unit (DSU)  
3.37 Hyperbolic Cosine Linearized Squashing Function (HcLSH)  
3.38 Polyexp  
3.39 Exponential  
3.40 E-Tanh  
3.41 Wave  
3.42 Non-Monotonic Cubic Unit (NCU)  
3.43 Triple  
3.44 Shifted Quadratic Unit (SQU)  
3.45 Knowledge Discovery Activation Function (KDAC)  
3.46 K-Winner-Takes-All Activation Function (k-WTA)  
3.47 Volatility-Based Activation Function (VBAF)  
3.48 Chaotic Activation Functions  

4 Adaptive Activation Functions  
4.1 Transformative Adaptive Activation Function (TAAF)  
4.2 The ReLU-Based Family of Adaptive Functions  
4.3 Sigmoid-Based Adaptive Functions  
4.4 Adaptive Sigmoid-Weighted Linear Units  
4.5 Tuned Softmax (tsoftmax)  
4.6 Generalized Lehmer Softmax (glsoftmax)  
4.7 Generalized Power Softmax (gpsoftmax)  
4.8 Adaptive Radial Basis Function (ARBF)  
4.9 Parametric Gaussian Error Linear Unit (PGELU)  
4.10 Parametric Flatted-T Swish (PFTS)  
4.11 Parametric Flatten-P Mish (PFPM)  
4.12 Gaussian Error Unit (GEU)  
4.13 Scaled-Gamma-Tanh Activation Function (SGT)  
4.14 RSign  
4.15 P-SIG-RAMP  
4.16 Locally Adaptive Activation Function (LAAF)  
4.17 Shape Autotuning Adaptive Activation Function (SAAAF)  
4.18 Noisy Activation Functions  
4.19 Fractional Adaptive Activation Functions  
4.20 Scaled Softsign  
4.21 Parameterized Softplus (s_{+}2L)  
4.22 Universal Activation Function (UAF)  
4.23 Learnable Extended Activation Function (LEAF)  
4.24 Generalized ReLU (GReLU)  
4.25 Multi Quadratic Activation Function (MAF)  
4.26 EIS Activation Functions  
4.27 Global-Local Neuron (GLN)  
4.28 Neuron-Adaptive Activation Function  
4.29 Adaptive Piece-Wise Linear Unit (APLU)  
4.30 Simple Piecewise Linear and Adaptive Function with Symmetric Hinges (SPLASH)  
4.31 Multi-Bias Activation (MBA)  
4.32 Mexican ReLU (MeLU)  
4.33 S-Shaped Rectified Linear Activation Unit (SReLU)  
4.34 Alternated Left ReLU (All-ReLU)  
4.35 Piecewise Linear Unit (PLU)  
4.36 Adaptive Linear Unit (AdaLU)  
4.37 Trapezoid-Shaped Activation Function (TSAF)  
4.38 Adaptive Richard’s Curve Weighted Activation (ARiA)  
4.39 Modified Weibull Function  
4.40 Sincos  
4.41 Combination of Sine and Logistic Sigmoid (CSS)  
4.42 Catalytic Activation Function (CatAF)  
4.43 Expcos  
4.44 Multi-Bin Trainable Linear Unit (MTLU)  
4.45 Continuous Piecewise Nonlinear Activation Function CPN  
4.46 Look-Up Table Unit (LuTU)  
4.47 Maxout Unit  
4.48 Adaptive Blending Unit (ABU)  
4.49 Padé Activation Unit (PAU)  
4.50 Randomized Padé Activation Unit (RPAU)  
4.51 Enhanced Rational Activation (ERA)  
4.52 Orthogonal Padé Activation Unit (OPAU)  
4.53 Spline Interpolating Activation Functions  
4.54 Truncated Gaussian Unit (TruG)  
4.55 Mollified Square Root Function (MSRF) Family  
4.56 Complex Approaches  
4.57 SAVE-Inspired Activation Functions  

5 Conclusion