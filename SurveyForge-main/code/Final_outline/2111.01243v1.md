1 Introduction
2 Paradigm 1: Pre-Train then Fine-Tune
2.1 The Beginnings of the Paradigm Shift
2.2 Modern Pre-Trained Language Models
2.3 Pre-Training Corpora
2.4 Fine-Tuning: Applying PLMs to NLP Tasks
3 Paradigm 2: Prompt-based Learning
3.1 Learning from Instructions and Demonstrations
3.2 Template-based Learning
3.3 Learning from Proxy Tasks
4 Paradigm 3: NLP as Text Generation
4.1 Generating Label-Augmented Texts
4.2 Generating Word Indices
4.3 Generating Answers
4.4 Filling Templates
4.5 Generating Structure-Linearized Texts
4.6 Ranking Input-Output Pairs
5 Data Generation via PLM
5.1 Augmenting NLP Models with Automatically Generated Data
5.2 Generating Auxiliary Data to Improve Different Aspects of NLP Models
6 Discussion
7 Conclusion