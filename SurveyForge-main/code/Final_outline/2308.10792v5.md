1 Introduction  
2 Methodology  
2.1 Instruction Dataset Construction  
2.2 Instruction Tuning  
3 Datasets  
3.1 Human-crafted Data  
3.2 Synthetic Data via Distillation  
3.3 Synthetic Data via Self-Improvement  
4 Instruction Fine-tuned LLMs  
5 Multi-modality Instruction Fine-tuning  
5.1 Multi-modality Datasets  
5.2 Multi-modality Instruction Fine-tuning Models  
6 Domain-specific Instruction Finetuning  
6.1 Dialogue  
6.2 Intent Classification and Slot Tagging  
6.3 Information Extraction  
6.4 Aspect-based Sentiment Analysis  
6.5 Writing  
6.6 Medical  
6.7 Arithmetic  
6.8 Code  
7 Efficient Tuning Techniques  
7.1 LoRA  
7.2 HINT  
7.3 Qlora  
7.4 LOMO  
7.5 Delta-tuning  
8 Evaluation, Analysis and Criticism  
8.1 HELM Evaluation  
8.2 Low-resource Instruction Tuning  
8.3 Smaller Instruction Dataset  
8.4 Evaluating Instruction-tuning Datasets  
8.5 Do IT just learn Pattern Copying?  
8.6 Proprietary LLMs Imitation  
9 Conclusion