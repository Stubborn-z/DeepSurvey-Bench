1 INTRODUCTION  
1.1 Mathematical Preliminaries and Notations  
1.2 Literature Overview  
1.3 Flat Minima  
1.4 Linear Subspace  
1.5 SGD Trajectory  
1.6 SGD Analysis  
1.7 Curse of Dimensionality  
1.8 Critical Points  
1.9 Difficulties in Neural Network Optimization  

2 FIRST ORDER METHODS  
2.1 Momentum  
2.3 Adaptive Learning Rates  
2.4 Adagrad  
2.5 RMSProp  
2.6 Adam  

3.1 Preliminaries and Motivation  
3.2 Quadratic Function Optimization  
3.3 Newtonâ€™s Method  
3.4 Dynamics of Optimization  
3.5 Gauss-Newton and Levenberg-Marquardt Algorithm  
3.6 Conjugate Gradients  
3.7 BFGS  
3.8 L-BFGS  
3.10 Newton-CG Method  
3.11 Lookahead  

4 CONCLUSION