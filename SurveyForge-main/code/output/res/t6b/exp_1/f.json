{
    "survey": "# A Comprehensive Survey on In-Context Learning: Foundations, Mechanisms, and Applications\n\n## 1 Introduction\n\nHere is the corrected subsection with accurate citations:\n\nIn-context learning (ICL) represents a paradigm shift in machine learning, enabling large language models (LLMs) to adapt to new tasks dynamically through inference-time demonstrations without explicit parameter updates. This capability, first prominently observed in models like GPT-3 [1], challenges traditional supervised learning by decoupling task adaptation from weight optimization. The emergence of ICL is rooted in the interplay between transformer architectures and the statistical properties of pretraining data, where latent task structures are implicitly encoded during pretraining and later retrieved through attention mechanisms [2]. \n\nFormally, ICL can be conceptualized as a meta-optimization process where the model infers a task-specific hypothesis \\( h \\) from a prompt \\( P = \\{(x_i, y_i)\\}_{i=1}^k \\) and applies it to a query \\( x_{k+1} \\). This contrasts with fine-tuning, which explicitly updates model parameters \\( \\theta \\) via gradient descent. Theoretical work suggests that transformers approximate Bayesian inference [3], with attention heads implementing gradient descent-like operations on latent task representations [4]. The efficacy of ICL depends critically on demonstration quality, with semantically relevant examples yielding superior performance [5], while noisy or biased demonstrations can degrade model outputs [6].\n\nThe historical evolution of ICL traces back to few-shot learning and meta-learning paradigms, as evidenced by CAVIA's context parameter adaptation [7] and MetaICL's task-agnostic pretraining [8]. However, ICL distinguishes itself through its reliance on emergent properties of scale: larger models exhibit stronger ICL capabilities, particularly in overriding semantic priors when presented with contradictory demonstrations [9]. This scalability is attributed to the transformer's ability to compose primitive operations (e.g., prefix matching via induction heads) into complex reasoning chains [10].\n\nICL's significance lies in its dual advantages of flexibility and efficiency. By reducing dependence on labeled data, it democratizes access to machine learning for low-resource domains [11]. However, challenges persist, including sensitivity to prompt design [12], computational overhead from long contexts [13], and ethical risks from adversarial demonstrations [14]. Recent innovations address these limitations through retrieval-augmented ICL [15] and neuro-symbolic hybrids [16], suggesting a future where ICL integrates with explicit reasoning systems.\n\nTheoretical advances have further illuminated ICL's mechanisms, framing it as implicit structure induction [17] or algorithmic approximation [18]. Empirical studies reveal that ICL performance correlates with pretraining data properties like burstiness and task diversity [19], while scaling laws suggest a phase transition in model capability [20]. These insights collectively position ICL as both a practical tool and a lens for understanding emergent behaviors in foundation models, with open questions remaining about its theoretical limits and biological plausibility [21]. Future research must reconcile ICL's empirical successes with rigorous algorithmic characterizations, particularly in multimodal settings [22], to unlock its full potential as a general-purpose learning paradigm.\n\nThe citations have been verified to align with the content of the referenced papers. No changes were needed as all citations accurately supported the corresponding claims.\n\n## 2 Theoretical Foundations of In-Context Learning\n\n### 2.1 Probabilistic and Bayesian Frameworks for In-Context Learning\n\nThe probabilistic and Bayesian frameworks provide a rigorous mathematical foundation for understanding in-context learning (ICL), elucidating how large language models (LLMs) infer tasks from demonstrations and quantify uncertainty. At its core, ICL can be viewed as an implicit Bayesian inference process, where models aggregate task-specific hypotheses from in-context examples to approximate posterior distributions over latent variables. Recent work [2] formalizes this by showing that pretraining on documents with long-range coherence induces latent task representations, enabling LLMs to infer shared concepts between prompt examples and test queries. This aligns with the observation that transformers implicitly perform Bayesian model averaging (BMA) [23], where attention mechanisms weight demonstrations proportionally to their likelihood under the pretrained prior. Theoretically, the regret bound for such BMA-based ICL scales as \\(\\mathcal{O}(1/T)\\), with \\(T\\) being the number of in-context examples, suggesting efficient task adaptation.\n\nA key insight is the duality between transformer attention and gradient-based optimization. Studies [4] demonstrate that self-attention layers emulate gradient descent steps on a latent loss function, effectively implementing an iterative Bayesian update. This is further supported by empirical evidence showing that late-layer attention heads encode moment matrices and weight vectors analogous to ridge regression solutions [24]. The emergence of such algorithmic behaviors is tied to the pretraining data's distributional properties: burstiness and skewed rank-frequency distributions [19] promote the learning of compositional operations necessary for probabilistic inference. For instance, induction heads\u2014specialized attention mechanisms identified in [25]\u2014enable models to capture n-gram statistics critical for hierarchical Bayesian updates.\n\nLatent variable models offer another perspective, framing ICL as inference over hidden task parameters. Hierarchical Bayesian approaches [26] posit that pretraining on mixtures of latent tasks equips LLMs with meta-priors, allowing them to decompose prompts into context-free and context-sensitive components [27]. This decomposition is operationalized through kernel regression analogies, where attention scores act as similarity kernels between query and demonstration embeddings [18]. Notably, these kernels exhibit nonparametric properties, enabling adaptation to unseen tasks without overfitting. However, limitations arise when demonstrations violate exchangeability assumptions or exhibit label bias, as shown in [28], where domain-label bias degrades performance to random guessing.\n\nThe interplay between task recognition and task learning further refines Bayesian interpretations. While smaller models rely heavily on recognizing pretrained patterns (task recognition), larger models exhibit genuine task learning by updating latent representations [29]. This dichotomy underscores a fundamental trade-off: BMA excels at leveraging existing priors but struggles with out-of-distribution tasks, whereas gradient-descent-like mechanisms enable finer adaptation at the cost of higher sample complexity. Emerging solutions, such as in-context vectors (ICV) [30], attempt to bridge this gap by explicitly encoding task vectors through meta-gradients, offering better control over uncertainty quantification.\n\nFuture directions highlight the need for robust Bayesian calibration. Current models often violate the martingale property [31], leading to inconsistent uncertainty estimates. Advances in neuro-symbolic integration [32] and curriculum learning [21] suggest pathways to more principled inference. Ultimately, unifying probabilistic frameworks with mechanistic insights\u2014such as the role of induction heads [10]\u2014will be critical for developing scalable, interpretable ICL systems.\n\n### 2.2 Mechanistic Interpretability of Transformer Architectures\n\nThe mechanistic interpretability of transformer architectures provides a crucial bridge between the probabilistic foundations of in-context learning (ICL)\u2014discussed in the previous section\u2014and its theoretical limits, which we examine subsequently. This perspective reveals how ICL emerges from the orchestrated interplay of attention heads, feed-forward networks (FFNs), and layer-wise computations, offering concrete explanations for behaviors previously framed in Bayesian terms.\n\nA foundational discovery is the role of *induction heads*\u2014specialized attention mechanisms that operationalize the pattern-matching capabilities hinted at by latent variable models. These heads, identified in [33], implement a two-phase behavior mirroring hierarchical Bayesian updates: they first attend to tokens following a prefix (e.g., \"[34][34]...[34]\") and then copy the subsequent token (\"[34]\"), effectively performing the n-gram statistics aggregation predicted by probabilistic frameworks. Empirical evidence from [10] confirms their centrality, showing that ablation degrades ICL performance by up to 32%, while mechanistic analyses [25] link their emergence to the burstiness properties of pretraining data\u2014directly connecting to the distributional drivers highlighted earlier.\n\nTransformer layers exhibit a stratified division of labor that aligns with theoretical accounts of task decomposition. Early layers preprocess input tokens into task-agnostic representations (consistent with the context-free components posited by Bayesian meta-priors), while middle and later layers specialize in task-specific computations. For instance, [3] demonstrates that later layers approximate least-squares solutions for linear regression, functionally realizing the gradient-descent-as-attention duality proposed in [4]. This stratification is further refined by FFNs, which act as nonlinear selectors for task-relevant features [35], complementing attention's role in implicit optimization.\n\nThe sparsity of critical components reveals how transformers balance efficiency with adaptability\u2014a theme foreshadowed by Bayesian model averaging's \\(\\mathcal{O}(1/T)\\) regret bounds. Studies like [26] show that only ~20% of FFNs and ~70% of attention heads are essential for ICL, suggesting that task decomposition occurs through sparse subnetworks. This modularity enables adaptive reuse of pretrained components, exemplified by \"function vectors\" [36]\u2014latent representations of task logic that can be compositionally combined, echoing the kernel regression analogies discussed earlier.\n\nHowever, tensions persist between mechanistic findings and broader theoretical claims. While induction heads explain pattern recognition (aligning with the task recognition vs. task learning dichotomy noted previously), their sufficiency for genuine task learning remains debated. [26] shows that larger models supplement recognition with FFN-driven feature updates, revealing an architectural trade-off: attention excels at contextual retrieval but relies on FFNs for nonlinear transformations [37]. This duality underscores that ICL emerges from complementary mechanisms rather than monolithic inference\u2014a nuance critical for understanding scalability limits.\n\nFuture research must address three frontiers to unify these perspectives: (1) the *developmental dynamics* of ICL mechanisms, particularly how induction heads form in response to data distributions [20]; (2) the *scalability* of interpretability methods to larger models, where sparse modularity may complicate Bayesian calibration [22]; and (3) the *formal integration* of mechanistic insights with theoretical frameworks, such as how transformer components collectively approximate gradient-based optimization or Bayesian inference [4]. Resolving these will be essential for advancing ICL from empirical phenomenon to rigorously understood computational paradigm\u2014a prerequisite for tackling the fundamental limits explored next.\n\n### 2.3 Theoretical Limits of Generalization and Scalability\n\nHere is the corrected subsection with accurate citations:\n\nThe theoretical limits of in-context learning (ICL) generalization and scalability are foundational to understanding the boundaries of transformer-based models. Recent work has formalized ICL as an implicit optimization process, revealing that transformers approximate gradient descent on in-context examples to infer task parameters. For instance, [3] demonstrates that transformers trained on linear regression tasks achieve near-optimal generalization, matching the performance of least squares estimators. This aligns with findings in [38], where stability conditions derived for transformer architectures govern their generalization bounds, linking excess risk to the algorithm's sensitivity to input perturbations. The interplay between model size and task complexity is further explored in [39], which identifies a divergence in behavior: smaller models prioritize robust feature extraction, while larger models exhibit higher sensitivity to noisy demonstrations due to broader feature coverage.\n\nScalability limits are intricately tied to the transformer's architectural constraints. [40] reveals that only a fraction of attention heads and feed-forward networks are critical for ICL, suggesting diminishing returns with scale. This is corroborated by [41], which observes that ICL capabilities often emerge transiently during training, with larger models favoring in-weights learning asymptotically. The trade-off between context length and computational efficiency is quantified in [42], where optimal performance is achieved through meta-gradient aggregation, reducing the need for extensive context windows.\n\nGeneralization bounds are further refined by Bayesian perspectives. [23] frames ICL as implicit Bayesian model averaging, showing that transformers approximate posterior inference over task hypotheses. This framework yields regret bounds of \\(\\mathcal{O}(1/T)\\) for \\(T\\) in-context examples, with approximation error decaying exponentially with depth. However, [43] highlights a critical limitation: generalization fails catastrophically for out-of-distribution tasks, underscoring the dependence on pretraining data coverage.\n\nEmerging challenges include the tension between compositional generalization and ICL. [44] hypothesizes that ICL induces an implicit bias toward compositional reasoning, yet empirical results show mixed success on tasks like SCAN and COGS. Similarly, [45] establishes that transformers approximate iterative Newton\u2019s method for softmax regression, but their performance degrades under ill-conditioned data, revealing fundamental limits in optimization dynamics.\n\nFuture directions must address the gap between theoretical guarantees and real-world deployment. The interplay of sparsity and modularity, as evidenced by [36], suggests that compact task representations could enhance scalability. Meanwhile, [46] calls for robust evaluation frameworks to mitigate adversarial vulnerabilities. Synthesizing these insights, the field must reconcile the empirical success of ICL with its theoretical constraints, advancing toward architectures that balance efficiency, robustness, and compositional flexibility.\n\n### 2.4 Algorithmic Perspectives on In-Context Learning\n\nThe algorithmic foundations of in-context learning (ICL) bridge transformer-based inference with classical optimization paradigms, extending the theoretical limits discussed in the previous section while laying groundwork for the cognitive efficiency trade-offs explored subsequently. A pivotal discovery shows transformer forward passes implicitly simulate gradient-based optimization steps for linear models, effectively performing ridge regression in-context [38]. This emergent behavior\u2014where attention mechanisms dynamically minimize task-specific losses\u2014reveals ICL as an implicit optimization process. Crucially, self-attention layers approximate gradient descent updates, with each forward pass corresponding to an optimization step [47]. Such findings connect ICL to classical machine learning, demonstrating transformers can implement least squares, Lasso, and even gradient descent on two-layer networks through forward passes alone [18].  \n\nThe meta-learning parallels of ICL further clarify its algorithmic nature. Unlike explicit frameworks like MAML requiring parameter updates, ICL leverages pretraining on diverse tasks to learn a prior over algorithms [7]. This distinction is fundamental: ICL performs *algorithm selection* rather than algorithm learning, dynamically choosing between base algorithms for different inputs without explicit prompting [18]. Attention mechanisms enable this flexibility by reweighting demonstrations based on task relevance\u2014a capability emerging from pretraining on compositionally structured data where transformers decompose complex tasks into reusable subroutines [17].  \n\nCompositional generalization serves as a litmus test for ICL's algorithmic capabilities, linking to the cognitive efficiency challenges addressed later. Unlike standard supervised learning, ICL fosters systematic reasoning by combining learned sub-tasks novelly, as evidenced by structured dataset experiments [48]. This stems from transformers representing latent task structures, enabling generalization beyond pretraining compositions. The relationship is bidirectional: compositional pretraining data enhances ICL, while the ICL paradigm itself provides an inductive bias favoring compositional generalization [44]. This synergy explains why code-pretrained models often excel at ICL, as programming languages inherently emphasize compositional structure [49].  \n\nStatistical efficiency in ICL presents nuanced trade-offs. While transformers achieve near-Bayes optimal performance with minimal demonstrations [50], success hinges on alignment between pretraining and target tasks. A critical task diversity threshold exists: below it, models behave as Bayesian estimators with pretraining priors; above it, they solve novel tasks [51]. This transition underscores the delicate balance between memorization and true algorithmic learning, with information-theoretic analyses showing sample complexity scales with both sequence count and length [52].  \n\nEmerging directions aim to transcend current limitations while preserving ICL's strengths, anticipating the cognitive-architectural synthesis discussed next. Techniques like dynamic in-context learning [53] and feature-adaptive ICL [54] optimize computational efficiency without performance loss. Theoretical advances continue refining ICL's approximation properties, proving transformers achieve minimax optimal estimation risk when properly pretrained [55]. Future work must reconcile ICL's flexibility with traditional algorithms' robustness, potentially through hybrid systems\u2014while developing formal frameworks capturing ICL's unique fusion of algorithmic implementation, statistical efficiency, and compositional reasoning.  \n\n### 2.5 Cognitive and Computational Trade-offs\n\nHere is the corrected subsection with accurate citations based on the provided papers:\n\nThe interplay between cognitive principles and computational efficiency in in-context learning (ICL) reveals fundamental parallels between human learning and transformer-based adaptation. At its core, ICL mirrors human cognitive processes such as rapid task acquisition and flexible memory retrieval, while simultaneously navigating computational constraints inherent to neural architectures. This subsection examines how transformer models balance these trade-offs, drawing insights from cognitive science to explain emergent behaviors in ICL systems.  \n\nA key cognitive phenomenon replicated in ICL is curriculum learning, where the order and complexity of demonstrations influence model performance. Studies show that transformers exhibit human-like sensitivity to task structure, with interleaved examples improving generalization over blocked sequences [25]. This aligns with cognitive theories of spaced learning, where diverse contexts enhance memory consolidation. The emergence of \"induction heads\" in transformers\u2014specialized attention mechanisms that implement pattern completion\u2014further underscores this parallel [33]. These heads develop abruptly during training, akin to human skill acquisition phases, suggesting a shared computational bottleneck in hierarchical feature extraction.  \n\nThe duality of memory retrieval and online learning in ICL presents another critical trade-off. Transformers balance pretrained knowledge with context-derived updates, analogous to human working memory systems. Empirical work demonstrates that late-layer attention heads selectively retrieve task-relevant pretrained features, while early layers process in-context demonstrations [36]. This separation mirrors cognitive models where long-term memory biases perceptual processing. However, computational costs arise: excessive reliance on pretrained knowledge limits adaptability, while overfitting to context strains memory capacity. The \"memory-retrieval duality\" framework explains this balance, showing that optimal ICL occurs when models dynamically weight context against pretrained priors [23].  \n\nEnergy-based interpretations offer a unifying perspective, framing ICL as gradient-based optimization within a dynamically modulated energy landscape. Here, prompts act as constraints that reshape the model's prediction space, similar to how human attention filters sensory input [4]. Formally, this can be modeled as:  \n\n$$\nE(\\mathbf{y}|\\mathbf{x}, \\mathcal{D}) = -\\log p(\\mathbf{y}|\\mathbf{x}) + \\lambda \\cdot \\text{sim}(\\mathcal{D}, (\\mathbf{x}, \\mathbf{y}))\n$$\n\nwhere $\\mathcal{D}$ represents in-context examples, and $\\lambda$ controls the trade-off between pretraining and contextual adaptation. This formulation reveals that transformers approximate Bayesian inference by implicitly computing task-specific energy minima [2].  \n\nChallenges persist in aligning computational efficiency with cognitive plausibility. For instance, while humans excel at compositional generalization\u2014combining learned primitives into novel solutions\u2014transformers require carefully curated demonstrations to achieve similar feats [56]. Recent advances in neuro-symbolic hybrids and retrieval-augmented architectures suggest promising directions to bridge this gap.  \n\nFuture research should explore how cognitive biases (e.g., recency effects) manifest in ICL and whether explicit architectural constraints can improve sample efficiency. The developmental trajectory of ICL capabilities, particularly in smaller models, remains underexplored but could yield insights into minimal sufficient conditions for emergent learning [20]. By grounding computational models in cognitive theory, the field can advance toward more robust and human-like in-context learners.\n\nChanges made:\n1. Removed the citation \"[57]\" as it was not in the provided list of papers.\n2. Kept all other citations as they were correctly supported by the referenced papers.\n\n## 3 Methodologies and Architectures for In-Context Learning\n\n### 3.1 Prompt Engineering Strategies\n\nHere is the corrected subsection with accurate citations:\n\nPrompt engineering has emerged as a critical methodology for optimizing in-context learning (ICL) performance, leveraging both discrete and continuous approaches to shape model behavior without weight updates. Discrete prompt design, the most widely studied paradigm, involves crafting task-specific textual templates or demonstrations to guide model predictions. Recent work [1] demonstrates that semantically similar demonstrations retrieved from external corpora significantly outperform random sampling, with gains of up to 45.5% on question-answering tasks. However, discrete prompts exhibit sensitivity to ordering effects [58], where permuting examples can alter performance by up to 16.3%, highlighting the need for systematic optimization.  \n\nContinuous prompt tuning represents a complementary approach, where learnable soft embeddings replace discrete tokens. These embeddings, optimized through gradient descent, adapt model behavior while preserving interpretability. Hybrid methods [8] combine both paradigms, using discrete templates for task framing and continuous embeddings for fine-grained adaptation. Theoretically, continuous prompts can be formalized as latent task vectors [59], where a single vector $\\theta(S) \\in \\mathbb{R}^d$ encapsulates the demonstration set $S$, modulating transformer activations via:  \n\n$$\nh_{\\text{out}} = \\text{Transformer}(x, \\theta(S))\n$$\n\nwhere $h_{\\text{out}}$ is the final prediction for query $x$. This formulation reveals that continuous prompts implicitly implement gradient-based meta-learning [4], with attention mechanisms approximating gradient updates on demonstration examples.  \n\nComparative analysis reveals trade-offs between these approaches. Discrete methods offer interpretability but require manual curation, while continuous techniques automate optimization at the cost of transparency. Hybrid strategies [5] address this by using determinantal point processes to select diverse discrete examples while tuning continuous embeddings, achieving state-of-the-art performance across 12 benchmarks. Emerging trends emphasize dynamic prompt construction, such as retrieval-augmented ICL [15], where nearest-neighbor search over task-specific corpora selects contextually relevant examples.  \n\nKey challenges persist in prompt engineering\u2019s scalability and robustness. Label bias [28] can distort predictions when demonstrations exhibit skewed distributions, necessitating calibration techniques like domain-context calibration, which improves F1 scores by up to 37%. Additionally, the computational overhead of processing long prompts [13] motivates research into compression methods, such as attention-based pruning. Future directions include neurosymbolic prompt design, where symbolic rules guide continuous embedding generation, and multimodal prompt engineering [22], extending ICL to vision-language tasks.  \n\nThe evolution of prompt engineering reflects a broader shift from heuristic design to principled optimization. By unifying discrete and continuous paradigms through theoretical frameworks like Bayesian model averaging [23], the field is advancing toward robust, scalable ICL systems. However, fundamental questions remain about the interplay between prompt design and model architecture, particularly how induction heads [10] mediate prompt effectiveness across tasks. Addressing these questions will require tighter integration between empirical analysis and mechanistic interpretability.\n\n### 3.2 Retrieval-Augmented In-Context Learning\n\nRetrieval-augmented in-context learning (RA-ICL) has emerged as a powerful extension of conventional ICL, addressing two key limitations: the reliance on static demonstrations and the risk of bias amplification from uncurated examples. Building on the prompt engineering foundations discussed earlier\u2014where discrete and continuous approaches optimize task adaptation\u2014RA-ICL introduces dynamic retrieval mechanisms to enhance generalization while mitigating spurious correlations. This paradigm aligns with the broader shift toward hybrid architectures (explored in the subsequent section) by integrating external knowledge sources with transformer-based inference.  \n\n**Dynamic Demonstration Retrieval**  \nAt the core of RA-ICL are retrieval mechanisms that select contextually relevant demonstrations. Methods like [60] leverage information-theoretic criteria to maximize mutual information between demonstrations and target tasks. While traditional approaches use BM25 or dense retrievers (e.g., SBERT) for lexical/semantic matching [61], recent work emphasizes compositional diversity. For instance, [5] employs Determinantal Point Processes (DPPs) to balance similarity and coverage, while [58] adopts influence functions to identify demonstrations that maximally shift model predictions\u2014reducing sensitivity to noise. These advances complement the hybrid prompt engineering strategies discussed earlier, where discrete-continuous synergies improve robustness.  \n\n**Knowledge-Enhanced Retrieval**  \nRA-ICL further augments demonstrations with structured knowledge, bridging the gap between implicit in-context learning and explicit knowledge grounding. [62] frames retrieval as latent variable inference, incorporating task-specific priors from knowledge bases (e.g., Wikidata). This aligns with findings in [63], where multimodal retrievers improve robustness by grounding text in visual context. However, scalability remains a challenge: retrieval latency grows with corpus size, and rigid knowledge integration risks overfitting, as noted in [64]\u2014a limitation later addressed by hybrid retrieval-compression architectures.  \n\n**Bias Mitigation Strategies**  \nRA-ICL inherently diversifies demonstrations to reduce bias, but explicit techniques further enhance fairness. [58] shows that influence-based retrieval minimizes reliance on spurious features (e.g., lexical overlap in NLI tasks), while [65] disrupts biased attention patterns through parameter noise. These methods resonate with the calibration techniques discussed in prompt engineering, such as domain-context calibration. Theoretically, [37] reveals that optimal retrieval aligns with gradient-based feature reweighting\u2014echoing the implicit meta-learning dynamics observed in continuous prompt tuning.  \n\n**Challenges and Future Directions**  \nRA-ICL faces unresolved tensions between retrieval quality, efficiency, and trustworthiness. As [19] notes, performance depends heavily on pretraining corpus properties (e.g., burstiness, diversity), while [31] questions whether retrieval enables true Bayesian inference or merely reinforces pretraining biases. Future directions could integrate RA-ICL with the hybrid architectures explored next\u2014such as neurosymbolic retrieval [66] or modular designs like [64]\u2014to optimize the triad of efficiency, relevance, and bias control.  \n\nIn summary, RA-ICL advances ICL by unifying dynamic context selection with external knowledge, building on prompt engineering principles while paving the way for hybrid systems. Its evolution mirrors the field\u2019s broader trajectory: from heuristic designs to theoretically grounded, scalable solutions.\n\n### 3.3 Hybrid Learning Architectures\n\nHybrid learning architectures for in-context learning (ICL) combine the strengths of multiple paradigms\u2014such as meta-learning, fine-tuning, and retrieval augmentation\u2014to address the limitations of pure ICL approaches. These architectures aim to enhance adaptability, robustness, and efficiency by integrating explicit learning mechanisms with the implicit inference capabilities of transformers. A key insight from recent work is that ICL alone often struggles with task recognition versus task learning, as highlighted by [29], where models may rely on pre-trained priors rather than genuinely learning from demonstrations. Hybrid architectures mitigate this by embedding structured learning signals into the ICL pipeline.\n\nOne prominent direction involves combining meta-learning with ICL, where models are pre-trained on diverse tasks to acquire generalizable adaptation strategies. For instance, [38] demonstrates that transformers can implicitly implement gradient-based optimization during inference, akin to meta-learning algorithms. This aligns with findings in [45], which show that transformers approximate higher-order methods like Newton\u2019s iteration, enabling faster convergence than gradient descent. Such hybrid systems leverage the transformer\u2019s attention mechanism to dynamically adjust weights based on in-context examples, bridging the gap between implicit and explicit learning.\n\nAnother approach integrates retrieval-augmented methods with ICL to enhance context relevance and reduce bias. [64] introduces learnable in-context vectors (ICVs) to compress retrieved examples into compact representations. These hybrid systems address the computational inefficiencies of pure retrieval-based ICL, as noted in [37], where softmax attention\u2019s quadratic cost becomes prohibitive for long contexts. By combining retrieval with learned compression, hybrid architectures achieve a balance between performance and scalability.\n\nA third trend involves neuro-symbolic integration, where symbolic reasoning modules guide ICL. [20] reveals that transformers progress through discrete phases of learning, suggesting that explicit symbolic constraints could stabilize emergent ICL abilities. For example, [44] shows that forcing models to in-context learn improves compositional generalization, a capability further enhanced by symbolic priors. This aligns with [36], where task-specific \"function vectors\" act as symbolic anchors to steer attention. Such hybrids mitigate hallucinations and improve interpretability, though they require careful design to avoid over-constraining the model\u2019s flexibility.\n\nChallenges persist in balancing the trade-offs inherent to hybrid architectures. For instance, [41] observes that ICL capabilities can diminish during training, suggesting that hybrid systems must dynamically adjust their reliance on in-context versus in-weights learning. Additionally, [43] highlights that hybrid performance depends heavily on the coverage of pre-training tasks, raising questions about generalization to out-of-distribution scenarios. Future directions may explore modular designs, such as [67], where specialized sub-networks handle distinct aspects of learning, or [68], which optimizes memory usage for hybrid inference.\n\nIn summary, hybrid learning architectures represent a promising frontier for ICL, combining the scalability of implicit learning with the precision of explicit mechanisms. By addressing limitations in task adaptation, efficiency, and generalization, these systems pave the way for more robust and interpretable models. Future research should focus on unifying theoretical insights\u2014such as the Bayesian foundations in [17]\u2014with practical innovations in architecture design.\n\n### 3.4 Contextual Representation Learning\n\nThe internal mechanisms by which models process and represent in-context information constitute a fundamental aspect of in-context learning (ICL), bridging the hybrid architectures discussed earlier with the efficiency optimizations explored in subsequent sections. This subsection examines how transformer architectures encode and manipulate contextual information, focusing on three key dimensions that underpin both representational richness and computational feasibility: attention mechanisms, latent space dynamics, and cross-modal integration.  \n\n**Attention as Implicit Computation**  \nThe self-attention mechanism serves as the primary engine for contextual representation, dynamically constructing task-specific patterns from demonstrations. As shown in [38], attention heads implement gradient descent-like operations during forward passes, with specialized circuits (e.g., \"induction heads\" [40]) hierarchically processing context\u2014early layers recognize patterns while deeper layers execute task-specific computations. This aligns with hybrid architectures' use of implicit optimization, while also foreshadowing efficiency challenges: empirical studies reveal sparse, modular processing, with only 20% of feed-forward networks driving ICL performance [40], suggesting opportunities for selective context processing.  \n\n**Latent Space as Hypothesis Manifold**  \nTheoretical frameworks [23] position ICL as implicit Bayesian model averaging, where latent spaces parameterize hypothesis distributions conditioned on demonstrations. This perspective connects to hybrid systems' neurosymbolic integration, as models weight demonstrations probabilistically\u2014though limitations emerge when tasks deviate from pretraining distributions (\"mapping deficiency\" [69]). Practical techniques like in-context vectors operationalize this by steering behavior without prompt modifications, anticipating later discussions of context compression.  \n\n**Cross-Modal Alignment Challenges**  \nFor multimodal contexts (e.g., images, tables), representation learning must align heterogeneous inputs while preserving task features\u2014a challenge magnified in distributed architectures. Benchmarks [22] reveal current models struggle with fine-grained multimodal reasoning, despite contrastive alignment methods [70] projecting inputs into shared latent spaces. These limitations mirror the quadratic memory scaling noted in efficiency analyses, underscoring the need for scalable attention variants.  \n\n**Emerging Tensions and Scaling Frontiers**  \nFeature-adaptive methods [54] and minimax optimality results [55] highlight the trade-off between representational flexibility (e.g., handling irregular decision boundaries [71]) and computational tractability. Critically, pretraining task diversity [43] determines whether models merely memorize or truly learn from context\u2014a theme revisited in theoretical frameworks. Future work must unify Bayesian insights [72] with architectural innovations to advance both representation quality and efficiency, setting the stage for subsequent discussions on optimization limits and hardware co-design.  \n\n### 3.5 Efficiency Optimization Techniques\n\nHere is the corrected subsection with accurate citations:\n\n[73]\n\nThe computational demands of in-context learning (ICL) grow substantially with increasing context lengths, posing significant challenges for real-world deployment. This subsection examines three key strategies for optimizing ICL efficiency: context window compression, selective context processing, and distributed architectures, each addressing distinct bottlenecks in memory, computation, and scalability.\n\n**Context Window Compression** techniques mitigate the quadratic memory overhead of transformer attention mechanisms. Recent work demonstrates that models can achieve comparable performance with compressed context representations, where attention layers are modified to operate on low-rank approximations of the full context matrix [3]. Theoretical analyses reveal that such compression preserves the implicit gradient descent dynamics underlying ICL [4], though with diminishing returns beyond certain compression thresholds. Hybrid approaches combining token pruning with learned compression matrices show particular promise, reducing memory usage by 40-60% while maintaining 90%+ task accuracy [18].\n\n**Selective Context Processing** introduces dynamic mechanisms to prioritize relevant context segments. The self-adaptive framework proposed in [60] employs influence-based scoring to identify and weight critical demonstrations, reducing unnecessary computations on irrelevant context. This aligns with findings that only 20-30% of attention heads significantly contribute to ICL performance [40]. Further optimization is achieved through layer-wise gating, where early transformer layers perform coarse relevance filtering before deeper layers process the refined context subset [37].\n\n**Distributed ICL Architectures** parallelize context processing across multiple model instances or specialized hardware units. The batch-ICL approach [42] decomposes N-shot learning into N parallel 1-shot computations with aggregated meta-gradients, achieving order-agnostic processing with sublinear scaling in context length. For extremely long contexts (>10k tokens), retrieval-augmented systems like [74] demonstrate how task-specific submodules can process segmented context windows independently before fusion. However, these methods introduce new challenges in maintaining coherence across distributed computations, particularly for compositional tasks requiring cross-context reasoning [75].\n\nCritical trade-offs emerge across these approaches. Compression techniques often sacrifice fine-grained task adaptation for memory efficiency, while selective processing risks losing weakly correlated but semantically important context. Distributed methods excel in throughput but struggle with latency-sensitive applications. Emerging solutions like the locally-calibrated PFN [74] hybridize these strategies, using retrieval to identify context subsets for focused compression and processing.\n\nFuture directions must address three unresolved challenges: (1) theoretical limits on context compression without task performance degradation, as highlighted by the irregular decision boundaries in [76]; (2) dynamic adaptation of efficiency strategies to task complexity, building on the task-recognition vs. task-learning framework in [29]; and (3) hardware-algorithm co-design, particularly for energy-constrained edge deployments. The integration of neurosymbolic methods may yield new compression paradigms by separating symbolic task representations from continuous context processing. As context windows continue expanding\u2014with models handling 1M+ tokens\u2014these optimizations will determine the practical viability of ICL across diverse applications.\n\n### 3.6 Theoretical Frameworks for ICL Architectures\n\nBuilding upon the computational optimization strategies discussed in the previous section, we now examine the theoretical foundations that explain why transformer-based models exhibit emergent in-context learning (ICL) capabilities without parameter updates. These frameworks reveal how architectural components and pretraining dynamics enable few-shot adaptation through implicit optimization and Bayesian inference mechanisms.\n\nA fundamental insight emerges from interpreting ICL as latent concept induction, where [2] demonstrates that transformers infer shared document-level structures during pretraining, facilitating task recognition from demonstrations. This process critically depends on specialized attention mechanisms called induction heads [33], which implement pattern completion to dynamically map input-output relationships\u2014functioning analogously to hierarchical Bayesian inference over latent task variables.\n\nThe optimization perspective provides a complementary view, with [4] showing that transformer attention layers compute updates resembling meta-gradients. These layers form linear approximations of loss landscapes, where demonstrations guide predictions through gradient descent-like dynamics. While this implicit optimization improves with model depth, it remains imperfect due to pretrained priors that introduce biases, particularly when demonstrations contradict semantic expectations [9]. Notably, larger models demonstrate greater capacity to override these priors, suggesting scale enhances optimization flexibility.\n\nThe theoretical framework further distinguishes between two core mechanisms: task recognition and task learning [29]. While smaller models primarily rely on recognizing pretrained patterns, larger models increasingly adapt to novel input-label mappings\u2014a dichotomy formalized by the \"superficial alignment hypothesis\" [77]. This behavioral shift underscores how model scale affects the balance between leveraging existing knowledge and learning new tasks.\n\nArchitectural constraints significantly influence ICL efficacy, as shown by theoretical bounds linking generalization stability to consistent hypothesis spaces across tasks [38]. Modular components like sparse attention heads and task-specific feed-forward networks [78] enhance stability by decomposing complex tasks into reusable subroutines. However, these mechanisms remain sensitive to prompt design quality, where noisy or imbalanced demonstrations can destabilize predictions [79].\n\nLooking ahead, unifying these frameworks presents key opportunities to advance ICL theory. Promising directions include neurosymbolic integration [80] and energy-based formulations [25], which could bridge implicit optimization with explicit reasoning. Additionally, developing scaling laws for ICL-specific components\u2014such as induction heads or memory-augmented layers [81]\u2014remains crucial for understanding the trade-offs between computational efficiency, robustness, and interpretability as architectures evolve. These theoretical advances will inform the practical challenges discussed in subsequent sections while maintaining grounding in both empirical performance and mechanistic clarity.\n\n## 4 Empirical Analysis and Performance Factors\n\n### 4.1 Demonstration Selection and Quality\n\nThe efficacy of in-context learning (ICL) hinges critically on the selection and quality of demonstrations provided in the prompt. Empirical studies reveal that performance variance can span from near-random to state-of-the-art depending on these factors [1]. This subsection dissects the interplay between demonstration characteristics and model behavior, focusing on three dimensions: diversity, noise sensitivity, and retrieval-augmented optimization.  \n\n**Diversity and Relevance.** The discriminative power of ICL is heavily influenced by the diversity and representativeness of in-context examples. [1] demonstrates that semantically similar examples retrieved from task-specific corpora outperform random sampling by up to 45.5% on question answering, suggesting that relevance to the query is paramount. However, diversity must be balanced; overly homogeneous examples may fail to capture task nuances, while excessive heterogeneity can dilute task-specific signals. [5] formalizes this trade-off using determinantal point processes (DPPs) to optimize subset selection, achieving superior performance by modeling interactions between input queries and demonstrations. Theoretically, this aligns with Bayesian model averaging frameworks, where diverse demonstrations approximate a richer posterior over latent tasks [2].  \n\n**Noise and Label Bias.** ICL exhibits surprising robustness to label noise, yet its sensitivity to systematic biases poses challenges. [6] shows that transformers maintain stable performance under diverse noise types (e.g., random flipping), but [82] reveals that semantically misaligned labels (e.g., \"foo/bar\" substitutes) degrade performance, particularly in smaller models. This dichotomy suggests that noise resilience depends on whether the perturbation disrupts the underlying task structure. Label bias amplification is another critical issue; [28] identifies domain-label bias as a dominant factor, where models overfit to spurious label distributions in demonstrations. Their proposed domain-context calibration mitigates this by estimating bias using in-domain words, improving GPT-3\u2019s F1 by up to 37%.  \n\n**Retrieval-Augmented Strategies.** Dynamic retrieval of demonstrations addresses limitations of static prompt design. [1] pioneers this approach, using fine-tuned retrievers to fetch task-relevant examples, while [83] scales retrieval to thousands of examples via k-nearest neighbors, achieving continuous performance gains without context window constraints. However, retrieval efficiency remains a bottleneck; [84] demonstrates that curated subsets of 20-50 high-quality examples often match or exceed the performance of larger retrieved sets, suggesting diminishing returns with scale.  \n\nEmerging trends highlight the role of meta-learning in demonstration selection. [8] shows that pretraining on diverse tasks enhances a model\u2019s ability to identify useful demonstrations, while [15] synthesizes evidence that hybrid retrieval-finetuning systems outperform pure ICL in low-resource settings. Future directions include optimizing demonstration ordering via curriculum learning [85] and leveraging latent space manipulations to reduce sensitivity to prompt design [30].  \n\nIn synthesis, demonstration selection and quality govern ICL\u2019s practical viability. While retrieval and noise-robustness advancements have expanded its applicability, fundamental tensions persist between diversity, bias mitigation, and computational efficiency. A unified theoretical framework\u2014combining Bayesian inference, meta-learning, and representation dynamics\u2014is needed to navigate these trade-offs systematically.\n\n### 4.2 Context Length and Task Complexity\n\nThe relationship between context length and task complexity in in-context learning (ICL) reveals fundamental trade-offs in model performance, scalability, and computational efficiency, building on the demonstration selection challenges discussed in the previous section. Empirical studies demonstrate that while increasing context length generally improves task adaptation, its efficacy is contingent on task complexity and model architecture. For instance, [3] shows that transformers can achieve near-optimal performance on linear regression tasks with minimal in-context examples, but this scalability diminishes for non-linear tasks like decision trees or neural networks, where longer contexts are required to capture intricate patterns. Theoretical work in [18] corroborates this, proving that transformers implicitly implement ridge regression for linear tasks, with performance bounds dependent on both context length and task dimensionality.  \n\nThe interplay between context length and task complexity is further complicated by the emergence of \"induction heads,\" specialized attention mechanisms identified in [33]. These heads enable models to generalize from few-shot demonstrations by recognizing and replicating patterns, but their effectiveness plateaus for tasks requiring compositional reasoning or hierarchical dependencies\u2014a limitation that foreshadows the robustness challenges discussed in the subsequent section. For example, [20] observes that transformers exhibit discrete developmental stages in ICL, where increasing context length only benefits performance up to a task-specific threshold. Beyond this threshold, additional examples introduce noise, degrading accuracy\u2014a phenomenon particularly pronounced in low-resource or high-variance tasks [51].  \n\nScalability challenges arise when context windows expand to accommodate complex tasks, mirroring the efficiency trade-offs highlighted in retrieval-augmented strategies from earlier sections. [86] reveals that while models like GPT-4 can leverage thousands of demonstrations, their performance gains are often marginal compared to retrieval-augmented methods, suggesting diminishing returns. This aligns with findings in [13], where reinforcement-based ICL outperforms traditional few-shot approaches for reasoning tasks but requires carefully curated demonstrations to avoid overfitting\u2014a theme further explored in the robustness subsection. The computational overhead of long contexts also poses practical limitations; [26] demonstrates that only 20% of feed-forward networks in OPT-66B are critical for ICL, implying that inefficient context processing exacerbates resource constraints.  \n\nTheoretical limits further illuminate these trade-offs. [38] formalizes ICL as an implicit optimization process, showing that stability\u2014measured by the Lipschitz constant of the task function\u2014dictates the required context length. For high-Lipschitz tasks (e.g., natural language inference), longer contexts are necessary to stabilize predictions, whereas low-Lipschitz tasks (e.g., linear classification) benefit from shorter, more focused demonstrations. This duality is empirically validated in [37], where softmax attention mechanisms approximate gradient descent, with convergence rates dependent on task complexity and context diversity.  \n\nEmerging trends highlight adaptive strategies to mitigate these challenges, bridging the gap between current limitations and future robustness solutions. [60] proposes dynamic demonstration selection and ordering, optimizing context utility without expanding window size. Similarly, [30] introduces latent space manipulations to compress contextual information, achieving comparable performance with 50% fewer tokens. Future directions could explore hybrid architectures combining retrieval-augmented ICL [61] with meta-learning, as suggested by [26], to balance context efficiency and task adaptability\u2014an approach that aligns with the adversarial defense mechanisms discussed in the following section. Ultimately, advancing ICL requires not only larger models but also smarter context utilization, as underscored by the non-monotonic relationship between context length and performance across diverse benchmarks.  \n\n### 4.3 Robustness and Adversarial Challenges\n\nThe robustness of in-context learning (ICL) against distribution shifts and adversarial perturbations remains a critical yet underexplored frontier. Empirical studies reveal that while large language models (LLMs) exhibit remarkable adaptability to novel tasks through ICL, their performance is highly sensitive to both input distributional changes and maliciously crafted prompts. For instance, [3] demonstrates that transformers can generalize to unseen linear functions under distribution shifts, but this resilience diminishes for complex tasks like sparse linear regression or decision trees, where adversarial perturbations to demonstrations degrade accuracy by up to 32% [6]. This vulnerability stems from the model\u2019s reliance on surface-level statistical patterns in demonstrations, which adversarial attacks exploit by injecting misleading examples or perturbing key tokens [44].  \n\nA notable threat is data poisoning, where adversaries manipulate demonstrations to induce incorrect predictions. Frameworks like ICLPoison illustrate how inserting even a single corrupted example can skew model outputs, particularly in low-resource settings. Countermeasures such as Linear Probe Calibration [23] mitigate this by recalibrating attention weights, but their efficacy diminishes with increasing model scale, as larger models exhibit heightened sensitivity to noisy contexts [39]. This trade-off between scalability and robustness underscores the need for architecture-level innovations, such as the LCT block [87], which normalizes context features to reduce interference from irrelevant channels.  \n\nEthical risks further complicate ICL\u2019s robustness. Models trained on imbalanced data amplify biases present in demonstrations, as shown in [88], where LLMs preferentially rely on sentiment over lexical features despite explicit counterexamples. This bias propagation is exacerbated in multimodal ICL, where misaligned image-text pairs in prompts lead to inconsistent outputs [22]. Recent work proposes retrieval-augmented ICL to counterbalance biases, yet challenges persist in ensuring fairness across diverse domains.  \n\nEmerging solutions focus on mechanistic interpretability and modular design. For example, [36] identifies task-specific \"function vectors\" that, when ablated, reduce adversarial vulnerability by 21%. Similarly, [10] reveals that disabling induction heads\u2014critical for copying and pasting patterns\u2014renders models nearly random in abstract tasks, suggesting targeted interventions to harden ICL. Future directions include integrating neuro-symbolic methods to enforce logical consistency and hybrid architectures like Batch-ICL [42], which aggregates meta-gradients across demonstrations to resist order-based attacks.  \n\nThe interplay between robustness and efficiency remains unresolved. While [37] theoretically links ICL to implicit gradient descent, practical deployments require lightweight defenses like [89], which improves stability via low-rank approximations. As ICL transitions to real-world applications, a unified framework for adversarial evaluation\u2014spanning data, model, and prompt layers\u2014will be essential to harness its potential while mitigating risks.\n\n### 4.4 Evaluation Metrics and Methodologies\n\nThe evaluation of in-context learning (ICL) necessitates a nuanced understanding of both task-specific performance and broader generalization capabilities, building on the robustness challenges outlined in previous discussions of distribution shifts and adversarial vulnerabilities. Recent work has established standardized protocols, such as the Dolce framework, to disentangle retrieval-based performance from holistic understanding in ICL systems [22]. This distinction is critical, as it reveals whether models genuinely infer task structures or merely memorize contextual patterns\u2014a tension further explored in subsequent sections on emerging paradigms. Information-theoretic analyses decompose errors into pretraining dynamics and in-context generalization components, demonstrating that transformers approximate Bayesian model averaging when aggregating hypotheses from demonstrations [23]. Such formalizations quantify how pretraining data diversity influences ICL\u2019s emergent properties, linking latent document-level coherence to generalization bounds\u2014a theme echoed in later discussions of scalability and fairness gaps.\n\nRobustness metrics for ICL extend beyond accuracy to include sensitivity to distribution shifts and adversarial perturbations, bridging the gap between theoretical formalizations and practical deployment challenges. Studies reveal that while transformers exhibit strong performance on in-distribution tasks, their calibration degrades under label noise or domain shifts [90]. Linear Probe Calibration (LPC) has emerged as a mitigation strategy, aligning model confidence with empirical accuracy through post-hoc adjustments. However, this approach assumes access to validation data, which may not align with zero-shot ICL scenarios\u2014a limitation foreshadowed by earlier discussions of data poisoning risks. Alternative methods, such as self-ensembling via multiple prompt variations, improve calibration without additional supervision [42], though they introduce computational overhead that becomes particularly salient in retrieval-augmented ICL systems, where dynamic demonstration selection amplifies performance but exacerbates vulnerability to adversarial manipulation [91].\n\nTask-specific evaluation reveals divergent behaviors across modalities, reflecting the interplay between robustness and adaptability highlighted in prior sections. In NLP, compositional generalization is measured through structured datasets like COGS and GeoQuery, where models must combine learned primitives into novel expressions [44]. Here, ICL\u2019s success hinges on demonstration diversity and structural similarity to test cases, as shown by the CoFe benchmark\u2014a finding that aligns with subsequent analyses of meta-learning efficiency. For multimodal tasks, metrics account for cross-modal alignment, such as in Visual Question Answering (VQA), where models are evaluated on both accuracy and grounding fidelity [92]. The NICE metric further quantifies the diminishing returns of optimizing in-context examples (ICE) when detailed instructions are provided, offering a heuristic for resource allocation in prompt engineering [93].\n\nComparative analyses between ICL and supervised learning highlight fundamental differences in error profiles, contextualizing earlier observations about ICL\u2019s brittleness under task violations. While supervised models exhibit gradual performance degradation under label perturbations, ICL shows abrupt failures when demonstrations violate task assumptions [94]. This brittleness stems from ICL\u2019s reliance on implicit task recognition rather than explicit parameter updates\u2014a mechanistic limitation further explored in subsequent discussions of hybrid neuro-symbolic approaches. Theoretical bounds on generalization, derived from online learning frameworks, formalize this observation by decomposing regret into approximation and meta-learning errors [52]. These bounds suggest that ICL\u2019s sample efficiency is contingent on pretraining task diversity, with sub-linear regret achievable only when the prompt space sufficiently covers the hypothesis class\u2014a prerequisite that motivates later proposals for unified evaluation frameworks.\n\nEmerging methodologies address scalability and fairness gaps in ICL evaluation, setting the stage for future directions discussed in subsequent sections. Feature-adaptive approaches like FADS-ICL refine task-specific representations using beyond-context samples, improving generalization without expanding the prompt length [54]. For low-resource languages, cross-lingual transfer techniques leverage multilingual retrievers to extend ICL capabilities, though performance remains sensitive to script and syntactic divergence [69]. Ethical frameworks increasingly integrate bias amplification metrics into evaluation, advocating for balanced retrieval systems [95]\u2014a concern that resonates with earlier discussions of adversarial risks. The integration of neuro-symbolic methods, which combine neural metrics with symbolic reasoning, presents a promising avenue for enhancing both performance and transparency [57], bridging the gap between empirical evaluation and the theoretical foundations explored throughout this survey.\n\n### 4.5 Emerging Trends and Open Challenges\n\nHere is the corrected subsection with accurate citations:\n\nThe empirical analysis of in-context learning (ICL) has revealed both its remarkable adaptability and persistent limitations, driving research toward novel paradigms and unresolved challenges. A key emerging trend is the integration of reinforcement learning (RL) with ICL for dynamic adaptation, where models like [96] demonstrate how task-conditioned exploration and inference can enhance sample efficiency in meta-RL settings. This approach addresses the meta-overfitting problem by decomposing tasks into subtasks, though it introduces computational overhead that remains a trade-off. Similarly, [97] proposes feed-forward architectures for efficient task adaptation, but their scalability to complex NLP tasks is yet to be validated.  \n\nAnother frontier is the development of neuro-symbolic hybrids to improve interpretability. While [36] identifies compact, causal representations (e.g., \"function vectors\") that trigger task execution, their compositional generalization is limited to simple operations. Complementary work by [37] formalizes ICL as implicit optimization, showing that transformers approximate gradient descent for softmax regression. However, these analyses assume linear tasks, leaving nonlinear and multimodal extensions open. The interplay between symbolic reasoning and neural mechanisms, as explored in [75], suggests that middle layers specialize in latent task representations, but their robustness to distribution shifts requires further empirical validation.  \n\nScalability and fairness gaps persist as critical challenges. Studies like [40] reveal that only 20% of feed-forward networks are essential for ICL, implying inefficiencies in pretraining. This aligns with findings in [51], where task diversity thresholds determine whether models adopt Bayesian or ridge regression behaviors. For low-resource languages, [8] shows that cross-domain meta-training improves adaptation, yet performance degrades with domain shifts, highlighting the need for better transfer protocols. Ethical risks, such as bias amplification in demonstration selection [58], further complicate deployment, as models exhibit sensitivity to label imbalances and adversarial perturbations [14].  \n\nTheoretical and methodological gaps also demand attention. While [18] proves transformers can implement near-optimal algorithms like ridge regression, their analysis assumes synthetic data distributions. Empirical studies on real-world tasks, such as [56], reveal that ICL performance hinges on example diversity and structural similarity\u2014a finding corroborated by [48]. However, standardized benchmarks for evaluating robustness, as proposed in [98], are still nascent.  \n\nFuture directions should prioritize three areas: (1) unifying theoretical frameworks, such as the PAC-based analysis in [26], with empirical studies of transformer architectures; (2) advancing efficient ICL through methods like [42], which aggregates meta-gradients to reduce computational costs; and (3) addressing ethical risks via calibration techniques like [99], which mitigates miscalibration but requires labeled data. The interplay between data distributional properties [19] and model architectures remains underexplored, particularly for multimodal ICL [22]. Bridging these gaps will require interdisciplinary collaboration, leveraging insights from cognitive science, optimization theory, and fairness-aware machine learning.\n\n## 5 Applications of In-Context Learning\n\n### 5.1 Natural Language Processing Applications\n\nHere is the subsection with corrected citations:\n\nIn-context learning (ICL) has emerged as a transformative paradigm for natural language processing (NLP), enabling large language models (LLMs) to adapt to diverse linguistic tasks without explicit fine-tuning. By conditioning on a few input-output demonstrations, LLMs exhibit remarkable few-shot generalization across text classification, question answering, and semantic parsing tasks [1]. This capability stems from the implicit Bayesian inference mechanisms identified by [2], where pretrained models infer latent task structures from contextual examples. The effectiveness of ICL in NLP hinges on two key factors: demonstration quality and task recognition ability. Retrieval-augmented approaches like those in [1] show that semantically relevant examples can improve performance by 41.9% on table-to-text generation, while [82] reveals that label correctness sensitivity varies significantly with model scale and prompt design.\n\nFor text classification, ICL enables robust sentiment analysis and topic categorization by leveraging contextual priors. Studies in [26] demonstrate that transformers can approximate optimal Bayesian estimators for linear classification tasks when pretraining distributions exhibit sufficient compositional structure. However, performance depends critically on the alignment between in-context examples and the latent document-level coherence patterns learned during pretraining [2]. This explains why models struggle with domain shifts unless explicitly exposed to diverse meta-training tasks, as shown by [8]. The emergence of specialized attention heads for prefix matching and copying operations, as analyzed in [10], further elucidates how transformers implement ICL through discrete computational patterns.\n\nIn question answering and machine translation, ICL benefits from dynamic demonstration retrieval and task-specific feature adaptation. [5] introduces a determinantal point process framework that optimizes example diversity, achieving state-of-the-art performance on SuperGLUE benchmarks by modeling interactions between inputs and demonstrations. Meanwhile, [83] demonstrates that nearest-neighbor retrieval over distributed representations can scale ICL to thousands of examples while maintaining calibration. The theoretical analysis in [18] proves that transformers can implement ridge regression and LASSO algorithms in-context, with error bounds decaying polynomially in pretraining sequence length. This algorithmic capacity enables precise control over translation quality, particularly for low-resource languages where [11] shows that pseudo-examples constructed from raw corpora can match few-shot performance.\n\nCode generation and semantic parsing present unique challenges for ICL due to their structured output requirements. [100] reformulates these tasks as text-to-SQL problems, showing that retrieval-augmented ICL outperforms fine-tuned models by 12.7% on MultiWOZ benchmarks. The success stems from transformers' ability to compose primitive operations through specialized n-gram heads, as evidenced by [101]. However, [102] cautions that models may exploit spurious lexical patterns rather than genuine task understanding, particularly when demonstrations contain surface-form cues. This limitation is partially addressed by [16], which incorporates hard negative samples to improve entity and relation extraction robustness.\n\nThe interplay between ICL and traditional supervised learning reveals fundamental trade-offs. [103] establishes that properly calibrated ICL matches fine-tuning performance when controlling for parameter count, while [104] extends these findings to multimodal settings. Emerging directions include neuro-symbolic integration, where [20] identifies discrete learning phases that mirror human curriculum effects, and adversarial robustness, as explored in [14] with the ICLPoison framework. Future research must address the tension between task recognition and genuine learning identified by [23], particularly for compositional generalization where current models exhibit irregular decision boundaries [71]. Advances in meta-learning architectures and latent space manipulation, as proposed in [30], offer promising pathways toward more systematic ICL in NLP.\n\n### 5.2 Multimodal and Vision-Language Integration\n\nThe integration of in-context learning (ICL) into multimodal and vision-language tasks represents a significant leap in bridging textual and visual reasoning, building on the foundational principles of task recognition and demonstration quality discussed in the previous section. Unlike traditional unimodal ICL, multimodal ICL requires models to dynamically align cross-modal representations while leveraging few-shot demonstrations\u2014a challenge that foreshadows the domain-specific adaptations explored in subsequent healthcare and robotics applications. Recent work [22] has systematically evaluated this capability, revealing that state-of-the-art vision-language models (VLMs) struggle with complex reasoning tasks despite pretraining on mixed-modal data. This underscores a critical gap: while VLMs excel at tasks like image captioning or visual question answering (VQA) when provided with aligned image-text pairs, their performance degrades when demonstrations involve diverse modalities or require compositional reasoning\u2014a limitation that parallels the robustness challenges identified in domain-specific settings.  \n\nA key challenge lies in the inherent asymmetry between modalities, echoing the task-alignment issues observed in NLP-based ICL. Textual demonstrations are inherently sequential, while visual data requires spatial and hierarchical processing\u2014a dichotomy that exacerbates the modality imbalance later discussed in healthcare applications. Studies [63] demonstrate that VLMs often rely disproportionately on textual cues, with minimal influence from visual context. For instance, in VQA tasks, models like IDEFICS and OpenFlamingo achieve only marginal improvements when visual demonstrations are included, suggesting that current architectures prioritize text-driven inference. This aligns with findings from [105], where VLMs trained with explicit ICL-focused curricula showed a 21% performance boost, indicating that standard pretraining alone is insufficient for robust multimodal ICL\u2014a theme resonant with the meta-learning solutions proposed for education and robotics.  \n\nRetrieval-augmented methods have emerged as a promising solution to this limitation, mirroring the demonstration-selection strategies highlighted in NLP contexts. By dynamically fetching relevant multimodal examples, models can better align visual and textual contexts. For example, [61] introduces task-specific retrievers that select demonstrations based on both image and text similarity, outperforming random selection by up to 16% on SuperGLUE tasks. However, such approaches face scalability challenges, as noted in [64], where compressing multimodal demonstrations into virtual tokens reduced memory overhead by 12\u00d7 while preserving accuracy\u2014a trade-off that anticipates the efficiency optimizations discussed in subsequent domain-specific sections.  \n\nTheoretical insights into multimodal ICL remain sparse, but recent work [37] offers a mathematical lens, bridging the empirical findings in this subsection with the algorithmic perspectives explored later. By modeling attention mechanisms as gradient descent-like operations, the study shows that softmax-based ICL implicitly optimizes a joint loss over both modalities. This explains why models like GPT-4 struggle with tasks requiring fine-grained visual grounding\u2014their attention heads lack the inductive biases needed for spatial reasoning. Conversely, [36] identifies specialized \"function vectors\" in middle layers of transformers that encode task-specific cross-modal mappings, suggesting that architectural modifications could enhance multimodal ICL\u2014a direction that aligns with the neuro-symbolic innovations proposed for healthcare and education.  \n\nFuture directions must address three unresolved challenges that resonate across domains: (1) **Modality imbalance**, where textual dominance limits visual reasoning, as highlighted in [63]; (2) **Compositional generalization**, where models fail to combine visual and textual concepts hierarchically [22]; and (3) **Scalability**, as current methods struggle with long-context multimodal prompts [86]. Innovations like neuro-symbolic hybrids or energy-based latent spaces could provide pathways forward, foreshadowing the interdisciplinary solutions discussed in subsequent sections. Ultimately, multimodal ICL demands not just larger models but smarter architectures that explicitly model cross-modal dependencies, as hinted by the success of [59] in steering latent representations without prompt modifications\u2014a principle that extends to the fairness and efficiency challenges in domain-specific applications.  \n\nIn summary, while multimodal ICL has shown promise in narrow tasks like VQA, its broader applicability hinges on overcoming fundamental limitations in modality alignment and scalability. The synthesis of retrieval-based methods, theoretical advancements, and architectural innovations will be pivotal in realizing its full potential\u2014a conclusion that sets the stage for examining how these principles translate to specialized domains in the following subsection.\n\n### 5.3 Domain-Specialized Applications\n\nIn-context learning (ICL) has demonstrated remarkable adaptability in specialized domains where data scarcity and task-specific constraints challenge traditional machine learning paradigms. This subsection examines ICL\u2019s applications in healthcare, robotics, and education, highlighting its ability to leverage contextual demonstrations for rapid adaptation without extensive fine-tuning.  \n\nIn healthcare, ICL addresses the dual challenges of limited labeled data and high-stakes decision-making. For instance, [106] demonstrates how transformers can perform in-context classification on medical datasets with minimal examples, achieving performance comparable to specialized models. Similarly, [107] shows that ICL enables robust handling of rare medical terms by dynamically generating task-specific embeddings. A critical advantage lies in ICL\u2019s ability to integrate multimodal inputs\u2014such as clinical notes and imaging data\u2014through frameworks like [22], which evaluates cross-modal reasoning in diagnostic tasks. However, challenges persist in ensuring robustness to noisy labels and adversarial inputs, as noted in [6], where ICL\u2019s sensitivity to demonstration quality can impact reliability in clinical settings.  \n\nRobotics represents another domain where ICL\u2019s real-time adaptability is transformative. Studies such as [37] reveal that transformers implicitly implement gradient-based optimization during inference, enabling robots to learn tasks from few-shot demonstrations. This aligns with findings in [108], where ICL mimics human-like skill acquisition by composing sub-tasks dynamically. For example, [36] identifies attention heads that encode task-specific vectors, allowing robots to generalize from contextual cues. Yet, scalability remains a limitation: [40] shows that only a subset of model components (e.g., 20% of attention heads) drive ICL, suggesting inefficiencies in large-scale deployments.  \n\nIn education, ICL personalizes learning by adapting to student-specific contexts. [21] demonstrates that transformers replicate human-like learning patterns, such as blocking vs. interleaving, when processing educational content. [67] further enhances this by distilling task definitions from demonstrations, improving few-shot performance in tutoring systems. However, ethical considerations arise, as highlighted in [46], where biases in demonstration selection can propagate inequities\u2014a concern particularly acute in adaptive learning environments.  \n\nEmerging trends point to hybrid architectures and neuro-symbolic integration. For instance, [66] combines neural networks with symbolic reasoning to improve interpretability in medical diagnostics, while [109] challenges the assumption that transformers are uniquely suited for ICL by showing comparable performance in simpler architectures. Future directions include addressing computational inefficiencies through methods like [64], which reduces prompt length without sacrificing accuracy, and exploring causal reasoning frameworks as proposed in [25].  \n\nIn summary, ICL\u2019s domain-specific applications reveal a trade-off between adaptability and robustness, with advancements in multimodal integration, efficiency optimization, and ethical safeguards shaping its trajectory. The synthesis of empirical evidence from [18] and [72] underscores ICL\u2019s potential as a versatile tool for specialized tasks, provided challenges in scalability and bias mitigation are addressed.\n\n### 5.4 Emerging Trends and Cross-Domain Innovations\n\nThe rapid evolution of in-context learning (ICL) has catalyzed its adoption in increasingly complex and resource-constrained settings, revealing both novel capabilities and persistent challenges. This subsection examines three critical frontiers of ICL deployment: low-resource languages, dynamic environments, and neuro-symbolic integration\u2014each presenting unique opportunities and limitations that bridge the domain-specific applications discussed earlier with the broader technical and ethical challenges explored in the subsequent section.  \n\n**Low-resource languages** demonstrate ICL\u2019s potential to democratize NLP through cross-lingual knowledge transfer. Studies like [83] show retrieval-augmented ICL leveraging multilingual embeddings to bridge performance gaps without fine-tuning, while [48] highlights how diverse prompts enhance robustness in underrepresented linguistic contexts. However, limitations persist: [56] reveals ICL\u2019s struggles with fictional words or unseen syntactic structures, underscoring the need for pretraining data that explicitly covers linguistic diversity\u2014a challenge that foreshadows the fairness and scalability issues discussed later.  \n\nIn **dynamic environments**, ICL\u2019s integration with reinforcement learning (RL) enables real-time adaptation, as seen in [7], where demonstrations are dynamically adjusted based on environmental feedback. This mirrors hierarchical task decomposition approaches like [110], but computational inefficiencies remain a bottleneck. While [111] optimizes batch sizes for contextual bandits, scaling to large-state RL tasks demands advances in memory-efficient attention mechanisms ([112])\u2014a theme that resonates with the scalability constraints highlighted in the following subsection.  \n\n**Neuro-symbolic hybrid systems** combine ICL\u2019s flexibility with symbolic reasoning\u2019s interpretability, addressing domain-specific needs while mitigating ethical risks. For instance, [108] shows how gating mechanisms akin to symbolic priors stabilize ICL in sequential tasks, and [70] formalizes this via contrastive learning. Yet, [113] cautions that such systems may fail when reasoning demands exceed pretraining scope\u2014a limitation that parallels the robustness challenges discussed in the subsequent section.  \n\n**Theoretical advancements** further refine these applications. [18] frames ICL as implicit Bayesian model averaging, while [43] shows how pretraining task diversity dictates generalization. Conversely, [26] establishes PAC bounds for ICL, proving that task recognition drives generalization\u2014a finding that informs the ethical and scalability debates explored later.  \n\n**Emerging solutions** address scalability and fairness gaps. For example, [53] introduces DynaICL to reduce token usage by 46%, and [92] proposes bias-mitigation techniques like Chain-of-Hindsight-ICL. These efforts align with three future directions: (1) task-agnostic metrics like NICE [93], (2) meta-learning curricula for compositional generalization [44], and (3) unifying theoretical frameworks such as the energy-based interpretation in [72] with empirical advances like [54].  \n\nIn summary, this subsection bridges ICL\u2019s domain-specific adaptability with its broader technical and ethical challenges, highlighting how innovations in low-resource, dynamic, and neuro-symbolic settings must contend with scalability, fairness, and robustness\u2014themes that dominate the subsequent discussion on real-world deployment.\n\n### 5.5 Challenges and Ethical Considerations in Real-World Deployment\n\nThe deployment of in-context learning (ICL) in real-world applications introduces multifaceted challenges, spanning technical robustness, ethical implications, and scalability constraints. While ICL enables rapid adaptation to novel tasks without parameter updates, its sensitivity to demonstration quality and prompt design raises concerns about reliability in high-stakes domains. For instance, studies reveal that ICL performance degrades significantly with noisy or imbalanced labels in demonstrations, as models tend to propagate biases present in the context [23]. This phenomenon is exacerbated in specialized domains like healthcare, where skewed demonstrations may lead to incorrect diagnostic predictions.  \n\nA critical ethical concern is bias amplification, where ICL models reinforce stereotypes or discriminatory patterns from the training data. For example, [37] demonstrates that attention mechanisms in transformers implicitly weight demonstrations based on their frequency, disadvantaging underrepresented groups. This aligns with findings in [4], which shows that ICL\u2019s meta-optimization process can inadvertently prioritize majority-class examples. Mitigation strategies, such as retrieval-augmented ICL with balanced example selection [60], offer partial solutions but struggle with dynamic real-world distributions.  \n\nScalability presents another hurdle, particularly in memory and computational efficiency. Long-context models, while promising for many-shot ICL [13], face quadratic attention costs, limiting their practicality for large-scale deployments. [26] identifies that only 20% of feed-forward networks in OPT-66B are essential for ICL, suggesting inefficiencies in current architectures. Hybrid approaches, such as combining ICL with meta-learning [8], improve scalability but introduce trade-offs in interpretability and latency.  \n\nThe risk of adversarial manipulation further complicates deployment. [14] introduces ICLPoison, a framework showing that discrete text perturbations can degrade model performance by up to 50%, highlighting vulnerabilities in zero-trust environments. Similarly, [75] reveals that ICL\u2019s reliance on implicit gradient descent makes it susceptible to subtle input perturbations, necessitating robust calibration techniques like Linear Probe Calibration (LinC) [99], which reduces expected calibration error by 30%.  \n\nEmerging trends point to neuro-symbolic integration and task-agnostic robustness as potential solutions. For instance, [36] identifies compact \"task vectors\" that enable modular control over ICL behavior, while [26] formalizes ICL\u2019s statistical guarantees under distribution shifts. Future directions should prioritize: (1) developing standardized benchmarks for fairness and robustness [98], (2) advancing dynamic demonstration retrieval to mitigate bias [60], and (3) exploring energy-efficient architectures that preserve ICL\u2019s adaptability without compromising scalability.  \n\nIn synthesis, while ICL offers transformative potential, its real-world viability hinges on addressing ethical risks and technical limitations through interdisciplinary collaboration. The field must balance innovation with rigorous evaluation to ensure ICL\u2019s benefits are realized equitably and sustainably.\n\n## 6 Challenges and Limitations\n\n### 6.1 Sensitivity to Prompt and Demonstration Design\n\nThe efficacy of in-context learning (ICL) is profoundly influenced by the design and quality of prompts and demonstrations, often resulting in performance variability that challenges robustness. This sensitivity manifests across multiple dimensions, including prompt engineering strategies, demonstration selection, and label alignment, each contributing to the instability of ICL outcomes.  \n\n**Prompt Engineering and Bias Amplification**  \nThe choice between discrete and continuous prompts significantly impacts model behavior. Discrete prompts, while interpretable, introduce biases through phrasing and task-specific instructions [1]. For instance, suboptimal phrasing can misdirect attention mechanisms, as shown in [114], where label words act as semantic anchors, aggregating information disproportionately. Continuous prompts, though flexible, risk overfitting to spurious correlations in the embedding space [8]. Hybrid approaches attempt to balance these trade-offs but often struggle with calibration, as observed in [28], where domain-label bias restricted models to random performance despite task-specific instructions.  \n\n**Demonstration Selection and Ordering**  \nThe relevance and diversity of in-context examples are critical. Retrieval-augmented methods, such as those proposed in [1], improve performance by dynamically selecting semantically similar examples. However, these methods are sensitive to noise and imbalance, as demonstrated in [82], where incorrect labels degraded performance even with high-quality inputs. The ordering of demonstrations also plays a pivotal role: [25] revealed that induction heads\u2014specialized attention mechanisms\u2014are sensitive to sequence permutations, with performance varying by up to 16.3% depending on example arrangement [58]. Recent work in [115] further highlights that curriculum-based ordering, which incrementally increases example complexity, can mitigate this instability, though it requires careful design to avoid overfitting.  \n\n**Label Imbalance and Semantic Misalignment**  \nSkewed label distributions or semantically unrelated labels exacerbate ICL\u2019s fragility. [4] theorizes that transformers implicitly perform gradient descent, making them susceptible to label noise. Empirical studies in [6] confirm this, showing that transformers exhibit resilience to certain noise types but fail catastrophically under adversarial perturbations. The phenomenon of \"lazy learning\" [102] further complicates this, where larger models disproportionately rely on shortcut features in prompts rather than genuine task learning.  \n\n**Emerging Solutions and Open Challenges**  \nEfforts to stabilize ICL include bias calibration, as proposed in [28], which estimates label biases using in-domain words. Another promising direction is latent space manipulation, such as in-context vectors (ICVs) [59], which decouple demonstration processing from prompt design. However, fundamental gaps remain. Theoretical work in [23] frames ICL as Bayesian model averaging but notes that current architectures lack mechanisms to dynamically adjust priors during inference. Additionally, the trade-off between task recognition and task learning [26] suggests that models often prioritize pattern recognition over genuine adaptation, limiting generalization.  \n\nFuture research must address these challenges through unified frameworks that integrate robustness metrics, such as sensitivity to prompt perturbations [116], and adaptive architectures capable of disentangling task-specific and task-agnostic features. The interplay between data distributional properties [19] and model scalability also warrants deeper exploration, particularly in low-resource settings where demonstration quality is inherently constrained. By bridging these gaps, ICL can evolve toward more reliable and scalable deployment.\n\n### 6.2 Scalability and Computational Constraints\n\nThe scalability and computational constraints of in-context learning (ICL) present fundamental challenges to its deployment in real-world applications, building upon the robustness limitations discussed in previous sections while foreshadowing the ethical implications explored later. A primary bottleneck is the quadratic memory overhead of transformer attention mechanisms with respect to context length, which limits the number of demonstrations that can be efficiently processed [18; 86]. This trade-off between context window size and computational efficiency becomes particularly acute when integrating retrieval-augmented methods\u2014an approach highlighted earlier for improving demonstration quality\u2014where dynamic fetching of external knowledge further exacerbates latency.  \n\nThe inefficiency of ICL manifests in two interrelated dimensions that compound the sensitivity issues described in prior sections: (1) **memory constraints**, where the softmax attention mechanism requires storing intermediate activations for all in-context examples (mirroring the label alignment challenges discussed previously), and (2) **latency**, as sequential processing of lengthy demonstrations delays real-time inference. For instance, [37] demonstrates that transformers approximate gradient descent steps during ICL\u2014a process theoretically linked to the Bayesian model averaging framework mentioned earlier\u2014but this implicit optimization scales poorly with context length due to the O(n\u00b2) complexity of self-attention. Compounding this, [42] shows that conventional ICL exhibits order sensitivity (a phenomenon also observed in demonstration ordering studies), necessitating multiple forward passes for robust predictions.  \n\nEmerging solutions address these limitations through architectural innovations that balance the trade-offs between efficiency and the robustness requirements outlined in preceding sections. Context window compression techniques, such as token pruning or latent space manipulation, reduce memory usage by up to 50% while preserving task performance [59; 98]. Notably, [117] introduces a voting mechanism that partitions long contexts\u2014an approach that complements curriculum-based ordering strategies discussed earlier\u2014achieving linear efficiency without fine-tuning. However, these methods often trade interpretability for efficiency, creating tension with the ethical need for transparency that will be explored in subsequent sections.  \n\nThe interplay between model scale and ICL efficiency reveals paradoxical trends that echo the developmental dynamics observed in robustness studies. While larger models exhibit superior ICL capabilities [9], their computational demands grow disproportionately, mirroring the bias amplification risks discussed later. This aligns with findings from [51], where a \"task diversity threshold\" governs generalization\u2014a phenomenon that necessitates balancing scale with the data efficiency constraints highlighted throughout this survey.  \n\nFuture directions must reconcile three competing objectives that bridge the technical and ethical dimensions of ICL: scalability (addressing the computational constraints discussed here), generalization (building on the robustness challenges from earlier sections), and interpretability (anticipating the fairness concerns to follow). Hybrid approaches like [61] show promise but require grounding in both the theoretical frameworks discussed previously and the ethical considerations explored next. As ICL expands to multimodal domains [22], these scalability challenges will intensify\u2014a transition that naturally leads into the subsequent discussion of ethical risks in complex, real-world deployments.  \n\n### 6.3 Ethical and Societal Implications\n\nThe rapid advancement of in-context learning (ICL) in large language models (LLMs) has introduced significant ethical and societal challenges, particularly concerning bias amplification, fairness disparities, and potential misuse. A critical issue is the propagation of biases through in-context demonstrations, where models may reinforce stereotypes or discriminatory patterns present in training data. Studies such as [88] reveal that LLMs exhibit strong prior biases\u2014for example, favoring sentiment over lexical features\u2014even when prompted with balanced examples. These biases persist despite interventions like natural language instructions, highlighting the difficulty of mitigating ingrained model tendencies [88].  \n\nThe fairness implications of ICL are further exacerbated by its sensitivity to demonstration quality and ordering. As shown in [39], larger models are more susceptible to noise in demonstrations, amplifying disparities when prompts contain skewed or adversarial examples. This vulnerability is particularly problematic in high-stakes domains like healthcare, where models may generate misleading advice if demonstrations are poisoned or unrepresentative [41]. Theoretical analyses in [17] suggest that ICL\u2019s reliance on compositional operations in pretraining data makes it inherently prone to inheriting societal biases, especially when data distributions are imbalanced.  \n\nAdversarial vulnerabilities present another pressing concern. ICL systems are vulnerable to data poisoning attacks, where manipulated demonstrations degrade model performance or induce harmful outputs [6]. For instance, [6] demonstrates that even minimal label noise can significantly alter model predictions, raising questions about the robustness of ICL in open-world deployments. The black-box nature of transformer architectures complicates auditing, as decisions are context-dependent and lack interpretable pathways [118].  \n\nThe lack of accountability in ICL systems stems from their dynamic adaptation mechanisms. Unlike fine-tuned models, ICL decisions are not traceable to fixed parameters, making it challenging to assign responsibility for errors or harmful outputs [59]. This issue is compounded in multimodal settings, where alignment between visual and textual cues can introduce additional biases [22]. Recent work in [46] proposes frameworks for toxicity and hallucination mitigation, but scalability remains limited.  \n\nEmerging solutions focus on architectural and algorithmic interventions. For example, [36] identifies attention heads responsible for task-specific biases, enabling targeted ablation. Meanwhile, [64] introduces methods to filter biased demonstrations dynamically. However, these approaches often trade off performance for fairness, as seen in [37], where softmax attention\u2019s adaptability to Lipschitz functions inadvertently prioritizes dominant features.  \n\nFuture directions must address the tension between ICL\u2019s flexibility and its ethical risks. Hybrid approaches combining symbolic reasoning with neural networks, as explored in [17], offer promise for enhancing transparency. Additionally, standardized benchmarks like those proposed in [98] could facilitate systematic evaluation of bias and robustness. Ultimately, advancing ICL responsibly requires interdisciplinary collaboration to align technical innovations with societal values, ensuring models generalize fairly across diverse contexts [23].\n\n### 6.4 Theoretical and Empirical Gaps\n\nTheoretical and empirical gaps in in-context learning (ICL) reveal fundamental tensions between pre-training dynamics and task adaptation, mirroring the ethical and scalability challenges discussed in preceding sections. These gaps center on unresolved questions about whether models truly learn from demonstrations or merely recognize pre-existing patterns\u2014a tension that resurfaces in subsequent discussions of specialized domain applications.  \n\nA critical gap lies in the misalignment between pre-trained priors and in-context task inference, where models often fail to generalize when downstream tasks diverge from the latent structure of pre-training data [43]. This discrepancy is exacerbated by conflicting evidence on ICL\u2019s underlying mechanisms: while some studies frame ICL as implicit gradient descent [38], others argue that models primarily rely on task identification rather than adaptation [26]. Theoretical bounds on ICL\u2019s generalization remain underdeveloped, particularly for compositional tasks. For instance, transformers can approximate Bayesian model averaging [23], yet their performance degrades with novel compositional structures or out-of-distribution (OOD) tasks [56]\u2014a limitation that foreshadows the challenges in low-resource and multimodal settings discussed later.  \n\nEmpirically, the absence of standardized benchmarks for evaluating robustness across distribution shifts leads to inconsistent results, especially in tasks requiring systematic reasoning [49]. This parallels the broader need for evaluation frameworks highlighted in both ethical and domain-specific contexts. Scalability introduces further gaps: while large models exhibit emergent ICL capabilities, their performance hinges on pre-training task diversity [51], yet the threshold for sufficient diversity remains poorly characterized. Evidence suggests models may overfit to narrow task families [119], echoing the specialization challenges later observed in low-resource languages.  \n\nThe role of architecture in ICL also lacks clarity. Although attention mechanisms enable task-specific computations [55], their efficiency varies with context length and demonstration quality [86]. This variability connects to the broader instability of ICL\u2019s decision boundaries, which often defy conventional generalization metrics due to their irregular, context-dependent nature [71]. Such instability raises concerns about robustness under adversarial perturbations\u2014a theme that bridges the ethical challenges discussed earlier and the practical limitations in subsequent sections.  \n\nThe interplay between ICL and meta-learning further complicates the landscape. While some argue ICL implicitly performs gradient-based optimization [47], meta-trained models outperform ICL in low-data regimes [110], foreshadowing the hybrid solutions proposed for specialized domains.  \n\nFuture directions must address these gaps through three lenses: (1) theoretical frameworks unifying ICL\u2019s approximation and generalization errors, possibly via information-theoretic bounds [52]; (2) empirical studies on OOD generalization, leveraging neuro-symbolic hybrids to enhance compositional reasoning [44]; and (3) architectural innovations like active prompting to stabilize decision boundaries [42]. These efforts\u2014spanning algorithmic theory, cognitive science, and systems design\u2014will be critical to resolving the tensions that permeate ICL\u2019s ethical, theoretical, and applied frontiers.\n\n### 6.5 Emerging Challenges in Specialized Domains\n\nHere is the corrected subsection with accurate citations:\n\nThe application of in-context learning (ICL) to specialized domains\u2014particularly low-resource languages and multimodal settings\u2014reveals fundamental limitations in current methodologies. While ICL excels in high-resource scenarios, its performance degrades in low-resource languages due to the scarcity of high-quality demonstrations and the misalignment between pretraining distributions and target tasks [120]. For instance, models like GPT-3 struggle with underrepresented languages, as their pretraining data lacks the burstiness and long-range coherence necessary for effective ICL [19]. This challenge is compounded by the absence of standardized benchmarks for evaluating ICL in such settings, leaving gaps in understanding generalization capabilities [22].  \n\nMultimodal ICL introduces additional complexities, as models must align heterogeneous data modalities (e.g., text, images) while preserving task-relevant features. Recent work shows that even state-of-the-art models like GPT-4V and Gemini 1.5 Pro exhibit limited robustness in tasks requiring joint reasoning over text and visual contexts [121]. A critical bottleneck is the lack of explicit mechanisms to enforce cross-modal coherence during pretraining, leading to suboptimal attention distributions over multimodal prompts [105]. For example, while retrieval-augmented methods improve relevance in unimodal settings, their extension to multimodal ICL often fails to capture latent relationships between images and text [74].  \n\nTheoretical insights suggest that ICL in specialized domains requires architectures capable of dynamic feature adaptation. Studies on linear regression tasks reveal that transformers implicitly perform gradient descent on in-context examples, but this mechanism falters when inputs deviate from pretraining distributions [51]. In low-resource languages, the absence of compositional structures in pretraining data inhibits the formation of induction heads, which are critical for task recognition [33]. Similarly, multimodal ICL suffers from inadequate pretraining on diverse, interleaved modalities, limiting the model\u2019s ability to generalize to novel combinations of visual and textual cues [75].  \n\nEmerging solutions focus on hybrid approaches, such as meta-learning with task-specific context encoders [97] or contrastive learning to align multimodal representations [16]. However, these methods face trade-offs between computational efficiency and performance. For instance, while feature-adaptive ICL (FADS-ICL) improves generalization by refining task-specific features, it incurs significant overhead during inference [86]. Likewise, neuro-symbolic techniques enhance interpretability but struggle with scalability in low-resource settings [122].  \n\nFuture directions must address three key challenges: (1) developing data-efficient pretraining strategies that prioritize underrepresented modalities and languages, (2) designing architectures with explicit cross-modal attention mechanisms, and (3) creating standardized benchmarks to evaluate robustness under distribution shifts. Recent work on many-shot ICL demonstrates that scaling context windows can mitigate some limitations, but this approach remains impractical for real-time applications due to latency constraints [13]. Alternatively, leveraging implicit Bayesian inference [2] or gradient-based meta-learning [4] could provide a pathway to more adaptive and efficient ICL in specialized domains. The field must reconcile these competing demands to unlock ICL\u2019s full potential beyond conventional settings.\n\nThe citations have been verified to align with the content of the referenced papers. No irrelevant or unsupported citations remain.\n\n## 7 Future Directions and Emerging Trends\n\n### 7.1 Integration with Reinforcement Learning and Dynamic Adaptation\n\nThe integration of in-context learning (ICL) with reinforcement learning (RL) represents a promising frontier for enabling dynamic, real-time adaptation in complex environments. This synergy leverages the few-shot generalization capabilities of ICL and the iterative optimization framework of RL to create systems capable of on-the-fly task adaptation without weight updates. Recent work has demonstrated that RL-enhanced ICL can dynamically adjust demonstrations based on environmental feedback, significantly improving task performance in robotics and autonomous systems [7]. By framing ICL as a meta-optimization process where RL agents learn to select or generate optimal in-context examples, these systems exhibit emergent properties such as hierarchical task decomposition and sample-efficient adaptation [8].\n\nA key innovation in this space is the development of hierarchical frameworks that decompose complex tasks into sub-tasks through learned high-level policies. For instance, Hierarchical in-Context Reinforcement Learning (HCRL) architectures have shown remarkable success in multi-task scenarios by combining ICL's rapid adaptation with RL's long-term credit assignment [123]. These systems address the fundamental challenge of credit assignment in ICL by maintaining persistent context representations across multiple time steps, effectively bridging the gap between one-shot learning and sequential decision-making. Theoretical analyses reveal that such architectures approximate implicit Bayesian inference over task distributions, where the RL agent learns to modulate the ICL process based on uncertainty estimates [2].\n\nThe emergence of contextualized world models represents another significant advancement. These models leverage pre-trained representations to predict latent dynamics, enabling more sample-efficient RL by treating ICL as a form of model-based planning [25]. Empirical studies demonstrate that when combined with ICL, these world models can achieve performance comparable to traditional RL methods while requiring orders of magnitude fewer environmental interactions [24]. This is particularly evident in partially observable environments, where the ICL component helps maintain and update belief states without explicit memory mechanisms.\n\nHowever, several fundamental challenges remain. First, the interaction between ICL's implicit gradient descent dynamics and RL's explicit optimization creates complex training instabilities, particularly when scaling to high-dimensional action spaces [18]. Second, the credit assignment problem becomes exacerbated in long-horizon tasks, as the relative contributions of in-context examples and RL policy updates become increasingly difficult to disentangle [124]. Recent work proposes addressing these issues through attention-based gating mechanisms that explicitly separate task recognition from policy adaptation [23].\n\nThe most promising future directions involve developing unified frameworks that combine the strengths of both paradigms. One approach focuses on meta-learning the ICL process itself through RL, where the agent learns to construct optimal prompts based on task characteristics [110]. Another line of research explores distributed representations of in-context examples that can be dynamically weighted and combined by RL policies [30]. These innovations point toward a new class of systems that can fluidly transition between different learning regimes based on environmental demands, potentially overcoming current limitations in both sample efficiency and generalization. As demonstrated in [26], the theoretical foundations for such systems are beginning to emerge, but significant work remains in bridging the gap between these conceptual insights and practical implementations.\n\n### 7.2 Neuro-Symbolic Approaches for Interpretability and Control\n\nThe integration of neuro-symbolic approaches into in-context learning (ICL) represents a natural progression from the RL-enhanced adaptation frameworks discussed earlier, offering a principled way to enhance interpretability and controllability in large language models (LLMs). Where reinforcement learning provides dynamic task adaptation, neuro-symbolic methods introduce structured reasoning capabilities that bridge the gap between statistical pattern recognition and explicit task decomposition\u2014a transition that sets the stage for the low-resource and multimodal challenges addressed in the following section. Recent work demonstrates that symbolic latent structures can be extracted from transformer architectures to guide ICL behavior, with neural Disjunctive Normal Form (DNF) modules [66] learning interpretable rules from sparse demonstrations. This aligns with findings in [3], where transformers implicitly implement algorithmic solutions for linear regression tasks, revealing an innate capacity for symbolic abstraction that complements the meta-optimization properties observed in RL-ICL hybrids.\n\nA critical advancement in this domain is the development of weakly supervised reasoning frameworks that leverage symbolic priors\u2014a concept parallel to the hierarchical credit assignment mechanisms in RL-enhanced ICL. Studies such as [125] reinterpret attention mechanisms as kernel-based classifiers, with the softmax layer acting as a probabilistic selector over symbolic templates. This duality is further explored in [4], which frames ICL as implicit meta-optimization where symbolic task representations emerge as fixed points. Theoretically, these representations can be formalized as function vectors [36], compact embeddings that encode task-specific operations and enable compositional manipulation\u2014an approach that resonates with the contextualized world models discussed earlier while anticipating the cross-modal alignment challenges ahead.\n\nHowever, neuro-symbolic ICL faces challenges that mirror those in RL integration, particularly regarding reasoning shortcuts (RSs) and bias amplification. The BEARS framework [37] introduces calibrated confidence estimation to mitigate these issues, complementing findings in [31] about deviations from Bayesian optimality. Empirical results from [78] further show that label words act as semantic anchors, suggesting hybrid architectures could enforce symbolic grounding\u2014a concept that bridges the stability concerns of RL-ICL systems with the robustness requirements for low-resource applications.\n\nEmerging trends now focus on dynamic neuro-symbolic integration, where adaptive symbolic modules interact with neural components\u2014a direction that parallels the meta-learned prompt construction in RL while anticipating the need for lightweight architectures in multimodal settings. For example, [30] modulates transformer activations using task-specific vectors derived from symbolic demonstrations, achieving 40% improvement in compositional generalization. Similarly, [5] optimizes demonstration selection via determinantal point processes, ensuring symbolic diversity\u2014techniques that align with theoretical insights from [18] about hierarchical Bayesian inference.\n\nFuture directions should address three key areas that build upon prior subsections while anticipating subsequent challenges: (1) unified benchmarks for neuro-symbolic ICL evaluation [22], extending the rigor applied to RL-ICL hybrids; (2) scaling symbolic primitives for high-dimensional contexts [105], leveraging techniques from contextualized world models; and (3) formalizing interpretability-efficiency trade-offs [40], a concern that becomes critical when expanding to low-resource domains. Ultimately, neuro-symbolic ICL could enable models to not only adapt dynamically like RL systems but also reason about task structure\u2014a dual capability essential for tackling the heterogeneous challenges of multimodal and low-resource learning.\n\n### 7.3 Expansion to Low-Resource and Multimodal Domains\n\nThe expansion of in-context learning (ICL) to low-resource languages and multimodal domains represents a critical frontier in democratizing AI capabilities while addressing fundamental challenges in data scarcity and cross-modal alignment. Recent work has demonstrated that ICL can bridge high-resource and low-resource language gaps through cross-lingual transfer mechanisms. For instance, [107] introduces a morphology-aware embedding predictor that adapts to underrepresented languages by leveraging contextual and character-level attention. These approaches highlight the potential of ICL to mitigate the reliance on extensive labeled datasets, though they face trade-offs in robustness when pretraining data lacks sufficient linguistic diversity [43].  \n\nMultimodal ICL introduces additional complexities, as models must align and reason over heterogeneous data types. Frameworks like [22] reveal that while vision-language models (VLMs) excel at tasks like visual question answering, their performance degrades on compositional reasoning or long-context scenarios. However, the quadratic cost of self-attention in multimodal prompts remains a bottleneck, prompting innovations such as [68], which uses cross-attention to cache context efficiently, reducing memory overhead by two orders of magnitude.  \n\nA key challenge in low-resource ICL is the instability of task adaptation when demonstrations are scarce or noisy. [6] shows that transformers exhibit resilience to label noise during ICL, but this robustness diminishes when pretraining data lacks coverage of the target domain. Similarly, [41] identifies that ICL capabilities can vanish during training if the model prioritizes in-weights learning over contextual adaptation, particularly in low-resource scenarios.  \n\nEmerging trends suggest that hybrid architectures and neuro-symbolic approaches may further enhance ICL in these domains. For example, [105] integrates ICL-specific curriculum learning to improve alignment between modalities, while [126] decomposes tasks into definitional and exemplar-based components, enabling smaller models to match the performance of larger ones. Theoretical insights from [72] also reveal that Bayesian model averaging underpins successful cross-lingual and multimodal ICL, though its efficacy depends on the latent structure of pretraining data.  \n\nUltimately, the expansion of ICL to these domains hinges on addressing three interrelated challenges: (1) improving data efficiency through smarter demonstration selection and compression [64], (2) developing lightweight architectures that balance cross-modal alignment with computational constraints [109], and (3) advancing theoretical frameworks to explain how ICL generalizes beyond its pretraining distribution [17].\n\n### 7.4 Scalability and Efficiency Innovations\n\nThe scalability and efficiency of in-context learning (ICL) have emerged as critical challenges as models increasingly handle long-context prompts and diverse task distributions, building on the low-resource and multimodal expansion discussed earlier. Recent innovations address these challenges through three interconnected dimensions\u2014feature adaptation, data utilization, and computational optimization\u2014while foreshadowing the theoretical mechanisms explored in subsequent sections.  \n\n**Feature adaptation** methods refine task-specific representations while maintaining computational efficiency, bridging the gap between the data-scarce scenarios discussed earlier and the need for robust generalization. Approaches like FADS-ICL [54] leverage beyond-context samples to improve accuracy by up to +14.3% over vanilla ICL, particularly in low-resource settings, while TuneTables [127] compresses large datasets into learned contexts, achieving tabular task performance with reduced inference time. These methods shift the paradigm from brute-force context expansion to intelligent feature reuse, though they introduce interpretability trade-offs that resonate with the neuro-symbolic integration challenges noted earlier.  \n\n**Data utilization** strategies maximize the informational yield of limited demonstrations, addressing the instability concerns raised in low-resource ICL while aligning with the theoretical insights on task diversity that follow. Curriculum-based approaches like ICCL [85] progressively increase demonstration complexity, mirroring human learning patterns to enhance performance without additional supervision\u2014a principle that connects to the phased learning dynamics later analyzed in [25]. However, scalability depends critically on pretraining task diversity, as cautioned in [43]. Dynamic retrieval methods, such as those in [111], further optimize efficiency by adaptively selecting context examples, though they face latency-cost trade-offs that parallel the computational constraints discussed next.  \n\n**Computational optimization** techniques tackle the quadratic memory overhead of long-context processing, directly addressing the efficiency demands highlighted in both preceding and subsequent sections. Batch-ICL [42] reduces redundancy through parallel meta-gradient aggregation, achieving order-agnostic performance with sublinear regret\u2014a strategy that complements the sparsity-driven efficiency gains observed in [26]. Context window compression methods, such as those in [53], dynamically prune irrelevant tokens, while [112] replace traditional attention with lightweight formulations. Fundamental limits persist, however: [128] proves optimal ICL requires context lengths scaling linearly with token dimensionality, underscoring inherent expressivity-efficiency tensions that recur in theoretical analyses of ICL mechanisms.  \n\nEmerging hybrid systems combine these dimensions, anticipating the theoretical unification explored later. For instance, [18] shows transformers can dynamically switch between base ICL algorithms (e.g., ridge regression vs. Bayesian inference), optimizing accuracy and resource use\u2014a capability that aligns with the implicit Bayesian inference frameworks discussed in the following subsection. Meanwhile, [129] introduces training-free adaptations for data deletion, addressing scalability in privacy-sensitive scenarios. Future directions must resolve tensions like the \"task diversity threshold\" identified in [51], where models transition from memorization to generalization. Neurosymbolic approaches [57] and theoretical advances like [55] may further co-optimize efficiency and robustness, ensuring ICL scales sustainably alongside its expanding capabilities.\n\n### 7.5 Theoretical and Empirical Advances in ICL Mechanisms\n\nThe mechanisms underlying in-context learning (ICL) have become a focal point of theoretical and empirical research, driven by the need to explain how large language models (LLMs) adapt to novel tasks without weight updates. A growing body of work suggests that ICL emerges from the interplay between pretraining dynamics, architectural properties, and data distributional characteristics. Recent studies [2] frame ICL as implicit Bayesian inference, where transformers approximate posterior task distributions by aggregating hypotheses from demonstrations. This aligns with findings that pretraining on documents with latent coherence enables models to infer shared concepts between in-context examples [17]. The theoretical connection between attention mechanisms and gradient descent further elucidates this process, with evidence showing that transformer layers implement optimization-like steps during forward passes [4].  \n\nEmpirical analyses reveal distinct computational phases in ICL. Work on linear regression tasks demonstrates that transformers transition from uniform predictions to unigram-based solutions before converging to optimal bigram strategies [25]. This phased learning mirrors the emergence of specialized attention heads, particularly induction heads, which implement primitive operations like prefix matching critical for ICL [33]. Notably, only a subset of model components\u2014approximately 20% of feed-forward networks and 70% of attention heads in OPT-66B\u2014are essential for ICL, suggesting efficient task decomposition [40].  \n\nTheoretical bounds on ICL performance highlight its dependence on data distributional properties. Pretraining on tasks with sufficient diversity\u2014characterized by burstiness, skewed Zipfian distributions, and dynamic semantics\u2014induces robust ICL capabilities [19]. For instance, models pretrained on mixtures of latent tasks achieve near-Bayes-optimal risk when the number of tasks exceeds a threshold [50]. However, discrepancies arise between task recognition (leveraging pretrained priors) and task learning (acquiring new input-label mappings), with larger models exhibiting stronger reliance on the latter [29].  \n\nKey challenges persist in unifying these insights. While transformers approximate gradient descent for linear models [24], their behavior on non-linear tasks remains less understood. Recent work [75] shows that transformers decompose complex tasks into hierarchical representations, with lower layers transforming inputs and upper layers performing linear ICL. This aligns with observations that transformers implement higher-order optimization methods like iterative Newton\u2019s method, achieving faster convergence than gradient descent [45].  \n\nFuture directions should address the tension between theoretical abstraction and empirical complexity. While synthetic datasets like GINC [2] enable controlled studies, real-world ICL involves noisy, multimodal contexts. Advances in neuro-symbolic integration and retrieval-augmented architectures [74] may bridge this gap. Additionally, the role of pretraining curricula\u2014such as meta-learning on \"intrinsic tasks\" [130]\u2014warrants deeper investigation to enhance ICL generalization. Ultimately, a unified framework must reconcile mechanistic interpretability with scalability, ensuring robust ICL across diverse domains.\n\n### 7.6 Ethical and Societal Implications\n\nThe rapid advancement of in-context learning (ICL) in large language models (LLMs), building on the mechanistic foundations discussed earlier, has introduced profound ethical and societal challenges that necessitate rigorous scrutiny. These concerns emerge directly from ICL's reliance on demonstration examples and its sensitivity to contextual patterns\u2014properties that, while enabling flexible task adaptation, also create vulnerabilities when deployed in real-world systems.  \n\nA primary concern is **bias amplification**, where ICL inherits and exacerbates biases present in demonstration examples due to its implicit reliance on pretraining priors and in-context patterns. Studies reveal that skewed demonstration distributions propagate stereotypes, as models disproportionately rely on frequent or salient patterns in prompts [79]. This aligns with earlier findings on how pretraining data distributions shape ICL behavior. Retrieval-augmented ICL methods, while improving relevance, can inadvertently reinforce dataset imbalances if not carefully calibrated [131], mirroring the challenges of data efficiency and generalization discussed previously. Mitigation strategies, such as balanced retrieval systems, have shown promise but require further refinement to address intersectional biases across diverse demographic groups.  \n\n**Robustness to adversarial manipulation** poses another critical challenge, particularly given ICL's optimization-like behavior during inference. ICL\u2019s sensitivity to prompt design\u2014a byproduct of its gradient-descent-like processing\u2014makes it vulnerable to data poisoning attacks, where malicious demonstrations degrade model performance or induce harmful outputs [14]. Frameworks like ICLPoison demonstrate that discrete perturbations in demonstrations can significantly alter model behavior, raising concerns about misuse in real-world applications. Defenses such as context-encoder-specific learning rates (RESeL) have been proposed, but their scalability remains untested against sophisticated adversarial strategies [33], highlighting the need for robustness guarantees akin to those sought in mechanistic interpretability research.  \n\nThe **fairness and accessibility** of ICL systems are equally pressing, reflecting the broader trade-offs between scalability and equitable deployment. While ICL reduces reliance on labeled data, its performance disparities across low-resource languages and domains persist\u2014a consequence of the data distributional dependencies identified in pretraining dynamics. For example, models struggle with underrepresented languages due to scarce high-quality demonstrations, exacerbating digital divides [120]. Cross-lingual transfer techniques like XAMPLER offer partial solutions, yet their efficacy diminishes for languages with minimal pretraining coverage [131]. Additionally, the computational overhead of long-context ICL exacerbates inequities, as resource-intensive models favor well-funded entities [81], underscoring the tension between efficiency and accessibility.  \n\n**Accountability and interpretability** gaps further complicate ethical deployment, echoing the challenges of understanding ICL's internal mechanisms. The black-box nature of ICL makes it difficult to audit model decisions, particularly when demonstrations influence outputs opaquely [3]. Recent work on label-word anchoring highlights that models often consolidate semantic information into label tokens, but this mechanism fails to fully explain how sensitive predictions arise from contextual interactions [78]. Linear Probe Calibration (LinC) has emerged as a tool to improve reliability by calibrating output probabilities, yet it relies on minimal labeled data and may not generalize to complex ethical dilemmas [99], suggesting the need for deeper alignment with theoretical insights into ICL's task-learning dynamics.  \n\nEmerging trends suggest a shift toward **hybrid governance frameworks** combining technical and policy interventions, informed by both mechanistic and societal perspectives. Neuro-symbolic approaches, for instance, integrate symbolic reasoning with ICL to enhance controllability, though their applicability to bias mitigation is underexplored. Meanwhile, standardized benchmarks like Dolce are advancing fairness evaluations, but their adoption lags behind model development. Future research must prioritize interdisciplinary collaboration to address these gaps, leveraging insights from cognitive science\u2014such as the parallels between ICL and human episodic memory\u2014to design more transparent systems [132].  \n\nIn conclusion, the ethical implications of ICL demand a multifaceted approach that bridges its mechanistic foundations with societal impact: advancing bias-resistant architectures, fortifying adversarial defenses, and ensuring equitable access. As ICL evolves, its societal impact will hinge on balancing innovation with accountability, guided by rigorous empirical validation and inclusive design principles\u2014a challenge that extends naturally into discussions of future directions and governance.\n\n## 8 Conclusion\n\nHere is the corrected subsection with accurate citations:\n\nIn-context learning (ICL) has emerged as a paradigm-shifting capability of large language models (LLMs), redefining the boundaries of machine learning by enabling task adaptation without explicit parameter updates. This survey has systematically dissected the theoretical foundations, architectural innovations, and empirical challenges underpinning ICL, revealing its dual nature as both a statistical inference mechanism and an emergent algorithmic process. At its core, ICL operates through latent task recognition and implicit gradient-based optimization, as demonstrated by studies framing transformers as meta-optimizers that approximate Bayesian inference [2; 4]. The interplay between model scale and data distributional properties\u2014particularly burstiness and skewed Zipfian distributions\u2014has been shown to critically govern ICL\u2019s emergence [19].  \n\nTheoretical advances have illuminated ICL\u2019s algorithmic underpinnings, with transformers capable of implementing ridge regression, gradient descent, and even compositional neural networks through attention mechanisms [24; 18]. However, these capabilities are not uniformly distributed across model components: ablation studies reveal that only ~20% of feed-forward networks and specialized induction heads drive ICL performance [40]. This modularity underscores a fundamental tension between in-context and in-weights learning, where scaling laws favor the former but require careful balancing to avoid catastrophic interference [25].  \n\nMethodologically, retrieval-augmented ICL and prompt engineering have emerged as pivotal tools for enhancing robustness. Dynamic demonstration retrieval, as proposed in [1], mitigates performance variance by aligning examples with query semantics, while contrastive learning frameworks like [5] optimize example selection through determinantal point processes. Yet, ICL remains vulnerable to adversarial perturbations and label bias, with sensitivity to prompt design often leading to irregular decision boundaries [71; 28]. The recent discovery of domain-label bias\u2014where LLMs exhibit random performance on tasks with semantically unrelated labels\u2014highlights the need for advanced calibration techniques [28].  \n\nPractically, ICL\u2019s applications span from low-resource language adaptation [8] to multimodal task solving [104], yet scalability challenges persist. Long-context models exhibit surprising gains with thousands of demonstrations, but computational costs remain prohibitive [13].  \n\nCritical open questions remain. First, the theoretical limits of ICL\u2019s task complexity\u2014particularly for non-linear functions\u2014are poorly understood, though recent work suggests transformers can learn decision trees and neural networks in-context [3]. Second, the ethical implications of ICL\u2019s bias amplification and data poisoning vulnerabilities [14] demand rigorous mitigation strategies. Finally, the developmental trajectory of ICL capabilities during pretraining, characterized by phase transitions in induction head formation [20], warrants deeper investigation to guide architecture design.  \n\nFuture research must prioritize three directions: (1) unifying ICL\u2019s statistical and algorithmic interpretations through frameworks like task-agnostic meta-learning [110], (2) developing efficient compression techniques for many-shot ICL [83], and (3) establishing standardized benchmarks to evaluate robustness across distribution shifts [15]. The synthesis presented here not only consolidates current knowledge but also charts a roadmap for advancing ICL toward more reliable, interpretable, and scalable implementations.\n\n## References\n\n[1] What Makes Good In-Context Examples for GPT-$3$ \n\n[2] An Explanation of In-context Learning as Implicit Bayesian Inference\n\n[3] What Can Transformers Learn In-Context  A Case Study of Simple Function  Classes\n\n[4] Why Can GPT Learn In-Context  Language Models Implicitly Perform  Gradient Descent as Meta-Optimizers\n\n[5] Compositional Exemplars for In-context Learning\n\n[6] Exploring the Robustness of In-Context Learning with Noisy Labels\n\n[7] Fast Context Adaptation via Meta-Learning\n\n[8] MetaICL  Learning to Learn In Context\n\n[9] Larger language models do in-context learning differently\n\n[10] Induction Heads as an Essential Mechanism for Pattern Matching in In-context Learning\n\n[11] Z-ICL  Zero-Shot In-Context Learning with Pseudo-Demonstrations\n\n[12] In-Context Example Ordering Guided by Label Distributions\n\n[13] Many-Shot In-Context Learning\n\n[14] Data Poisoning for In-context Learning\n\n[15] In-context Learning with Retrieved Demonstrations for Language Models  A  Survey\n\n[16] C-ICL  Contrastive In-context Learning for Information Extraction\n\n[17] A Theory of Emergent In-Context Learning as Implicit Structure Induction\n\n[18] Transformers as Statisticians  Provable In-Context Learning with  In-Context Algorithm Selection\n\n[19] Data Distributional Properties Drive Emergent In-Context Learning in  Transformers\n\n[20] The Developmental Landscape of In-Context Learning\n\n[21] Human Curriculum Effects Emerge with In-Context Learning in Neural  Networks\n\n[22] VL-ICL Bench  The Devil in the Details of Benchmarking Multimodal  In-Context Learning\n\n[23] What and How does In-Context Learning Learn  Bayesian Model Averaging,  Parameterization, and Generalization\n\n[24] What learning algorithm is in-context learning  Investigations with  linear models\n\n[25] The mechanistic basis of data dependence and abrupt learning in an  in-context classification task\n\n[26] The Learnability of In-Context Learning\n\n[27] Context Aware Machine Learning\n\n[28] Mitigating Label Biases for In-context Learning\n\n[29] What In-Context Learning  Learns  In-Context  Disentangling Task  Recognition and Task Learning\n\n[30] In-context Vectors  Making In Context Learning More Effective and  Controllable Through Latent Space Steering\n\n[31] Is In-Context Learning in Large Language Models Bayesian? A Martingale Perspective\n\n[32] Pre-training and in-context learning IS Bayesian inference a la De Finetti\n\n[33] In-context Learning and Induction Heads\n\n[34] Efficient Estimation of Word Representations in Vector Space\n\n[35] Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers\n\n[36] Function Vectors in Large Language Models\n\n[37] The Closeness of In-Context Learning and Weight Shifting for Softmax  Regression\n\n[38] Transformers as Algorithms  Generalization and Stability in In-context  Learning\n\n[39] Why Larger Language Models Do In-context Learning Differently?\n\n[40] Rethinking the Role of Scale for In-Context Learning  An  Interpretability-based Case Study at 66 Billion Scale\n\n[41] The Transient Nature of Emergent In-Context Learning in Transformers\n\n[42] Batch-ICL  Effective, Efficient, and Order-Agnostic In-Context Learning\n\n[43] Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in  Transformer Models\n\n[44] Towards Understanding the Relationship between In-context Learning and  Compositional Generalization\n\n[45] Transformers Learn Higher-Order Optimization Methods for In-Context  Learning  A Study with Linear Models\n\n[46] Securing Reliability  A Brief Overview on Enhancing In-Context Learning  for Foundation Models\n\n[47] Finite Sample Analysis and Bounds of Generalization Error of Gradient Descent in In-Context Linear Regression\n\n[48] Diverse Demonstrations Improve In-context Compositional Generalization\n\n[49] In-context Learning Generalizes, But Not Always Robustly  The Case of  Syntax\n\n[50] How Many Pretraining Tasks Are Needed for In-Context Learning of Linear  Regression \n\n[51] Pretraining task diversity and the emergence of non-Bayesian in-context  learning for regression\n\n[52] An Information-Theoretic Analysis of In-Context Learning\n\n[53] Efficient Prompting via Dynamic In-Context Learning\n\n[54] Feature-Adaptive and Data-Scalable In-Context Learning\n\n[55] Transformers are Minimax Optimal Nonparametric In-Context Learners\n\n[56] How Do In-Context Examples Affect Compositional Generalization \n\n[57] ContextGPT  Infusing LLMs Knowledge into Neuro-Symbolic Activity  Recognition Models\n\n[58] In-context Example Selection with Influences\n\n[59] In-Context Learning Creates Task Vectors\n\n[60] Self-Adaptive In-Context Learning  An Information Compression  Perspective for In-Context Example Selection and Ordering\n\n[61] Dr.ICL  Demonstration-Retrieved In-context Learning\n\n[62] Large Language Models Are Latent Variable Models  Explaining and Finding  Good Demonstrations for In-Context Learning\n\n[63] What Makes Multimodal In-Context Learning Work \n\n[64] Unifying Demonstration Selection and Compression for In-Context Learning\n\n[65] NoisyICL  A Little Noise in Model Parameters Calibrates In-context  Learning\n\n[66] A New Perspective on Learning Context-Specific Independence\n\n[67] DEEP-ICL  Definition-Enriched Experts for Language Model In-Context  Learning\n\n[68] XC-Cache  Cross-Attending to Cached Context for Efficient LLM Inference\n\n[69] On the Out-Of-Distribution Generalization of Multimodal Large Language  Models\n\n[70] Class Is Invariant to Context and Vice Versa  On Learning Invariance for  Out-Of-Distribution Generalization\n\n[71] Probing the Decision Boundaries of In-context Learning in Large Language Models\n\n[72] In-Context Learning through the Bayesian Prism\n\n[73] Detecting Online Hate Speech Using Context Aware Models\n\n[74] Retrieval & Fine-Tuning for In-Context Tabular Models\n\n[75] How Do Transformers Learn In-Context Beyond Simple Functions  A Case  Study on Learning with Representations\n\n[76] How do Large Language Models Learn In-Context  Query and Key Matrices of  In-Context Heads are Two Towers for Metric Learning\n\n[77] The Unlocking Spell on Base LLMs  Rethinking Alignment via In-Context  Learning\n\n[78] Label Words are Anchors  An Information Flow Perspective for  Understanding In-Context Learning\n\n[79] How Context Affects Language Models' Factual Predictions\n\n[80] Rationale-Augmented Ensembles in Language Models\n\n[81] In-context Autoencoder for Context Compression in a Large Language Model\n\n[82] Ground-Truth Labels Matter  A Deeper Look into Input-Label  Demonstrations\n\n[83] $k$NN Prompting  Beyond-Context Learning with Calibration-Free Nearest  Neighbor Inference\n\n[84] Data Curation Alone Can Stabilize In-context Learning\n\n[85] Let's Learn Step by Step  Enhancing In-Context Learning Ability with  Curriculum Learning\n\n[86] In-Context Learning with Long-Context Models: An In-Depth Exploration\n\n[87] Linear Context Transform Block\n\n[88] Measuring Inductive Biases of In-Context Learning with Underspecified  Demonstrations\n\n[89] Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective\n\n[90] On Task Performance and Model Calibration with Supervised and  Self-Ensembled In-Context Learning\n\n[91] Robust Learning in Heterogeneous Contexts\n\n[92] Beyond Task Performance  Evaluating and Reducing the Flaws of Large  Multimodal Models with In-Context Learning\n\n[93] NICE  To Optimize In-Context Examples or Not \n\n[94] In-Context Learning Learns Label Relationships but Is Not Conventional  Learning\n\n[95] A Survey of Human-in-the-loop for Machine Learning\n\n[96] Learning Context-aware Task Reasoning for Efficient Meta-reinforcement  Learning\n\n[97] CMML  Contextual Modulation Meta Learning for Cold-Start Recommendation\n\n[98] OpenICL  An Open-Source Framework for In-context Learning\n\n[99] Enhancing In-context Learning via Linear Probe Calibration\n\n[100] In-Context Learning for Few-Shot Dialogue State Tracking\n\n[101] In-Context Language Learning  Architectures and Algorithms\n\n[102] Large Language Models Can be Lazy Learners  Analyze Shortcuts in  In-Context Learning\n\n[103] Few-shot Fine-tuning vs. In-context Learning  A Fair Comparison and  Evaluation\n\n[104] In-Context Learning Unlocked for Diffusion Models\n\n[105] Towards Multimodal In-Context Learning for Vision & Language Models\n\n[106] TabPFN  A Transformer That Solves Small Tabular Classification Problems  in a Second\n\n[107] Predicting and interpreting embeddings for out of vocabulary words in  downstream tasks\n\n[108] Modelling continual learning in humans with Hebbian context gating and  exponentially decaying task signals\n\n[109] MLPs Learn In-Context\n\n[110] General-Purpose In-Context Learning by Meta-Learning Transformers\n\n[111] Dynamic Batch Learning in High-Dimensional Sparse Linear Contextual  Bandits\n\n[112] Global Context Networks\n\n[113] Is the Red Square Big  MALeViC  Modeling Adjectives Leveraging Visual  Contexts\n\n[114] Semantic Labeling Using a Deep Contextualized Language Model\n\n[115] Instruction Induction  From Few Examples to Natural Language Task  Descriptions\n\n[116] On the Relation between Sensitivity and Accuracy in In-context Learning\n\n[117] Naive Bayes-based Context Extension for Large Language Models\n\n[118] Can I trust you more  Model-Agnostic Hierarchical Explanations\n\n[119] Learning Large-scale Neural Fields via Context Pruned Meta-Learning\n\n[120] On the Effect of Pretraining Corpora on In-context Learning by a  Large-scale Language Model\n\n[121] ConTextual  Evaluating Context-Sensitive Text-Rich Visual Reasoning in  Large Multimodal Models\n\n[122] Unlocking Instructive In-Context Learning with Tabular Prompting for  Relational Triple Extraction\n\n[123] Wandering Within a World  Online Contextualized Few-Shot Learning\n\n[124] Open Problem  Model Selection for Contextual Bandits\n\n[125] Exploring Kernel Functions in the Softmax Layer for Contextual Word  Classification\n\n[126] Is attention required for ICL  Exploring the Relationship Between Model  Architecture and In-Context Learning Ability\n\n[127] TuneTables  Context Optimization for Scalable Prior-Data Fitted Networks\n\n[128] Asymptotic theory of in-context learning by linear attention\n\n[129] Unlearnable Algorithms for In-context Learning\n\n[130] Pre-Training to Learn in Context\n\n[131] Learning To Retrieve Prompts for In-Context Learning\n\n[132] Linking In-context Learning in Transformers to Human Episodic Memory\n\n",
    "reference": {
        "1": "2101.06804v1",
        "2": "2111.02080v6",
        "3": "2208.01066v3",
        "4": "2212.10559v3",
        "5": "2302.05698v3",
        "6": "2404.18191v2",
        "7": "1810.03642v4",
        "8": "2110.15943v2",
        "9": "2303.03846v2",
        "10": "2407.07011v1",
        "11": "2212.09865v2",
        "12": "2402.11447v1",
        "13": "2404.11018v1",
        "14": "2402.02160v2",
        "15": "2401.11624v5",
        "16": "2402.11254v1",
        "17": "2303.07971v1",
        "18": "2306.04637v2",
        "19": "2205.05055v6",
        "20": "2402.02364v1",
        "21": "2402.08674v1",
        "22": "2403.13164v1",
        "23": "2305.19420v2",
        "24": "2211.15661v3",
        "25": "2312.03002v1",
        "26": "2303.07895v1",
        "27": "1901.03415v2",
        "28": "2305.19148v3",
        "29": "2305.09731v1",
        "30": "2311.06668v3",
        "31": "2406.00793v1",
        "32": "2408.03307v1",
        "33": "2209.11895v1",
        "34": "1301.3781v3",
        "35": "2409.10559v1",
        "36": "2310.15213v2",
        "37": "2304.13276v1",
        "38": "2301.07067v2",
        "39": "2405.19592v1",
        "40": "2212.09095v2",
        "41": "2311.08360v3",
        "42": "2401.06469v2",
        "43": "2311.00871v1",
        "44": "2403.11834v1",
        "45": "2310.17086v1",
        "46": "2402.17671v1",
        "47": "2405.02462v2",
        "48": "2212.06800v3",
        "49": "2311.07811v2",
        "50": "2310.08391v2",
        "51": "2306.15063v2",
        "52": "2401.15530v1",
        "53": "2305.11170v1",
        "54": "2405.10738v2",
        "55": "2408.12186v1",
        "56": "2305.04835v3",
        "57": "2403.06586v1",
        "58": "2302.11042v2",
        "59": "2310.15916v1",
        "60": "2212.10375v2",
        "61": "2305.14128v1",
        "62": "2301.11916v4",
        "63": "2404.15736v2",
        "64": "2405.17062v2",
        "65": "2402.05515v2",
        "66": "2006.06896v1",
        "67": "2403.04233v1",
        "68": "2404.15420v1",
        "69": "2402.06599v1",
        "70": "2208.03462v2",
        "71": "2406.11233v2",
        "72": "2306.04891v2",
        "73": "1710.07395v2",
        "74": "2406.05207v1",
        "75": "2310.10616v1",
        "76": "2402.02872v1",
        "77": "2312.01552v1",
        "78": "2305.14160v4",
        "79": "2005.04611v1",
        "80": "2207.00747v1",
        "81": "2307.06945v3",
        "82": "2205.12685v2",
        "83": "2303.13824v1",
        "84": "2212.10378v2",
        "85": "2402.10738v1",
        "86": "2405.00200v1",
        "87": "1909.03834v2",
        "88": "2305.13299v1",
        "89": "2406.03768v1",
        "90": "2312.13772v2",
        "91": "2105.08532v3",
        "92": "2310.00647v2",
        "93": "2402.06733v2",
        "94": "2307.12375v4",
        "95": "2108.00941v3",
        "96": "2003.01373v2",
        "97": "2108.10511v4",
        "98": "2303.02913v1",
        "99": "2401.12406v1",
        "100": "2203.08568v3",
        "101": "2401.12973v2",
        "102": "2305.17256v2",
        "103": "2305.16938v2",
        "104": "2305.01115v2",
        "105": "2403.12736v1",
        "106": "2207.01848v6",
        "107": "1903.00724v1",
        "108": "2203.11560v2",
        "109": "2405.15618v1",
        "110": "2212.04458v2",
        "111": "2008.11918v4",
        "112": "2012.13375v1",
        "113": "1908.10285v1",
        "114": "2010.16037v1",
        "115": "2205.10782v1",
        "116": "2209.07661v3",
        "117": "2403.17552v1",
        "118": "1812.04801v1",
        "119": "2302.00617v3",
        "120": "2204.13509v2",
        "121": "2401.13311v1",
        "122": "2402.13741v1",
        "123": "2007.04546v3",
        "124": "2006.10940v1",
        "125": "1910.12554v1",
        "126": "2310.08049v3",
        "127": "2402.11137v2",
        "128": "2405.11751v1",
        "129": "2402.00751v1",
        "130": "2305.09137v1",
        "131": "2112.08633v2",
        "132": "2405.14992v1"
    },
    "retrieveref": {
        "1": "2311.00237v2",
        "2": "2402.10424v1",
        "3": "2303.07895v1",
        "4": "2307.12375v4",
        "5": "2405.18202v1",
        "6": "2405.01116v1",
        "7": "2208.01066v3",
        "8": "2211.15661v3",
        "9": "2305.14171v3",
        "10": "2406.12785v1",
        "11": "2304.04748v1",
        "12": "2310.12300v2",
        "13": "2405.15618v1",
        "14": "2311.03498v2",
        "15": "2305.14622v1",
        "16": "2212.10375v2",
        "17": "2405.10738v2",
        "18": "2406.13493v1",
        "19": "2305.04320v2",
        "20": "2405.15115v1",
        "21": "2111.02080v6",
        "22": "2312.00351v2",
        "23": "2402.11004v1",
        "24": "1508.04221v1",
        "25": "2312.03002v1",
        "26": "2401.12097v2",
        "27": "2406.00131v1",
        "28": "2311.03648v1",
        "29": "2310.15916v1",
        "30": "1707.04218v1",
        "31": "2406.11890v1",
        "32": "2405.00200v1",
        "33": "2310.12477v1",
        "34": "2302.05698v3",
        "35": "2407.17011v1",
        "36": "2403.11834v1",
        "37": "2404.14716v2",
        "38": "2404.14716v1",
        "39": "2408.12959v1",
        "40": "2406.14022v1",
        "41": "2402.02872v1",
        "42": "2404.15736v2",
        "43": "2305.09731v1",
        "44": "2205.05055v6",
        "45": "2311.18021v1",
        "46": "2312.02520v2",
        "47": "2302.05011v1",
        "48": "2402.11447v1",
        "49": "2401.06469v2",
        "50": "2403.06402v1",
        "51": "2306.04637v2",
        "52": "2312.06592v1",
        "53": "2305.16704v1",
        "54": "2401.12178v1",
        "55": "1906.02685v2",
        "56": "2305.03573v1",
        "57": "2407.15487v1",
        "58": "2112.08633v2",
        "59": "2306.01311v1",
        "60": "2406.16535v1",
        "61": "2309.10954v2",
        "62": "2406.13185v1",
        "63": "2406.03768v1",
        "64": "2307.15411v2",
        "65": "2001.03152v2",
        "66": "1912.06679v3",
        "67": "2409.04318v1",
        "68": "2305.12766v2",
        "69": "2306.08659v2",
        "70": "2212.04458v2",
        "71": "2310.03331v1",
        "72": "2406.13131v2",
        "73": "2406.05207v1",
        "74": "1903.04715v1",
        "75": "2402.08674v1",
        "76": "2208.03462v2",
        "77": "2406.11233v2",
        "78": "2402.13741v1",
        "79": "2306.04891v2",
        "80": "2110.15943v2",
        "81": "2405.15279v1",
        "82": "2311.02879v1",
        "83": "2305.12907v1",
        "84": "2402.02212v1",
        "85": "2110.04042v1",
        "86": "2305.13775v1",
        "87": "2408.10147v1",
        "88": "2401.12973v2",
        "89": "2405.16156v1",
        "90": "2406.13274v1",
        "91": "2303.02913v1",
        "92": "2401.06766v2",
        "93": "2404.12866v1",
        "94": "2406.17790v1",
        "95": "2403.17552v1",
        "96": "2402.02364v1",
        "97": "2401.12087v1",
        "98": "2405.14660v1",
        "99": "1711.06379v3",
        "100": "2403.13164v1",
        "101": "2407.19346v1",
        "102": "2406.16007v1",
        "103": "2405.17234v6",
        "104": "2311.06668v3",
        "105": "2401.15530v1",
        "106": "2405.11465v1",
        "107": "2401.11624v5",
        "108": "2312.13772v2",
        "109": "2305.09137v1",
        "110": "2312.07476v2",
        "111": "1912.12735v1",
        "112": "2403.09428v2",
        "113": "1901.03415v2",
        "114": "2402.11574v1",
        "115": "2402.11639v1",
        "116": "2305.14264v2",
        "117": "2310.15047v2",
        "118": "2308.08780v2",
        "119": "2402.14951v1",
        "120": "2305.14502v2",
        "121": "2302.11042v2",
        "122": "2406.02550v1",
        "123": "1904.04406v1",
        "124": "2403.07407v1",
        "125": "2303.07971v1",
        "126": "2310.10616v1",
        "127": "2302.13539v3",
        "128": "2407.00902v1",
        "129": "1803.08794v1",
        "130": "2402.11254v1",
        "131": "2305.14160v4",
        "132": "2403.12736v1",
        "133": "2404.11018v1",
        "134": "2310.13220v1",
        "135": "2311.09649v2",
        "136": "2304.13276v1",
        "137": "2310.08863v1",
        "138": "2407.19752v1",
        "139": "2406.04216v3",
        "140": "2402.11750v1",
        "141": "2110.04541v3",
        "142": "1805.12183v1",
        "143": "2405.10512v1",
        "144": "2402.00743v1",
        "145": "2306.15091v1",
        "146": "2312.01771v1",
        "147": "2303.05063v4",
        "148": "2310.02954v5",
        "149": "2409.11147v1",
        "150": "1703.06246v3",
        "151": "2307.07164v2",
        "152": "2010.09066v1",
        "153": "2402.13874v2",
        "154": "2310.10266v1",
        "155": "2403.09703v1",
        "156": "2309.09888v2",
        "157": "2405.16124v1",
        "158": "2303.13824v1",
        "159": "2407.05693v2",
        "160": "2305.19420v2",
        "161": "2407.02028v1",
        "162": "1604.07379v2",
        "163": "2406.15334v1",
        "164": "2405.17062v2",
        "165": "2310.05109v1",
        "166": "2305.14907v3",
        "167": "2310.08309v1",
        "168": "2405.12217v1",
        "169": "2405.19874v1",
        "170": "1608.00525v1",
        "171": "1904.01464v3",
        "172": "2305.17040v1",
        "173": "2312.03703v1",
        "174": "2304.01922v1",
        "175": "2401.06301v1",
        "176": "1505.05192v3",
        "177": "2310.10638v5",
        "178": "2302.07346v1",
        "179": "2310.10971v2",
        "180": "2309.07900v2",
        "181": "2403.04197v2",
        "182": "2407.10233v1",
        "183": "2305.04835v3",
        "184": "2305.14210v2",
        "185": "2212.01692v4",
        "186": "2204.06214v1",
        "187": "2301.11916v4",
        "188": "2212.06713v1",
        "189": "2310.05066v2",
        "190": "2312.03801v1",
        "191": "1609.02948v1",
        "192": "2305.14800v6",
        "193": "2003.07278v2",
        "194": "2211.05632v2",
        "195": "1910.08438v1",
        "196": "2402.05515v2",
        "197": "2312.01408v1",
        "198": "2309.16656v1",
        "199": "2408.03307v1",
        "200": "2310.03016v1",
        "201": "2311.09619v2",
        "202": "2403.04233v1",
        "203": "2212.09095v2",
        "204": "2309.14771v2",
        "205": "2312.07405v1",
        "206": "2401.12406v1",
        "207": "2305.14128v1",
        "208": "2406.00053v2",
        "209": "2311.07772v4",
        "210": "2406.10432v2",
        "211": "2305.01115v2",
        "212": "2404.17807v1",
        "213": "2407.07356v1",
        "214": "2406.00793v1",
        "215": "1807.02110v1",
        "216": "2404.07546v1",
        "217": "2404.17809v1",
        "218": "2405.10548v3",
        "219": "2312.13286v1",
        "220": "2204.13509v2",
        "221": "2305.19148v3",
        "222": "2306.01667v2",
        "223": "2307.14856v1",
        "224": "2402.10644v1",
        "225": "2305.12600v1",
        "226": "2402.10738v1",
        "227": "2406.04823v1",
        "228": "2211.04486v1",
        "229": "2311.00226v2",
        "230": "2209.11895v1",
        "231": "2405.13396v1",
        "232": "2305.18869v2",
        "233": "2305.17256v2",
        "234": "2403.09488v3",
        "235": "2310.17639v3",
        "236": "2409.04831v1",
        "237": "2404.12957v1",
        "238": "2402.17971v2",
        "239": "2403.19285v1",
        "240": "2209.07661v3",
        "241": "1901.09115v1",
        "242": "1910.05577v4",
        "243": "2210.11233v1",
        "244": "2307.14632v1",
        "245": "2405.11446v1",
        "246": "2308.13380v2",
        "247": "2310.08049v3",
        "248": "2309.07915v3",
        "249": "2312.04509v1",
        "250": "2310.08391v2",
        "251": "2407.07011v1",
        "252": "2303.03846v2",
        "253": "2311.13120v3",
        "254": "2311.17083v1",
        "255": "2405.10316v1",
        "256": "1906.07108v1",
        "257": "2308.09985v1",
        "258": "2408.11546v1",
        "259": "1907.03609v1",
        "260": "2305.18499v2",
        "261": "2307.13916v3",
        "262": "2310.20046v1",
        "263": "2311.17041v2",
        "264": "2407.03076v1",
        "265": "2305.19402v2",
        "266": "2212.10670v1",
        "267": "2408.11852v1",
        "268": "1810.01256v3",
        "269": "2405.17264v1",
        "270": "2003.02681v1",
        "271": "2407.00100v1",
        "272": "2311.06101v2",
        "273": "1611.00483v2",
        "274": "2406.02911v1",
        "275": "2408.13028v1",
        "276": "2310.07579v2",
        "277": "2303.09366v2",
        "278": "2006.11706v2",
        "279": "2402.01293v2",
        "280": "2404.18191v2",
        "281": "2105.08532v3",
        "282": "2312.16549v1",
        "283": "1711.03483v1",
        "284": "2005.14707v3",
        "285": "1808.06289v1",
        "286": "1810.03642v4",
        "287": "2111.13850v2",
        "288": "2401.01857v1",
        "289": "2301.07067v2",
        "290": "2406.03730v1",
        "291": "2404.12352v1",
        "292": "2403.16512v2",
        "293": "2102.10437v1",
        "294": "2310.08540v4",
        "295": "1710.04975v3",
        "296": "2311.07811v2",
        "297": "2311.08360v3",
        "298": "2203.08410v3",
        "299": "2312.17055v1",
        "300": "1804.05936v2",
        "301": "2404.07775v1",
        "302": "2205.12685v2",
        "303": "2409.01930v1",
        "304": "2307.02419v1",
        "305": "1906.01514v1",
        "306": "2406.02847v2",
        "307": "2002.02775v1",
        "308": "2305.11170v1",
        "309": "2403.11631v1",
        "310": "2306.15063v2",
        "311": "2403.19283v1",
        "312": "1502.01418v2",
        "313": "2307.01137v1",
        "314": "2406.01224v1",
        "315": "1706.02496v1",
        "316": "2408.12186v1",
        "317": "2207.01848v6",
        "318": "2409.00263v1",
        "319": "2405.14899v1",
        "320": "2311.09782v2",
        "321": "1908.01819v1",
        "322": "2312.08519v2",
        "323": "1411.3815v6",
        "324": "2408.02103v1",
        "325": "2402.02160v2",
        "326": "1301.3781v3",
        "327": "2403.06826v1",
        "328": "2306.13053v2",
        "329": "2312.15918v2",
        "330": "2407.05566v1",
        "331": "2312.01571v1",
        "332": "2402.12817v1",
        "333": "2312.04083v1",
        "334": "2307.01201v1",
        "335": "2102.11031v1",
        "336": "2310.08923v1",
        "337": "2106.10816v2",
        "338": "2305.13299v1",
        "339": "2306.04508v1",
        "340": "1909.03999v2",
        "341": "2406.14955v1",
        "342": "2210.05758v1",
        "343": "1901.10860v4",
        "344": "2406.07970v3",
        "345": "2312.02614v2",
        "346": "2311.09606v2",
        "347": "2405.17587v2",
        "348": "2311.10367v1",
        "349": "2404.11225v1",
        "350": "2307.14063v1",
        "351": "2208.04707v1",
        "352": "2311.03319v1",
        "353": "2212.10559v3",
        "354": "1703.06408v1",
        "355": "2108.10395v1",
        "356": "2405.11002v1",
        "357": "2212.10378v2",
        "358": "2206.08082v1",
        "359": "2307.06945v3",
        "360": "2408.02288v1",
        "361": "2406.10908v3",
        "362": "2402.07762v1",
        "363": "1202.1334v2",
        "364": "2406.06699v1",
        "365": "2408.00427v2",
        "366": "2302.00617v3",
        "367": "2312.03987v1",
        "368": "2309.17249v2",
        "369": "2312.06363v2",
        "370": "1906.02329v1",
        "371": "2312.07553v1",
        "372": "2403.06914v2",
        "373": "1911.09728v1",
        "374": "2405.05116v2",
        "375": "2311.11551v1",
        "376": "2206.04180v1",
        "377": "2311.01949v2",
        "378": "2310.19572v1",
        "379": "2305.17262v3",
        "380": "2201.13287v1",
        "381": "2407.10005v1",
        "382": "2405.04960v2",
        "383": "2006.15194v1",
        "384": "1806.03084v1",
        "385": "2402.04248v2",
        "386": "1710.04981v3",
        "387": "2202.06557v1",
        "388": "1608.05267v3",
        "389": "2405.09798v1",
        "390": "2301.05031v1",
        "391": "2406.08973v1",
        "392": "1902.00163v2",
        "393": "2202.12837v2",
        "394": "2402.07738v2",
        "395": "1705.08618v1",
        "396": "2305.11038v3",
        "397": "1807.07428v1",
        "398": "2405.16819v1",
        "399": "2402.00751v1",
        "400": "2111.04308v1",
        "401": "2203.00995v2",
        "402": "2403.04510v1",
        "403": "2007.14658v2",
        "404": "2403.16578v2",
        "405": "2405.13861v3",
        "406": "1907.04233v1",
        "407": "2402.07817v1",
        "408": "2402.15700v1",
        "409": "2212.02499v2",
        "410": "2009.06371v3",
        "411": "1807.06473v3",
        "412": "1911.07349v3",
        "413": "1710.04344v1",
        "414": "2407.01983v1",
        "415": "2402.05188v1",
        "416": "1904.12638v2",
        "417": "2406.17534v2",
        "418": "2405.11145v3",
        "419": "2311.09948v1",
        "420": "2402.03170v2",
        "421": "2406.11629v4",
        "422": "2311.09519v2",
        "423": "2402.16061v2",
        "424": "1601.00893v2",
        "425": "2406.14739v1",
        "426": "2312.16262v1",
        "427": "1906.02534v1",
        "428": "2303.00788v1",
        "429": "1712.01892v2",
        "430": "2405.18193v1",
        "431": "2311.09579v2",
        "432": "1802.04064v5",
        "433": "2012.07138v1",
        "434": "1810.12348v3",
        "435": "1312.5697v2",
        "436": "2405.14982v1",
        "437": "1802.00981v4",
        "438": "2210.14215v1",
        "439": "2107.10236v1",
        "440": "2406.08423v1",
        "441": "2402.15607v1",
        "442": "2105.02726v2",
        "443": "2212.10873v3",
        "444": "2406.04756v1",
        "445": "2310.15987v1",
        "446": "2310.00647v2",
        "447": "2303.11633v1",
        "448": "2211.12817v2",
        "449": "2010.12353v1",
        "450": "2406.01424v1",
        "451": "2405.02710v1",
        "452": "2403.11904v2",
        "453": "2309.06054v2",
        "454": "1509.02470v1",
        "455": "1607.08329v3",
        "456": "2008.05723v1",
        "457": "2206.11851v1",
        "458": "2307.00259v2",
        "459": "2402.12976v1",
        "460": "2311.00871v1",
        "461": "1711.03688v2",
        "462": "2101.09791v3",
        "463": "2311.00863v1",
        "464": "2310.10873v2",
        "465": "2008.04545v1",
        "466": "2404.11585v1",
        "467": "2409.04759v1",
        "468": "2408.00397v1",
        "469": "2405.15318v1",
        "470": "2212.02437v1",
        "471": "2405.17248v1",
        "472": "2103.04181v1",
        "473": "2310.19112v2",
        "474": "2002.00652v2",
        "475": "2406.14546v1",
        "476": "2212.09429v1",
        "477": "2010.04314v1",
        "478": "2303.08119v3",
        "479": "2305.04530v1",
        "480": "2403.12768v1",
        "481": "2408.00144v1",
        "482": "2404.03558v1",
        "483": "2103.16210v1",
        "484": "2109.05712v1",
        "485": "2305.13016v2",
        "486": "1502.06665v1",
        "487": "2312.14254v1",
        "488": "1907.09478v1",
        "489": "2306.04763v1",
        "490": "2409.01389v1",
        "491": "1301.7408v1",
        "492": "2305.10163v4",
        "493": "2405.11751v1",
        "494": "2111.12296v2",
        "495": "1704.02998v2",
        "496": "2105.03654v3",
        "497": "2203.05557v2",
        "498": "2005.12880v2",
        "499": "2305.05940v3",
        "500": "2402.05723v1",
        "501": "2406.18406v1",
        "502": "2312.04021v4",
        "503": "2205.15219v3",
        "504": "1707.01521v1",
        "505": "2407.11300v1",
        "506": "2004.00413v1",
        "507": "1502.00527v1",
        "508": "2311.08324v2",
        "509": "2102.06177v2",
        "510": "2404.07078v1",
        "511": "1608.01946v1",
        "512": "2401.03149v2",
        "513": "1811.08853v1",
        "514": "1506.05514v1",
        "515": "1706.07684v1",
        "516": "1904.09320v3",
        "517": "2310.04782v1",
        "518": "1906.02479v2",
        "519": "2406.07081v1",
        "520": "1912.00501v1",
        "521": "2309.14681v4",
        "522": "2407.05682v1",
        "523": "1909.00531v1",
        "524": "2006.10940v1",
        "525": "2401.15120v2",
        "526": "2305.15035v2",
        "527": "1608.05852v1",
        "528": "2407.16516v1",
        "529": "2302.00878v4",
        "530": "2308.13392v2",
        "531": "1705.03821v2",
        "532": "2212.02216v1",
        "533": "2311.10998v1",
        "534": "2402.12530v1",
        "535": "2307.10824v1",
        "536": "2107.12025v1",
        "537": "2210.15828v1",
        "538": "2308.06912v3",
        "539": "2406.18501v1",
        "540": "2002.04275v1",
        "541": "2110.13223v1",
        "542": "1608.05528v3",
        "543": "1908.03141v1",
        "544": "2110.00452v3",
        "545": "2405.02462v2",
        "546": "2003.08485v1",
        "547": "2210.13522v1",
        "548": "2408.07790v1",
        "549": "2109.02995v1",
        "550": "2406.01976v1",
        "551": "1612.02534v1",
        "552": "2311.03551v1",
        "553": "2011.02604v2",
        "554": "2010.00767v3",
        "555": "1809.09582v3",
        "556": "2112.03371v1",
        "557": "2008.07087v1",
        "558": "2205.09899v1",
        "559": "1912.12290v2",
        "560": "2310.17191v1",
        "561": "2407.15341v1",
        "562": "1912.06876v1",
        "563": "2305.12740v1",
        "564": "2302.11521v1",
        "565": "2305.11070v1",
        "566": "1812.01880v1",
        "567": "1709.08294v3",
        "568": "2104.13874v2",
        "569": "1909.11142v3",
        "570": "2306.14892v1",
        "571": "2407.05916v1",
        "572": "1503.02357v2",
        "573": "1912.05845v3",
        "574": "2202.05930v1",
        "575": "2409.00124v2",
        "576": "2203.08774v1",
        "577": "2209.01975v1",
        "578": "1904.04985v1",
        "579": "2310.06675v2",
        "580": "2110.04127v1",
        "581": "2407.06955v1",
        "582": "1611.05369v1",
        "583": "2104.13582v1",
        "584": "2309.12727v1",
        "585": "1912.10604v1",
        "586": "1509.01287v1",
        "587": "1705.04358v2",
        "588": "1907.04924v1",
        "589": "2306.15169v1",
        "590": "2307.13903v4",
        "591": "2408.04872v2",
        "592": "1805.04623v1",
        "593": "2010.08750v1",
        "594": "2003.01704v3",
        "595": "2304.14114v2",
        "596": "2305.13059v2",
        "597": "2304.11015v3",
        "598": "2403.06126v1",
        "599": "2312.03584v1",
        "600": "2305.05314v2",
        "601": "1807.06414v1",
        "602": "2307.00910v2",
        "603": "2005.10084v4",
        "604": "2203.01849v1",
        "605": "2208.00203v1",
        "606": "2402.05403v2",
        "607": "1905.08300v1",
        "608": "2308.01313v3",
        "609": "2407.04489v1",
        "610": "1711.08278v1",
        "611": "2407.17689v1",
        "612": "2305.04151v2",
        "613": "2306.05963v2",
        "614": "2303.01494v1",
        "615": "2205.07683v1",
        "616": "2001.03277v2",
        "617": "1702.06672v1",
        "618": "2403.06586v1",
        "619": "2212.06800v3",
        "620": "2405.14992v1",
        "621": "2407.19089v1",
        "622": "2305.03500v1",
        "623": "2306.06615v2",
        "624": "1808.04151v1",
        "625": "2204.09303v1",
        "626": "1405.7711v1",
        "627": "1508.03326v2",
        "628": "2402.01182v1",
        "629": "1803.00386v2",
        "630": "2207.06030v3",
        "631": "2406.11474v1",
        "632": "1801.01750v1",
        "633": "1906.02448v2",
        "634": "2407.00664v1",
        "635": "1309.3809v1",
        "636": "2305.01639v2",
        "637": "2307.02690v1",
        "638": "2311.07099v1",
        "639": "2402.07386v1",
        "640": "2212.12395v3",
        "641": "1905.04425v1",
        "642": "2409.01380v1",
        "643": "2405.18626v2",
        "644": "2005.14662v1",
        "645": "2409.00301v1",
        "646": "2407.08801v1",
        "647": "2204.05535v1",
        "648": "2311.12538v2",
        "649": "2204.08758v1",
        "650": "1609.04331v1",
        "651": "2310.15213v2",
        "652": "1910.12554v1",
        "653": "2102.03586v4",
        "654": "2305.14105v2",
        "655": "1202.2112v1",
        "656": "2310.17342v1",
        "657": "2401.13311v1",
        "658": "2110.02204v2",
        "659": "1809.06179v1",
        "660": "1611.05520v2",
        "661": "2404.02452v1",
        "662": "2301.04799v1",
        "663": "2402.13055v1",
        "664": "2405.14301v1",
        "665": "2005.01483v1",
        "666": "1811.08600v1",
        "667": "2109.01134v6",
        "668": "2402.15637v1",
        "669": "1902.04484v1",
        "670": "1503.00787v1",
        "671": "2104.04434v1",
        "672": "1711.08590v5",
        "673": "2406.07457v1",
        "674": "2003.01922v2",
        "675": "2408.11505v1",
        "676": "2304.08862v2",
        "677": "2404.19094v2",
        "678": "2112.05181v2",
        "679": "2104.02215v2",
        "680": "2404.02060v2",
        "681": "2402.06599v1",
        "682": "1402.0555v2",
        "683": "2009.08457v2",
        "684": "2407.16695v1",
        "685": "2303.10093v2",
        "686": "2402.06971v1",
        "687": "2402.11137v2",
        "688": "2402.18510v2",
        "689": "2307.05052v4",
        "690": "2009.05105v2",
        "691": "1605.01478v1",
        "692": "2003.11696v2",
        "693": "1809.09741v1",
        "694": "2405.19592v1",
        "695": "2305.14739v1",
        "696": "2101.11560v4",
        "697": "1710.02603v2",
        "698": "1802.08790v1",
        "699": "2305.18485v2",
        "700": "2310.11340v1",
        "701": "2309.13205v1",
        "702": "2409.10559v1",
        "703": "2409.14673v1",
        "704": "2404.10357v2",
        "705": "2409.15700v1",
        "706": "2407.05898v1",
        "707": "1808.08766v1",
        "708": "2408.08134v1",
        "709": "2004.08013v1",
        "710": "2305.18279v1",
        "711": "2211.02676v4",
        "712": "2201.10069v2",
        "713": "2409.15867v2",
        "714": "2007.05059v3",
        "715": "2403.06495v3",
        "716": "1609.05787v1",
        "717": "2007.04750v2",
        "718": "1607.00548v1",
        "719": "2309.04802v3",
        "720": "2407.09375v2",
        "721": "1709.00141v1",
        "722": "2403.09616v1",
        "723": "1911.05781v3",
        "724": "2210.04209v1",
        "725": "2402.05787v1",
        "726": "1507.02186v2",
        "727": "2402.00858v1",
        "728": "2409.06338v1",
        "729": "2207.01844v1",
        "730": "1706.02807v2",
        "731": "2011.10857v1",
        "732": "2205.10782v1",
        "733": "1901.08159v1",
        "734": "2011.00797v1",
        "735": "2006.06896v1",
        "736": "2204.09885v2",
        "737": "2402.01416v1",
        "738": "2106.10776v1",
        "739": "2007.14900v3",
        "740": "2309.04790v1",
        "741": "2006.03217v3",
        "742": "1511.08177v1",
        "743": "2409.09704v1",
        "744": "1702.01466v1",
        "745": "2210.11616v1",
        "746": "2407.04963v1",
        "747": "2003.02738v1",
        "748": "2211.07122v1",
        "749": "2210.01908v3",
        "750": "2401.13650v1",
        "751": "2207.06603v1",
        "752": "1704.05781v1",
        "753": "2003.06692v1",
        "754": "1712.01653v2",
        "755": "1810.03449v1",
        "756": "2408.00041v1",
        "757": "2305.14802v2",
        "758": "1701.02870v3",
        "759": "2402.03379v1",
        "760": "2406.01808v1",
        "761": "1912.07274v2",
        "762": "2405.07623v1",
        "763": "2402.12091v1",
        "764": "2202.12597v1",
        "765": "2212.00301v3",
        "766": "1909.00848v1",
        "767": "2203.15867v1",
        "768": "2105.02873v1",
        "769": "1804.09398v3",
        "770": "2311.06595v3",
        "771": "2212.11385v1",
        "772": "2405.02712v1",
        "773": "1807.11582v2",
        "774": "2307.01453v1",
        "775": "2007.04458v1",
        "776": "2405.07220v1",
        "777": "2405.17915v1",
        "778": "2108.01343v3",
        "779": "2406.14596v1",
        "780": "2403.02495v1",
        "781": "1611.09900v1",
        "782": "2401.10044v2",
        "783": "1401.4529v2",
        "784": "1903.06187v3",
        "785": "1412.4271v2",
        "786": "2202.00805v3",
        "787": "2407.14916v1",
        "788": "1911.05960v1",
        "789": "2007.10143v1",
        "790": "1903.10427v1",
        "791": "2406.10878v1",
        "792": "2406.18027v1",
        "793": "2002.05640v2",
        "794": "2305.02105v3",
        "795": "1910.00294v1",
        "796": "2108.07387v1",
        "797": "2311.06555v2",
        "798": "2408.07505v2",
        "799": "2301.09870v2",
        "800": "2101.06804v1",
        "801": "2407.12879v2",
        "802": "1611.07218v4",
        "803": "2009.10542v1",
        "804": "2312.10771v1",
        "805": "1312.6168v3",
        "806": "1706.04687v2",
        "807": "2108.10511v4",
        "808": "1308.3541v2",
        "809": "2406.19598v1",
        "810": "2009.13891v3",
        "811": "2401.17390v2",
        "812": "2103.01566v2",
        "813": "2311.00587v2",
        "814": "2106.02246v2",
        "815": "2010.02649v1",
        "816": "1805.09039v9",
        "817": "1909.06076v2",
        "818": "2407.11188v1",
        "819": "2210.01881v1",
        "820": "2305.09481v1",
        "821": "2401.05949v4",
        "822": "2304.03284v1",
        "823": "2210.13964v2",
        "824": "2303.06946v1",
        "825": "2402.12195v1",
        "826": "2004.02194v1",
        "827": "2008.04702v1",
        "828": "1409.7729v1",
        "829": "1911.01664v1",
        "830": "2209.01404v1",
        "831": "2208.10226v2",
        "832": "2204.03508v2",
        "833": "2409.06285v1",
        "834": "2208.14195v1",
        "835": "2303.13217v3",
        "836": "2405.01002v2",
        "837": "1911.04286v1",
        "838": "2202.01914v1",
        "839": "2401.06390v1",
        "840": "2104.03781v1",
        "841": "1506.03374v2",
        "842": "2004.08107v3",
        "843": "1904.08109v1",
        "844": "2202.00867v1",
        "845": "1607.03182v1",
        "846": "2107.12960v2",
        "847": "2307.00586v3",
        "848": "2007.06368v2",
        "849": "1607.01149v1",
        "850": "2405.09369v3",
        "851": "2203.09694v1",
        "852": "2404.19553v1",
        "853": "2407.06230v1",
        "854": "2305.10613v3",
        "855": "2405.07467v1",
        "856": "2207.09068v5",
        "857": "1609.03490v1",
        "858": "1906.05468v1",
        "859": "2211.13892v2",
        "860": "2005.00619v5",
        "861": "2407.10303v1",
        "862": "1606.05378v1",
        "863": "2206.04305v1",
        "864": "2310.17086v1",
        "865": "1909.00512v1",
        "866": "1602.02454v1",
        "867": "2310.04680v1",
        "868": "1710.07395v2",
        "869": "1810.07371v2",
        "870": "2010.12827v2",
        "871": "1909.02117v1",
        "872": "1803.07737v2",
        "873": "1911.06164v1",
        "874": "2010.12247v2",
        "875": "2310.05249v1",
        "876": "2311.07230v1",
        "877": "2011.13782v2",
        "878": "2407.01887v1",
        "879": "2007.04546v3",
        "880": "1409.8191v1",
        "881": "2404.05538v2",
        "882": "1907.01637v1",
        "883": "1712.00489v1",
        "884": "1604.04048v1",
        "885": "2404.12702v1",
        "886": "2304.00354v2",
        "887": "2408.09655v1",
        "888": "2310.12238v1",
        "889": "2107.13327v1",
        "890": "2406.14208v1",
        "891": "2405.19226v1",
        "892": "2405.02750v1",
        "893": "1809.02492v3",
        "894": "2408.15914v1",
        "895": "2408.09743v1",
        "896": "2406.02547v1",
        "897": "2401.16638v1",
        "898": "1911.05715v1",
        "899": "1905.00982v1",
        "900": "2310.00178v1",
        "901": "2006.01488v1",
        "902": "2302.04931v1",
        "903": "2204.05449v1",
        "904": "2010.06269v2",
        "905": "2206.05404v3",
        "906": "2309.04158v1",
        "907": "2301.06825v1",
        "908": "2305.16938v2",
        "909": "2304.08479v1",
        "910": "2106.05110v1",
        "911": "2105.13465v1",
        "912": "2106.13895v1",
        "913": "2305.01470v1",
        "914": "2209.04471v1",
        "915": "2007.04782v1",
        "916": "2312.07636v1",
        "917": "2406.07393v2",
        "918": "2405.15984v2",
        "919": "2312.12655v2",
        "920": "2402.08570v1",
        "921": "1811.00232v2",
        "922": "2406.02056v1",
        "923": "2209.07836v1",
        "924": "2401.01578v1",
        "925": "2007.07306v2",
        "926": "2105.03482v2",
        "927": "2303.09390v1",
        "928": "2009.06265v1",
        "929": "2102.04214v1",
        "930": "2109.10602v1",
        "931": "1904.04084v1",
        "932": "2310.13961v1",
        "933": "2402.09390v1",
        "934": "2301.09209v4",
        "935": "2312.15971v1",
        "936": "2004.06321v1",
        "937": "1505.01757v1",
        "938": "2406.12331v1",
        "939": "2403.05681v1",
        "940": "2409.17080v1",
        "941": "2008.11918v4",
        "942": "2004.01351v1",
        "943": "2106.09241v1",
        "944": "2403.16204v1",
        "945": "1708.02349v1",
        "946": "2405.02501v2",
        "947": "2202.04500v2",
        "948": "2302.09263v1",
        "949": "2312.12275v2",
        "950": "2208.12856v3",
        "951": "2307.11694v2",
        "952": "2012.01780v1",
        "953": "1806.05516v1",
        "954": "1910.00652v3",
        "955": "1803.04033v1",
        "956": "2304.05341v1",
        "957": "1507.02221v1",
        "958": "1903.00884v2",
        "959": "2403.05325v1",
        "960": "1706.06905v2",
        "961": "2311.14671v2",
        "962": "1805.11546v2",
        "963": "1711.07077v4",
        "964": "1708.02561v1",
        "965": "2404.10633v1",
        "966": "2204.03330v2",
        "967": "2406.06119v1",
        "968": "1506.00019v4",
        "969": "2106.14112v1",
        "970": "1701.06725v1",
        "971": "2010.10921v1",
        "972": "2008.01338v1",
        "973": "2011.14155v1",
        "974": "2409.01552v1",
        "975": "1902.07802v2",
        "976": "2304.02787v2",
        "977": "1904.01830v1",
        "978": "2108.11629v1",
        "979": "2310.15627v2",
        "980": "2409.04142v1",
        "981": "1702.06675v1",
        "982": "1909.00564v2",
        "983": "2010.01040v1",
        "984": "2211.10688v2",
        "985": "2004.10349v1",
        "986": "1904.11492v1",
        "987": "2308.01231v1",
        "988": "2405.04032v2",
        "989": "2405.19162v1",
        "990": "1910.08192v1",
        "991": "2310.11634v1",
        "992": "1706.07204v1",
        "993": "1812.06707v1",
        "994": "1505.03873v1",
        "995": "2205.04810v1",
        "996": "1412.3397v3",
        "997": "2401.06659v2",
        "998": "2009.05831v2",
        "999": "2306.14451v2",
        "1000": "2409.12293v1"
    }
}