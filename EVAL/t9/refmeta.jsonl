{"paper_id": 218974353, "title": "Exploring Transformer Text Generation for Medical Dataset Augmentation", "author_names": ["A. Amin-Nejad", "Julia Ive", "S. Velupillai"], "venue": "International Conference on Language Resources and Evaluation", "abstract": null, "year": 2020, "publicationdate": "2020-05-01", "externalids": {}, "doi_lower": null}
{"paper_id": 9662636, "title": "Guided Open Vocabulary Image Captioning with Constrained Beam Search", "author_names": ["Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these models in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time, without re-training. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain captioning on MSCOCO (and improved results for in-domain captioning). Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm. We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels.", "year": 2016, "publicationdate": "2016-12-02", "externalids": {"DOI": "10.18653/v1/D17-1098"}, "doi_lower": "10.18653/v1/d17-1098"}
{"paper_id": 226239359, "title": "PET: a Tool for Post-editing and Assessing Machine Translation", "author_names": ["Wilker Aziz", "Lucia Specia"], "venue": "European Association for Machine Translation Conferences/Workshops", "abstract": null, "year": 2012, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 235358955, "title": "RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models", "author_names": ["Soumya Barikeri", "Anne Lauscher", "Ivan Vulic", "Goran Glavas"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Text representation models are prone to exhibit a range of societal biases, reflecting the non-controlled and biased nature of the underlying pretraining data, which consequently leads to severe ethical issues and even bias amplification. Recent work has predominantly focused on measuring and mitigating bias in pretrained language models. Surprisingly, the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited to only a few types of bias, artificially constructed resources, and completely ignores the impact that debiasing methods may have on the final perfor mance in dialog tasks, e.g., conversational response generation. In this work, we present REDDITBIAS, the first conversational data set grounded in the actual human conversations from Reddit, allowing for bias measurement and mitigation across four important bias dimensions: gender,race,religion, and queerness. Further, we develop an evaluation framework which simultaneously 1)measures bias on the developed REDDITBIAS resource, and 2)evaluates model capability in dialog tasks after model debiasing. We use the evaluation framework to benchmark the widely used conversational DialoGPT model along with the adaptations of four debiasing methods. Our results indicate that DialoGPT is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance.", "year": 2021, "publicationdate": "2021-06-07", "externalids": {"DOI": "10.18653/v1/2021.acl-long.151"}, "doi_lower": "10.18653/v1/2021.acl-long.151"}
{"paper_id": 215737171, "title": "Longformer: The Long-Document Transformer", "author_names": ["Iz Beltagy", "Matthew E. Peters", "Arman Cohan"], "venue": "arXiv.org", "abstract": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.", "year": 2020, "publicationdate": "2020-04-10", "externalids": {}, "doi_lower": null}
{"paper_id": 264550170, "title": "A neural probabilistic language model", "author_names": ["Y. Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "", "abstract": null, "year": 2003, "publicationdate": "2003-03-01", "externalids": {"DOI": "10.1162/153244303322533223"}, "doi_lower": "10.1162/153244303322533223"}
{"paper_id": 218971783, "title": "Language Models are Few-Shot Learners", "author_names": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "R. Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Ma-teusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "I. Sutskever", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "year": 2020, "publicationdate": "2020-05-28", "externalids": {}, "doi_lower": null}
{"paper_id": 248780067, "title": "Fine-Grained Controllable Text Generation Using Non-Residual Prompting", "author_names": ["F. Carlsson", "Joey Öhman", "Fangyu Liu", "S. Verlinden", "Joakim Nivre", "Magnus Sahlgren"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The introduction of immensely large Causal Language Models (CLMs) has rejuvenated the interest in open-ended text generation. However, controlling the generative process for these Transformer-based models is at large an unsolved problem. Earlier work has explored either plug-and-play decoding strategies, or more powerful but blunt approaches such as prompting. There hence currently exists a trade-off between fine-grained control, and the capability for more expressive high-level instructions. To alleviate this trade-off, we propose an encoder-decoder architecture that enables intermediate text prompts at arbitrary time steps. We propose a resource-efficient method for converting a pre-trained CLM into this architecture, and demonstrate its potential on various experiments, including the novel task of contextualized word inclusion. Our method provides strong results on multiple experimental settings, proving itself to be both expressive and versatile.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.acl-long.471"}, "doi_lower": "10.18653/v1/2022.acl-long.471"}
{"paper_id": 4406182, "title": "Deep Communicating Agents for Abstractive Summarization", "author_names": ["Asli Celikyilmaz", "Antoine Bosselut", "Xiaodong He", "Yejin Choi"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With deep communicating agents, the task of encoding a long text is divided across multiple collaborating agents, each in charge of a subsection of the input text. These encoders are connected to a single decoder, trained end-to-end using reinforcement learning to generate a focused and coherent summary. Empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines, including those based on a single encoder or multiple non-communicating encoders.", "year": 2018, "publicationdate": "2018-02-14", "externalids": {"DOI": "10.18653/v1/N18-1150"}, "doi_lower": "10.18653/v1/n18-1150"}
{"paper_id": 220128348, "title": "Evaluation of Text Generation: A Survey", "author_names": ["Asli Celikyilmaz", "Elizabeth Clark", "Jianfeng Gao"], "venue": "arXiv.org", "abstract": "The paper surveys evaluation methods of natural language generation (NLG) systems that have been developed in the last few years. We group NLG evaluation methods into three categories: (1) human-centric evaluation metrics, (2) automatic metrics that require no training, and (3) machine-learned metrics. For each category, we discuss the progress that has been made and the challenges still being faced, with a focus on the evaluation of recently proposed NLG tasks and neural NLG models. We then present two case studies of automatic text summarization and long text generation, and conclude the paper by proposing future research directions.", "year": 2020, "publicationdate": "2020-06-26", "externalids": {}, "doi_lower": null}
{"paper_id": 219401872, "title": "CoCon: A Self-Supervised Approach for Controlled Text Generation", "author_names": ["Alvin Chan", "Y. Ong", "B. Pung", "Aston Zhang", "Jie Fu"], "venue": "International Conference on Learning Representations", "abstract": "Pretrained Transformer-based language models (LMs) display remarkable natural language generation capabilities. With their immense potential, controlling text generation of such LMs is getting attention. While there are studies that seek to control high-level attributes (such as sentiment and topic) of generated text, there is still a lack of more precise control over its content at the word- and phrase-level. Here, we propose Content-Conditioner (CoCon) to control an LM's output text with a target content, at a fine-grained level. In our self-supervised approach, the CoCon block learns to help the LM complete a partially-observed text sequence by conditioning with content inputs that are withheld from the LM. Through experiments, we show that CoCon can naturally incorporate target content into generated texts and control high-level text attributes in a zero-shot manner.", "year": 2020, "publicationdate": "2020-06-05", "externalids": {}, "doi_lower": null}
{"paper_id": 189927897, "title": "“My Way of Telling a Story”: Persona based Grounded Story Generation", "author_names": ["Shrimai Prabhumoye", "Khyathi Raghavi Chandu", "R. Salakhutdinov", "A. Black"], "venue": "Proceedings of the Second Workshop on Storytelling", "abstract": "Visual storytelling is the task of generating stories based on a sequence of images. Inspired by the recent works in neural generation focusing on controlling the form of text, this paper explores the idea of generating these stories in different personas. However, one of the main challenges of performing this task is the lack of a dataset of visual stories in different personas. Having said that, there are independent datasets for both visual storytelling and annotated sentences for various persona. In this paper we describe an approach to overcome this by getting labelled persona data from a different task and leveraging those annotations to perform persona based story generation. We inspect various ways of incorporating personality in both the encoder and the decoder representations to steer the generation in the target direction. To this end, we propose five models which are incremental extensions to the baseline model to perform the task at hand. In our experiments we use five different personas to guide the generation process. We find that the models based on our hypotheses perform better at capturing words while generating stories in the target persona.", "year": 2019, "publicationdate": "2019-06-14", "externalids": {"DOI": "10.18653/v1/W19-3402"}, "doi_lower": "10.18653/v1/w19-3402"}
{"paper_id": 232403983, "title": "Changing the Mind of Transformers for Topically-Controllable Language Generation", "author_names": ["Haw-Shiuan Chang", "Jiaming Yuan", "Mohit Iyyer", "A. McCallum"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Large Transformer-based language models can aid human authors by suggesting plausible continuations of text written so far. However, current interactive writing assistants do not allow authors to guide text generation in desired topical directions. To address this limitation, we design a framework that displays multiple candidate upcoming topics, of which a user can select a subset to guide the generation. Our framework consists of two components: (1) a method that produces a set of candidate topics by predicting the centers of word clusters in the possible continuations, and (2) a text generation model whose output adheres to the chosen topics. The training of both components is self-supervised, using only unlabeled text. Our experiments demonstrate that our topic options are better than those of standard clustering approaches, and our framework often generates fluent sentences related to the chosen topics, as judged by automated metrics and crowdsourced workers.", "year": 2021, "publicationdate": "2021-03-29", "externalids": {"DOI": "10.18653/v1/2021.eacl-main.223"}, "doi_lower": "10.18653/v1/2021.eacl-main.223"}
{"paper_id": 199466228, "title": "Sentiment-Controllable Chinese Poetry Generation", "author_names": ["Huimin Chen", "Xiaoyuan Yi", "Maosong Sun", "Wenhao Li", "Cheng Yang", "Zhipeng Guo"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Expressing diverse sentiments is one of the main purposes of human poetry creation. Existing Chinese poetry generation models have made great progress in poetry quality, but they all neglected to endow generated poems with specific sentiments. Such defect leads to strong sentiment collapse or bias and thus hurts the diversity and semantics of generated poems. Meanwhile, there are few sentimental Chinese poetry resources for studying. To address this problem, we first collect a manually-labelled sentimental poetry corpus with fine-grained sentiment labels. Then we propose a novel semi-supervised conditional Variational Auto-Encoder model for sentiment-controllable poetry generation. Besides, since poetry is discourse-level text where the polarity and intensity of sentiment could transfer among lines, we incorporate a temporal module to capture sentiment transition patterns among different lines. Experimental results show our model can control the sentiment of not only a whole poem but also each line, and improve the poetry diversity against the state-of-the-art models without losing quality.", "year": 2019, "publicationdate": "2019-08-01", "externalids": {"DOI": "10.24963/ijcai.2019/684"}, "doi_lower": "10.24963/ijcai.2019/684"}
{"paper_id": 247951931, "title": "PaLM: Scaling Language Modeling with Pathways", "author_names": ["A. Chowdhery", "Sharan Narang", "Jacob Devlin", "Maarten Bosma", "Gaurav Mishra", "Adam Roberts", "P. Barham", "Hyung Won Chung", "Charles Sutton", "Sebastian Gehrmann", "Parker Schuh", "Kensen Shi", "Sasha Tsvyashchenko", "Joshua Maynez", "Abhishek Rao", "Parker Barnes", "Yi Tay", "Noam Shazeer", "Vinodkumar Prabhakaran", "Emily Reif", "Nan Du", "Ben Hutchinson", "Reiner Pope", "James Bradbury", "Jacob Austin", "M. Isard", "Guy Gur-Ari", "Pengcheng Yin", "Toju Duke", "Anselm Levskaya", "S. Ghemawat", "Sunipa Dev", "H. Michalewski", "Xavier García", "Vedant Misra", "Kevin Robinson", "L. Fedus", "Denny Zhou", "Daphne Ippolito", "D. Luan", "Hyeontaek Lim", "Barret Zoph", "A. Spiridonov", "Ryan Sepassi", "David Dohan", "Shivani Agrawal", "Mark Omernick", "Andrew M. Dai", "Thanumalayan Sankaranarayana Pillai", "Marie Pellat", "Aitor Lewkowycz", "Erica Moreira", "R. Child", "Oleksandr Polozov", "Katherine Lee", "Zongwei Zhou", "Xuezhi Wang", "Brennan Saeta", "Mark Díaz", "Orhan Firat", "Michele Catasta", "Jason Wei", "K. Meier-Hellstern", "D. Eck", "J. Dean", "Slav Petrov", "Noah Fiedel"], "venue": "Journal of machine learning research", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "year": 2022, "publicationdate": "2022-04-05", "externalids": {}, "doi_lower": null}
{"paper_id": 253018554, "title": "Scaling Instruction-Finetuned Language Models", "author_names": ["Hyung Won Chung", "Le Hou", "S. Longpre", "Barret Zoph", "Yi Tay", "W. Fedus", "Eric Li", "Xuezhi Wang", "Mostafa Dehghani", "Siddhartha Brahma", "Albert Webson", "S. Gu", "Zhuyun Dai", "Mirac Suzgun", "Xinyun Chen", "A. Chowdhery", "Dasha Valter", "Sharan Narang", "Gaurav Mishra", "Adams Wei Yu", "Vincent Zhao", "Yanping Huang", "Andrew M. Dai", "Hongkun Yu", "Slav Petrov", "Ed H. Chi", "J. Dean", "Jacob Devlin", "Adam Roberts", "Denny Zhou", "Quoc V. Le", "Jason Wei"], "venue": "Journal of machine learning research", "abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.", "year": 2022, "publicationdate": "2022-10-20", "externalids": {"DOI": "10.48550/arXiv.2210.11416"}, "doi_lower": "10.48550/arxiv.2210.11416"}
{"paper_id": 3046638, "title": "TESLA at WMT 2011: Translation Evaluation and Tunable Metric", "author_names": ["Daniel Dahlmeier", "Chang Liu", "H. Ng"], "venue": "WMT@EMNLP", "abstract": null, "year": 2011, "publicationdate": "2011-07-30", "externalids": {}, "doi_lower": null}
{"paper_id": 57759363, "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context", "author_names": ["Zihang Dai", "Zhilin Yang", "Yiming Yang", "J. Carbonell", "Quoc V. Le", "R. Salakhutdinov"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.", "year": 2019, "publicationdate": "2019-01-09", "externalids": {"DOI": "10.18653/v1/P19-1285"}, "doi_lower": "10.18653/v1/p19-1285"}
{"paper_id": 208617790, "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "author_names": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "J. Yosinski", "Rosanne Liu"], "venue": "International Conference on Learning Representations", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "year": 2019, "publicationdate": "2019-09-25", "externalids": {}, "doi_lower": null}
{"paper_id": 8010515, "title": "Learning from Post-Editing: Online Model Adaptation for Statistical Machine Translation", "author_names": ["Michael J. Denkowski", "Chris Dyer", "A. Lavie"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Using machine translation output as a starting point for human translation has become an increasingly common application of MT. We propose and evaluate three computationally efficient online methods for updating statistical MT systems in a scenario where post-edited MT output is constantly being returned to the system: (1) adding new rules to the translation model from the post-edited content, (2) updating a Bayesian language model of the target language that is used by the MT system, and (3) updating the MT system’s discriminative parameters with a MIRA step. Individually, these techniques can substantially improve MT quality, even over strong baselines. Moreover, we see super-additive improvements when all three techniques are used in tandem.", "year": 2014, "publicationdate": "2014-04-01", "externalids": {"DOI": "10.3115/v1/E14-1042"}, "doi_lower": "10.3115/v1/e14-1042"}
{"paper_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "author_names": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/N19-1423"}, "doi_lower": "10.18653/v1/n19-1423"}
{"paper_id": 207852875, "title": "Queens Are Powerful Too: Mitigating Gender Bias in Dialogue Generation", "author_names": ["Emily Dinan", "Angela Fan", "Adina Williams", "Jack Urbanek", "Douwe Kiela", "J. Weston"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Models often easily learn biases present in the training data, and their predictions directly reflect this bias. We analyze gender bias in dialogue data, and examine how this bias is actually amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets, and focus on the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for our bias mitigation techniques. The LIGHT dataset is highly imbalanced with respect to gender, containing predominantly male characters, likely because it is entirely collected by crowdworkers and reflects common biases that exist in fantasy or medieval settings. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias in LIGHT by balancing the genderedness of generated dialogue utterances and are particularly effective in combination. We quantify performance using various evaluation methods---such as quantity of gendered words, a dialogue safety classifier, and human studies---all of which show that our models generate less gendered, but equally engaging chit-chat responses.", "year": 2019, "publicationdate": "2019-11-10", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.656"}, "doi_lower": "10.18653/v1/2020.emnlp-main.656"}
{"paper_id": 232134936, "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth", "author_names": ["Yihe Dong", "Jean-Baptiste Cordonnier", "Andreas Loukas"], "venue": "International Conference on Machine Learning", "abstract": "Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards\"token uniformity\". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures.", "year": 2021, "publicationdate": "2021-03-05", "externalids": {}, "doi_lower": null}
{"paper_id": 2172129, "title": "Learning to Ask: Neural Question Generation for Reading Comprehension", "author_names": ["Xinya Du", "Junru Shao", "Claire Cardie"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e.,, grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).", "year": 2017, "publicationdate": "2017-04-29", "externalids": {"DOI": "10.18653/v1/P17-1123"}, "doi_lower": "10.18653/v1/p17-1123"}
{"paper_id": 44134226, "title": "Hierarchical Neural Story Generation", "author_names": ["Angela Fan", "M. Lewis", "Yann Dauphin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.", "year": 2018, "publicationdate": "2018-05-01", "externalids": {"DOI": "10.18653/v1/P18-1082"}, "doi_lower": "10.18653/v1/p18-1082"}
{"paper_id": 11054023, "title": "Controlling Linguistic Style Aspects in Neural Language Generation", "author_names": ["Jessica Ficler", "Yoav Goldberg"], "venue": "arXiv.org", "abstract": "Most work on neural natural language generation (NNLG) focus on controlling the content of the generated text. We experiment with controling several stylistic aspects of the generated text, in addition to its content. The method is based on conditioned RNN language model, where the desired content as well as the stylistic parameters serve as conditioning contexts. We demonstrate the approach on the movie reviews domain and show that it is successful in generating coherent sentences corresponding to the required linguistic style and content.", "year": 2017, "publicationdate": "2017-07-09", "externalids": {"DOI": "10.18653/v1/W17-4912"}, "doi_lower": "10.18653/v1/w17-4912"}
{"paper_id": 226718515, "title": "EmoSen: Generating Sentiment and Emotion Controlled Responses in a Multimodal Dialogue System", "author_names": ["Mauajama Firdaus", "Hardik Chauhan", "Asif Ekbal", "P. Bhattacharyya"], "venue": "IEEE Transactions on Affective Computing", "abstract": "An essential skill for effective communication is the ability to express specific sentiment and emotion in a conversation. Any robust dialogue system should handle the combined effect of both sentiment and emotion while generating responses. This is expected to provide a better experience and concurrently increase users’ satisfaction. Previously, research on either emotion or sentiment controlled dialogue generation has shown great promise in developing the next generation conversational agents, but the simultaneous effect of both is still unexplored. The existing dialogue systems are majorly based on unimodal sources, predominantly the text, and thereby cannot utilize the information present in the other sources, such as video, audio, image, etc. In this article, we present at first a large scale benchmark Sentiment Emotion aware Multimodal Dialogue (SEMD) dataset for the task of sentiment and emotion controlled dialogue generation. The SEMD dataset consists of 55k conversations from 10 TV shows having text, audio, and video information. To utilize multimodal information, we propose multimodal attention based conditional variational autoencoder (M-CVAE) that outperforms several baselines. Quantitative and qualitative analyses show that multimodality, along with contextual information, plays an essential role in generating coherent and diverse responses for any given emotion and sentiment.", "year": 2022, "publicationdate": "2022-07-01", "externalids": {"DOI": "10.1109/TAFFC.2020.3015491"}, "doi_lower": "10.1109/taffc.2020.3015491"}
{"paper_id": 249905317, "title": "Being Polite: Modeling Politeness Variation in a Personalized Dialog Agent", "author_names": ["Mauajama Firdaus", "A. Shandilya", "Asif Ekbal", "P. Bhattacharyya"], "venue": "IEEE Transactions on Computational Social Systems", "abstract": "Politeness enhances interactions by improving relations between the participants. If there is a display of rudeness, even the finest conversation can fall through. In addition, if lathered with kindness, even the most angst-prone circumstance can be expressed with far less suffering. Previously, researchers have focused upon including politeness in conversations. But the existing research does not focus on variations in politeness according to the user profile. Therefore, in this article, we propose a novel task of generating polite personalized dialog responses in accordance with the user profile and consistent with the conversational history. We design a novel Polite Personalized Dialog Generation (PoPe-DG) framework that employs a reinforced deliberation network. We create human-annotated politeness templates according to user profiles to induce politeness variation in the generated responses for the proposed task. Precisely, the personality profile is transformed and normalized into a vector using the fusion attention combined with dialogue utterances to build context. Furthermore, the context modules and the annotated templates are appended to initialize the deliberation decoder. Experimental analysis validates that our proposed approach inculcates politeness in responses in accordance with the user profile and the conversational history.", "year": 2023, "publicationdate": "2023-08-01", "externalids": {"DOI": "10.1109/TCSS.2022.3182986"}, "doi_lower": "10.1109/tcss.2022.3182986"}
{"paper_id": 229923710, "title": "Making Pre-trained Language Models Better Few-shot Learners", "author_names": ["Tianyu Gao", "Adam Fisch", "Danqi Chen"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF—better few-shot fine-tuning of language models—a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.", "year": 2021, "publicationdate": "2021-01-01", "externalids": {"DOI": "10.18653/v1/2021.acl-long.295"}, "doi_lower": "10.18653/v1/2021.acl-long.295"}
{"paper_id": 202565872, "title": "Structuring Latent Spaces for Stylized Response Generation", "author_names": ["Xiang Gao", "Yizhe Zhang", "Sungjin Lee", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "W. Dolan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Generating responses in a targeted style is a useful yet challenging task, especially in the absence of parallel data. With limited data, existing methods tend to generate responses that are either less stylized or less context-relevant. We propose StyleFusion, which bridges conversation modeling and non-parallel style transfer by sharing a structured latent space. This structure allows the system to generate stylized relevant responses by sampling in the neighborhood of the conversation model prediction, and continuously control the style level. We demonstrate this method using dialogues from Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, the system generates responses of the targeted style and outperforms competitive baselines.", "year": 2019, "publicationdate": "2019-09-03", "externalids": {"DOI": "10.18653/v1/D19-1190"}, "doi_lower": "10.18653/v1/d19-1190"}
{"paper_id": 16426981, "title": "A Snapshot of NLG Evaluation Practices 2005 - 2014", "author_names": ["Dimitra Gkatzia", "Saad Mahamood"], "venue": "European Workshop on Natural Language Generation", "abstract": "In this paper we present a snapshot of endto-end NLG system evaluations as presented in conference and journal papers1 over the last ten years in order to better understand the nature and type of evaluations that have been undertaken. We find that researchers tend to favour specific evaluation methods, and that their evaluation approaches are also correlated with the publication venue. We further discuss what factors may influence the types of evaluation used for a given NLG system.", "year": 2015, "publicationdate": "2015-09-01", "externalids": {"DOI": "10.18653/v1/W15-4708"}, "doi_lower": "10.18653/v1/w15-4708"}
{"paper_id": 221818956, "title": "Content Planning for Neural Story Generation with Aristotelian Rescoring", "author_names": ["Seraphina Goldfarb-Tarrant", "Tuhin Chakrabarty", "R. Weischedel", "Nanyun Peng"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation. We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle's Poetics. We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way.", "year": 2020, "publicationdate": "2020-09-21", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.351"}, "doi_lower": "10.18653/v1/2020.emnlp-main.351"}
{"paper_id": 202695327, "title": "1 Generative Adversarial Network Generative Adversarial Networks", "author_names": ["Grigorios G. Chrysos", "P. Favaro", "S. Zafeiriou"], "venue": "", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 195345228, "title": "Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting", "author_names": ["Aditya Grover", "Jiaming Song", "Alekh Agarwal", "Kenneth Tran", "Ashish Kapoor", "E. Horvitz", "Stefano Ermon"], "venue": "DGS@ICLR", "abstract": "A learned generative model often produces biased statistics relative to the underlying data distribution. A standard technique to correct this bias is importance sampling, where samples from the model are weighted by the likelihood ratio under model and true distributions. When the likelihood ratio is unknown, it can be estimated by training a probabilistic classifier to distinguish samples from the two distributions. We employ this likelihood-free importance weighting method to correct for the bias in generative models. We find that this technique consistently improves standard goodness-of-fit metrics for evaluating the sample quality of state-of-the-art deep generative models, suggesting reduced bias. Finally, we demonstrate its utility on representative applications in a) data augmentation for classification using generative adversarial networks, and b) model-based policy evaluation using off-policy data.", "year": 2019, "publicationdate": "2019-03-27", "externalids": {}, "doi_lower": null}
{"paper_id": 102351981, "title": "Unifying Human and Statistical Evaluation for Natural Language Generation", "author_names": ["Tatsunori B. Hashimoto", "Hugh Zhang", "Percy Liang"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "How can we measure whether a natural language generation system produces both high quality and diverse outputs? Human evaluation captures quality but not diversity, as it does not catch models that simply plagiarize from the training set. On the other hand, statistical evaluation (i.e., perplexity) captures diversity but not quality, as models that occasionally emit low quality samples would be insufficiently penalized. In this paper, we propose a unified framework which evaluates both diversity and quality, based on the optimal error rate of predicting whether a sentence is human- or machine-generated. We demonstrate that this error rate can be efficiently estimated by combining human and statistical evaluation, using an evaluation metric which we call HUSE. On summarization and chit-chat dialogue, we show that (i) HUSE detects diversity defects which fool pure human evaluation and that (ii) techniques such as annealing for improving quality actually decrease HUSE due to decreased diversity.", "year": 2019, "publicationdate": "2019-04-04", "externalids": {"DOI": "10.18653/v1/N19-1169"}, "doi_lower": "10.18653/v1/n19-1169"}
{"paper_id": 211069439, "title": "A Probabilistic Formulation of Unsupervised Text Style Transfer", "author_names": ["Junxian He", "Xinyi Wang", "Graham Neubig", "Taylor Berg-Kirkpatrick"], "venue": "International Conference on Learning Representations", "abstract": "We present a deep generative model for unsupervised text style transfer that unifies previously proposed non-generative techniques. Our probabilistic approach models non-parallel data from two domains as a partially observed parallel corpus. By hypothesizing a parallel latent sequence that generates each observed sequence, our model learns to transform sequences from one domain to another in a completely unsupervised fashion. In contrast with traditional generative sequence models (e.g. the HMM), our model makes few assumptions about the data it generates: it uses a recurrent language model as a prior and an encoder-decoder as a transduction distribution. While computation of marginal data likelihood is intractable in this model class, we show that amortized variational inference admits a practical surrogate. Further, by drawing connections between our variational objective and other recent unsupervised style transfer and machine translation techniques, we show how our probabilistic view can unify some known non-generative objectives such as backtranslation and adversarial loss. Finally, we demonstrate the effectiveness of our method on a wide range of unsupervised style transfer tasks, including sentiment transfer, formality transfer, word decipherment, author imitation, and related language translation. Across all style transfer tasks, our approach yields substantial gains over state-of-the-art non-generative baselines, including the state-of-the-art unsupervised machine translation techniques that our approach generalizes. Further, we conduct experiments on a standard unsupervised machine translation task and find that our unified approach matches the current state-of-the-art.", "year": 2020, "publicationdate": "2020-02-10", "externalids": {}, "doi_lower": null}
{"paper_id": 127986954, "title": "The Curious Case of Neural Text Degeneration", "author_names": ["Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi"], "venue": "International Conference on Learning Representations", "abstract": "Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. \nIn this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.", "year": 2019, "publicationdate": "2019-04-22", "externalids": {}, "doi_lower": null}
{"paper_id": 261507835, "title": "Building Natural-Language Generation Systems", "author_names": ["Ehud Reiter"], "venue": "arXiv.org", "abstract": "This is a very short paper that briefly discusses some of the tasks that NLG systems perform. It is of no research interest, but I have occasionally found it useful as a way of introducing NLG to potential project collaborators who know nothing about the field.", "year": 1996, "publicationdate": "1996-05-02", "externalids": {}, "doi_lower": null}
{"paper_id": 64212582, "title": "Flat holonomies on automata networks : (a more recent version available at: http //arxiv.org/abs/cs.DC/0512077)", "author_names": ["G. Itkis", "L. Levin"], "venue": "", "abstract": null, "year": 2006, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 153312680, "title": "Challenges in Building Intelligent Open-domain Dialog Systems", "author_names": ["Minlie Huang", "Xiaoyan Zhu", "Jianfeng Gao"], "venue": "ACM Trans. Inf. Syst.", "abstract": "There is a resurgent interest in developing intelligent open-domain dialog systems due to the availability of large amounts of conversational data and the recent progress on neural approaches to conversational AI [33]. Unlike traditional task-oriented bots, an open-domain dialog system aims to establish long-term connections with users by satisfying the human need for communication, affection, and social belonging. This article reviews the recent work on neural approaches that are devoted to addressing three challenges in developing such systems: semantics, consistency, and interactiveness. Semantics requires a dialog system to not only understand the content of the dialog but also identify users’ emotional and social needs during the conversation. Consistency requires the system to demonstrate a consistent personality to win users’ trust and gain their long-term confidence. Interactiveness refers to the system’s ability to generate interpersonal responses to achieve particular social goals such as entertainment and conforming. The studies we select to present in this survey are based on our unique views and are by no means complete. Nevertheless, we hope that the discussion will inspire new research in developing more intelligent open-domain dialog systems.", "year": 2019, "publicationdate": "2019-05-13", "externalids": {"DOI": "10.1145/3383123"}, "doi_lower": "10.1145/3383123"}
{"paper_id": 207847197, "title": "Reducing Sentiment Bias in Language Models via Counterfactual Evaluation", "author_names": ["Po-Sen Huang", "Huan Zhang", "Ray Jiang", "Robert Stanforth", "Johannes Welbl", "Jack W. Rae", "Vishal Maini", "Dani Yogatama", "Pushmeet Kohli"], "venue": "Findings", "abstract": "Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in models capable of generating coherent texts, it also prompts models to internalize social biases present in the training corpus. This paper aims to quantify and reduce a particular type of bias exhibited by language models: bias in the sentiment of generated text. Given a conditioning context (e.g., a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g., country names, occupations, genders) in the conditioning context using a form of counterfactual evaluation. We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature, and demonstrate that large-scale models trained on two different corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We then propose embedding and sentiment prediction-derived regularization on the language model’s latent representations. The regularizations improve fairness metrics while retaining comparable levels of perplexity and semantic similarity.", "year": 2019, "publicationdate": "2019-11-08", "externalids": {"DOI": "10.18653/v1/2020.findings-emnlp.7"}, "doi_lower": "10.18653/v1/2020.findings-emnlp.7"}
{"paper_id": 14888472, "title": "Quality management on Amazon Mechanical Turk", "author_names": ["Panagiotis G. Ipeirotis", "F. Provost", "Jing Wang"], "venue": "AAAI Conference on Human Computation & Crowdsourcing", "abstract": null, "year": 2010, "publicationdate": "2010-07-25", "externalids": {"DOI": "10.1145/1837885.1837906"}, "doi_lower": "10.1145/1837885.1837906"}
{"paper_id": 246652372, "title": "Survey of Hallucination in Natural Language Generation", "author_names": ["Ziwei Ji", "Nayeon Lee", "Rita Frieske", "Tiezheng Yu", "D. Su", "Yan Xu", "Etsuko Ishii", "Yejin Bang", "Delong Chen", "Wenliang Dai", "Andrea Madotto", "Pascale Fung"], "venue": "ACM Computing Surveys", "abstract": "Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.", "year": 2022, "publicationdate": "2022-02-08", "externalids": {"DOI": "10.1145/3571730"}, "doi_lower": "10.1145/3571730"}
{"paper_id": 226262225, "title": "Template Guided Text Generation for Task Oriented Dialogue", "author_names": ["Mihir Kale", "Abhinav Rastogi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language. In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across a large number of APIs. First, we propose a schema-guided approach which conditions the generation on a schema describing the API in natural language. Our second method investigates the use of a small number of templates, growing linearly in number of slots, to convey the semantics of the API. To generate utterances for an arbitrary slot combination, a few simple templates are first concatenated to give a semantically correct, but possibly incoherent and ungrammatical utterance. A pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text. Through automatic metrics and human evaluation, we show that our method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency.", "year": 2020, "publicationdate": "2020-04-30", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.527"}, "doi_lower": "10.18653/v1/2020.emnlp-main.527"}
{"paper_id": 218763259, "title": "Text-to-Text Pre-Training for Data-to-Text Tasks", "author_names": ["Mihir Kale"], "venue": "International Conference on Natural Language Generation", "abstract": "We study the pre-train + fine-tune strategy for data-to-text tasks. Our experiments indicate that text-to-text pre-training in the form of T5 (Raffel et al., 2019), enables simple, end-to-end transformer based models to outperform pipelined neural architectures tailored for data-to-text generation, as well as alternatives such as BERT and GPT-2. Importantly, T5 pre-training leads to better generalization, as evidenced by large improvements on out-ofdomain test sets. We hope our work serves as a useful baseline for future research, as transfer learning becomes ever more prevalent for data-to-text tasks.", "year": 2020, "publicationdate": "2020-05-21", "externalids": {"DOI": "10.18653/v1/2020.inlg-1.14"}, "doi_lower": "10.18653/v1/2020.inlg-1.14"}
{"paper_id": 247939526, "title": "CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation", "author_names": ["Pei Ke", "Hao Zhou", "Yankai Lin", "Peng Li", "Jie Zhou", "Xiaoyan Zhu", "Minlie Huang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets. In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks. On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training. Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities.", "year": 2022, "publicationdate": "2022-04-02", "externalids": {"DOI": "10.48550/arXiv.2204.00862"}, "doi_lower": "10.48550/arxiv.2204.00862"}
{"paper_id": 202573071, "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation", "author_names": ["N. Keskar", "Bryan McCann", "L. Varshney", "Caiming Xiong", "R. Socher"], "venue": "arXiv.org", "abstract": "Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at this https URL.", "year": 2019, "publicationdate": "2019-09-11", "externalids": {}, "doi_lower": null}
{"paper_id": 229348988, "title": "A Distributional Approach to Controlled Text Generation", "author_names": ["Muhammad Khalifa", "Hady ElSahar", "Marc Dymetman"], "venue": "International Conference on Learning Representations", "abstract": "We propose a Distributional Approach for addressing Controlled Text Generation from pre-trained Language Models (LMs). This approach permits to specify, in a single formal framework, both “pointwise” and “distributional” constraints over the target LM — to our knowledge, the first model with such generality — while minimizing KL divergence from the initial LM distribution. The optimal target distribution is then uniquely determined as an explicit EBM (Energy-Based Model) representation. From that optimal representation we then train a target controlled Autoregressive LM through an adaptive distributional variant of Policy Gradient. We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the initial LM. We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models. Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.1", "year": 2020, "publicationdate": "2020-12-22", "externalids": {}, "doi_lower": null}
{"paper_id": 211146177, "title": "AUTO-ENCODING VARIATIONAL BAYES", "author_names": ["Romain Lopez", "Pierre Boyeau", "N. Yosef", "Michael I. Jordan", "J. Regier"], "venue": "", "abstract": null, "year": 2020, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 209315300, "title": "Reformer: The Efficient Transformer", "author_names": ["Nikita Kitaev", "Lukasz Kaiser", "Anselm Levskaya"], "venue": "International Conference on Learning Representations", "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.", "year": 2020, "publicationdate": "2020-01-13", "externalids": {}, "doi_lower": null}
{"paper_id": 221655075, "title": "GeDi: Generative Discriminator Guided Sequence Generation", "author_names": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "N. Keskar", "Shafiq R. Joty", "R. Socher", "Nazneen Rajani"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "year": 2020, "publicationdate": "2020-09-14", "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.424"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.424"}
{"paper_id": 233296808, "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "author_names": ["Brian Lester", "Rami Al-Rfou", "Noah Constant"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3’s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” We release code and model checkpoints to reproduce our experiments.", "year": 2021, "publicationdate": "2021-04-18", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.243"}, "doi_lower": "10.18653/v1/2021.emnlp-main.243"}
{"paper_id": 2955580, "title": "A Persona-Based Neural Conversation Model", "author_names": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Georgios P. Spithourakis", "Jianfeng Gao", "W. Dolan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.", "year": 2016, "publicationdate": "2016-03-19", "externalids": {"DOI": "10.18653/v1/P16-1094"}, "doi_lower": "10.18653/v1/p16-1094"}
{"paper_id": 215814127, "title": "Rigid Formats Controlled Text Generation", "author_names": ["Piji Li", "Haisong Zhang", "Xiaojiang Liu", "Shuming Shi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Although they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we propose a simple and elegant framework named SongNet to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. We improve the attention mechanism to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality. Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation.", "year": 2020, "publicationdate": "2020-04-17", "externalids": {"DOI": "10.18653/v1/2020.acl-main.68"}, "doi_lower": "10.18653/v1/2020.acl-main.68"}
{"paper_id": 267833818, "title": "EmoElicitor: An Open Domain Response Generation Model with User Emotional Reaction Awareness", "author_names": ["Shifeng Li", "Shi Feng", "Daling Wang", "Kaisong Song", "Yifei Zhang", "Weichao Wang"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Generating emotional responses is crucial for building human-like dialogue systems. However, existing studies have focused only on generating responses by controlling the agents' emotions, while the feelings of the users, which are the ultimate concern of a dialogue system, have been neglected. In this paper, we propose a novel variational model named EmoElicitor to generate appropriate responses that can elicit user's specific emotion. We incorporate the next-round utterance after the response into the posterior network to enrich the context, and we decompose single latent variable into several sequential ones to guide response generation with the help of a pre-trained language model. Extensive experiments conducted on real-world dataset show that EmoElicitor not only performs better than the baselines in term of diversity and semantic similarity, but also can elicit emotion with higher accuracy.", "year": 2020, "publicationdate": "2020-07-01", "externalids": {"DOI": "10.24963/ijcai.2020/503"}, "doi_lower": "10.24963/ijcai.2020/503"}
{"paper_id": 230433941, "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "author_names": ["Xiang Lisa Li", "Percy Liang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.", "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2021.acl-long.353"}, "doi_lower": "10.18653/v1/2021.acl-long.353"}
{"paper_id": 195767456, "title": "GPT-based Generation for Classical Chinese Poetry", "author_names": ["Yi Liao", "Yasheng Wang", "Qun Liu", "Xin Jiang"], "venue": "arXiv.org", "abstract": "We present a simple yet effective method for generating high quality classical Chinese poetry with Generative Pre-trained Language Model (GPT). The method adopts a simple GPT model, without using any human crafted rules or features, or designing any additional neural components. While the proposed model learns to generate various forms of classical Chinese poems, including Jueju, Lushi, various Cipai and Couples, the generated poems are of very high quality. We also propose and implement a method to fine-tune the model to generate acrostic poetry. To the best of our knowledge, this is the first to employ GPT in developing a poetry generation system. We have released an online mini demonstration program on Wechat to show the generation capability of the proposed method for classical Chinese poetry.", "year": 2019, "publicationdate": "2019-06-29", "externalids": {}, "doi_lower": null}
{"paper_id": 964287, "title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author_names": ["Chin-Yew Lin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": null, "year": 2004, "publicationdate": "2004-07-25", "externalids": {}, "doi_lower": null}
{"paper_id": 236470168, "title": "Plug-and-Blend: A Framework for Plug-and-Play Controllable Story Generation with Sketches", "author_names": ["Zhiyu Lin", "Mark O. Riedl"], "venue": "Artificial Intelligence and Interactive Digital Entertainment Conference", "abstract": "Large pre-trained neural language models (LM) have very powerful text generation capabilities. \nHowever, in practice, they are hard to control for creative purposes.\nWe describe a Plug-and-Play controllable language generation framework, Plug-and-Blend, that allows a human user to input multiple control codes (topics).\nIn the context of automated story generation, this allows a human user loose or fine-grained control of the topics and transitions between them that will appear in the generated story, and can even allow for overlapping, blended topics.\nAutomated evaluations show our framework, working with different generative LMs,\ncontrols the generation towards given continuous-weighted control codes while keeping the generated sentences fluent, demonstrating strong blending capability.\nA human participant evaluation shows that the generated stories are observably transitioning between two topics.", "year": 2021, "publicationdate": "2021-03-23", "externalids": {"DOI": "10.1609/aiide.v17i1.18891"}, "doi_lower": "10.1609/aiide.v17i1.18891"}
{"paper_id": 235313967, "title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts", "author_names": ["Alisa Liu", "Maarten Sap", "Ximing Lu", "Swabha Swayamdipta", "Chandra Bhagavatula", "Noah A. Smith", "Yejin Choi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with “expert” LMs and/or “anti-expert” LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.", "year": 2021, "publicationdate": "2021-05-07", "externalids": {"DOI": "10.18653/v1/2021.acl-long.522"}, "doi_lower": "10.18653/v1/2021.acl-long.522"}
{"paper_id": 8882144, "title": "TESLA: Translation Evaluation of Sentences with Linear-Programming-Based Analysis", "author_names": ["Chang Liu", "Daniel Dahlmeier", "H. Ng"], "venue": "WMT@ACL", "abstract": null, "year": 2010, "publicationdate": "2010-07-15", "externalids": {}, "doi_lower": null}
{"paper_id": 213827782, "title": "A Character-Centric Neural Model for Automated Story Generation", "author_names": ["Danyang Liu", "Juntao Li", "Meng-Hsuan Yu", "Ziming Huang", "Gongshen Liu", "Dongyan Zhao", "Rui Yan"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Automated story generation is a challenging task which aims to automatically generate convincing stories composed of successive plots correlated with consistent characters. Most recent generation models are built upon advanced neural networks, e.g., variational autoencoder, generative adversarial network, convolutional sequence to sequence model. Although these models have achieved prompting results on learning linguistic patterns, very few methods consider the attributes and prior knowledge of the story genre, especially from the perspectives of explainability and consistency. To fill this gap, we propose a character-centric neural storytelling model, where a story is created encircling the given character, i.e., each part of a story is conditioned on a given character and corresponded context environment. In this way, we explicitly capture the character information and the relations between plots and characters to improve explainability and consistency. Experimental results on open dataset indicate that our model yields meaningful improvements over several strong baselines on both human and automatic evaluations.", "year": 2020, "publicationdate": "2020-04-03", "externalids": {"DOI": "10.1609/AAAI.V34I02.5536"}, "doi_lower": "10.1609/aaai.v34i02.5536"}
{"paper_id": 236493269, "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing", "author_names": ["Pengfei Liu", "Weizhe Yuan", "Jinlan Fu", "Zhengbao Jiang", "Hiroaki Hayashi", "Graham Neubig"], "venue": "ACM Computing Surveys", "abstract": "This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia–Pretrain including constantly updated survey and paperlist.", "year": 2021, "publicationdate": "2021-07-28", "externalids": {"DOI": "10.1145/3560815"}, "doi_lower": "10.1145/3560815"}
{"paper_id": 233476528, "title": "Mitigating Political Bias in Language Models Through Reinforced Calibration", "author_names": ["Ruibo Liu", "Chenyan Jia", "Jason Wei", "Guangxuan Xu", "Lili Wang", "Soroush Vosoughi"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Current large-scale language models can be politically biased as a result of the data they are trained on, potentially causing serious problems when they are deployed in real-world settings. In this paper, we describe metrics for measuring political bias in GPT-2 generation and propose a reinforcement learning (RL) framework for mitigating political biases in generated text. By using rewards from word embeddings or a classifier, our RL framework guides debiased generation without having access to the training data or requiring the model to be retrained. In empirical experiments on three attributes sensitive to political bias (gender, location, and topic), our methods reduced bias according to both our metrics and human evaluation, while maintaining readability and semantic coherence.", "year": 2021, "publicationdate": "2021-04-30", "externalids": {"DOI": "10.1609/aaai.v35i17.17744"}, "doi_lower": "10.1609/aaai.v35i17.17744"}
{"paper_id": 226262374, "title": "Data Boost: Text Data Augmentation through Reinforcement Learning Guided Conditional Generation", "author_names": ["Ruibo Liu", "Guangxuan Xu", "Chenyan Jia", "Weicheng Ma", "Lili Wang", "Soroush Vosoughi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We evaluate Data Boost on three diverse text classification tasks under five different classifier architectures. The result shows that Data Boost can boost the performance of classifiers especially in low-resource data scenarios. For instance, Data Boost improves F1 for the three tasks by 8.7% on average when given only 10% of the whole data for training. We also compare Data Boost with six prior text augmentation methods. Through human evaluations (N=178), we confirm that Data Boost augmentation has comparable quality as the original data with respect to readability and class consistency.", "year": 2020, "publicationdate": "2020-11-01", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.726"}, "doi_lower": "10.18653/v1/2020.emnlp-main.726"}
{"paper_id": 210861178, "title": "Multilingual Denoising Pre-training for Neural Machine Translation", "author_names": ["Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "M. Lewis", "Luke Zettlemoyer"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Abstract This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART—a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.1", "year": 2020, "publicationdate": "2020-01-22", "externalids": {"DOI": "10.1162/tacl_a_00343"}, "doi_lower": "10.1162/tacl_a_00343"}
{"paper_id": 237431491, "title": "Naturalness Evaluation of Natural Language Generation in Task-oriented Dialogues Using BERT", "author_names": ["Ye Liu", "Wolfgang Maier", "W. Minker", "Stefan Ultes"], "venue": "Recent Advances in Natural Language Processing", "abstract": "This paper presents an automatic method to evaluate the naturalness of natural language generation in dialogue systems. While this task was previously rendered through expensive and time-consuming human labor, we present this novel task of automatic naturalness evaluation of generated language. By fine-tuning the BERT model, our proposed naturalness evaluation method shows robust results and outperforms the baselines: support vector machines, bi-directional LSTMs, and BLEURT. In addition, the training speed and evaluation performance of naturalness model are improved by transfer learning from quality and informativeness linguistic knowledge.", "year": 2021, "publicationdate": "2021-09-07", "externalids": {"DOI": "10.26615/978-954-452-072-4_096"}, "doi_lower": "10.26615/978-954-452-072-4_096"}
{"paper_id": 198953378, "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "author_names": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "M. Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "venue": "arXiv.org", "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.", "year": 2019, "publicationdate": "2019-07-26", "externalids": {}, "doi_lower": null}
{"paper_id": 1880070, "title": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses", "author_names": ["Ryan Lowe", "Michael Noseworthy", "Iulian Serban", "Nicolas Angelard-Gontier", "Yoshua Bengio", "Joelle Pineau"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016). Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem.We present an evaluation model (ADEM)that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model’s predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue mod-els unseen during training, an important step for automatic dialogue evaluation.", "year": 2017, "publicationdate": "2017-02-17", "externalids": {"DOI": "10.18653/v1/P17-1103"}, "doi_lower": "10.18653/v1/p17-1103"}
{"paper_id": 196182171, "title": "Learning to Control the Fine-grained Sentiment for Story Ending Generation", "author_names": ["Fuli Luo", "Damai Dai", "Pengcheng Yang", "Tianyu Liu", "Baobao Chang", "Zhifang Sui", "Xu Sun"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Automatic story ending generation is an interesting and challenging task in natural language generation. Previous studies are mainly limited to generate coherent, reasonable and diversified story endings, and few works focus on controlling the sentiment of story endings. This paper focuses on generating a story ending which meets the given fine-grained sentiment intensity. There are two major challenges to this task. First is the lack of story corpus which has fine-grained sentiment labels. Second is the difficulty of explicitly controlling sentiment intensity when generating endings. Therefore, we propose a generic and novel framework which consists of a sentiment analyzer and a sentimental generator, respectively addressing the two challenges. The sentiment analyzer adopts a series of methods to acquire sentiment intensities of the story dataset. The sentimental generator introduces the sentiment intensity into decoder via a Gaussian Kernel Layer to control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.", "year": 2019, "publicationdate": "2019-07-01", "externalids": {"DOI": "10.18653/v1/P19-1603"}, "doi_lower": "10.18653/v1/p19-1603"}
{"paper_id": 203905453, "title": "Controlled Text Generation for Data Augmentation in Intelligent Artificial Agents", "author_names": ["Nikolaos Malandrakis", "Minmin Shen", "Anuj Goyal", "Shuyang Gao", "Abhishek Sethi", "A. Metallinou"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Data availability is a bottleneck during early stages of development of new capabilities for intelligent artificial agents. We investigate the use of text generation techniques to augment the training data of a popular commercial artificial agent across categories of functionality, with the goal of faster development of new functionality. We explore a variety of encoder-decoder generative models for synthetic training data generation and propose using conditional variational auto-encoders. Our approach requires only direct optimization, works well with limited data and significantly outperforms the previous controlled text generation techniques. Further, the generated data are used as additional training samples in an extrinsic intent classification task, leading to improved performance by up to 5% absolute f-score in low-resource cases, validating the usefulness of our approach.", "year": 2019, "publicationdate": "2019-10-04", "externalids": {"DOI": "10.18653/v1/D19-5609"}, "doi_lower": "10.18653/v1/d19-5609"}
{"paper_id": 196185246, "title": "Putting Evaluation in Context: Contextual Embeddings Improve Machine Translation Evaluation", "author_names": ["Nitika Mathur", "Timothy Baldwin", "Trevor Cohn"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Accurate, automatic evaluation of machine translation is critical for system tuning, and evaluating progress in the field. We proposed a simple unsupervised metric, and additional supervised metrics which rely on contextual word embeddings to encode the translation and reference sentences. We find that these models rival or surpass all existing metrics in the WMT 2017 sentence-level and system-level tracks, and our trained model has a substantially higher correlation with human judgements than all existing metrics on the WMT 2017 to-English sentence level dataset.", "year": 2019, "publicationdate": "2019-07-01", "externalids": {"DOI": "10.18653/v1/P19-1269"}, "doi_lower": "10.18653/v1/p19-1269"}
{"paper_id": 5959482, "title": "Efficient Estimation of Word Representations in Vector Space", "author_names": ["Tomas Mikolov", "Kai Chen", "G. Corrado", "J. Dean"], "venue": "International Conference on Learning Representations", "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.", "year": 2013, "publicationdate": "2013-01-16", "externalids": {}, "doi_lower": null}
{"paper_id": 247748809, "title": "Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models", "author_names": ["Fatemehsadat Mireshghallah", "Kartik Goyal", "Taylor Berg-Kirkpatrick"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.", "year": 2022, "publicationdate": "2022-03-24", "externalids": {"DOI": "10.48550/arXiv.2203.13299"}, "doi_lower": "10.48550/arxiv.2203.13299"}
{"paper_id": 15486967, "title": "Comparing Person- and Process-centric Strategies for Obtaining Quality Data on Amazon Mechanical Turk", "author_names": ["Tanushree Mitra", "C. J. Hutto", "Eric Gilbert"], "venue": "International Conference on Human Factors in Computing Systems", "abstract": null, "year": 2015, "publicationdate": "2015-04-18", "externalids": {"DOI": "10.1145/2702123.2702553"}, "doi_lower": "10.1145/2702123.2702553"}
{"paper_id": 220364230, "title": "DART: Open-Domain Structured Data Record to Text Generation", "author_names": ["Dragomir R. Radev", "Rui Zhang", "Amrit Rau", "Abhinand Sivaprasad", "Chia-Hsuan Hsieh", "Nazneen Rajani", "Xiangru Tang", "Aadit Vyas", "Neha Verma", "P. Krishna", "Yangxiaokang Liu", "Nadia Irwanto", "Jessica Pan", "Faiaz Rahman", "A. Zaidi", "Murori Mutuma", "Yasin Tarabar", "Ankit Gupta", "Tao Yu", "Y. Tan", "Xi Victoria Lin", "Caiming Xiong", "R. Socher"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-LILY/dart.", "year": 2020, "publicationdate": "2020-07-06", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.37"}, "doi_lower": "10.18653/v1/2021.naacl-main.37"}
{"paper_id": 13690180, "title": "Polite Dialogue Generation Without Parallel Data", "author_names": ["Tong Niu", "Mohit Bansal"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Stylistic dialogue response generation, with valuable applications in personality-based conversational agents, is a challenging task because the response needs to be fluent, contextually-relevant, as well as paralinguistically accurate. Moreover, parallel datasets for regular-to-stylistic pairs are usually unavailable. We present three weakly-supervised models that can generate diverse, polite (or rude) dialogue responses without parallel data. Our late fusion model (Fusion) merges the decoder of an encoder-attention-decoder dialogue model with a language model trained on stand-alone polite utterances. Our label-finetuning (LFT) model prepends to each source sequence a politeness-score scaled label (predicted by our state-of-the-art politeness classifier) during training, and at test time is able to generate polite, neutral, and rude responses by simply scaling the label embedding by the corresponding score. Our reinforcement learning model (Polite-RL) encourages politeness generation by assigning rewards proportional to the politeness classifier score of the sampled response. We also present two retrievalbased, polite dialogue model baselines. Human evaluation validates that while the Fusion and the retrieval-based models achieve politeness with poorer context-relevance, the LFT and Polite-RL models can produce significantly more polite responses without sacrificing dialogue quality.", "year": 2018, "publicationdate": "2018-05-08", "externalids": {"DOI": "10.1162/tacl_a_00027"}, "doi_lower": "10.1162/tacl_a_00027"}
{"paper_id": 3942023, "title": "RankME: Reliable Human Ratings for Natural Language Generation", "author_names": ["Jekaterina Novikova", "Ondrej Dusek", "Verena Rieser"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Human evaluation for natural language generation (NLG) often suffers from inconsistent user ratings. While previous research tends to attribute this problem to individual user preferences, we show that the quality of human judgements can also be improved by experimental design. We present a novel rank-based magnitude estimation method (RankME), which combines the use of continuous scales and relative assessments. We show that RankME significantly improves the reliability and consistency of human ratings compared to traditional evaluation methods. In addition, we show that it is possible to evaluate NLG systems according to multiple, distinct criteria, which is important for error analysis. Finally, we demonstrate that RankME, in combination with Bayesian estimation of system quality, is a cost-effective alternative for ranking multiple NLG systems.", "year": 2018, "publicationdate": "2018-03-15", "externalids": {"DOI": "10.18653/v1/N18-2012"}, "doi_lower": "10.18653/v1/n18-2012"}
{"paper_id": 11080756, "title": "Bleu: a Method for Automatic Evaluation of Machine Translation", "author_names": ["Kishore Papineni", "Salim Roukos", "T. Ward", "Wei-Jing Zhu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.", "year": 2002, "publicationdate": "2002-07-06", "externalids": {"DOI": "10.3115/1073083.1073135"}, "doi_lower": "10.3115/1073083.1073135"}
{"paper_id": 237571784, "title": "A Plug-and-Play Method for Controlled Text Generation", "author_names": ["Damian Pascual", "Béni Egressy", "Clara Meister", "Ryan Cotterell", "R. Wattenhofer"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included, either require additional models or fine-tuning, or work poorly when the task at hand is semantically unconstrained, e.g., story generation. In this work, we present a plug-and-play decoding method for controlled language generation that is so simple and intuitive, it can be described in a single sentence: given a topic or keyword, we add a shift to the probability distribution over our vocabulary towards semantically similar words. We show how annealing this distribution can be used to impose hard constraints on language generation, something no other plug-and-play method is currently able to do with SOTA language generators. Despite the simplicity of this approach, we see it works incredibly well in practice: decoding from GPT-2 leads to diverse and fluent sentences while guaranteeing the appearance of given guide words. We perform two user studies, revealing that (1) our method outperforms competing methods in human evaluations; and (2) forcing the guide words to appear in the generated text has no impact on the fluency of the generated text.", "year": 2021, "publicationdate": "2021-09-20", "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.334"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.334"}
{"paper_id": 3626819, "title": "Deep Contextualized Word Representations", "author_names": ["Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.", "year": 2018, "publicationdate": "2018-02-15", "externalids": {"DOI": "10.18653/v1/N18-1202"}, "doi_lower": "10.18653/v1/n18-1202"}
{"paper_id": 218502475, "title": "Exploring Controllable Text Generation Techniques", "author_names": ["Shrimai Prabhumoye", "A. Black", "R. Salakhutdinov"], "venue": "International Conference on Computational Linguistics", "abstract": "Neural controllable text generation is an important area gaining attention due to its plethora of applications. Although there is a large body of prior work in controllable text generation, there is no unifying theme. In this work, we provide a new schema of the pipeline of the generation process by classifying it into five modules. The control of attributes in the generation process requires modification of these modules. We present an overview of different techniques used to perform the modulation of these modules. We also provide an analysis on the advantages and disadvantages of these techniques. We further pave ways to develop new architectures based on the combination of the modules described in this paper.", "year": 2020, "publicationdate": "2020-05-04", "externalids": {"DOI": "10.18653/V1/2020.COLING-MAIN.1"}, "doi_lower": "10.18653/v1/2020.coling-main.1"}
{"paper_id": 208248333, "title": "Automatically Neutralizing Subjective Bias in Text", "author_names": ["Reid Pryzant", "Richard Diehl Martinez", "Nathan Dass", "S. Kurohashi", "Dan Jurafsky", "Diyi Yang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Texts like news, encyclopedias, and some social media strive for objectivity. Yet bias in the form of inappropriate subjectivity — introducing attitudes via framing, presupposing truth, and casting doubt — remains ubiquitous. This kind of bias erodes our collective trust and fuels social conflict. To address this issue, we introduce a novel testbed for natural language generation: automatically bringing inappropriately subjective text into a neutral point of view (“neutralizing” biased text). We also offer the first parallel corpus of biased language. The corpus contains 180,000 sentence pairs and originates from Wikipedia edits that removed various framings, presuppositions, and attitudes from biased sentences. Last, we propose two strong encoder-decoder baselines for the task. A straightforward yet opaque concurrent system uses a BERT encoder to identify subjective words as part of the generation process. An interpretable and controllable modular algorithm separates these steps, using (1) a BERT-based classifier to identify problematic words and (2) a novel join embedding through which the classifier can edit the hidden states of the encoder. Large-scale human evaluation across four domains (encyclopedias, news headlines, books, and political speeches) suggests that these algorithms are a first step towards the automatic identification and reduction of bias.", "year": 2019, "publicationdate": "2019-11-21", "externalids": {"DOI": "10.1609/aaai.v34i01.5385"}, "doi_lower": "10.1609/aaai.v34i01.5385"}
{"paper_id": 247158838, "title": "Controllable Natural Language Generation with Contrastive Prefixes", "author_names": ["Jing Qian", "Li Dong", "Yelong Shen", "Furu Wei", "Weizhu Chen"], "venue": "Findings", "abstract": "To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator. In this work, we propose a novel lightweight framework for controllable GPT2 generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation. Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously. We propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect control. Experimental results on both single-aspect and multi-aspect control show that our methods can guide generation towards the desired attributes while keeping high linguistic quality.", "year": 2022, "publicationdate": "2022-02-27", "externalids": {"DOI": "10.18653/v1/2022.findings-acl.229"}, "doi_lower": "10.18653/v1/2022.findings-acl.229"}
{"paper_id": 170078973, "title": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function", "author_names": ["Yusu Qian", "Urwa Muaz", "Ben Zhang", "J. Hyun"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Gender bias exists in natural language datasets, which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach and show that it outperforms existing strategies in all bias evaluation metrics.", "year": 2019, "publicationdate": "2019-05-30", "externalids": {"DOI": "10.18653/v1/P19-2031"}, "doi_lower": "10.18653/v1/p19-2031"}
{"paper_id": 174801285, "title": "Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading", "author_names": ["Lianhui Qin", "Michel Galley", "Chris Brockett", "Xiaodong Liu", "Xiang Gao", "W. Dolan", "Yejin Choi", "Jianfeng Gao"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Although neural conversational models are effective in learning how to produce fluent responses, their primary challenge lies in knowing what to say to make the conversation contentful and non-vacuous. We present a new end-to-end approach to contentful neural conversation that jointly models response generation and on-demand machine reading. The key idea is to provide the conversation model with relevant long-form text on the fly as a source of external knowledge. The model performs QA-style reading comprehension on this text in response to each conversational turn, thereby allowing for more focused integration of external knowledge than has been possible in prior approaches. To support further research on knowledge-grounded conversation, we introduce a new large-scale conversation dataset grounded in external web pages (2.8M turns, 7.4M sentences of grounding). Both human evaluation and automated metrics show that our approach results in more contentful responses compared to a variety of previous methods, improving both the informativeness and diversity of generated output.", "year": 2019, "publicationdate": "2019-06-01", "externalids": {"DOI": "10.18653/v1/P19-1539"}, "doi_lower": "10.18653/v1/p19-1539"}
{"paper_id": 247058662, "title": "COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics", "author_names": ["Lianhui Qin", "S. Welleck", "Daniel Khashabi", "Yejin Choi"], "venue": "Neural Information Processing Systems", "abstract": "Many applications of text generation require incorporating different constraints to control the semantics or style of generated text. These constraints can be hard (e.g., ensuring certain keywords are included in the output) and soft (e.g., contextualizing the output with the left- or right-hand context). In this paper, we present Energy-based Constrained Decoding with Langevin Dynamics (COLD), a decoding framework which unifies constrained generation as specifying constraints through an energy function, then performing efficient differentiable reasoning over the constraints through gradient-based sampling. COLD decoding is a flexible framework that can be applied directly to off-the-shelf left-to-right language models without the need for any task-specific fine-tuning, as demonstrated through three challenging text generation applications: lexically-constrained generation, abductive reasoning, and counterfactual reasoning. Our experiments on these constrained generation tasks point to the effectiveness of our approach, both in terms of automatic and human evaluation.", "year": 2022, "publicationdate": "2022-02-23", "externalids": {}, "doi_lower": null}
{"paper_id": 160025533, "title": "Language Models are Unsupervised Multitask Learners", "author_names": ["Alec Radford", "Jeff Wu", "R. Child", "D. Luan", "Dario Amodei", "I. Sutskever"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "author_names": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "venue": "Journal of machine learning research", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.", "year": 2019, "publicationdate": "2019-10-23", "externalids": {}, "doi_lower": null}
{"paper_id": 235898896, "title": "Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features", "author_names": ["Hannah Rashkin", "D. Reitter", "Gaurav Singh Tomar", "Dipanjan Das"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Knowledge-grounded dialogue systems are intended to convey information that is based on evidence provided in a given source text. We discuss the challenges of training a generative neural dialogue model for such systems that is controlled to stay faithful to the evidence. Existing datasets contain a mix of conversational responses that are faithful to selected evidence as well as more subjective or chit-chat style responses. We propose different evaluation measures to disentangle these different styles of responses by quantifying the informativeness and objectivity. At training time, additional inputs based on these evaluation measures are given to the dialogue model. At generation time, these additional inputs act as stylistic controls that encourage the model to generate responses that are faithful to the provided evidence. We also investigate the usage of additional controls at decoding time using resampling techniques. In addition to automatic metrics, we perform a human evaluation study where raters judge the output of these controlled generation models to be generally more objective and faithful to the evidence compared to baseline dialogue systems.", "year": 2021, "publicationdate": "2021-07-14", "externalids": {"DOI": "10.18653/v1/2021.acl-long.58"}, "doi_lower": "10.18653/v1/2021.acl-long.58"}
{"paper_id": 201646309, "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "author_names": ["Nils Reimers", "Iryna Gurevych"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.", "year": 2019, "publicationdate": "2019-08-14", "externalids": {"DOI": "10.18653/v1/D19-1410"}, "doi_lower": "10.18653/v1/d19-1410"}
{"paper_id": 220546439, "title": "Investigating Pretrained Language Models for Graph-to-Text Generation", "author_names": ["Leonardo F. R. Ribeiro", "Martin Schmitt", "Hinrich Schütze", "Iryna Gurevych"], "venue": "NLP4CONVAI", "abstract": "Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recent pretrained language models (PLMs) and analyze the impact of different task-adaptive pretraining strategies for PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. We show that approaches based on PLMs BART and T5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. We report new state-of-the-art BLEU scores of 49.72 on AMR-LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative improvement of 31.8%, 4.5%, and 42.4%, respectively, with our models generating significantly more fluent texts than human references. In an extensive analysis, we identify possible reasons for the PLMs’ success on graph-to-text tasks. Our findings suggest that the PLMs benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels.", "year": 2020, "publicationdate": "2020-07-16", "externalids": {"DOI": "10.18653/v1/2021.nlp4convai-1.20"}, "doi_lower": "10.18653/v1/2021.nlp4convai-1.20"}
{"paper_id": 232240435, "title": "Structural Adapters in Pretrained Language Models for AMR-to-Text Generation", "author_names": ["Leonardo F. R. Ribeiro", "Yue Zhang", "Iryna Gurevych"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Pretrained language models (PLM) have recently advanced graph-to-text generation, where the input graph is linearized into a sequence and fed into the PLM to obtain its representation. However, efficiently encoding the graph structure in PLMs is challenging because such models were pretrained on natural language, and modeling structured data may lead to catastrophic forgetting of distributional knowledge. In this paper, we propose StructAdapt, an adapter method to encode graph structure into PLMs. Contrary to prior work, StructAdapt effectively models interactions among the nodes based on the graph connectivity, only training graph structure-aware adapter parameters. In this way, we incorporate task-specific knowledge while maintaining the topological structure of the graph. We empirically show the benefits of explicitly encoding graph structure into PLMs using StructAdapt, outperforming the state of the art on two AMR-to-text datasets, training only 5.1% of the PLM parameters.", "year": 2021, "publicationdate": "2021-03-16", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.351"}, "doi_lower": "10.18653/v1/2021.emnlp-main.351"}
{"paper_id": 233297067, "title": "Emotion-Regularized Conditional Variational Autoencoder for Emotional Response Generation", "author_names": ["Yu-Ping Ruan", "Zhenhua Ling"], "venue": "IEEE Transactions on Affective Computing", "abstract": "This article presents an emotion-regularized conditional variational autoencoder (Emo-CVAE) model for generating emotional conversation responses. In conventional CVAE-based emotional response generation, emotion labels are simply used as additional conditions in prior, posterior and decoder networks. Considering that emotion styles are naturally entangled with semantic contents in the language space, the Emo-CVAE model utilizes emotion labels to regularize the CVAE latent space by introducing an extra emotion prediction network. In the training stage, the estimated latent variables are required to predict the emotion labels and token sequences of the input responses simultaneously. Experimental results show that our Emo-CVAE model can learn a more informative and structured latent space than a conventional CVAE model and output responses with better content and emotion performance than baseline CVAE and sequence-to-sequence (Seq2Seq) models.", "year": 2021, "publicationdate": "2021-04-18", "externalids": {"DOI": "10.1109/TAFFC.2021.3073809"}, "doi_lower": "10.1109/taffc.2021.3073809"}
{"paper_id": 248571874, "title": "CounterGeDi: A controllable approach to generate polite, detoxified and emotional counterspeech", "author_names": ["Punyajoy Saha", "Kanishk Singh", "Adarsh Kumar", "Binny Mathew", "Animesh Mukherjee"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Recently, many studies have tried to create generation models to assist counter speakers by providing counterspeech suggestions for combating the explosive proliferation of online hate. However, since these suggestions are from a vanilla generation model, they might not include the appropriate properties required to counter a particular hate speech instance. In this paper, we propose CounterGeDi - an ensemble of generative discriminators (GeDi) to guide the generation of a DialoGPT model toward more polite, detoxified, and emotionally laden counterspeech. We generate counterspeech using three datasets and observe significant improvement across different attribute scores. The politeness and detoxification scores increased by around 15% and 6% respectively, while the emotion in the counterspeech increased by at least 10% across all the datasets. We also experiment with triple-attribute control and observe significant improvement over single attribute results when combining complementing attributes, e.g., politeness, joyfulness and detoxification. In all these experiments, the relevancy of the generated text does not deteriorate due to the application of these controls.", "year": 2022, "publicationdate": "2022-05-09", "externalids": {"DOI": "10.48550/arXiv.2205.04304"}, "doi_lower": "10.48550/arxiv.2205.04304"}
{"paper_id": 219721430, "title": "Fine-grained Sentiment Controlled Text Generation", "author_names": ["Bidisha Samanta", "Mohit Agarwal", "Niloy Ganguly"], "venue": "arXiv.org", "abstract": "Controlled text generation techniques aim to regulate specific attributes (e.g. sentiment) while preserving the attribute independent content. The state-of-the-art approaches model the specified attribute as a structured or discrete representation while making the content representation independent of it to achieve a better control. However, disentangling the text representation into separate latent spaces overlooks complex dependencies between content and attribute, leading to generation of poorly constructed and not so meaningful sentences. Moreover, such an approach fails to provide a finer control on the degree of attribute change. To address these problems of controlled text generation, in this paper, we propose DE-VAE, a hierarchical framework which captures both information enriched entangled representation and attribute specific disentangled representation in different hierarchies. DE-VAE achieves better control of sentiment as an attribute while preserving the content by learning a suitable lossless transformation network from the disentangled sentiment space to the desired entangled representation. Through feature supervision on a single dimension of the disentangled representation, DE-VAE maps the variation of sentiment to a continuous space which helps in smoothly regulating sentiment from positive to negative and vice versa. Detailed experiments on three publicly available review datasets show the superiority of DE-VAE over recent state-of-the-art approaches.", "year": 2020, "publicationdate": "2020-06-17", "externalids": {}, "doi_lower": null}
{"paper_id": 253420279, "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "author_names": ["Teven Le Scao", "Angela Fan", "Christopher Akiki", "Ellie Pavlick", "Suzana Ili'c", "Daniel Hesslow", "Roman Castagn'e", "A. Luccioni", "François Yvon", "Matthias Gallé", "J. Tow", "Alexander M. Rush", "Stella Biderman", "Albert Webson", "Pawan Sasanka Ammanamanchi", "Thomas Wang", "Benoît Sagot", "Niklas Muennighoff", "Albert Villanova del Moral", "Olatunji Ruwase", "Rachel Bawden", "Stas Bekman", "Angelina McMillan-Major", "Iz Beltagy", "Huu Nguyen", "Lucile Saulnier", "Samson Tan", "Pedro Ortiz Suarez", "Victor Sanh", "Hugo Laurenccon", "Yacine Jernite", "Julien Launay", "Margaret Mitchell", "Colin Raffel", "Aaron Gokaslan", "Adi Simhi", "Aitor Soroa Etxabe", "Alham Fikri Aji", "Amit Alfassy", "Anna Rogers", "Ariel Kreisberg Nitzav", "Canwen Xu", "Chenghao Mou", "Chris C. Emezue", "Christopher Klamm", "Colin Leong", "Daniel Alexander van Strien", "David Ifeoluwa Adelani", "Dragomir R. Radev", "E. G. Ponferrada", "Efrat Levkovizh", "Ethan Kim", "Eyal Natan", "F. Toni", "Gérard Dupont", "Germán Kruszewski", "Giada Pistilli", "Hady ElSahar", "Hamza Benyamina", "H. Tran", "Ian Yu", "Idris Abdulmumin", "Isaac Johnson", "Itziar Gonzalez-Dios", "Javier de la Rosa", "Jenny Chim", "Jesse Dodge", "Jian Zhu", "Jonathan Chang", "Jorg Frohberg", "Josephine Tobing", "J. Bhattacharjee", "Khalid Almubarak", "Kimbo Chen", "Kyle Lo", "L. V. Werra", "Leon Weber", "Long Phan", "Loubna Ben Allal", "Ludovic Tanguy", "Manan Dey", "M. Muñoz", "Maraim Masoud", "María Grandury", "Mario vSavsko", "Max Huang", "Maximin Coavoux", "Mayank Singh", "Mike Tian-Jian Jiang", "Minh Chien Vu", "M. A. Jauhar", "Mustafa Ghaleb", "Nishant Subramani", "Nora Kassner", "Nurulaqilla Khamis", "Olivier Nguyen", "Omar Espejel", "Ona de Gibert", "Paulo Villegas", "Peter Henderson", "Pierre Colombo", "Priscilla Amuok", "Quentin Lhoest", "Rheza Harliman", "Rishi Bommasani", "R. L'opez", "Rui Ribeiro", "Salomey Osei", "Sampo Pyysalo", "Sebastian Nagel", "Shamik Bose", "Shamsuddeen Hassan Muhammad", "S. Sharma", "S. Longpre", "Somaieh Nikpoor", "S. Silberberg", "S. Pai", "S. Zink", "Tiago Timponi Torrent", "Timo Schick", "Tristan Thrush", "V. Danchev", "Vassilina Nikoulina", "Veronika Laippala", "Violette Lepercq", "V. Prabhu", "Zaid Alyafeai", "Zeerak Talat", "Arun Raja", "Benjamin Heinzerling", "Chenglei Si", "Elizabeth Salesky", "Sabrina J. Mielke", "Wilson Y. Lee", "Abheesht Sharma", "Andrea Santilli", "Antoine Chaffin", "Arnaud Stiegler", "Debajyoti Datta", "Eliza Szczechla", "Gunjan Chhablani", "Han Wang", "Harshit Pandey", "Hendrik Strobelt", "Jason Alan Fries", "Jos Rozen", "Leo Gao", "Lintang Sutawika", "M Saiful Bari", "Maged Saeed Al-shaibani", "Matteo Manica", "Nihal V. Nayak", "R. Teehan", "Samuel Albanie", "Sheng Shen", "Srulik Ben-David", "Stephen H. Bach", "Taewoon Kim", "T. Bers", "Thibault Févry", "Trishala Neeraj", "Urmish Thakker", "Vikas Raunak", "Xiang Tang", "Zheng-Xin Yong", "Zhiqing Sun", "Shaked Brody", "Y. Uri", "Hadar Tojarieh", "Adam Roberts", "Hyung Won Chung", "Jaesung Tae", "Jason Phang", "Ofir Press", "Conglong Li", "D. Narayanan", "Hatim Bourfoune", "J. Casper", "Jeff Rasley", "Max Ryabinin", "Mayank Mishra", "Minjia Zhang", "M. Shoeybi", "Myriam Peyrounette", "N. Patry", "Nouamane Tazi", "Omar Sanseviero", "Patrick von Platen", "Pierre Cornette", "Pierre Franccois Lavall'ee", "R. Lacroix", "Samyam Rajbhandari", "Sanchit Gandhi", "Shaden Smith", "S. Requena", "Suraj Patil", "Tim Dettmers", "Ahmed Baruwa", "Amanpreet Singh", "Anastasia Cheveleva", "Anne-Laure Ligozat", "Arjun Subramonian", "Aur'elie N'ev'eol", "Charles Lovering", "Dan Garrette", "D. Tunuguntla", "Ehud Reiter", "Ekaterina Taktasheva", "E. Voloshina", "Eli Bogdanov", "Genta Indra Winata", "Hailey Schoelkopf", "Jan-Christoph Kalo", "Jekaterina Novikova", "J. Forde", "Xiangru Tang", "Jungo Kasai", "Ken Kawamura", "Liam Hazan", "Marine Carpuat", "Miruna Clinciu", "Najoung Kim", "Newton Cheng", "Oleg Serikov", "Omer Antverg", "Oskar van der Wal", "Rui Zhang", "Ruochen Zhang", "Sebastian Gehrmann", "Shachar Mirkin", "S. Pais", "Tatiana Shavrina", "Thomas Scialom", "Tian Yun", "Tomasz Limisiewicz", "Verena Rieser", "Vitaly Protasov", "V. Mikhailov", "Yada Pruksachatkun", "Yonatan Belinkov", "Zachary Bamberger", "Zdenˇek Kasner", "Zdeněk Kasner", "A. Pestana", "A. Feizpour", "Ammar Khan", "Amy Faranak", "A. Santos", "Anthony Hevia", "Antigona Unldreaj", "Arash Aghagol", "Arezoo Abdollahi", "A. Tammour", "A. HajiHosseini", "Bahareh Behroozi", "Benjamin Ayoade Ajibade", "B. Saxena", "Carlos Muñoz Ferrandis", "Danish Contractor", "D. Lansky", "Davis David", "Douwe Kiela", "D. A. Nguyen", "Edward Tan", "Emi Baylor", "Ez-inwanne Ozoani", "F. Mirza", "Frankline Ononiwu", "Habib Rezanejad", "H.A. Jones", "Indrani Bhattacharya", "Irene Solaiman", "Irina Sedenko", "Isar Nejadgholi", "J. Passmore", "Joshua Seltzer", "Julio Bonis Sanz", "Karen Fort", "Lívia Dutra", "Mairon Samagaio", "Maraim Elbadri", "Margot Mieskes", "M. Gerchick", "Martha Akinlolu", "Michael McKenna", "Mike Qiu", "M. Ghauri", "Mykola Burynok", "Nafis Abrar", "Nazneen Rajani", "Nour Elkott", "N. Fahmy", "Olanrewaju Samuel", "Ran An", "R. Kromann", "Ryan Hao", "S. Alizadeh", "Sarmad Shubber", "Silas L. Wang", "Sourav Roy", "S. Viguier", "Thanh-Cong Le", "Tobi Oyebade", "T. Le", "Yoyo Yang", "Zach Nguyen", "Abhinav Ramesh Kashyap", "A. Palasciano", "A. Callahan", "Anima Shukla", "Antonio Miranda-Escalada", "Ayush Singh", "Benjamin Beilharz", "Bo Wang", "C. Brito", "Chenxi Zhou", "Chirag Jain", "Chuxin Xu", "Clémentine Fourrier", "Daniel Le'on Perin'an", "Daniel Molano", "Dian Yu", "Enrique Manjavacas", "Fabio Barth", "Florian Fuhrimann", "Gabriel Altay", "Giyaseddin Bayrak", "Gully Burns", "Helena U. Vrabec", "I. Bello", "Isha Dash", "J. Kang", "John Giorgi", "Jonas Golde", "J. Posada", "Karthi Sivaraman", "Lokesh Bulchandani", "Lu Liu", "Luisa Shinzato", "Madeleine Hahn de Bykhovetz", "Maiko Takeuchi", "Marc Pàmies", "M. A. Castillo", "Marianna Nezhurina", "Mario Sanger", "M. Samwald", "Michael Cullan", "Michael Weinberg", "M. Wolf", "Mina Mihaljcic", "Minna Liu", "M. Freidank", "Myungsun Kang", "Natasha Seelam", "N. Dahlberg", "N. Broad", "N. Muellner", "Pascale Fung", "Patricia Haller", "Patrick Haller", "R. Eisenberg", "Robert Martin", "Rodrigo Canalli", "Rosaline Su", "Ruisi Su", "Samuel Cahyawijaya", "Samuele Garda", "Shlok S Deshmukh", "Shubhanshu Mishra", "Sid Kiblawi", "Simon Ott", "Sinee Sang-aroonsiri", "Srishti Kumar", "Stefan Schweter", "S. Bharati", "Tanmay Laud", "Théo Gigant", "Tomoya Kainuma", "Wojciech Kusa", "Yanis Labrak", "Yashasvi Bajaj", "Y. Venkatraman", "Yifan Xu", "Ying Xu", "Yu Xu", "Z. Tan", "Zhongli Xie", "Zifan Ye", "M. Bras", "Younes Belkada", "Thomas Wolf"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.", "year": 2022, "publicationdate": "2022-11-09", "externalids": {"DOI": "10.48550/arXiv.2211.05100"}, "doi_lower": "10.48550/arxiv.2211.05100"}
{"paper_id": 211259070, "title": "Discriminative Adversarial Search for Abstractive Summarization", "author_names": ["Thomas Scialom", "Paul-Alexis Dray", "S. Lamprier", "Benjamin Piwowarski", "Jacopo Staiano"], "venue": "International Conference on Machine Learning", "abstract": "We introduce a novel approach for sequence decoding, Discriminative Adversarial Search (DAS), which has the desirable properties of alleviating the effects of exposure bias without requiring external metrics. Inspired by Generative Adversarial Networks (GANs), wherein a discriminator is used to improve the generator, our method differs from GANs in that the generator parameters are not updated at training time and the discriminator is only used to drive sequence generation at inference time. \nWe investigate the effectiveness of the proposed approach on the task of Abstractive Summarization: the results obtained show that a naive application of DAS improves over the state-of-the-art methods, with further gains obtained via discriminator retraining. Moreover, we show how DAS can be effective for cross-domain adaptation. Finally, all results reported are obtained without additional rule-based filtering strategies, commonly used by the best performing systems available: this indicates that DAS can effectively be deployed without relying on post-hoc modifications of the generated outputs.", "year": 2020, "publicationdate": "2020-02-24", "externalids": {}, "doi_lower": null}
{"paper_id": 215548699, "title": "BLEURT: Learning Robust Metrics for Text Generation", "author_names": ["Thibault Sellam", "Dipanjan Das", "Ankur P. Parikh"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.", "year": 2020, "publicationdate": "2020-04-09", "externalids": {"DOI": "10.18653/v1/2020.acl-main.704"}, "doi_lower": "10.18653/v1/2020.acl-main.704"}
{"paper_id": 845121, "title": "Controlling Politeness in Neural Machine Translation via Side Constraints", "author_names": ["Rico Sennrich", "B. Haddow", "Alexandra Birch"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Many languages use honoriﬁcs to express politeness, social distance, or the relative social status between the speaker and their ad-dressee(s). In machine translation from a language without honoriﬁcs such as English, it is difﬁcult to predict the appropriate honoriﬁc, but users may want to control the level of politeness in the output. In this paper, we perform a pilot study to control honoriﬁcs in neural machine translation (NMT) via side constraints , focusing on English → German. We show that by marking up the (English) source side of the training data with a feature that en-codes the use of honoriﬁcs on the (German) target side, we can control the honoriﬁcs produced at test time. Experiments show that the choice of honoriﬁcs has a big impact on translation quality as measured by B LEU , and oracle experiments show that substantial im-provements are possible by constraining the translation to the desired level of politeness.", "year": 2016, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/N16-1005"}, "doi_lower": "10.18653/v1/n16-1005"}
{"paper_id": 240230681, "title": "A Sentiment and Style Controllable Approach for Chinese Poetry Generation", "author_names": ["Yizhan Shao", "Tong Shao", "Minghao Wang", "Peng Wang", "Jie Gao"], "venue": "International Conference on Information and Knowledge Management", "abstract": "Sentiment and style control are two vital aspects in automatic poetry generation. Excellent Chinese classical poetry should express a certain emotion and embody a specific style at the same time. Existing work still has deficiencies in controlling sentiment and style simultaneously. To address above issues, in this paper, we propose a novel approach for Chinese classical poetry generation, which can generate sentiment-controllable and style-controllable poems. First, it classifies hundreds of thousands of poems by style, sentiment, format, and primary keyword. Then, it utilizes masking self-attention mechanism to associate multiple tags and verses. Besides, it can generate metrical rhyming verses with distinctive sentiment and style characteristics according to the tag-set and secondary keywords. Finally, this approach is applied in Chang Qing Yin, which can collaborate with users to polish generated poems, providing alternatives automatically. Experimental results show that our approach performs well in sentiment and style control, and quality of generated poems outperforms several strong baselines.", "year": 2021, "publicationdate": "2021-10-26", "externalids": {"DOI": "10.1145/3459637.3481964"}, "doi_lower": "10.1145/3459637.3481964"}
{"paper_id": 218470535, "title": "Towards Controllable Biases in Language Generation", "author_names": ["Emily Sheng", "Kai-Wei Chang", "P. Natarajan", "Nanyun Peng"], "venue": "Findings", "abstract": "We present a general approach towards controllable societal biases in natural language generation (NLG). Building upon the idea of adversarial triggers, we develop a method to induce societal biases in generated text when input prompts contain mentions of specific demographic groups. We then analyze two scenarios: 1) inducing negative biases for one demographic and positive biases for another demographic, and 2) equalizing biases between demographics. The former scenario enables us to detect the types of biases present in the model. Specifically, we show the effectiveness of our approach at facilitating bias analysis by finding topics that correspond to demographic inequalities in generated text and comparing the relative effectiveness of inducing biases for different demographics. The second scenario is useful for mitigating biases in downstream applications such as dialogue generation. In our experiments, the mitigation technique proves to be effective at equalizing the amount of biases across demographics while simultaneously generating less negatively biased text overall.", "year": 2020, "publicationdate": "2020-05-01", "externalids": {"DOI": "10.18653/v1/2020.findings-emnlp.291"}, "doi_lower": "10.18653/v1/2020.findings-emnlp.291"}
{"paper_id": 228064027, "title": "SongMASS: Automatic Song Writing with Pre-training and Alignment Constraint", "author_names": ["Zhonghao Sheng", "Kaitao Song", "Xu Tan", "Yi Ren", "Wei Ye", "Shikun Zhang", "Tao Qin"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Automatic song writing aims to compose a song (lyric and/or melody) by machine, which is an interesting topic in both academia and industry. In automatic song writing, lyric-to-melody generation and melody-to-lyric generation are two important tasks, both of which usually suffer from the following challenges: 1) the paired lyric and melody data are limited, which affects the generation quality of the two tasks, considering a lot of paired training data are needed due to the weak correlation between lyric and melody; 2) Strict alignments are required between lyric and melody, which relies on specific alignment modeling. In this paper, we propose SongMASS to address the above challenges, which leverages masked sequence to sequence (MASS) pre-training and attention based alignment modeling for lyric-to-melody and melody-to-lyric generation. Specifically, 1) we extend the original sentence-level MASS pre-training to song level to better capture long contextual information in music, and use a separate encoder and decoder for each modality (lyric or melody); 2) we leverage sentence-level attention mask and token-level attention constraint during training to enhance the alignment between lyric and melody. During inference, we use a dynamic programming strategy to obtain the alignment between each word/syllable in lyric and note in melody. We pre-train SongMASS on unpaired lyric and melody datasets, and both objective and subjective evaluations demonstrate that SongMASS generates lyric and melody with significantly better quality than the baseline method.", "year": 2020, "publicationdate": "2020-12-09", "externalids": {"DOI": "10.1609/aaai.v35i15.17626"}, "doi_lower": "10.1609/aaai.v35i15.17626"}
{"paper_id": 220633497, "title": "Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation", "author_names": ["Wenxian Shi", "Hao Zhou", "Ning Miao", "Lei Li"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 2020, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 226222232, "title": "Eliciting Knowledge from Language Models Using Automatically Generated Prompts", "author_names": ["Taylor Shin", "Yasaman Razeghi", "Robert L Logan IV", "Eric Wallace", "Sameer Singh"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.", "year": 2020, "publicationdate": "2020-10-29", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.346"}, "doi_lower": "10.18653/v1/2020.emnlp-main.346"}
{"paper_id": 13936837, "title": "Learning Structured Output Representation using Deep Conditional Generative Models", "author_names": ["Kihyuk Sohn", "Honglak Lee", "Xinchen Yan"], "venue": "Neural Information Processing Systems", "abstract": null, "year": 2015, "publicationdate": "2015-12-07", "externalids": {}, "doi_lower": null}
{"paper_id": 235417177, "title": "BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data", "author_names": ["Haoyu Song", "Yan Wang", "Kaiyan Zhang", "Weinan Zhang", "Ting Liu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Maintaining a consistent persona is essential for dialogue agents. Although tremendous advancements have been brought, the limited-scale of annotated personalized dialogue datasets is still a barrier towards training robust and consistent persona-based dialogue models. This work shows how this challenge can be addressed by disentangling persona-based dialogue generation into two sub-tasks with a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a BERT-based encoder and two BERT-based decoders, where one decoder is for response generation, and another is for consistency understanding. In particular, to learn the ability of consistency understanding from large-scale non-dialogue inference data, we train the second decoder in an unlikelihood manner. Under different limited data settings, both automatic and human evaluations demonstrate that the proposed model outperforms strong baselines in response quality and persona consistency.", "year": 2021, "publicationdate": "2021-06-11", "externalids": {"DOI": "10.18653/v1/2021.acl-long.14"}, "doi_lower": "10.18653/v1/2021.acl-long.14"}
{"paper_id": 215786161, "title": "Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation", "author_names": ["Haoyu Song", "Yan Wang", "Weinan Zhang", "Xiaojiang Liu", "Ting Liu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines. The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models. Despite the success of existing persona-based models on generating human-like responses, their one-stage decoding framework can hardly avoid the generation of inconsistent persona words. In this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one. We carry out evaluations by both human and automatic metrics. Experiments on the Persona-Chat dataset show that our approach achieves good performance.", "year": 2020, "publicationdate": "2020-04-16", "externalids": {"DOI": "10.18653/v1/2020.acl-main.516"}, "doi_lower": "10.18653/v1/2020.acl-main.516"}
{"paper_id": 220045428, "title": "Structural Information Preserving for Graph-to-Text Generation", "author_names": ["Linfeng Song", "Ante Wang", "Jinsong Su", "Yue Zhang", "Kun Xu", "Yubin Ge", "Dong Yu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The task of graph-to-text generation aims at producing sentences that preserve the meaning of input graphs. As a crucial defect, the current state-of-the-art models may mess up or even drop the core structural information of input graphs when generating outputs. We propose to tackle this problem by leveraging richer training signals that can guide our model for preserving input information. In particular, we introduce two types of autoencoding losses, each individually focusing on different aspects (a.k.a. views) of input graphs. The losses are then back-propagated to better calibrate our model via multi-task training. Experiments on two benchmarks for graph-to-text generation show the effectiveness of our approach over a state-of-the-art baseline.", "year": 2020, "publicationdate": "2020-07-01", "externalids": {"DOI": "10.18653/v1/2020.acl-main.712"}, "doi_lower": "10.18653/v1/2020.acl-main.712"}
{"paper_id": 196193055, "title": "Generating Responses with a Specific Emotion in Dialog", "author_names": ["Zhenqiao Song", "Xiaoqing Zheng", "Lu Liu", "Mu Xu", "Xuanjing Huang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "It is desirable for dialog systems to have capability to express specific emotions during a conversation, which has a direct, quantifiable impact on improvement of their usability and user satisfaction. After a careful investigation of real-life conversation data, we found that there are at least two ways to express emotions with language. One is to describe emotional states by explicitly using strong emotional words; another is to increase the intensity of the emotional experiences by implicitly combining neutral words in distinct ways. We propose an emotional dialogue system (EmoDS) that can generate the meaningful responses with a coherent structure for a post, and meanwhile express the desired emotion explicitly or implicitly within a unified framework. Experimental results showed EmoDS performed better than the baselines in BLEU, diversity and the quality of emotional expression.", "year": 2019, "publicationdate": "2019-07-01", "externalids": {"DOI": "10.18653/v1/P19-1359"}, "doi_lower": "10.18653/v1/p19-1359"}
{"paper_id": 221665105, "title": "Learning to summarize from human feedback", "author_names": ["Nisan Stiennon", "Long Ouyang", "Jeff Wu", "Daniel M. Ziegler", "Ryan J. Lowe", "Chelsea Voss", "Alec Radford", "Dario Amodei", "Paul Christiano"], "venue": "Neural Information Processing Systems", "abstract": "As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.", "year": 2020, "publicationdate": "2020-09-02", "externalids": {}, "doi_lower": null}
{"paper_id": 245986550, "title": "A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models", "author_names": ["Hanqing Zhang", "Haolin Song", "Shaoyu Li", "Ming Zhou", "Dawei Song"], "venue": "ACM Computing Surveys", "abstract": "Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of controlled constraints. In this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey article to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.", "year": 2022, "publicationdate": "2022-01-14", "externalids": {"DOI": "10.1145/3617680"}, "doi_lower": "10.1145/3617680"}
{"paper_id": 247444384, "title": "https://habibiaislamicus.com/index.php/hirj/article/view/253", "author_names": ["Shabana Nazar"], "venue": "Habibia Islamicus", "abstract": "Dr. Aisha, Bint Al-Shati, was one of the Pioneers in the Movement of Feminist and Scholars as well in the Arab and Islamic world. She is a famous Writer and a great Linguist of Arabic of 20th Century, Who wrote a series of Books. She was example of patience and jihad, but rather the reward of patience and repentance. She has priority in the different aspects of life. She writes about treasuries of sorrows in her, but she only writes what pleases her ALLAH, Glory be to Him. She also spends her life busy, and keeps her education and deep experience augmented for those who want to provide. She guided everyone who want to get enhance knowledge between obstacles. She left a lot of her memories, as well as left important writings and valuable Books. Thus, this paper is the depiction of Bint Al-Shati’s services and enlightens her views on Contemporary Issues in order to easily and clearly understands her direction in life, and the impact of her culture on literature and criticism for the facilitation of upcoming researchers. His works of Arabic Language and Arabic Literature were revolutionary.", "year": 2021, "publicationdate": "2021-12-30", "externalids": {"DOI": "10.47720/hi.2021.0504a4"}, "doi_lower": "10.47720/hi.2021.0504a4"}
{"paper_id": 237365031, "title": "Plan-then-Generate: Controlled Data-to-Text Generation via Planning", "author_names": ["Yixuan Su", "David Vandyke", "Sihui Wang", "Yimai Fang", "Nigel Collier"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent developments in neural networks have led to the advance in data-to-text generation. However, the lack of ability of neural models to control the structure of generated output can be limiting in certain real-world applications. In this study, we propose a novel Plan-then-Generate (PlanGen) framework to improve the controllability of neural data-to-text models. Extensive experiments and analyses are conducted on two benchmark datasets, ToTTo and WebNLG. The results show that our model is able to control both the intra-sentence and inter-sentence structure of the generated output. Furthermore, empirical comparisons against previous state-of-the-art methods show that our model improves the generation quality as well as the output diversity as judged by human and automatic evaluations.", "year": 2021, "publicationdate": "2021-08-31", "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.76"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.76"}
{"paper_id": 199465680, "title": "Controllable Neural Story Plot Generation via Reward Shaping", "author_names": ["Pradyumna Tambwekar", "Murtaza Dhuliawala", "Lara J. Martin", "Animesh Mehta", "Brent Harrison", "Mark O. Riedl"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Language-modeling--based approaches to story plot generation attempt to construct a plot by sampling from a language model (LM) to predict the next character, word, or sentence to add to the story. LM techniques lack the ability to receive guidance from the user to achieve a specific goal, resulting in stories that don't have a clear sense of progression and lack coherence. We present a reward-shaping technique that analyzes a story corpus and produces intermediate rewards that are backpropagated into a pre-trained LM in order to guide the model toward a given goal. Automated evaluations show our technique can create a model that generates story plots which consistently achieve a specified goal. Human-subject studies show that the generated stories have more plausible event ordering than baseline plot generation techniques.", "year": 2018, "publicationdate": "2018-09-27", "externalids": {"DOI": "10.24963/ijcai.2019/829"}, "doi_lower": "10.24963/ijcai.2019/829"}
{"paper_id": 202763690, "title": "A Topic Augmented Text Generation Model: Joint Learning of Semantics and Structural Features", "author_names": ["Hongyin Tang", "Miao Li", "Beihong Jin"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Text generation is among the most fundamental tasks in natural language processing. In this paper, we propose a text generation model that learns semantics and structural features simultaneously. This model captures structural features by a sequential variational autoencoder component and leverages a topic modeling component based on Gaussian distribution to enhance the recognition of text semantics. To make the reconstructed text more coherent to the topics, the model further adapts the encoder of the topic modeling component for a discriminator. The results of experiments over several datasets demonstrate that our model outperforms several states of the art models in terms of text perplexity and topic coherence. Moreover, the latent representations learned by our model is superior to others in a text classification task. Finally, given the input texts, our model can generate meaningful texts which hold similar structures but under different topics.", "year": 2019, "publicationdate": "2019-11-01", "externalids": {"DOI": "10.18653/v1/D19-1513"}, "doi_lower": "10.18653/v1/d19-1513"}
{"paper_id": 257219404, "title": "LLaMA: Open and Efficient Foundation Language Models", "author_names": ["Hugo Touvron", "Thibaut Lavril", "Gautier Izacard", "Xavier Martinet", "M. Lachaux", "Timothée Lacroix", "Baptiste Rozière", "Naman Goyal", "Eric Hambro", "Faisal Azhar", "Aur'elien Rodriguez", "Armand Joulin", "Edouard Grave", "Guillaume Lample"], "venue": "arXiv.org", "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.", "year": 2023, "publicationdate": "2023-02-27", "externalids": {}, "doi_lower": null}
{"paper_id": 218486908, "title": "ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation", "author_names": ["Lifu Tu", "Richard Yuanzhe Pang", "Sam Wiseman", "Kevin Gimpel"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model. In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy. This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model. Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.", "year": 2020, "publicationdate": "2020-05-02", "externalids": {"DOI": "10.18653/v1/2020.acl-main.251"}, "doi_lower": "10.18653/v1/2020.acl-main.251"}
{"paper_id": 229491990, "title": "Human evaluation of automatically generated text: Current trends and best practice guidelines", "author_names": ["Chris van der Lee", "Albert Gatt", "Emiel van Miltenburg", "E. Krahmer"], "venue": "Computer Speech and Language", "abstract": null, "year": 2021, "publicationdate": "2021-05-21", "externalids": {"DOI": "10.1016/j.csl.2020.101151"}, "doi_lower": "10.1016/j.csl.2020.101151"}
{"paper_id": 233210249, "title": "Not All Attention Is All You Need", "author_names": ["Hongqiu Wu", "Hai Zhao", "Min Zhang"], "venue": "arXiv.org", "abstract": "Beyond the success story of pre-trained language models (PrLMs) in recent natural language processing, they are susceptible to over-fitting due to unusual large model size. To this end, dropout serves as a therapy. However, existing methods like random-based, knowledge-based and search-based dropout are more general but less effective onto self-attention based models, which are broadly chosen as the fundamental architecture of PrLMs. In this paper, we propose a novel dropout method named AttendOut to let self-attention empowered PrLMs capable of more robust task-specific tuning. We demonstrate that state-of-the-art models with elaborate training design may achieve much stronger results. We verify the universality of our approach on extensive natural language processing tasks.", "year": 2021, "publicationdate": "2021-04-10", "externalids": {}, "doi_lower": null}
{"paper_id": 56517433, "title": "Generating lyrics with variational autoencoder and multi-modal artist embeddings", "author_names": ["Olga Vechtomova", "Hareesh Bahuleyan", "Amirpasha Ghabussi", "V. John"], "venue": "arXiv.org", "abstract": "We present a system for generating song lyrics lines conditioned on the style of a specified artist. The system uses a variational autoencoder with artist embeddings. We propose the pre-training of artist embeddings with the representations learned by a CNN classifier, which is trained to predict artists based on MEL spectrograms of their song clips. This work is the first step towards combining audio and text modalities of songs for generating lyrics conditioned on the artist's style. Our preliminary results suggest that there is a benefit in initializing artists' embeddings with the representations learned by a spectrogram classifier.", "year": 2018, "publicationdate": "2018-12-20", "externalids": {}, "doi_lower": null}
{"paper_id": 235390604, "title": "A Template-guided Hybrid Pointer Network for Knowledge-based Task-oriented Dialogue Systems", "author_names": ["Dingmin Wang", "Ziyao Chen", "Wanwei He", "Li Zhong", "Yunzhe Tao", "Min Yang"], "venue": "Workshop on Document-grounded Dialogue and Conversational Question Answering", "abstract": "Most existing neural network based task-oriented dialog systems follow encoder-decoder paradigm, where the decoder purely depends on the source texts to generate a sequence of words, usually suffering from instability and poor readability. Inspired by the traditional template-based generation approaches, we propose a template-guided hybrid pointer network for knowledge-based task-oriented dialog systems, which retrieves several potentially relevant answers from a pre-constructed domain-specific conversational repository as guidance answers, and incorporates the guidance answers into both the encoding and decoding processes. Specifically, we design a memory pointer network model with a gating mechanism to fully exploit the semantic correlation between the retrieved answers and the ground-truth response. We evaluate our model on four widely used task-oriented datasets, including one simulated and three manually created datasets. The experimental results demonstrate that the proposed model achieves significantly better performance than the state-of-the-art methods over different automatic evaluation metrics.", "year": 2021, "publicationdate": "2021-06-10", "externalids": {"DOI": "10.18653/v1/2021.dialdoc-1.3"}, "doi_lower": "10.18653/v1/2021.dialdoc-1.3"}
{"paper_id": 51609768, "title": "SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks", "author_names": ["Ke Wang", "Xiaojun Wan"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Generating texts of different sentiment labels is getting more and more attention in the area of natural language generation. Recently, Generative Adversarial Net (GAN) has shown promising results in text generation. However, the texts generated by GAN usually suffer from the problems of poor quality, lack of diversity and mode collapse. In this paper, we propose a novel framework - SentiGAN, which has multiple generators and one multi-class discriminator, to address the above problems. In our framework, multiple generators are trained simultaneously, aiming at generating texts of different sentiment labels without supervision. We propose a penalty based objective in the generators to force each of them to generate diversified examples of a specific sentiment label. Moreover, the use of multiple generators and one multi-class discriminator can make each generator focus on generating its own examples of a specific sentiment label accurately. Experimental results on four datasets demonstrate that our model consistently outperforms several state-of-the-art text generation methods in the sentiment accuracy and quality of generated texts.", "year": 2018, "publicationdate": "2018-07-01", "externalids": {"DOI": "10.24963/ijcai.2018/618"}, "doi_lower": "10.24963/ijcai.2018/618"}
{"paper_id": 207853041, "title": "Keep it Consistent: Topic-Aware Storytelling from an Image Stream via Iterative Multi-agent Communication", "author_names": ["Ruize Wang", "Zhongyu Wei", "Ying Cheng", "Piji Li", "Haijun Shan", "Ji Zhang", "Qi Zhang", "Xuanjing Huang"], "venue": "International Conference on Computational Linguistics", "abstract": "Visual storytelling aims to generate a narrative paragraph from a sequence of images automatically. Existing approaches construct text description independently for each image and roughly concatenate them as a story, which leads to the problem of generating semantically incoherent content. In this paper, we propose a new way for visual storytelling by introducing a topic description task to detect the global semantic context of an image stream. A story is then constructed with the guidance of the topic description. In order to combine the two generation tasks, we propose a multi-agent communication framework that regards the topic description generator and the story generator as two agents and learn them simultaneously via iterative updating mechanism. We validate our approach on VIST dataset, where quantitative results, ablations, and human evaluation demonstrate our method’s good ability in generating stories with higher quality compared to state-of-the-art methods.", "year": 2019, "publicationdate": "2019-11-11", "externalids": {"DOI": "10.18653/V1/2020.COLING-MAIN.204"}, "doi_lower": "10.18653/v1/2020.coling-main.204"}
{"paper_id": 81982679, "title": "Topic-Guided Variational Auto-Encoder for Text Generation", "author_names": ["Wenlin Wang", "Zhe Gan", "Hongteng Xu", "Ruiyi Zhang", "Guoyin Wang", "Dinghan Shen", "Changyou Chen", "L. Carin"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We propose a topic-guided variational auto-encoder (TGVAE) model for text generation. Distinct from existing variational auto-encoder (VAE) based approaches, which assume a simple Gaussian prior for latent code, our model specifies the prior as a Gaussian mixture model (GMM) parametrized by a neural topic module. Each mixture component corresponds to a latent topic, which provides a guidance to generate sentences under the topic. The neural topic module and the VAE-based neural sequence module in our model are learned jointly. In particular, a sequence of invertible Householder transformations is applied to endow the approximate posterior of the latent code with high flexibility during the model inference. Experimental results show that our TGVAE outperforms its competitors on both unconditional and conditional text generation, which can also generate semantically-meaningful sentences with various topics.", "year": 2019, "publicationdate": "2019-03-17", "externalids": {"DOI": "10.18653/v1/N19-1015"}, "doi_lower": "10.18653/v1/n19-1015"}
{"paper_id": 236460115, "title": "Mention Flags (MF): Constraining Transformer-based Text Generators", "author_names": ["Yufei Wang", "Ian D. Wood", "Stephen Wan", "M. Dras", "Mark Johnson"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "This paper focuses on Seq2Seq (S2S) constrained text generation where the text generator is constrained to mention specific words which are inputs to the encoder in the generated outputs. Pre-trained S2S models or a Copy Mechanism are trained to copy the surface tokens from encoders to decoders, but they cannot guarantee constraint satisfaction. Constrained decoding algorithms always produce hypotheses satisfying all constraints. However, they are computationally expensive and can lower the generated text quality. In this paper, we propose Mention Flags (MF), which traces whether lexical constraints are satisfied in the generated outputs in an S2S decoder. The MF models can be trained to generate tokens in a hypothesis until all constraints are satisfied, guaranteeing high constraint satisfaction. Our experiments on the Common Sense Generation task (CommonGen) (Lin et al., 2020), End2end Restaurant Dialog task (E2ENLG) (Duˇsek et al., 2020) and Novel Object Captioning task (nocaps) (Agrawal et al., 2019) show that the MF models maintain higher constraint satisfaction and text quality than the baseline models and other constrained decoding algorithms, achieving state-of-the-art performance on all three tasks. These results are achieved with a much lower run-time than constrained decoding algorithms. We also show that the MF models work well in the low-resource setting.", "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2021.acl-long.9"}, "doi_lower": "10.18653/v1/2021.acl-long.9"}
{"paper_id": 237416585, "title": "Finetuned Language Models Are Zero-Shot Learners", "author_names": ["Jason Wei", "Maarten Bosma", "Vincent Zhao", "Kelvin Guu", "Adams Wei Yu", "Brian Lester", "Nan Du", "Andrew M. Dai", "Quoc V. Le"], "venue": "International Conference on Learning Representations", "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.", "year": 2021, "publicationdate": "2021-09-03", "externalids": {}, "doi_lower": null}
{"paper_id": 207757493, "title": "Emotion-aware Chat Machine: Automatic Emotional Response Generation for Human-like Emotional Interaction", "author_names": ["Wei Wei", "Jiayi Liu", "Xian-Ling Mao", "G. Guo", "Feida Zhu", "Pan Zhou", "Yuchong Hu"], "venue": "International Conference on Information and Knowledge Management", "abstract": "The consistency of a response to a given post at semantic-level and emotional-level is essential for a dialogue system to deliver human-like interactions. However, this challenge is not well addressed in the literature, since most of the approaches neglect the emotional information conveyed by a post while generating responses. This article addresses this problem by proposing a unified end-to-end neural architecture, which is capable of simultaneously encoding the semantics and the emotions in a post for generating more intelligent responses with appropriately expressed emotions. Extensive experiments on real-world data demonstrate that the proposed method outperforms the state-of-the-art methods in terms of both content coherence and emotion appropriateness.", "year": 2019, "publicationdate": "2019-11-03", "externalids": {"DOI": "10.1145/3357384.3357937"}, "doi_lower": "10.1145/3357384.3357937"}
{"paper_id": 199551982, "title": "Neural Text Generation with Unlikelihood Training", "author_names": ["S. Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "J. Weston"], "venue": "International Conference on Learning Representations", "abstract": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-$k$ and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.", "year": 2019, "publicationdate": "2019-08-12", "externalids": {}, "doi_lower": null}
{"paper_id": 218487421, "title": "A Controllable Model of Grounded Response Generation", "author_names": ["Zeqiu Wu", "Michel Galley", "Chris Brockett", "Yizhe Zhang", "Xiang Gao", "Chris Quirk", "Rik Koncel-Kedziorski", "Jianfeng Gao", "Hannaneh Hajishirzi", "Mari Ostendorf", "Bill Dolan"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Current end-to-end neural conversation models inherently lack the flexibility to impose semantic control in the response generation process, often resulting in uninteresting responses. Attempts to boost informativeness alone come at the expense of factual accuracy, as attested by pretrained language models' propensity to \"hallucinate\" facts. While this may be mitigated by access to background knowledge, there is scant guarantee of relevance and informativeness in generated responses. We propose a framework that we call controllable grounded response generation (CGRG), in which lexical control phrases are either provided by a user or automatically extracted by a control phrase predictor from dialogue context and grounding knowledge. Quantitative and qualitative results show that, using this framework, a transformer based model with a novel inductive attention mechanism, trained on a conversation-like Reddit dataset, outperforms strong generation baselines.", "year": 2020, "publicationdate": "2020-05-01", "externalids": {"DOI": "10.1609/aaai.v35i16.17658"}, "doi_lower": "10.1609/aaai.v35i16.17658"}
{"paper_id": 241035330, "title": "An Explanation of In-context Learning as Implicit Bayesian Inference", "author_names": ["Sang Michael Xie", "Aditi Raghunathan", "Percy Liang", "Tengyu Ma"], "venue": "International Conference on Learning Representations", "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.", "year": 2021, "publicationdate": "2021-11-03", "externalids": {}, "doi_lower": null}
{"paper_id": 167217319, "title": "Unsupervised Controllable Text Generation with Global Variation Discovery and Disentanglement", "author_names": ["Peng Xu", "Yanshuai Cao", "J. Cheung"], "venue": "arXiv.org", "abstract": "Existing controllable text generation systems rely on annotated attributes, which greatly limits their capabilities and applications. In this work, we make the first successful attempt to use VAEs to achieve controllable text generation without supervision. We do so by decomposing the latent space of the VAE into two parts: one incorporates structural constraints to capture dominant global variations implicitly present in the data, e.g., sentiment or topic; the other is unstructured and is used for the reconstruction of the source sentences. With the enforced structural constraint, the underlying global variations will be discovered and disentangled during the training of the VAE. The structural constraint also provides a natural recipe for mitigating posterior collapse for the structured part, which cannot be fully resolved by the existing techniques. On the task of text style transfer, our unsupervised approach achieves significantly better performance than previous supervised approaches. By showcasing generation with finer-grained control including Cards-Against-Humanity-style topic transitions within a sentence, we demonstrate that our model can perform controlled text generation in a more flexible way than existing methods.", "year": 2019, "publicationdate": "2019-05-28", "externalids": {}, "doi_lower": null}
{"paper_id": 222125036, "title": "Controllable Story Generation with External Knowledge Using Large-Scale Language Models", "author_names": ["Peng Xu", "M. Patwary", "M. Shoeybi", "Raul Puri", "Pascale Fung", "Anima Anandkumar", "Bryan Catanzaro"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding. The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset. We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process. Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords. Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).", "year": 2020, "publicationdate": "2020-10-02", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.226"}, "doi_lower": "10.18653/v1/2020.emnlp-main.226"}
{"paper_id": 233210709, "title": "FUDGE: Controlled Text Generation With Future Discriminators", "author_names": ["Kevin Yang", "D. Klein"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a pre-existing model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G’s output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor’s outputs to adjust G’s original probabilities. We show that FUDGE models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks — couplet completion in poetry, topic control in language generation, and formality change in machine translation — and observe gains in all three tasks.", "year": 2021, "publicationdate": "2021-04-11", "externalids": {"DOI": "10.18653/v1/2021.naacl-main.276"}, "doi_lower": "10.18653/v1/2021.naacl-main.276"}
{"paper_id": 248426828, "title": "Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation", "author_names": ["Kexin Yang", "Dayiheng Liu", "Wenqiang Lei", "Baosong Yang", "Mingfeng Xue", "Boxing Chen", "Jun Xie"], "venue": "arXiv.org", "abstract": "Attribute-based Controlled Text Generation (CTG) refers to generating sentences that satisfy desirable attributes (e.g., emotions and topics). Existing works often utilize fine-tuning or resort to extra attribute classifiers, yet suffer from storage and inference time increases. To address these concerns, we explore attribute-based CTG in a prompt-based manner. In short, the proposed Tailor represents each attribute as a pre-trained continuous vector (i.e., single-attribute prompt) and guides the generation of a fixed PLM switch to a pre-specified attribute. We experimentally find that these prompts can be simply concatenated as a whole to multi-attribute CTG without any re-training, yet raises problems of fluency decrease and position sensitivity. To this end, Tailor provides a multi-attribute prompt mask and a re-indexing position-ids sequence to bridge the gap between the training (one prompt for each task) and testing stage (concatenating more than one prompt). To further enhance such single-attribute prompt combinations, Tailor also introduces a trainable prompt connector, which can be concatenated with any two single-attribute prompts to multi-attribute text generation. Experiments on 11 attribute-specific generation tasks demonstrate strong performances of Tailor on both single-attribute and multi-attribute CTG, with 0.08\\% training parameters of a GPT-2.", "year": 2022, "publicationdate": "2022-04-28", "externalids": {"DOI": "10.48550/arXiv.2204.13362"}, "doi_lower": "10.48550/arxiv.2204.13362"}
{"paper_id": 196170658, "title": "Enhancing Topic-to-Essay Generation with External Commonsense Knowledge", "author_names": ["Pengcheng Yang", "Lei Li", "Fuli Luo", "Tianyu Liu", "Xu Sun"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Automatic topic-to-essay generation is a challenging task since it requires generating novel, diverse, and topic-consistent paragraph-level text with a set of topics as input. Previous work tends to perform essay generation based solely on the given topics while ignoring massive commonsense knowledge. However, this commonsense knowledge provides additional background information, which can help to generate essays that are more novel and diverse. Towards filling this gap, we propose to integrate commonsense from the external knowledge base into the generator through dynamic memory mechanism. Besides, the adversarial training based on a multi-label discriminator is employed to further improve topic-consistency. We also develop a series of automatic evaluation metrics to comprehensively assess the quality of the generated essay. Experiments show that with external commonsense knowledge and adversarial training, the generated essays are more novel, diverse, and topic-consistent than existing methods in terms of both automatic and human evaluation.", "year": 2019, "publicationdate": "2019-07-01", "externalids": {"DOI": "10.18653/v1/P19-1193"}, "doi_lower": "10.18653/v1/p19-1193"}
{"paper_id": 32239851, "title": "Generating Thematic Chinese Poetry using Conditional Variational Autoencoders with Hybrid Decoders", "author_names": ["Xiaopeng Yang", "Xiaowen Lin", "Simon Suo", "Ming Li"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Computer poetry generation is our first step towards computer writing. Writing must have a theme. The current approaches of using sequence-to-sequence models with attention often produce non-thematic poems. We present a novel conditional variational autoencoder with a hybrid decoder adding the deconvolutional neural networks to the general recurrent neural networks to fully learn topic information via latent variables. This approach significantly improves the relevance of the generated poems by representing each line of the poem not only in a context-sensitive manner but also in a holistic way that is highly related to the given keyword and the learned topic. A proposed augmented word2vec model further improves the rhythm and symmetry. Tests show that the generated poems by our approach are mostly satisfying with regulated rules and consistent themes, and 73.42% of them receive an Overall score no less than 3 (the highest score is 5).", "year": 2017, "publicationdate": "2017-11-21", "externalids": {"DOI": "10.24963/ijcai.2018/631"}, "doi_lower": "10.24963/ijcai.2018/631"}
{"paper_id": 195069387, "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "author_names": ["Zhilin Yang", "Zihang Dai", "Yiming Yang", "J. Carbonell", "R. Salakhutdinov", "Quoc V. Le"], "venue": "Neural Information Processing Systems", "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.", "year": 2019, "publicationdate": "2019-06-19", "externalids": {}, "doi_lower": null}
{"paper_id": 220265537, "title": "Technical Report: Auxiliary Tuning and its Application to Conditional Text Generation", "author_names": ["Yoel Zeldes", "Dan Padnos", "Or Sharir", "Barak Peleg"], "venue": "arXiv.org", "abstract": "We introduce a simple and efficient method, called Auxiliary Tuning, for adapting a pre-trained Language Model to a novel task; we demonstrate this approach on the task of conditional text generation. Our approach supplements the original pre-trained model with an auxiliary model that shifts the output distribution according to the target task. The auxiliary model is trained by adding its logits to the pre-trained model logits and maximizing the likelihood of the target task output. Our method imposes no constraints on the auxiliary architecture. In particular, the auxiliary model can ingest additional input relevant to the target task, independently from the pre-trained model's input. Furthermore, mixing the models at the logits level provides a natural probabilistic interpretation of the method. Our method achieved similar results to training from scratch for several different tasks, while using significantly fewer resources for training; we share a specific example of text generation conditioned on keywords.", "year": 2020, "publicationdate": "2020-06-30", "externalids": {}, "doi_lower": null}
{"paper_id": 235097623, "title": "A Simple and Efficient Multi-Task Learning Approach for Conditioned Dialogue Generation", "author_names": ["Yan Zeng", "J. Nie"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Conditioned dialogue generation suffers from the scarcity of labeled responses. In this work, we exploit labeled non-dialogue text data related to the condition, which are much easier to collect. We propose a multi-task learning approach to leverage both labeled dialogue and text data. The 3 tasks jointly optimize the same pre-trained Transformer – conditioned dialogue generation task on the labeled dialogue data, conditioned language encoding task and conditioned language generation task on the labeled text data. Experimental results show that our approach outperforms the state-of-the-art models by leveraging the labeled texts, and it also obtains larger improvement in performance comparing to the previous methods to leverage text data.", "year": 2021, "publicationdate": "2021-06-01", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.392"}, "doi_lower": "10.18653/v1/2021.naacl-main.392"}
{"paper_id": 252967684, "title": "DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation", "author_names": ["Hanqing Zhang", "Dawei Song"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Prompt learning with immensely large Casual Language Models (CLMs) has been shown promising for attribute-controllable text generation (CTG). However, vanilla prompt tuning tends to imitate training corpus characteristics beyond the control attributes, resulting in a poor generalization ability. Moreover, it is less able to capture the relationship between different attributes, further limiting the control performance. In this paper, we propose a new CTG approach, namely DisCup, which incorporates the attribute knowledge of discriminator to optimize the control-prompts, steering a frozen CLM to produce attribute-specific texts. Specifically, the frozen CLM model, capable of producing multitudinous texts, is first used to generate the next-token candidates based on the context, so as to ensure the diversity of tokens to be predicted. Then, we leverage an attribute-discriminator to select desired/undesired tokens from those candidates, providing the inter-attribute knowledge. Finally, we bridge the above two traits by an unlikelihood objective for prompt-tuning. Extensive experimental results show that DisCup can achieve a new state-of-the-art control performance while maintaining an efficient and high-quality text generation, only relying on around 10 virtual tokens.", "year": 2022, "publicationdate": "2022-10-18", "externalids": {"DOI": "10.48550/arXiv.2210.09551"}, "doi_lower": "10.48550/arxiv.2210.09551"}
{"paper_id": 199489014, "title": "Emotional Text Generation Based on Cross-Domain Sentiment Transfer", "author_names": ["Rui Zhang", "Zhenyu Wang", "Kai Yin", "Zhenhua Huang"], "venue": "IEEE Access", "abstract": "Emotional intelligence plays an important role in human intelligence and is a recent research hotspot. With the rapid development of deep learning techniques in recent years, several neural network-based emotional text generation methods have been investigated. However, the existing emotional text generation approaches often suffer from the problem of requiring large-scale annotated data. Generative adversarial network (GAN) has shown promising results in natural language generation and data enhancement. In order to solve the above problem, this paper proposes a GAN-based cross-domain text sentiment transfer model, which uses annotated data from other domains to assist in the training of emotional text generation network. By combining adversarial reinforcement learning with supervised learning, our model is able to extract patterns of sentiment transformation and apply them in emotional text generation. The experimental results have shown that our approach outperforms the state-of-the-art methods and is able to generate high-quality emotional text while maintaining the consistency of domain information and content semantics.", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.1109/ACCESS.2019.2931036"}, "doi_lower": "10.1109/access.2019.2931036"}
{"paper_id": 6869582, "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?", "author_names": ["Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "J. Weston"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.", "year": 2018, "publicationdate": "2018-01-22", "externalids": {"DOI": "10.18653/v1/P18-1205"}, "doi_lower": "10.18653/v1/p18-1205"}
{"paper_id": 127986044, "title": "BERTScore: Evaluating Text Generation with BERT", "author_names": ["Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi"], "venue": "International Conference on Learning Representations", "abstract": "We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.", "year": 2019, "publicationdate": "2019-04-21", "externalids": {}, "doi_lower": null}
{"paper_id": 207869708, "title": "DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation", "author_names": ["Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "W. Dolan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present a large, tunable neural conversational response generation model, DIALOGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.", "year": 2019, "publicationdate": "2019-11-01", "externalids": {"DOI": "10.18653/V1/2020.ACL-DEMOS.30"}, "doi_lower": "10.18653/v1/2020.acl-demos.30"}
{"paper_id": 226604173, "title": "POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training", "author_names": ["Yizhe Zhang", "Guoyin Wang", "Chunyuan Li", "Zhe Gan", "Chris Brockett", "Bill Dolan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these models cannot be directly employed to generate text under specified lexical constraints. To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation. The proposed method operates by progressively inserting new tokens between existing tokens in a parallel manner. This procedure is recursively applied until a sequence is completed. The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable. We pre-train our model with the proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks. Non-autoregressive decoding yields an empirically logarithmic time complexity during inference time. Experimental results on both News and Yelp datasets demonstrate that POINTER achieves state-of-the-art performance on constrained text generation. We released the pre-trained models and the source code to facilitate future research (this https URL).", "year": 2020, "publicationdate": "2020-05-01", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.698"}, "doi_lower": "10.18653/v1/2020.emnlp-main.698"}
{"paper_id": 158046772, "title": "ERNIE: Enhanced Language Representation with Informative Entities", "author_names": ["Zhengyan Zhang", "Xu Han", "Zhiyuan Liu", "Xin Jiang", "Maosong Sun", "Qun Liu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.", "year": 2019, "publicationdate": "2019-05-17", "externalids": {"DOI": "10.18653/v1/P19-1139"}, "doi_lower": "10.18653/v1/p19-1139"}
{"paper_id": 13619197, "title": "Calibrating Energy-based Generative Adversarial Networks", "author_names": ["Zihang Dai", "Amjad Almahairi", "Philip Bachman", "E. Hovy", "Aaron C. Courville"], "venue": "International Conference on Learning Representations", "abstract": "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples. Specifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.", "year": 2017, "publicationdate": "2017-02-01", "externalids": {}, "doi_lower": null}
{"paper_id": 207863734, "title": "A Pre-training Based Personalized Dialogue Generation Model with Persona-sparse Data", "author_names": ["Yinhe Zheng", "Rongsheng Zhang", "Xiaoxi Mao", "Minlie Huang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Endowing dialogue systems with personas is essential to deliver more human-like conversations. However, this problem is still far from well explored due to the difficulties of both embodying personalities in natural languages and the persona sparsity issue observed in most dialogue corpora. This paper proposes a pre-training based personalized dialogue model that can generate coherent responses using persona-sparse dialogue data. In this method, a pre-trained language model is used to initialize an encoder and decoder, and personal attribute embeddings are devised to model richer dialogue contexts by encoding speakers' personas together with dialogue histories. Further, to incorporate the target persona in the decoding process and to balance its contribution, an attention routing structure is devised in the decoder to merge features extracted from the target persona and dialogue contexts using dynamically predicted weights. Our model can utilize persona-sparse dialogues in a unified manner during the training process, and can also control the amount of persona-related features to exhibit during the inference process. Both automatic and manual evaluation demonstrates that the proposed model outperforms state-of-the-art methods for generating more coherent and persona consistent responses with persona-sparse data.", "year": 2019, "publicationdate": "2019-11-12", "externalids": {"DOI": "10.1609/AAAI.V34I05.6518"}, "doi_lower": "10.1609/aaai.v34i05.6518"}
{"paper_id": 271403894, "title": "Towards Persona-Based Empathetic Conversational Models", "author_names": ["Peixiang Zhong", "Chen Zhang", "Hao Wang", "Yong Liu", "Chun Miao"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains. In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy. In addition, our empirical analysis also suggests that persona plays an important role in empathetic conversations. To this end, we propose a new task towards persona-based empathetic conversations and present the first empirical study on the impact of persona on empathetic responding. Specifically, we first present a novel large-scale multi-domain dataset for persona-based empathetic conversations. We then propose CoBERT, an efficient BERT-based response selection model that obtains the state-of-the-art performance on our dataset. Finally, we conduct extensive experiments to investigate the impact of persona on empathetic responding. Notably, our results show that persona improves empathetic responding more when CoBERT is trained on empathetic conversations than non-empathetic ones, establishing an empirical link between persona and empathy in human conversations.", "year": 2020, "publicationdate": "2020-04-26", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.531"}, "doi_lower": "10.18653/v1/2020.emnlp-main.531"}
{"paper_id": 211082630, "title": "Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models", "author_names": ["Wangchunshu Zhou", "Ke Xu"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Automated evaluation of open domain natural language generation (NLG) models remains a challenge and widely used metrics such as BLEU and Perplexity can be misleading in some cases. In our paper, we propose to evaluate natural language generation models by learning to compare a pair of generated sentences by fine-tuning BERT, which has been shown to have good natural language understanding ability. We also propose to evaluate the model-level quality of NLG models with sample-level comparison results with skill rating system. While able to be trained in a fully self-supervised fashion, our model can be further fine-tuned with a little amount of human preference annotation to better imitate human judgment. In addition to evaluating trained models, we propose to apply our model as a performance indicator during training for better hyperparameter tuning and early-stopping. We evaluate our approach on both story generation and chit-chat dialogue response generation. Experimental results show that our model correlates better with human preference compared with previous automated evaluation approaches. Training with the proposed metric yields better performance in human evaluation, which further demonstrates the effectiveness of the proposed model.", "year": 2020, "publicationdate": "2020-02-12", "externalids": {"DOI": "10.1609/aaai.v34i05.6521"}, "doi_lower": "10.1609/aaai.v34i05.6521"}
{"paper_id": 3636178, "title": "Texygen: A Benchmarking Platform for Text Generation Models", "author_names": ["Yaoming Zhu", "Sidi Lu", "Lei Zheng", "Jiaxian Guo", "Weinan Zhang", "Jun Wang", "Yong Yu"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "We introduce Texygen, a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and improve the reproductivity and reliability of future research work in text generation.", "year": 2018, "publicationdate": "2018-02-06", "externalids": {"DOI": "10.1145/3209978.3210080"}, "doi_lower": "10.1145/3209978.3210080"}
{"paper_id": 202660943, "title": "Fine-Tuning Language Models from Human Preferences", "author_names": ["Daniel M. Ziegler", "Nisan Stiennon", "Jeff Wu", "Tom B. Brown", "Alec Radford", "Dario Amodei", "Paul Christiano", "G. Irving"], "venue": "arXiv.org", "abstract": "Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.", "year": 2019, "publicationdate": "2019-09-18", "externalids": {}, "doi_lower": null}
{"paper_id": 232290492, "title": "Controllable Generation from Pre-trained Language Models via Inverse Prompting", "author_names": ["Xu Zou", "Da Yin", "Qingyang Zhong", "Hongxia Yang", "Zhilin Yang", "Jie Tang"], "venue": "Knowledge Discovery and Data Mining", "abstract": "Large-scale pre-trained language models have demonstrated strong capabilities of generating realistic texts. However, it remains challenging to control the generation results. Previous approaches such as prompting are far from sufficient, and lack of controllability limits the usage of language models. To tackle this challenge, we propose an innovative method, inverse prompting, to better control text generation. The core idea of inverse prompting is to use generated text to inversely predict the prompt during beam search, which enhances the relevance between the prompt and the generated text and thus improves controllability. Empirically, we pre-train a large-scale Chinese language model to perform a systematic study using human evaluation on the tasks of open-domain poem generation and open-domain long-form question answering. Results demonstrate that our proposed method substantially outperforms the baselines and that our generation quality is close to human performance on some of the tasks.", "year": 2021, "publicationdate": "2021-03-19", "externalids": {"DOI": "10.1145/3447548.3467418"}, "doi_lower": "10.1145/3447548.3467418"}
