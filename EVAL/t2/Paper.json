{
  "authors": [
    "Haitao Li",
    "Qian Dong",
    "Junjie Chen",
    "Huixue Su",
    "Yujia Zhou",
    "Qingyao Ai",
    "Ziyi Ye",
    "Yiqun Liu"
  ],
  "literature_review_title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
  "year": "2024",
  "date": "2024-12-07",
  "category": "cs.CL",
  "abstract": "The rapid advancement of Large Language Models (LLMs) has driven their\nexpanding application across various fields. One of the most promising\napplications is their role as evaluators based on natural language responses,\nreferred to as ''LLMs-as-judges''. This framework has attracted growing\nattention from both academia and industry due to their excellent effectiveness,\nability to generalize across tasks, and interpretability in the form of natural\nlanguage. This paper presents a comprehensive survey of the LLMs-as-judges\nparadigm from five key perspectives: Functionality, Methodology, Applications,\nMeta-evaluation, and Limitations. We begin by providing a systematic definition\nof LLMs-as-Judges and introduce their functionality (Why use LLM judges?). Then\nwe address methodology to construct an evaluation system with LLMs (How to use\nLLM judges?). Additionally, we investigate the potential domains for their\napplication (Where to use LLM judges?) and discuss methods for evaluating them\nin various contexts (How to evaluate LLM judges?). Finally, we provide a\ndetailed analysis of the limitations of LLM judges and discuss potential future\ndirections. Through a structured and comprehensive analysis, we aim aims to\nprovide insights on the development and application of LLMs-as-judges in both\nresearch and practice. We will continue to maintain the relevant resource list\nat https://github.com/CSHaitao/Awesome-LLMs-as-Judges.",
  "structure": [
    {
      "section_title": "Introduction/Pre-section",
      "level": "1",
      "content": "\\documentclass[acmsmall,screen]{acmart} booktabs % For formal tables multirow booktabs % For formal tables multirow subfigure array float \\usepackage[utf8]{inputenc} % allow utf-8 input \\usepackage[T1]{fontenc} % use 8-bit T1 fonts hyperref % hyperlinks setspace threeparttable supertabular bm amsmath amsthm mathrsfs booktabs siunitx footmisc footnote enumitem array CJK footnote enumitem tabularx colortbl tikz \\usepackage[edges]{forest} trees,positioning,shapes,shadows,arrows.meta \\usepackage[edges]{forest} hidden-draw{RGB}{0,0,0} hidden-blue{RGB}{194,232,247} hidden-orange{RGB}{243,202,120} hidden-yellow{RGB}{242,244,193} tree-level-1{RGB}{245,20,85} tree-level-2{RGB}{246,86,118} tree-level-3{RGB}{248,177,193} tree-leaf{RGB}{176,230,198} % \\providecommand\\BibTeX{{% \\normalfont B\\kern-0.5em{\\scshape i\\kern-0.25em b\\kern-0.8em\\TeX}}} document LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods Haitao Li liht22@mails.tsinghua.edu.cn % \\institution{Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University Beijing China } Qian Dong dq22@mails.tsinghua.edu.cn % \\institution{Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University Beijing China } Junjie Chen chenjj826@gmail.com % \\institution{Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University Beijing China } Huixue Su suhuixue@ruc.edu.cn % \\institution{Gaoling School of Artificial Intelligence, Renmin University of China Beijing China } Yujia Zhou suhuixue@ruc.edu.cn % \\institution{Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University Beijing China } Qingyao Ai aiqy@tsinghua.edu.cn % \\institution{Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University Beijing China } Ziyi Ye yeziyi1998@gmail.com % \\institution{Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University Beijing China } Yiqun Liu yiqunliu@tsinghua.edu.cn % \\institution{Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University Beijing China } \\shortauthors{Li, et al.} \\yzy[1]{blue{#1}} Large Language Models, Evaluation, LLMs-as-Judges \\maketitle",
      "origin_cites_number": 0
    },
    {
      "section_title": "Introduction",
      "level": "1",
      "content": "Studies on evaluation methods have long been a key force in guiding the development of modern Artificial Intelligence (AI)~chang2024survey. AI researchers have continuously sought to measure and validate the intelligence of AI models through various tasks~chang2024survey,guo2023evaluating. In the mid-20th century, AI evaluation primarily centered on assessing algorithm performance in specific tasks, such as logical reasoning and numerical computation~nilsson2014principles. Traditional machine learning tasks like classification and regression often use programmable and statistical metrics, including accuracy, precision, and recall. With the emergence of deep learning, the complexity of AI systems grew rapidly, prompting a shift in evaluation standards~lecun2015deep. The evaluation of AI has expanded from pre-defined, programmable machine metrics to more flexible, robust evaluators for solving complex, realistic tasks. A typical example is the Turing Test~french2000turing,turing2009computing, which determines whether an AI model can exhibit human-like intelligent behavior through dialogue with humans. The Turing Test provides a fundamental guideline in the evaluation of AI models, especially on AI models' intelligence in flexible and realistic environments. Recently, the emergence of Large Language Models (LLMs) and generative AI serves as a new milestone in the evolution of AI evaluation. LLMs exhibit remarkable generalization and adaptability, showcasing strong transfer capabilities across previously unseen tasks and diverse domains~achiam2023gpt,bai2023qwen. However, their powerful capabilities also present new challenges for evaluation. Due to the highly generative and open-ended nature of their outputs, standardized metrics are often insufficient for a comprehensive evaluation. For example, in natural language generation (NLG) tasks, traditional metrics like BLEU~papineni2002bleu and ROUGE~lin2004rouge often fail to capture key aspects such as text fluency, logical coherence, and creativity. Moreover, modern AI evaluation extends beyond task performance and must account for the ability to address complex, dynamic problems in real-world scenarios, including robustness, fairness, and interpretability. Human annotations, frequently regarded as the ``ground truth,'' can offer comprehensive insights and valuable feedback. By gathering responses from experts or users, researchers can gain a deeper understanding of a model's performance, practicality, and potential risks. However, collecting them are typically time-consuming and resource-intensive, making it challenging to scale up for large-scale evaluation. In this context, a new paradigm has emerged to replace humans and statistical metrics with LLMs in evaluation, referred to as LLMs-as-judges~ashktorab2024aligning,tseng2024expert,bavaresco2024llms,bavaresco2024llms. Compared to traditional evaluation methods, LLMs-as-judges show significant strengths. First, LLM judges can adjust their evaluation criteria based on the specific task context, rather than relying on a fixed set of metrics, making the evaluation process more flexible and refined. Second, LLM judges can generate interpretive evaluations, offering more comprehensive feedback on model performance and enabling researchers to gain deeper insights into the evaluater's strengths and weaknesses. Finally, LLM judges offer a scalable and reproducible alternative to human evaluation, significantly reducing the costs and time associated with human involvement. Despite its great potential and significant advantages, LLMs-as-judges also face several critical challenges. For example, the evaluation results of LLMs are often influenced by the prompt template, which can lead to biased or inconsistent assessments~xu2023llm. Considering that LLMs are trained on extensive text corpus, they may also inherit various implicit biases, impacting the fairness and reliability of their assessments~ye2024justice. Moreover, distinct tasks and domains require specific evaluation criteria, making it difficult for LLMs to adapt their standards dynamically to specific contexts. Considering the vast potential of this field, this survey aims to systematically review and analyze the current state and key challenges of the LLMs-as-judges. As shown in Figures Taxonomy_1 and Taxonomy_2, we discuss existing research across five key perspectives: 1) Functionality: Why use LLM judges, 2) Methodology: How to use LLM judges, 3) Application: Where to use LLM judges, 4) Meta-evaluation: How to evaluate LLM judges and 5) Limitation: Existing problems of LLM judges. We explore the key challenges confronting LLMs-as-judges and hope to provide a clearer guideline for their future development. In summary, the main contributions of this paper are as follows: enumerate \\item Comprehensive and Timely Survey: We present the extensive survey on the emerging paradigm of LLMs-as-judges, systematically reviewing the current state of research and developments in this field. By examining LLMs as performance evaluators based on their generated natural language, we highlight the unique role of LLMs in shaping the future of AI evaluation. \\item Systematic Analysis Across Five Key Perspectives: We organize our survey around five critical aspects: Functionality, Methodology, Application, Meta-evaluation, and Limitation. This structured approach allows for a nuanced understanding of how and why LLMs are utilized as evaluators, their practical implementations, and reliability concerns. \\item Current Challenges and Future Research Directions: We discuss the existing challenges for adopting LLMs-as-judges, highlighting potential research opportunities and directions while offering a forward-looking perspective on the future development of this paradigm, encouraging researchers to delve deeper into this exciting area. We also provide an open-source repository at https://github.com/CSHaitao/Awesome-LLMs-as-Judges, with the goal of fostering a collaborative community and advancing best practices in this area. enumerate The organization of this paper is as follows. In Section (\\Ssec:PRELIMINARIES), we provide the formal definition of LLMs-as-judges. Then, Section (\\Ssec:Functionality) reviews existing work from the perspective of ``Why use LLM judges''. Following that, Section (\\Ssec:Methodology) covers ``How to use LLM judges'', summarizing the current technical developments in LLMs-as-judges. Section (\\Ssec:Domain) discusses ``Where to use LLM judges'', focusing on their application domains. In Section (\\Ssec:Meta), we review the metrics and benchmarks used for evaluating LLMs-as-judges. Section (\\Ssec:Limitation) discusses the limitations and challenges of LLM judges. We discuss major future work in Sections (\\Ssec:Future) and (\\Ssec:Conclusion) to conclude the paper. my-box= [ rectangle, draw=hidden-draw, rounded corners, text opacity=1, minimum height=1.5em, minimum width=5em, inner sep=2pt, align=center, fill opacity=.5, ] leaf=[my-box, minimum height=1.5em, fill=blue!15, text=black, align=left,font=\\scriptsize, inner xsep=2pt, inner ysep =4pt, ] figure*[t] \\centering \\textwidth{!}{ forest forked edges, for tree={ grow=east, reversed=true, anchor=base west, parent anchor=east, child anchor=west, base=left, font=\\small, rectangle, draw=hidden-draw, rounded corners, align=left, minimum width=4em, edge+={darkgray, line width=1pt}, s sep=3pt, inner xsep=2pt, inner ysep=3pt, ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center}, }, where level=1{text width=6.4em,font=\\scriptsize,}{}, where level=2{text width=6.4em,font=\\scriptsize,}{}, where level=3{text width=6.4em,font=\\scriptsize,}{}, where level=4{text width=6.4em,font=\\scriptsize,}{}, [LLMs-as-Judges, ver [FUNCTIONALITY(\\S sec:Functionality) [Performance \\\\Evaluation (\\S sec:Performance Evaluation) [Responses \\\\Evaluation(\\S sec:Responses Evaluation) [LLM-Eval~lin2023llm{, }Wang et al.~wang2024automated{, }Zhou et al.~zhou2024llm{, }ARES~saad2023ares{, }SELF-RAG~asai2023self{, }Lei et al.~lei2024recexplainer, leaf, text width=41em]] [Model \\\\Evaluation (\\S sec:Model Evaluation) [Auto-Arena~zhao2024auto,luo2024videoautoarena{, }LMExam~bai2024benchmarking{, }KIEval~yu2024kieval, leaf, text width=41em]]] [Model \\\\Enhancement (\\S sec:Model Enhancement) [Reward Modeling \\\\During Training \\\\ (\\S sec: Reward Modeling During Training) [SRLMs~yuan2024self{, }OAIF~guo2024direct{, }RLAIF~lee2023rlaif{, }RELC~cao2024enhancing{, }CREAM~wang2024cream{, }CGPO~xu2024perfect{, }Chen et al.~chen2023adaptation, leaf, text width=41em]] [Acting as Verifier \\\\During Inference \\\\(\\S sec: Acting as Verifier During Inference) [Best-of-N sampling~jinnai2024regularized,sun2024fast{, }ToT~yao2024tree{, }GoT~besta2024graph{, }Lightman et al.~lightman2023let{, }SE-GBS~xie2024self{, }REPS~kawabata2024rationale{, }Musolesi et al.~musolesi2024creative, leaf, text width=41em]] [Feedback for \\\\Refinement (\\S sec: Feedback for Refinement) [SELF-REFINE~madaan2024self{, }SELF-DEBUGGING~chen2023teaching{, }REFINER~paul2023refiner{, }Xu et al.~xu2023towards{, }Self-Correct~huang2023large,tyen2023llms{, }Valmeekam et al.~valmeekam2023can, leaf, text width=41em]]] [Data \\\\ Construction (\\S sec:Data Construction) [Data \\\\Annotation (\\S sec:Data Annotation) [He et al.~he2024if{, }Gilardi et al.~gilardi2023chatgpt{, }TÃ¶rnberg et al.~tornberg2023chatgpt{, }FullAnno~hao2024fullanno{, }Latif et al.~latif2023can{, }AnnoLLM~he2023annollm{, }LLMAAA~zhang2023llmaaa, leaf, text width=41em]] [Data \\\\Synthesize (\\S sec:Data Synthesize) [SELFEE~ye2023selfee{, }SynPO~dong2024self{, }Arif et al.~arif2024fellowship{, }SELF-INSTRUCT~wang2022self{, }Evol-Instruct~xu2023wizardlm,zeng2024automatic{, }STaR~zelikman2024star{, }Mendoncca et al.~mendoncca2024soda{, }\\\\ReSTEM~singh2023beyond{, }Kim et al.~kim2024aligning, leaf, text width=41em]]]] [ METHODOLOGY (\\S sec:Methodology) [ Single-LLM (\\S sec:Single-LLM System) [ Prompt-based (\\S sec:Prompt Engineering) [ In-Context Learning [ GPTScore~fu2023gptscore{, }LLM-EVAL~lin2023llm{, }TALEC~zhang2024talec{, }Jain et al.~jain2023multi{, }ALLURE~hasanbeig2023allure{, }Song et al.~song2024can, leaf, text width=33em ] ] [ Step-by-step [ Chain-of-Thought (CoT)~wei2022chain,kotonya2023little{, }G-EVAL~liu2023g{, }ICE-Score~zhuo2023ice{, }ProtocoLLM~yi2024protocollm{, }Chiang et al.~chiang2023closer{, }\\\\FineSurE~song2024finesure, leaf, text width=33em ] ] [ Definition \\\\Augmentation [ AUTOCALIBRATE~liu2023calibrating{, }PORTIA~li2023split{, }SALC~gupta2024unveiling{, }LLM-as-a-personalized-judge~dong2024can{, }BiasAlert~fan2024biasalert{, }\\\\Chen et al.~chen2024llms, leaf, text width=33em ] ] [ Multi-turn \\\\Optimization [ ACTIVE-CRITIC~xu2024large{, }AUTOCALIBRATE~liu2023calibrating{, }Auto-Arena~zhao2024auto,luo2024videoautoarena{, }LMExam~bai2024benchmarking{, }KIEval~yu2024kieval, leaf, text width=33em ] ]] [ Tuning-based (\\S sec:Tuning) [ Score-based Tuning [ Chen et al.~chen2023adaptation{, }AttrScore~yue2023automatic{, }PHUDGE~deshwal2024phudge{, }ECT~wang2023learning{, }SELF-J~ye2024self{, }SorryBench~xie2024sorry{, }TIGERScore~jiang2023tigerscore{, }\\\\FENCE~xie2024improving{, }ARES~saad2023ares, leaf, text width=33em ] ] [ Preference-based \\\\Learning [ Meta-Rewarding~wu2024meta{, }Con-J~ye2024beyond{, }JudgeLM~zhu2023judgelm{, }INSTRUCTSCORE~xu2023instructscore{, }AUTO-J~li2023generative{, }Shepherd~wang2023shepherd{, }\\\\X-EVAL~liu2023x{, }Themis~hu2024themis{, }CritiqueLLM~ke2024critiquellm{, }FedEval-LLM~he2024fedeval{, }PandaLM~wang2023pandalm{, }Self-Taught~wang2024self{, }\\\\FLAMe~vu2024foundational{, }Self-Rationalization~trivedi2024self{, }CompassJudger-1~cao2024compassjudger{, }Zhou et al.~zhou2024mitigating{, }HALU-J~wang2024halu{, }\\\\PROMETHEUS~kim2023prometheus{, }PROMETHEUS 2~kim2024prometheus{, }PROMETHEUS-VISION~lee2024prometheusvision{, }LLaVA-Critic~xiong2024llava, leaf, text width=33em ] ] ] [ Post-processing(\\S sec:Post-processing) [ Probability \\\\Calibration [ Daynauth et al.~daynauth2024aligning{, }ProbDiff~xia2024language{, }PoE~liusie2024efficient{, }CRISPR~yang2024mitigating, leaf, text width=33em ] ] [ Text Reprocessing [ Sottana et al.~sottana2023evaluation{, }AUTO-J~li2023generative{, }Yan et al.~yan2024consolidating{, }Tessler et al.~tessler2024ai{, }REVISEVAL~zhang2024revisevalRen et al.~ren2023self{, }\\\\Open-LLM-Leaderboard~myrzakhan2024open, leaf, text width=33em ] ] ]] [ Multi-LLM (\\S sec:Multi-LLM System) [ Communication(\\S sec:Communication) [ Cooperation [ WideDeep~zhang2023wider{, }Xu et al.~xu2023towards{, }ABSEval~liang2024abseval, leaf, text width=33em ] ] [ Competition [ Owens et al.~owens2024multi{, }Auto-Arena~zhao2024auto{, }Bandi et al.~bandi2024adversarial{, }Moniri et al.~moniri2024evaluating{, }ChatEval~chan2023chateval{, }PRD~li2023prd, leaf, text width=33em ] ] ] [ Aggregation (\\S sec:Aggregation) [ Badshah et al.~badshah2024reference{, }PoLL~verga2024replacing{, }Language-Model-as-an-Examiner~bai2024benchmarking{, }MULTI-NEWS+~choi2024multi{, }PiCO~ning2024pico{, }PRE~chu2024pre{, }Chen et al. ~chen2024automaticcostefficientpeerreviewframework{, }\\\\Zhang et al.~zhang2024large{, } AIME~patel2024aime{, }HD-EVAL~liu2024hd{, }Gao et al.~gao2024bayesian{, }GED~hu2024language{, }Fusion-Eval~shu2024fusion{, }Jung et al.~jung2024trust{, }CascadedEval~huang2024empirical, leaf, text width=41em ] ]] [ Human-AI \\\\ Collaboration (\\S sec:Hybrid System) [ COEVALli2023collaborative{, }EvalGenshankar2024validates{, }EvaluLLMpan2024human{, }LLM TAschiang2024large{, }Wang et al.~wang2023large, leaf, text width=49em ] ] ] [APPLICATION (\\S sec:Domain) [ General (\\S sec:General) [ Dialogue Generation~li2017dailydialog{, }Summarization~narayan2018don{, }Translation~feng2024improving{, }Fusion-Eval~shu2024fusion, leaf, text width=49em ] ] [ Multimodal (\\S sec:Multimodal) [ LLaVA-Critic~xiong2024llava{, }Chen et al.~chen2024mllm{, }Latif et al.~latif2023can{, }self-reward~zhou2024calibrated, deng2024efficient{,}CODA-LM~chen2024automated, leaf, text width=49em ] ] [ Medical (\\S sec:Medical) [ Xie et al.~xie2024doclens{, }Brake et al.~brake2024comparing{, }Krolik et al.~krolik2024towards{, }Li et al.~li2024automatic{, }Medical Reasoning~jeong2024improving, leaf, text width=49em ] ] [ Legal (\\S sec:Legal) [ Yue et al.~yue2023disc{,}Ryu et al.~ryu2023retrieval{, }Raju et al.~raju2024constructing{, }Ma et al.~ma2024leveraging, leaf, text width=49em ] ] [ Financial (\\S sec:Financial) [ FinMA~xie2023pixiu{, }Babaei et al.~babaei2024gpt{, }Son et al.~son2024krx, leaf, text width=49em ] ] [ Education (\\S sec:Education) [ LLM TA~chiang2024large{, }Wang et al.~wang2024automated{, }Song et al.~song2024automated{, }Zhou et al.~zhou2024llm{, }Xia et al.~xia2024evaluating{, }Debatrix~liang2024debatrix, leaf, text width=49em ] ] [ Information Retrieval \\\\(\\S sec: Information Retrieval) [ Rahmani et al.~rahmani2024llmjudge{, }JudgeRank~niu2024judgerank{, }Zhang et al.~zhang2024large{, }Soboroff et al. ~soboroff2024don{, }ARES~saad2023ares, leaf, text width=49em ] ] [ Others (\\S sec: Others) [ Code~patel2024aime, weyssow2024codeultrafeedback{, }Kumar et al.~kumar2024llms{, }Hijazi et al.~hijazi2024using{, }Tessler et al.~tessler2024ai{, }Sotopia~zhou2023sotopia, leaf, text width=49em ] ]] [META-EVALUATION \\\\ (\\S sec:Meta) [Benchmarks (\\S sec:benchmarks) [ Code Generation \\\\(\\S sec:Code Generation) [ HumanEval~chen2021evaluating{, }SWEBench~jimenez2023swe{, }DevAI~zhuge2024agent{, }CrossCodeEval~ding2024crosscodeeval{, }CodeUltraFeedback~weyssow2024codeultrafeedback, leaf, text width=41em ] ] [ Machine Translation \\\\(\\S sec:Machine Translation) [ Freitag et al.~freitag2021results{, }Literary Translation Comparisons~karpinska2023large{, }MQM~freitag2021experts, leaf, text width=41em ] ] [ Text Summarization \\\\(\\S sec:Text Summarization) [ SummEval~fabbri2021summeval{, }FRANK~pagnoni2021understanding{, }OpinsummEval~shen2023opinsummeval, leaf, text width=41em ] ] [ Dialogue Generation \\\\(\\S sec:Dialogue Generation) [ Topical-Chat~gopalakrishnan2023topical{, }PERSONA-CHAT~zhang2018personalizing{, }Mehri et al.~mehri2020usr{, }DSTC10 Track 5~yoshino2023overview,zhang2021automatic, leaf, text width=41em ] ] [ Automatic Story \\\\Generation (\\S sec:Automatic Story Generation) [ HANNA~chhun2022human{, }MANS~guan2021openmeva{, }OpenMEVA~guan2021openmeva{, }StoryER~chen2023storyer{, }PERSER~wang2023learning, leaf, text width=41em ] ] [ Values Alignment \\\\(\\S sec:Values Alignment) [ PKU-SafeRLHF~ji2024pku{, }HHH~askell2021general{, }CVALUES~xu2023cvalues, leaf, text width=41em ] ] [ Recommendation \\\\(\\S sec:Recommendation) [ MovieLens~harper2015movielens{, }Zhang et al.~zhang2024large{, }Yelp~asghar2016yelp, leaf, text width=41em ] ] [ Search (\\S sec:Search) [ TREC Deep Learning Track~lawrie2024overview{, }MS MARCO v2 collection~bajaj2016ms{, }LeCaRDv2~li2024lecardv2, leaf, text width=41em ] ] [ Comprehensive Data \\\\(\\S sec:Comprehensive Data) [ HelpSteer~wang2023helpsteer{, }HelpSteer2~wang2024helpsteer2{, }UltraFeedback~cui2024ultrafeedback{, }UltraChat~ding2023enhancing{, }ShareGPT~chiang2023vicuna{, }TruthfulQA~lin2021truthfulqa{, }AlpacaEval~dubois2024length{, }\\\\Chatbot Arena~zheng2023judging{, }MT-Bench~zheng2023judging{, } WildBench~lin2024wildbench{, }FLASK~ye2023flask{, }RewardBench~lambert2024rewardbench{, }RM-Bench~liu2024rm{, } JudgeBench~tan2024judgebench{, }\\\\MLLM-as-a-Judge~chen2024mllm{, }MM-Eval~son2024mm, leaf, text width=41em ] ]] [Metric (\\S sec:metric) [Accuracy{, }Pearson~cohen2009pearson{, }Spearman~sedgwick2014spearman{, }Kendall's Tau~sen1968estimates{, }Cohen's Kappa~warrens2015five{, }ICC~bartko1966intraclass, leaf, text width=49em ]]]] forest } Taxonomy of LLMs-as-judges in functionality, methodology, application, meta-evaluation. figure* my-box= [ rectangle, draw=hidden-draw, rounded corners, text opacity=1, minimum height=1.5em, minimum width=5em, inner sep=2pt, align=center, fill opacity=.5, ] leaf=[my-box, minimum height=1.5em, fill=blue!15, text=black, align=left,font=\\scriptsize, inner xsep=2pt, inner ysep =4pt, ] figure*[t] \\centering \\textwidth{!}{ forest forked edges, for tree={ grow=east, reversed=true, anchor=base west, parent anchor=east, child anchor=west, base=left, font=\\small, rectangle, draw=hidden-draw, rounded corners, align=left, minimum width=4em, edge+={darkgray, line width=1pt}, s sep=3pt, inner xsep=2pt, inner ysep=3pt, ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center}, }, where level=1{text width=6.4em,font=\\scriptsize,}{}, where level=2{text width=6.4em,font=\\scriptsize,}{}, where level=3{text width=6.4em,font=\\scriptsize,}{}, where level=4{text width=6.4em,font=\\scriptsize,}{}, [LLMs-as-judges, ver [LIMITATION (\\S sec:Limitation) [Biases (\\Ssec:biases) [Presentation-Related \\\\ (\\Ssec:presentationbiases) [Position bias~blunch1984position,raghubir2006center, ko2020look, wang2018position, llmsjuding2024openreview, zheng2023judging,chen2024humans,wang2023large,li2023generative,zheng2023large, raina2024llm,hou2024large, li2023split, li2023prd,khan2024debating, zhou2023batch, li2024calibraeval, shi2024judging, stureborg2024large, zhao2024measuring{, }Verbosity bias~nasrabadi2024juree, ye2024justice, ye2024beyond, leaf, text width=41em] ] [Social-Related (\\Ssec:socialbiases) [Authority bias~chen2024humans,ye2024justice, zhao2023mind{, }Bandwagon-effect bias~koo2023benchmarking,ye2024justice{, }Compassion-fade bias~koo2023benchmarking,ye2024justice{, }Diversity bias~chen2024humans,ye2024justice, leaf, text width=41em] ] [Content-Related \\\\(\\Ssec:contentbiases) [Sentiment bias~ye2024justice{, }Token Bias~jiang2024peek,li2024calibraeval,pezeshkpour2023large,raina2024llm{, }Contextual Bias~poulain2024bias, zhou2024large,zhou2023batch,fei2023mitigating,zhao2021calibrate,han2022prototypical, leaf, text width=41em] ] [Cognitive-Related \\\\(\\Ssec:cognitivebiases) [Overconfidence bias~khan2024debating,jung2024trust{, }Self-enhancement bias~liu2023g,zheng2023judging,li2023prd,liu2023g, brown1986evaluations,ye2024justice, badshah2024reference{, }Refinement-aware bias~ye2024justice, xu2024pride\\\\Distraction bias~ye2024justice,koo2023benchmarking,shi2023large{, }Fallacy-oversight bias~chen2024humans,ye2024justice, leaf, text width=41em] ] ] [Adversarial Attacks \\\\(\\Ssec:attacks) [Adversarial Attacks \\\\on LLMs (\\S sec:Adversarial Attacks on LLMs) [Text-Level Manipulations~ebrahimi2017hotflip, jiang2023prompt, branch2022evaluating, perez2022ignore{, }Structural and Semantic Distortions~xu2023llm{, }Optimization-Based Attacks~sun2020natural,sun2020adv, lee2022query, leaf, text width=41em] ] [Adversarial Attacks \\\\on LLMs-as-Judges \\\\(\\S sec:Adversarial Attacks on LLMs-as-judges) [Zheng et al.~zheng2024cheating{, }Doddapaneni et al.~doddapaneni2024finding{, }MT-Bench~zheng2023judging{, }Raina et al.~raina2024llm{, }Shi et al.~shi2024optimization, leaf, text width=41em] ] ] [Inherent Weaknesses \\\\(\\Ssec:weaknesses) [Knowledge Recency \\\\(\\S sec:Knowledge Recency) [Zhao et al. ~zhao2023survey{, }Luo et al.~luo2023empirical{, }Gao et al.~gao2023retrieval{, }Lewis et al.~lewis2020retrieval{, }Wu et al.~wu2024continual{, }Dierickx et al. ~dierickx2024striking, leaf, text width=41em]] [Hallucination (\\S sec:Hallucination) [Dierickx et al.~dierickx2024striking{, }Ji et al.~ji2023survey{, }Tonmoy et al.~tonmoy2024comprehensive, leaf, text width=41em]] [Domain-Specific \\\\Knowledge Gaps \\\\(\\S sec:Domain-Specific Knowledge Gaps) [Feng et al.~feng2023knowledge{, }Pan et al.~pan2024unifying{, }Gao et al.~gao2023retrieval{, }Szymanski et al.~szymanski2024limitations{, }Dorner et al.~dorner2024limitsscalableevaluationfrontier, leaf, text width=41em]] ]] [ FUTURE WORK \\\\(\\Ssec:Future) [ More Efficient (\\Ssec:more-Efficient) [ Automated Construction of Evaluation Criteria and Tasks~bai2024benchmarking,zhao2024auto,yu2024kieval,zhang2024talec,wang2024revisiting{, }Scalable Evaluation Systems~xu2024perfect{, }Accelerating Evaluation Processes~lee2024aligning,liu2024aligning, chen2024internet, leaf, text width=49em ] ] [ More Effective (\\Ssec:more-Effective) [ Integration of Reasoning and Judge Capabilities~zhuo2023ice,yi2024protocollm,stephan2024calculation{, }Establishing a Collective Judgment Mechanism~chan2023chateval,chu2024pre{, }Enhancing Domain Knowledge~raju2024constructing{, }\\\\Cross-Domain and Cross-Language Transferability~son2024mm,hada2023large,watts2024pariksha{, }Multimodal Integration Evaluation~chen2024mllm, leaf, text width=49em ] ] [ More Reliable (\\Ssec:more-Reliable) [ Enhancing Interpretability and Transparency~liu2024hd{, } Mitigating Bias and Ensuring Fairness~li2024calibraeval{, } Enhancing Robustness~shi2024optimization,elangovan2024beyond, leaf, text width=49em ]]]] forest } Taxonomy of LLMs-as-judges in limitation and future work. figure*",
      "origin_cites_number": 283
    },
    {
      "section_title": "PRELIMINARIES",
      "level": "1",
      "content": "In this section, we will provide a formal definition of LLMs-as-judges, aiming to encompass all current evaluation paradigms and methods, thereby offering readers a clear and thorough understanding. Figure figure:overreview presents an overview of the LLMs-as-judges system. The LLMs-as-judges paradigm is a flexible and powerful evaluation framework where LLMs are employed as evaluative tools, responsible for assessing the quality, relevance, and effectiveness of generated outputs based on defined evaluation criteria. This framework leverages the extensive knowledge and deep contextual understanding of LLMs, enabling it to flexibly adapt to various tasks in NLP and machine learning. We formalize the input-output structure of the LLMs-as-Judges paradigm, unifying various evaluation scenarios into a unified perspective. Specifically, the evaluation process can be defined as follows: equation (Y, E, F)=E(T, C, X, R) equation where $E$ is the evaluation function, taking the evaluation type $T$, evaluation criteria $C$, evaluation item $X$ and optional references $R$ as input. Based on these inputs, the LLM can produces three outputs: evaluation result $Y$, explanation $E$ and feedback $F$. Different input-output configurations correspond to distinct methods and objectives. This unified formulation brings together diverse evaluation paradigms, offering a structured framework for categorizing and understanding various approaches within LLMs-as-judges.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Evaluation Function $E$",
      "level": "2",
      "content": "The evaluation function $E$ in the context of LLMs-as-judges can be categorized into three primary configurations: Single-LLM systems, Multi-LLM systems, and Hybrid systems that combine LLMs with human evaluators. Each of these configurations serves distinct purposes, offers different advantages, and faces unique challenges. itemize[leftmargin=*] \\item Single-LLM Evaluation System~\\cite{lin2023llm,zhang2024talec,liu2023g: } A single LLM evaluation system relies on a single model to perform the evaluation tasks. It is simple to deploy and scale, making it efficient for tasks that don't require specialized evaluation. However, its flexibility is limited, as it may struggle with tasks that demand specialized knowledge or reasoning over complex inputs. Additionally, if not properly trained, a single model may introduce biases, leading to inaccurate evaluations. \\item Multi-LLM Evaluation Systems~\\cite{chu2024pre,chan2023chateval,li2023prd: } A Multi-LLM evaluation system combines multiple models that work together to perform evaluation tasks. These models may interact through various mechanisms, such as collaboration, or competition, to refine their outputs and achieve more accurate results. By leveraging the strengths of different models, a multi-model system can cover a broader range of evaluation criteria and provide a more comprehensive assessment. However, this comes at a higher computational cost and requires more resources, making deployment and maintenance more challenging, particularly for large-scale tasks. Moreover, while cooperation between models often enhances evaluation results, the methods through which these models achieve consensus or resolve differences remain key areas of ongoing exploration. \\item Human-AI Collaboration System~\\cite{li2023collaborative,wang2023large,shankar2024validates: } In this system, LLMs work alongside human evaluators, combining the efficiency of automated evaluation with the nuanced judgment of human expertise. This configuration allows human evaluators to mitigate potential biases in the LLM's output and provide subjective insights into complex evaluation tasks. While this system offers greater reliability and depth, it comes with challenges in coordinating between the models and humans, ensuring consistent evaluation standards, and integrating feedback. Additionally, the inclusion of human evaluators increases both the cost and time required for the evaluation process, making it less scalable than purely model-based systems. itemize",
      "origin_cites_number": 3
    },
    {
      "section_title": "Evaluation Input",
      "level": "2",
      "content": "In the LLMs-as-judges paradigm, in addition to the evaluation item $X$, LLM judges typically receive three other types of inputs: Evaluation Type $T$ , Evaluation Criteria $C$, and Evaluation References $R$. The following provides a detailed explanation:",
      "origin_cites_number": 0
    },
    {
      "section_title": "Evaluation Type $\\mathcal{T",
      "level": "3",
      "content": "$} The Evaluation Type $T$ defines the specific evaluation mode, determining how the evaluation will be conducted. It typically includes three approaches: pointwise, pairwise, and listwise evaluation. itemize[leftmargin=*] \\item Pointwise Evaluation~\\cite{wang2023learning,kim2023prometheus,ye2024self:} This method evaluates each candidate item individually based on the specified criteria. For example, in a text summarization task, the LLM might evaluate each generated summary separately, assigning a score based on factors like informativeness, coherence, and conciseness. Although pointwise evaluation is simple and easy to apply, it may fail to capture the relative quality differences between candidates and can be influenced by biases arising from evaluating items in isolation. \\item Pairwise Evaluation~\\cite{saad2023ares,cao2024compassjudger,he2024fedeval,hu2024rethinking:} This method involves directly comparing two candidate items to determine which one performs better according to the specified criteria. It is commonly used in preference-based tasks. For example, given two summaries of a news article, the LLM may be asked to decide which summary is more coherent or informative. Pairwise evaluation closely mirrors human decision-making processes by focusing on relative preferences rather than assigning absolute scores. This approach is especially effective when the differences between outputs are subtle and difficult to quantify. \\item Listwise Evaluation~\\cite{10.1145/3626772.3657813,yan-etal-2024-consolidating,hou2024large,niu2024judgerank:} This method is designed to collectively evaluate the entire list of candidate items, evaluating and ranking them based on the specific criteria. It is often applied in ranking tasks, such as document retrieval in search engines, where the objective is to determine the relevance of the documents in relation to a user query. Listwise evaluation takes into account the interactions between multiple candidates, making it well-suited for applications that require holistic analysis. itemize In general, these three evaluation modes are not entirely independent. pointwise scores can be aggregated to create pairwise comparisons or used to construct a ranked list. Similarly, pairwise preferences can be organized into a complete ranking list for listwise analysis. However, these transformations are not always reliable within the LLMs-as-judges framework~liu2024aligning. For example, in pointwise evaluation, output $A$ may receive a score of 5, while output $B$ receives a score of 4, yet, a direct pairwise comparison might not consistently yield $A > B$ due to potential bias. Additionally, LLM judges do not always satisfy transitivity in their judgments. For instance, given pairwise preferences where $z_i > z_j$ and $z_j > z_k$ , the LLM may not necessarily yield $z_i > z_k$. These inconsistencies contribute to concerns about the reliability and trustworthiness of the LLM-as-Judge framework, which we will discuss in detail in Section (\\Ssec:Limitation).",
      "origin_cites_number": 4
    },
    {
      "section_title": "Evaluation Criteria $\\mathcal{C",
      "level": "3",
      "content": "$.} The evaluation criteria $C$ define the specific standards that determine which aspects of the output should be assessed. These criteria are designed to cover a broad range of quality attributes and can be tailored based on the nature of the task. Typically, the criteria encompass the following aspects: itemize[leftmargin=*] \\item Linguistic Quality~\\cite{fabbri2021summeval,zhang2018personalizing,chhun2022human:} This category evaluates the language-related features of the output, such as fluency, grammatical accuracy, coherence, and Conciseness. Linguistic quality is crucial in tasks like text generation, machine translation, and summarization, where clarity and readability are essential. \\item Content Accuracy~\\cite{chen2021evaluating,jimenez2023swe,tan2024judgebench:} This dimension focuses on the correctness and relevance of the content. It includes evaluating aspects such as factual accuracy, ensuring that the output does not contain misleading or incorrect information. Content accuracy is particularly crucial in tasks such as code generation and fact-checking. \\item Task-Specific Metrics~\\cite{ji2024pku,lin2024wildbench,tan2024judgebench:} In addition to general quality metrics, many tasks require evaluation based on standards specific to their respective domains. These standards may include metrics such as informativeness (assessing whether the output provides comprehensive and valuable information) or completeness (ensuring all key aspects of the input are covered). Other criteria may include diversity, well-structured content, and logical clarity. itemize In addition to providing clear evaluation criteria, offering several examples can also be beneficial for the assessment. By incorporating well-structured examples, LLMs can better align its output with user expectations, especially when handling complex tasks or ambiguous queries.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Evaluation References $\\mathcal{R",
      "level": "3",
      "content": "$.} Evaluation References $R$ are optional. Depending on the availability of evaluation reference, the evaluation process can be broadly divided into reference-based and reference-free scenarios. itemize[leftmargin=*] \\item Reference-Based Evaluation~\\cite{freitag2021results,karpinska2023large: } The reference-based evaluation leverages reference data to determine whether the performance meets the expected standards. It is commonly applied in tasks where the quality of the output can be objectively judged by its similarity to established reference. In Natural Language Generation (NLG) tasks, this method is widely used to evaluate the resemblance between generated content and reference content. For example, in machine translation or text summarization, an LLM can compare the generated translations or summaries against high-quality references. The key strength of this approach is its well-defined benchmarking process; however, its effectiveness may be constrained by the quality and variety of the reference data. \\item Reference-Free Evaluation~\\cite{shen2023opinsummeval,zheng2023judging,he2023socreval: } The reference-free evaluation does not rely on a specific reference $R$, instead, it evaluates $X$ based on intrinsic quality standards or its alignment with the source context. For example, when assessing language fluency or content coherence, an LLM can autonomously generate evaluation results using internal grammatical and semantic rules. This method is widely used in fields like sentiment analysis and dialogue generation. The main advantage of this approach is its independence from specific references, providing greater flexibility for open-ended tasks. However, its drawback lies in the difficulty of obtaining satisfactory evaluations in domains where the LLM lacks relevant knowledge. itemize",
      "origin_cites_number": 2
    },
    {
      "section_title": "Evaluation Output",
      "level": "2",
      "content": "In the LLMs-as-judges paradigm, the LLM typically generates three types of outputs: the evaluation result $Y$, the explanation $E$, and the feedback $F$. Below are detailed descriptions. itemize[leftmargin=*] \\item Evaluation Result $\\mathcal{Y$~zhao2024auto,saad2023ares: } The evaluation result $Y$ is the primary output, which can take the form of a numerical score, a ranking, a categorical label, or a qualitative assessment. It reflects the quality, relevance, or performance of the candidate items according to the specified evaluation criteria. For example, in a machine translation task, $Y$ could be a score indicating translation quality, while in a dialogue generation task, it might be a rating of coherence and appropriateness on a scale from 1 to 5. The evaluation result $Y$ provides a clear measure of performance, enabling researchers to effectively compare different models or outputs. \\item Explanation $\\mathcal{E$~ye2024beyond,xie2024improving: } The explanation $E$ provides detailed reasoning and justifications for the evaluation result. It offers insights into why certain result received higher or lower scores, highlighting specific features of the candidate item that influenced the evaluation. For example, in a summarization task, the LLM judges might explain that the score was lowered due to missing critical information or the presence of redundant content. The explanation component enhances transparency, allowing users to understand the decision-making process of the LLM and gain deeper insights into the strengths and weaknesses of the evaluated content. \\item Feedback $\\mathcal{F$~madaan2024self,chen2023teaching: } The feedback $F$ consists of actionable suggestions or recommendations aimed at improving the evaluated output. Unlike the evaluation result, which merely indicates performance, the feedback component is designed to guide the refinement of the content. For instance, in a creative writing task, feedback might include recommendations for enhancing the narrative flow or improving clarity. This component is especially valuable for the iterative development of the evaluated item, as it provides concrete pointers that help both the LLM and content creators enhance the quality of the generated outputs. itemize Depending on the intended purpose and specific requirements of the evaluation, the LLM judges can generate various combinations of the three outputs ( $Y$, $E$, $F$ ) for a given task. In most cases, providing explanation $E$ not only helps users better understand and trust the evaluation results but also leads to more human-aligned and accurate evaluation result $Y$. Moreover, generating feedback $F$ generally demands a higher level of model capability, as it requires not only assessing the quality of the input but also providing concrete, actionable recommendations for improvement.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Functionality",
      "level": "1",
      "content": "As an emerging evaluation paradigm, LLMs-as-judges play a significant role across various scenarios. Based on their functionality, we categorize the application of LLM evaluators into three main directions: Performance Evaluation (\\Ssec:Performance Evaluation), Model Enhancement (\\Ssec:Model Enhancement), and Data Construction (\\Ssec:Data Construction). In this section, we will delve into these functionalities, explore their potential, and discuss specific implementation methods.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Performance Evaluation",
      "level": "2",
      "content": "Performance evaluation represents the most fundamental application objective of an LLM judges, serving as the cornerstone for understanding and optimizing their other function. It can be broadly divided into two components: Responses Evaluation (\\Ssec:Responses Evaluation) and Model Evaluation (\\Ssec:Model Evaluation). Response Evaluation focuses on aspects such as the quality, relevance, and coherence, and fluency of the responses for a given task. In contrast, model evaluation takes a holistic approach, assessing the overall capabilities of LLMs. Although these two aspects are interconnected, they focus on different levels of analysis, providing multidimensional insights into performance.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Responses Evaluation",
      "level": "3",
      "content": "The purpose of evaluating responses is to identify better answers within the context of a specific question or task, which can enhance overall decision-making. These responses can originate from either AI models or humans. Evaluation criteria typically consider general attributes such as accuracy, relevance, coherence, and fluency. However, in practical applications, the evaluation of responses often requires customized metrics tailored to specific tasks. For instance, in the education domain, the focus may be more on the inspirational and educational value of the answers. LLM judges have also been widely applied in the assessment of text response~lin2023llm,wang2024automated,zhou2024llm. Lin et al.~lin2023llm propose LLM-Eval, a unified framework employing a single-prompt strategy to evaluate the performance of open-domain dialogue systems across multiple dimensions, including content, grammar, relevance, and appropriateness. Wang et al.~wang2024automated proposed an article scoring and feedback system tailored to different genres, such as essays, narratives, and question-answering articles. Using BERT and ChatGPT models, they enabled automated scoring and detailed feedback, showcasing the potential of LLMs in article evaluation. Moreover, Zhou et al.~zhou2024llm conduct a detailed evaluation of whether LLMs can serve as reliable tools for automated paper review. Their findings indicate that current LLMs are still not sufficiently reliable for such tasks, particularly in scenarios requiring logical reasoning or a deep knowledge base. Furthermore, the evaluation of a single response is not limited to assessing the quality of the final answer but can also extend to analyzing the response process~saad2023ares,asai2023self,lei2024recexplainer. For instance, this can include evaluating whether retrieval is necessary at a given step, the relevance of the retrieved documents, and the interpretability of the response. For example, ARES~saad2023ares uses LLM judges to evaluate RAG systems across three dimensions: Contextual Relevance, Answer Faithfulness, and Answer Relevance. Similarly, Asai et al. ~asai2023self proposed SELF-RAG, which employs reflective token to determine whether retrieval is required and to self-assess the quality of generated outputs. Lei et al.~lei2024recexplainer introduced LLMs to evaluate the quality of generated explanations, demonstrating the effectiveness of LLMs in understanding and generating explanations for recommendation tasks.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Model Evaluation",
      "level": "3",
      "content": "Model evaluation typically begins with assessing individual responses and then extends to analyzing overall capabilities. This wider perspective aims to analyze the model's performance across various tasks or domains, such as coding ability, instruction-following proficiency, reasoning, and other specialized skills relevant to its intended applications. A common and straightforward approach is to represent model performance using average performance on static benchmarks~zheng2023judging,lin2024wildbench,tan2024judgebench. LLM judges assess the model's performance using a set of carefully designed metrics, which results in a performance ranking. This method is widely adopted due to its simplicity and comparability. For example, task sets can be designed to evaluate the model's knowledge coverage, reasoning depth, and language generation quality~son2024mm,liu2024rm,lambert2024rewardbench, or real-world scenarios can be simulated to assess the model's ability to handle complex situations~liu2023agentbench,trivedi2024appworld. As the demand for evaluation increases, the evaluation process has gradually shifted from traditional static testing to more dynamic, interactive assessments~bai2024benchmarking,zhao2024auto,yu2024kieval. LLMs-as-judges has pioneered this approach, similar to Chatbot Arena~zheng2023judging, a crowdsourced platform that collects anonymous votes on LLM performance and ranks them using Elo scores. Auto-Arena~zhao2024auto,luo2024videoautoarena and LMExam~bai2024benchmarking assess model capabilities by using LLMs as both question setters and evaluators. These frameworks innovatively combine diverse question generation, multi-turn question-answering evaluation, and a decentralized model-to-model evaluation mechanism, providing more detailed and granular performance assessments. Additionally, KIEval~yu2024kieval introduces an LLM-driven ``interactor'' role, which evaluates the knowledge mastery and generation abilities of LLMs through dynamic multi-turn conversations. These dynamic evaluation methods effectively address data leakage and evaluation bias issues common in traditional benchmark tests.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Model Enhancement",
      "level": "2",
      "content": "In addition to Performance Evaluation, LLMs-as-judges is also widely used for Model Enhancement. From training to inference, LLMs-as-judges plays a key role in improving model performance. Its application in model enhancement offers a novel optimization pathway for artificial intelligence, fostering the refinement and personalization of intelligent systems across a broader spectrum of real-world applications.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Reward Modeling During Training",
      "level": "3",
      "content": "A primary application of LLMs-as-judges is in reward modeling during training, particularly in reinforcement learning with feedback~yuan2024self,guo2024direct,wang2024cream,xu2024perfect,cao2024enhancing. LLM judges assign scores to model outputs by evaluating them against human-defined criteria, guiding optimization toward desired behaviors. This ensures alignment with human values, improving the quality and relevance of the generated outputs and improving the effectiveness of LLMs in real-world tasks. A series of works, such as SRLMs~yuan2024self, OAIF~guo2024direct, and RLAIF~lee2023rlaif, have enabled LLMs to become their own reward models. This overcomes the traditional RLHF dependency on fixed reward models, allowing the model to iteratively reward and self-optimize, fostering self-evolution through continuous self-assessment. RELC~cao2024enhancing tackles the challenge of sparse rewards in traditional RL by introducing a Critic Language Model (Critic LM) to evaluate intermediate generation steps. This dense feedback at each step helps mitigate reward sparsity, offering more detailed guidance to the model during training. However, using the same LLM for both policy generation and reward modeling can pose challenges in ensuring the accuracy of the rewards. This dual role setup may lead to accumulated biases and preference data noise, which can undermine the training effectiveness. To address this issue, CREAM~wang2024cream introduces cross-iteration consistency constraints to regulate the training process and prevent the model from learning unreliable preference data. This significantly enhances reward consistency and alignment performance. In addition, CGPO~xu2024perfect groups tasks by category (such as dialogue, mathematical reasoning, safety, etc.) and uses ``Mixed Judges'' to assign a specific reward model to each task group. This ensures that the reward signals are closely aligned with the task objectives, thereby preventing conflicts between different goals.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Acting as Verifier During Inference",
      "level": "3",
      "content": "During inference, LLM judges serve as verifier, responsible for selecting the optimal response from multiple candidates~yao2024tree,besta2024graph,lightman2023let,musolesi2024creative,besta2024graph. By comparing the outputs based on various metrics, such as factual accuracy and reasoning consistency, they are able to identify the best fit for the given task or context, thereby optimizing the inference process or improving the quality of the generated results. One of the simplest applications is Best-of-N sampling~jinnai2024regularized,sun2024fast, where the model is sampled N times, and the best result is selected to improve model performance. Similarly, Wang et al.~wang2022self introduced a promising sampling method called self-consistency, where n samples are drawn from the judge model, and the average score is output. These sampling methods enhance inference stability by selecting the best result from multiple evaluations. Further optimization strategies include the Tree of Thoughts (ToT)~yao2024tree method, which models the problem-solving process as a tree structure. This allows the model to explore multiple solution paths and optimize path selection through self-assessment mechanisms. The Graph of Thoughts (GoT)~besta2024graph method extends this concept by introducing directed graphs, where the non-linear interactions between nodes improve the efficiency and precision of multi-step reasoning. In both methods, LLM judges play a crucial role in guiding the model to select the most promising paths, thereby enhancing the quality and accuracy of reasoning. Similarly, Lightman et al.~lightman2023let discuss how step-by-step validation can enhance the performance of LLMs in multi-step reasoning tasks, particularly in the domain of mathematics. SE-GBS~xie2024self integrates self-assessment into the multi-step reasoning decoding process, generating scores that reflect logical correctness and further ensuring the accuracy and consistency of the reasoning chain. The REPS~kawabata2024rationale improves the accuracy and reliability of reasoning validation models by comparing reasoning paths pairwise, verifying their logical consistency and factual basis. Also, Musolesi et al. ~musolesi2024creative proposed Creative Beam Search, with the LLM acting as a judge to simulate the human creative selection process, thereby enhancing the diversity and creativity of the generated results.",
      "origin_cites_number": 9
    },
    {
      "section_title": "Feedback for Refinement",
      "level": "3",
      "content": "After receiving the initial response, LLM judges provide actionable feedback to iteratively improve output quality. By analyzing the response based on specific task criteria, such as accuracy, coherence, or creativity, the LLM can identify weaknesses in the output and offer suggestions for improvement. This iterative refinement process plays a crucial role in applications that require adaptability~madaan2024self,paul2023refiner,chen2023teaching,xu2023towards,huang2023large. SELF-REFINE~madaan2024self enables LLMs to iteratively improve output quality through feedback generated by the model itself, without requiring additional training or supervision data. On the other hand, SELF-DEBUGGING~chen2023teaching demonstrates a practical application of self-correction in code generation by identifying and rectifying errors through self-explanation and feedback. This approach has significantly enhanced the performance of LLMs across various code generation tasks. In addition to refining response quality, LLMs judges are also widely used to enhance reasoning abilities. For example, REFINER~paul2023refiner optimizes the reasoning performance of LLMs through interactions between a generator model and a critic model. In this framework, the generator model is responsible for producing intermediate reasoning steps, while the critic model analyzes these steps and provides detailed feedback, such as identifying calculation errors or logical inconsistencies. Xu et al.~xu2023towards propose a multi-agent collaboration strategy to enhance the reasoning abilities of LLMs by simulating the academic peer review process. The framework is divided into three stages: generation, review, and revision. Agents provide feedback and attach confidence scores to refine the initial answers, with the final result determined through majority voting. While the feedback and correction mechanisms of LLMs judges are continually evolving, the limitations of self-feedback in improving quality should not be overlooked. Research on Self-Correct~huang2023large,tyen2023llms shows that, the intrinsic self-correction capabilities of LLMs often fall short of effectively improving reasoning quality. Valmeekam et al.~valmeekam2023can also raise concerns about the effectiveness of LLMs as self-validation tools in the absence of reliable external validators. Future research can focus on improving the accuracy of feedback provided by these LLM judges and incorporating external validation mechanisms to optimize their performance in complex reasoning tasks.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Data Construction",
      "level": "2",
      "content": "Data collection is a crucial stage in the development of machine learning systems, especially those driven by the rapid advancements in deep learning. The quality of the data directly determines the performance of the trained models. The LLMs-as-judges has significantly transformed the landscape of data collection, substantially reducing reliance on human effort. In this section, we will explore the pivotal role of LLMs-as-judges in data collection from two key perspectives: Data Annotation (\\Ssec:Data Annotation) and Data Synthesize (\\Ssec:Data Synthesize).",
      "origin_cites_number": 0
    },
    {
      "section_title": "Data Annotation",
      "level": "3",
      "content": "Data Annotation involves leveraging LLM judges to label large, unlabeled datasets efficiently~he2024if,gilardi2023chatgpt,tornberg2023chatgpt,hao2024fullanno. By utilizing the advanced natural language understanding and reasoning capabilities of LLMs, the annotation process can be automated to a significant extent, enabling the generation of high-quality labels with reduced human intervention. LLMs have demonstrated remarkable potential in text annotation tasks, consistently outperforming traditional methods and human annotators in various settings. He et al.~he2024if evaluated the performance of GPT-4 in crowdsourced data annotation workflows, particularly in text annotation tasks. Their comparative study revealed that, even with best practices, the highest accuracy achievable by MTurk workers was 81.5\\%, whereas GPT-4 achieved an accuracy of 83.6\\%. Similarly, Gilardi et al.~gilardi2023chatgpt analyzed 6,183 tweets and news articles, demonstrating that ChatGPT outperformed crowdsourced workers in tasks such as stance detection, topic detection, and framing. TÃ¶rnberg et al.~tornberg2023chatgpt further investigated the classification of Twitter users' political leanings based on their tweet content. Their findings revealed that ChatGPT-4 not only surpassed human classifiers in accuracy and reliability but also exhibited bias levels that were comparable to or lower than those of human classifiers. As technology advances, more and more research is exploring their application in multimodal data annotation. For example, the FullAnno~hao2024fullanno uses the GPT-4V model to generate image annotations, significantly improving the quality of image descriptions through a multi-stage annotation process. Furthermore, Latif et al.~latif2023can explored the application of LLMs in speech emotion annotation, demonstrating that, with data augmentation, LLM-annotated samples can significantly enhance the performance of speech emotion recognition models. By integrating text, audio features, and gender information, the effectiveness of LLM-based annotations was further improved, highlighting their potential in advancing multimodal annotation tasks. As LLMs perform excellently in annotation tasks, researchers are actively exploring methods to further improve annotation quality and address potential challenges. For example, AnnoLLM~he2023annollm introducedthe ``explain-then-annotate'' method, which enhances both the accuracy and transparency of annotations by prompting the LLM to justify its label assignments. Additionally, the LLMAAA~zhang2023llmaaa framework incorporates an active learning strategy to efficiently select high-information samples for annotation, thereby mitigating the effects of noisy labels and reducing the reliance on costly human annotation. These approach not only enhance the performance of task-specific models but also offer new perspectives on the efficient application of LLMs in annotation workflows.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Data Synthesize",
      "level": "3",
      "content": "The goal of Data Synthesis is to create entirely new data, either from scratch or based on seed data, while ensuring it is similar in distribution to real data. Data Synthesis enables the generation of diverse data samples, enhancing a model's generalization ability to unseen examples while reducing reliance on sensitive real-world data~ye2023selfee,dong2024self,arif2024fellowship,wang2022self,kim2024aligning,mendoncca2024soda. In recent years, advancements in LLMs have led to significant improvements in both the quality and efficiency of data synthesis methods. In this domain, methods like SELFEE~ye2023selfee and SynPO~dong2024self have effectively enhanced the alignment capabilities of LLMs by leveraging small amounts of labeled data and iteratively generating preference-aligned data. Arif et al.~arif2024fellowship also introduce a multi-agent workflow for generating optimized preference datasets. SELF-INSTRUCT~wang2022self and Evol-Instruct~xu2023wizardlm,zeng2024automatic represent innovative approaches to improving model alignment and performance through self-generated instruction data. SELF-INSTRUCT~wang2022self requires minimal human annotation, instead relying on self-generated instruction data to align pre-trained models. Evol-Instruct~xu2023wizardlm,zeng2024automatic further enhances LLM performance by automatically generating instruction data, significantly boosting model capabilities. STaR~zelikman2024star and ReSTEM~singh2023beyond are research efforts aimed at enhancing reasoning capabilities through synthetic data. STaR~zelikman2024star employs a self-guided iterative process to improve model performance on complex reasoning tasks, offering an effective solution for tackling increasingly sophisticated reasoning challenges in the future. ReSTEM~singh2023beyond, on the other hand, utilizes a self-training approach based on the expectation-maximization framework to enhance the problem-solving capabilities of large language models, particularly in areas such as solving mathematical problems and generating code. mygray{gray}{.9}",
      "origin_cites_number": 12
    },
    {
      "section_title": "Methodology",
      "level": "1",
      "content": "\\par The use of LLM judges requires careful methodological considerations to ensure the accuracy and consistency of judgments. Researchers have developed various approaches according to the complexity and specific requirements of different judgment tasks, each offering unique advantages. In this section, we categorize these methodologies into three broad approaches: Single-LLM System (\\Ssec:Single-LLM System): evaluation by a single-LLM, Multi-LLM System (\\Ssec:Multi-LLM System): evaluation by cooperation among multi-LLMs, and Human-AI Collaboration (\\Ssec:Hybrid System): evaluation by cooperation of LLMs and Human. Figure ~figure:method presents an overview of methodology.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Single-LLM System",
      "level": "2",
      "content": "\\par Single-LLM System relies on a single model to perform judgment tasks, with its effectiveness largely determined by the LLM's capabilities and the strategies used to process input data. This approach can generally be divided into three fundamental components: Prompt Engineering (\\Ssec:Prompt Engineering), Tuning (\\Ssec:Tuning), and Post-processing (\\Ssec:Post-processing) of model outputs.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Prompt-based",
      "level": "3",
      "content": "\\par Prompt engineering~sahoo2024systematic involves crafting clear and structured input prompts tailored to elicit accurate and contextually appropriate responses from LLM judges. This approach is crucial for ensuring that LLMs grasp the complexities of specific tasks and provide relevant, consistent, and goal-aligned judgments. In many cases, well-designed prompts significantly reduce the need for extensive model training. In-Context Learning. In-Context Learning (ICL) is a distinctive capability of LLMs that allows them to dynamically adapt to evaluation tasks using carefully curated examples or explanations within the prompt~dong2022survey. Several recent methods have demonstrated the power of ICL in LLM-as-judges, showcasing how it enhances the flexibility and performance of LLMs in diverse settings. For example, GPTScore~fu2023gptscore leverages the few-shot learning capability of generative pre-trained models to evaluate generated text. By using relevant examples to customize prompts, it provides a flexible, training-free approach to assess multiple aspects of text quality. Similarly, LLM-EVAL~lin2023llm incorporates carefully crafted examples into prompts, proposing a unified, multi-dimensional automatic evaluation method for open-domain dialogue. Another notable example is TALEC~zhang2024talec, a model-based evaluation method that leverages in-context learning to enable users to set custom evaluation criteria for LLMs in specific domains. Through careful prompt engineering, users can iteratively adjust the examples to refine the evaluation process as needed. In addition, Jain et al.~jain2023multi proposed the In-Context Learning-based Evaluator (ICE) for multi-dimensional text evaluation. ICE leverages LLMs and a small number of in-context examples to evaluate generated text summaries, achieving competitive results. While ICL can enable effective evaluation, it is not without challenges. One major issue is that the model's responses may be influenced by the selection of prompt examples, potentially leading to bias~zhao2021calibrate,zhou2023batch,han2022prototypical,fei2023mitigating. To address this issue, Hasanbeig et al. proposed ALLURE~hasanbeig2023allure, a comprehensive protocol designed to mitigate bias in ICL for LLMs during text evaluation. ALLURE~hasanbeig2023allure improves evaluator accuracy by iteratively incorporating discrepancies between its assessments and annotated data into the learning context. Moreover, after uncovering the existence of symbol bias within LLM evaluators when using ICL, Song et al.~song2024can proposed two effective mitigation strategy prompt templates, Many-Shot with Reference (MSwR) and Many-Shot without Reference (MSoR), to bolster the reliability and precision of LLM-based assessments. Step-by-step. Step-by-step involves breaking down complex evaluation tasks into fine-grained components, leveraging the reasoning capabilities of LLMs to simplify the evaluation process. The most straightforward example of which is perhaps Chain-of-Thought (CoT)~wei2022chain,kotonya2023little. Building on that, frameworks like G-EVAL~liu2023g have been proposed to assess the quality of NLG outputs. G-EVAL~liu2023g combines CoT with a form-filling paradigm, allowing the LLM to assess outputs in a structured manner. Similarly, ICE-Score~zhuo2023ice introduces a step-by-step framework for evaluating code, in which the LLM is instructed with task definitions, evaluation criteria, and detailed evaluation steps. By breaking the task down into clear steps, ICE-Score~zhuo2023ice improves the quality and consistency of code evaluation. Also, ProtocoLLM~yi2024protocollm employs a similar step-by-step approach to evaluate the specialized capabilities of LLMs in generating scientific protocols. Portia~li2023split achieves better evaluation results in a lightweight yet effective manner. It divides the answer into multiple parts, aligns similar content between candidate answers, and then merges them back into a single prompt for evaluation by the LLM. Some studies break down evaluations into two steps: ``explanation-rating.'' This approach suggests that providing an explanation enhances the reliability of the rating. Chiang et al.~chiang2023closer offer empirical guidelines to improve the quality of LLM evaluations, demonstrating that combining rating with explanation (rate-explain) or explanation with rating (explain-rate) leads to higher correlations with human ratings. Another effective strategy is to decompose complex evaluation standards into specific, discrete criteria, allowing the LLM to assess each aspect independently. FineSurE~song2024finesure is an advanced example of this method, offering a framework for the fine-grained evaluation of text summarization quality. It breaks down the evaluation into multiple dimensions, such as faithfulness, completeness, and conciseness. Through detailed analysis, including fact-checking and key fact alignment, FineSurE~song2024finesure outperforms traditional methods in terms of evaluation accuracy. Definition Augmentation. The Enhanced Definition approach involves refining prompts to inject improved evaluation criteria, establish assessment principles, or incorporate external knowledge into the LLM judge's decision-making process. Some studies focus on enriching and clarifying the prompts to ensure that the evaluation criteria are both comprehensive and well-defined. For example, Liu et al. propose AUTOCALIBRATE~liu2023calibrating, a multi-stage, gradient-free approach. This method involves the drafting, revision, and application of calibrated criteria, and it automatically calibrates and aligns an LLM-based evaluator to match human preferences for NLG quality assessment. Furthermore, SALC~gupta2024unveiling enables LLMs to autonomously generate context-aware evaluation criteria for self-assessment, overcoming the limitations of static, human-defined metrics. On the other hand, the LLM-as-a-Personalized-Judge approach~dong2024can introduces a novel perspective by incorporating diverse evaluative roles and principles. This allows LLMs to adapt to complex, varied evaluation scenarios, resulting in more nuanced and context-sensitive assessments. Another key aspect of Definition Augmentation is the retrieval of external knowledge, which helps reduce hallucinations and provides more factual support. For instance, BiasAlert~fan2024biasalert, a tool designed to detect social bias in LLM-generated open-text outputs. It integrates external human knowledge with the LLM judge's inherent reasoning capabilities to reliably identify and mitigate bias, outperforming GPT4-as-A-Judge across various scenarios. Moreover, Chen et al.~chen2024llms found that within retrieval-augmented generation (RAG) frameworks, LLM judges do not exhibit a significant self-preference effect during evaluation. Multi-turn Optimization. Multi-turn optimization involves iterative interactions between the evaluator and the evaluated entity, refining evaluation results through diverse forms of feedback, thus fostering deeper analysis and a progressive improvement in evaluation quality~zhou2024fairer. Unlike traditional methods that rely on predefined criteria, Xu et al. proposed ACTIVE-CRITIC~xu2024large, enabling LLMs to infer evaluation criteria from data and dynamically optimize prompts through multiple rounds of interaction. Moreover, Some studies~zhao2024auto,luo2024videoautoarena,bai2024benchmarking,yu2024kieval leverage LLMs as question designers to engage in dynamic interactions with the evaluated entities, adjusting the questions and task design in real time. This allows for flexible modification of the evaluation content based on the performance of the evaluated entity, thereby enabling more comprehensive assessments.",
      "origin_cites_number": 28
    },
    {
      "section_title": "Tuning-based",
      "level": "3",
      "content": "\\par Tuning involves training a pre-existing LLM on a specialized dataset to adapt it to specific judgment tasks. It's especially useful when the judgment domain involves highly specialized knowledge or nuanced decision-making~huang2024limitations. Score-based Tuning. Score-based tuning involves using data with scores to train models and enhance their ability to predict judgment scores based on specific evaluation criteria~chen2023adaptation,deshwal2024phudge,wang2023learning. Many studies have explored the enhancement of LLM-as-judges by fine-tuning them on human-labeled datasets. For instance, PHUDGE~deshwal2024phudge, fine-tuned from the Phi-3 model, achieves state-of-the-art performance in terms of latency and throughput when automatically evaluating the quality of outputs from LLMs. This fine-tuning process equips the model with the necessary judgment skills, enabling it to assess various types of content in a structured and accurate manner. Additionally, ECT~wang2023learning introduces a novel method for transferring scoring capabilities from LLMs to lighter models. This allows the lighter models to function as effective reward models for sequence generation tasks, enhancing sequence generation models through reinforcement learning and reranking approaches. AttrScore~yue2023automatic is another framework for evaluating attribution and identifying specific types of attribution errors, using a curated test set from a generative search engine and simulated examples from existing benchmarks. The above research highlights that LLMs can better align their decision-making process with humans through fine-tuning with human-constructed datasets. In addition to human-labeled data, some studies have also attempted to fine-tune models using synthetic datasets like SorryBench~xie2024sorry generated for evaluation tasks. These datasets are often created through rule-based methods or by generating artificial evaluation examples, which also give rise to some metrics like TIGERScore~jiang2023tigerscore. SELF-J~ye2024self is a self-training framework for developing judge models to evaluate LLMs' adherence to human instructions without human-annotated quality scores. SELF-J~ye2024self proposes selective instruction following, allowing systems to decline low-quality instructions. FENCE~xie2024improving is another factuality evaluator designed to provide claim-level feedback to language model generators. It details a data augmentation approach that enriches public datasets with textual critiques and diverse source documents from various tools, thereby enhancing factuality without introducing lesser-known facts. Utilizing synthetic training data to fine-tune lightweight language model judges and employing prediction-powered inference (PPI) for statistical confidence to mitigate potential prediction errors, ARES~saad2023ares can automatically assess RAG systems. Preference-based Learning. Preference-based learning focuses on training LLMs to make inferences and learn based on preferences, enabling the development of more adaptive and customizable evaluation capabilities. Initially, researchers leverage these data in conjunction with advanced techniques like Direct Preference Optimization (DPO)~rafailov2024direct to train LLMs for more nuanced evaluative capabilities. In this method, the model is trained to predict which of two outputs is preferred according to human-like values, rather than learning a scalar reward signal. Such self-improving approach is well reflected in Meta-Rewarding~wu2024meta. Con-J~ye2024beyond trains a generative judge by using the DPO loss on contrastive judgments and the SFT loss on positive judgments to align LLMs with human values. In terms of evaluating other LLMs effectively in open-ended scenarios, JudgeLM~zhu2023judgelm addresses key biases in the fine-tuning process with a high-quality preference dataset. Another typical method is PandaLM~wang2023pandalm, which is trained on a reliable human-annotated preference dataset, focusing extends beyond just the objective correctness of responses, and addresses vital subjective factors. Moreover, Self-Taught~wang2024self is another approach to train LLMs as effective evaluators without relying on human-annotated preference judgments, using synthetic training data only. Through an iterative self-improvement scheme, LLM judges are able to produce reasoning traces and final judgments. Not quite the same, FedEval-LLM~he2024fedeval fine-tunes many personalized LLMs without relying on labeled datasets to provide domain-specific evaluation, mitigating biases associated with single referees. It is designed to assess the performance of LLMs on downstream tasks, at the same time, ensuring privacy preservation. As research has progressed, newer methods have emerged that combine both score-based and preference-based data to refine model evaluation capabilities, not to mention some novel metrics like INSTRUCTSCORE~xu2023instructscore. FLAMe~vu2024foundational is an example of such an approach. It's a family of Foundational Large Autorater Models which significantly improves generalization to a wide variety of held-out tasks using both pointwise and pairwise methods during training. As generative judge model, AUTO-J~li2023generative addresses challenges in generality, flexibility, and interpretability by training on a diverse dataset containing scoring and preference. To critique and refine the outputs of large language models, Shepherd~wang2023shepherd leverages a high-quality feedback dataset to identify errors and suggest improvements across various domains. In the domain of NLG, X-EVAL~liu2023x consists of a vanilla instruction tuning stage and an enhanced instruction tuning stage that exploits connections between fine-grained evaluation aspects. Notably, Themis~hu2024themis also achieved outstanding results acting as a reference-free NLG evaluation language model designed for flexibility and interpretability. Similarly, CritiqueLLM~ke2024critiquellm provides effective and explainable evaluations of LLM outputs, and uses a dialogue-based prompting method to generate high-quality referenced and reference-free evaluation data. Self-Rationalization~trivedi2024self enhances LLM performance by iteratively fine-tuning the judge via DPO, which allows LLMs to learn from their own reasoning. Based on pointwise and pairwise dataset, CompassJudger-1~cao2024compassjudger acts as an open-source, versatile LLM for efficient and accurate evaluation of other LLMs. Likewise, Zhou et al.~zhou2024mitigating introduces a systematic framework for bias reduction, employing calibration for closed-source models and contrastive training for open-source models. Apart from that, HALU-J~wang2024halu is designed to enhance hallucination detection in LLMs by selecting pertinent evidence and providing detailed critiques. PROMETHEUS~kim2023prometheus and PROMETHEUS 2~kim2024prometheus are open-source LLMs specialized for fine-grained evaluation that can generalize to diverse, real-world scoring rubrics beyond a single-dimensional preference, supporting both direct assessment and pairwise ranking, and can evaluate based on custom criteria. What's more, the following PROMETHEUS-VISION~lee2024prometheusvision fills the gap in the visual field. As for various multimodal tasks, LLaVA-Critic~xiong2024llava demonstrates its effectiveness in providing reliable evaluation scores and generating reward signals for preference learning, highlighting the potential of open-source LMMs in self-critique and evaluation. table*[t] \\centering An Overview of Fine-Tuning Methods in Single-LLM Evaluation (Sorted in ascending alphabetical order). 0.56{ tabular{cccccccc} \\hline 2{*}{Method} & 3{c}{Data Construction} & 2{c}{Tuning Method} & 2{*}{Base LLM} \\\\ \\cmidrule(lr){2-4} \\cmidrule(lr){5-6} &Annotator&Domain&Scale&Evaluation Type&Technique&\\\\ \\hline ARES~saad2023ares&Human \\& LLM&RAG System&-&Pairwise&PPI&DeBERTa-v3-Large\\\\ mygray AttrScore~yue2023automatic&Human&Various&63.8K&Pointwise&SFT&Multiple LLMs\\\\ AUTO-J~li2023generative&Human \\& GPT-4&Various&4396&Pointwise \\& Pairwise&SFT&Llama2-13B-Chat\\\\ mygray &&&&Pointwise, Pairwise,&&\\\\ mygray -2{*}{CompassJudger-1~cao2024compassjudger}&-2{*}{Human \\& LLM}&-2{*}{Various}&-2{*}{900K}& \\& Generative&-2{*}{SFT}&-2{*}{Qwen2.5 Series}\\\\ Con-J~ye2024beyond&Human \\& ChatGPT&Creation, Math, \\& Code&220K&Pairwise&SFT \\& DPO&Qwen2-7B-Instruct\\\\ mygray CritiqueLLM~ke2024critiquellm&Human \\& GPT-4&Various&7722&Pointwise \\& Pairwise&SFT&ChatGLM3-6B\\\\ &&Machine Translation,&&&&\\\\ &&Text Style Transfer,&&&&\\\\ -3{*}{ECT~wang2023learning}&-3{*}{ChatGPT}&\\& Summarization&-3{*}{-}&-3{*}{Pointwise}&-3{*}{SFT \\& RLHF}&-3{*}{RoBERTa}\\\\ mygray &&Instruct-tuning&5K, 10K,&&&\\\\ mygray -2{*}{FedEval-LLM~he2024fedeval}&-2{*}{Human}&\\& Summary&per client&-2{*}{Pairwise}&-2{*}{LoRA}&-2{*}{Llama-7B}\\\\ &&Summarization,&&&&\\\\ -2{*}{FENCE~xie2024improving}&-2{*}{Human \\& LLM}&QA, \\& Dialogue&-2{*}{-}&-2{*}{Pointwise}&-2{*}{SFT \\& DPO}&-2{*}{Llama3-8B-Chat}\\\\ mygray &&&&Pointwise, Pairwise,&&\\\\ mygray &&&&Classification,&&\\\\ mygray -3{*}{FLAMe~vu2024foundational}&-3{*}{Human}&-3{*}{Various}&-3{*}{5.3M}&\\& Open-ended generation&-3{*}{RLHF}&-3{*}{PaLM-2-24B}\\\\ &GPT-4-Turbo&Multiple-Evidence&&&&\\\\ -2{*}{HALU-J~wang2024halu}&\\& GPT-3.5-Turbo&Hallucination Detection&-2{*}{2663}&-2{*}{Pointwise \\& Pairwise}&-2{*}{SFT \\& DPO}&-2{*}{Mistral-7B-Instruct}\\\\ mygray HelpSteer2~wang2024helpsteer2&Human&Various&-&Pointwise \\& Pairwise&PPI \\& RLHF&Llama3.1-70B-Instruct\\\\ INSTRUCTSCORE~xu2023instructscore&GPT-4&Various&40K&Pointwise \\& Pairwise&SFT&Llama-7B\\\\ mygray JudgeLM~zhu2023judgelm&GPT-4&Open-ended Tasks&100K&Pairwise&SFT&Vicuna Series\\\\ LLaVA-Critic~xiong2024llava&GPT-4o&Various&113K&Pointwise \\& Pairwise&DPO&LLaVA-OneVision(OV) 7B \\& 72B\\\\ mygray Meta-Rewarding~wu2024meta&Llama3&Various&20K&Pairwise&DPO&Llama3-8B-Instruct\\\\ OffsetBias~park2024offsetbias&Human \\& LLM&Bias Detection&268K&Pairwise&RLHF&Llama3-8B-Instruct\\\\ mygray PandaLM~wang2023pandalm&Human&Various&300K&Pairwise&SFT&Llama-7B\\\\ PHUDGE~deshwal2024phudge&Human \\& GPT-4&NLG&-&Pointwise \\& Pairwise&LoRA&Phi-3\\\\ mygray PROMETHEUS~kim2023prometheus&Human&Various&100K&Pointwise&SFT&Llama2-Chat-7B \\& 13B\\\\ PROMETHEUS2~kim2024prometheus&Human&Various&300K&Pointwise \\& Paiwise&SFT&Mistral-7B \\& Mistral-8x7B\\\\ mygray PROMETHEUS-VISION~lee2024prometheusvision&GPT-4V&Various&15K&Pointwise&SFT&Llava-1.5\\\\ &&Common, Coding,&&&&\\\\ -2{*}{SELF-J~ye2024self}&-2{*}{Human \\& GPT-4}&\\& Academic&-2{*}{5.7M}&-2{*}{Pointwise}&-2{*}{LoRA}&-2{*}{Llama2-13B}\\\\ mygray Self-Rationalization~trivedi2024self&LLM&Various&-&Pointwise \\& Pairwise&SFT \\& DPO&Llama3.1-8B-Instruct\\\\ Self-Taught~wang2024self&LLM&Various&20K&Pairwise&-&Llama3-70B-Instruct\\\\ mygray Shepherd~wang2023shepherd&Human&Various&-&Pointwise \\& Pairwise&SFT&Llama-7B\\\\ SorryBench~xie2024sorry&Human \\& GPT-4&Unsafe Topics&2.7K&Pointwise&SFT&Multiple LLMs\\\\ mygray Themis~hu2024themis&Human \\& GPT-4&NLG&67K&Pointwise \\& Pairwise&SFT \\& DPO&Llama3-8B\\\\ TIGERScore~jiang2023tigerscore&Human \\& GPT-4&Text Generation&42K&Pointwise&SFT&Llama2-7B \\& 13B\\\\ mygray X-EVAL~liu2023x&Human&NLG&55,602&Pointwise \\& Pairwise&SFT&Flan-T5\\\\ \\hline tabular } table*",
      "origin_cites_number": 63
    },
    {
      "section_title": "Post-processing",
      "level": "3",
      "content": "\\par Post-processing involves further refining evaluation results to extract more precise and reliable outcomes. This step typically includes analyzing the initial outputs to identify patterns, inconsistencies, or areas requiring improvement, followed by targeted adjustments and in-depth analysis. By addressing these issues, post-processing ensures that the evaluation results are not only accurate but also aligned with the specific objectives and standards of the task. Probability Calibration. During the post-hoc process of the model output, some studies use rigorous mathematical derivations to quantify the differences, thereby optimizing them. For instance, Daynauth et al.~daynauth2024aligning investigates the discrepancy between human preferences and automated evaluations in language model assessments, particularly employs Bayesian statistics and a t-test to quantify bias towards higher token counts, and develops a recalibration procedure to adjust the GPTScorers. Apart from that, ProbDiff~xia2024language is another novel self-evaluation method for LLMs that assesses model efficacy by computing the probability discrepancy between initial responses and their revised versions. Moreover, Liusie et al.~liusie2024efficient introduces a Product of Experts (PoE) framework for efficient comparative assessment using LLMs, which yield an expression that can be maximized with respect to the underlying set of candidates. This paper proposes two experts, a soft Bradley-Terry expert and a Gaussian expert that has closed-form solutions. Unlike from frameworks above, CRISPR~yang2024mitigating is a novel bias mitigation method for LLMs executing instruction-based tasks, which identifies and prunes bias neurons with probability calibration, reducing bad performance without compromising pre-existing knowledge. Text Reprocessing. In LLMs-as-judges, text reprocessing methods are essential for enhancing the accuracy and reliability of evaluation outcomes. Specifically, text processing can improve the evaluation process by integrating multiple evaluation results or outcomes from several rounds of assessment. For example, Sottana et al.~sottana2023evaluation employs a multi-round evaluation process. Each round involves scoring model outputs based on specific criteria, with the human and GPT-4 evaluations ranking model performances from best to worst and averaging these rankings to mitigate subjectivity. For the single-response evaluation, AUTO-J~li2023generative employs a \"divide-and-conquer\" strategy. Critiques that either adhere to or deviate from the scenario-specific criteria are consolidated to form a comprehensive evaluation judgment and then generate the final assessment. Consistent with former aforementioned studies, Yan et al.~yan2024consolidating introduces a post-processing method to consolidate the relevance labels generated by LLMs. It demonstrates that this approach effectively combines both the ranking and labeling abilities of LLMs through post-processing. Furthermore, REVISEVAL~zhang2024reviseval is a novel evaluation paradigm that enhances the reliability of LLM Judges by generating response-adapted references through text revision capabilities of LLMs. Apart from that, Tessler et al.~tessler2024ai explores the use of AI as a mediator in democratic deliberation, aiming to help diverse groups find common ground on complex social and political issues. With the goal of maximizing group approval, the researchers developed the \"Habermas Machine\", which iteratively generate group statements based on individual opinions. Another category of text reprocessing methods involves task transformation, primarily focusing on the conversion between open-ended and multiple-choice question (MCQ) formats. Ren et al.~ren2023self explores the use of self-evaluation to enhance the selective generation capabilities of LLMs. Specifically, the authors reformulate open-ended generation tasks into token-level prediction tasks, reduce sequence-level scores to token-level scores to improve quality calibration. Conversely, Myrzakhan et al.~myrzakhan2024open introduces the Open-LLM-Leaderboard, a new benchmark for evaluating LLMs using open-style questions, which eliminates selection bias and random guessing issues associated with multiple-choice questions. It presents a method to identify suitable open-style questions and validate the correctness of LLM open-style responses against human-annotated ground-truths.",
      "origin_cites_number": 11
    },
    {
      "section_title": "Multi-LLM System",
      "level": "2",
      "content": "\\par Multi-LLM Evaluation harnesses the collective intelligence of multiple LLMs to bolster the robustness and reliability of evaluations. By either facilitating inter-model communication or independently aggregating their outputs, these systems can effectively mitigate biases, leverage complementary strengths across different models, refine decision-making precision, and foster a more nuanced understanding of complex judgments.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Communication",
      "level": "3",
      "content": "\\par Communication means the dynamic flow of information between LLMs, which is pivotal for sparking insights and sharing rationales during the judgment process. Recent research has shown that communication among LLMs can enable emergent abilities through their interactions~xu2023towards, leading to a cohesive decision-making process and better judgment performance. The Multi-LLM system can benefit from LLM interactions in two ways: cooperation and competition. Cooperation. Multi-LLMs can work together to achieve a common goal with information and rationales sharing through interactions to enhance the overall evaluation process. For example, zhang2023wider proposed an architecture named WideDeep to aggregate information at the LLM's neuro-level. In addition, xu2023towards introduced a multi-agent collaboration strategy that mimics the academic peer review process to enhance complex reasoning in LLMs. The approach involves agents creating solutions, reviewing each otherâs work, and revising their initial submissions based on feedback. Similarly, ABSEval~liang2024abseval utilizes four agents for answer synthesize, critique, execution, and commonsense, to build the overall workflow. Although the cooperation can complement each other's strengths between LLMs to a certain degree, this method still includes the risk of groupthink, where similar models reinforce each otherâs biases rather than providing diverse insights. Competition. Multi-LLMs systems can also benefit from competitive or adversarial communication, i.e., LLMs argue or debate to evaluate each other's outputs~zhao2024auto,moniri2024evaluating,chan2023chateval,li2023prd. Such multi-LLMs systems could be categorized into centralized and decentralized structures~owens2024multi. In the centralized structure, a single central LLM acts as the orchestrator of the conversation, highlighting the efficiency of a unified decision-making process. Auto-Arena~zhao2024auto is such a novel framework that automates the evaluation of LLMs through agent peer battles and committee discussions, aiming to provide timely and reliable assessments. In detail, the framework conducts multi-round debates between LLM candidates, and uses an LLM judge committee to decide the winner. Inspired by courtroom dynamics, bandi2024adversarial propose two architectures, MORE and SAMRE, which utilize multiple advocates and iterative debates to dynamically assess LLM outputs. In contrast, the decentralized structure emphasizes a collective intelligence where all models engage in direct communication, promoting a resilient and distributed decision-making structure. In the domain of LLM debates, Moniri et al.~moniri2024evaluating introduced a unique automated benchmarking framework, employing another LLM as the judge to assess not only the models' domain knowledge but also their abilities in problem definition and inconsistency recognition. ChatEval~chan2023chateval is another multi-agent debate framework that utilizes multiple LLMs with diverse role prompts and communication strategies on open-ended questions and traditional NLG tasks, significantly improves evaluation performance compared to single-agent methods. Moreover, PRD~li2023prd applied peer rank and discussion to address issues like self-enhancement and positional bias in current LLM evaluation methods, leading to better alignment with human judgments and a path for fair model capability ranking.",
      "origin_cites_number": 11
    },
    {
      "section_title": "Aggregation",
      "level": "3",
      "content": "Alternatively, in multi-LLM systems without communication, judgments are independently generated by multiple models, which are subsequently synthesized into a final decision through various aggregation strategies. Techniques such as majority vote, weighted averages, and prioritizing the highest confidence predictions, each play a crucial role. These methods allow each model to assess without interference, and eventually extract and combine the most effective elements from each model's response. Simple voting methods, such as majority voting, by selecting the most frequent answers, offers a straightforward approach to synthesize evaluations. For example, Badshah et al.~badshah2024reference introduced a reference-guided verdict method for evaluating free-form text using multiple LLMs as judges. Combining these LLMs through majority vote significantly improves the reliability and accuracy of evaluations, particularly for complex tasks. Furthermore, PoLL~verga2024replacing demonstrates that using a diverse panel of smaller models as judges through max voting and average pooling is not only an effective method for evaluating LLM performance, but also reduces intra-model bias of a single large model. Language-Model-as-an-Examiner~bai2024benchmarking is another benchmarking framework to evaluate the performance of foundation models on open-ended question answering through voting. In the peer-examination mechanism, the LM serves as a knowledgeable examiner that formulates questions and evaluates responses in a reference-free manner. What's more, multi-LLM evaluation could also be used in improving dataset quality. Choi et al.~choi2024multi provided an enhanced dataset, MULTI-NEWS+, which is the result of a cleansing strategy leveraging CoT and majority voting to identify and exclude irrelevant documents through LLM-based data annotation. Weighted scoring aggregation involves assigning different importance to different model outputs, either by aggregating multiple overall scores for the same response or by combining assessments of different aspects of the response to form a comprehensive evaluation. On the one hand, through a peer-review mechanism, PiCO~ning2024pico allows LLMs to answer unlabeled questions and evaluate each other without human annotations. It formalizes the evaluation as a constrained optimization problem, maximizing the consistency between LLMs' capabilities and corresponding weights. Likewise, PRE~chu2024pre,chen2024automaticcostefficientpeerreviewframework can automatically evaluate LLMs through a peer-review process. It selects qualified LLMs as reviewers through a qualification exam and aggregates their ratings using weights which is proportional to their agreement of humans, demonstrating effectiveness and robustness in evaluating text summarization tasks. In the field of recommendation explanations, Zhang et al.~zhang2024large suggests that ensembles like averaging ratings of multiple LLMs can enhance evaluation accuracy and stability. On the other hand, for example, AIME~patel2024aime is an evaluation protocol that utilizes multiple LLMs that each with a specific role independently generate an evaluation on separate criteria and then combine them via concatenation. Similarly, a paper introduces HD-EVAL~liu2024hd, which iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. By decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria, HD-EVAL demonstrates its superiority. Apart from weighting methods, there are some other advance mathematical aggregation techniques, such as Bayesian methods and graph-based approaches, offering more robust ways to handle uncertainties and inconsistencies across multiple evaluators. Notably, a paper introduces two calibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian Dawid-Skene~gao2024bayesian, to address the win rate estimation bias when using many LLMs as evaluators for text generation quality. In addition to that, GED~hu2024language addresses inconsistencies in LLM preference evaluations by leveraging multiple weak evaluators to construct preference graphs, and then utilize DAG structure to ensemble and denoise these graphs for better, non-contradictory evaluation results. LLM-based aggregation is a grand-new perspective like Fusion-Eval~shu2024fusion. It's a novel framework that integrates various assistant evaluators using LLMs, each of which specializes in assessing distinct aspects of responses, to enhance the correlation of evaluation scores with human judgments for natural language systems. In addition to the above direct use of multiple model evaluation, the cascade framework employs a tiered approach, where weaker models are used initially for evaluations, and stronger models are engaged only when higher confidence is required, optimizing resource use and enhancing evaluation precision. Jung et al.~jung2024trust proposes \"Cascaded Selective Evaluation\" to ensure high agreement with human judgments while using cheaper models. Similar to the work above, Huang et al.~huang2024limitations proposes CascadedEval, a novel method integrating proprietary models, in order to compensate for the limitations of fine-tuned judge models.",
      "origin_cites_number": 14
    },
    {
      "section_title": "Human-AI Collaboration System",
      "level": "2",
      "content": "Human-AI Collaboration Systems bridge the gap between automated LLM judgments and the essential need for human oversight, particularly in high-stakes domains such as law, healthcare, and education. Human evaluators act either as the ultimate deciders, or as intermediaries who verify and refine model outputs. By incorporating human insights, Hybrid systems can ensure the final judgment is more reliable and aligned with ethical considerations, and empower continuous model improvement through feedback loops. In many Human-AI Collaboration systems, human evaluators play a vital role during the evaluation process itself, actively collaborating with the LLMs to review and refine the generated outputs. For example, COEVALli2023collaborative introduces a collaborative evaluation pipeline where LLMs generate initial criteria and evaluations for open-ended natural language tasks. These machine-generated outputs are then reviewed and refined by human evaluators to guarantee reliability. To address a significant positional bias in LLMs when used as evaluators, Wang et al.~wang2023large proposes a calibration framework with three strategies: Multiple Evidence Calibration, Balanced Position Calibration, and Human-in-the-Loop Calibration. Similarly, EvalGenshankar2024validates integrates human feedback iteratively to refine evaluation criteria, addressing challenges such as ``criteria drift'', where the standards of evaluation evolve as humans interact with the model. These systems allow human evaluators to provide real-time adjustments, enhancing the accuracy and trustworthiness of the evaluation process. While in other systems, human involvement takes place after the LLM has completed its evaluations, providing a final layer of verification and adjustment. This method ensures that the LLM's judgments are thoroughly scrutinized and aligned with human values. EvaluLLMpan2024human allows humans to intervene and refine the evaluation results, thereby enhancing trust in the modelâs performance while also controlling for potential biases. Additionally, Chiang et al.chiang2024large tried LLM TAs as an assignment evaluator in a large university course. After students submit assignments and receive LLM-generated feedback, the teaching team reviews and finalizes the evaluation results. This process illustrates how human oversight after the initial automated evaluation can guarantee fairness and consistentcy with academic standards.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Application",
      "level": "1",
      "content": "Due to the convenience and effectiveness of LLM Judges, they have been widely applied as judges across various domains. These applications not only cover general domains but also specific domains such as multimodal, medical, legal, financial, education, information retrieval and others. In this section, we will provide a detailed introduction to these applications, demonstrating how LLMs achieve precise and efficient evaluations in different domains.",
      "origin_cites_number": 0
    },
    {
      "section_title": "General",
      "level": "2",
      "content": "In general domains, LLM Judges are applied to tasks requiring both understanding and generation, such as dialogue generation, open-ended question answering, summarization, and language translation. Each task follows its own set of evaluation criteria to meet its specific requirements. For instance, in dialogue generation li2017dailydialog, the criteria emphasize the natural flow, emotional resonance, and contextual relevance of the conversation. In summarization tasks narayan2018don, the evaluation focuses on the coherence, consistency, fluency, and relevance of the text. In translation tasks feng2024improving, the assessment prioritizes the quality, accuracy, fluency, and style. As these diverse sub-tasks require specialized evaluation criteria, LLM judges provides refined evaluation methods that go beyond traditional metrics, paving the way for more comprehensive and in-depth assessments. For instance, Shu et al. shu2024fusion introduced Fusion-Eval, an innovative approach that leverages LLMs to integrate insights from various assistant evaluators. Fusion-Eval evaluated summary quality across four dimensionsâcoherence, consistency, fluency, and relevance, achieving a system-level Kendall-Tau correlation of 0.962 with human judgments. For dialogue quality, it assessed six aspects: coherence, engagingness, naturalness, groundedness, understandability, and overall quality, attaining a turn-level Spearman correlation of 0.744. Furthermore, Xu et al. xu2024large proposed the ACTIVE-CRITIC framework, which enables LLMs to actively infer the target task and relevant evaluation criteria while dynamically optimizing prompts. In the story generation task, this framework achieved superior evaluation performance.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Multimodal",
      "level": "2",
      "content": "In the multimodal domain, the evaluation objects of LLMs are not limited to textual data but extend to various forms of information such as images, audio, and video. One of the primary challenges in evaluating multimodal tasks lies in the significant heterogeneity among these modalities, including substantial differences in data structures, representation methods, and feature distributions. To address this challenge, advanced techniques are often required to help LLMs integrate different forms of information, ensuring that they can provide accurate and meaningful evaluations. For example, Xiong et al. xiong2024llava trained an open-source multimodal LLM, LLaVA-Critic, specifically to evaluate model performance in multimodal scenarios. Similarly, Chen et al. chen2024mllm developed a Multimodal LLM-as-a-judge for 14 Vision-Language tasks, providing a unified evaluation framework. In addition, LLMs-as-judges can also be used in audio. For instance, Latif et al. latif2023can used LLMs for identifying and evaluating emotional cues in speech, achieving remarkable accuracy in the process. Beyond these efforts, some recent studies zhou2024calibrated, deng2024efficient have also explored the potential of multimodal LLMs to self-evaluate and self-reward, enhancing their performance without the need for external evaluators or human annotations. As the application of LLMs-as-judges continues to expand in multimodal domains, there is a growing interest in exploring their use in more specific real-world scenarios, such as autonomous driving. Chen et al. chen2024automated proposed CODA-LM, a novel vision-language benchmark for self-driving, which provides automatic and systematic evaluation of Large Vision-Language Models (LVLMs) on road corner cases. Interestingly, they found that using the text-only LLM judges resulted in a closer alignment with human preferences than LVLMs.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Medical",
      "level": "2",
      "content": "In the medical field, LLMs-as-judges have demonstrated significant potential, particularly in areas such as diagnostic support, medical text analysis, clinical decision-making, and patient education. In this domain, high-quality evaluation requires LLM judges to possess precise interpretation capabilities for domain-specific terminology, the ability to comprehensively analyze diverse data types (such as clinical records and medical imaging), and strict compliance with high accuracy standards and ethical guidelines. In the realm of medical text generation, Xie et al. xie2024doclens used LLMs to evaluate the compduikeyi1leteness, conciseness, and attribution of medical texts at a fine-grained level. Similarly, Brake et al. brake2024comparing leveraged LLMs, such as Llama2, to assess clinical note consistency, with results indicating agreement levels comparable to human annotators. When it comes to medical question answering, Krolik et al. krolik2024towards explored the use of LLMs to automatically evaluate answer quality. Their focus was on evaluating adherence to medical knowledge and professional standards, completeness of information, accuracy of terminology, clarity of expression, and relevance to the question. In the area of mental health counseling, Li et al. li2024automatic utilized LLMs to automate the evaluation of counseling effectiveness and quality. Key assessments included whether the counseling identified the clientâs emotional needs, provided appropriate responses, demonstrated empathy, managed negative emotions, and met the overall goals of mental health support. Beyond these above applications, LLMs' judging capabilities have also been applied to assist in improving performance in specialized medical reasoning tasks. For instance, many studies jeong2024improving employed LLMs to evaluate and filter medical information, thereby supporting enhanced medical reasoning.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Legal",
      "level": "2",
      "content": "Due to the powerful evaluation capabilities of LLMs, LLMs-as-judges have been widely applied in the legal domain, covering multiple key scenarios, including evaluating the performance of law LLMs and relevance judgment in legal case retrieval. In the legal domain, the application of LLMs requires a deep understanding of the legal framework of specific jurisdictions, complex legal language, and rigorous logical reasoning abilities~li2024lexevalcomprehensivechineselegal. At the same time, interpretability and transparency of the evaluation results are essential core requirements, as legal practice highly depends on clear logic and verifiable conclusions. Furthermore, the bias and fairness of the model are of significant concern, as any bias in legal evaluations could have a profound impact on judicial fairness. These unique demands set higher standards for LLM judges. In response to these challenges, recent research has explored various ways in which LLMs can be effectively employed in legal evaluations. Some research used LLMs as judges to assist in evaluating the performance of Law LLMs. For example, Yue et al. yue2023disc introduced DISC-LawLLM to provide a wide range of legal services and utilized GPT-3.5 as a referee to evaluate the modelâs performance. They assessed three key criteriaâaccuracy, completeness, and clarityâby assigning a rating score from 1 to 5. Similarly, Ryu et al. ryu2023retrieval applied retrieval-based evaluation to assess the performance of LLMs in Korean legal question-answering tasks, which applied RAG not for generation but for evaluation. What's more, LLMs have also been utilized to construct evaluation sets. Raju et al. raju2024constructing explored methods for constructing these domain-specific evaluation sets, which are essential for enabling LLMs-as-judges to perform effective evaluation in legal domain. Beyond performance evaluation, LLMs have also been utilized for relevance judgment in legal case retrieval. For instance, Ma et al. ma2024leveraging used LLMs to automate the evaluation of large numbers of retrieved legal documents, improving both the scalability and accuracy of legal case retrieval systems. In conclusion, the application of LLMs-as-judges in law holds significant promise in future.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Financial",
      "level": "2",
      "content": "In the financial domain, LLM judges have been extensively explored in scenarios such as investment risk assessment and credit scoring, which presenting unique challenges. For example, the complexity of risk assessment requires LLMs to accurately capture the influence of multifaceted factors, including market volatility, regulatory changes, and geopolitical events. Real-time processing demands further elevate the challenge, requiring LLMs not only to be computationally efficient but also to deliver rapid response times. Additionally, the dynamic nature of high-frequency trading demanding that LLMs swiftly adapt to fluctuating market conditions. In investment risk assessment, LLMs have proven effective due to their ability to process large amounts of data and make informed judgments. For instance, Xie et al. xie2023pixiu developed a financial LLM, FinMA, fine-tuned on LLaMA to evaluate investment risks more effectively. Their model is designed to follow instructions for risk assessment and decision analysis, improving the accuracy and efficiency of financial evaluations. Another key application in the financial domain is credit scoring, which predicts the future repayment ability and default risk of individuals or businesses. By analyzing a vast array of data, including credit history, financial status, and other relevant factors, LLMs can help financial institutions make more accurate credit scoring assessments. For example, Babaei et al. babaei2024gpt demonstrated how LLMs can process unstructured text data, such as customer histories, contract terms, and news reports, to enhance the precision of credit assessments. Furthermore, as the use of LLMs in finance continues to grow, there is a rising need to evaluate the performance of these financial LLMs. To address this, Son et al. son2024krx developed an automated financial evaluation benchmark that leverages LLMs to extract valuable insights from both unstructured and structured data. This framework helps optimize the construction, updating, and compliance checks of financial benchmarks, supporting more efficient and scalable evaluation processes. Based on this, they facilitated the continuous optimization of financial LLMs, driving further advancements in the financial domain.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Education",
      "level": "2",
      "content": "LLMs-as-judges have found extensive applications in the education domain, covering a wide range of tasks, such as grading student assignments, evaluating essays, assessing mathematical reasoning, and judging debate performance. These applications present several key challenges, including the diversity of student responses and individual differences, as well as the need for multidimensional evaluation. Effective evaluation in education requires LLMs to consider not only correctness but also creativity, clarity, and logical coherence. Additionally, the interpretability and fairness of the evaluation results are crucial, as educational assessments significantly impact students' development and future opportunities. In assignment grading, Chiang et al. chiang2024large introduced the concept of an LLM Teaching Assistant (LLM TA) in university classrooms, utilizing GPT-4 to automate the grading of student assignments. By employing prompt engineering to define scoring criteria and task descriptions, LLM TA generates quantitative scores and detailed feedback. Their study emphasized the systemâs ability to maintain consistency, adhere to grading standards, and resist adversarial prompts, highlighting its robustness and practicality for classroom use. In addition to assignment grading, LLMs-as-judges are also being explored for automated essay scoring. Wang et al. wang2024automated proposed an advanced intelligent essay scoring system, integrating LLMs such as BERT and ChatGPT to enable automated scoring and feedback generation for essays across various genres. Similarly, Song et al. song2024automated investigated a framework and methodology for automated essay scoring and revision based on open-source LLMs. Furthermore, Zhou et al. zhou2024llm explored the potential of LLMs in academic paper reviewing tasks, assessing their reliability, effectiveness, and possible biases as reviewer. They found that while LLMs show certain promise in the domain of automated reviewing, they are not yet sufficient to fully replace human reviewers, particularly in areas with high technical complexity or strong innovation. Another area where LLMs-as-judges are making an impact is in the evaluation of math reasoning. Unlike traditional mathematical task evaluation, which focuses solely on the correctness of the final results, Xia et al. xia2024evaluating argued that additional aspects of the reasoning process should also be assessed, such as logical errors or unnecessary steps. In their work, the authors proposed ReasonEval, a new methodology for evaluating the quality of reasoning steps based on LLMs-as-judges. LLMs have also been employed in judging debate performance. Liang et al. liang2024debatrix proposed Debatrix, a new method which leverages LLMs to evaluate and analyze debates. The main aspects assessed include the logical consistency of arguments, the effectiveness of rebuttals, the appropriateness of emotional expression, and the coherence of the debate.",
      "origin_cites_number": 6
    },
    {
      "section_title": "Information Retrieval",
      "level": "2",
      "content": "Information retrieval refers to the process of effectively retrieving, filtering, and ranking relevant information from a large collection of data, matching information resources to users' needs (queries). However, evaluating these systems presents several challenges, particularly due to the the complexity of real-world data, the diversity of user needs, and personalization. To solve these challenges, LLMs-as-judges have been used across various applications, including relevance judgment, text ranking, recommendation explanations evaluation, and assessing retrieval-augmented generation (RAG) systems. One key area in information retrieval is the evaluation of the relevance of retrieved results to user queries, a task that traditionally relies on manual annotations~soboroff2024don,rahmani2024llmjudge. Rahmani et al. rahmani2024llmjudge proposed a framework called LLMJudge which leveraged LLMs to assess the relevance of information retrieval system results to user queries, providing a more scalable and efficient evaluation approach. Another important aspect of information retrieval is the ranking of search results or recommendation lists. Traditional ranking models often rely on shallow features or direct matching scores, which may not yield optimal results. To address this, Qin et al. qin2023large examined the performance of LLMs in text ranking tasks and proposed a novel method based on pairwise ranking prompting, utilizing LLMs for text ranking. Additionally, Niu et al. niu2024judgerank introduced a framework called JudgeRank, which leveraged LLMs to rerank results in reasoning-intensive tasks. By evaluating the logic, relevance, and quality of candidate results, this approach tried to enhance ranking performance. In recommendation systems, explanation evaluation plays a crucial role in helping users understand why a specific product, movie, or piece of content is recommended. Zhang et al. zhang2024large investigated the potential of LLMs as automated evaluators of recommendation explanations, assessing them across multiple dimensions such as quality, clarity, and relevance. This approach provides a more efficient way to evaluate the effectiveness of explanations, which is essential for improving user trust and satisfaction. Furthermore, with the growing use of retrieval-augmented generation (RAG) systems in tasks like question answering, fact-checking, and customer support, there is an increasing need to evaluate the quality of these systems. Traditional evaluation methods rely on large manually annotated datasets, which are time-consuming and costly. To address this, Saad et al. saad2023ares proposed a new automated evaluation framework called ARES, which leveraged LLMs as the core evaluation tool to directly assess retrieval and generated content across multiple dimensions, including relevance, accuracy, coverage, fluency, and coherence.",
      "origin_cites_number": 6
    },
    {
      "section_title": "Others",
      "level": "2",
      "content": "",
      "origin_cites_number": 0
    },
    {
      "section_title": "Soft Engineering",
      "level": "3",
      "content": "The challenges that LLMs-as-judges need to overcome in the software engineering domain include complex code structures and the diversity of evaluation criteria. A numerous of articles patel2024aime, weyssow2024codeultrafeedback used LLMs-as-judges to assess the quality of code generation. Moreover, Kumar et al. kumar2024llms employed LLMs to evaluate the quality of Bug Report Summarization.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Biology",
      "level": "3",
      "content": "The main evaluation challenges in biological field include the complexity and diversity of the data and the need for specialized biological knowledge~o2023bioplanner,hijazi2024using. For example, Hijazi et al. hijazi2024using used LLMs to evaluate Query-focused summarization (QFS), which refers to generating concise and accurate summaries from a large set of biomedical literature based on a specified query. In this context, the LLMs are used to assess whether these summaries accurately answer the specified query and whether they cover the correct biological knowledge.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Social Science",
      "level": "3",
      "content": "LLMs-as-Judges have also found applications in social sciences. On one hand, they are used in real-world human social contexts. For example, Tessler et al. tessler2024ai used LLMs to participate in democratic discussions, assessing the quality of arguments, identifying fallacies, or providing a balanced view of an issue, thus helping people reach consensus on complex social and political matters. On the other hand, LLMs-as-judges are also used in social scenarios constructed by language agents. Zhou et al. zhou2023sotopia proposed an interactive evaluation framework called Sotopia, which used LLMs to assess the social intelligence of language agents from multiple dimensions, such as emotional understanding, response adaptability, and other social skills. In this section, we have outlined the specific applications of LLMs-as-judges across various domains. In these applications, LLMs leverage their powerful text understanding and generation capabilities to perform effective evaluations and judgments, providing accurate feedback and improvement suggestions. Although LLMs-as-judges have shown tremendous potential in these areas, especially in handling large-scale data and automating assessments, they still face challenges such as the depth of domain-specific knowledge, limitations in reasoning abilities, and the diversity of evaluation criteria. In the future, with continuous improvements in model performance and domain adaptation capabilities,, we believe the application of LLMs-as-judges will become more widespread and precise across various domains.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Meta-evaluation",
      "level": "1",
      "content": "Meta-evaluation, the process of assessing the quality of the evaluator itself, is a crucial step in determining the reliability, consistency, and validity of LLM judges. Given the diverse applications of LLMs as evaluators, meta-evaluation methods have also been evolving. Researchers have proposed various datasets and metrics tailored to different tasks and evaluation objectives to assess the reliability and validity of LLM-based evaluations. This chapter will explore state-of-the-art Benchmarks (\\Ssec:benchmarks) and evaluation Metrics (\\Ssec:metric), categorize existing approaches, and discuss their advantages and limitations. table*[ht] \\footnotesize Statistical information of different benchmarks (Part 1). tabular{lccccc} \\hline Benchmark & Task & Type & Num & Evaluation Criteria & Language \\\\ \\hline \\hline mygray HumanEval~chen2021evaluating & Code & Pointwise & 164 & Functional correctness & English \\\\ SWE-bench~jimenez2023swe & Code & Pointwise & 2,294 & Task solve & English \\\\ mygray DevAI~zhuge2024agent & Code & Pointwise & 365 & tabular[c]{@{}c@{}}Disagreement, Task solve, \\\\ Requirements mettabular & English \\\\ CrossCodeEval~ding2024crosscodeeval & Code & Pointwise & 1,002 & Code match, Identifier match & English \\\\ mygray CodeUltraFeedback~weyssow2024codeultrafeedback & Code & tabular[c]{@{}c@{}}Pointwise\\\\ Pairwisetabular & 10k & tabular[c]{@{}c@{}}Code explanation, \\\\ Code complexity and efficiency, \\\\ Code readability, Coding styletabular & English \\\\ tabular[c]{@{}l@{}}Literary Translation \\\\ Comparisons~karpinska2023largetabular & Translation & Pairwise & 720 & Translation quality and errors & Multilingual \\\\ mygray MQM~freitag2021experts & Translation & Pointwise & 100k & tabular[c]{@{}c@{}}Accuracy, Fluency, \\\\ Terminology, Style, Localetabular & Multilingual \\\\ tabular[c]{@{}l@{}}WMT Metrics \\\\ Shared Task~freitag2021resultstabular & Translation & Pointwise & - & Adequacy, Fluency & Multilingual \\\\ mygray SummEval~fabbri2021summeval & Summary & Pointwise & 1,600 & tabular[c]{@{}c@{}}Coherence, Consistency, \\\\ Fluency, Relevancetabular & English \\\\ Opinsummeval~shen2023opinsummeval & Summary & Pointwise & 1,400 & tabular[c]{@{}c@{}}Aspect relevance, \\\\ Self-coherence, Readability\\\\ Sentiment consistency,tabular & English \\\\ mygray Frank ~pagnoni2021understanding & Summary & Pointwise & 2,250 & Factual errors & English \\\\ Topical-Chat~gopalakrishnan2023topical & Dialogue & Pointwise & 60 & tabular[c]{@{}c@{}}Understandable, Natural, \\\\ Maintains context, Interesting, \\\\ Uses knowledge, Overall qualitytabular & English \\\\ mygray Personal-Chat~zhang2018personalizing & Dialogue & Pointwise & 60 & tabular[c]{@{}c@{}}Understandable, Natural, \\\\ Maintains context, Interesting, \\\\ Uses knowledge, Overall qualitytabular & English \\\\ DSTC10 Hidden Set~zhang2021automatic & Dialogue & Pointwise & 9,500 & tabular[c]{@{}c@{}}Coherence, Appropriateness, \\\\ Naturalness, Toxicity controltabular & English \\\\ mygray HANNA~chhun2022human & Story & Pointwise & 1,056 & tabular[c]{@{}c@{}}Relevance, Coherence, \\\\ Empathy, Surprise, \\\\ Engagement, Complexitytabular & English \\\\ MANS~guan2021openmeva & Story & Pointwise & 2,000 & Coherence & English \\\\ mygray StoryER~chen2023storyer & Story & Pairwise & 100k & Upvoted & English \\\\ Per-DOC~wang2023learning & Story & Pointwise & 7,000 & tabular[c]{@{}c@{}}Interestingness, Adaptability,\\\\ Character developmentSurprise, Endingtabular & English \\\\ mygray PKU-SafeRLHF~ji2024pku & Value & Pairwise & 83.4K & Helpfulness, Harmlessness & English \\\\ HHH~askell2021general & Value & Pairwise & 221 & tabular[c]{@{}c@{}}Helpfulness, Honesty, \\\\ Harmlessnesstabular & English \\\\ mygray Cvalue~xu2023cvalues & Value & Pairwise & 145k & Safety, Responsibility & Chinese \\\\ Yelp ~asghar2016yelp & Recom & Pointwise & 8,630k & User perference & English \\\\ mygray Movielens\\_Explanation~zhang2024large & Recom & Pointwise & 2,496 & tabular[c]{@{}c@{}}Persuasiveness, Transparency, \\\\ Accuracy, Satisfactiontabular & English \\\\ Trec DL21\\&22 ~DBLP:conf/trec/Craswell0YCL21,DBLP:conf/trec/Craswell0YCLVS22 & Search & Pointwise & tabular[c]{@{}c@{}}1,549/\\\\ 2,673tabular & Relevacne & English \\\\ mygray LeCarDv2~li2024lecardv2 & Search & Pointwise & 55,192 & tabular[c]{@{}c@{}}Characterization relevance, \\\\ Penalty relevance, \\\\ Procedure relevancetabular & English \\\\ \\hline tabular table* table*[ht] \\footnotesize Statistical information of different benchmarks (Part 2). tabular{lccccc} \\hline Benchmark & Domain & Type & Num & Evaluation Criteria & Language \\\\ \\hline \\hline mygray UltraFeedback~cui2024ultrafeedback & Compre. & Pairwise & 64k & tabular[c]{@{}c@{}}Helpfulness, Honesty, \\\\ Instruction following, \\\\ Truthfulnesstabular & English \\\\ AlpacaEval~dubois2024length & Compre. & Pairwise & 20k & Instruction-following & English \\\\ mygray Chatbot Arena~zheng2023judging & Compre. & Pairwise & 33k & User perference & English \\\\ MTBench~zheng2023judging & Compre. & Pairwise & 3,000 & tabular[c]{@{}c@{}}Multi-turn conversational, \\\\ Instruction-followingtabular & English \\\\ mygray RewardBench~lambert2024rewardbench & Compre. & Pairwise & 2,998 & User perference & English \\\\ JudgerBench~cao2024compassjudger & Compre. & Pairwise & 1,900 & Instruction following & tabular[c]{@{}c@{}}English\\\\ Chinesetabular \\\\ mygray RM-Benchh~liu2024rm & Compre. & Pairwise & 1,327 & Instruction following & English \\\\ JUDGEBENCH~tan2024judgebench & Compre. & Pairwise & 350 & Factual, Logical correctness & English \\\\ mygray Infinity-Preference\\url{https://huggingface.co/datasets/BAAI/Infinity-Preference} & Compre. & Pairwise & 59.3k & User perference & tabular[c]{@{}c@{}}English\\\\ Chinesetabular \\\\ LLMeval~zhang2023wider & Compre. & tabular[c]{@{}c@{}}Pointwise\\\\ Pairwisetabular & 453 & tabular[c]{@{}c@{}}Correctness, Fluency, Logic, \\\\ Informativeness, Harmlessnesstabular & Chinese \\\\ mygray WildBench~lin2024wildbench & Compre. & Pointwise & 1,024 & Checklists & English \\\\ Flask~ye2023flask & Compre. & Pointwise & 1,740 & tabular[c]{@{}c@{}}Logical thinking, \\\\ Background knowledge, \\\\ Problem handling, User alignmenttabular & English \\\\ mygray AlignBench~liu2023alignbench & Compre. & Pointwise & 683 & tabular[c]{@{}c@{}}Task-oriented, Clarity \\& Fluency, \\\\ Complexity \\& Difficulty, \\\\ Desensitizationtabular & Chinese \\\\ HELPSTEER~wang2023helpsteer & Compre. & tabular[c]{@{}c@{}}Pointwise\\\\ Pairwisetabular & 37,120 & tabular[c]{@{}c@{}}Helpfulness, Correctness, \\\\ Coherence, Complexity Verbositytabular & English \\\\ mygray HELPSTEER2~wang2024helpsteer2 & Compre. & tabular[c]{@{}c@{}}Pointwise\\\\ Pairwisetabular & 21,362 & tabular[c]{@{}c@{}}Helpfulness, Correctness, \\\\ Coherence, Complexity, Verbositytabular & English \\\\ MLLM-as-a-Judge~chen2024mllm & Compre. & tabular[c]{@{}c@{}}Pointwise\\\\ Pairwise\\\\ Listwisetabular & 17,000 & {\\color[HTML]{4A4A4A} tabular[c]{@{}c@{}}Relevance, Accuracy, \\\\ Creativity, Response granularitytabular} & English \\\\ mygray MM-EvalMM-Eval~son2024mm & Compre. & Pairwise & 4,981 & Task-oriented & Multilingual \\\\ \\hline tabular table*",
      "origin_cites_number": 41
    },
    {
      "section_title": "Benchmarks",
      "level": "2",
      "content": "To evaluate LLM-based judges, a common approach is to measure their alignment with human preferences, as human judgments are often considered the gold standard for quality and reliability. Given the diverse range of applications for LLM-based judges, different benchmarks have been created, each tailored to specific evaluation criteria and use cases. In this section, we present a comprehensive collection of 40 widely-used benchmarks, each designed to capture different aspects of evaluation, such as language understanding, factual accuracy, coherence, creativity, and fairness. To enhance clarity and facilitate comparison, we categorize these benchmarks by application domain.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Code Generation",
      "level": "3",
      "content": "Code generation aims to produce executable program code from natural language input. This task typically involves translating user requirements or descriptions into precise code. The applications of code generation are vast, including automated script creation, bug fixing, and the generation of complex programming tasks.Evaluating code generation is highly challenging, and LLMs are increasingly being used as evaluators for assessing code quality. HumanEval~chen2021evaluating is a widely used benchmark dataset designed to evaluate programming capabilities. It consists of 164 coding tasks, each accompanied by a brief natural language description. The tasks primarily involve algorithmic problems and data structure exercises, with difficulty levels ranging from basic to intermediate. One notable feature of HumanEval~chen2021evaluating is the inclusion of input-output examples, which facilitate the assessment of functional correctness. However, the dataset's limited size and scope may not sufficiently capture the diversity of real-world programming challenges. SWEBench~jimenez2023swe targets more complex programming tasks that are closer to real-world software development scenarios. It includes 2,294 tasks requiring advanced operations such as reasoning, multi-step problem solving, and API usage. Unlike simpler benchmarks, SWEBench~jimenez2023swe assesses the model's ability to handle comprehensive problem-solving and logical reasoning. However, the increased complexity also introduces challenges in establishing consistent evaluation criteria, particularly when it comes to subjective aspects like code style and efficiency. Moreover, DevAI~zhuge2024agent was introduced to address the limitations of existing benchmarks, which often fail to capture the iterative nature of software development and lack adequate signals for measuring long-term progress. The dataset includes 365 task requirements, focusing on more complex and challenging programming scenarios. CrossCodeEval~ding2024crosscodeeval focuses on assessing cross-language programming models, containing over 1,000 tasks that involve translating code between different programming language pairs, such as Python to Java or JavaScript to C++. This dataset tests the modelâs ability to adapt and transform code across languages, highlighting the challenges of understanding varied syntax and semantics. CodeUltraFeedback~weyssow2024codeultrafeedback is designed to evaluate and enhance the alignment between LLMs and user-defined programming preferences. It includes 10,000 programming instructions, each paired with four responses from 14 different LLMs. These responses are scored by GPT-3.5 based on five distinct programming preferences, such as readability, efficiency, and adherence to user specifications. The dataset emphasizes fine-grained feedback and user-centered evaluation, making it a useful tool for analyzing preference alignment.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Machine Translation",
      "level": "3",
      "content": "Machine Translation (MT) refers to the process of automatically translating text from a source language to a target language. Over time, MT technology has progressed significantly, evolving from rule-based methods to Statistical Machine Translation (SMT), and more recently to Neural Machine Translation (NMT), which is now the dominant approach. With the widespread adoption of NMT and the emergence of LLMs, evaluating translation quality has become a complex task, requiring robust evaluation frameworks that can assess accuracy, fluency, and contextual relevance across diverse language pairs. The Workshop on Machine Translation (WMT)~freitag2021results is a prominent annual evaluation event in the field of MT. It provides large-scale, human-annotated datasets for a variety of language pairs, including English-French, English-German, and English-Russian. Each year, WMT releases benchmark datasets that include source texts, model-generated translations, reference translations, and human evaluation scores. These datasets are widely used for assessing the performance of automated evaluation metrics by comparing their outputs against human judgments. WMT covers a broad range of tasks, from sentence-level translation to document-level and domain-specific challenges, making it a comprehensive resource for evaluating the correlation between automated evaluators and human assessments. However, WMT primarily focuses on high-resource languages, which may limit its applicability to low-resource or underrepresented languages. Literary Translation Comparisons~karpinska2023large is designed to assess document-level translation quality, particularly in the context of literary works. It includes carefully selected paragraphs from various literary pieces, covering 18 language pairs such as Japanese-English, Polish-English, and French-English. Unlike sentence-level benchmarks, this dataset emphasizes the importance of evaluating translations in a broader context, as literary texts often require understanding of stylistic elements and cultural subtleties. This makes it particularly useful for evaluating the performance of LLMs, which may excel in capturing broader contextual information. The MQM~freitag2021experts study is the largest evaluation effort to date focusing on machine translation quality. It involves professional translators annotating the outputs of top-performing systems from the WMT 2020 shared task, specifically targeting English-German and Chinese-English translations. MQM introduces a multidimensional quality assessment framework that goes beyond traditional metrics like BLEU or ROUGE. It evaluates translations across multiple dimensions, including accuracy, fluency, terminology, style, and locale, providing a more nuanced understanding of translation quality.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Text Summarization",
      "level": "3",
      "content": "Text Summarization (TS) is the task of generating a concise and coherent summary from a given piece of text while preserving its essential meaning. The main goal is to provide a quick, accurate overview of the source content, capturing key information and eliminating unnecessary details. As LLMs have shown impressive capabilities in generating summaries, the need for robust meta-evaluation benchmarks is critical to effectively assess their performance across various dimensions like coherence, relevance, consistency, and fluency. SummEval~fabbri2021summeval is one of the most widely used benchmarks for evaluating summarization models. It includes summaries generated by 16 different models based on 100 news articles randomly sampled from the CNN/DailyMail test set. Each summary was annotated by five independent crowd-sourced workers and three expert evaluators, using a Likert scale from 1 to 5 across four key dimensions: coherence, consistency, fluency, and relevance. The dataset is valuable for analyzing the correlation between human judgments and automated evaluation metrics. The FRANK~pagnoni2021understanding dataset is dedicated to assessing the factual accuracy of summaries generated by automatic summarization systems. It provides detailed human annotations of factual errors, including semantic frame errors, discourse errors, and content verifiability issues. The dataset includes summaries from both the CNN/DailyMail and XSum datasets, making it a comprehensive resource for evaluating factual correctness. FRANKâs detailed categorization of errors offers valuable insights into the types of factual inaccuracies common in generated summaries, highlighting areas where LLMs often struggle. However, focusing solely on factual errors may overlook other aspects of summary quality, such as coherence and fluency. OpinsummEval~shen2023opinsummeval is a meta-evaluation benchmark specifically designed for opinion summarization tasks, where the goal is to extract and summarize opinions from a large volume of user reviews. This dataset includes outputs from 14 different opinion summarization models and provides human annotations across four dimensions: aspect relevance, self-consistency, sentiment consistency, and readability.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Dialogue Generation",
      "level": "3",
      "content": "Dialogue Generation is the task of automatically generating natural language conversations that are relevant to a given context. The primary goal is to develop dialogue systems that can understand context, generate fluent responses, and maintain logical consistency and contextual accuracy. Dialogue generation encompasses a wide range of applications, from chatbots and virtual assistants to social conversational agents. With the increasing capabilities of large language models (LLMs), evaluating dialogue generation has become more complex, requiring multi-faceted evaluation frameworks to assess various aspects of conversational quality. In the field of dialogue generation, the most commonly used datasets include Topical-Chat~gopalakrishnan2023topical and PERSONA-CHAT~zhang2018personalizing. The Topical-Chat~gopalakrishnan2023topical dataset aims to advance research in open-domain conversational AI, covering eight major topics such as entertainment, health, and technology. The PERSONA-CHAT~zhang2018personalizing dataset, on the other hand, focuses on enhancing dialogue systems by incorporating predefined personas to generate more personalized responses. Each dialogue participant is assigned a persona profile, consisting of several descriptive sentences about their personality or preferences. Mehri and Eskenazi~mehri2020usr conducted a meta-evaluation study on these two widely-used open-domain dialogue corpora. They manually annotated 60 dialogue contexts from each dataset, with six responses per context for Topical-Chat and five for PERSONA-CHAT~zhang2018personalizing, including both model-generated and human responses. Each response was evaluated across six key dimensions: naturalness, coherence, engagement, groundedness, understandability, and overall quality. This study highlights the importance of multi-dimensional evaluation in dialogue generation, providing valuable insights into the strengths and weaknesses of different dialogue models. Additionally, the dataset from DSTC10 Track 5~yoshino2023overview,zhang2021automatic focuses on evaluating open-domain dialogue systems and is designed for automatic evaluation and moderation of dialogue systems. The challenge aims to develop automatic evaluation mechanisms that accurately reflect human judgments while effectively handling harmful user inputs, maintaining conversational flow and engagement. The dataset includes annotations across four aspects: coherence, appropriateness, naturalness, and toxicity control.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Automatic Story Generation",
      "level": "3",
      "content": "Automatic Story Generation (ASG) is a challenging task that aims to enable models to create coherent, engaging narratives based on a given prompt or context. It emulates human storytelling abilities by generating stories that exhibit a logical structure, compelling characters, and interesting plot developments. Evaluating story generation systems is inherently complex, as it involves assessing not only linguistic quality but also narrative elements like coherence, engagement, and surprise. The HANNA~chhun2022human dataset is tailored for evaluating automatic story generation (ASG), featuring 1,056 stories generated by 10 different systems from 96 prompts. Each story is annotated by three human reviewers across six criteria: relevance, coherence, resonance, surprise, engagement, and complexity. This comprehensive annotation framework provides a detailed assessment of narrative quality, making HANNA a valuable benchmark for comparing ASG models. Another notable dataset is the MANS~guan2021openmeva, which forms part of the OpenMEVA~guan2021openmeva framework. It compiles stories from various natural language generation models using well-known corpora like ROCStories~mostafazadeh2016corpus and WritingPrompts~fan2018hierarchical. MANS~guan2021openmeva focuses on manual annotations of narrative elements, serving as a robust testbed for exploring diverse evaluation metrics. The StoryER~chen2023storyer dataset offers a distinct approach to evaluating story generation by focusing on preference prediction and aspect-based rating. StoryER is divided into two primary components: the first is a 100k Story Ranking Data, which pairs stories from the WritingPrompts dataset. Each pair includes one story with high user engagement (upvotes $\\ge$ 50) and another with low engagement (upvotes $\\le$ 0). This component leverages real-world user feedback to capture implicit preferences, providing a practical basis for training models to predict story quality. The second component, Aspect Rating and Reasoning Data, contains 46,000 entries where annotators provide detailed ratings (on a scale of 1-5) for various story aspects such as introduction, character development, and plot description, along with explanatory comments. This combination of quantitative rankings and qualitative reasoning enables a nuanced evaluation of stories, making StoryER particularly useful for both automated scoring and interpretability research. The PERSER~wang2023learning dataset takes a different approach by addressing the subjectivity inherent in open-domain text generation evaluations. PERSER restructures existing datasets and introduces personalized tags, resulting in two sub-datasets: Per-MPST and Per-DOC. Per-MPST is an adapted version of the Movie Plot Synopsis Dataset, while Per-DOC includes 7,000 instances of paired stories generated from the same premise. These stories are evaluated based on dimensions such as interestingness, adaptability, surprise, character development, and the quality of the ending.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Values Alignment",
      "level": "3",
      "content": "Values alignment is a critical task in the development of AI systems, focused on ensuring that their behavior and decisions consistently reflect core human values and ethical standards. In the context of LLM-as-Judge, the alignment process is vital to verify that the model's outputs adhere to societal norms and ethical principles, minimizing risks related to harmful, biased, or unethical behavior. To support research and model development in values alignment, several datasets have been created, each with unique characteristics designed to evaluate or enhance the ethical behavior of LLMs. One notable dataset is PKU-SafeRLHF~ji2024pku, which was specifically curated for studying safe alignment in large language models. The dataset comprises 83.4K preference entries, focusing on two primary dimensions: harmlessness and usefulness. In each sample, the dataset presents a pair of model responses to a given prompt, annotated with safety meta-labels and preferences based on the levels of safety and utility. Another influential dataset is the HHH~askell2021general (Honesty, Helpfulness, and Harmlessness) dataset, designed to evaluate LLM performance across various human-model interaction scenarios. The dataset emphasizes three core human-centered values: honesty, helpfulness, and harmlessness. It includes a diverse collection of conversational examples where models are tested on their adherence to these values. By exposing models to a wide range of contexts, the HHH dataset serves as a comprehensive benchmark for assessing whether LLMs align with essential ethical standards and effectively mitigate risks of misinformation, harmful advice, or biased outputs. Moreover, the CVALUES~xu2023cvalues benchmark is a more recent contribution aimed at evaluating human values alignment specifically for Chinese LLMs. It represents the first comprehensive framework tailored to assess values alignment in the Chinese language context, focusing on two critical criteria: Safety and Responsibility.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Recommendation",
      "level": "3",
      "content": "Recommendation systems aim to provide personalized suggestions based on users' preferences and historical behavior. As the use of large language models (LLMs) expands, their role in evaluating the performance of recommendation systems has garnered increasing attention. LLMs can serve as versatile evaluators, offering insights into multiple aspects of recommendation systems beyond traditional metrics like accuracy. They can assess factors such as user engagement, satisfaction, and the quality of generated explanations. The MovieLens~harper2015movielens dataset is a widely-used public dataset for movie recommendations, available in multiple versions with varying scales, ranging from thousands of users and ratings to millions. Zhang et al.~zhang2024large further annotated the MovieLens~harper2015movielens data to create a sub-dataset featuring user self-explanation texts. In this sub-dataset, users write explanatory texts after being presented with a recommended movie. These explanations are then rated on a five-point Likert scale across four dimensions: Persuasiveness, Transparency, Accuracy, and Satisfaction. This annotated data provides valuable reference texts for LLMs in the context of explainability evaluation. Another commonly used dataset is the Yelp dataset~asghar2016yelp, which contains detailed review data from 11 metropolitan areas, covering approximately 150,000 businesses, nearly 7 million user reviews, and over 200,000 images. User reviews include ratings for businesses, such as hotel ratings (1-5 stars), as well as additional feedback like ``cool'' and ``funny'' votes. Furthermore, the Yelp dataset provides extensive business attribute information (e.g., operating hours, parking availability, and delivery options), offering rich contextual information that can be leveraged for developing and evaluating recommendation systems.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Search",
      "level": "3",
      "content": "The search task is a fundamental component of information retrieval (IR), focusing on identifying the most relevant documents from extensive text collections based on user queries. Traditionally, relevance assessments in search tasks have been conducted by human annotators following established guidelines. However, recent advances in large language models (LLMs) have opened up new opportunities for utilizing these models as evaluators, offering an automated and scalable approach to relevance assessment. With the advent of retrieval-augmented generation (RAG) models, the role of LLMs as evaluators has expanded. There is now a growing need to assess various dimensions of retrieved contexts, including context relevance, answer faithfulness, and answer relevance. This shift highlights the potential of LLMs to provide nuanced judgments that go beyond simple topical relevance. A key resource for evaluating the performance of LLMs as relevance assessors is the series of datasets from the Text Retrieval Conference (TREC). TREC workshops aim to advance research in IR by offering large-scale test collections, standardized evaluation procedures, and a platform for benchmarking retrieval models. The datasets from the TREC Deep Learning Track~lawrie2024overview, specifically from 2021 (DL21)~DBLP:conf/trec/Craswell0YCL21 and 2022 (DL22)~DBLP:conf/trec/Craswell0YCLVS22, are commonly used for this purpose. These datasets are derived from the expanded MS MARCO v2 collection~bajaj2016ms, which contains approximately 138 million passages. Relevance judgments are provided by assessors from the National Institute of Standards and Technology (NIST) using a 4-point scale (0 to 3). This structured and fine-grained annotation scheme allows for a detailed comparison between LLM-generated relevance scores and human judgments. While general-purpose datasets offer valuable benchmarks, specialized retrieval tasks often require domain-specific datasets that reflect unique relevance criteria. One notable example is LeCaRDv2~li2024lecardv2, a large-scale dataset tailored for legal case retrieval. LeCaRDv2 enriches the concept of relevance by incorporating three distinct aspects: characterization, penalty, and procedure. These additional criteria provide a more comprehensive perspective on relevance.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Comprehensive Data",
      "level": "3",
      "content": "To thoroughly assess the role of LLMs-as-Judges and better align them with human preferences, a diverse set of comprehensive datasets has been developed. These datasets provide large-scale, well-annotated data, allowing for the effective training and evaluation of LLMs in complex, real-world contexts. As a result, they contribute to improving the modelsâ reliability and effectiveness in their role as evaluators. Datasets such as HelpSteer~wang2023helpsteer and HelpSteer2~wang2024helpsteer2 are designed to improve the alignment and usefulness of LLMs. They provide multi-attribute data, enabling the training of models that can generate responses that are factually correct, coherent, and tailored to diverse user preferences. These open-source datasets support adjustments in response complexity and verbosity, catering to varying user needs. Additionally, UltraFeedback~cui2024ultrafeedback offers a large-scale dataset with around 64,000 prompts from sources like UltraChat~ding2023enhancing, ShareGPT~chiang2023vicuna, and TruthfulQA~lin2021truthfulqa. It includes multiple responses per prompt generated by different LLMs, with high-quality preference labels and textual feedback covering aspects like instruction-following, truthfulness, and helpfulness. UltraFeedbackâs fine-grained annotations and diverse prompts provide a robust resource for training reward and critic models, enhancing the evaluative capabilities of LLMs. In exploring instruction following and dialogue capabilities, specialized tools like AlpacaEval~dubois2024length, alongside interactive platforms such as Chatbot Arena~zheng2023judging and benchmarks like MT-Bench~zheng2023judging, provide critical insights. AlpacaEval is an automated evaluation tool using GPT-4 or Claude as evaluators. It assesses chat-based LLMs against the AlpacaFarm dataset, providing win-rate calculations across a variety of tasks, enabling rapid and cost-effective comparisons with baseline models like GPT-3.5 (Davinci-003). Chatbot Arena, on the other hand, offers a user-driven evaluation framework where participants interact with anonymous models and vote based on their preferences. The platform has collected over 1,000,000 user votes, using the Bradley-Terry model to rank LLMs and chatbots, providing valuable insights into user preferences and model performance in open-domain dialogue. Benchmarks like WildBench~lin2024wildbench and FLASK~ye2023flask aim to evaluate LLMs on tasks more reflective of real-world applications. WildBench~lin2024wildbench collects challenging examples from real users via the AI2 WildChat project, providing fine-grained annotations, task types, and checklists for response quality evaluation, and employs length-penalized Elo ratings to ensure unbiased assessments. FLASK~ye2023flask introduces a fine-grained evaluation protocol that decomposes overall scoring into skill set-level scoring for each instruction, enhancing interpretability and reliability in both human-based and model-based evaluations. Additionally, comprehensive evaluations covering multiple domainsâincluding factual question answering, reading comprehension, summarization, mathematical problem-solving, reasoning, poetry generation, and programmingâhave been conducted. These evaluations involve assessing models across multiple criteria such as correctness, fluency, informativeness, logicality, and harmlessness. Reward models and LLM-based judges face the crucial task of ensuring alignment with human expectations, a challenge addressed by datasets like RewardBench~lambert2024rewardbench, RM-Bench~liu2024rm, and JudgerBench~cao2024compassjudger. RewardBench~lambert2024rewardbench focuses on assessing models through complex prompt-choice trios, covering diverse areas like chat, reasoning, and safety, with a particular emphasis on out-of-distribution scenarios. RM-Bench~liu2024rm introduces a new benchmark for evaluating reward models based on their sensitivity to subtle content differences and resistance to stylistic biases, emphasizing the need for refined assessments that correlate highly with aligned language models' performance. JudgerBench~cao2024compassjudger, with its dual components (JDB-A and JDB-B), offers a structured framework for evaluating alignment and critique abilities. By including data from human voting results and combining insights from varied sources, JudgerBench~cao2024compassjudger provides a nuanced understanding of model performance across different languages and dialogue formats. With the growing complexity of tasks handled by LLMs, there is an increasing demand for more objective and reliable evaluation frameworks. JUDGEBENCH~tan2024judgebench proposes a novel approach to assessing LLM-based judges on challenging response pairs across domains like knowledge, reasoning, mathematics, and coding. It addresses the limitations of existing benchmarks by introducing preference labels that reflect objective correctness, providing a robust platform for evaluating the capabilities of advanced LLM-based judges. As LLMs evolve beyond text-only tasks, evaluation frameworks have expanded to encompass multimodal and multilingual contexts. MLLM-as-a-Judge~chen2024mllm serves as a benchmark for assessing Multimodal LLMs, covering tasks like image description, mathematical reasoning, and infographic interpretation. By integrating human annotations, it provides a comprehensive evaluation across visual and textual domains, reflecting the growing demand for models capable of processing diverse inputs. In a parallel effort, MM-Eval~son2024mm addresses the multilingual aspect, offering extensive analysis across 18 languages. With core subsets like Chat, Reasoning, and Linguistics, alongside a broader Language Resource subset spanning 122 languages, MM-Eval~son2024mm highlights performance discrepancies, especially in low-resource languages where models tend to default to neutral scores.",
      "origin_cites_number": 24
    },
    {
      "section_title": "Metric",
      "level": "2",
      "content": "The evaluation of LLMs-as-Judges models centers around assessing the extent to which the model's judgments align with human evaluations, which are typically considered the benchmark for quality. Given the complexity and subjectivity of many evaluation tasks, achieving high agreement with human ratings is a key indicator of the LLM's performance. To quantify this agreement, a range of statistical metrics is employed. Below, we outline these metrics and their applications in evaluating LLMs-as-Judges models.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Accuracy",
      "level": "3",
      "content": "Accuracy is a fundamental metric used to assess the proportion of correct judgments made by the LLM compared to human evaluations. In classification tasks, it is defined as: equation Accuracy = \\text{Number of Correct Predictions}{Total Number of Predictions}, equation where the number of correct predictions corresponds to instances where the LLM's judgment matches the human evaluator's judgment. While accuracy is simple to compute and intuitive, it may not fully capture the quality of the model, especially when dealing with tasks that involve nuanced or continuous evaluations.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Pearson Correlation Coefficient",
      "level": "3",
      "content": "The Pearson Correlation Coefficient~cohen2009pearson measures the linear relationship between two continuous variables, in this case, the evaluation scores assigned by the LLM and those assigned by human evaluators. It is defined as: equation r = \\sum (x_i - \\bar{x)(y_i - y)}{\\sum (x_i - \\bar{x)^2 \\sum (y_i - y)^2}}, equation where $x_i$ and $y_i$ are the scores from the LLM and the human, respectively, and $x$ and $y$ are their means. Pearson correlation values range from $-1$ to $1$.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Spearman's Rank Correlation Coefficient",
      "level": "3",
      "content": "Spearman's Rank Correlation Coefficient ($\\rho$) ~sedgwick2014spearman assesses the monotonic relationship between two variables by comparing their ranked values rather than the raw scores. It is defined as: equation \\rho = 1 - 6 \\sum d_i^2{n(n^2 - 1)}, equation where $d_i$ is the difference between the ranks of corresponding scores from the LLM and the human evaluator, and $n$ is the number of paired scores. Spearman's $\\rho$ is less sensitive to outliers and non-linear relationships compared to Pearson's correlation, making it a robust choice for evaluating tasks where the relative order of scores is more important than the exact values. It is commonly used in ranking-based evaluations such as preference judgments or ranking tasks.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Kendall's Tau",
      "level": "3",
      "content": "Kendall's Tau ($\\tau$) ~sen1968estimates is another rank-based correlation metric that measures the ordinal association between two ranked lists. It is defined as: equation \\tau = C - D{1{2}n(n-1)}, equation where $C$ is the number of concordant pairs (where the rank order agrees between the LLM and human), and $D$ is the number of discordant pairs. Kendall's $\\tau$ is particularly useful when evaluating the consistency of rankings produced by LLMs and human evaluators. It is often preferred when the dataset contains many ties, as it provides a more nuanced measure of agreement than Spearman's $\\rho$.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Cohen's Kappa",
      "level": "3",
      "content": "Cohen's Kappa ($\\kappa$) ~warrens2015five measures the level of agreement between two raters (in this case, the LLM and the human) beyond what would be expected by chance. It is defined as: equation \\kappa = p_o - p_e{1 - p_e}, equation where $p_o$ is the observed agreement and $p_e$ is the expected agreement by chance. Cohen's Kappa is particularly effective in classification tasks where both the LLM and the human evaluators assign categorical labels. It accounts for the possibility of random agreement, making it a more robust metric than simple accuracy.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Intraclass Correlation Coefficient (ICC)",
      "level": "3",
      "content": "The Intraclass Correlation Coefficient (ICC)~bartko1966intraclass assesses the reliability of ratings when there are multiple evaluators. It evaluates the consistency or conformity of measurements made by different raters, including LLMs and human annotators. ICC is defined based on the variance components derived from a one-way or two-way ANOVA model. The ICC is particularly useful when comparing multiple LLMs or when evaluating the consistency of an LLM across different subsets of data, providing a broader view of its reliability as an evaluator. table*[t] \\centering \\small Summary of Common Metrics for Evaluating LLMs-as-Judges Models tabular{l|l|l|l} \\hline Metric & Type & Use Case & Robustness to Outliers \\\\ \\hline \\hline Accuracy & Agreement measure & Proportion of correct judgments & Sensitive \\\\ \\hline Pearson & Linear correlation & Continuous score comparison & Sensitive \\\\ \\hline Spearman & Rank correlation & Rank-based evaluation tasks & Robust \\\\ \\hline Kendall's Tau & Rank correlation & Consistency in ordinal rankings & Robust, handles ties \\\\ \\hline Cohen's Kappa & Agreement measure & Two raters, consistency analysis & Adjusts for chance \\\\ \\hline ICC & Agreement measure & Multiple raters, consistency analysis & Robust for group ratings \\\\ \\hline tabular table*",
      "origin_cites_number": 1
    },
    {
      "section_title": "Limitation",
      "level": "1",
      "content": "Although the application of LLMs-as-judges holds great promise, there are still several significant limitations that can affect their effectiveness, reliability, and fairness~thakur2024judging,stureborg2024large. These limitations arise from the inherent characteristics of LLMs, including their reliance on large-scale data for training and token-based decoding mechanisms. In this section, we will primarily explore the limitations in the following three key aspets: Biases (\\Ssec:biases), Adversarial Attacks (\\Ssec:attacks), and Inherent Weaknesses (\\Ssec:weaknesses). table*[t] Overview of different biases. tabular{ll} \\hline Bias & Description \\\\ \\hline \\hline 2{l}{Presentation-Related Biases (\\Ssec:presentationbiases)} \\\\ \\hline mygray Position Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency to make judgments based on the position of the input, where responses earlier or later in the sequence are favored over those in other positions.tabular \\\\ Verbosity Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency to favor longer responses, potentially equating length with quality, regardless of the contentâs actual value.tabular \\\\ \\hline \\hline 2{l}{Social-Related Biases (\\Ssec:socialbiases)} \\\\ \\hline mygray Authority Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency to be swayed by references to authoritative sources, such as books, websites, or famous individuals.tabular \\\\ Bandwagon-Effect Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency to align with majority opinions, where LLMs-as-judges favor prevailing views over objectively assessing the content.tabular \\\\ mygray Compassion-Fade Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency to be influenced by anonymization strategies, such as the removal of model names, affecting the judgments of LLMs.tabular \\\\ Diversity Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency to shift judgments based on identity-related markers, such as gender, ethnicity, or other social categorizations.tabular \\\\ \\hline \\hline 2{l}{Content-Related Biases (\\Ssec:contentbiases)} \\\\ \\hline mygray Sentiment Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency to favor responses with specific emotional tones, such as cheerful or neutral, over negative or fearful ones.tabular \\\\ Token Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency of LLMs to favor more frequent or prominent tokens during pre-training, leading to skewed judgments.tabular \\\\ mygray Context Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency of LLMs to produce biased judgments influenced by contextual examples or cultural contexts, potentially leading to biased or culturally insensitive outcomes.tabular \\\\ \\hline \\hline 2{l}{Cognitive-Related Biases (\\Ssec:cognitivebiases)} \\\\ \\hline mygray Overconfidence Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency to exhibit inflated confidence in evaluation judgments, leading to overly assertive but potentially incorrect conclusionstabular \\\\ Self-Enhancement Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency to favor outputs generated by the same model acting as a judge, undermining objectivity.tabular \\\\ mygray Refinement-Aware Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency for scoring variations influenced by whether an answer is original, refined, or accompanied by conversation history during evaluation.tabular \\\\ Distraction Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency to be influenced by irrelevant content, which can detract from the quality of judgments by diverting attention from critical elements.tabular \\\\ mygray Fallacy-Oversight Bias & tabular[c]{@{\\ }p{0.7\\textwidth}@{\\ }}A tendency to overlook logical fallacies, which can undermine the accuracy of judgments.tabular \\\\ \\hline tabular table*",
      "origin_cites_number": 1
    },
    {
      "section_title": "Biases",
      "level": "2",
      "content": "Essentially, LLMs are trained on vast amounts of data gathered from diverse sources. While this allows them to generate human-like responses, it also makes them inherit to the biases inherent in the training data. These biases are presented in various forms, which can significantly affect evaluation results, compromising the fairness and accuracy of decisions. To gain a deeper understanding of the impact of bias, we have provided a detailed classification of bias. As shown in Table~tab:biases, the biases exhibited by LLMs-as-judges can be systematically categorized into four groups based on their underlying causes and manifestations: Presentation-Related Biases (\\Ssec:presentationbiases), Social-Related Biases (\\Ssec:socialbiases), Content-Related Biases (\\Ssec:contentbiases), and Cognitive-Related Biases (\\Ssec:cognitivebiases). In this section, we provide a detailed overview of the definition, impact, and solutions to these biases.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Presentation-Related Biases",
      "level": "3",
      "content": "Presentation-Related Biases refer to tendencies in LLMs where judgments are influenced more by the structure or presentation of information than by its substantive content. For example, models may prioritize certain formats, styles, or patterns of expression, which can affect the quality of the input. Next, we introduce two biases related to Presentation-Related Biases: position bias and verbosity bias. \\noindentPosition bias is a prevalent issue not only in the context of LLMs-as-judges but also in human decision-making and across various machine learning domains. Research has shown that humans are often influenced by the order of options presented to them, leading to biased decision that can impact fairness and objectivity~shi2024judging,blunch1984position,raghubir2006center,zhao2024measuring. Similarly, in other ML applications, models trained on ordered data exhibit a positional preference, skewing outcomes based on the sequence of input~ko2020look,wang2018position. Position bias in LLMs-as-judges refers to the tendency of LLMs to favor certain answers based on their position in the response set. For example, when presented with multiple answer choices or compared pairwise, LLMs disproportionately select options that appear earlier in the list, leading to skewed judgment. Recent studies have further examined position bias in the LLMs-as-judges context. For instance, a framework~llmsjuding2024openreview is proposed to investigate position bias in pairwise comparisons, introducing metrics such as repetition stability, position consistency, and preference fairness to better understand how positions affect LLM judgments. Another study~zheng2023judging explores the limitations of LLMs-as-judges, including position biases, and verifies agreement between LLM judgments and human preferences across multiple benchmarks. These findings underscore the need for robust debiasing strategies to enhance the fairness and reliableness of LLMs-as-judges. Several methods are proposed to mitigate position bias. The naive approach involves excluding inconsistent judgments by swapping the positions of the candidate answers and verifying whether the LLM's judgment remains consistent. Inconsistent judgments are then filtered out~zheng2023judging,chen2024humans,wang2023large,li2023generative,zheng2023large,li2024calibraeval. The swap-based debiasing method can be further divided into two categories: score-based and comparison-based. Both approaches start by swapping the positions of the candidate answers. The difference lies in how the final judgment is determined. In the score-based method, each candidate answer is scored, and the average score across multiple swaps is taken as the final score for that answer~zheng2023judging,wang2023large,li2023generative,raina2024llm,hou2024large.In contrast, the comparison-based method considers the outcome a tie if the LLM's judgments are inconsistent after swapping. The conclusion of a tie is based on an analysis of the quality gap between answers. The larger the quality gap between candidate answers, the smaller the impact of position bias, resulting in higher consistency in predictions after swapping their positions, which is detailed in a recently study~llmsjuding2024openreview. In addition to the aforementioned methods, PORTIA~li2023split employs an alignment-based approach that simulates human comparison strategies. It divides each answer into multiple segments, aligns similar content across candidate answers, and then merges these aligned segments into a single prompt for the LLM to evaluate. By presenting content in a balanced and aligned format, PORTIA enables the model to make more consistent and unbiased judgments, focusing on answer quality rather than order. This approach is effective across various LLMs, significantly improving evaluation consistency and reducing costs. Further efforts to enhance LLM-based evaluations have explored new techniques to address position bias and other judgment inconsistencies. Discussion-based methods~li2023prd,khan2024debating incorporate peer ranking and discussion to improve evaluation accuracy. Instead of relying solely on a single LLMâs judgment, these methods prompt multiple LLMs to compare answers and discuss preferences to reach a consensus, thereby reducing individual positional bias and enhancing alignment with human judgments. This collaborative evaluation approach represents a promising direction for mitigating biases inherent in LLM assessments. \\noindentVerbosity bias~khan2024debating,chen2024humans,zheng2023judging,nasrabadi2024juree refers to the tendency of a judge, whether human or model-based, to favor lengthier responses over shorter ones, irrespective of the actual content quality or relevance. This bias may cause LLMs prefer longer responses, even if the extended content does not contribute substantively to the correctness of the judgments. To mitigate verbosity bias in LLMs-as-judges, several approaches~khan2024debating,ye2024justice,ye2024beyond have been proposed. One approach~khan2024debating employs persuasive debating techniques, structuring responses to prioritize substance. By training LLMs-as-judges to engage in a debate-like format, this method encourages clarity and relevance, reducing the tendency to favor verbose arguments that lack substantive content. Additionally, the CALM~ye2024justice framework introduces controlled modifications to systematically assess and quantify verbosityâs impact on judgments, using automated perturbations to evaluate robustness against verbosity bias and refine LLMs-as-judges toward objective and concise assessments. Complementing these methods, the contrastive judgments (Con-J)~ye2024beyond approach trains models with structured rationale pairs instead of scalar scores, encouraging LLMs-as-judges to focus on well-reasoned content rather than associating verbosity with quality.",
      "origin_cites_number": 14
    },
    {
      "section_title": "Social-Related Biases",
      "level": "3",
      "content": "Social-Related Biases refer to biases in language models that resemble social phenomena~zhao2023mind. These biases may manifest when models are swayed by references to authoritative sources (Authority Bias), align with prevailing majority opinions without independent evaluation (Bandwagon-Effect Bias), or adjust their judgments based on anonymization strategies or identity markers such as gender and ethnicity (Compassion-Fade Bias and Diversity Bias). Next, we present the details of these biases. \\noindentAuthority bias~chen2024humans,ye2024justice in the context of LLMs-as-judges refers to the tendency of the model to attribute greater credibility to statements associated with authoritative references, regardless of the actual evidence supporting them. For instance, LLMs-as-judges may favor responses that include references to well-known sources or experts, even when the content is inaccurate or irrelevant. This bias highlights a critical vulnerability where the appearance of authority can unduly influence judgment outcomes. While specific solutions to mitigate authority bias in LLMs-as-judges are still under active exploration, potential approaches include using retrieval-augmented generation (RAG) techniques to verify the validity of authoritative claims against external knowledge bases. This approach allows the model to cross-check referenced information and ensure its alignment with factual evidence. Another possible strategy is to design prompts that explicitly emphasize semantic accuracy and relevance over perceived authority. Further research is needed to validate these approaches and develop robust methods for addressing authority bias effectively in evaluative contexts. \\noindentBandwagon-effect bias~koo2023benchmarking,ye2024justice in the context of LLMs-as-judges refers to the tendency of the model to align its judgments with the majority opinion or prevailing trends, regardless of the actual quality or correctness of the evaluated content. For instance, when multiple responses are presented with indications of popular support or consensus, LLMs-as-judges may disproportionately favor these responses over alternatives, even when the consensus is flawed or biased. This bias reflects a susceptibility to groupthink dynamics, undermining the objectivity and fairness of the judgment process. Solutions to address bandwagon-effect bias include designing evaluation prompts that anonymize information about majority opinions, ensuring that judgments are based solely on the intrinsic quality of the responses rather than external indicators of popularity. Further exploration of debiasing strategies tailored to specific evaluative contexts is necessary to mitigate the impact of bandwagon-effect bias effectively. \\noindentCompassion-fade bias~koo2023benchmarking,ye2024justice occurs when the anonymity of model names or the absence of identifiable contextual cues affects the judgments made by LLMs-as-judges. For example, anonymizing model names or using neutral identifiers may lead to shifts in evaluation outcomes. This bias highlights how the lack of personalized or contextual information can diminish the modelâs sensitivity to equitable considerations. To mitigate compassion-fade bias, it is important to design evaluation prompts that standardize judgment criteria, ensuring that assessments remain consistent regardless of whether identifying details are present. Additionally, fairness-driven frameworks that explicitly address anonymization effects can further enhance the reliability of LLMs-as-judges. \\noindentDiversity bias~chen2024humans,ye2024justice in the context of LLMs-as-judges refers to the modelâs tendency to exhibit judgment shifts based on identity-related markers, such as gender, ethnicity, religion, or other social categorizations. For example, LLMs-as-judges might favor responses associated with certain demographic groups over others, leading to unfair or skewed judgments. This bias reflects the modelâs susceptibility to implicit stereotypes or unequal treatment of diverse identities present in the training data. Continued efforts to address this bias are crucial for ensuring fairness and inclusivity in the judgments conducted by LLMs-as-judges.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Content-Related Biases",
      "level": "3",
      "content": "Content-Related Biases involve preferences or skewed judgments based on the contentâs characteristics. An LLM might favor responses with certain emotional tones (Sentiment Bias), prefer frequently occurring words from its training data (Token Bias), or be influenced by specific cultural or domain contexts leading to insensitive outcomes (Context Bias). We present the details of these biases in the following. \\noindentSentiment bias~ye2024justice in the context of LLMs-as-judges refers to the tendency of the model to favor responses that exhibit certain emotional tones, such as positive or neutral sentiments, over others, regardless of their actual content quality or relevance. For instance, LLMs-as-judges may disproportionately reward responses that are cheerful or optimistic while penalizing those that are negative or emotionally intense, even if the latter are more contextually appropriate or accurate. To address sentiment bias, potential solution is the use of sentiment-neutralizing mechanisms, such as filtering or adjusting responses to remove sentiment-driven influences during evaluation. \\noindentToken Bias~jiang2024peek,li2024calibraeval,pezeshkpour2023large,raina2024llm refers to that LLMs favor certain tokens during the evaluation process. This bias often arises from the model's pre-training data, where more frequently occurring tokens are prioritized over less common ones, regardless of the contextual appropriateness or correctness in judgment. \\noindentContextual Bias refers to the tendency of LLMs to produce skewed or biased judgments based on the specific context in which they are applied. For instance, models used in healthcare may propagate biases found in medical datasets, potentially influencing diagnoses or treatment recommendations~poulain2024bias, while in finance, they might reflect biases in credit scoring or loan approval processes~zhou2024large. In addition, the selection of contextual examples may also introduce bias~zhou2023batch,fei2023mitigating,zhao2021calibrate,han2022prototypical.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Cognitive-Related Biases",
      "level": "3",
      "content": "Cognitive-Related Biases pertain to the inherent cognitive tendencies of LLMs in processing information. This includes exhibiting unwarranted confidence in judgments (Overconfidence Bias), favoring outputs generated by themselves (Self-Enhancement Bias), varying scores based on whether an answer is original or refined (Refinement-Aware Bias), being distracted by irrelevant information (Distraction Bias), or overlooking logical fallacies (Fallacy-Oversight Bias). The details of these biases are presented in the following. \\noindentOverconfidence bias~khan2024debating,jung2024trust in the context of LLMs-as-judges refers to the tendency of models to exhibit an inflated level of confidence in their judgments, often resulting in overly assertive evaluations that may not accurately reflect the true reliability of the answer. This bias is particularly concerning in evaluative contexts, as it can lead LLMs-as-judges to overstate the correctness of certain outputs, compromising the objectivity and dependability of assessments. To address Overconfidence bias, researchers have proposed several methods. Cascaded Selective Evaluation~jung2024trust addresses overconfidence by using Simulated Annotators to estimate confidence. This involves simulating diverse annotator preferences through in-context learning, which provides a more realistic measure of the likelihood that a human would agree with the LLMâs judgment. By analyzing multiple simulated responses, this method offers a confidence metric that reflects human-like disagreement, which helps to avoid overconfidence bias. Another method uses an adversarial debate mechanism, where two LLMs argue for different outcomes. Through structured debate rounds, each model is required to substantiate its position, which can reveal overconfidence by prompting self-reflection and critical analysis. This approach has been shown to improve truthfulness and reduces overconfidence by fostering a balanced evaluation, aligning LLMs' judgments more closely with accurate and reasoned conclusions. \\noindentSelf-enhancement bias is the tendency to favor their own outputs~liu2023g,zheng2023judging,li2023prd,liu2023g,panickssery2024llm. This concept of self-enhancement is drawn from social psychology, as discussed in Brownâs work in social cognition literature~brown1986evaluations. In the context of LLMs-as-judges, this bias manifests when a LLM evaluates its own generated outputs more favorably than those of other LLMs. Such bias is particularly concerning in applications involving self-assessment or feedback generation, as it compromises the objectivity of the LLMs-as-judges. To address self-enhancement bias, PRD~li2023prd introduces Peer Rank (PR) and Peer Discussion (PD) mechanisms. PR mitigates bias by using multiple LLMs as reviewers, each assessing pairwise comparisons between responses from different LLMs. By aggregating evaluations from several peer LLMs and weighting their preferences based on consistency with human judgments, PR reduces the impact of any single LLMâs self-enhancement bias, as more reliable reviewers have a greater influence. PD further alleviates self-enhancement bias by enabling two LLMs to engage in a dialogue to reach a mutual agreement on their preference between two answers. This multi-turn discussion encourages models to re-evaluate their initial judgments and consider alternative perspectives, focusing on content quality rather than self-generated responses. By promoting collaborative assessment and accountability, PR and PD effectively mitigate self-enhancement bias, aligning evaluations more closely with human standards. Recently, an automated bias quantification framework named CALM~ye2024justice has been proposed to systematically evaluate biases in LLMs-as-judges. CALMâs findings suggest that one effective way to reduce self-enhancement bias is to avoid using the same model to both generate and judge answers, thereby ensuring that evaluation remains more impartial. Moreover, the Reference-Guided Verdict method~badshah2024reference further addresses self-enhancement bias by providing a definitive gold-standard answer as a reference for LLM judges. This reference anchor helps align judgments to objective criteria, even when an LLM evaluates its own output, thus reducing the tendency to favor self-generated answers. Through structured prompts, this method has been shown to enhance reliability and mitigate variability in judgments, especially when multiple LLMs are used collectively. The integration of multiple LLMs, trained on varied datasets or fine-tuned with different parameters, has proven instrumental in producing less biased, more balanced evaluations, highlighting the effectiveness of model diversity and reference-guided criteria in combating self-enhancement bias. \\noindentRefinement-aware bias~ye2024justice in the context of LLMs-as-judges refers to the tendency of the model to evaluate responses differently based on whether they are original, refined, or include revision history. For instance, an answer that has been iteratively refined may be judged more favorably than an original response, even if the refinement process does not significantly improve the content quality. Similarly, responses that explicitly present their improvement process or revision rationale might receive undue preference, skewing the evaluation outcomes. While research on refinement-aware bias in the specific context of LLMs-as-judges remains limited, solution~xu2024pride developed for general LLMs offer valuable insights. One potential solution involves incorporating external feedback mechanisms during judgment, as it introduces an objective and independent judgment mechanism that is not influenced by the LLMâs internal iterations or self-perception. \\noindentDistraction bias in the context of LLMs-as-judges refers to the modelâs tendency to be influenced by irrelevant or unimportant details when making judgments. For instance, introducing unrelated information, such as a meaningless statement like âSystem Star likes to eat oranges and apples,â~ye2024justice,koo2023benchmarking,shi2023large can significantly alter the modelâs evaluation outcomes. This bias highlights the vulnerability of LLMs to attentional diversion caused by inconsequential content. While existing studies~ye2024justice,koo2023benchmarking have analyzed and discussed distraction bias, effective strategies to mitigate this issue in LLMs-as-judges remain underexplored. Potential solutions could involve input sanitization to preprocess and remove irrelevant information before presenting it to the model, ensuring that evaluations focus solely on relevant content. Additionally, explicit prompting with clear and strict guidelines could be designed to direct the LLM to evaluate only task-related aspects, reducing its susceptibility to distractions. Further research is needed to develop and validate robust methods that can systematically address distraction bias in the context of LLMs-as-judges. \\noindentFallacy-oversight bias~chen2024humans,ye2024justice refers to the tendency of LLMs-as-judges to overlook logical fallacies or inconsistencies within the evaluated responses. For instance, when presented with arguments or answers containing reasoning errorsâsuch as circular reasoning, false dilemmas, or strawman argumentsâLLMs-as-judges may fail to identify these issues and treat the responses as valid, potentially compromising the integrity of their evaluations. In summary, while LLMs-as-judges have garnered significant attention for their effectiveness in diverse scenarios, the exploration of various biases that impact their performance remains relatively underdeveloped. These biases pose significant challenges to ensuring fair, objective, and reliable judgments across tasks, particularly in various applications where the implications of biased judgments can be severe. Future research must focus on systematically identifying, quantifying, and addressing these biases within the LLMs-as-judges framework. Drawing from methodologies developed for general LLMs, such as external feedback mechanisms, balanced datasets, and fairness-aware prompting, could offer initial insights. However, domain-specific challenges require tailored solutions that align with the unique demands of LLMs-as-judges.",
      "origin_cites_number": 12
    },
    {
      "section_title": "Adversarial Attacks",
      "level": "2",
      "content": "Adversarial attacks involve carefully crafted inputs designed to deceive the model into producing incorrect or unintended outputs. For LLM judges, attackers may subtly modify the input content, alter the wording of questions, or introduce misleading context to influence the model's evaluation results. Researchers have found that for LLMs, even small, seemingly insignificant changes to the input data, such as adding or removing words, changing word order, or introducing ambiguous phrasing, can significantly affect the model's response~shen2023anything,jiang2023prompt,zou2023universal. Such attacks can lead to inaccurate ratings or assessments, particularly when evaluating complex or high-risk tasks. In this section, we first review research on adversarial attacks on LLMs within general domains. Then, we specifically focus on adversarial attacks in the context of LLMs-as-judges.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Adversarial Attacks on LLMs",
      "level": "3",
      "content": "Adversarial attacks on LLMs focus on exploiting vulnerabilities within the general framework of language model functionality. These attacks can be classified into three main categories based on the manipulation level: text-level manipulations, structural and semantic distortions, and optimization-based attacks. Text-Level Manipulations involve subtle changes to the input text to deceive the model. Character-level perturbations, such as introducing typos, swapping letters, or inserting unnecessary characters, can cause significant changes in predictions despite minimal visible alterations~ebrahimi2017hotflip,jiang2023prompt. Sentence-level modifications, such as rearranging phrases, adding irrelevant information, or paraphrasing inputs, further exploit the modelâs sensitivity to surface-level changes~branch2022evaluating,perez2022ignore. Structural and Semantic Distortions focus on the syntactic and semantic properties of the input. Syntactic attacks rewrite sentence structures while preserving semantic meaning, targeting the modelâs reliance on specific linguistic patterns~xu2023llm. Semantic preservation with perturbations modifies critical tokens identified through saliency analysis, ensuring the attack minimally affects meaning but significantly alters predictions. Optimization-Based Attacks leverage algorithmic techniques to craft adversarial inputs. Gradient-based methods utilize the modelâs gradients to identify and manipulate influential input features, causing substantial shifts in predictions~sun2020natural,sun2020adv. Population-based optimization techniques iteratively generate adversarial examples in black-box settings, exploiting the modelâs outputs to refine attacks~lee2022query. These attacks highlight the vulnerabilities in LLMs, demonstrating their susceptibility to subtle manipulations. Studying these adversarial attacks is essential, as it provides insights that can guide the development of robust defense mechanisms, ensuring that LLMs maintain reliability against such manipulations.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Adversarial Attacks on LLMs-as-judges",
      "level": "3",
      "content": "Recent studies have unveiled significant vulnerabilities in LLMs-as-judges to adversarial attacks~zheng2024cheating, doddapaneni2024finding, raina2024llm, shi2024optimization. Zheng et al.~zheng2024cheating and Doddapaneni et al.~doddapaneni2024finding demonstrated that automatic benchmarking systems like MT-Bench~zheng2023judging can be easily deceived to yield artificially high scores. These findings highlight that malicious inputs can manipulate evaluation metrics, undermining the reliability of such benchmarks. Building on this, Raina et al.~raina2024llm investigated the robustness of LLMs-as-judges against universal adversarial attacks. Their work showed that appending short, carefully crafted phrases to evaluated texts can effortlessly manipulate LLM scores, inflating them to their maximum regardless of the actual quality. Remarkably, these universal attack phrases are transferable across models; phrases optimized on smaller surrogate models (e.g., FlanT5-xl~chung2024scaling) can successfully deceive larger models like GPT-3.5 and Llama2~touvron2023llama. Furthermore, Shi et al.~shi2024optimization introduced JudgeDeceiver, an optimization-based prompt injection attack tailored for the LLMs-as-judges framework. Unlike handcrafted methods, JudgeDeceiver formulates a precise optimization objective to efficiently generate adversarial sequences. These sequences can mislead LLMs-as-judges into selecting biased or incorrect responses among candidate answers, thereby compromising the evaluation process. Although preliminary studies~shi2024optimization, raina2024llm have highlighted the vulnerability of LLMs-as-judges to adversarial manipulations, this field remains largely underexplored. It is imperative to advance our understanding of these weaknesses and devise effective defense strategies. As the use of LLMs-as-judges grows across diverse applications, future research should focus on uncovering new attack methods and strengthening the models against such adversarial threats.",
      "origin_cites_number": 9
    },
    {
      "section_title": "Inherent Weaknesses",
      "level": "2",
      "content": "Despite the remarkable capabilities of LLMs, they possess several inherent weaknesses that can compromise their reliability and robustness in LLMs-as-judges. This subsection discusses key limitations, including issues related to knowledge recency, hallucination, and other domain-specific knowledge gaps~zhao2023survey.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Knowledge Recency",
      "level": "3",
      "content": "One significant limitation of LLMs is their inability to access or incorporate up-to-date information reliably. LLMs are generally trained on static datasets that may become outdated over time, limiting their ability to evaluate scenarios that require knowledge of recent events, legislation, or rapidly evolving fields. The most straightforward solution is to retrain the model on new data; however, this approach is resource-intensive and risks catastrophic forgetting~luo2023empirical, where previously learned knowledge is overwritten during training. This temporal disconnect can lead to judgments based on invalid data, or obsolete practices, compromising their reliability in real-world, time-sensitive applications. Consider a case where LLMs-as-judges are used to evaluate which of two responses from LLMs better answers a prompt about the COVID-19 pandemic. Suppose one response references the WHO guidelines updated timely, while the other relies on outdated 2020 guidelines. If the LLM-as-Judge has not been updated with the latest guidelines, it might erroneously prefer the outdated response, incorrectly deeming it more accurate. This failure to account for recent developments highlights the importance of addressing knowledge recency in LLMs-as-judges. Addressing the issue of knowledge recency can involve integrating retrieval-augmented generation (RAG) methods~gao2023retrieval,lewis2020retrieval, which enable LLMs to query external, dynamically updated databases or knowledge sources during evaluation. Additionally, periodic fine-tuning with updated datasets or leveraging continual learning frameworks~wu2024continual can ensure that LLMs-as-judges remain aligned with the latest information. Combining these approaches with robust fact-checking mechanisms~dierickx2024striking can further enhance temporal reliability in judgment contexts.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Hallucination",
      "level": "3",
      "content": "Another critical issue in LLMs is the hallucination problem, where models generate incorrect or fabricated information with high confidence. In the context of LLMs-as-judges, hallucination can manifest as the invention of non-existent precedents, misinterpretation of facts, or fabrication of sources, which can severely undermine the reliability of their judgments. This issue is particularly concerning in various applications, where such errors can lead to unfair or harmful outcomes. Employing fact-checking mechanisms~dierickx2024striking,ji2023survey,tonmoy2024comprehensive during evaluation is crucial to mitigate hallucination. By cross-verifying the outputs of LLMs-as-judges with trusted databases and external knowledge sources, hallucinated information can be identified and corrected.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Domain-Specific Knowledge Gaps",
      "level": "3",
      "content": "While LLMs demonstrate broad generalization capabilities, they often lack the depth of understanding required for specialized domains~feng2023knowledge,pan2024unifying,szymanski2024limitations,dorner2024limitsscalableevaluationfrontier. For instance, legal judgments demand intricate knowledge of statutes, precedents, and contextual nuances, which may not be adequately captured in the training data of general-purpose LLMs. This limitation can lead to shallow or incorrect judgments in domain-specific contexts. Domain adaptation techniques, such as integrating LLMs with domain-specific knowledge graphs~feng2023knowledge,pan2024unifying or leveraging RAG systems~gao2023retrieval, can substantially improve their performance in specialized domains. Knowledge graphs provide structured, expert-curated information that enhances context-awareness, while RAG enables LLMs to dynamically retrieve relevant knowledge from specific domain. The inherent weaknesses of LLMs highlight the need for continued research and innovation. Addressing these limitations through RAG, enhanced training methods, and knowledge graph techniques is crucial for ensuring that LLMs-as-judges deliver reliable, accurate, and trustworthy evaluations in diverse applications.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Future Work",
      "level": "1",
      "content": "In this section, we will explore the key directions for future work, focusing on how to build more efficient, more effective, and more reliable LLM judges. These directions aim to address the bottlenecks and challenges in current technologies and practices, while also promoting broader applications and deeper integration of LLM judges in diverse scenarios.",
      "origin_cites_number": 0
    },
    {
      "section_title": "More Efficient LLMs-as-Judges",
      "level": "2",
      "content": "",
      "origin_cites_number": 0
    },
    {
      "section_title": "Automated Construction of Evaluation Criteria and Tasks",
      "level": "3",
      "content": "Current LLM judges often rely on manually predefined evaluation criteria, lacking the ability to adapt dynamically during the assessment process. Designing prompts for these systems is not only tedious and time-consuming but also struggles to address the diverse requirements of various task scenarios~bai2024benchmarking,zhao2024auto,yu2024kieval,zhang2024talec,wang2024revisiting. To overcome these limitations, future LLM judges could incorporate enhanced adaptability by tailoring evaluation criteria based on task types, target audiences, and domain-specific knowledge. Such advancements would significantly streamline the configuration process of LLM judges, while also greatly improving their practicality and efficiency in real-world applications. Moreover, existing static evaluation datasets are prone to issues such as training data contamination, which can compromise their effectiveness in accurately assessing the evolving capabilities of LLMs. To address this, future LLM judges could focus on dynamically constructing more suitable evaluation tasks and continuously optimizing the evaluation process, thereby enhancing applicability and precision~bai2024benchmarking,zhao2024auto.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Scalable Evaluation Systems",
      "level": "3",
      "content": "Existing LLM judges often exhibit limited adaptability in practical applications. While these judges may perform effectively on specific downstream tasks, they frequently struggle in cross-domain or multi-task settings, thereby falling short of meeting the diverse and broader demands of real-world applications. To address these limitations, future research could focus on modular design principles to create scalable evaluation frameworks~xu2024perfect. Such frameworks would allow users to flexibly add or customize evaluation modules to suit their specific needs. This modular approach not only enhances the usability and flexibility of the system but also significantly reduces the cost and complexity of transferring the framework across different domains.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Accelerating Evaluation Processes",
      "level": "3",
      "content": "Existing LLM systems often face significant computational costs when performing evaluation tasks. For example, pairwise comparison methods require multiple rounds of comparisons for each candidate, which becomes extremely time-consuming as the number of candidates grows. In resource-constrained environments, such high-cost evaluation methods are challenging to deploy effectively. To address this issue, future research could focus on developing more efficient candidate selection algorithms, thereby unlocking new opportunities for the use of LLMs in low-resource settings~lee2024aligning,liu2024aligning. Similarly, the multi-LLM evaluation paradigm, which relies on multiple rounds of interaction, further exacerbates computational demands. To mitigate these challenges, future efforts could explore streamlined communication frameworks that support high-quality evaluation tasks while minimizing resource requirements~chen2024internet. Advances in these areas could lead to the development of more efficient and scalable evaluation systems, making LLM-based evaluations more practical across diverse and resource-limited scenarios.",
      "origin_cites_number": 2
    },
    {
      "section_title": "More Effective LLMs-as-Judges",
      "level": "2",
      "content": "",
      "origin_cites_number": 0
    },
    {
      "section_title": "Integration of Reasoning and Judge Capabilities",
      "level": "3",
      "content": "Current LLMs-as-judges systems often treat reasoning and evaluation capabilities as distinct and independent modules, which can hinder effectiveness when addressing complex tasks. As the demand for evaluating increasingly complex systems grows, future LLM-as-Judge systems should prioritize the deep integration of reasoning and evaluation capabilities to achieve a seamless synergy~zhuo2023ice,yi2024protocollm,stephan2024calculation. For instance, in legal scenarios, the model could first infer the relevant legal provisions and then assess the case's relevance, making the evaluation process more effective.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Establishing a Collective Judgment Mechanism",
      "level": "3",
      "content": "Current LLMs-as-Judge systems typically rely on a single model for evaluation. While this approach is straightforward, it is prone to biases inherent in individual models, leading to reduced accuracy and stabilitys. Moreover, a single model often struggles to comprehensively address the diverse requirements of such tasks. Future research could investigate collaborative multi-agent mechanisms to enable ``collective judgment'' where multiple LLMs work together, leveraging their respective strengths in reasoning and knowledge~chan2023chateval,chu2024pre,. Additionally, ensemble techniques could be employed to dynamically balance the contributions of different models, leading to more stable and reliable judgment outcomes.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Enhancing Domain Knowledge",
      "level": "3",
      "content": "Current LLMs-as-Judge systems often fall short when handling tasks in specialized fields due to insufficient domain knowledge. Furthermore, as domain knowledge continues to evolve, these models struggle to keep up with the latest developments, further limiting their effectiveness and applicability in real-world scenarios. To address these challenges, future LLMs-as-judges systems should focus on integrating comprehensive domain knowledge to enhance their performance in specialized tasks~raju2024constructing. This can be achieved by utilizing knowledge graphs, embedding domain-specific expertise, and fine-tuning models based on feedback from subject-matter experts. In addition, these systems should incorporate dynamic knowledge updating capabilities. For instance, in the legal domain, models could regularly acquire and integrate updates on new statutes, case law, and policy changes, ensuring that their judgments remain current and aligned with the latest legal standards.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Cross-Domain and Cross-Language Transferability",
      "level": "3",
      "content": "Current LLMs-as-Judge systems are often confined to specific domains or languages, making it challenging for them to transfer across different fields. For instance, an LLM proficient in processing legal texts may struggle to effectively handle evaluation tasks in the medical or financial domains. This limitation greatly restricts the applicability of such systems. Future research can focus on exploring cross-domain and cross-language transfer learning techniques to enhance the adaptability of LLMs in diverse fields. By leveraging shared general knowledge across fields, models can quickly adapt to new tasks with minimal additional training costs~son2024mm,hada2023large,watts2024pariksha. For example, evaluation capabilities developed in English could be transferred to contexts in German, thereby improving the evaluation performance in these new areas.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Multimodal Integration Evaluation",
      "level": "3",
      "content": "Current LLM-as-Judge systems primarily focus on processing textual data, with limited attention to integrating other modalities like images, audio, and video. This single-modal approach falls short in complex scenarios requiring multimodal analysis, such as combining visual and textual information in medical assessments. Future systems should develop cross-modal integration capabilities to process and evaluate multimodal data simultaneously~chen2024mllm. Leveraging cross-modal validation can enhance evaluation accuracy. Key research areas include efficient multimodal feature extraction, integration, and the design of unified frameworks for more comprehensive and precise evaluations.",
      "origin_cites_number": 1
    },
    {
      "section_title": "More Reliable LLMs-as-Judges",
      "level": "2",
      "content": "",
      "origin_cites_number": 0
    },
    {
      "section_title": "Enhancing Interpretability and Transparency",
      "level": "3",
      "content": "Current LLM-as-Judge systems often operate as black boxes, with their rulings lacking transparency and a clear reasoning process. This opacity is particularly concerning in high-stakes domains such as legal judgments, where users cannot fully understand the basis of the model's decisions or trust its outputs. Future research should focus on improving the interpretability of LLMs~liu2024hd. For example, the LLM judges should not only provide evaluation results but also present a clear explanation. Research could explore designing validation models based on logical frameworks to make the decision-making process more transparent.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Mitigating Bias and Ensuring Fairness",
      "level": "3",
      "content": "LLMs may be influenced by biases present in their training data, leading to unfair judgments in different social, cultural, or legal contexts. These biases could be amplified by the model and compromise the fairness of its decisions. Future research could focus on ensuring fairness in model outputs through debiasing algorithms and fairness constraints~li2024calibraeval. Targeted approaches, such as adversarial debiasing training or bias detection tools, can dynamically identify and mitigate potential biases during the model's reasoning process.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Enhancing Robustness",
      "level": "3",
      "content": "LLMs are sensitive to noise, incompleteness, or ambiguity in input instructions, which may lead to errors or instability in evaluation results when handling complex or highly uncertain texts. This lack of robustness significantly limits their reliability in practical applications. Future research can adopt several methods to enable LLMs robust and reliable performance in real-world environments~shi2024optimization,elangovan2024beyond. For instance, introducing more advanced data augmentation techniques to generate diverse and uncertain simulated cases can help train models to adapt to various complex input conditions.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Conclusion",
      "level": "1",
      "content": "This survey systematically examined the LLMs-as-Judges framework across five dimensions: functionality, methodology, applications, meta-evaluation, and limitations, providing a comprehensive understanding of its advantages, limitations, practical implementations, applications, and methods for evaluating its effectiveness. To advance research in this field, we also outlined several promising directions for future exploration, including the development of more efficient, effective, and reliable LLM judges. We hope to promote the ongoing development of this field by providing foundational resources and will continue to update relevant content. \\newpage ACM-Reference-Format",
      "origin_cites_number": 0
    }
  ],
  "literature_review_id": 274596907,
  "meta_info": {
    "cite_counts": 306,
    "Conference_journal_name": "ArXiv",
    "influentialcitationcount": 13,
    "Author_info": {
      "Publicationsh": 44,
      "h_index": 15,
      "Citations": 787,
      "Highly Influential Citations": 0
    },
    "all_cites_title": [
      "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "The fellowship of the llms: Multi-agent workflows for synthetic preference optimization dataset generation",
      "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
      "Yelp dataset challenge: Review rating prediction",
      "Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences",
      "A general language assistant as a laboratory for alignment",
      "GPT classifications, with application to credit lending",
      "Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text",
      "Qwen technical report",
      "Benchmarking foundation models with language-model-as-an-examiner",
      "Ms marco: A human generated machine reading comprehension dataset",
      "Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates",
      "The intraclass correlation coefficient as a measure of reliability",
      "Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks",
      "Graph of thoughts: Solving elaborate problems with large language models",
      "Position bias in multiple-choice questions",
      "Comparing Two Model Designs for Clinical Note Generation",
      "Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples",
      "Evaluations of self and others: Self-enhancement biases in social judgments",
      "CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution",
      "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic",
      "Chateval: Towards better llm-based evaluators through multi-agent debate",
      "A survey on evaluation of large language models",
      "Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark",
      "Humans or llms as the judge? a study on judgement biases",
      "StoryER: Automatic story evaluation via ranking, rating and reasoning",
      "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
      "Adaptation with self-evaluation to improve selective prediction in llms",
      "Automated evaluation of large vision-language models on self-driving corner cases",
      "Evaluating large language models trained on code",
      "Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence",
      "Teaching large language models to self-debug",
      "LLMs are Biased Evaluators But Not Biased for Retrieval Augmented Generation",
      "Of human criteria and automatic metrics: A benchmark of the evaluation of story generation",
      "Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course",
      "A closer look into automatic evaluation using large language models",
      "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation",
      "Pre: A peer review based large language model evaluator",
      "Scaling instruction-finetuned language models",
      "Pearson correlation coefficient. Noise reduction in speech processing",
      "Overview of the TREC 2021 Deep Learning Track",
      "Overview of the TREC 2022 Deep Learning Track",
      "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback",
      "Aligning Model Evaluations with Human Preferences: Mitigating Token Count Bias in Language Model Assessments",
      "Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach",
      "PHUDGE: Phi-3 as Scalable Judge",
      "Striking the balance in using LLMs for fact-checking: A narrative literature review",
      "Enhancing chat language models by scaling high-quality instructional conversations",
      "Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion",
      "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
      "Self-Boosting Large Language Models with Synthetic Preference Data",
      "A survey on in-context learning",
      "Can LLM be a Personalized Judge? arXiv preprint",
      "Limits to scalable evaluation at the frontier: LLM as Judge won't beat twice the data",
      "Length-controlled alpacaeval: A simple way to debias automatic evaluators",
      "Hotflip: White-box adversarial examples for text classification",
      "Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-ajudge",
      "Summeval: Re-evaluating summarization evaluation",
      "Hierarchical neural story generation",
      "Biasalert: A plug-and-play tool for social bias detection in llms",
      "Mitigating label biases for in-context learning",
      "Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs",
      "Improving llm-based machine translation with systematic self-correction",
      "Experts, errors, and context: A large-scale study of human evaluation for machine translation",
      "Alon Lavie, and OndÅej Bojar. 2021. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain",
      "The Turing Test: the first 50 years",
      "Gptscore: Evaluate as you desire",
      "Retrieval-augmented generation for large language models: A survey",
      "Bayesian Calibration of Win Rate Estimation with LLM Evaluators",
      "ChatGPT outperforms crowd workers for text-annotation tasks",
      "Topical-chat: Towards knowledge-grounded open-domain conversations",
      "OpenMEVA: A benchmark for evaluating open-ended story generation metrics",
      "Direct language model alignment from online ai feedback",
      "Evaluating large language models: A comprehensive survey",
      "Unveiling Context-Aware Criteria in Self-Assessing LLMs",
      "Are large language model-based evaluators the solution to scaling up multilingual evaluation",
      "Prototypical calibration for few-shot learning of language models",
      "Fullanno: A data engine for enhancing image comprehension of mllms",
      "The movielens datasets: History and context",
      "ALLURE: auditing and improving llm-based evaluation of text using iterative in-context-learning",
      "Socreval: Large language models with the socratic method for reference-free reasoning evaluation",
      "Annollm: Making large language models to be better crowdsourced annotators",
      "Lixin Fan, and Qiang Yang. 2024. FedEval-LLM: Federated Evaluation of Large Language Models on Downstream Tasks with Collective Wisdom",
      "If in a Crowdsourced Data Annotation Pipeline, a GPT-4",
      "Using Large Language Models to Evaluate Biomedical Query-Focused Summarisation",
      "Large language models are zero-shot rankers for recommender systems",
      "Themis: A reference-free nlg evaluation language model with flexibility and interpretability",
      "Rethinking llm-based preference evaluation",
      "Language Model Preference Evaluation with Multiple Weak Evaluators",
      "An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers",
      "On the limitations of fine-tuned judge models for llm evaluation",
      "Large language models cannot self-correct reasoning yet",
      "Multi-dimensional evaluation of text summarization with in-context learning",
      "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
      "PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference",
      "Survey of hallucination in natural language generation",
      "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners",
      "Tigerscore: Towards building explainable metric for all text generation tasks",
      "Prompt packer: Deceiving llms through compositional instruction with hidden attacks",
      "Swe-bench: Can language models resolve real-world github issues?",
      "Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment",
      "Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement",
      "Large language models effectively leverage document-level context for literary translation, but critical errors persist",
      "Rationale-Aware Answer Verification by Pairwise Self-Evaluation",
      "Critiquellm: Towards an informative critique generation model for evaluation of large language model generation",
      "Debating with more persuasive llms leads to more truthful answers",
      "Aligning Large Language Models with Selfgenerated Preference Data",
      "Prometheus: Inducing fine-grained evaluation capability in language models",
      "Prometheus 2: An open source language model specialized in evaluating other language models",
      "Look at the first sentence: Position bias in question answering",
      "Benchmarking cognitive biases in large language models as evaluators",
      "Little giants: Exploring the potential of small llms as evaluation metrics in summarization in the eval4nlp 2023 shared task",
      "Towards Leveraging Large Language Models for Automated Medical Q&A Evaluation",
      "LLMs as Evaluators: A Novel Approach to Evaluate Bug Report Summarization",
      "Rewardbench: Evaluating reward models for language modeling",
      "Can large language models aid in annotating speech emotional data? uncovering new frontiers",
      "Overview of the TREC 2023 NeuCLIR Track",
      "Query-efficient and scalable black-box adversarial attacks on discrete sequential data via bayesian optimization",
      "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
      "Prometheusvision: Visionlanguage model as a judge for fine-grained evaluation",
      "Aligning Large Language Models by On-Policy Self-Judgment",
      "RecExplainer: Aligning Large Language Models for Explaining Recommendation Models",
      "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "Automatic evaluation for mental health counseling using llms",
      "Calibraeval: Calibrating prediction distribution to mitigate selection bias in llms-as-judges",
      "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models",
      "Lecardv2: A large-scale chinese legal case retrieval dataset",
      "Generative judge for evaluating alignment",
      "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
      "Prd: Peer rank and discussion improve large language model based evaluations",
      "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset",
      "Split and merge: Aligning position biases in large language model based evaluators",
      "Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM",
      "ABSEval: An Agent-based Framework for Script Evaluation",
      "Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step",
      "WILDBENCH: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
      "Rouge: A package for automatic evaluation of summaries",
      "Truthfulqa: Measuring how models mimic human falsehoods",
      "Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models",
      "X-eval: Generalizable multi-aspect text evaluation via augmented instruction tuning with auxiliary evaluation aspects",
      "Li, et al. arXiv preprint arXiv:2311.08788 (2023).",
      "Alignbench: Benchmarking chinese alignment of large language models",
      "Agentbench: Evaluating llms as agents",
      "Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment",
      "Calibrating llm-based evaluator",
      "HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition",
      "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style",
      "Aligning with human judgement: The role of pairwise preference in large language model evaluators",
      "Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons",
      "JUDING THE JUDGES: ASYSTEMATIC INVESTIGATION OF POSITION BIAS IN PAIRWISE COMPARATIVE AS",
      "An empirical study of catastrophic forgetting in large language models during continual fine-tuning",
      "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation",
      "Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval",
      "Self-refine: Iterative refinement with self-feedback",
      "USR: An unsupervised and reference free evaluation metric for dialog generation",
      "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs",
      "Evaluating the Performance of Large Language Models via Debates",
      "A corpus and cloze evaluation for deeper understanding of commonsense stories",
      "Creative Beam Search: LLM-as-a-Judge for Improving Response Generation",
      "Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena",
      "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "JurEE not Judges: safeguarding llm interactions with small, specialised Encoder Ensembles",
      "Principles of artificial intelligence",
      "PiCO: Peer Review in LLMs based on the Consistency Optimization",
      "JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking",
      "BioPlanner: automatic evaluation of LLMs on protocol planning in biology",
      "Hanieh Deilamsalehy, and Nedim Lipka. 2024. A multi-llm debiasing framework",
      "Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics",
      "Human-Centered Design Recommendations for LLM-as-a-judge",
      "Unifying large language models and knowledge graphs: A roadmap",
      "Llm evaluators recognize and favor their own generations",
      "Bleu: a method for automatic evaluation of machine translation",
      "Offsetbias: Leveraging debiased data for tuning evaluators",
      "AIME: AI System Optimization via Multiple LLM Evaluators",
      "Refiner: Reasoning feedback on intermediate representations",
      "Ignore previous prompt: Attack techniques for language models",
      "Large language models sensitivity to the order of options in multiple-choice questions",
      "Bias patterns in the application of LLMs for clinical decision support: A comprehensive study",
      "Large language models are effective text rankers with pairwise ranking prompting",
      "Direct preference optimization: Your language model is secretly a reward model",
      "Center-of-inattention: Position biases in decision-making",
      "LLMJudge: LLMs for Relevance Judgments",
      "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment",
      "Constructing domain-specific evaluation sets for llm-as-a-judge",
      "Self-evaluation improves selective generation in large language models",
      "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
      "Ares: An automated evaluation framework for retrieval-augmented generation systems",
      "A systematic survey of prompt engineering in large language models: Techniques and applications",
      "Spearman's rank correlation coefficient",
      "Estimates of the regression coefficient based on Kendall's tau",
      "Who validates the validators? aligning llm-assisted evaluation of llm outputs with human preferences",
      "Annual ACM Symposium on User Interface Software and Technology",
      "do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
      "Opinsummeval: Revisiting automated evaluation for opinion summarization",
      "Large language models can be easily distracted by irrelevant context",
      "Optimizationbased Prompt Injection Attack to LLM-as-a-Judge",
      "Judging the judges: A investigation of position bias in pairwise comparative assessments by llms",
      "Fusion-Eval: Integrating Assistant Evaluators with LLMs",
      "Beyond human data: Scaling self-training for problem-solving with language models",
      "Don't Use LLMs to Make Relevance Judgments",
      "KRX Bench: Automating Financial Benchmark Creation via Large Language Models",
      "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
      "FineSurE: Fine-grained summarization evaluation using LLMs",
      "Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More",
      "Automated Essay Scoring and Revising Based on Open-Source Large Language Models",
      "Evaluation metrics in the era of GPT-4: reliably evaluating large language models on sequence to sequence tasks",
      "From calculation to adjudication: Examining llm judges on mathematical reasoning tasks",
      "Large language models are inconsistent and biased evaluators",
      "Fast Best-of-N Decoding via Speculative Rejection",
      "Natural backdoor attack on text data",
      "Adv-bert: Bert is not robust on misspellings! generating nature adversarial samples on bert",
      "Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks",
      "Raluca Ada Popa, and Ion Stoica. 2024. JudgeBench: A Benchmark for Evaluating LLM-based Judges",
      "AI can help humans find common ground in democratic deliberation",
      "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges",
      "A comprehensive survey of hallucination mitigation techniques in large language models",
      "Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning",
      "Open foundation and fine-tuned chat models",
      "Appworld: A controllable world of apps and people for benchmarking interactive coding agents",
      "Self-rationalization improves LLM as a fine-grained judge",
      "Are Expert-Level Language Models Expert-Level Annotators? arXiv preprint",
      "Computing machinery and intelligence",
      "LLMs cannot find reasoning errors, but can correct them!",
      "Can large language models really improve by self-critiquing their own plans",
      "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
      "Foundational autoraters: Taming large language models for better automatic evaluation",
      "Halu-j: Critique-based hallucination judge",
      "Automated Genre-Aware Article Scoring and Feedback Using Large Language Models",
      "Learning Evaluation Models from Large Language Models for Sequence Generation",
      "Large language models are not fair evaluators",
      "Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. 2024. Self-taught evaluators. arXiv preprint arXiv:2408.02666 (2024).",
      "A critic for language model generation",
      "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic Evaluation Framework for LLMs",
      "Position bias estimation for unbiased learning to rank in personal search",
      "Self-consistency improves chain of thought reasoning in language models",
      "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization",
      "HelpSteer2-Preference: Complementing Ratings with Preferences",
      "Helpsteer: Multi-attribute helpfulness dataset for steerlm",
      "Cream: Consistency regularized self-rewarding language models",
      "Five ways to look at Cohen's kappa",
      "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data",
      "Chain-of-thought prompting elicits reasoning in large language models",
      "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences",
      "Continual learning for large language models: A survey",
      "Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge",
      "Evaluating Mathematical Reasoning Beyond Accuracy",
      "Language Models can Evaluate Themselves via Probability Discrepancy",
      "Pixiu: A large language model, instruction data and evaluation benchmark for finance",
      "Sorry-bench: Systematically evaluating large language model safety refusal behaviors",
      "Selfevaluation guided beam search for reasoning",
      "DOCLENS: Multi-aspect fine-grained evaluation for medical text generation",
      "Improving Model Factuality with Fine-grained Critique-based Evaluator",
      "Llava-critic: Learning to evaluate multimodal models",
      "Wizardlm: Empowering large language models to follow complex instructions",
      "Cvalues: Measuring the values of chinese large language models from safety to responsibility",
      "Large Language Models Are Active Critics in NLG Evaluation",
      "The perfect blend: Redefining RLHF with mixture of judges",
      "STRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback",
      "Pride and prejudice: LLM amplifies self-bias in self-refinement",
      "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
      "Towards reasoning in large language models via multi-agent peer review collaboration",
      "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing",
      "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing",
      "Mitigating biases for instruction-following language models via bias neurons elimination",
      "Tree of thoughts: Deliberate problem solving with large language models",
      "Self-Judge: Selective Instruction Following with Alignment Self-Evaluation",
      "Justice or prejudice? quantifying biases in llm-as-a-judge",
      "Selfee: Iterative self-revising llm empowered by self-feedback generation",
      "Flask: Fine-grained language model evaluation based on alignment skill sets",
      "Beyond Scalar Reward Model: Learning Generative Judge from Preference Data",
      "ProtocoLLM: Automatic Evaluation Framework of LLMs on Domain-Specific Scientific Protocol Formulation Tasks",
      "Overview of the Tenth Dialog System Technology Challenge: DSTC10",
      "Kieval: A knowledge-grounded interactive evaluation framework for large language models",
      "Self-rewarding language models",
      "Disc-lawllm: Fine-tuning large language models for intelligent legal services",
      "Automatic evaluation of attribution by large language models",
      "STaR: Self-taught reasoner bootstrapping reasoning with reasoning",
      "Automatic Instruction Evolving for Large Language Models",
      "Automatic evaluation and moderation of open-domain dialogue systems",
      "TALEC: Teach Your LLM to Evaluate in Specific Domain with",
      "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
      "Llmaaa: Making large language models as active annotators",
      "Personalizing dialogue agents: I have a dog, do you have pets too",
      "Large language models as evaluators for recommendation explanations",
      "Wider and deeper llm networks are fairer llm evaluators",
      "Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions",
      "A survey of large language models",
      "Measuring the inconsistency of large language models in preferential ranking",
      "Ruifang He, and Yuexian Hou. 2023. Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models",
      "Calibrate before use: Improving few-shot performance of language models",
      "Large language models are not robust multiple choice selectors",
      "Judging llm-as-a-judge with mt-bench and chatbot arena",
      "Cheating automatic llm benchmarks: Null models achieve high win rates",
      "Mitigating the Bias of Large Language Model Evaluation",
      "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments",
      "Batch calibration: Rethinking calibration for in-context learning and prompt engineering",
      "Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks",
      "Sotopia: Interactive evaluation for social intelligence in language agents",
      "Calibrated self-rewarding vision language models",
      "Are Large Language Models Rational Investors? arXiv preprint",
      "Judgelm: Fine-tuned large language models are scalable judges",
      "A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models",
      "Yuandong Tian, et al. 2024. Agent-as-a-Judge: Evaluate Agents with Agents",
      "ICE-Score: Instructing Large Language Models to Evaluate Code",
      "Universal and transferable adversarial attacks on aligned language models"
    ]
  }
}