{"paper_id": 257532815, "title": "GPT-4 Technical Report", "author_names": ["OpenAI Josh Achiam", "Steven Adler", "Sandhini Agarwal", "Lama Ahmad", "Ilge Akkaya", "Florencia Leoni Aleman", "Diogo Almeida", "Janko Altenschmidt", "Sam Altman", "Shyamal Anadkat", "Red Avila", "Igor Babuschkin", "S. Balaji", "Valerie Balcom", "Paul Baltescu", "Haim-ing Bao", "Mo Bavarian", "J. Belgum", "Irwan Bello", "Jake Berdine", "Gabriel Bernadett-Shapiro", "Christopher Berner", "Lenny Bogdonoff", "Oleg Boiko", "Made-laine Boyd", "Anna-Luisa Brakman", "Greg Brockman", "Tim Brooks", "Miles Brundage", "Kevin Button", "Trevor Cai", "Rosie Campbell", "Andrew Cann", "Brittany Carey", "Chelsea Carlson", "Rory Carmichael", "Brooke Chan", "Che Chang", "Fotis Chantzis", "Derek Chen", "Sully Chen", "Ruby Chen", "Jason Chen", "Mark Chen", "Benjamin Chess", "Chester Cho", "Casey Chu", "Hyung Won Chung", "Dave Cummings", "Jeremiah Currier", "Yunxing Dai", "Cory Decareaux", "Thomas Degry", "Noah Deutsch", "Damien Deville", "Arka Dhar", "David Dohan", "Steve Dowling", "Sheila Dunning", "Adrien Ecoffet", "Atty Eleti", "Tyna Eloundou", "David Farhi", "L. Fedus", "Niko Felix", "Sim'on Posada Fishman", "Juston Forte", "Is-abella Fulford", "Leo Gao", "Elie Georges", "C. Gibson", "Vik Goel", "Tarun Gogineni", "Gabriel Goh", "Raphael Gontijo-Lopes", "Jonathan Gordon", "Morgan Grafstein", "Scott Gray", "Ryan Greene", "Joshua Gross", "S. Gu", "Yufei Guo", "Chris Hallacy", "Jesse Han", "Jeff Harris", "Yuchen He", "Mike Heaton", "Johannes Heidecke", "Chris Hesse", "Alan Hickey", "W. Hickey", "Peter Hoeschele", "Brandon Houghton", "Kenny Hsu", "Shengli Hu", "Xin Hu", "Joost Huizinga", "Shantanu Jain", "Shawn Jain", "Joanne Jang", "Angela Jiang", "Roger Jiang", "Haozhun Jin", "Denny Jin", "Shino Jomoto", "B. Jonn", "Heewoo Jun", "Tomer Kaftan", "Lukasz Kaiser", "Ali Kamali", "I. Kanitscheider", "N. Keskar", "Tabarak Khan", "Logan Kilpatrick", "Jong Wook Kim", "Christina Kim", "Yongjik Kim", "Hendrik Kirchner", "J. Kiros", "Matthew Knight", "Daniel Kokotajlo", "Lukasz Kondraciuk", "Andrew Kondrich", "Aris Konstantinidis", "Kyle Kosic", "Gretchen Krueger", "Vishal Kuo", "Michael Lampe", "Ikai Lan", "Teddy Lee", "Jan Leike", "Jade Leung", "Daniel Levy", "Chak Li", "Rachel Lim", "Molly Lin", "Stephanie Lin", "Ma-teusz Litwin", "Theresa Lopez", "Ryan Lowe", "Patricia Lue", "A. Makanju", "Kim Malfacini", "Sam Manning", "Todor Markov", "Yaniv Markovski", "Bianca Martin", "Katie Mayer", "Andrew Mayne", "Bob McGrew", "S. McKinney", "Christine McLeavey", "Paul McMillan", "Jake McNeil", "David Medina", "Aalok Mehta", "Jacob Menick", "Luke Metz", "An-drey Mishchenko", "Pamela Mishkin", "Vinnie Monaco", "Evan Morikawa", "Daniel P. Mossing", "Tong Mu", "Mira Murati", "O. Murk", "David M'ely", "Ashvin Nair", "Reiichiro Nakano", "Rajeev Nayak", "Arvind Neelakantan", "Richard Ngo", "Hyeonwoo Noh", "Ouyang Long", "Cullen O'Keefe", "J. Pachocki", "A. Paino", "Joe Palermo", "Ashley Pantuliano", "Giambattista Parascandolo", "J. Parish", "Emy Parparita", "Alexandre Passos", "Mikhail Pavlov", "Andrew Peng", "Adam Perelman", "Filipe de Avila Belbute Peres", "Michael Petrov", "Henrique Pondé de Oliveira Pinto", "Michael Pokorny", "Michelle Pokrass", "Vitchyr H. Pong", "Tolly Powell", "Alethea Power", "Boris Power", "Elizabeth Proehl", "Raul Puri", "Alec Radford", "Jack W. Rae", "Aditya Ramesh", "Cameron Raymond", "Francis Real", "Kendra Rimbach", "Carl Ross", "Bob Rotsted", "Henri Roussez", "N. Ryder", "M. Saltarelli", "Ted Sanders", "Shibani Santurkar", "Girish Sastry", "Heather Schmidt", "David Schnurr", "John Schulman", "Daniel Selsam", "Kyla Sheppard", "Toki Sherbakov", "Jessica Shieh", "Sarah Shoker", "Pranav Shyam", "Szymon Sidor", "Eric Sigler", "Maddie Simens", "Jordan Sitkin", "Katarina Slama", "Ian Sohl", "Benjamin Sokolowsky", "Yang Song", "Natalie Staudacher", "F. Such", "Natalie Summers", "I. Sutskever", "Jie Tang", "N. Tezak", "Madeleine Thompson", "P. Tillet", "Amin Tootoonchian", "Elizabeth Tseng", "Preston Tuggle", "Nick Turley", "Jerry Tworek", "Juan Felipe Cer'on Uribe", "Andrea Vallone", "Arun Vijayvergiya", "Chelsea Voss", "Carroll L. Wainwright", "Justin Jay Wang", "Alvin Wang", "Ben Wang", "Jonathan Ward", "Jason Wei", "CJ Weinmann", "Akila Welihinda", "Peter Welinder", "Jiayi Weng", "Lilian Weng", "Matt Wiethoff", "Dave Willner", "Clemens Winter", "Samuel Wolrich", "Hannah Wong", "Lauren Workman", "Sherwin Wu", "Jeff Wu", "Michael Wu", "Kai Xiao", "Tao Xu", "Sarah Yoo", "Kevin Yu", "Qim-ing Yuan", "Wojciech Zaremba", "Rowan Zellers", "Chong Zhang", "Marvin Zhang", "Shengjia Zhao", "Tianhao Zheng", "Juntang Zhuang", "William Zhuk", "Barret Zoph"], "venue": "", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {}, "doi_lower": null}
{"paper_id": 271891958, "title": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation", "author_names": ["Samee Arif", "Sualeha Farid", "A. H. Azeemi", "Awais Athar", "Agha Ali Raza"], "venue": "arXiv.org", "abstract": "This paper presents a novel methodology for generating synthetic Preference Optimization (PO) datasets using multi-model workflows. We evaluate the effectiveness and potential of these workflows in automating and enhancing the dataset generation process. PO dataset generation requires two modules: (1) $\\textit{response evaluation}$, and (2) $\\textit{response generation}$. In the $\\textit{response evaluation}$ module, the responses from Large Language Models (LLMs) are evaluated and ranked - a task typically carried out by human annotators that we automate using LLMs. We assess the response evaluation module in a 2 step process. In step 1, we assess LLMs as evaluators using three distinct prompting strategies. In step 2, we apply the winning prompting strategy to compare the performance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that GPT-4o-as-a-Judge is more consistent across all datasets. For the $\\textit{response generation}$ module, we use the identified LLM evaluator configuration and compare different configurations of the LLM Feedback Loop. We use the win rate to determine the best multi-model configuration for generation. Experimenting with various configurations, we find that the LLM Feedback Loop, with Llama as the generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win rate over single-model Llama and Gemma, respectively. After identifying the best configurations for both modules, we generate our PO datasets using the above pipeline.", "year": 2024, "publicationdate": "2024-08-16", "externalids": {"DOI": "10.48550/arXiv.2408.08688"}, "doi_lower": "10.48550/arxiv.2408.08688"}
{"paper_id": 264288947, "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "author_names": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "venue": "International Conference on Learning Representations", "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.", "year": 2023, "publicationdate": "2023-10-17", "externalids": {}, "doi_lower": null}
{"paper_id": 18058702, "title": "Yelp Dataset Challenge: Review Rating Prediction", "author_names": ["Nabiha Asghar"], "venue": "arXiv.org", "abstract": "Review websites, such as TripAdvisor and Yelp, allow users to post online reviews for various businesses, products and services, and have been recently shown to have a significant influence on consumer shopping behaviour. An online review typically consists of free-form text and a star rating out of 5. The problem of predicting a user's star rating for a product, given the user's text review for that product, is called Review Rating Prediction and has lately become a popular, albeit hard, problem in machine learning. In this paper, we treat Review Rating Prediction as a multi-class classification problem, and build sixteen different prediction models by combining four feature extraction methods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent Semantic Indexing, with four machine learning algorithms, (i) logistic regression, (ii) Naive Bayes classification, (iii) perceptrons, and (iv) linear Support Vector Classification. We analyse the performance of each of these sixteen models to come up with the best model for predicting the ratings from reviews. We use the dataset provided by Yelp for training and testing the models.", "year": 2016, "publicationdate": "2016-05-17", "externalids": {}, "doi_lower": null}
{"paper_id": 280537241, "title": "Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences", "author_names": ["Zahra Ashktorab", "Michael Desmond", "Qian Pan", "James M. Johnson", "Martín Santillán Cooper", "Elizabeth M. Daly", "Rahul Nair", "Tejaswini Pedapati", "Swapnaja Achintalwar", "Werner Geyer"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2410.00873"}, "doi_lower": "10.48550/arxiv.2410.00873"}
{"paper_id": 244799619, "title": "A General Language Assistant as a Laboratory for Alignment", "author_names": ["Amanda Askell", "Yuntao Bai", "Anna Chen", "Dawn Drain", "Deep Ganguli", "T. Henighan", "Andy Jones", "Nicholas Joseph", "Benjamin Mann", "Nova Dassarma", "Nelson Elhage", "Zac Hatfield-Dodds", "Danny Hernandez", "John Kernion", "Kamal Ndousse", "Catherine Olsson", "Dario Amodei", "Tom B. Brown", "Jack Clark", "Sam McCandlish", "Chris Olah", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.", "year": 2021, "publicationdate": "2021-12-01", "externalids": {}, "doi_lower": null}
{"paper_id": 267975550, "title": "GPT classifications, with application to credit lending", "author_names": ["G. Babaei", "Paolo Giudici"], "venue": "Machine Learning with Applications", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.1016/j.mlwa.2024.100534"}, "doi_lower": "10.1016/j.mlwa.2024.100534"}
{"paper_id": 271903579, "title": "Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text", "author_names": ["Sher Badshah", "Hassan Sajjad"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2408.09235"}, "doi_lower": "10.48550/arxiv.2408.09235"}
{"paper_id": 263134555, "title": "Qwen Technical Report", "author_names": ["Jinze Bai", "Shuai Bai", "Yunfei Chu", "Zeyu Cui", "Kai Dang", "Xiaodong Deng", "Yang Fan", "Wenhang Ge", "Yu Han", "Fei Huang", "Binyuan Hui", "Luo Ji", "Mei Li", "Junyang Lin", "Runji Lin", "Dayiheng Liu", "Gao Liu", "Chengqiang Lu", "K. Lu", "Jianxin Ma", "Rui Men", "Xingzhang Ren", "Xuancheng Ren", "Chuanqi Tan", "Sinan Tan", "Jianhong Tu", "Peng Wang", "Shijie Wang", "Wei Wang", "Shengguang Wu", "Benfeng Xu", "Jin Xu", "An Yang", "Hao Yang", "Jian Yang", "Jian Yang", "Shusheng Yang", "Yang Yao", "Bowen Yu", "Yu Bowen", "Hongyi Yuan", "Zheng Yuan", "Jianwei Zhang", "Xing Zhang", "Yichang Zhang", "Zhenru Zhang", "Chang Zhou", "Jingren Zhou", "Xiaohuan Zhou", "Tianhang Zhu"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.", "year": 2023, "publicationdate": "2023-09-28", "externalids": {"DOI": "10.48550/arXiv.2309.16609"}, "doi_lower": "10.48550/arxiv.2309.16609"}
{"paper_id": 259095491, "title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner", "author_names": ["Yushi Bai", "Jiahao Ying", "Yixin Cao", "Xin Lv", "Yuze He", "Xiaozhi Wang", "Jifan Yu", "Kaisheng Zeng", "Yijia Xiao", "Haozhe Lyu", "Jiayin Zhang", "Juanzi Li", "Lei Hou"], "venue": "Neural Information Processing Systems", "abstract": "Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains to probe for a broad acquisition, and raise follow-up questions to engage in a more in-depth assessment. (2) Upon evaluation, the examiner combines both scoring and ranking measurements, providing a reliable result as it aligns closely with human annotations. (3) We additionally propose a decentralized Peer-examination method to address the biases in a single examiner. Our data and benchmarking results are available at: https://lmexam.com.", "year": 2023, "publicationdate": "2023-06-07", "externalids": {"DOI": "10.48550/arXiv.2306.04181"}, "doi_lower": "10.48550/arxiv.2306.04181"}
{"paper_id": 260460088, "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "author_names": ["Payal Bajaj", "Daniel Fernando Campos", "Nick Craswell", "Li Deng", "Jianfeng Gao", "Xiaodong Liu", "Rangan Majumder", "Andrew McNamara", "Bhaskar Mitra", "Tri Minh Nguyen", "Mir Rosenberg", "Xia Song", "A. Stoica", "Saurabh Tiwary", "Tong Wang"], "venue": "", "abstract": null, "year": 2016, "publicationdate": "2016-11-28", "externalids": {}, "doi_lower": null}
{"paper_id": 274130306, "title": "Enhancing Fake News Detection with Large Language Models Through Multi-agent Debates", "author_names": ["Korir Nancy Jeptoo", "Chengjie Sun"], "venue": "Natural Language Processing and Chinese Computing", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.1007/978-981-97-9434-8_37"}, "doi_lower": "10.1007/978-981-97-9434-8_37"}
{"paper_id": 145480729, "title": "The Intraclass Correlation Coefficient as a Measure of Reliability", "author_names": ["J. Bartko"], "venue": "Psychological Reports", "abstract": null, "year": 1966, "publicationdate": "1966-08-01", "externalids": {"DOI": "10.2466/pr0.1966.19.1.3"}, "doi_lower": "10.2466/pr0.1966.19.1.3"}
{"paper_id": 270738074, "title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks", "author_names": ["A. Bavaresco", "Raffaella Bernardi", "Leonardo Bertolazzi", "Desmond Elliott", "R. Fern'andez", "Albert Gatt", "E. Ghaleb", "Mario Giulianelli", "Michael Hanna", "Alexander Koller", "Andr'e F. T. Martins", "Philipp Mondorf", "Vera Neplenbroek", "Sandro Pezzelle", "Barbara Plank", "David Schlangen", "Alessandro Suglia", "Aditya K Surikuchi", "Ece Takmaz", "A. Testoni"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models. We provide JUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show substantial variance across models and datasets. Models are reliable evaluators on some tasks, but overall display substantial variability depending on the property being evaluated, the expertise level of the human judges, and whether the language is human or model-generated. We conclude that LLMs should be carefully validated against human judgments before being used as evaluators.", "year": 2024, "publicationdate": "2024-06-26", "externalids": {"DOI": "10.48550/arXiv.2406.18403"}, "doi_lower": "10.48550/arxiv.2406.18403"}
{"paper_id": 261030303, "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models", "author_names": ["Maciej Besta", "Nils Blach", "Aleš Kubíček", "Robert Gerstenberger", "Lukas Gianinazzi", "Joanna Gajda", "Tomasz Lehmann", "Michal Podstawski", "H. Niewiadomski", "P. Nyczyk", "Torsten Hoefler"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "We introduce Graph of Thoughts (GoT): a framework that\nadvances prompting capabilities in large language models\n(LLMs) beyond those offered by paradigms such as \nChain-of-Thought or Tree of Thoughts (ToT). The key idea and \nprimary advantage of GoT is the ability to model the information \ngenerated by an LLM as an arbitrary graph, where units of \ninformation (\"LLM thoughts\") are vertices, and edges correspond\nto dependencies between these vertices. This approach enables \ncombining arbitrary LLM thoughts into synergistic outcomes, \ndistilling the essence of whole networks of thoughts,\nor enhancing thoughts using feedback loops. We illustrate\nthat GoT offers advantages over state of the art on different\ntasks, for example increasing the quality of sorting by 62%\nover ToT, while simultaneously reducing costs by >31%.\nWe ensure that GoT is extensible with new thought \ntransformations and thus can be used to spearhead new prompting\nschemes. This work brings the LLM reasoning closer to human \nthinking or brain mechanisms such as recurrence, both\nof which form complex networks", "year": 2023, "publicationdate": "2023-08-18", "externalids": {"DOI": "10.1609/aaai.v38i16.29720"}, "doi_lower": "10.1609/aaai.v38i16.29720"}
{"paper_id": 147259862, "title": "Position Bias in Multiple-Choice Questions", "author_names": ["N. Blunch"], "venue": "", "abstract": null, "year": 1984, "publicationdate": "1984-05-01", "externalids": {"DOI": "10.1177/002224378402100210"}, "doi_lower": "10.1177/002224378402100210"}
{"paper_id": 269010004, "title": "Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?", "author_names": ["Nathan Brake", "Thomas Schaaf"], "venue": "NAACL-HLT", "abstract": "Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.", "year": 2024, "publicationdate": "2024-04-09", "externalids": {"DOI": "10.48550/arXiv.2404.06503"}, "doi_lower": "10.48550/arxiv.2404.06503"}
{"paper_id": 252090310, "title": "Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples", "author_names": ["Hezekiah J. Branch", "Jonathan Rodriguez Cefalu", "Jeremy McHugh", "Leyla Hujer", "Aditya Bahl", "Daniel del Castillo Iglesias", "Ron Heichman", "Ramesh Darwishi"], "venue": "arXiv.org", "abstract": "Recent advances in the development of large language models have resulted in public access to state-of-the-art pre-trained language models (PLMs), including Generative Pre-trained Transformer 3 (GPT-3) and Bidirectional Encoder Representations from Transformers (BERT). However, evaluations of PLMs, in practice, have shown their susceptibility to adversarial attacks during the training and fine-tuning stages of development. Such attacks can result in erroneous outputs, model-generated hate speech, and the exposure of users' sensitive information. While existing research has focused on adversarial attacks during either the training or the fine-tuning of PLMs, there is a deficit of information on attacks made between these two development phases. In this work, we highlight a major security vulnerability in the public release of GPT-3 and further investigate this vulnerability in other state-of-the-art PLMs. We restrict our work to pre-trained models that have not undergone fine-tuning. Further, we underscore token distance-minimized perturbations as an effective adversarial approach, bypassing both supervised and unsupervised quality measures. Following this approach, we observe a significant decrease in text classification quality when evaluating for semantic similarity.", "year": 2022, "publicationdate": "2022-09-05", "externalids": {"DOI": "10.48550/arXiv.2209.02128"}, "doi_lower": "10.48550/arxiv.2209.02128"}
{"paper_id": 146345744, "title": "Evaluations of Self and Others: Self-Enhancement Biases in Social Judgments", "author_names": ["Jonathon D. Brown"], "venue": "", "abstract": null, "year": 1986, "publicationdate": "1986-12-01", "externalids": {"DOI": "10.1521/SOCO.1986.4.4.353"}, "doi_lower": "10.1521/soco.1986.4.4.353"}
{"paper_id": 273507159, "title": "CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution", "author_names": ["Maosong Cao", "Alexander Lam", "Haodong Duan", "Hong-wei Liu", "Songyang Zhang", "Kai Chen"], "venue": "arXiv.org", "abstract": "Efficient and accurate evaluation is crucial for the continuous improvement of large language models (LLMs). Among various assessment methods, subjective evaluation has garnered significant attention due to its superior alignment with real-world usage scenarios and human preferences. However, human-based evaluations are costly and lack reproducibility, making precise automated evaluators (judgers) vital in this process. In this report, we introduce \\textbf{CompassJudger-1}, the first open-source \\textbf{all-in-one} judge LLM. CompassJudger-1 is a general-purpose LLM that demonstrates remarkable versatility. It is capable of: 1. Performing unitary scoring and two-model comparisons as a reward model; 2. Conducting evaluations according to specified formats; 3. Generating critiques; 4. Executing diverse tasks like a general LLM. To assess the evaluation capabilities of different judge models under a unified setting, we have also established \\textbf{JudgerBench}, a new benchmark that encompasses various subjective evaluation tasks and covers a wide range of topics. CompassJudger-1 offers a comprehensive solution for various evaluation tasks while maintaining the flexibility to adapt to diverse requirements. Both CompassJudger and JudgerBench are released and available to the research community athttps://github.com/open-compass/CompassJudger. We believe that by open-sourcing these tools, we can foster collaboration and accelerate progress in LLM evaluation methodologies.", "year": 2024, "publicationdate": "2024-10-21", "externalids": {"DOI": "10.48550/arXiv.2410.16256"}, "doi_lower": "10.48550/arxiv.2410.16256"}
{"paper_id": 273901456, "title": "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic", "author_names": ["Meng Cao", "Lei Shu", "Lei Yu", "Yun Zhu", "Nevan Wichers", "Yinxiao Liu", "Lei Meng"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful critic model, and another where a single language model fulfills both roles. We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial intrinsic rewards significantly improve both sample efficiency and the overall performance of the policy model, supported by both automatic and human evaluation.", "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2024.emnlp-main.515"}, "doi_lower": "10.18653/v1/2024.emnlp-main.515"}
{"paper_id": 260887105, "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate", "author_names": ["Chi-Min Chan", "Weize Chen", "Yusheng Su", "Jianxuan Yu", "Wei Xue", "Shan Zhang", "Jie Fu", "Zhiyuan Liu"], "venue": "arXiv.org", "abstract": "Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.", "year": 2023, "publicationdate": "2023-08-14", "externalids": {"DOI": "10.48550/arXiv.2308.07201"}, "doi_lower": "10.48550/arxiv.2308.07201"}
{"paper_id": 259360395, "title": "A Survey on Evaluation of Large Language Models", "author_names": ["Yu-Chu Chang", "Xu Wang", "Jindong Wang", "Yuan Wu", "Kaijie Zhu", "Hao Chen", "Linyi Yang", "Xiaoyuan Yi", "Cunxiang Wang", "Yidong Wang", "Weirong Ye", "Yue Zhang", "Yi Chang", "Philip S. Yu", "Qian Yang", "Xingxu Xie"], "venue": "ACM Transactions on Intelligent Systems and Technology", "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey", "year": 2023, "publicationdate": "2023-07-06", "externalids": {"DOI": "10.1145/3641289"}, "doi_lower": "10.1145/3641289"}
{"paper_id": 267523079, "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark", "author_names": ["Dongping Chen", "Ruoxi Chen", "Shilin Zhang", "Yinuo Liu", "Yaochen Wang", "Huichi Zhou", "Qihui Zhang", "Pan Zhou", "Yao Wan", "Lichao Sun"], "venue": "International Conference on Machine Learning", "abstract": "Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence of multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparison, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking. Furthermore, a closer examination reveals persistent challenges in the judgment capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: \\url{https://mllm-judge.github.io/}.", "year": 2024, "publicationdate": "2024-02-07", "externalids": {}, "doi_lower": null}
{"paper_id": 267740522, "title": "Humans or LLMs as the Judge? A Study on Judgement Bias", "author_names": ["Guiming Hardy Chen", "Shunian Chen", "Ziche Liu", "Feng Jiang", "Benyou Wang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Adopting human and large language models (LLM) as judges (*a.k.a* human- and LLM-as-a-judge) for evaluating the performance of LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLMs, questioning the reliability of the evaluation results. In this paper, we propose a novel framework that is free from referencing groundtruth annotations for investigating **Misinformation Oversight Bias**, **Gender Bias**, **Authority Bias** and **Beauty Bias** on LLM and human judges. We curate a dataset referring to the revised Bloom’s Taxonomy and conduct thousands of evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the cutting-edge judges possess considerable biases. We further exploit these biases to conduct attacks on LLM judges. We hope that our work can notify the community of the bias and vulnerability of human- and LLM-as-a-judge, as well as the urgency of developing robust evaluation systems.", "year": 2024, "publicationdate": "2024-02-16", "externalids": {"DOI": "10.18653/v1/2024.emnlp-main.474"}, "doi_lower": "10.18653/v1/2024.emnlp-main.474"}
{"paper_id": 252918409, "title": "StoryER: Automatic Story Evaluation via Ranking, Rating and Reasoning", "author_names": ["Hong Chen", "D. Vo", "Hiroya Takamura", "Yusuke Miyao", "Hideki Nakayama"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Existing automatic story evaluation methods place a premium on story lexical level coherence, deviating from human preference.We go beyond this limitation by considering a novel Story Evaluation method that mimics human preference when judging a story, namely StoryER, which consists of three sub-tasks: Ranking, Rating and Reasoning.Given either a machine-generated or a human-written story, StoryER requires the machine to output 1) a preference score that corresponds to human preference, 2) specific ratings and their corresponding confidences and 3) comments for various aspects (e.g., opening, character-shaping).To support these tasks, we introduce a well-annotated dataset comprising (i) 100k ranked story pairs; and (ii) a set of 46k ratings and comments on various aspects of the story.We finetune Longformer-Encoder-Decoder (LED) on the collected dataset, with the encoder responsible for preference score and aspect prediction and the decoder for comment generation.Our comprehensive experiments result a competitive benchmark for each task, showing the high correlation to human preference.In addition, we have witnessed the joint learning of the preference scores, the aspect ratings, and the comments brings gain each single task.Our dataset and benchmarks are publicly available to advance the research of story evaluation tasks.", "year": 2022, "publicationdate": "2022-10-16", "externalids": {"DOI": "10.48550/arXiv.2210.08459"}, "doi_lower": "10.48550/arxiv.2210.08459"}
{"paper_id": 273374964, "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation", "author_names": ["Junjie Chen", "Weihang Su", "Zhumin Chu", "Haitao Li", "Qinyao Ai", "Yiqun Liu", "Min Zhang", "Shaoping Ma"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2410.12265"}, "doi_lower": "10.48550/arxiv.2410.12265"}
{"paper_id": 264289232, "title": "Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs", "author_names": ["Jiefeng Chen", "Jinsung Yoon", "Sayna Ebrahimi", "Sercan Ö. Arik", "Tomas Pfister", "Somesh Jha"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AUROC from 74.61% to 80.25%.", "year": 2023, "publicationdate": "2023-10-18", "externalids": {"DOI": "10.48550/arXiv.2310.11689"}, "doi_lower": "10.48550/arxiv.2310.11689"}
{"paper_id": 269157590, "title": "Automated Evaluation of Large Vision-Language Models on Self-Driving Corner Cases", "author_names": ["Yanze Li", "Wenhua Zhang", "Kai Chen", "Yanxin Liu", "Pengxiang Li", "Ruiyuan Gao", "Lanqing Hong", "Meng Tian", "Xinhai Zhao", "Zhenguo Li", "Dit-Yan Yeung", "Huchuan Lu", "Xu Jia"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "Large Vision-Language Models (LVLMs) have received widespread attentions for advancing the interpretable self-driving. Existing evaluations of LVLMs primarily focus on multi-faceted capabilities in natural circumstances, lacking automated and quantifiable assessment for self-driving, let alone the severe road corner cases. In this work, we propose CODA-LM, the very first benchmark for the automatic evaluation of LVLMs for self-driving corner cases. We adopt a hierarchical data structure and prompt powerful LVLMs to analyze complex driving scenes and generate high-quality pre-annotations for the human annotators, while for LVLM evaluation, we show that using the text-only large language models (LLMs) as judges reveals even better alignment with human preferences than the LVLM judges. Moreover, with our CODA-LM, we build CODA-VLM, a new driving LVLM surpassing all open-sourced counterparts on CODA-LM. Our CODA-VLM performs comparably with GPT-4V, even surpassing GPT-4V by +21.42% on the regional perception task. We hope CODA-LM can become the catalyst to promote interpretable self-driving empowered by LVLMs.", "year": 2024, "publicationdate": "2024-04-16", "externalids": {"DOI": "10.1109/WACV61041.2025.00759"}, "doi_lower": "10.1109/wacv61041.2025.00759"}
{"paper_id": 235755472, "title": "Evaluating Large Language Models Trained on Code", "author_names": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Pondé", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "Scott Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mo Bavarian", "Clemens Winter", "P. Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "Jan Leike", "Josh Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "I. Sutskever", "Wojciech Zaremba"], "venue": "arXiv.org", "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.", "year": 2021, "publicationdate": "2021-07-07", "externalids": {}, "doi_lower": null}
{"paper_id": 271064295, "title": "Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence", "author_names": ["Weize Chen", "Ziming You", "Ran Li", "Yitong Guan", "Cheng Qian", "Chenyang Zhao", "Cheng Yang", "Ruobing Xie", "Zhiyuan Liu", "Maosong Sun"], "venue": "International Conference on Learning Representations", "abstract": "The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. Our codebase has been released at \\url{https://github.com/OpenBMB/IoA}.", "year": 2024, "publicationdate": "2024-07-09", "externalids": {"DOI": "10.48550/arXiv.2407.07061"}, "doi_lower": "10.48550/arxiv.2407.07061"}
{"paper_id": 258059885, "title": "Teaching Large Language Models to Self-Debug", "author_names": ["Xinyun Chen", "Maxwell Lin", "Nathanael Schärli", "Denny Zhou"], "venue": "International Conference on Learning Representations", "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.", "year": 2023, "publicationdate": "2023-04-11", "externalids": {"DOI": "10.48550/arXiv.2304.05128"}, "doi_lower": "10.48550/arxiv.2304.05128"}
{"paper_id": 273653866, "title": "LLMs are Biased Evaluators But Not Biased for Retrieval Augmented Generation", "author_names": ["Yen-Shan Chen", "Jing Jin", "Peng-Ting Kuo", "Chao-Wei Huang", "Yun-Nung Chen"], "venue": "arXiv.org", "abstract": "Recent studies have demonstrated that large language models (LLMs) exhibit significant biases in evaluation tasks, particularly in preferentially rating and favoring self-generated content. However, the extent to which this bias manifests in fact-oriented tasks, especially within retrieval-augmented generation (RAG) frameworks-where keyword extraction and factual accuracy take precedence over stylistic elements-remains unclear. Our study addresses this knowledge gap by simulating two critical phases of the RAG framework. In the first phase, we access the suitability of human-authored versus model-generated passages, emulating the pointwise reranking process. The second phase involves conducting pairwise reading comprehension tests to simulate the generation process. Contrary to previous findings indicating a self-preference in rating tasks, our results reveal no significant self-preference effect in RAG frameworks. Instead, we observe that factual accuracy significantly influences LLMs' output, even in the absence of prior knowledge. Our research contributes to the ongoing discourse on LLM biases and their implications for RAG-based system, offering insights that may inform the development of more robust and unbiased LLM systems.", "year": 2024, "publicationdate": "2024-10-28", "externalids": {"DOI": "10.48550/arXiv.2410.20833"}, "doi_lower": "10.48550/arxiv.2410.20833"}
{"paper_id": 251765463, "title": "Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation", "author_names": ["Cyril Chhun", "Pierre Colombo", "C. Clavel", "Fabian M. Suchanek"], "venue": "International Conference on Computational Linguistics", "abstract": "Research on Automatic Story Generation (ASG) relies heavily on human and automatic evaluation. However, there is no consensus on which human evaluation criteria to use, and no analysis of how well automatic criteria correlate with them. In this paper, we propose to re-evaluate ASG evaluation. We introduce a set of 6 orthogonal and comprehensive human criteria, carefully motivated by the social sciences literature. We also present HANNA, an annotated dataset of 1,056 stories produced by 10 different ASG systems. HANNA allows us to quantitatively evaluate the correlations of 72 automatic metrics with human criteria. Our analysis highlights the weaknesses of current metrics for ASG and allows us to formulate practical recommendations for ASG evaluation.", "year": 2022, "publicationdate": "2022-08-24", "externalids": {"DOI": "10.48550/arXiv.2208.11646"}, "doi_lower": "10.48550/arxiv.2208.11646"}
{"paper_id": 271050312, "title": "Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course", "author_names": ["Cheng-Han Chiang", "Wei-Chih Chen", "Chun-Yi Kuan", "Chienchou Yang", "Hung-yi Lee"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Using large language models (LLMs) for automatic evaluation has become an important evaluation method in NLP research. However, it is unclear whether these LLM-based evaluators can be effectively applied in real-world classrooms to assess student assignments. This empirical report shares how we use GPT-4 as an automatic assignment evaluator in a university course with over 1000 students. Based on student responses, we found that LLM-based assignment evaluators are generally acceptable to students when they have free access to these tools. However, students also noted that the LLM sometimes fails to adhere to the evaluation instructions, resulting in unreasonable assessments. Additionally, we observed that students can easily manipulate the LLM to output specific strings, allowing them to achieve high scores without meeting the assignment rubric. Based on student feedback and our experience, we offer several recommendations for effectively integrating LLMs into future classroom evaluations. Our observation also highlights potential directions for improving LLM-based evaluators, including their instruction-following ability and vulnerability to prompt hacking.", "year": 2024, "publicationdate": "2024-07-07", "externalids": {"DOI": "10.48550/arXiv.2407.05216"}, "doi_lower": "10.48550/arxiv.2407.05216"}
{"paper_id": 266166678, "title": "A Closer Look into Using Large Language Models for Automatic Evaluation", "author_names": ["Cheng-Han Chiang", "Hunghuei Lee"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.findings-emnlp.599"}, "doi_lower": "10.18653/v1/2023.findings-emnlp.599"}
{"paper_id": 258070012, "title": "Marketing with ChatGPT: Navigating the Ethical Terrain of GPT-Based Chatbot Technology", "author_names": ["Pablo Rivas"], "venue": "Applied Informatics", "abstract": "ChatGPT is an AI-powered chatbot platform that enables human users to converse with machines. It utilizes natural language processing and machine learning algorithms, transforming how people interact with AI technology. ChatGPT offers significant advantages over previous similar tools, and its potential for application in various fields has generated attention and anticipation. However, some experts are wary of ChatGPT, citing ethical implications. Therefore, this paper shows that ChatGPT has significant potential to transform marketing and shape its future if certain ethical considerations are taken into account. First, we argue that ChatGPT-based tools can help marketers create content faster and potentially with quality similar to human content creators. It can also assist marketers in conducting more efficient research and understanding customers better, automating customer service, and improving efficiency. Then we discuss ethical implications and potential risks for marketers, consumers, and other stakeholders, that are essential for ChatGPT-based marketing; doing so can help revolutionize marketing while avoiding potential harm to stakeholders.", "year": 2023, "publicationdate": "2023-04-10", "externalids": {"DOI": "10.3390/ai4020019"}, "doi_lower": "10.3390/ai4020019"}
{"paper_id": 269149352, "title": "Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation", "author_names": ["Juhwan Choi", "Jungmin Yun", "Kyohoon Jin", "Youngbin Kim"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The quality of the dataset is crucial for ensuring optimal performance and reliability of downstream task models. However, datasets often contain noisy data inadvertently included during the construction process. Numerous attempts have been made to correct this issue through human annotators. However, hiring and managing human annotators is expensive and time-consuming. As an alternative, recent studies are exploring the use of large language models (LLMs) for data annotation.In this study, we present a case study that extends the application of LLM-based data annotation to enhance the quality of existing datasets through a cleansing strategy. Specifically, we leverage approaches such as chain-of-thought and majority voting to imitate human annotation and classify unrelated documents from the Multi-News dataset, which is widely used for the multi-document summarization task. Through our proposed cleansing method, we introduce an enhanced Multi-News+. By employing LLMs for data cleansing, we demonstrate an efficient and effective approach to improving dataset quality without relying on expensive human annotation efforts.", "year": 2024, "publicationdate": "2024-04-15", "externalids": {"DOI": "10.48550/arXiv.2404.09682"}, "doi_lower": "10.48550/arxiv.2404.09682"}
{"paper_id": 267311508, "title": "PRE: A Peer Review Based Large Language Model Evaluator", "author_names": ["Zhumin Chu", "Qingyao Ai", "Yiteng Tu", "Haitao Li", "Yiqun Liu"], "venue": "arXiv.org", "abstract": "The impressive performance of large language models (LLMs) has attracted considerable attention from the academic and industrial communities. Besides how to construct and train LLMs, how to effectively evaluate and compare the capacity of LLMs has also been well recognized as an important yet difficult problem. Existing paradigms rely on either human annotators or model-based evaluators to evaluate the performance of LLMs on different tasks. However, these paradigms often suffer from high cost, low generalizability, and inherited biases in practice, which make them incapable of supporting the sustainable development of LLMs in long term. In order to address these issues, inspired by the peer review systems widely used in academic publication process, we propose a novel framework that can automatically evaluate LLMs through a peer-review process. Specifically, for the evaluation of a specific task, we first construct a small qualification exam to select\"reviewers\"from a couple of powerful LLMs. Then, to actually evaluate the\"submissions\"written by different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to rate or compare the submissions. The final ranking of evaluatee LLMs is generated based on the results provided by all reviewers. We conducted extensive experiments on text summarization tasks with eleven LLMs including GPT-4. The results demonstrate the existence of biasness when evaluating using a single LLM. Also, our PRE model outperforms all the baselines, illustrating the effectiveness of the peer review mechanism.", "year": 2024, "publicationdate": "2024-01-28", "externalids": {"DOI": "10.48550/arXiv.2401.15641"}, "doi_lower": "10.48550/arxiv.2401.15641"}
{"paper_id": 253018554, "title": "Scaling Instruction-Finetuned Language Models", "author_names": ["Hyung Won Chung", "Le Hou", "S. Longpre", "Barret Zoph", "Yi Tay", "W. Fedus", "Eric Li", "Xuezhi Wang", "Mostafa Dehghani", "Siddhartha Brahma", "Albert Webson", "S. Gu", "Zhuyun Dai", "Mirac Suzgun", "Xinyun Chen", "A. Chowdhery", "Dasha Valter", "Sharan Narang", "Gaurav Mishra", "Adams Wei Yu", "Vincent Zhao", "Yanping Huang", "Andrew M. Dai", "Hongkun Yu", "Slav Petrov", "Ed H. Chi", "J. Dean", "Jacob Devlin", "Adam Roberts", "Denny Zhou", "Quoc V. Le", "Jason Wei"], "venue": "Journal of machine learning research", "abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.", "year": 2022, "publicationdate": "2022-10-20", "externalids": {"DOI": "10.48550/arXiv.2210.11416"}, "doi_lower": "10.48550/arxiv.2210.11416"}
{"paper_id": 17625170, "title": "On the Importance of the Pearson Correlation Coefficient in Noise Reduction", "author_names": ["J. Benesty", "Jingdong Chen", "Yiteng Huang"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "abstract": "Noise reduction, which aims at estimating a clean speech from noisy observations, has attracted a considerable amount of research and engineering attention over the past few decades. In the single-channel scenario, an estimate of the clean speech can be obtained by passing the noisy signal picked up by the microphone through a linear filter/transformation. The core issue, then, is how to find an optimal filter/transformation such that, after the filtering process, the signal-to-noise ratio (SNR) is improved but the desired speech signal is not noticeably distorted. Most of the existing optimal filters (such as the Wiener filter and subspace transformation) are formulated from the mean-square error (MSE) criterion. However, with the MSE formulation, many desired properties of the optimal noise-reduction filters such as the SNR behavior cannot be seen. In this paper, we present a new criterion based on the Pearson correlation coefficient (PCC). We show that in the context of noise reduction the squared PCC (SPCC) has many appealing properties and can be used as an optimization cost function to derive many optimal and suboptimal noise-reduction filters. The clear advantage of using the SPCC over the MSE is that the noise-reduction performance (in terms of the SNR improvement and speech distortion) of the resulting optimal filters can be easily analyzed. This shows that, as far as noise reduction is concerned, the SPCC-based cost function serves as a more natural criterion to optimize as compared to the MSE.", "year": 2008, "publicationdate": "2008-05-01", "externalids": {"DOI": "10.1109/TASL.2008.919072"}, "doi_lower": "10.1109/tasl.2008.919072"}
{"paper_id": 261242374, "title": "Overview of the TREC 2021 Deep Learning Track", "author_names": ["Nick Craswell", "Bhaskar Mitra", "Emine Yilmaz", "Daniel Fernando Campos", "Jimmy J. Lin"], "venue": "Text Retrieval Conference", "abstract": "This is the third year of the TREC Deep Learning track. As in previous years, we leverage the MS MARCO datasets that made hundreds of thousands of human annotated training labels available for both passage and document ranking tasks. In addition, this year we refreshed both the document and the passage collections which also led to a nearly four times increase in the document collection size and nearly $16$ times increase in the size of the passage collection. Deep neural ranking models that employ large scale pretraininig continued to outperform traditional retrieval methods this year. We also found that single stage retrieval can achieve good performance on both tasks although they still do not perform at par with multistage retrieval pipelines. Finally, the increase in the collection size and the general data refresh raised some questions about completeness of NIST judgments and the quality of the training labels that were mapped to the new collections from the old ones which we discuss in this report.", "year": 2025, "publicationdate": "2025-07-10", "externalids": {"DOI": "10.48550/arXiv.2507.08191"}, "doi_lower": "10.48550/arxiv.2507.08191"}
{"paper_id": 261302277, "title": "Overview of the TREC 2022 Deep Learning Track", "author_names": ["Nick Craswell", "Bhaskar Mitra", "Emine Yilmaz", "Daniel Fernando Campos", "Jimmy Lin", "E. Voorhees", "I. Soboroff"], "venue": "Text Retrieval Conference", "abstract": "This is the fourth year of the TREC Deep Learning track. As in previous years, we leverage the MS MARCO datasets that made hundreds of thousands of human annotated training labels available for both passage and document ranking tasks. In addition, this year we also leverage both the refreshed passage and document collections that were released last year leading to a nearly $16$ times increase in the size of the passage collection and nearly four times increase in the document collection size. Unlike previous years, in 2022 we mainly focused on constructing a more complete test collection for the passage retrieval task, which has been the primary focus of the track. The document ranking task was kept as a secondary task, where document-level labels were inferred from the passage-level labels. Our analysis shows that similar to previous years, deep neural ranking models that employ large scale pretraining continued to outperform traditional retrieval methods. Due to the focusing our judging resources on passage judging, we are more confident in the quality of this year's queries and judgments, with respect to our ability to distinguish between runs and reuse the dataset in future. We also see some surprises in overall outcomes. Some top-performing runs did not do dense retrieval. Runs that did single-stage dense retrieval were not as competitive this year as they were last year.", "year": 2025, "publicationdate": "2025-07-10", "externalids": {"DOI": "10.48550/arXiv.2507.10865"}, "doi_lower": "10.48550/arxiv.2507.10865"}
{"paper_id": 283290883, "title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback", "author_names": ["Ganqu Cui", "Lifan Yuan", "Ning Ding", "Guanming Yao", "Bingxiang He", "Wei Zhu", "Yuan Ni", "Guo Tong Xie", "Ruobing Xie", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "venue": "International Conference on Machine Learning", "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability, resulting in small sizes or limited topics of current datasets. This further hinders feedback learning as well as alignment research within the open-source community. To address this issue, we explore how to go beyond human feedback and collect high-quality \\textit{AI feedback} automatically for a scalable alternative. Specifically, we identify \\textbf{scale and diversity} as the key factors for feedback data to take effect. Accordingly, we first broaden instructions and responses in both amount and breadth to encompass a wider range of user-assistant interactions. Then, we meticulously apply a series of techniques to mitigate annotation biases for more reliable AI feedback. We finally present \\textsc{UltraFeedback}, a large-scale, high-quality, and diversified AI feedback dataset, which contains over 1 million GPT-4 feedback for 250k user-assistant conversations from various aspects. Built upon \\textsc{UltraFeedback}, we align a LLaMA-based model by best-of-$n$ sampling and reinforcement learning, demonstrating its exceptional performance on chat benchmarks. Our work validates the effectiveness of scaled AI feedback data in constructing strong open-source chat language models, serving as a solid foundation for future feedback learning research. Our data and models are available at https://github.com/thunlp/UltraFeedback.", "year": 2023, "publicationdate": "2023-10-02", "externalids": {}, "doi_lower": null}
{"paper_id": 271270577, "title": "Aligning Model Evaluations with Human Preferences: Mitigating Token Count Bias in Language Model Assessments", "author_names": ["Roland Daynauth", "Jason Mars"], "venue": "arXiv.org", "abstract": "The SLAM paper demonstrated that on-device Small Language Models (SLMs) are a viable and cost-effective alternative to API-based Large Language Models (LLMs), such as OpenAI's GPT-4, offering comparable performance and stability. However, SLAM also identified discrepancies between human preferences and traditional auto-evaluators. This follow-up paper explores methods to align LLM evaluator preferences with human evaluations by addressing biases, particularly toward higher token counts. We employed Bayesian statistics and a t-test to quantify this bias and developed a recalibration procedure to adjust the GPTScorer. Our findings significantly improve aligning the recalibrated LLM evaluator with human evaluations across multiple use cases. For instance, spearman's ranking correlation score in the Recommendation use case improved from -27.27 to 44.55. These results highlight the importance of accounting for biases in automated evaluations to ensure fair and accurate model assessments. The recalibration process enhances the reliability of automated evaluators, leading to better AI models that align with human values and expectations. This study provides a robust methodology for future research into bias correction and emphasizes the feasibility and benefits of developing human-aligned AI evaluation systems.", "year": 2024, "publicationdate": "2024-07-05", "externalids": {"DOI": "10.48550/arXiv.2407.12847"}, "doi_lower": "10.48550/arxiv.2407.12847"}
{"paper_id": 274306023, "title": "Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach", "author_names": ["Shijian Deng", "Wentian Zhao", "Yu-Jhe Li", "Kun Wan", "Daniel Miranda", "Ajinkya Kale", "Yapeng Tian"], "venue": "arXiv.org", "abstract": "Self-improvement in multimodal large language models (MLLMs) is crucial for enhancing their reliability and robustness. However, current methods often rely heavily on MLLMs themselves as judges, leading to high computational costs and potential pitfalls like reward hacking and model collapse. This paper introduces a novel, model-level judge-free self-improvement framework. Our approach employs a controlled feedback mechanism while eliminating the need for MLLMs in the verification loop. We generate preference learning pairs using a controllable hallucination mechanism and optimize data quality by leveraging lightweight, contrastive language-image encoders to evaluate and reverse pairs when necessary. Evaluations across public benchmarks and our newly introduced IC dataset designed to challenge hallucination control demonstrate that our model outperforms conventional techniques. We achieve superior precision and recall with significantly lower computational demands. This method offers an efficient pathway to scalable self-improvement in MLLMs, balancing performance gains with reduced resource requirements.", "year": 2024, "publicationdate": "2024-11-26", "externalids": {"DOI": "10.48550/arXiv.2411.17760"}, "doi_lower": "10.48550/arxiv.2411.17760"}
{"paper_id": 269762045, "title": "PHUDGE: Phi-3 as Scalable Judge", "author_names": ["Mahesh Deshwal", "Apoorva Chawla"], "venue": "arXiv.org", "abstract": "In this paper cum technical report, we present PHUDGE A fine tuned Phi3 model that achieved SOTA results in 4 tasks as Feedback Test, Feedback OOD, MT Human, Preference Test surpassing each and every existing model in latency and throughput. It shows very strong correlation not only with GPT4 but with Human annotators too in unseen data as well as in both absolute and relative grading tasks. We have not only addressed the usage of small LMs for cost effective production grade systems but have also shown that Causal modelling is not only slow in nature but sometimes it can hinder models learning capabilities and should be replaced by simpler tasks whenever we can to make the overall system faster and better. We show that by following systematic ML experimentation, thoughtful data augmentation and re purposing the problem itself, we can even beat 10x bigger models even with lesser training data. To the best of our knowledge, we are re the first one to experiment and showcase the usage of generalised version of Earth Movers Distance AKA Wasserstein distance by using Minkowski Distance with a penalty to control loss smoothing and can be used as a loss function instead of Cross Entropy to get stable training and better results for grading tasks.", "year": 2024, "publicationdate": "2024-05-12", "externalids": {"DOI": "10.48550/arXiv.2405.08029"}, "doi_lower": "10.48550/arxiv.2405.08029"}
{"paper_id": 272512238, "title": "Striking the Balance in Using LLMs for Fact-Checking: A Narrative Literature Review", "author_names": ["Laurence Dierickx", "A. Dalen", "A. Opdahl", "Carl-Gustav Lindén"], "venue": "Multidisciplinary International Symposium on Disinformation in Open Online Media", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.1007/978-3-031-71210-4_1"}, "doi_lower": "10.1007/978-3-031-71210-4_1"}
{"paper_id": 258840897, "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations", "author_names": ["Ning Ding", "Yulin Chen", "Bokai Xu", "Yujia Qin", "Zhi Zheng", "Shengding Hu", "Zhiyuan Liu", "Maosong Sun", "Bowen Zhou"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to improve the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions that a human might have with an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently outperforms other open-source models, including Vicuna, the previously recognized state-of-the-art open-source model. The dataset and the model will be publicly released\\footnote{\\url{https://github.com/thunlp/UltraChat}}.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14233"}, "doi_lower": "10.48550/arxiv.2305.14233"}
{"paper_id": 264172238, "title": "CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion", "author_names": ["Yangruibo Ding", "Zijian Wang", "Wasi Uddin Ahmad", "Hantian Ding", "Ming Tan", "Nihal Jain", "M. K. Ramanathan", "Ramesh Nallapati", "Parminder Bhatia", "Dan Roth", "Bing Xiang"], "venue": "Neural Information Processing Systems", "abstract": "Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly. To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file. Extensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt. However, despite such improvements, the pinnacle of performance remains notably unattained even with the highest-performing model, indicating that CrossCodeEval is also capable of assessing model's capability in leveraging extensive context to make better code completion. Finally, we benchmarked various methods in retrieving cross-file context, and show that CrossCodeEval can also be used to measure the capability of code retrievers.", "year": 2023, "publicationdate": "2023-10-17", "externalids": {"DOI": "10.48550/arXiv.2310.11248"}, "doi_lower": "10.48550/arxiv.2310.11248"}
{"paper_id": 270620029, "title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "author_names": ["Sumanth Doddapaneni", "Mohammed Safi Ur Rahman Khan", "Sshubam Verma", "Mitesh M. Khapra"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large Language Models (LLMs) are increasingly relied upon to evaluate text outputs of other LLMs, thereby influencing leaderboards and development decisions. However, concerns persist over the accuracy of these assessments and the potential for misleading conclusions. In this work, we investigate the effectiveness of LLMs as evaluators for text generation tasks. We propose FBI, a novel framework designed to examine the proficiency of Evaluator LLMs in assessing four critical abilities in other LLMs: factual accuracy, instruction following, coherence in long-form writing, and reasoning proficiency. By introducing targeted perturbations in answers generated by LLMs, that clearly impact one of these key capabilities, we test whether an Evaluator LLM can detect these quality drops. By creating a total of 2400 perturbed answers covering 22 perturbation categories, we conduct a comprehensive study using different evaluation strategies on five prominent LLMs commonly used as evaluators in the literature. Our findings reveal significant shortcomings in current Evaluator LLMs, which failed to identify quality drops in over 50% of cases on average. Single-answer and pairwise evaluations demonstrated notable limitations, whereas reference-based evaluations showed comparatively better performance. These results underscore the unreliable nature of current Evaluator LLMs and advocate for cautious implementation in practical applications.", "year": 2024, "publicationdate": "2024-06-19", "externalids": {"DOI": "10.48550/arXiv.2406.13439"}, "doi_lower": "10.48550/arxiv.2406.13439"}
{"paper_id": 273228296, "title": "Self-Boosting Large Language Models with Synthetic Preference Data", "author_names": ["Qingxiu Dong", "Li Dong", "Xingxing Zhang", "Zhifang Sui", "Furu Wei"], "venue": "International Conference on Learning Representations", "abstract": "Through alignment with human preferences, Large Language Models (LLMs) have advanced significantly in generating honest, harmless, and helpful responses. However, collecting high-quality preference data is a resource-intensive and creativity-demanding process, especially for the continual improvement of LLMs. We introduce SynPO, a self-boosting paradigm that leverages synthetic preference data for model alignment. SynPO employs an iterative mechanism wherein a self-prompt generator creates diverse prompts, and a response improver refines model responses progressively. This approach trains LLMs to autonomously learn the generative rewards for their own outputs and eliminates the need for large-scale annotation of prompts and human preferences. After four SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements in instruction-following abilities, achieving over 22.1% win rate improvements on AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general performance of LLMs on various tasks, validated by a 3.2 to 5.0 average score increase on the well-recognized Open LLM leaderboard.", "year": 2024, "publicationdate": "2024-10-09", "externalids": {"DOI": "10.48550/arXiv.2410.06961"}, "doi_lower": "10.48550/arxiv.2410.06961"}
{"paper_id": 263886074, "title": "A Survey for In-context Learning", "author_names": ["Qingxiu Dong", "Lei Li", "Damai Dai", "Ce Zheng", "Zhiyong Wu", "Baobao Chang", "Xu Sun", "Jingjing Xu", "Lei Li", "Zhifang Sui"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 270560577, "title": "Can LLM be a Personalized Judge?", "author_names": ["Yijiang River Dong", "Tiancheng Hu", "Nigel Collier"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Ensuring that large language models (LLMs) reflect diverse user values and preferences is crucial as their user bases expand globally. It is therefore encouraging to see the growing interest in LLM personalization within the research community. However, current works often rely on the LLM-as-a-Judge approach for evaluation without thoroughly examining its validity. In this paper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking LLMs to judge user preferences based on personas. Our findings suggest that directly applying LLM-as-a-Personalized-Judge is less reliable than previously assumed, showing low and inconsistent agreement with human ground truth. The personas typically used are often overly simplistic, resulting in low predictive power. To address these issues, we introduce verbal uncertainty estimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to express low confidence on uncertain judgments. This adjustment leads to much higher agreement (above 80%) on high-certainty samples for binary tasks. Through human evaluation, we find that the LLM-as-a-Personalized-Judge achieves comparable performance to third-party humans evaluation and even surpasses human performance on high-certainty samples. Our work indicates that certainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for developing more reliable and scalable methods for evaluating LLM personalization.", "year": 2024, "publicationdate": "2024-06-17", "externalids": {"DOI": "10.48550/arXiv.2406.11657"}, "doi_lower": "10.48550/arxiv.2406.11657"}
{"paper_id": 273404208, "title": "Limits to scalable evaluation at the frontier: LLM as Judge won't beat twice the data", "author_names": ["Florian E. Dorner", "Vivian Y. Nastl", "Moritz Hardt"], "venue": "International Conference on Learning Representations", "abstract": "High quality annotations are increasingly a bottleneck in the explosively growing machine learning ecosystem. Scalable evaluation methods that avoid costly annotation have therefore become an important research ambition. Many hope to use strong existing models in lieu of costly labels to provide cheap model evaluations. Unfortunately, this method of using models as judges introduces biases, such as self-preferencing, that can distort model comparisons. An emerging family of debiasing tools promises to fix these issues by using a few high quality labels to debias a large number of model judgments. In this paper, we study how far such debiasing methods, in principle, can go. Our main result shows that when the judge is no more accurate than the evaluated model, no debiasing method can decrease the required amount of ground truth labels by more than half. Our result speaks to the severe limitations of the LLM-as-a-judge paradigm at the evaluation frontier where the goal is to assess newly released models that are possibly better than the judge. Through an empirical evaluation, we demonstrate that the sample size savings achievable in practice are even more modest than what our theoretical limit suggests. Along the way, our work provides new observations about debiasing methods for model evaluation, and points out promising avenues for future work.", "year": 2024, "publicationdate": "2024-10-17", "externalids": {"DOI": "10.48550/arXiv.2410.13341"}, "doi_lower": "10.48550/arxiv.2410.13341"}
{"paper_id": 269004605, "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators", "author_names": ["Yann Dubois", "Bal'azs Galambosi", "Percy Liang", "Tatsunori Hashimoto"], "venue": "arXiv.org", "abstract": "LLM-based auto-annotators have become a key component of the LLM development process due to their cost-effectiveness and scalability compared to human-based evaluation. However, these auto-annotators can introduce biases that are hard to remove. Even simple, known confounders such as preference for longer outputs remain in existing automated evaluation metrics. We propose a simple regression analysis approach for controlling biases in auto-evaluations. As a real case study, we focus on reducing the length bias of AlpacaEval, a fast and affordable benchmark for instruction-tuned LLMs that uses LLMs to estimate response quality. Despite being highly correlated with human preferences, AlpacaEval is known to favor models that generate longer outputs. We introduce a length-controlled AlpacaEval that aims to answer the counterfactual question:\"What would the preference be if the model's and baseline's output had the same length?\"To achieve this, we first fit a generalized linear model to predict the biased auto-annotator's preferences based on the mediators we want to control for (length difference) and other relevant features. We then obtain length-controlled preferences by predicting preferences while conditioning the GLM with a zero difference in lengths. Length-controlling not only improves the robustness of the metric to manipulations in model verbosity, but we also find that it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94 to 0.98.", "year": 2024, "publicationdate": "2024-04-06", "externalids": {"DOI": "10.48550/arXiv.2404.04475"}, "doi_lower": "10.48550/arxiv.2404.04475"}
{"paper_id": 21698802, "title": "HotFlip: White-Box Adversarial Examples for Text Classification", "author_names": ["J. Ebrahimi", "Anyi Rao", "Daniel Lowd", "D. Dou"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.", "year": 2017, "publicationdate": "2017-12-19", "externalids": {"DOI": "10.18653/v1/P18-2006"}, "doi_lower": "10.18653/v1/p18-2006"}
{"paper_id": 273187056, "title": "Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge", "author_names": ["Aparna Elangovan", "Jongwoo Ko", "Lei Xu", "Mahsa Elyasi", "Ling Liu", "S. Bodapati", "Dan Roth"], "venue": "International Conference on Learning Representations", "abstract": "The effectiveness of automatic evaluation of generative models is typically measured by comparing the labels generated via automation with labels by humans using correlation metrics. However, metrics like Krippendorff's $\\alpha$ and Randolph's $\\kappa$ were originally designed to measure the reliability of human labeling, thus make assumptions about typical human labeling behavior, and these assumptions may not be applicable to machine generated labels. In this paper, we show how *relying on a single aggregate correlation score* can obscure fundamental differences between human labels and those from automatic evaluation, including LLM-as-a-Judge. Specifically, we demonstrate that when the proportion of samples with variation or uncertainty in human assigned labels is relatively high, machine labels (generated by automatic evaluation methods) may superficially appear to have similar or better correlation with the human majority label compared to the human-to-human (HH) correlation. This can create the illusion that labels from automatic evaluation approximates the human majority label. However, as the proportion of samples with consistent human labels increases, the correlation between machine and human labels fall well below HH correlation. Based on these findings, we first propose stratifying data by human label uncertainty to provide a more robust analysis of automatic evaluation performance. Second, recognizing that uncertainty and variation are inherent in perception-based human evaluations, such as those involving attitudes or preferences, we introduce a new metric - binned Jensen-Shannon Divergence for perception for such scenarios to better measure the effectiveness of automatic evaluations. We present visualization techniques -- perception charts, to contextualize correlation measures appropriately. We have open-sourced at https://github.com/amazon-science/BeyondCorrelation.", "year": 2024, "publicationdate": "2024-10-03", "externalids": {"DOI": "10.48550/arXiv.2410.03775"}, "doi_lower": "10.48550/arxiv.2410.03775"}
{"paper_id": 220768873, "title": "SummEval: Re-evaluating Summarization Evaluation", "author_names": ["A. R. Fabbri", "Wojciech Kryscinski", "Bryan McCann", "R. Socher", "Dragomir R. Radev"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Abstract The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.", "year": 2020, "publicationdate": "2020-07-24", "externalids": {"DOI": "10.1162/tacl_a_00373"}, "doi_lower": "10.1162/tacl_a_00373"}
{"paper_id": 44134226, "title": "Hierarchical Neural Story Generation", "author_names": ["Angela Fan", "M. Lewis", "Yann Dauphin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.", "year": 2018, "publicationdate": "2018-05-01", "externalids": {"DOI": "10.18653/v1/P18-1082"}, "doi_lower": "10.18653/v1/p18-1082"}
{"paper_id": 271213154, "title": "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs", "author_names": ["Zhiting Fan", "Ruizhe Chen", "Ruiling Xu", "Zuozhu Liu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Evaluating the bias of LLMs becomes more crucial with their rapid development. However, existing evaluation approaches rely on fixed-form outputs and cannot adapt to the flexible open-text generation scenarios of LLMs (e.g., sentence completion and question answering). To address this, we introduce BiasAlert, a plug-and-play tool designed to detect social bias in open-text generations of LLMs. BiasAlert integrates external human knowledge with its inherent reasoning capabilities to detect bias reliably. Extensive experiments demonstrate that BiasAlert significantly outperforms existing state-of-the-art methods like GPT-4-as-Judge in detecting bias. Furthermore, through application studies, we showcase the utility of BiasAlert in reliable LLM fairness evaluation and bias mitigation across various scenarios. Model and code will be publicly released.", "year": 2024, "publicationdate": "2024-07-14", "externalids": {"DOI": "10.48550/arXiv.2407.10241"}, "doi_lower": "10.48550/arxiv.2407.10241"}
{"paper_id": 258967265, "title": "Mitigating Label Biases for In-context Learning", "author_names": ["Yu Fei", "Yifan Hou", "Zeming Chen", "Antoine Bosselut"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias the model’s predictions. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time). Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model’s label bias using random in-domain words from the task corpus. After controlling for this estimated bias when making predictions, our novel domain-context calibration significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks. The gain is substantial on tasks with large domain-label bias (up to 37% in Macro-F1). Furthermore, our results generalize to models with different scales, pretraining methods, and manually-designed task instructions, showing the prevalence of label biases in ICL.", "year": 2023, "publicationdate": "2023-05-28", "externalids": {"DOI": "10.48550/arXiv.2305.19148"}, "doi_lower": "10.48550/arxiv.2305.19148"}
{"paper_id": 261557137, "title": "Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs", "author_names": ["Chao Feng", "Xinyu Zhang", "Zichu Fei"], "venue": "arXiv.org", "abstract": "Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and can solve different tasks due to their emergent ability and generalizability. However, LLMs sometimes lack domain-specific knowledge to perform tasks, which would also cause hallucination during inference. In some previous works, additional modules like graph neural networks (GNNs) are trained on retrieved knowledge from external knowledge bases, aiming to mitigate the problem of lacking domain-specific knowledge. However, incorporating additional modules: 1) would need retraining additional modules when encountering novel domains; 2) would become a bottleneck since LLMs' strong abilities are not fully utilized for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver (KSL), to teach LLMs to search for essential knowledge from external knowledge bases by harnessing their own strong generalizability. Specifically, we design a simple yet effective prompt to transform retrieval into a multi-hop decision sequence, which empowers LLMs with searching knowledge ability in zero-shot manner. Additionally, KSL is able to provide complete retrieval paths and therefore increase explainability of LLMs' reasoning processes. We conduct experiments on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, and found that our approach improves LLM baseline performance by a relatively large margin.", "year": 2023, "publicationdate": "2023-09-06", "externalids": {"DOI": "10.48550/arXiv.2309.03118"}, "doi_lower": "10.48550/arxiv.2309.03118"}
{"paper_id": 268032991, "title": "Improving LLM-based Machine Translation with Systematic Self-Correction", "author_names": ["Zhaopeng Feng", "Yan Zhang", "Hao Li", "Wenqiang Liu", "Jun Lang", "Yang Feng", "Jian Wu", "Zuozhu Liu"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2402.16379"}, "doi_lower": "10.48550/arxiv.2402.16379"}
{"paper_id": 233444275, "title": "Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation", "author_names": ["Markus Freitag", "George F. Foster", "David Grangier", "Viresh Ratnakar", "Qijun Tan", "Wolfgang Macherey"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Abstract Human evaluation of modern high-quality machine translation systems is a difficult problem, and there is increasing evidence that inadequate evaluation procedures can lead to erroneous conclusions. While there has been considerable research on human evaluation, the field still lacks a commonly accepted standard procedure. As a step toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework. We carry out the largest MQM research study to date, scoring the outputs of top systems from the WMT 2020 shared task in two language pairs using annotations provided by professional translators with access to full document context. We analyze the resulting data extensively, finding among other results a substantially different ranking of evaluated systems from the one established by the WMT crowd workers, exhibiting a clear preference for human over machine output. Surprisingly, we also find that automatic metrics based on pre-trained embeddings can outperform human crowd workers. We make our corpus publicly available for further research.", "year": 2021, "publicationdate": "2021-04-29", "externalids": {"DOI": "10.1162/tacl_a_00437"}, "doi_lower": "10.1162/tacl_a_00437"}
{"paper_id": 245855921, "title": "Results of the WMT21 Metrics Shared Task: Evaluating Metrics with Expert-based Human Evaluations on TED and News Domain", "author_names": ["Markus Freitag", "Ricardo Rei", "Nitika Mathur", "Chi-kiu (羅致翹) Lo", "Craig Alan Stewart", "George F. Foster", "A. Lavie", "Ondrej Bojar"], "venue": "Conference on Machine Translation", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 1930455, "title": "The Turing Test: the first 50 years", "author_names": ["R. French"], "venue": "Trends in Cognitive Sciences", "abstract": null, "year": 2000, "publicationdate": "2000-03-01", "externalids": {"DOI": "10.1016/S1364-6613(00)01453-4"}, "doi_lower": "10.1016/s1364-6613(00)01453-4"}
{"paper_id": 256662188, "title": "GPTScore: Evaluate as You Desire", "author_names": ["Jinlan Fu", "See-Kiong Ng", "Zhengbao Jiang", "Pengfei Liu"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models.Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently.This paper proposes a novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., in-context learning, zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., Flan-T5-small) to 175B (e.g., GPT3).Experimental results on four text generation tasks, 22 evaluation aspects, and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions.This nature helps us overcome several long-standing challenges in text evaluation–how to achieve customized, multi-faceted evaluation without model training. We make our code publicly available.", "year": 2023, "publicationdate": "2023-02-08", "externalids": {"DOI": "10.18653/v1/2024.naacl-long.365"}, "doi_lower": "10.18653/v1/2024.naacl-long.365"}
{"paper_id": 266359151, "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "author_names": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Qianyu Guo", "Meng Wang", "Haofen Wang"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.", "year": 2023, "publicationdate": "2023-12-18", "externalids": {}, "doi_lower": null}
{"paper_id": 273877669, "title": "Bayesian Calibration of Win Rate Estimation with LLM Evaluators", "author_names": ["Yicheng Gao", "Gonghan Xu", "Zhe Wang", "Arman Cohan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent advances in large language models (LLMs) show the potential of using LLMs as evaluators for assessing the quality of text generations from LLMs. However, applying LLM evaluators naively to compare different systems can lead to unreliable results due to the inaccuracy and intrinsic bias of LLM evaluators. In order to mitigate this problem, we propose two calibration methods, Bayesian Win-Rate Sampling (BWRS) and Bayesian Dawid-Skene, both of which leverage Bayesian inference to more accurately infer the true win rate of generative language models. We empirically validate our methods on six datasets covering story generation, summarization, and instruction following tasks. We show that both our methods are effective in improving the accuracy of win rate estimation using LLMs as evaluators, offering a promising direction for reliable automatic text quality evaluation.", "year": 2024, "publicationdate": "2024-11-07", "externalids": {"DOI": "10.18653/v1/2024.emnlp-main.273"}, "doi_lower": "10.18653/v1/2024.emnlp-main.273"}
{"paper_id": 257766307, "title": "ChatGPT outperforms crowd workers for text-annotation tasks", "author_names": ["F. Gilardi", "Meysam Alizadeh", "M. Kubli"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "abstract": "Many NLP applications require manual text annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using four samples of tweets and news articles (n = 6,183), we show that ChatGPT outperforms crowd workers for several annotation tasks, including relevance, stance, topics, and frame detection. Across the four datasets, the zero-shot accuracy of ChatGPT exceeds that of crowd workers by about 25 percentage points on average, while ChatGPT’s intercoder agreement exceeds that of both crowd workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003—about thirty times cheaper than MTurk. These results demonstrate the potential of large language models to drastically increase the efficiency of text classification.", "year": 2023, "publicationdate": "2023-03-27", "externalids": {"DOI": "10.1073/pnas.2305016120"}, "doi_lower": "10.1073/pnas.2305016120"}
{"paper_id": 202717047, "title": "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations", "author_names": ["Karthik Gopalakrishnan", "Behnam Hedayatnia", "Qinlang Chen", "Anna Gottardi", "Sanjeev Kwatra", "Anu Venkatesh", "Raefer Gabriel", "Dilek Z. Hakkani-Tür"], "venue": "Interspeech", "abstract": "Building socialbots that can have deep, engaging open-domain conversations with humans is one of the grand challenges of artificial intelligence (AI). To this end, bots need to be able to leverage world knowledge spanning several domains effectively when conversing with humans who have their own world knowledge. Existing knowledge-grounded conversation datasets are primarily stylized with explicit roles for conversation partners. These datasets also do not explore depth or breadth of topical coverage with transitions in conversations. We introduce Topical-Chat, a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don't have explicitly defined roles, to help further research in open-domain conversational AI. We also train several state-of-the-art encoder-decoder conversational models on Topical-Chat and perform automated and human evaluation for benchmarking.", "year": 2019, "publicationdate": "2019-09-15", "externalids": {"DOI": "10.21437/interspeech.2019-3079"}, "doi_lower": "10.21437/interspeech.2019-3079"}
{"paper_id": 234778136, "title": "OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics", "author_names": ["Jian Guan", "Zhexin Zhang", "Zhuoer Feng", "Zitao Liu", "Wenbiao Ding", "Xiaoxi Mao", "Changjie Fan", "Minlie Huang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Automatic metrics are essential for developing natural language generation (NLG) models, particularly for open-ended language generation tasks such as story generation. However, existing automatic metrics are observed to correlate poorly with human evaluation. The lack of standardized benchmark datasets makes it difficult to fully evaluate the capabilities of a metric and fairly compare different metrics. Therefore, we propose OpenMEVA, a benchmark for evaluating open-ended story generation metrics. OpenMEVA provides a comprehensive test suite to assess the capabilities of metrics, including (a) the correlation with human judgments, (b) the generalization to different model outputs and datasets, (c) the ability to judge story coherence, and (d) the robustness to perturbations. To this end, OpenMEVA includes both manually annotated stories and auto-constructed test examples. We evaluate existing metrics on OpenMEVA and observe that they have poor correlation with human judgments, fail to recognize discourse-level incoherence, and lack inferential knowledge (e.g., causal order between events), the generalization ability and robustness. Our study presents insights for developing NLG models and metrics in further research.", "year": 2021, "publicationdate": "2021-05-19", "externalids": {"DOI": "10.18653/v1/2021.acl-long.500"}, "doi_lower": "10.18653/v1/2021.acl-long.500"}
{"paper_id": 267522951, "title": "Direct Language Model Alignment from Online AI Feedback", "author_names": ["Shangmin Guo", "Biao Zhang", "Tianlin Liu", "Tianqi Liu", "Misha Khalman", "Felipe Llinares-López", "Alexandre Ramé", "Thomas Mesnard", "Yao Zhao", "Bilal Piot", "Johan Ferret", "Mathieu Blondel"], "venue": "arXiv.org", "abstract": "Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.", "year": 2024, "publicationdate": "2024-02-07", "externalids": {"DOI": "10.48550/arXiv.2402.04792"}, "doi_lower": "10.48550/arxiv.2402.04792"}
{"paper_id": 264825354, "title": "Evaluating Large Language Models: A Comprehensive Survey", "author_names": ["Zishan Guo", "Renren Jin", "Chuang Liu", "Yufei Huang", "Dan Shi", "Supryadi", "Linhao Yu", "Yan Liu", "Jiaxuan Li", "Bojian Xiong", "Deyi Xiong"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs. This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to LLMs' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover LLM evaluations on capabilities, alignment, safety, and applicability. We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs. We envision that this will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.", "year": 2023, "publicationdate": "2023-10-30", "externalids": {"DOI": "10.48550/arXiv.2310.19736"}, "doi_lower": "10.48550/arxiv.2310.19736"}
{"paper_id": 273482168, "title": "Context-Aware or Context-Insensitive? Assessing LLMs' Performance in Document-Level Translation", "author_names": ["Wafaa Mohammed", "Vlad Niculae"], "venue": "", "abstract": "Large language models (LLMs) are increasingly strong contenders in machine translation. In this work, we focus on document-level translation, where some words cannot be translated without context from outside the sentence. Specifically, we investigate the ability of prominent LLMs to utilize the document context during translation through a perturbation analysis (analyzing models' robustness to perturbed and randomized document context) and an attribution analysis (examining the contribution of relevant context to the translation). We conduct an extensive evaluation across nine LLMs from diverse model families and training paradigms, including translation-specialized LLMs, alongside two encoder-decoder transformer baselines. We find that LLMs' improved document-translation performance compared to encoder-decoder models is not reflected in pronoun translation performance. Our analysis highlight the need for context-aware finetuning of LLMs with a focus on relevant parts of the context to improve their reliability for document-level translation.", "year": 2024, "publicationdate": "2024-10-18", "externalids": {}, "doi_lower": null}
{"paper_id": 261822638, "title": "Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?", "author_names": ["Rishav Hada", "Varun Gumma", "Adrian de Wynter", "Harshita Diddee", "Mohamed Ahmed", "M. Choudhury", "Kalika Bali", "Sunayana Sitaram"], "venue": "Findings", "abstract": "Large Language Models (LLMs) excel in various Natural Language Processing (NLP) tasks, yet their evaluation, particularly in languages beyond the top 20, remains inadequate due to existing benchmarks and metrics limitations. Employing LLMs as evaluators to rank or score other models’ outputs emerges as a viable solution, addressing the constraints tied to human annotators and established benchmarks. In this study, we explore the potential of LLM-based evaluators in enhancing multilingual evaluation by calibrating them against 20K human judgments across three text-generation tasks, five metrics, and eight languages. Our analysis reveals a bias in LLM-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages.", "year": 2023, "publicationdate": "2023-09-14", "externalids": {"DOI": "10.48550/arXiv.2309.07462"}, "doi_lower": "10.48550/arxiv.2309.07462"}
{"paper_id": 248964978, "title": "Prototypical Calibration for Few-shot Learning of Language Models", "author_names": ["Zhixiong Han", "Y. Hao", "Li Dong", "Yutao Sun", "Furu Wei"], "venue": "International Conference on Learning Representations", "abstract": "In-context learning of GPT-like models has been recognized as fragile across different hand-crafted templates, and demonstration permutations. In this work, we propose prototypical calibration to adaptively learn a more robust decision boundary for zero- and few-shot classification, instead of greedy decoding. Concretely, our method first adopts Gaussian mixture distribution to estimate the prototypical clusters for all categories. Then we assign each cluster to the corresponding label by solving a weighted bipartite matching problem. Given an example, its prediction is calibrated by the likelihood of prototypical clusters. Experimental results show that prototypical calibration yields a substantial improvement on a diverse set of tasks. Extensive analysis across different scales also indicates that our method calibrates the decision boundary as expected, greatly improving the robustness of GPT to templates, permutations, and class imbalance.", "year": 2022, "publicationdate": "2022-05-20", "externalids": {"DOI": "10.48550/arXiv.2205.10183"}, "doi_lower": "10.48550/arxiv.2205.10183"}
{"paper_id": 272770369, "title": "FullAnno: A Data Engine for Enhancing Image Comprehension of MLLMs", "author_names": ["Jing Hao", "Yuxiang Zhao", "Song Chen", "Yanpeng Sun", "Qiang Chen", "Gang Zhang", "Kun Yao", "Errui Ding", "Jingdong Wang"], "venue": "arXiv.org", "abstract": "Multimodal Large Language Models (MLLMs) have shown promise in a broad range of vision-language tasks with their strong reasoning and generalization capabilities. However, they heavily depend on high-quality data in the Supervised Fine-Tuning (SFT) phase. The existing approaches aim to curate high-quality data via GPT-4V, but they are not scalable due to the commercial nature of GPT-4V and the simplicity of the prompts used to instruct the model. To this end, we devised the FullAnno system, which is a data engine that can generate large-scale, high-quality, and fine-grained image annotations consisting of the category and position of objects, region descriptions, text information, as well as image dense captions. This engine is characterized by its cascade annotation process, which involves multiple expert models and employs rich prompts to instruct LLMs in generating dense image captions. We re-annotated the COCO and Visual Genome datasets using our FullAnno system, tripling the number of object annotations and increasing the length of the original image captions by a factor of 15. Experiments show that the regenerated annotation can significantly enhance the capabilities of LLaVA-v1.5 on several benchmarks. The re-annotated data are available at: https://arcana-project-page.github.io", "year": 2024, "publicationdate": "2024-09-20", "externalids": {"DOI": "10.48550/arXiv.2409.13540"}, "doi_lower": "10.48550/arxiv.2409.13540"}
{"paper_id": 16619709, "title": "The MovieLens Datasets: History and Context", "author_names": ["F. M. Harper", "J. Konstan", "J. A."], "venue": "TIIS", "abstract": null, "year": 2016, "publicationdate": "2016-01-07", "externalids": {"DOI": "10.1145/2827872"}, "doi_lower": "10.1145/2827872"}
{"paper_id": 262464455, "title": "ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning", "author_names": ["Hosein Hasanbeig", "Hiteshi Sharma", "Leo Betthauser", "F. Frujeri", "Ida Momennejad"], "venue": "arXiv.org", "abstract": "From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.", "year": 2023, "publicationdate": "2023-09-24", "externalids": {"DOI": "10.48550/arXiv.2309.13701"}, "doi_lower": "10.48550/arxiv.2309.13701"}
{"paper_id": 263334142, "title": "SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation", "author_names": ["Hangfeng He", "Hongming Zhang", "Dan Roth"], "venue": "NAACL-HLT", "abstract": "To comprehensively gauge the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains as references to assess the model-derived chains. However, such\"gold-standard\"human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning evaluation metrics, while eliminating the need for human-crafted reasoning chains as references, often require fine-tuning with human-derived chains before evaluation, complicating the process and questioning their adaptability to other datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, thereby removing the dependency on human-written reasoning chains for both model fine-tuning and evaluative purposes. Leveraging the Socratic method, we develop SocREval ({\\bf Soc}ratic Method-Inspired {\\bf R}easoning {\\bf Eval}uation), a novel approach for prompt design in reference-free reasoning evaluation. Empirical results from four human annotated datasets reveal that SocREval significantly improves GPT-4's performance, surpassing existing reference-free and reference-based reasoning evaluation metrics. Beyond its demonstrated efficacy, SocREval, proves to be both cost-efficient and robust to prompt writing and example selection, as substantiated by our in-depth analysis.", "year": 2023, "publicationdate": "2023-09-29", "externalids": {"DOI": "10.48550/arXiv.2310.00074"}, "doi_lower": "10.48550/arxiv.2310.00074"}
{"paper_id": 257805087, "title": "AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators", "author_names": ["Xingwei He", "Zheng-Wen Lin", "Yeyun Gong", "Alex Jin", "Hang Zhang", "Chen Lin", "Jian Jiao", "S. Yiu", "Nan Duan", "Weizhu Chen"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Many natural language processing (NLP) tasks rely on labeled data to train machine learning models with high performance. However, data annotation is time-consuming and expensive, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator when provided with sufficient guidance and demonstrated examples. Accordingly, we propose AnnoLLM, an annotation system powered by LLMs, which adopts a two-step approach, explain-then-annotate. Concretely, we first prompt LLMs to provide explanations for why the specific ground truth answer/label was assigned for a given example. Then, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data with LLMs. Our experiment results on three tasks, including user input and keyword relevance assessment, BoolQ, and WiC, demonstrate that AnnoLLM surpasses or performs on par with crowdsourced annotators. Furthermore, we build the first conversation-based information retrieval dataset employing AnnoLLM. This dataset is designed to facilitate the development of retrieval models capable of retrieving pertinent documents for conversational text. Human evaluation has validated the dataset's high quality.", "year": 2023, "publicationdate": "2023-03-29", "externalids": {"DOI": "10.48550/arXiv.2303.16854"}, "doi_lower": "10.48550/arxiv.2303.16854"}
{"paper_id": 269214048, "title": "FedEval-LLM: Federated Evaluation of Large Language Models on Downstream Tasks with Collective Wisdom", "author_names": ["Yuanqin He", "Yan Kang", "Lixin Fan", "Qiang Yang"], "venue": "arXiv.org", "abstract": "Federated Learning (FL) has emerged as a promising solution for collaborative training of large language models (LLMs). However, the integration of LLMs into FL introduces new challenges, particularly concerning the evaluation of LLMs. Traditional evaluation methods that rely on labeled test sets and similarity-based metrics cover only a subset of the acceptable answers, thereby failing to accurately reflect the performance of LLMs on generative tasks. Meanwhile, although automatic evaluation methods that leverage advanced LLMs present potential, they face critical risks of data leakage due to the need to transmit data to external servers and suboptimal performance on downstream tasks due to the lack of domain knowledge. To address these issues, we propose a Federated Evaluation framework of Large Language Models, named FedEval-LLM, that provides reliable performance measurements of LLMs on downstream tasks without the reliance on labeled test sets and external tools, thus ensuring strong privacy-preserving capability. FedEval-LLM leverages a consortium of personalized LLMs from participants as referees to provide domain knowledge and collective evaluation capability, thus aligning to the respective downstream tasks and mitigating uncertainties and biases associated with a single referee. Experimental results demonstrate a significant improvement in the evaluation capability of personalized evaluation models on downstream tasks. When applied to FL, these evaluation models exhibit strong agreement with human preference and RougeL-score on meticulously curated test sets. FedEval-LLM effectively overcomes the limitations of traditional metrics and the reliance on external services, making it a promising framework for the evaluation of LLMs within collaborative training scenarios.", "year": 2024, "publicationdate": "2024-04-18", "externalids": {"DOI": "10.48550/arXiv.2404.12273"}, "doi_lower": "10.48550/arxiv.2404.12273"}
{"paper_id": 268032396, "title": "If in a Crowdsourced Data Annotation Pipeline, a GPT-4", "author_names": ["Zeyu He", "Huang Chieh-Yang", "C. C. Ding", "Shaurya Rohatgi", "Ting-Hao Huang"], "venue": "International Conference on Human Factors in Computing Systems", "abstract": "Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk). However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers’ performances over the whole data-annotation process. This paper compared GPT-4 and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms. Our evaluation showed that despite best practices, MTurk pipeline’s highest accuracy was 81.5%, whereas GPT-4 achieved 83.6%. Interestingly, when combining GPT-4’s labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (87.5%, 87.0%). Further analysis suggested that, when the crowd’s and GPT-4’s labeling strengths are complementary, aggregating them could increase labeling accuracy.", "year": 2024, "publicationdate": "2024-02-26", "externalids": {"DOI": "10.1145/3613904.3642834"}, "doi_lower": "10.1145/3613904.3642834"}
{"paper_id": 271769401, "title": "Using Large Language Models to Evaluate Biomedical Query-Focused Summarisation", "author_names": ["Hashem Hijazi", "Diego Molla", "Vincent Nguyen", "Sarvnaz Karimi"], "venue": "Workshop on Biomedical Natural Language Processing", "abstract": "Biomedical question-answering systems remain popular for biomedical experts interacting with the literature to answer their medical questions. However, these systems are difficult to evaluate in the absence of costly human experts. Therefore, automatic evaluation metrics are often used in this space. Traditional automatic metrics such as ROUGE or BLEU, which rely on token overlap, have shown a low correlation with humans. We present a study that uses large language models (LLMs) to automatically evaluate systems from an international challenge on biomedical semantic indexing and question answering, called BioASQ. We measure the agreement of LLM-produced scores against human judgements. We show that LLMs correlate similarly to lexical methods when using basic prompting techniques. However, by aggregating evaluators with LLMs or by fine-tuning, we find that our methods outperform the baselines by a large margin, achieving a Spearman correlation of 0.501 and 0.511, respectively.", "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2024.bionlp-1.18"}, "doi_lower": "10.18653/v1/2024.bionlp-1.18"}
{"paper_id": 258686540, "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems", "author_names": ["Yupeng Hou", "Junjie Zhang", "Zihan Lin", "Hongyu Lu", "Ruobing Xie", "Julian McAuley", "Wayne Xin Zhao"], "venue": "European Conference on Information Retrieval", "abstract": "Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates. To solve the ranking task by LLMs, we carefully design the prompting template and conduct extensive experiments on two widely-used datasets. We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts. We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies. Equipped with these insights, zero-shot LLMs can even challenge conventional recommendation models when ranking candidates are retrieved by multiple candidate generators. The code and processed datasets are available at https://github.com/RUCAIBox/LLMRank.", "year": 2023, "publicationdate": "2023-05-15", "externalids": {"DOI": "10.48550/arXiv.2305.08845"}, "doi_lower": "10.48550/arxiv.2305.08845"}
{"paper_id": 270737769, "title": "Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability", "author_names": ["Xinyu Hu", "Li Lin", "Mingqi Gao", "Xunjian Yin", "Xiaojun Wan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The evaluation of natural language generation (NLG) tasks is a significant and longstanding research area. With the recent emergence of powerful large language models (LLMs), some studies have turned to LLM-based automatic evaluation methods, which demonstrate great potential to become a new evaluation paradigm following traditional string-based and model-based metrics. However, despite the improved performance of existing methods, they still possess some deficiencies, such as dependency on references and limited evaluation flexibility. Therefore, in this paper, we meticulously construct a large-scale NLG evaluation corpus **NLG-Eval** with annotations from both human and GPT-4 to alleviate the lack of relevant data in this field. Furthermore, we propose **Themis**, an LLM dedicated to NLG evaluation, which has been trained with our designed multi-perspective consistency verification and rating-oriented preference alignment methods. Themis can conduct flexible and interpretable evaluations without references, and it exhibits superior evaluation performance on various NLG tasks, simultaneously generalizing well to unseen tasks and surpassing other evaluation models, including GPT-4.", "year": 2024, "publicationdate": "2024-06-26", "externalids": {"DOI": "10.18653/v1/2024.emnlp-main.891"}, "doi_lower": "10.18653/v1/2024.emnlp-main.891"}
{"paper_id": 278899524, "title": "Rethinking LLM-based Preference Evaluation", "author_names": ["Zhengyu Hu", "Linxin Song", "Jieyu Zhang", "Zheyuan Xiao", "Jingang Wang", "Zhenyu Chen", "Jieyu Zhao", "Hui Xiong"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2407.01085"}, "doi_lower": "10.48550/arxiv.2407.01085"}
{"paper_id": 273404080, "title": "Language Model Preference Evaluation with Multiple Weak Evaluators", "author_names": ["Zhengyu Hu", "Jieyu Zhang", "Zhihan Xiong", "Alexander J. Ratner", "Hui Xiong", "Ranjay Krishna"], "venue": "arXiv.org", "abstract": "Despite the remarkable success of Large Language Models (LLMs), evaluating their outputs'quality regarding preference remains a critical challenge. While existing works usually leverage a strong LLM as the judge for comparing LLMs'response pairwisely, such a single-evaluator approach is vulnerable to cyclic preference, i.e., output A is better than B, B than C, but C is better than A, causing contradictory evaluation results. To address this, we introduce PGED (Preference Graph Ensemble and Denoise), a novel approach that leverages multiple model-based evaluators to construct preference graphs, and then ensembles and denoises these graphs for acyclic, non-contradictory evaluation results. We provide theoretical guarantees for our framework, demonstrating its efficacy in recovering the ground truth preference structure. Extensive experiments on ten benchmarks demonstrate PGED's superiority in three applications: 1) model ranking for evaluation, 2) response selection for test-time scaling, and 3) data selection for model fine-tuning. Notably, PGED combines small LLM evaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to outperform strong ones (e.g., Qwen2-72B), showcasing its effectiveness in enhancing evaluation reliability and improving model performance.", "year": 2024, "publicationdate": "2024-10-14", "externalids": {"DOI": "10.48550/arXiv.2410.12869"}, "doi_lower": "10.48550/arxiv.2410.12869"}
{"paper_id": 273820967, "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers", "author_names": ["Hui Huang", "Yingqi Qu", "Hongli Zhou", "Jing Liu", "Muyun Yang", "Tiejun Zhao"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2403.02839"}, "doi_lower": "10.48550/arxiv.2403.02839"}
{"paper_id": 273820967, "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers", "author_names": ["Hui Huang", "Yingqi Qu", "Hongli Zhou", "Jing Liu", "Muyun Yang", "Tiejun Zhao"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2403.02839"}, "doi_lower": "10.48550/arxiv.2403.02839"}
{"paper_id": 263609132, "title": "Large Language Models Cannot Self-Correct Reasoning Yet", "author_names": ["Jie Huang", "Xinyun Chen", "Swaroop Mishra", "Huaixiu Steven Zheng", "Adams Wei Yu", "Xinying Song", "Denny Zhou"], "venue": "International Conference on Learning Representations", "abstract": "Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.", "year": 2023, "publicationdate": "2023-10-03", "externalids": {"DOI": "10.48550/arXiv.2310.01798"}, "doi_lower": "10.48550/arxiv.2310.01798"}
{"paper_id": 259064002, "title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning", "author_names": ["Sameer Jain", "Vaishakh Keshava", "Swarnashree Mysore Sathyendra", "Patrick Fernandes", "Pengfei Liu", "Graham Neubig", "Chunting Zhou"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.18653/v1/2023.findings-acl.537"}, "doi_lower": "10.18653/v1/2023.findings-acl.537"}
{"paper_id": 267312134, "title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models", "author_names": ["Minbyul Jeong", "Jiwoong Sohn", "Mujeen Sung", "Jaewoo Kang"], "venue": "Bioinform.", "abstract": "Abstract Summary Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Similarly, Self-BioRAG outperforms RAG by 8% Rouge-1 score in generating more proficient answers on two long-form question-answering benchmarks on average. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains. Availability and implementation Self-BioRAG is available at https://github.com/dmis-lab/self-biorag.", "year": 2024, "publicationdate": "2024-01-27", "externalids": {"DOI": "10.1093/bioinformatics/btae238"}, "doi_lower": "10.1093/bioinformatics/btae238"}
{"paper_id": 273374751, "title": "PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference", "author_names": ["Jiaming Ji", "Donghai Hong", "Borong Zhang", "Boyuan Chen", "Juntao Dai", "Boren Zheng", "Tianyi Qiu", "Jiayi Zhou", "Kaile Wang", "Boxuan Li", "Sirui Han", "Yike Guo", "Yaodong Yang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In this study, we introduce the safety human preference dataset, PKU-SafeRLHF, designed to promote research on safety alignment in large language models (LLMs). As a sibling project to SafeRLHF and BeaverTails, we separate annotations of helpfulness and harmlessness for question-answering pairs, providing distinct perspectives on these coupled attributes. Overall, we provide 44.6k refined prompts and 265k question-answer pairs with safety meta-labels for 19 harm categories and three severity levels ranging from minor to severe, with answers generated by Llama-family models. Based on this, we collected 166.8k preference data, including dual-preference (helpfulness and harmlessness decoupled) and single-preference data (trade-off the helpfulness and harmlessness from scratch), respectively. Using the large-scale annotation data, we further train severity-sensitive moderation for the risk control of LLMs and safety-centric RLHF algorithms for the safety alignment of LLMs. We believe this dataset will be a valuable resource for the community, aiding in the safe deployment of LLMs. Data is available at https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF.", "year": 2024, "publicationdate": "2024-06-20", "externalids": {"DOI": "10.18653/v1/2025.acl-long.1544"}, "doi_lower": "10.18653/v1/2025.acl-long.1544"}
{"paper_id": 246652372, "title": "Survey of Hallucination in Natural Language Generation", "author_names": ["Ziwei Ji", "Nayeon Lee", "Rita Frieske", "Tiezheng Yu", "D. Su", "Yan Xu", "Etsuko Ishii", "Yejin Bang", "Delong Chen", "Wenliang Dai", "Andrea Madotto", "Pascale Fung"], "venue": "ACM Computing Surveys", "abstract": "Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.", "year": 2022, "publicationdate": "2022-02-08", "externalids": {"DOI": "10.1145/3571730"}, "doi_lower": "10.1145/3571730"}
{"paper_id": 270559271, "title": "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners", "author_names": ["Bowen Jiang", "Yangxinyu Xie", "Zhuoqun Hao", "Xiaomeng Wang", "Tanwi Mallick", "Weijie J. Su", "C. J. Taylor", "Dan Roth"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks. Specifically, we develop carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems. Our framework outlines a list of hypotheses where token biases are readily identifiable, with all null hypotheses assuming genuine reasoning capabilities of LLMs. The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning. While they may perform well on classic problems, their success largely depends on recognizing superficial patterns with strong token bias, thereby raising concerns about their actual reasoning and generalization abilities.", "year": 2024, "publicationdate": "2024-06-16", "externalids": {"DOI": "10.48550/arXiv.2406.11050"}, "doi_lower": "10.48550/arxiv.2406.11050"}
{"paper_id": 263334281, "title": "TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks", "author_names": ["Dongfu Jiang", "Yishan Li", "Ge Zhang", "Wenhao Huang", "Bill Yuchen Lin", "Wenhu Chen"], "venue": "Trans. Mach. Learn. Res.", "abstract": "We present TIGERScore, a \\textbf{T}rained metric that follows \\textbf{I}nstruction \\textbf{G}uidance to perform \\textbf{E}xplainable, and \\textbf{R}eference-free evaluation over a wide spectrum of text generation tasks. Different from other automatic evaluation methods that only provide arcane scores, TIGERScore is guided by natural language instruction to provide error analysis to pinpoint the mistakes in the generated text. Our metric is based on LLaMA-2, trained on our meticulously curated instruction-tuning dataset MetricInstruct which covers 6 text generation tasks and 23 text generation datasets. The dataset consists of 42K quadruple in the form of (instruction, input, system output $\\rightarrow$ error analysis). We collected the `system outputs' through from a large variety of models to cover different types of errors. To quantitatively assess our metric, we evaluate its correlation with human ratings on 5 held-in datasets, 2 held-out datasets and show that TIGERScore can achieve the open-source SoTA correlation with human ratings across these datasets and almost approaches GPT-4 evaluator. As a reference-free metric, its correlation can even surpass the best existing reference-based metrics. To further qualitatively assess the rationale generated by our metric, we conduct human evaluation on the generated explanations and found that the explanations are 70.8\\% accurate. Through these experimental results, we believe TIGERScore demonstrates the possibility of building universal explainable metrics to evaluate any text generation task. All the resourced are released in our project website: \\url{https://tiger-ai-lab.github.io/TIGERScore/}.", "year": 2023, "publicationdate": "2023-10-01", "externalids": {"DOI": "10.48550/arXiv.2310.00752"}, "doi_lower": "10.48550/arxiv.2310.00752"}
{"paper_id": 264147056, "title": "Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks", "author_names": ["Shuyu Jiang", "Xingshu Chen", "Rui Tang"], "venue": "arXiv.org", "abstract": "Recently, Large language models (LLMs) with powerful general capabilities have been increasingly integrated into various Web applications, while undergoing alignment training to ensure that the generated content aligns with user intent and ethics. Unfortunately, they remain the risk of generating harmful content like hate speech and criminal activities in practical applications. Current approaches primarily rely on detecting, collecting, and training against harmful prompts to prevent such risks. However, they typically focused on the\"superficial\"harmful prompts with a solitary intent, ignoring composite attack instructions with multiple intentions that can easily elicit harmful content in real-world scenarios. In this paper, we introduce an innovative technique for obfuscating harmful instructions: Compositional Instruction Attacks (CIA), which refers to attacking by combination and encapsulation of multiple instructions. CIA hides harmful prompts within instructions of harmless intentions, making it impossible for the model to identify underlying malicious intentions. Furthermore, we implement two transformation methods, known as T-CIA and W-CIA, to automatically disguise harmful instructions as talking or writing tasks, making them appear harmless to LLMs. We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safety assessment datasets and two harmful prompt datasets. It achieves an attack success rate of 95%+ on safety assessment datasets, and 83%+ for GPT-4, 91%+ for ChatGPT (gpt-3.5-turbo backed) and ChatGLM2-6B on harmful prompt datasets. Our approach reveals the vulnerability of LLMs to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to LLM security development. Warning: this paper may contain offensive or upsetting content!", "year": 2023, "publicationdate": "2023-10-16", "externalids": {"DOI": "10.48550/arXiv.2310.10077"}, "doi_lower": "10.48550/arxiv.2310.10077"}
{"paper_id": 263829697, "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?", "author_names": ["Carlos E. Jimenez", "John Yang", "Alexander Wettig", "Shunyu Yao", "Kexin Pei", "Ofir Press", "Karthik Narasimhan"], "venue": "International Conference on Learning Representations", "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.", "year": 2023, "publicationdate": "2023-10-10", "externalids": {"DOI": "10.48550/arXiv.2310.06770"}, "doi_lower": "10.48550/arxiv.2310.06770"}
{"paper_id": 278479534, "title": "Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment", "author_names": ["Yuu Jinnai", "Tetsuro Morimura", "Kaito Ariu", "Kenshi Abe"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2404.01054"}, "doi_lower": "10.48550/arxiv.2404.01054"}
{"paper_id": 271516195, "title": "Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement", "author_names": ["Jaehun Jung", "Faeze Brahman", "Yejin Choi"], "venue": "International Conference on Learning Representations", "abstract": "We present a principled approach to provide LLM-based evaluation with a rigorous guarantee of human agreement. We first propose that a reliable evaluation method should not uncritically rely on model preferences for pairwise evaluation, but rather assess the confidence of judge models and selectively decide when to trust its judgement. We then show that under this selective evaluation framework, human agreement can be provably guaranteed -- such that the model evaluation aligns with that of humans to a user-specified agreement level. As part of our framework, we also introduce Simulated Annotators, a novel confidence estimation method that significantly improves judge calibration and thus enables high coverage of evaluated instances. Finally, we propose Cascaded Selective Evaluation, where we use cheaper models as initial judges and escalate to stronger models only when necessary -- again, while still providing a provable guarantee of human agreement. Experimental results show that Cascaded Selective Evaluation guarantees strong alignment with humans, far beyond what LLM judges could achieve without selective evaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never achieves 80% human agreement, our method, even while employing substantially cost-effective models such as Mistral-7B, guarantees over 80% human agreement with almost 80% test coverage.", "year": 2024, "publicationdate": "2024-07-25", "externalids": {"DOI": "10.48550/arXiv.2407.18370"}, "doi_lower": "10.48550/arxiv.2407.18370"}
{"paper_id": 257985066, "title": "Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist", "author_names": ["Marzena Karpinska", "Mohit Iyyer"], "venue": "Conference on Machine Translation", "abstract": "Large language models (LLMs) are competitive with the state of the art on a wide range of sentence-level translation datasets. However, their ability to translate paragraphs and documents remains unexplored because evaluation in these settings is costly and difficult. We show through a rigorous human evaluation that asking the GPT-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation across 18 linguistically-diverse language pairs (e.g., translating into and out of Japanese, Polish, and English). Our evaluation, which took approximately 350 hours of effort for annotation and analysis, is conducted by hiring translators fluent in both the source and target language and asking them to provide both span-level error annotations as well as preference judgments of which system’s translations are better. We observe that discourse-level LLM translators commit fewer mistranslations, grammar errors, and stylistic inconsistencies than sentence-level approaches. With that said, critical errors still abound, including occasional content omissions, and a human translator’s intervention remains necessary to ensure that the author’s voice remains intact. We publicly release our dataset and error annotations to spur future research on the evaluation of document-level literary translation.", "year": 2023, "publicationdate": "2023-04-06", "externalids": {"DOI": "10.48550/arXiv.2304.03245"}, "doi_lower": "10.48550/arxiv.2304.03245"}
{"paper_id": 273185960, "title": "Rationale-Aware Answer Verification by Pairwise Self-Evaluation", "author_names": ["Akira Kawabata", "Saku Sugawara"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Answer verification identifies correct solutions among candidates generated by large language models (LLMs). Current approaches typically train verifier models by labeling solutions as correct or incorrect based solely on whether the final answer matches the gold answer. However, this approach neglects any flawed rationale in the solution yielding the correct answer, undermining the verifier’s ability to distinguish between sound and flawed rationales. We empirically show that in StrategyQA, only 19% of LLM-generated solutions with correct answers have valid rationales, thus leading to an unreliable verifier. Furthermore, we demonstrate that training a verifier on valid rationales significantly improves its ability to distinguish valid and flawed rationale. To make a better verifier without extra human supervision, we introduce REPS (Rationale Enhancement through Pairwise Selection), a method for selecting valid rationales from candidates by iteratively applying pairwise self-evaluation using the same LLM that generates the solutions. Verifiers trained on solutions selected by REPS outperform those trained using conventional training methods on three reasoning benchmarks (ARC-Challenge, DROP, and StrategyQA). Our results suggest that training reliable verifiers requires ensuring the validity of rationales in addition to the correctness of the final answers, which would be critical for models assisting humans in solving complex reasoning tasks.", "year": 2024, "publicationdate": "2024-10-07", "externalids": {"DOI": "10.48550/arXiv.2410.04838"}, "doi_lower": "10.48550/arxiv.2410.04838"}
{"paper_id": 270738200, "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation", "author_names": ["Pei Ke", "Bosi Wen", "Andrew Feng", "Xiao Liu", "Xuanyu Lei", "Jiale Cheng", "Shengyuan Wang", "Aohan Zeng", "Yuxiao Dong", "Hongning Wang", "Jie Tang", "Minlie Huang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT.", "year": 2023, "publicationdate": "2023-11-30", "externalids": {"DOI": "10.18653/v1/2024.acl-long.704"}, "doi_lower": "10.18653/v1/2024.acl-long.704"}
{"paper_id": 267627652, "title": "Debating with More Persuasive LLMs Leads to More Truthful Answers", "author_names": ["Akbir Khan", "John Hughes", "Dan Valentine", "Laura Ruis", "Kshitij Sachan", "Ansh Radhakrishnan", "Edward Grefenstette", "Samuel R. Bowman", "Tim Rocktaschel", "Ethan Perez"], "venue": "International Conference on Machine Learning", "abstract": "Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.", "year": 2024, "publicationdate": "2024-02-09", "externalids": {"DOI": "10.48550/arXiv.2402.06782"}, "doi_lower": "10.48550/arxiv.2402.06782"}
{"paper_id": 282735646, "title": "A Survey on Human Preference Learning for Aligning Large Language Models", "author_names": ["Ruili Jiang", "Kehai Chen", "Xuefeng Bai", "Zhixuan He", "Juntao Li", "Muyun Yang", "Tiejun Zhao", "Liqiang Nie", "Min Zhang"], "venue": "ACM Computing Surveys", "abstract": "The recent surge in versatile large language models (LLMs) demonstrates remarkable success across a wide range of contexts. A key factor contributing to this success is LLM alignment, in which human preference learning plays a decisive role in steering the models’ capabilities toward fulfilling human objectives. In this survey, we review the progress in human preference learning within a unified framework, aiming to provide a comprehensive perspective on established methodologies while exploring avenues to further advance LLM alignment. Specifically, we categorize human preference feedback based on data sources and formats, summarize techniques for human preference modeling and usage, and present an overview of prevailing evaluation protocols for LLM alignment. Finally, we discuss the existing challenges and identify potential directions for future research, with a particular emphasis on generalizability, transferability, and controllability.", "year": 2025, "publicationdate": "2025-11-04", "externalids": {"DOI": "10.1145/3773279"}, "doi_lower": "10.1145/3773279"}
{"paper_id": 265675839, "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models", "author_names": ["Seungone Kim", "Jamin Shin", "Yejin Cho", "Joel Jang", "S. Longpre", "Hwaran Lee", "Sangdoo Yun", "Seongjin Shin", "Sungdong Kim", "James Thorne", "Minjoon Seo"], "venue": "International Conference on Learning Representations", "abstract": "Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator LLM. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment&MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open-source our code, dataset, and model at https://kaistai.github.io/prometheus/.", "year": 2023, "publicationdate": "2023-10-12", "externalids": {"DOI": "10.48550/arXiv.2310.08491"}, "doi_lower": "10.48550/arxiv.2310.08491"}
{"paper_id": 269502688, "title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models", "author_names": ["Seungone Kim", "Juyoung Suk", "Shayne Longpre", "Bill Yuchen Lin", "Jamin Shin", "S. Welleck", "Graham Neubig", "Moontae Lee", "Kyungjae Lee", "Minjoon Seo"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available.", "year": 2024, "publicationdate": "2024-05-02", "externalids": {"DOI": "10.48550/arXiv.2405.01535"}, "doi_lower": "10.48550/arxiv.2405.01535"}
{"paper_id": 216868500, "title": "Look at the First Sentence: Position Bias in Question Answering", "author_names": ["Miyoung Ko", "Jinhyuk Lee", "Hyunjae Kim", "Gangwoo Kim", "Jaewoo Kang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Many extractive question answering models are trained to predict start and end positions of answers. The choice of predicting answers as positions is mainly due to its simplicity and effectiveness. In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions learn spurious positional cues and fail to give answers in different positions. We first illustrate this position bias in popular extractive QA models such as BiDAF and BERT and thoroughly examine how position bias propagates through each layer of BERT. To safely deliver position information without position bias, we train models with various de-biasing methods including entropy regularization and bias ensembling. Among them, we found that using the prior distribution of answer positions as a bias model is very effective at reducing position bias recovering the performance of BERT from 35.24% to 81.17% when trained on a biased SQuAD dataset.", "year": 2020, "publicationdate": "2020-04-30", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.84"}, "doi_lower": "10.18653/v1/2020.emnlp-main.84"}
{"paper_id": 263310448, "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators", "author_names": ["Ryan Koo", "Minhwa Lee", "Vipul Raheja", "Jong Inn Park", "Zae Myung Kim", "Dongyeop Kang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large Language Models are cognitively biased judges. Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences. Our project page is at: https://minnesotanlp.github.io/cobbler.", "year": 2023, "publicationdate": "2023-09-29", "externalids": {"DOI": "10.48550/arXiv.2309.17012"}, "doi_lower": "10.48550/arxiv.2309.17012"}
{"paper_id": 264833277, "title": "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task", "author_names": ["Neema Kotonya", "Saran Krishnasamy", "Joel R. Tetreault", "Alejandro Jaimes"], "venue": "EVAL4NLP", "abstract": "This paper describes and analyzes our participation in the 2023 Eval4NLP shared task, which focuses on assessing the effectiveness of prompt-based techniques to empower Large Language Models to handle the task of quality estimation, particularly in the context of evaluating machine translations and summaries. We conducted systematic experiments with various prompting techniques, including standard prompting, prompts informed by annotator instructions, and innovative chain-of-thought prompting. In addition, we integrated these approaches with zero-shot and one-shot learning methods to maximize the efficacy of our evaluation procedures. Our work reveals that combining these approaches using a “small”, open source model (orca_mini_v3_7B) yields competitive results.", "year": 2023, "publicationdate": "2023-11-01", "externalids": {"DOI": "10.48550/arXiv.2311.00686"}, "doi_lower": "10.48550/arxiv.2311.00686"}
{"paper_id": 272367338, "title": "Towards Leveraging Large Language Models for Automated Medical Q&A Evaluation", "author_names": ["Jack Krolik", "Herprit Mahal", "Feroz Ahmad", "Gaurav Trivedi", "B. Saket"], "venue": "arXiv.org", "abstract": "This paper explores the potential of using Large Language Models (LLMs) to automate the evaluation of responses in medical Question and Answer (Q\\&A) systems, a crucial form of Natural Language Processing. Traditionally, human evaluation has been indispensable for assessing the quality of these responses. However, manual evaluation by medical professionals is time-consuming and costly. Our study examines whether LLMs can reliably replicate human evaluations by using questions derived from patient data, thereby saving valuable time for medical experts. While the findings suggest promising results, further research is needed to address more specific or complex questions that were beyond the scope of this initial investigation.", "year": 2024, "publicationdate": "2024-09-03", "externalids": {"DOI": "10.48550/arXiv.2409.01941"}, "doi_lower": "10.48550/arxiv.2409.01941"}
{"paper_id": 272368407, "title": "LLMs as Evaluators: A Novel Approach to Evaluate Bug Report Summarization", "author_names": ["Abhishek Kumar", "Sonia Haiduc", "Partha Pratim Das", "P. Chakrabarti"], "venue": "arXiv.org", "abstract": "Summarizing software artifacts is an important task that has been thoroughly researched. For evaluating software summarization approaches, human judgment is still the most trusted evaluation. However, it is time-consuming and fatiguing for evaluators, making it challenging to scale and reproduce. Large Language Models (LLMs) have demonstrated remarkable capabilities in various software engineering tasks, motivating us to explore their potential as automatic evaluators for approaches that aim to summarize software artifacts. In this study, we investigate whether LLMs can evaluate bug report summarization effectively. We conducted an experiment in which we presented the same set of bug summarization problems to humans and three LLMs (GPT-4o, LLaMA-3, and Gemini) for evaluation on two tasks: selecting the correct bug report title and bug report summary from a set of options. Our results show that LLMs performed generally well in evaluating bug report summaries, with GPT-4o outperforming the other LLMs. Additionally, both humans and LLMs showed consistent decision-making, but humans experienced fatigue, impacting their accuracy over time. Our results indicate that LLMs demonstrate potential for being considered as automated evaluators for bug report summarization, which could allow scaling up evaluations while reducing human evaluators effort and fatigue.", "year": 2024, "publicationdate": "2024-09-01", "externalids": {"DOI": "10.48550/arXiv.2409.00630"}, "doi_lower": "10.48550/arxiv.2409.00630"}
{"paper_id": 278664751, "title": "RewardBench: Evaluating Reward Models for Language Modeling", "author_names": ["Nathan Lambert", "Valentina Pyatkin", "Jacob Daniel Morrison", "Lester James V. Miranda", "Bill Yuchen Lin", "K. Chandu", "Nouha Dziri", "Sachin Kumar", "Tom Zick", "Yejin Choi", "Noah A. Smith", "Hanna Hajishirzi"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": null, "year": 2025, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2025.findings-naacl.96"}, "doi_lower": "10.18653/v1/2025.findings-naacl.96"}
{"paper_id": 259837093, "title": "Can Large Language Models Aid in Annotating Speech Emotional Data? Uncovering New Frontiers [Research Frontier]", "author_names": ["S. Latif", "Muhammad Usama", "Mohammad Ibrahim Malik", "Björn Schuller"], "venue": "IEEE Computational Intelligence Magazine", "abstract": "Despite recent advancements in speech emotion recognition (SER) models, state-of-the-art deep learning (DL) approaches face the challenge of the limited availability of annotated data. The advent of large language models (LLMs) has revolutionised our understanding of natural language, introducing emergent properties that broaden comprehension in language, speech, and vision. This paper explores the potential of LLMs, such as ChatGPT, to annotate abundant speech data with the goal of advancing the state-of-the-art in SER. Specifically, it proposes a method that integrates audio representations and gender information with textual prompts to enhance the annotation process using LLMs. Our evaluation encompasses single-shot and few-shots scenarios, revealing performance variability in SER. Notably, this work achieves improved results through data augmentation by incorporating ChatGPT-annotated samples into the existing datasets. Our work also uncovers new frontiers in speech emotion classification, highlighting the increasing significance of LLMs in this field moving forward.", "year": 2023, "publicationdate": "2023-07-12", "externalids": {"DOI": "10.1109/MCI.2024.3504833"}, "doi_lower": "10.1109/mci.2024.3504833"}
{"paper_id": 269137394, "title": "Overview of the TREC 2023 NeuCLIR Track", "author_names": ["Dawn J Lawrie", "Sean MacAvaney", "J. Mayfield", "Paul McNamee", "Douglas W. Oard", "Luca Soldaini", "Eugene Yang"], "venue": "Text Retrieval Conference", "abstract": "The principal goal of the TREC Neural Cross-Language Information Retrieval (NeuCLIR) track is to study the impact of neural approaches to cross-language information retrieval. The track has created four collections, large collections of Chinese, Persian, and Russian newswire and a smaller collection of Chinese scientific abstracts. The principal tasks are ranked retrieval of news in one of the three languages, using English topics. Results for a multilingual task, also with English topics but with documents from all three newswire collections, are also reported. New in this second year of the track is a pilot technical documents CLIR task for ranked retrieval of Chinese technical documents using English topics. A total of 220 runs across all tasks were submitted by six participating teams and, as baselines, by track coordinators. Task descriptions and results are presented.", "year": 2024, "publicationdate": "2024-04-11", "externalids": {"DOI": "10.48550/arXiv.2404.08071"}, "doi_lower": "10.48550/arxiv.2404.08071"}
{"paper_id": 249847889, "title": "Query-Efficient and Scalable Black-Box Adversarial Attacks on Discrete Sequential Data via Bayesian Optimization", "author_names": ["Deokjae Lee", "Seungyong Moon", "Junhyeok Lee", "Hyun Oh Song"], "venue": "International Conference on Machine Learning", "abstract": "We focus on the problem of adversarial attacks against models on discrete sequential data in the black-box setting where the attacker aims to craft adversarial examples with limited query access to the victim model. Existing black-box attacks, mostly based on greedy algorithms, find adversarial examples using pre-computed key positions to perturb, which severely limits the search space and might result in suboptimal solutions. To this end, we propose a query-efficient black-box attack using Bayesian optimization, which dynamically computes important positions using an automatic relevance determination (ARD) categorical kernel. We introduce block decomposition and history subsampling techniques to improve the scalability of Bayesian optimization when an input sequence becomes long. Moreover, we develop a post-optimization algorithm that finds adversarial examples with smaller perturbation size. Experiments on natural language and protein classification tasks demonstrate that our method consistently achieves higher attack success rate with significant reduction in query count and modification rate compared to the previous state-of-the-art methods.", "year": 2022, "publicationdate": "2022-06-17", "externalids": {"DOI": "10.48550/arXiv.2206.08575"}, "doi_lower": "10.48550/arxiv.2206.08575"}
{"paper_id": 261493811, "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback", "author_names": ["Harrison Lee", "Samrat Phatale", "Hassan Mansoor", "Kellie Lu", "Thomas Mesnard", "Colton Bishop", "Victor Carbune", "Abhinav Rastogi"], "venue": "International Conference on Machine Learning", "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards\"self-improvement\"by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.", "year": 2023, "publicationdate": "2023-09-01", "externalids": {}, "doi_lower": null}
{"paper_id": 266977481, "title": "Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation", "author_names": ["Seongyun Lee", "Seungone Kim", "Sue Hyun Park", "Geewook Kim", "Minjoon Seo"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model at https://github.com/kaistAI/prometheus-vision", "year": 2024, "publicationdate": "2024-01-12", "externalids": {"DOI": "10.48550/arXiv.2401.06591"}, "doi_lower": "10.48550/arxiv.2401.06591"}
{"paper_id": 267751124, "title": "Aligning Large Language Models by On-Policy Self-Judgment", "author_names": ["Sangkyu Lee", "Sungdong Kim", "Ashkan Yousefpour", "Minjoon Seo", "Kang Min Yoo", "Youngjae Yu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Existing approaches for aligning large language models with human preferences face a trade-off that requires a separate reward model (RM) for on-policy learning. In this paper, we present a novel alignment framework, SELF-JUDGE that (1) does on-policy learning and 2) is parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model to act as both a policy and a judge. Specifically, we view the pairwise judgment task, choosing the better response from a response pair, as a special case of the instruction-following task. The resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines in preference benchmarks. We also show that the rejecting sampling by itself can improve performance further without an additional evaluator.", "year": 2024, "publicationdate": "2024-02-17", "externalids": {"DOI": "10.48550/arXiv.2402.11253"}, "doi_lower": "10.48550/arxiv.2402.11253"}
{"paper_id": 270703442, "title": "RecExplainer: Aligning Large Language Models for Explaining Recommendation Models", "author_names": ["Yuxuan Lei", "Jianxun Lian", "Jing Yao", "Xu Huang", "Defu Lian", "Xing Xie"], "venue": "Knowledge Discovery and Data Mining", "abstract": "Recommender systems are widely used in online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often function as a black box, making them less transparent and reliable for both users and developers. Recently, large language models (LLMs) have demonstrated remarkable intelligence in understanding, reasoning, and instruction following. This paper presents the initial exploration of using LLMs as surrogate models to explaining black-box recommender models. The primary concept involves training LLMs to comprehend and emulate the behavior of target recommender models. By leveraging LLMs' own extensive world knowledge and multi-step reasoning abilities, these aligned LLMs can serve as advanced surrogates, capable of reasoning about observations. Moreover, employing natural language as an interface allows for the creation of customizable explanations that can be adapted to individual user preferences. To facilitate an effective alignment, we introduce three methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to mimic the target model's behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model's behavior; hybrid alignment combines both language and latent spaces. Comprehensive experiments conducted on three public datasets show that our approach yields promising results in understanding and mimicking target models, producing high-quality, high-fidelity, and distinct explanations. Our code is available at https://github.com/microsoft/RecAI.", "year": 2023, "publicationdate": "2023-11-18", "externalids": {"DOI": "10.1145/3637528.3671802"}, "doi_lower": "10.1145/3637528.3671802"}
{"paper_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "author_names": ["Patrick Lewis", "Ethan Perez", "Aleksandara Piktus", "F. Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Kuttler", "M. Lewis", "Wen-tau Yih", "Tim Rocktäschel", "Sebastian Riedel", "Douwe Kiela"], "venue": "Neural Information Processing Systems", "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.", "year": 2020, "publicationdate": "2020-05-22", "externalids": {}, "doi_lower": null}
{"paper_id": 279594528, "title": "Automatic Evaluation for Mental Health Counseling using LLMs", "author_names": ["Anqi Li", "Yu Lu", "Nirui Song", "Shuai Zhang", "Lizhi Ma", "Zhenzhong Lan"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2402.11958"}, "doi_lower": "10.48550/arxiv.2402.11958"}
{"paper_id": 273501717, "title": "CalibraEval: Calibrating Prediction Distribution to Mitigate Selection Bias in LLMs-as-Judges", "author_names": ["Haitao Li", "Junjie Chen", "Qingyao Ai", "Zhumin Chu", "Yujia Zhou", "Qian Dong", "Yiqun Liu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The use of large language models (LLMs) as automated evaluation tools to assess the quality of generated natural language, known as LLMs-as-Judges, has demonstrated promising capabilities and is rapidly gaining widespread attention. However, when applied to pairwise comparisons of candidate responses, LLM-based evaluators often exhibit selection bias. Specifically, their judgments may become inconsistent when the option positions or ID tokens are swapped, compromising the effectiveness and fairness of the evaluation result. To address this challenge, we introduce CalibraEval, a novel label-free method for mitigating selection bias during inference. Specifically, CalibraEval reformulates debiasing as an optimization task aimed at adjusting observed prediction distributions to align with unbiased prediction distributions. To solve this optimization problem, we propose a non-parametric order-preserving algorithm (NOA). This algorithm leverages the partial order relationships between model prediction distributions, thereby eliminating the need for explicit labels and precise mathematical function modeling.Empirical evaluations of LLMs in multiple representative benchmarks demonstrate that CalibraEval effectively mitigates selection bias and improves performance compared to existing debiasing methods. This work marks a step toward building more robust and unbiased automated evaluation frameworks, paving the way for improved reliability in AI-driven assessments", "year": 2024, "publicationdate": "2024-10-20", "externalids": {"DOI": "10.48550/arXiv.2410.15393"}, "doi_lower": "10.48550/arxiv.2410.15393"}
{"paper_id": 272987186, "title": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models", "author_names": ["Haitao Li", "You Chen", "Qingyao Ai", "Yueyue Wu", "Ruizhe Zhang", "Yiqun Liu"], "venue": "Neural Information Processing Systems", "abstract": "Large language models (LLMs) have made significant progress in natural language processing tasks and demonstrate considerable potential in the legal domain. However, legal applications demand high standards of accuracy, reliability, and fairness. Applying existing LLMs to legal systems without careful evaluation of their potential and limitations could pose significant risks in legal practice. To this end, we introduce a standardized comprehensive Chinese legal benchmark LexEval. This benchmark is notable in the following three aspects: (1) Ability Modeling: We propose a new taxonomy of legal cognitive abilities to organize different tasks. (2) Scale: To our knowledge, LexEval is currently the largest Chinese legal evaluation dataset, comprising 23 tasks and 14,150 questions. (3) Data: we utilize formatted existing datasets, exam datasets and newly annotated datasets by legal experts to comprehensively evaluate the various capabilities of LLMs. LexEval not only focuses on the ability of LLMs to apply fundamental legal knowledge but also dedicates efforts to examining the ethical issues involved in their application. We evaluated 38 open-source and commercial LLMs and obtained some interesting findings. The experiments and findings offer valuable insights into the challenges and potential solutions for developing Chinese legal systems and LLM evaluation pipelines. The LexEval dataset and leaderboard are publicly available at \\url{https://github.com/CSHaitao/LexEval} and will be continuously updated.", "year": 2024, "publicationdate": "2024-09-30", "externalids": {"DOI": "10.48550/arXiv.2409.20288"}, "doi_lower": "10.48550/arxiv.2409.20288"}
{"paper_id": 264490404, "title": "LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset", "author_names": ["Haitao Li", "Yunqiu Shao", "Yueyue Wu", "Qingyao Ai", "Yixiao Ma", "Yiqun Liu"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "As an important component of intelligent legal systems, legal case retrieval plays a critical role in ensuring judicial justice and fairness. However, the development of legal case retrieval technologies in the Chinese legal system is restricted by three problems in existing datasets: limited data size, narrow definitions of legal relevance, and naive candidate pooling strategies used in data sampling. To alleviate these issues, we introduce LeCaRDv2, a large-scale Legal Case Retrieval Dataset (version 2). It consists of 800 queries and 55,192 candidates extracted from 4.3 million criminal case documents. To the best of our knowledge, LeCaRDv2 is one of the largest Chinese legal case retrieval datasets, providing extensive coverage of criminal charges. Additionally, we enrich the existing relevance criteria by considering three key aspects: characterization, penalty, procedure. This comprehensive criteria enriches the dataset and may provides a more holistic perspective. Furthermore, we propose a two-level candidate set pooling strategy that effectively identify potential candidates for each query case. It's important to note that all cases in the dataset have been annotated by multiple legal experts specializing in criminal law. Their expertise ensures the accuracy and reliability of the annotations. We evaluate several state-of-the-art retrieval models at LeCaRDv2, demonstrating that there is still significant room for improvement in legal case retrieval. The details of LeCaRDv2 can be found at the anonymous website https://github.com/THUIR/LeCaRDv2.", "year": 2023, "publicationdate": "2023-10-26", "externalids": {"DOI": "10.1145/3626772.3657887"}, "doi_lower": "10.1145/3626772.3657887"}
{"paper_id": 263829791, "title": "Generative Judge for Evaluating Alignment", "author_names": ["Junlong Li", "Shichao Sun", "Weizhe Yuan", "Run-Ze Fan", "Hai Zhao", "Pengfei Liu"], "venue": "International Conference on Learning Representations", "abstract": "The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regarding generality (i.e., assessing performance across diverse scenarios), flexibility (i.e., examining under different protocols), and interpretability (i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters, Auto-J, designed to address these challenges. Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. To demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. Experimentally, Auto-J outperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. We also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://github.com/GAIR-NLP/auto-j.", "year": 2023, "publicationdate": "2023-10-09", "externalids": {"DOI": "10.48550/arXiv.2310.05470"}, "doi_lower": "10.48550/arxiv.2310.05470"}
{"paper_id": 276647862, "title": "Re-evaluating Open-ended Evaluation of Large Language Models", "author_names": ["Siqi Liu", "Ian M. Gemp", "Luke Marris", "Georgios Piliouras", "N. Heess", "Marc Lanctot"], "venue": "International Conference on Learning Representations", "abstract": "Evaluation has traditionally focused on ranking candidates for a specific skill. Modern generalist models, such as Large Language Models (LLMs), decidedly outpace this paradigm. Open-ended evaluation systems, where candidate models are compared on user-submitted prompts, have emerged as a popular solution. Despite their many advantages, we show that the current Elo-based rating systems can be susceptible to and even reinforce biases in data, intentional or accidental, due to their sensitivity to redundancies. To address this issue, we propose evaluation as a 3-player game, and introduce novel game-theoretic solution concepts to ensure robustness to redundancy. We show that our method leads to intuitive ratings and provide insights into the competitive landscape of LLM development.", "year": 2025, "publicationdate": "2025-02-27", "externalids": {"DOI": "10.48550/arXiv.2502.20170"}, "doi_lower": "10.48550/arxiv.2502.20170"}
{"paper_id": 259360619, "title": "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations", "author_names": ["Ruosen Li", "Teerth Patel", "Xinya Du"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Nowadays, the quality of responses generated by different modern large language models (LLMs) is hard to evaluate and compare automatically. Recent studies suggest and predominantly use LLMs for reference-free evaluation of open-ended question answering. More specifically, they use the recognized\"strongest\"LLM as the evaluator, which conducts pairwise comparisons of candidate models' answers and provides a ranking score. However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias. We draw insights and lessons from the educational domain (Cho&MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifically, we propose (1) the peer rank (PR) algorithm that takes into account each peer LLM's pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on the preferences of two answers. We conduct experiments on two benchmark datasets. We find that our approaches achieve higher accuracy and align better with human judgments. Interestingly, PR can induce a relatively accurate self-ranking of models under the anonymous setting, where each model's name is unrevealed. Our work provides space to explore evaluating models that are hard to compare for humans.", "year": 2023, "publicationdate": "2023-07-06", "externalids": {"DOI": "10.48550/arXiv.2307.02762"}, "doi_lower": "10.48550/arxiv.2307.02762"}
{"paper_id": 11267601, "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset", "author_names": ["Yanran Li", "Hui Su", "Xiaoyu Shen", "Wenjie Li", "Ziqiang Cao", "Shuzi Niu"], "venue": "International Joint Conference on Natural Language Processing", "abstract": "We develop a high-quality multi-turn dialog dataset, DailyDialog, which is intriguing in several aspects. The language is human-written and less noisy. The dialogues in the dataset reflect our daily communication way and cover various topics about our daily life. We also manually label the developed dataset with communication intention and emotion information. Then, we evaluate existing approaches on DailyDialog dataset and hope it benefit the research field of dialog systems. The dataset is available on http://yanran.li/dailydialog", "year": 2017, "publicationdate": "2017-10-01", "externalids": {}, "doi_lower": null}
{"paper_id": 274597511, "title": "Split and Merge: Aligning Position Biases in Large Language Model based Evaluators", "author_names": ["Zongjie Li", "Chaozheng Wang", "Pingchuan Ma", "Daoyuan Wu", "Shuai Wang", "Cuiyun Gao", "Yang Liu"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2310.01432"}, "doi_lower": "10.48550/arxiv.2310.01432"}
{"paper_id": 271875000, "title": "Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM", "author_names": ["Jingcong Liang", "Rong Ye", "Meng Han", "Ruofei Lai", "Xinyu Zhang", "Xuanjing Huang", "Zhongyu Wei"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2403.08010"}, "doi_lower": "10.48550/arxiv.2403.08010"}
{"paper_id": 273901484, "title": "ABSEval: An Agent-based Framework for Script Evaluation", "author_names": ["Sirui Liang", "Baoli Zhang", "Jun Zhao", "Kang Liu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent research indicates that large language models (LLMs) possess a certain degree of script planning capability. However, there is still a lack of focused work on evaluating scripts generated by LLMs. The evaluation of scripts poses challenges due to their logical structure, sequential organization, adherence to commonsense constraints, and open-endedness. In this work, We introduced a novel script evaluation dataset, MCScript, consisting of more than 1,500 script evaluation tasks and steps, and developed an agent-based script evaluation framework, ABSEval, to collaboratively evaluate scripts generated by LLMs. Our experiments demonstrate that ABSEval provides superior accuracy and relevance, aligning closely with human evaluation. We evaluated the script planning capabilities of 15 mainstream LLMs and provided a detailed analysis. Furthermore, we observed phenomena like the key factor influencing the script planning ability of LLM is not parameter size and suggested improvements for evaluating open-ended questions.", "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2024.emnlp-main.691"}, "doi_lower": "10.18653/v1/2024.emnlp-main.691"}
{"paper_id": 258987659, "title": "Let's Verify Step by Step", "author_names": ["H. Lightman", "Vineet Kosaraju", "Yura Burda", "Harrison Edwards", "Bowen Baker", "Teddy Lee", "Jan Leike", "John Schulman", "I. Sutskever", "K. Cobbe"], "venue": "International Conference on Learning Representations", "abstract": "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.", "year": 2023, "publicationdate": "2023-05-31", "externalids": {"DOI": "10.48550/arXiv.2305.20050"}, "doi_lower": "10.48550/arxiv.2305.20050"}
{"paper_id": 270357771, "title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild", "author_names": ["Bill Yuchen Lin", "Yuntian Deng", "K. Chandu", "Faeze Brahman", "Abhilasha Ravichander", "Valentina Pyatkin", "Nouha Dziri", "Ronan Le Bras", "Yejin Choi"], "venue": "International Conference on Learning Representations", "abstract": "We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of ``slightly better/worse'' to ``tie'' if the winner response exceeds the loser one by more than $K$ characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard's 0.91 and AlpacaEval2.0's 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates.", "year": 2024, "publicationdate": "2024-06-07", "externalids": {"DOI": "10.48550/arXiv.2406.04770"}, "doi_lower": "10.48550/arxiv.2406.04770"}
{"paper_id": 964287, "title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author_names": ["Chin-Yew Lin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": null, "year": 2004, "publicationdate": "2004-07-25", "externalids": {}, "doi_lower": null}
{"paper_id": 237532606, "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods", "author_names": ["Stephanie C. Lin", "Jacob Hilton", "Owain Evans"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.", "year": 2021, "publicationdate": "2021-09-08", "externalids": {"DOI": "10.18653/v1/2022.acl-long.229"}, "doi_lower": "10.18653/v1/2022.acl-long.229"}
{"paper_id": 258841681, "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models", "author_names": ["Yen-Ting Lin", "Yun-Nung (Vivian) Chen"], "venue": "NLP4CONVAI", "abstract": "We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.13711"}, "doi_lower": "10.48550/arxiv.2305.13711"}
{"paper_id": 265212670, "title": "X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects", "author_names": ["Minqian Liu", "Ying Shen", "Zhiyang Xu", "Yixin Cao", "Eunah Cho", "Vaibhav Kumar", "Reza Ghanadan", "Lifu Huang"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Natural Language Generation (NLG) typically involves evaluating the generated text in various aspects (e.g., consistency and naturalness) to obtain a comprehensive assessment. However, multi-aspect evaluation remains challenging as it may require the evaluator to generalize to any given evaluation aspect even if it’s absent during training. In this paper, we introduce X-Eval, a two-stage instruction tuning framework to evaluate text in both seen and unseen aspects customized by end users. X-Eval consists of two learning stages: the vanilla instruction tuning stage that improves the model’s ability to follow evaluation instructions, and an enhanced instruction tuning stage that exploits the connections between fine-grained evaluation aspects to better assess text quality. To support the training of X-Eval, we collect AspectInstruct, the first instruction tuning dataset tailored for multi-aspect NLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance task diversity, we devise an augmentation strategy that converts human rating annotations into diverse forms of NLG evaluation tasks, including scoring, comparison, ranking, and Boolean question answering. Extensive experiments across three essential categories of NLG tasks: dialogue generation, summarization, and data-to-text coupled with 21 aspects in meta-evaluation, demonstrate that X-Eval enables even a lightweight language model to achieve a comparable if not higher correlation with human judgments compared to the state-of-the-art NLG evaluators like GPT-4.", "year": 2023, "publicationdate": "2023-11-15", "externalids": {"DOI": "10.48550/arXiv.2311.08788"}, "doi_lower": "10.48550/arxiv.2311.08788"}
{"paper_id": 264958163, "title": "Reply to arXiv:1211.3957 and arXiv:1211.4731 by Leader et al. and arXiv:1212.0761 by Harindranath et al", "author_names": ["Xiangdong Ji", "Xiaonu Xiong", "Feng Yuan"], "venue": "", "abstract": null, "year": 2013, "publicationdate": "2013-04-03", "externalids": {}, "doi_lower": null}
{"paper_id": 265506600, "title": "AlignBench: Benchmarking Chinese Alignment of Large Language Models", "author_names": ["Xiao Liu", "Xuanyu Lei", "Shengyuan Wang", "Yue Huang", "Zhuoer Feng", "Bosi Wen", "Jiale Cheng", "Pei Ke", "Yifan Xu", "W. Tam", "Xiaohan Zhang", "Lichao Sun", "Hongning Wang", "Jing Zhang", "Minlie Huang", "Yuxiao Dong", "Jie Tang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, the effective evaluation of alignment for emerging Chinese LLMs is still largely unexplored. To fill in this gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark for evaluating LLMs' alignment in Chinese. We design a human-in-the-loop data curation pipeline, containing eight main categories, 683 real-scenario rooted queries and corresponding human verified references. To ensure the correctness of references, each knowledge-intensive query is accompanied with evidences collected from reliable web sources (including URLs and quotations) by our annotators. For automatic evaluation, our benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge~\\cite{zheng2023judging} approach with Chain-of-Thought to generate explanations and final ratings, ensuring high reliability and interpretability. All evaluation code, data, and LLM generations are available at \\url{https://github.com/THUDM/AlignBench}. Since its release, AlignBench has been adopted by top (Chinese) LLMs for evaluating their alignment capabilities in Chinese, including ChatGLM, Qwen, DeepSeek, Yi, Baichuan, and Abab.", "year": 2023, "publicationdate": "2023-11-30", "externalids": {"DOI": "10.48550/arXiv.2311.18743"}, "doi_lower": "10.48550/arxiv.2311.18743"}
{"paper_id": 260682249, "title": "AgentBench: Evaluating LLMs as Agents", "author_names": ["Xiao Liu", "Hao Yu", "Hanchen Zhang", "Yifan Xu", "Xuanyu Lei", "Hanyu Lai", "Yu Gu", "Yuxian Gu", "Hangliang Ding", "Kai Men", "Kejuan Yang", "Shudan Zhang", "Xiang Deng", "Aohan Zeng", "Zhengxiao Du", "Chenhui Zhang", "Shengqi Shen", "Tianjun Zhang", "Sheng Shen", "Yu Su", "Huan Sun", "Minlie Huang", "Yuxiao Dong", "Jie Tang"], "venue": "International Conference on Learning Representations", "abstract": "The potential of Large Language Model (LLM) as agents has been widely acknowledged recently. Thus, there is an urgent need to quantitatively \\textit{evaluate LLMs as agents} on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities. Our extensive test over \\num API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Improving instruction following and training on high quality multi-round alignment data could improve agent performance. And different from existing assumptions, training on code present ambivalent impacts on different agent tasks. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.", "year": 2023, "publicationdate": "2023-08-07", "externalids": {"DOI": "10.48550/arXiv.2308.03688"}, "doi_lower": "10.48550/arxiv.2308.03688"}
{"paper_id": 257804696, "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment", "author_names": ["Yang Liu", "Dan Iter", "Yichong Xu", "Shuo Wang", "Ruochen Xu", "Chenguang Zhu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval", "year": 2023, "publicationdate": "2023-03-29", "externalids": {"DOI": "10.18653/v1/2023.emnlp-main.153"}, "doi_lower": "10.18653/v1/2023.emnlp-main.153"}
{"paper_id": 262464745, "title": "Calibrating LLM-Based Evaluator", "author_names": ["Yuxuan Liu", "Tianchi Yang", "Shaohan Huang", "Zihan Zhang", "Haizhen Huang", "Furu Wei", "Weiwei Deng", "Feng Sun", "Qi Zhang"], "venue": "International Conference on Language Resources and Evaluation", "abstract": "Recent advancements in large language models (LLMs) and their emergent capabilities make LLM a promising reference-free evaluator on the quality of natural language generation, and a competent alternative to human evaluation. However, hindered by the closed-source or high computational demand to host and tune, there is a lack of practice to further calibrate an off-the-shelf LLM-based evaluator towards better human alignment. In this work, we propose AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an LLM-based evaluator toward human preference. Instead of explicitly modeling human preferences, we first implicitly encompass them within a set of human labels. Then, an initial set of scoring criteria is drafted by the language model itself, leveraging in-context learning on different few-shot examples. To further calibrate this set of criteria, we select the best performers and re-draft them with self-refinement. Our experiments on multiple text quality evaluation datasets illustrate a significant improvement in correlation with expert evaluation through calibration. Our comprehensive qualitative analysis conveys insightful intuitions and observations on the essence of effective scoring criteria.", "year": 2023, "publicationdate": "2023-09-23", "externalids": {"DOI": "10.48550/arXiv.2309.13308"}, "doi_lower": "10.48550/arxiv.2309.13308"}
{"paper_id": 267938738, "title": "HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition", "author_names": ["Yuxuan Liu", "Tianchi Yang", "Shaohan Huang", "Zihan Zhang", "Haizhen Huang", "Furu Wei", "Weiwei Deng", "Feng Sun", "Qi Zhang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large language models (LLMs) have emerged as a promising alternative to expensive human evaluations. However, the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria. To address this challenge, we propose HD-Eval, a novel framework that iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. HD-Eval inherits the essence from the evaluation mindset of human experts and enhances the alignment of LLM-based evaluators by decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria. By integrating these steps within an iterative alignment training process, we obtain a hierarchical decomposition of criteria that comprehensively captures aspects of natural language at multiple levels of granularity. Implemented as a white box, the human preference-guided aggregator is efficient to train and more explainable than relying solely on prompting, and its independence from model parameters makes it applicable to closed-source LLMs. Extensive experiments on three evaluation domains demonstrate the superiority of HD-Eval in further aligning state-of-the-art evaluators and providing deeper insights into the explanation of evaluation results and the task itself.", "year": 2024, "publicationdate": "2024-02-24", "externalids": {"DOI": "10.48550/arXiv.2402.15754"}, "doi_lower": "10.48550/arxiv.2402.15754"}
{"paper_id": 273507377, "title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style", "author_names": ["Yantao Liu", "Zijun Yao", "Rui Min", "Yixin Cao", "Lei Hou", "Juanzi Li"], "venue": "International Conference on Learning Representations", "abstract": "Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance. To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively. We evaluate nearly 40 reward models on RM-Bench. Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference. These findings highlight the significant room for improvement in current reward models. Related code and data are available at https://github.com/THU-KEG/RM-Bench.", "year": 2024, "publicationdate": "2024-10-21", "externalids": {"DOI": "10.48550/arXiv.2410.16184"}, "doi_lower": "10.48550/arxiv.2410.16184"}
{"paper_id": 268681113, "title": "Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators", "author_names": ["Yinhong Liu", "Han Zhou", "Zhijiang Guo", "Ehsan Shareghi", "Ivan Vulic", "Anna Korhonen", "Nigel Collier"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human evaluation, revealing that existing calibration methods aimed at mitigating biases of LLMs are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search-based rank aggregation method that employs LLMs to conduct pairwise comparisons locally and efficiently ranks candidate texts globally. PAIRS achieves state-of-the-art performance on representative evaluation tasks in long-form generations and demonstrates significant improvements over direct scoring. Furthermore, we provide insights into the role of pairwise preference in quantifying the transitivity of LLMs and demonstrate how PAIRS benefits from calibration using debiased pairwise evaluations.", "year": 2024, "publicationdate": "2024-03-25", "externalids": {"DOI": "10.48550/arXiv.2403.16950"}, "doi_lower": "10.48550/arxiv.2403.16950"}
{"paper_id": 269635640, "title": "Efficient LLM Comparative Assessment: A Product of Experts Framework for Pairwise Comparisons", "author_names": ["Adian Liusie", "Vatsal Raina", "Yassir Fathullah", "Mark J. F. Gales"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "LLM-as-a-judge approaches are a practical and effective way of assessing a range of text tasks. However, when using pairwise comparisons to rank a set of candidates, the computational cost scales quadratically with the number of candidates, which has practical limitations. This paper introduces a Product of Expert (PoE) framework for efficient LLM Comparative Assessment. Here individual comparisons are considered experts that provide information on a pair’s score difference. The PoE framework combines the information from these experts to yield an expression that can be maximized with respect to the underlying set of candidates, and is highly flexible where any form of expert can be assumed. When Gaussian experts are used one can derive simple closed-form solutions for the optimal candidate ranking, as well as expressions for selecting which comparisons should be made to maximize the probability of this ranking. Our approach enables efficient comparative assessment, where by using only a small subset of the possible comparisons, one can generate score predictions that correlate well with human judgements. We evaluate the approach on multiple NLG tasks and demonstrate that our framework can yield considerable computational savings when performing pairwise comparative assessment. With many candidate texts, using as few as 2% of comparisons the PoE solution can achieve similar performance to when all comparisons are used.", "year": 2024, "publicationdate": "2024-05-09", "externalids": {"DOI": "10.48550/arXiv.2405.05894"}, "doi_lower": "10.48550/arxiv.2405.05894"}
{"paper_id": 273798760, "title": "Juding the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative LLM-as-a-Judge", "author_names": ["Lin Shi", "Chiyu Ma", "Wenhua Liang", "Xingjian Diao", "Weicheng Ma", "Soroush Vosoughi"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 261031244, "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-Tuning", "author_names": ["Yun Luo", "Zhen Yang", "Fandong Meng", "Yafu Li", "Jie Zhou", "Yue Zhang"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "abstract": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information while acquiring new knowledge for achieving satisfactory performance in downstream tasks. As large language models (LLMs) have demonstrated remarkable performance, it is intriguing to investigate whether CF exists during the continual instruction tuning of LLMs. This study empirically evaluates the forgetting phenomenon in LLMs’ knowledge during continual instruction tuning from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1 b to 7 b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model scale range, which may result from the much more significant initial performance in the larger LLM. The finding is also observed by the experiment of Qwen-2.5-Inst from 3 B to 14 B. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less forgetting and retains more knowledge. Interestingly, we also observe that LLMs can mitigate language biases, such as gender bias, during continual fine-tuning. Furthermore, our findings indicate that general instruction tuning can help alleviate the forgetting phenomenon in LLMs during subsequent fine-tuning.", "year": 2023, "publicationdate": "2023-08-17", "externalids": {"DOI": "10.1109/TASLPRO.2025.3606231"}, "doi_lower": "10.1109/taslpro.2025.3606231"}
{"paper_id": 274150456, "title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation", "author_names": ["Ziyang Luo", "Haoning Wu", "Dongxu Li", "Jing Ma", "Mohan S. Kankanhalli", "Junnan Li"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Large multimodal models (LMMs) with advanced video analysis capabilities have recently garnered significant attention. However, most evaluations rely on traditional methods like multiple-choice question answering in benchmarks such as VideoMME and LongVideoBench, which are prone to lack the depth needed to capture the complex demands of real-world users. To address this limitation—and due to the prohibitive cost and slow pace of human annotation for video tasks—we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS Chatbot Arena’s framework, designed to automatically assess LMMs’ video analysis abilities. VideoAutoArena utilizes user simulation to generate open-ended, adaptive questions that rigorously assess model performance in video understanding. The benchmark features an automated, scalable evaluation framework, incorporating a modified ELO Rating System for fair and continuous comparisons across multiple LMMs. To validate our automated judging system, we construct a \"gold standard\" using a carefully curated subset of human annotations, demonstrating that our arena strongly aligns with human judgment while maintaining scalability. Additionally, we introduce a fault-driven evolution strategy, progressively increasing question complexity to push models toward handling more challenging video analysis scenarios. Experimental results demonstrate that VideoAutoArena effectively differentiates among state-of-the-art LMMs, providing insights into model strengths and areas for improvement. To further streamline our evaluation, we introduce VideoAutoBench as an auxiliary benchmark, where human annotators label winners in a subset of VideoAutoArena battles. We use GPT-4o as a judge to compare responses against these human-validated answers. Together, VideoAu-Toarena and VideoAutoBench offer a cost-effective, and scalable framework for evaluating LMMs in user-centric video analysis.", "year": 2024, "publicationdate": "2024-11-20", "externalids": {"DOI": "10.1109/CVPR52734.2025.00792"}, "doi_lower": "10.1109/cvpr52734.2025.00792"}
{"paper_id": 268724165, "title": "Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval", "author_names": ["Shengjie Ma", "Chong Chen", "Qi Chu", "Jiaxin Mao"], "venue": "arXiv.org", "abstract": "Determining which legal cases are relevant to a given query involves navigating lengthy texts and applying nuanced legal reasoning. Traditionally, this task has demanded significant time and domain expertise to identify key Legal Facts and reach sound juridical conclusions. In addition, existing data with legal case similarities often lack interpretability, making it difficult to understand the rationale behind relevance judgments. With the growing capabilities of large language models (LLMs), researchers have begun investigating their potential in this domain. Nonetheless, the method of employing a general large language model for reliable relevance judgments in legal case retrieval remains largely unexplored. To address this gap in research, we propose a novel few-shot approach where LLMs assist in generating expert-aligned interpretable relevance judgments. The proposed approach decomposes the judgment process into several stages, mimicking the workflow of human annotators and allowing for the flexible incorporation of expert reasoning to improve the accuracy of relevance judgments. Importantly, it also ensures interpretable data labeling, providing transparency and clarity in the relevance assessment process. Through a comparison of relevance judgments made by LLMs and human experts, we empirically demonstrate that the proposed approach can yield reliable and valid relevance assessments. Furthermore, we demonstrate that with minimal expert supervision, our approach enables a large language model to acquire case analysis expertise and subsequently transfers this ability to a smaller model via annotation-based knowledge distillation.", "year": 2024, "publicationdate": "2024-03-27", "externalids": {"DOI": "10.48550/arXiv.2403.18405"}, "doi_lower": "10.48550/arxiv.2403.18405"}
{"paper_id": 257900871, "title": "Self-Refine: Iterative Refinement with Self-Feedback", "author_names": ["Aman Madaan", "Niket Tandon", "Prakhar Gupta", "Skyler Hallinan", "Luyu Gao", "Sarah Wiegreffe", "Uri Alon", "Nouha Dziri", "Shrimai Prabhumoye", "Yiming Yang", "S. Welleck", "Bodhisattwa Prasad Majumder", "Shashank Gupta", "A. Yazdanbakhsh", "Peter Clark"], "venue": "Neural Information Processing Systems", "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {"DOI": "10.48550/arXiv.2303.17651"}, "doi_lower": "10.48550/arxiv.2303.17651"}
{"paper_id": 218470058, "title": "USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation", "author_names": ["Shikib Mehri", "M. Eskénazi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research. Standard language generation metrics have been shown to be ineffective for evaluating dialog models. To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog. USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog. USR is shown to strongly correlate with human judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and PersonaChat (turn-level: 0.48 and system-level: 1.0). USR additionally produces interpretable measures for several desirable properties of dialog.", "year": 2020, "publicationdate": "2020-05-01", "externalids": {"DOI": "10.18653/v1/2020.acl-main.64"}, "doi_lower": "10.18653/v1/2020.acl-main.64"}
{"paper_id": 271909334, "title": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs", "author_names": ["John Mendonça", "Isabel Trancoso", "A. Lavie"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Although human evaluation remains the gold standard for open-domain dialogue evaluation, the growing popularity of automated evaluation using Large Language Models (LLMs) has also extended to dialogue. However, most frameworks leverage benchmarks that assess older chatbots on aspects such as fluency and relevance, which are not reflective of the challenges associated with contemporary models. In fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset, suggests that current chatbots may exhibit several recurring issues related to coherence and commonsense knowledge, but generally produce highly fluent and relevant responses. Noting the aforementioned limitations, this paper introduces Soda-Eval, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, where the annotations were generated by GPT-4. Using Soda-Eval as a benchmark, we then study the performance of several open-access instruction-tuned LLMs, finding that dialogue evaluation remains challenging. Fine-tuning these models improves performance over few-shot inferences, both in terms of correlation and explanation.", "year": 2024, "publicationdate": "2024-08-20", "externalids": {"DOI": "10.48550/arXiv.2408.10902"}, "doi_lower": "10.48550/arxiv.2408.10902"}
{"paper_id": 270558946, "title": "Evaluating the Performance of Large Language Models via Debates", "author_names": ["Behrad Moniri", "Hamed Hassani", "Edgar Dobriban"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Large Language Models (LLMs) are rapidly evolving and impacting various fields, necessitating the development of effective methods to evaluate and compare their performance. Most current approaches for performance evaluation are either based on fixed, domain-specific questions that lack the flexibility required in many real-world applications, or rely on human input, making them unscalable. To address these issues, we propose an automated benchmarking framework based on debates between LLMs, judged by another LLM. This method assesses not only domain knowledge, but also skills such as argumentative reasoning and inconsistency recognition. We evaluate the performance of various state-of-the-art LLMs using the debate framework and achieve rankings that align closely with popular rankings based on human input, eliminating the need for costly human crowdsourcing.", "year": 2024, "publicationdate": "2024-06-16", "externalids": {"DOI": "10.48550/arXiv.2406.11044"}, "doi_lower": "10.48550/arxiv.2406.11044"}
{"paper_id": 1726501, "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories", "author_names": ["N. Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James F. Allen"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.", "year": 2016, "publicationdate": "2016-04-06", "externalids": {"DOI": "10.18653/v1/N16-1098"}, "doi_lower": "10.18653/v1/n16-1098"}
{"paper_id": 269484318, "title": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation", "author_names": ["Giorgio Franceschelli", "Mirco Musolesi"], "venue": "International Conference on Innovative Computing and Cloud Computing", "abstract": "Large language models are revolutionizing several areas, including artificial creativity. However, the process of generation in machines profoundly diverges from that observed in humans. In particular, machine generation is characterized by a lack of intentionality and an underlying creative process. We propose a method called Creative Beam Search that uses Diverse Beam Search and LLM-as-a-Judge to perform response generation and response validation. The results of a qualitative experiment show how our approach can provide better output than standard sampling techniques. We also show that the response validation step is a necessary complement to the response generation step.", "year": 2024, "publicationdate": "2024-04-30", "externalids": {"DOI": "10.48550/arXiv.2405.00099"}, "doi_lower": "10.48550/arxiv.2405.00099"}
{"paper_id": 270379906, "title": "Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena", "author_names": ["Aidar Myrzakhan", "S. Mahmoud Bsharat", "Zhiqiang Shen"], "venue": "arXiv.org", "abstract": "Multiple-choice questions (MCQ) are frequently used to assess large language models (LLMs). Typically, an LLM is given a question and selects the answer deemed most probable after adjustments for factors like length. Unfortunately, LLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to inherent biases of priori unbalanced probabilities, influencing the prediction of answers based on these IDs. Previous research has introduced methods to reduce this ''selection bias'' by simply permutating options on a few test samples and applying to new ones. Another problem of MCQ is the lottery ticket choice by ''random guessing''. The LLM does not learn particular knowledge, but the option is guessed correctly. This situation is especially serious for those small-scale LLMs. To address them, a more thorough approach involves shifting from MCQ to open-style questions, which can fundamentally eliminate selection bias and random guessing issues. However, transitioning causes its own set of challenges in (1) identifying suitable open-style questions and (2) validating the correctness of LLM open-style responses against human-annotated ground-truths. This work aims to tackle these significant difficulties, and establish a new LLM evaluation benchmark through entirely open-style questions. Consequently, we introduce the Open-LLM-Leaderboard to track various LLMs' performance and reflect true capability of them, such as GPT-4o/4/3.5, Claude 3, Gemini, etc. Our code and dataset are available at https://github.com/VILA-Lab/Open-LLM-Leaderboard.", "year": 2024, "publicationdate": "2024-06-11", "externalids": {"DOI": "10.48550/arXiv.2406.07545"}, "doi_lower": "10.48550/arxiv.2406.07545"}
{"paper_id": 215768182, "title": "Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization", "author_names": ["Shashi Narayan", "Shay B. Cohen", "Mirella Lapata"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "We introduce “extreme summarization”, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question “What is the article about?”. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article’s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.", "year": 2018, "publicationdate": "2018-08-27", "externalids": {"DOI": "10.18653/v1/D18-1206"}, "doi_lower": "10.18653/v1/d18-1206"}
{"paper_id": 273323719, "title": "JurEE not Judges: safeguarding llm interactions with small, specialised Encoder Ensembles", "author_names": ["Dom Nasrabadi"], "venue": "arXiv.org", "abstract": "We introduce JurEE, an ensemble of efficient, encoder-only transformer models designed to strengthen safeguards in AI-User interactions within LLM-based systems. Unlike existing LLM-as-Judge methods, which often struggle with generalization across risk taxonomies and only provide textual outputs, JurEE offers probabilistic risk estimates across a wide range of prevalent risks. Our approach leverages diverse data sources and employs progressive synthetic data generation techniques, including LLM-assisted augmentation, to enhance model robustness and performance. We create an in-house benchmark comprising of other reputable benchmarks such as the OpenAI Moderation Dataset and ToxicChat, where we find JurEE significantly outperforms baseline models, demonstrating superior accuracy, speed, and cost-efficiency. This makes it particularly suitable for applications requiring stringent content moderation, such as customer-facing chatbots. The encoder-ensemble's modular design allows users to set tailored risk thresholds, enhancing its versatility across various safety-related applications. JurEE's collective decision-making process, where each specialized encoder model contributes to the final output, not only improves predictive accuracy but also enhances interpretability. This approach provides a more efficient, performant, and economical alternative to traditional LLMs for large-scale implementations requiring robust content moderation.", "year": 2024, "publicationdate": "2024-10-11", "externalids": {"DOI": "10.48550/arXiv.2410.08442"}, "doi_lower": "10.48550/arxiv.2410.08442"}
{"paper_id": 20370792, "title": "Principles of Artificial Intelligence", "author_names": ["N. Nilsson"], "venue": "Symbolic computation", "abstract": null, "year": 1980, "publicationdate": "1980-01-23", "externalids": {"DOI": "10.1007/978-3-662-09438-9"}, "doi_lower": "10.1007/978-3-662-09438-9"}
{"paper_id": 278602965, "title": "PiCO: Peer Review in LLMs based on Consistency Optimization", "author_names": ["Kun-Peng Ning", "Shuo Yang", "Yuyang Liu", "Jia-Yu Yao", "Zhen-Hui Liu", "Yonghong Tian", "Yibing Song", "Li Yuan"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2025, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 273798418, "title": "JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking", "author_names": ["Tong Niu", "Shafiq Joty", "Ye Liu", "Caiming Xiong", "Yingbo Zhou", "Semih Yavuz"], "venue": "arXiv.org", "abstract": "Accurate document retrieval is crucial for the success of retrieval-augmented generation (RAG) applications, including open-domain question answering and code completion. While large language models (LLMs) have been employed as dense encoders or listwise rerankers in RAG systems, they often struggle with reasoning-intensive tasks because they lack nuanced analysis when judging document relevance. To address this limitation, we introduce JudgeRank, a novel agentic reranker that emulates human cognitive processes when assessing document relevance. Our approach consists of three key steps: (1) query analysis to identify the core problem, (2) document analysis to extract a query-aware summary, and (3) relevance judgment to provide a concise assessment of document relevance. We evaluate JudgeRank on the reasoning-intensive BRIGHT benchmark, demonstrating substantial performance improvements over first-stage retrieval methods and outperforming other popular reranking approaches. In addition, JudgeRank performs on par with fine-tuned state-of-the-art rerankers on the popular BEIR benchmark, validating its zero-shot generalization capability. Through comprehensive ablation studies, we demonstrate that JudgeRank's performance generalizes well across LLMs of various sizes while ensembling them yields even more accurate reranking than individual models.", "year": 2024, "publicationdate": "2024-10-31", "externalids": {"DOI": "10.48550/arXiv.2411.00142"}, "doi_lower": "10.48550/arxiv.2411.00142"}
{"paper_id": 264172681, "title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology", "author_names": ["Odhran O'Donoghue", "Aleksandar Shtedritski", "John Ginger", "Ralph Abboud", "Ali E. Ghareeb", "J. Booth", "Samuel G. Rodriques"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The ability to automatically generate accurate protocols for scientific experiments would represent a major step towards the automation of science. Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and code. However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments. Moreover, evaluation of the accuracy of scientific protocols is challenging, because experiments can be described correctly in many different ways, require expert knowledge to evaluate, and cannot usually be executed automatically. Here we present an automatic evaluation framework for the task of planning experimental protocols, and we introduce BioProt: a dataset of biology protocols with corresponding pseudocode representations. To measure performance on generating scientific protocols, we use an LLM to convert a natural language protocol into pseudocode, and then evaluate an LLM's ability to reconstruct the pseudocode from a high-level description and a list of admissible pseudocode functions. We evaluate GPT-3 and GPT-4 on this task and explore their robustness. We externally validate the utility of pseudocode representations of text by generating accurate novel protocols using retrieved pseudocode, and we run a generated protocol successfully in our biological laboratory. Our framework is extensible to the evaluation and improvement of language model planning abilities in other areas of science or other areas that lack automatic evaluation.", "year": 2023, "publicationdate": "2023-10-16", "externalids": {"DOI": "10.48550/arXiv.2310.10632"}, "doi_lower": "10.48550/arxiv.2310.10632"}
{"paper_id": 272827587, "title": "A Multi-LLM Debiasing Framework", "author_names": ["Deonna M. Owens", "Ryan A. Rossi", "Sungchul Kim", "Tong Yu", "Franck Dernoncourt", "Xiang Chen", "Ruiyi Zhang", "Jiuxiang Gu", "Hanieh Deilamsalehy", "Nedim Lipka"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) are powerful tools with the potential to benefit society immensely, yet, they have demonstrated biases that perpetuate societal inequalities. Despite significant advancements in bias mitigation techniques using data augmentation, zero-shot prompting, and model fine-tuning, biases continuously persist, including subtle biases that may elude human detection. Recent research has shown a growing interest in multi-LLM approaches, which have been demonstrated to be effective in improving the quality of reasoning and factuality in LLMs. Building on this approach, we propose a novel multi-LLM debiasing framework aimed at reducing bias in LLMs. Our work is the first to introduce and evaluate two distinct approaches within this framework for debiasing LLMs: a centralized method, where the conversation is facilitated by a single central LLM, and a decentralized method, where all models communicate directly. Our findings reveal that our multi-LLM framework significantly reduces bias in LLMs, outperforming the baseline method across several social groups.", "year": 2024, "publicationdate": "2024-09-20", "externalids": {"DOI": "10.48550/arXiv.2409.13884"}, "doi_lower": "10.48550/arxiv.2409.13884"}
{"paper_id": 233407441, "title": "Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics", "author_names": ["Artidoro Pagnoni", "Vidhisha Balachandran", "Yulia Tsvetkov"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses.", "year": 2021, "publicationdate": "2021-04-27", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.383"}, "doi_lower": "10.18653/v1/2021.naacl-main.383"}
{"paper_id": 271039905, "title": "Human-Centered Design Recommendations for LLM-as-a-judge", "author_names": ["Qian Pan", "Zahra Ashktorab", "Michael Desmond", "Martín Santillán Cooper", "James M. Johnson", "Rahul Nair", "Elizabeth M. Daly", "Werner Geyer"], "venue": "HUCLLM", "abstract": "Traditional reference-based metrics, such as BLEU and ROUGE, are less effective for assessing outputs from Large Language Models (LLMs) that produce highly creative or superior-quality text, or in situations where reference outputs are unavailable. While human evaluation remains an option, it is costly and difficult to scale. Recent work using LLMs as evaluators (LLM-as-a-judge) is promising, but trust and reliability remain a significant concern. Integrating human input is crucial to ensure criteria used to evaluate are aligned with the human’s intent, and evaluations are robust and consistent. This paper presents a user study of a design exploration called EvaluLLM, that enables users to leverage LLMs as customizable judges, promoting human involvement to balance trust and cost-saving potential with caution. Through interviews with eight domain experts, we identified the need for assistance in developing effective evaluation criteria aligning the LLM-as-a-judge with practitioners’ preferences and expectations. We offer findings and design recommendations to optimize human-assisted LLM-as-judge systems.", "year": 2024, "publicationdate": "2024-07-03", "externalids": {"DOI": "10.48550/arXiv.2407.03479"}, "doi_lower": "10.48550/arxiv.2407.03479"}
{"paper_id": 259165563, "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap", "author_names": ["Shirui Pan", "Linhao Luo", "Yufei Wang", "Chen Chen", "Jiapu Wang", "Xindong Wu"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia, and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and, simultaneously, leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely: 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.", "year": 2023, "publicationdate": "2023-06-14", "externalids": {"DOI": "10.1109/TKDE.2024.3352100"}, "doi_lower": "10.1109/tkde.2024.3352100"}
{"paper_id": 269293311, "title": "LLM Evaluators Recognize and Favor Their Own Generations", "author_names": ["Arjun Panickssery", "Samuel R. Bowman", "Shi Feng"], "venue": "Neural Information Processing Systems", "abstract": "Self-evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling, constitutional AI, and self-refinement. But new biases are introduced due to the same LLM acting as both the evaluator and the evaluatee. One such bias is self-preference, where an LLM evaluator scores its own outputs higher than others' while human annotators consider them of equal quality. But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence? In this paper, we investigate if self-recognition capability contributes to self-preference. We discover that, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from other LLMs and humans. By fine-tuning LLMs, we discover a linear correlation between self-recognition capability and the strength of self-preference bias; using controlled experiments, we show that the causal explanation resists straightforward confounders. We discuss how self-recognition can interfere with unbiased evaluations and AI safety more generally.", "year": 2024, "publicationdate": "2024-04-15", "externalids": {"DOI": "10.48550/arXiv.2404.13076"}, "doi_lower": "10.48550/arxiv.2404.13076"}
{"paper_id": 11080756, "title": "Bleu: a Method for Automatic Evaluation of Machine Translation", "author_names": ["Kishore Papineni", "Salim Roukos", "T. Ward", "Wei-Jing Zhu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.", "year": 2002, "publicationdate": "2002-07-06", "externalids": {"DOI": "10.3115/1073083.1073135"}, "doi_lower": "10.3115/1073083.1073135"}
{"paper_id": 271064337, "title": "OffsetBias: Leveraging Debiased Data for Tuning Evaluators", "author_names": ["Junsoo Park", "Seungyeon Jwa", "Meiying Ren", "Daeyoung Kim", "Sanghyuk Choi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Employing Large Language Models (LLMs) to assess the quality of generated responses, such as prompting instruct-tuned models or fine-tuning judge models, has become a widely adopted evaluation method. It is also known that such evaluators are vulnerable to biases, such as favoring longer responses. While it is important to overcome this problem, the specifics of these biases remain under-explored. In this work, we qualitatively identify six types of biases inherent in various judge models. We propose EvalBiasBench as a meta-evaluation collection of hand-crafted test cases for each bias type. Additionally, we present de-biasing dataset construction methods and the associated preference dataset OffsetBias. Experimental results demonstrate that fine-tuning on our dataset significantly enhances the robustness of judge models against biases and improves performance across most evaluation scenarios. We release our datasets and the fine-tuned judge model to public.", "year": 2024, "publicationdate": "2024-07-09", "externalids": {"DOI": "10.48550/arXiv.2407.06551"}, "doi_lower": "10.48550/arxiv.2407.06551"}
{"paper_id": 273162350, "title": "AIME: AI System Optimization via Multiple LLM Evaluators", "author_names": ["Bhrij Patel", "Souradip Chakraborty", "Wesley A. Suttle", "Mengdi Wang", "A. S. Bedi", "Dinesh Manocha"], "venue": "arXiv.org", "abstract": "Text-based AI system optimization typically involves a feedback loop scheme where a single LLM generates an evaluation in natural language of the current output to improve the next iteration's output. However, in this work, we empirically demonstrate that for a practical and complex task (code generation) with multiple criteria to evaluate, utilizing only one LLM evaluator tends to let errors in generated code go undetected, thus leading to incorrect evaluations and ultimately suboptimal test case performance. Motivated by this failure case, we assume there exists an optimal evaluation policy that samples an evaluation between response and ground truth. We then theoretically prove that a linear combination of multiple evaluators can approximate this optimal policy. From this insight, we propose AI system optimization via Multiple LLM Evaluators (AIME). AIME is an evaluation protocol that utilizes multiple LLMs that each independently generate an evaluation on separate criteria and then combine them via concatenation. We provide an extensive empirical study showing AIME outperforming baseline methods in code generation tasks, with up to $62\\%$ higher error detection rate and up to $16\\%$ higher success rate than a single LLM evaluation protocol on LeetCodeHard and HumanEval datasets. We also show that the selection of the number of evaluators and which criteria to utilize is non-trivial as it can impact pact success rate by up to $12\\%$.", "year": 2024, "publicationdate": "2024-10-04", "externalids": {"DOI": "10.48550/arXiv.2410.03131"}, "doi_lower": "10.48550/arxiv.2410.03131"}
{"paper_id": 257921623, "title": "REFINER: Reasoning Feedback on Intermediate Representations", "author_names": ["Debjit Paul", "Mete Ismayilzada", "Maxime Peyrard", "Beatriz Borges", "Antoine Bosselut", "Robert West", "B. Faltings"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences,e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial contextand lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT-3.5 or ChatGPT as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be substituted with humans at inference time.", "year": 2023, "publicationdate": "2023-04-04", "externalids": {"DOI": "10.48550/arXiv.2304.01904"}, "doi_lower": "10.48550/arxiv.2304.01904"}
{"paper_id": 253581710, "title": "Ignore Previous Prompt: Attack Techniques For Language Models", "author_names": ["Fábio Perez", "Ian Ribeiro"], "venue": "arXiv.org", "abstract": "Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PromptInject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject is available at https://github.com/agencyenterprise/PromptInject.", "year": 2022, "publicationdate": "2022-11-17", "externalids": {"DOI": "10.48550/arXiv.2211.09527"}, "doi_lower": "10.48550/arxiv.2211.09527"}
{"paper_id": 261064970, "title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions", "author_names": ["Pouya Pezeshkpour", "Estevam Hruschka"], "venue": "NAACL-HLT", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.", "year": 2023, "publicationdate": "2023-08-22", "externalids": {"DOI": "10.48550/arXiv.2308.11483"}, "doi_lower": "10.48550/arxiv.2308.11483"}
{"paper_id": 269303081, "title": "Bias patterns in the application of LLMs for clinical decision support: A comprehensive study", "author_names": ["Raphael Poulain", "Hamed Fayyaz", "Rahmatollah Beheshti"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) have emerged as powerful candidates to inform clinical decision-making processes. While these models play an increasingly prominent role in shaping the digital landscape, two growing concerns emerge in healthcare applications: 1) to what extent do LLMs exhibit social bias based on patients' protected attributes (like race), and 2) how do design choices (like architecture design and prompting strategies) influence the observed biases? To answer these questions rigorously, we evaluated eight popular LLMs across three question-answering (QA) datasets using clinical vignettes (patient descriptions) standardized for bias evaluations. We employ red-teaming strategies to analyze how demographics affect LLM outputs, comparing both general-purpose and clinically-trained models. Our extensive experiments reveal various disparities (some significant) across protected groups. We also observe several counter-intuitive patterns such as larger models not being necessarily less biased and fined-tuned models on medical data not being necessarily better than the general-purpose models. Furthermore, our study demonstrates the impact of prompt design on bias patterns and shows that specific phrasing can influence bias patterns and reflection-type approaches (like Chain of Thought) can reduce biased outcomes effectively. Consistent with prior studies, we call on additional evaluations, scrutiny, and enhancement of LLMs used in clinical decision support applications.", "year": 2024, "publicationdate": "2024-04-23", "externalids": {"DOI": "10.48550/arXiv.2404.15149"}, "doi_lower": "10.48550/arxiv.2404.15149"}
{"paper_id": 259309299, "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting", "author_names": ["Zhen Qin", "R. Jagerman", "Kai Hui", "Honglei Zhuang", "Junru Wu", "Jiaming Shen", "Tianqi Liu", "Jialu Liu", "Donald Metzler", "Xuanhui Wang", "Michael Bendersky"], "venue": "NAACL-HLT", "abstract": "Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP based on the Flan-UL2 model with 20B parameters performs favorably with the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, while outperforming other LLM-based solutions, such as InstructGPT which has 175B parameters, by over 10% for all ranking metrics. By using the same prompt template on seven BEIR tasks, PRP outperforms supervised baselines and outperforms the blackbox commercial ChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on average NDCG@10. Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity.", "year": 2023, "publicationdate": "2023-06-30", "externalids": {"DOI": "10.48550/arXiv.2306.17563"}, "doi_lower": "10.48550/arxiv.2306.17563"}
{"paper_id": 258959321, "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "author_names": ["Rafael Rafailov", "Archit Sharma", "E. Mitchell", "Stefano Ermon", "Christopher D. Manning", "Chelsea Finn"], "venue": "Neural Information Processing Systems", "abstract": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {}, "doi_lower": null}
{"paper_id": 145153607, "title": "Center-of-inattention: Position biases in decision-making", "author_names": ["Priya Raghubir", "A. Valenzuela"], "venue": "", "abstract": null, "year": 2006, "publicationdate": null, "externalids": {"DOI": "10.1016/J.OBHDP.2005.06.001"}, "doi_lower": "10.1016/j.obhdp.2005.06.001"}
{"paper_id": 271903371, "title": "LLMJudge: LLMs for Relevance Judgments", "author_names": ["Hossein A. Rahmani", "Emine Yilmaz", "Nick Craswell", "Bhaskar Mitra", "Paul Thomas", "Charles L. A. Clarke", "Mohammad Aliannejadi", "Clemencia Siro", "G. Faggioli"], "venue": "LLM4Eval@SIGIR", "abstract": "The LLMJudge challenge is organized as part of the LLM4Eval workshop at SIGIR 2024. Test collections are essential for evaluating information retrieval (IR) systems. The evaluation and tuning of a search system is largely based on relevance labels, which indicate whether a document is useful for a specific search and user. However, collecting relevance judgments on a large scale is costly and resource-intensive. Consequently, typical experiments rely on third-party labelers who may not always produce accurate annotations. The LLMJudge challenge aims to explore an alternative approach by using LLMs to generate relevance judgments. Recent studies have shown that LLMs can generate reliable relevance judgments for search systems. However, it remains unclear which LLMs can match the accuracy of human labelers, which prompts are most effective, how fine-tuned open-source LLMs compare to closed-source LLMs like GPT-4, whether there are biases in synthetically generated data, and if data leakage affects the quality of generated labels. This challenge will investigate these questions, and the collected data will be released as a package to support automatic relevance judgment research in information retrieval and search.", "year": 2024, "publicationdate": "2024-08-09", "externalids": {"DOI": "10.48550/arXiv.2408.08896"}, "doi_lower": "10.48550/arxiv.2408.08896"}
{"paper_id": 267770121, "title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment", "author_names": ["Vyas Raina", "Adian Liusie", "Mark J. F. Gales"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large Language Models (LLMs) are powerful zero-shot assessors used in real-world situations such as assessing written exams and benchmarking systems. Despite these critical applications, no existing work has analyzed the vulnerability of judge-LLMs to adversarial manipulation. This work presents the first study on the adversarial robustness of assessment LLMs, where we demonstrate that short universal adversarial phrases can be concatenated to deceive judge LLMs to predict inflated scores. Since adversaries may not know or have access to the judge-LLMs, we propose a simple surrogate attack where a surrogate model is first attacked, and the learned attack phrase then transferred to unknown judge-LLMs. We propose a practical algorithm to determine the short universal attack phrases and demonstrate that when transferred to unseen models, scores can be drastically inflated such that irrespective of the assessed text, maximum scores are predicted. It is found that judge-LLMs are significantly more susceptible to these adversarial attacks when used for absolute scoring, as opposed to comparative assessment. Our findings raise concerns on the reliability of LLM-as-a-judge methods, and emphasize the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios.", "year": 2024, "publicationdate": "2024-02-21", "externalids": {"DOI": "10.48550/arXiv.2402.14016"}, "doi_lower": "10.48550/arxiv.2402.14016"}
{"paper_id": 271892269, "title": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge", "author_names": ["Ravi Raju", "Swayambhoo Jain", "Bo Li", "Jonathan Li", "Urmish Thakker"], "venue": "CUSTOMNLP4U", "abstract": "Large Language Models (LLMs) have revolutionized the landscape of machine learning, yet current benchmarks often fall short in capturing the diverse behavior of these models in real-world applications. A benchmark’s usefulness is determined by its ability to clearly differentiate between models of varying capabilities (separability) and closely align with human preferences. Existing frameworks like Alpaca-Eval 2.0 LC (CITATION) and Arena-Hard v0.1 (CITATION) are limited by their focus on general-purpose queries and lack of diversity across domains such as law, medicine, and multilingual contexts. In this paper, we address these limitations by introducing a novel data pipeline that curates diverse, domain-specific evaluation sets tailored for LLM-as-a-Judge frameworks. Our approach leverages a combination of manual curation, semi-supervised learning to generate clusters, and stratified sampling to ensure balanced representation across a wide range of domains and languages. The resulting evaluation set, which includes 1573 samples across 14 categories, demonstrates high separability (84%) across ten top-ranked models, and agreement (84%) with Chatbot Arena and (0.915) Spearman correlation. The agreement values are 9% better than Arena Hard and 20% better than AlpacaEval 2.0 LC, while the Spearman coefficient is 0.7 more than the next best benchmark, showcasing a significant improvement in the usefulness of the benchmark. We further provide an open-source evaluation tool that enables fine-grained analysis of model performance across user-defined categories, offering valuable insights for practitioners. This work contributes to the ongoing effort to enhance the transparency, diversity, and effectiveness of LLM evaluation methodologies.", "year": 2024, "publicationdate": "2024-08-16", "externalids": {"DOI": "10.48550/arXiv.2408.08808"}, "doi_lower": "10.48550/arxiv.2408.08808"}
{"paper_id": 266335508, "title": "Self-Evaluation Improves Selective Generation in Large Language Models", "author_names": ["Jie Ren", "Yao Zhao", "Tu Vu", "Peter J. Liu", "Balaji Lakshminarayanan"], "venue": "ICBINB", "abstract": "Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.", "year": 2023, "publicationdate": "2023-12-14", "externalids": {"DOI": "10.48550/arXiv.2312.09300"}, "doi_lower": "10.48550/arxiv.2312.09300"}
{"paper_id": 265608091, "title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA", "author_names": ["Cheol Ryu", "Seolhwa Lee", "Subeen Pang", "Chanyeol Choi", "Hojun Choi", "Myeonggee Min", "Jy-yong Sohn"], "venue": "NLLP", "abstract": "While large language models (LLMs) have demonstrated significant capabilities in text generation, their utilization in areas requiring domain-specific expertise, such as law, must be approached cautiously. This caution is warranted due to the inherent challenges associated with LLM-generated texts, including the potential presence of factual errors. Motivated by this issue, we propose Eval-RAG, a new evaluation method for LLM-generated texts. Unlike existing methods, Eval-RAG evaluates the validity of generated texts based on the related document that are collected by the retriever. In other words, Eval-RAG adopts the idea of retrieval augmented generation (RAG) for the purpose of evaluation. Our experimental results on Korean Legal Question-Answering (QA) tasks show that conventional LLM-based evaluation methods can be better aligned with Lawyers’ evaluations, by combining with Eval-RAG. In addition, our qualitative analysis show that Eval-RAG successfully finds the factual errors in LLM-generated texts, while existing evaluation methods cannot.", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.nllp-1.13"}, "doi_lower": "10.18653/v1/2023.nllp-1.13"}
{"paper_id": 265221210, "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "author_names": ["Jon Saad-Falcon", "O. Khattab", "Christopher Potts", "Matei Zaharia"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.", "year": 2023, "publicationdate": "2023-11-16", "externalids": {"DOI": "10.48550/arXiv.2311.09476"}, "doi_lower": "10.48550/arxiv.2311.09476"}
{"paper_id": 267636769, "title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications", "author_names": ["Pranab Sahoo", "Ayush Kumar Singh", "Sriparna Saha", "Vinija Jain", "S. Mondal", "Aman Chadha"], "venue": "arXiv.org", "abstract": "Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized. We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique. This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering.", "year": 2024, "publicationdate": "2024-02-05", "externalids": {"DOI": "10.48550/arXiv.2402.07927"}, "doi_lower": "10.48550/arxiv.2402.07927"}
{"paper_id": 120576250, "title": "Spearman’s rank correlation coefficient", "author_names": ["A. Culyer"], "venue": "British medical journal", "abstract": null, "year": 2014, "publicationdate": "2014-07-31", "externalids": {"DOI": "10.1136/bmj.g7528"}, "doi_lower": "10.1136/bmj.g7528"}
{"paper_id": 122323572, "title": "Estimates of the Regression Coefficient Based on Kendall's Tau", "author_names": ["P. Sen"], "venue": "", "abstract": null, "year": 1968, "publicationdate": "1968-12-01", "externalids": {"DOI": "10.1080/01621459.1968.10480934"}, "doi_lower": "10.1080/01621459.1968.10480934"}
{"paper_id": 269213992, "title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences", "author_names": ["Shreya Shankar", "J.D. Zamfirescu-Pereira", "Bjorn Hartmann", "Aditya G. Parameswaran", "Ian Arawjo"], "venue": "ACM Symposium on User Interface Software and Technology", "abstract": "Due to the cumbersome nature of human evaluation and limitations of code-based evaluation, Large Language Models (LLMs) are increasingly being used to assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply inherit all the problems of the LLMs they evaluate, requiring further human validation. We present a mixed-initiative approach to “validate the validators”—aligning LLM-generated evaluation functions (be it prompts or code) with human requirements. Our interface, EvalGen, provides automated assistance to users in generating evaluation criteria and implementing assertions. While generating candidate implementations (Python functions, LLM grader prompts), EvalGen asks humans to grade a subset of LLM outputs; this feedback is used to select implementations that better align with user grades. A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative nature of alignment. In particular, we identify a phenomenon we dub criteria drift: users need criteria to grade outputs, but grading outputs helps users define criteria. What is more, some criteria appear dependent on the specific LLM outputs observed (rather than independent and definable a priori), raising serious questions for approaches that assume the independence of evaluation from observation of model outputs. We present our interface and implementation details, a comparison of our algorithm with a baseline approach, and implications for the design of future LLM evaluation assistants.", "year": 2024, "publicationdate": "2024-04-18", "externalids": {"DOI": "10.1145/3654777.3676450"}, "doi_lower": "10.1145/3654777.3676450"}
{"paper_id": 238638709, "title": "The 34th Annual ACM Symposium on User Interface Software and Technology", "author_names": [], "venue": "", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.1145/3472749"}, "doi_lower": "10.1145/3472749"}
{"paper_id": 260704242, "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models", "author_names": ["Xinyue Shen", "Zeyuan Chen", "M. Backes", "Yun Shen", "Yang Zhang"], "venue": "Conference on Computer and Communications Security", "abstract": "The misuse of large language models (LLMs) has drawn significant attention from the general public and LLM vendors. One particular type of adversarial prompt, known as jailbreak prompt, has emerged as the main attack vector to bypass the safeguards and elicit harmful content from LLMs. In this paper, employing our new framework JailbreakHub, we conduct a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023. We identify 131 jailbreak communities and discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from online Web communities to prompt-aggregation websites and 28 user accounts have consistently optimized jailbreak prompts over 100 days. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 107,250 samples across 13 forbidden scenarios. Leveraging this dataset, our experiments on six popular LLMs show that their safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify five highly effective jailbreak prompts that achieve 0.95 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and the earliest one has persisted online for over 240 days. We hope that our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.", "year": 2023, "publicationdate": "2023-08-07", "externalids": {"DOI": "10.1145/3658644.3670388"}, "doi_lower": "10.1145/3658644.3670388"}
{"paper_id": 264555138, "title": "OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization", "author_names": ["Yuchen Shen", "Xiaojun Wan"], "venue": "arXiv.org", "abstract": "Opinion summarization sets itself apart from other types of summarization tasks due to its distinctive focus on aspects and sentiments. Although certain automated evaluation methods like ROUGE have gained popularity, we have found them to be unreliable measures for assessing the quality of opinion summaries. In this paper, we present OpinSummEval, a dataset comprising human judgments and outputs from 14 opinion summarization models. We further explore the correlation between 24 automatic metrics and human ratings across four dimensions. Our findings indicate that metrics based on neural networks generally outperform non-neural ones. However, even metrics built on powerful backbones, such as BART and GPT-3/3.5, do not consistently correlate well across all dimensions, highlighting the need for advancements in automated evaluation methods for opinion summarization. The code and data are publicly available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.", "year": 2023, "publicationdate": "2023-10-27", "externalids": {"DOI": "10.48550/arXiv.2310.18122"}, "doi_lower": "10.48550/arxiv.2310.18122"}
{"paper_id": 256459776, "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "author_names": ["Freda Shi", "Xinyun Chen", "Kanishka Misra", "Nathan Scales", "David Dohan", "Ed H. Chi", "Nathanael Scharli", "Denny Zhou"], "venue": "International Conference on Machine Learning", "abstract": "Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.", "year": 2023, "publicationdate": "2023-01-31", "externalids": {"DOI": "10.48550/arXiv.2302.00093"}, "doi_lower": "10.48550/arxiv.2302.00093"}
{"paper_id": 268691814, "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge", "author_names": ["Jiawen Shi", "Zenghui Yuan", "Yinuo Liu", "Yue Huang", "Pan Zhou", "Lichao Sun", "Neil Zhenqiang Gong"], "venue": "Conference on Computer and Communications Security", "abstract": "LLM-as-a-Judge uses a large language model (LLM) to select the best response from a set of candidates for a given question. LLM-as-a-Judge has many applications such as LLM-powered search, reinforcement learning with AI feedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver, an optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver injects a carefully crafted sequence into an attacker-controlled candidate response such that LLM-as-a-Judge selects the candidate response for an attacker-chosen question no matter what other candidate responses are. Specifically, we formulate finding such sequence as an optimization problem and propose a gradient based method to approximately solve it. Our extensive evaluation shows that JudgeDeceive is highly effective, and is much more effective than existing prompt injection attacks that manually craft the injected sequences and jailbreak attacks when extended to our problem. We also show the effectiveness of JudgeDeceiver in three case studies, i.e., LLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses including known-answer detection, perplexity detection, and perplexity windowed detection. Our results show these defenses are insufficient, highlighting the urgent need for developing new defense strategies.", "year": 2024, "publicationdate": "2024-03-26", "externalids": {"DOI": "10.1145/3658644.3690291"}, "doi_lower": "10.1145/3658644.3690291"}
{"paper_id": 270391552, "title": "Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs", "author_names": ["Lin Shi", "Chiyu Ma", "Weicheng Ma", "Soroush Vosoughi"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2406.07791"}, "doi_lower": "10.48550/arxiv.2406.07791"}
{"paper_id": 265213217, "title": "Fusion-Eval: Integrating Assistant Evaluators with LLMs", "author_names": ["Lei Shu", "Nevan Wichers", "Liangchen Luo", "Yun Zhu", "Yinxiao Liu", "Jindong Chen", "Lei Meng"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Evaluating natural language generation (NLG) systems automatically poses significant challenges.Recent studies have employed large language models (LLMs) as reference-free metrics for NLG evaluation, enhancing adaptability to new tasks tasks. However, these methods still show lower correspondence with human judgments compared to specialized neural evaluators.In this paper, we introduce “Fusion-Eval”, an innovative approach that leverages LLMs to integrate insights from various assistant evaluators. The LLM is given the example to evaluate along with scores from the assistant evaluators. Each of these evaluators specializes in assessing distinct aspects of responses.Fusion-Eval achieves a 0.962 system-level Kendall-Tau correlation with humans on SummEval and a 0.744 turn-level Spearman correlation on TopicalChat, which is significantly higher than baseline methods. These results highlight Fusion-Eval’s significant potential in the realm of natural language system evaluation.", "year": 2023, "publicationdate": "2023-11-15", "externalids": {"DOI": "10.18653/v1/2024.emnlp-industry.18"}, "doi_lower": "10.18653/v1/2024.emnlp-industry.18"}
{"paper_id": 266163375, "title": "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models", "author_names": ["Avi Singh", "John D. Co-Reyes", "Rishabh Agarwal", "Ankesh Anand", "Piyush Patil", "Peter J. Liu", "James Harrison", "Jaehoon Lee", "Kelvin Xu", "Aaron T Parisi", "Abhishek Kumar", "A. Alemi", "Alex Rizkowsky", "Azade Nova", "Ben Adlam", "Bernd Bohnet", "Hanie Sedghi", "Igor Mordatch", "Isabelle Simpson", "Izzeddin Gur", "Jasper Snoek", "Jeffrey Pennington", "Jiri Hron", "Kathleen Kenealy", "Kevin Swersky", "Kshiteej Mahajan", "Laura Culp", "Lechao Xiao", "Maxwell Bileschi", "Noah Constant", "Roman Novak", "Rosanne Liu", "Tris Warkentin", "Yundi Qian", "Ethan Dyer", "Behnam Neyshabur", "Jascha Narain Sohl-Dickstein", "Noah Fiedel"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Fine-tuning language models~(LMs) on human-generated data remains a prevalent practice. However, the performance of such models is often limited by the quantity and diversity of high-quality human data. In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness. To do so, we investigate a simple self-training method based on expectation-maximization, which we call ReST$^{EM}$, where we (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find that ReST$^{EM}$ scales favorably with model size and significantly surpasses fine-tuning only on human data. Overall, our findings suggest self-training with feedback can substantially reduce dependence on human-generated data.", "year": 2023, "publicationdate": "2023-12-11", "externalids": {"DOI": "10.48550/arXiv.2312.06585"}, "doi_lower": "10.48550/arxiv.2312.06585"}
{"paper_id": 272828021, "title": "Don’t Use LLMs to Make Relevance Judgments", "author_names": ["Ian Soboroff"], "venue": "Inf. Retr. Res. J.", "abstract": "Relevance judgments and other truth data for information retrieval (IR) evaluations are created manually. There is a strong temptation to use large language models (LLMs) as proxies for human judges. However, letting the LLM write your truth data handicaps the evaluation by setting that LLM as a ceiling on performance. There are ways to use LLMs in the relevance assessment process, but just generating relevance judgments with a prompt isn’t one of them.1", "year": 2024, "publicationdate": "2024-09-23", "externalids": {"DOI": "10.54195/irrj.19625"}, "doi_lower": "10.54195/irrj.19625"}
{"paper_id": 269950965, "title": "KRX Bench: Automating Financial Benchmark Creation via Large Language Models", "author_names": ["Guijin Son", "Hyunjun Jeon", "Chami Hwang", "Hanearl Jung"], "venue": "FINNLP", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 273532388, "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models", "author_names": ["Guijin Son", "Dongkeun Yoon", "Juyoung Suk", "Javier Aula-Blasco", "Mano Aslan", "Vu Trong Kim", "Shayekh Bin Islam", "Jaume Prats-Cristià", "Lucía Tormo-Bañuelos", "Seungone Kim"], "venue": "arXiv.org", "abstract": "As Large Language Models (LLMs) are now capable of producing fluent and coherent content in languages other than English, it is not imperative to precisely evaluate these non-English outputs. However, when assessing the outputs from mutlilingual LLMs, prior works often employed LLM based evaluators that excel at assessing English outputs, without a thorough examination of whether these evaluators could effectively assess non-English text as well. Moreover, existing benchmarks to test evaluator LLMs (referred to as\"meta-evaluation benchmarks\") are mostly English-centric. To bridge this gap and examine whether evaluator LLMs can reliably assess the outputs of multilingual LLMs, we introduce MM-Eval, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages. A core attribute of MM-Eval is that, instead of merely translating existing English meta-evaluation benchmarks, it is designed with multilingual-specific challenges in mind. Additionally, unlike existing meta-evaluation benchmarks that focus solely on ranking accuracy over pairwise data, MM-Eval also evaluates the consistency and fairness of absolute score values across a wide range of languages. Our results show that existing evaluator LLMs that excel in English contexts have considerable room for improvement when assessing non-English outputs. Furthermore, we find that evaluators are unfair and inconsistent when evaluating lower-resourced languages. Finally, we validate MM-Eval by measuring its correlation with Best-of-N rankings, finding a significantly stronger correlation compared to other meta-evaluation benchmarks. We publicly release our benchmark and code.", "year": 2024, "publicationdate": "2024-10-23", "externalids": {"DOI": "10.48550/arXiv.2410.17578"}, "doi_lower": "10.48550/arxiv.2410.17578"}
{"paper_id": 270869629, "title": "FineSurE: Fine-grained Summarization Evaluation using LLMs", "author_names": ["Hwanjun Song", "Hang Su", "Igor Shalyminov", "Jason (Jinglun) Cai", "Saab Mansour"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Automated evaluation is crucial for streamlining text summarization benchmarking and model development, given the costly and time-consuming nature of human evaluation. Traditional methods like ROUGE do not correlate well with human judgment, while recently proposed LLM-based metrics provide only summary-level assessment using Likert-scale scores. This limits deeper model analysis, e.g., we can only assign one hallucination score at the summary level, while at the sentence level, we can count sentences containing hallucinations. To remedy those limitations, we propose FineSurE, a fine-grained evaluator specifically tailored for the summarization task using large language models (LLMs). It also employs completeness and conciseness criteria, in addition to faithfulness, enabling multi-dimensional assessment. We compare various open-source and proprietary LLMs as backbones for FineSurE. In addition, we conduct extensive benchmarking of FineSurE against SOTA methods including NLI-, QA-, and LLM-based methods, showing improved performance especially on the completeness and conciseness dimensions. The code is available at https://github.com/DISL-Lab/FineSurE-ACL24.", "year": 2024, "publicationdate": "2024-07-01", "externalids": {"DOI": "10.48550/arXiv.2407.00908"}, "doi_lower": "10.48550/arxiv.2407.00908"}
{"paper_id": 273963283, "title": "On Many-Shot In-Context Learning for Long-Context Evaluation", "author_names": ["Kaijian Zou", "Muhammad Khalifa", "Lu Wang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Many-shot in-context learning (ICL) has emerged as a unique setup to both utilize and test the ability of large language models to handle long context. This paper delves into long-context language model (LCLM) evaluation through many-shot ICL. We first ask: what types of ICL tasks benefit from additional demonstrations, and how effective are they in evaluating LCLMs? We find that classification and summarization tasks show performance improvements with additional demonstrations, while translation and reasoning tasks do not exhibit clear trends. Next, we investigate the extent to which different tasks necessitate retrieval versus global context understanding. We develop metrics to categorize ICL tasks into two groups: (i) similar-sample learning (SSL): tasks where retrieval of the most similar examples is sufficient for good performance, and (ii) all-sample learning (ASL): tasks that necessitate a deeper comprehension of all examples in the prompt. Lastly, we introduce a new many-shot ICL benchmark, MANYICLBENCH, to characterize model's ability on both fronts and benchmark 12 LCLMs using MANYICLBENCH. We find that while state-of-the-art models demonstrate good performance up to 64k tokens in SSL tasks, many models experience significant performance drops at only 16k tokens in ASL tasks.", "year": 2024, "publicationdate": "2024-11-11", "externalids": {"DOI": "10.18653/v1/2025.acl-long.1245"}, "doi_lower": "10.18653/v1/2025.acl-long.1245"}
{"paper_id": 269621356, "title": "Automated Essay Scoring and Revising Based on Open-Source Large Language Models", "author_names": ["Yishen Song", "Qianta Zhu", "Huaibo Wang", "Qinhua Zheng"], "venue": "IEEE Transactions on Learning Technologies", "abstract": "Manually scoring and revising student essays has long been a time-consuming task for educators. With the rise of natural language processing techniques, automated essay scoring (AES) and automated essay revising (AER) have emerged to alleviate this burden. However, current AES and AER models require large amounts of training data and lack generalizability, which makes them hard to implement in daily teaching activities. Moreover, online sites offering AES and AER services charge high fees and have security issues uploading student content. In light of these challenges and recognizing the advancements in large language models (LLMs), we aim to fill these research gaps by analyzing the performance of open-source LLMs when accomplishing AES and AER tasks. Using a human-scored essay dataset (n = 600) collected in an online assessment, we implemented zero-shot, few-shot, and p-tuning AES methods based on the LLMs and conducted a human–machine consistency check. We conducted a similarity test and a score difference test for the results of AER with LLMs support. The human–machine consistency check result shows that the performance of open-source LLMs with a 10 B parameter size in the AES task is close to that of some deep-learning baseline models, and it can be improved by integrating the comment with the score into the shot or training continuous prompts. The similarity test and score difference test results show that open-source LLMs can effectively accomplish the AER task, improving the quality of the essays while ensuring that the revision results are similar to the original essays. This study reveals a practical path to cost-effectively, time-efficiently, and content-safely assisting teachers with student essay scoring and revising using open-source LLMs.", "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.1109/TLT.2024.3396873"}, "doi_lower": "10.1109/tlt.2024.3396873"}
{"paper_id": 264426756, "title": "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks", "author_names": ["Andrea Sottana", "Bin Liang", "Kai Zou", "Zheng Yuan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large Language Models (LLMs) evaluation is a patchy and inconsistent landscape, and it is becoming clear that the quality of automatic evaluation metrics is not keeping up with the pace of development of generative models. We aim to improve the understanding of current models' performance by providing a preliminary and hybrid evaluation on a range of open and closed-source generative LLMs on three NLP benchmarks: text summarisation, text simplification and grammatical error correction (GEC), using both automatic and human evaluation. We also explore the potential of the recently released GPT-4 to act as an evaluator. We find that ChatGPT consistently outperforms many other popular models according to human reviewers on the majority of metrics, while scoring much more poorly when using classic automatic evaluation metrics. We also find that human reviewers rate the gold reference as much worse than the best models' outputs, indicating the poor quality of many popular benchmarks. Finally, we find that GPT-4 is capable of ranking models' outputs in a way which aligns reasonably closely to human judgement despite task-specific variations, with a lower alignment in the GEC task.", "year": 2023, "publicationdate": "2023-10-20", "externalids": {"DOI": "10.48550/arXiv.2310.13800"}, "doi_lower": "10.48550/arxiv.2310.13800"}
{"paper_id": 272463940, "title": "From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks", "author_names": ["Andreas Stephan", "Dawei Zhu", "Matthias Aßenmacher", "Xiaoyu Shen", "Benjamin Roth"], "venue": "arXiv.org", "abstract": "To reduce the need for human annotations, large language models (LLMs) have been proposed as judges of the quality of other candidate models. The performance of LLM judges is typically evaluated by measuring the correlation with human judgments on generative tasks such as summarization or machine translation. In contrast, we study LLM judges on mathematical reasoning tasks. These tasks require multi-step reasoning, and the correctness of their solutions is verifiable, enabling a more objective evaluation. We perform a detailed performance analysis and find that easy samples are easy to judge, and difficult samples are difficult to judge. Our analysis uncovers a strong correlation between judgment performance and the candidate model task performance, indicating that judges tend to favor higher-quality models even if their answer is incorrect. As a consequence, we test whether we can predict the behavior of LLM judges using simple features such as part-of-speech tags and find that we can correctly predict 70%-75% of judgments. We conclude this study by analyzing practical use cases, showing that LLM judges consistently detect the on-average better model but largely fail if we use them to improve task performance.", "year": 2024, "publicationdate": "2024-09-06", "externalids": {"DOI": "10.48550/arXiv.2409.04168"}, "doi_lower": "10.48550/arxiv.2409.04168"}
{"paper_id": 269588132, "title": "Large Language Models are Inconsistent and Biased Evaluators", "author_names": ["Rickard Stureborg", "Dimitris Alikaniotis", "Yoshi Suhara"], "venue": "arXiv.org", "abstract": "The zero-shot capability of Large Language Models (LLMs) has enabled highly flexible, reference-free metrics for various tasks, making LLM evaluators common tools in NLP. However, the robustness of these LLM evaluators remains relatively understudied; existing work mainly pursued optimal performance in terms of correlating LLM scores with human expert scores. In this paper, we conduct a series of analyses using the SummEval dataset and confirm that LLMs are biased evaluators as they: (1) exhibit familiarity bias-a preference for text with lower perplexity, (2) show skewed and biased distributions of ratings, and (3) experience anchoring effects for multi-attribute judgments. We also found that LLMs are inconsistent evaluators, showing low\"inter-sample\"agreement and sensitivity to prompt differences that are insignificant to human understanding of text quality. Furthermore, we share recipes for configuring LLM evaluators to mitigate these limitations. Experimental results on the RoSE dataset demonstrate improvements over the state-of-the-art LLM evaluators.", "year": 2024, "publicationdate": "2024-05-02", "externalids": {"DOI": "10.48550/arXiv.2405.01724"}, "doi_lower": "10.48550/arxiv.2405.01724"}
{"paper_id": 273654642, "title": "Fast Best-of-N Decoding via Speculative Rejection", "author_names": ["Hanshi Sun", "Momin Haider", "Ruiqi Zhang", "Huitao Yang", "Jiahao Qiu", "Ming Yin", "Mengdi Wang", "Peter L. Bartlett", "Andrea Zanette"], "venue": "Neural Information Processing Systems", "abstract": "The safe and effective deployment of Large Language Models (LLMs) involves a critical step called alignment, which ensures that the model's responses are in accordance with human preferences. Prevalent alignment techniques, such as DPO, PPO and their variants, align LLMs by changing the pre-trained model weights during a phase called post-training. While predominant, these post-training methods add substantial complexity before LLMs can be deployed. Inference-time alignment methods avoid the complex post-training step and instead bias the generation towards responses that are aligned with human preferences. The best-known inference-time alignment method, called Best-of-N, is as effective as the state-of-the-art post-training procedures. Unfortunately, Best-of-N requires vastly more resources at inference time than standard decoding strategies, which makes it computationally not viable. In this work, we introduce Speculative Rejection, a computationally-viable inference-time alignment algorithm. It generates high-scoring responses according to a given reward model, like Best-of-N does, while being between 16 to 32 times more computationally efficient.", "year": 2024, "publicationdate": "2024-10-26", "externalids": {"DOI": "10.48550/arXiv.2410.20290"}, "doi_lower": "10.48550/arxiv.2410.20290"}
{"paper_id": 220249771, "title": "Natural Backdoor Attack on Text Data", "author_names": ["Lichao Sun"], "venue": "arXiv.org", "abstract": "Deep learning has been widely adopted in natural language processing applications in recent years. Many existing studies show the vulnerabilities of machine learning and deep learning models against adversarial examples. However, most existing works currently focus on evasion attack on text data instead of positioning attack, also named backdoor attack. In this paper, we systematically study the backdoor attack against models on text data. First, we define the backdoor attack on text data. Then, we propose the different attack strategies to generate trigger on text data. Next, we propose different types of the triggers based on modification scope, human recognition and special cases. Last, we evaluate the backdoor attack and the results show the excellent performance of with 100% backdoor attack success rate and sacrificing of 0.71% on text classification task.", "year": 2020, "publicationdate": "2020-06-29", "externalids": {}, "doi_lower": null}
{"paper_id": 212657400, "title": "Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT", "author_names": ["Lichao Sun", "Kazuma Hashimoto", "Wenpeng Yin", "Akari Asai", "Jia Li", "Philip S. Yu", "Caiming Xiong"], "venue": "arXiv.org", "abstract": "There is an increasing amount of literature that claims the brittleness of deep neural networks in dealing with adversarial examples that are created maliciously. It is unclear, however, how the models will perform in realistic scenarios where \\textit{natural rather than malicious} adversarial instances often exist. This work systematically explores the robustness of BERT, the state-of-the-art Transformer-style model in NLP, in dealing with noisy data, particularly mistakes in typing the keyboard, that occur inadvertently. Intensive experiments on sentiment analysis and question answering benchmarks indicate that: (i) Typos in various words of a sentence do not influence equally. The typos in informative words make severer damages; (ii) Mistype is the most damaging factor, compared with inserting, deleting, etc.; (iii) Humans and machines have different focuses on recognizing adversarial attacks.", "year": 2020, "publicationdate": "2020-02-27", "externalids": {}, "doi_lower": null}
{"paper_id": 273654506, "title": "Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks", "author_names": ["Annalisa Szymanski", "Noah Ziems", "H. Eicher-Miller", "Toby Jia-Jun Li", "Meng Jiang", "Ronald A. Metoyer"], "venue": "International Conference on Intelligent User Interfaces", "abstract": "The potential of using Large Language Models (LLMs) themselves to evaluate LLM outputs offers a promising method for assessing model performance across various contexts. Previous research indicates that LLM-as-a-judge exhibits a strong correlation with human judges in the context of general instruction following. However, for instructions that require specialized knowledge, the validity of using LLMs as judges remains uncertain. In our study, we applied a mixed-methods approach, conducting pairwise comparisons in which both subject matter experts (SMEs) and LLMs evaluated outputs from domain-specific tasks. We focused on two distinct fields: dietetics, with registered dietitian experts, and mental health, with clinical psychologist experts. Our results showed that SMEs agreed with LLM judges 68% of the time in the dietetics domain and 64% in mental health when evaluating overall preference. Additionally, the results indicated variations in SME-LLM agreement across domain-specific aspect questions. Our findings emphasize the importance of keeping human experts in the evaluation process, as LLMs alone may not provide the depth of understanding required for complex, knowledge specific tasks. We also explore the implications of LLM evaluations across different domains and discuss how these insights can inform the design of evaluation workflows that ensure better alignment between human experts and LLMs in interactive systems.", "year": 2024, "publicationdate": "2024-10-26", "externalids": {"DOI": "10.1145/3708359.3712091"}, "doi_lower": "10.1145/3708359.3712091"}
{"paper_id": 273374769, "title": "JudgeBench: A Benchmark for Evaluating LLM-based Judges", "author_names": ["Sijun Tan", "Siyuan Zhuang", "Kyle Montgomery", "William Y. Tang", "Alejandro Cuadron", "Chenguang Wang", "R. Popa", "Ion Stoica"], "venue": "International Conference on Learning Representations", "abstract": "LLM-based judges have emerged as a scalable alternative to human evaluation and are increasingly used to assess, compare, and improve models. However, the reliability of LLM-based judges themselves is rarely scrutinized. As LLMs become more advanced, their responses grow more sophisticated, requiring stronger judges to evaluate them. Existing benchmarks primarily focus on a judge's alignment with human preferences, but often fail to account for more challenging tasks where crowdsourced human preference is a poor indicator of factual and logical correctness. To address this, we propose a novel evaluation framework to objectively evaluate LLM-based judges. Based on this framework, we propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging response pairs spanning knowledge, reasoning, math, and coding. JudgeBench leverages a novel pipeline for converting existing difficult datasets into challenging response pairs with preference labels reflecting objective correctness. Our comprehensive evaluation on a collection of prompted judges, fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench poses a significantly greater challenge than previous benchmarks, with many strong models (e.g., GPT-4o) performing just slightly better than random guessing. Overall, JudgeBench offers a reliable platform for assessing increasingly advanced LLM-based judges. Data and code are available at https://github.com/ScalerLab/JudgeBench.", "year": 2024, "publicationdate": "2024-10-16", "externalids": {}, "doi_lower": null}
{"paper_id": 273403309, "title": "AI can help humans find common ground in democratic deliberation", "author_names": ["Michael Henry Tessler", "Michiel A. Bakker", "Daniel Jarrett", "Hannah Sheahan", "Martin J. Chadwick", "Raphael Koster", "Georgina Evans", "Lucy Campbell-Gillingham", "Tantum Collins", "David C. Parkes", "Matthew M. Botvinick", "Christopher Summerfield"], "venue": "Science", "abstract": "Finding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants’ personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the UK population. Editor’s summary To act collectively, groups must reach agreement; however, this can be challenging when discussants present very different but valid opinions. Tessler et al. investigated whether artificial intelligence (AI) can help groups reach a consensus during democratic debate (see the Policy Forum by Nyhan and Titiunik). The authors trained a large language model called the Habermas Machine to serve as an AI mediator that helped small UK groups find common ground while discussing divisive political issues such as Brexit, immigration, the minimum wage, climate change, and universal childcare. Compared with human mediators, AI mediators produced more palatable statements that generated wide agreement and left groups less divided. The AI’s statements were more clear, logical, and informative without alienating minority perspectives. This work carries policy implications for AI’s potential to unify deeply divided groups. —Ekeoma Uzogara INTRODUCTION Democracy, at its best, rests upon the free and equal exchange of views among people with diverse perspectives. Collective deliberation can be effectively supported by structured events, such as citizens’ assemblies, but such events are expensive, are difficult to scale, and can result in voices being heard unequally. This study investigates the potential of artificial intelligence (AI) to overcome these limitations, using AI mediation to help people find common ground on complex social and political issues. RATIONALE We asked whether an AI system based on large language models (LLMs) could successfully capture the underlying shared perspectives of a group of human discussants by writing a “group statement” that the discussants would collectively endorse. Inspired by Jürgen Habermas’s theory of communicative action, we designed the “Habermas Machine” to iteratively generate group statements that were based on the personal opinions and critiques from individual users, with the goal of maximizing group approval ratings. Through successive rounds of human data collection, we used supervised fine-tuning and reward modeling to progressively enhance the Habermas Machine’s ability to capture shared perspectives. To evaluate the efficacy of AI-mediated deliberation, we conducted a series of experiments with over 5000 participants from the United Kingdom. These experiments investigated the impact of AI mediation on finding common ground, how the views of discussants changed across the process, the balance between minority and majority perspectives in group statements, and potential biases present in those statements. Lastly, we used the Habermas Machine for a virtual citizens’ assembly, assessing its ability to support deliberation on controversial issues within a demographically representative sample of UK residents. RESULTS Group opinion statements generated by the Habermas Machine were consistently preferred by group members over those written by human mediators and received higher ratings from external judges for quality, clarity, informativeness, and perceived fairness. AI-mediated deliberation also reduced division within groups, with participants’ reported stances converging toward a common position on the issue after deliberation; this result did not occur when discussants directly exchanged views, unmediated. Although support for the majority position increased after deliberation, the Habermas Machine demonstrably incorporated minority critiques into revised statements. We replicated these results in a virtual citizens’ assembly, additionally finding that during AI-mediated deliberation, the views of groups of discussants tended to move in a similar direction on controversial issues. These shifts were not attributable to biases in the AI, suggesting that the deliberation process genuinely aided the emergence of shared perspectives on potentially polarizing social and political issues. CONCLUSION This research demonstrates the potential of AI to enhance collective deliberation by finding common ground among discussants with diverse views. The AI-mediated approach is time-efficient, fair, scalable, and outperforms human mediators on key dimensions. Rather than simply appealing to the majority, the Habermas Machine prominently incorporated dissenting voices into the group statements. AI-assisted deliberation is not without its risks, however; to ensure fair and inclusive debate, steps must be taken to ensure users are representative of the target population and are prepared to contribute in good faith. Under such conditions, AI may be leveraged to improve collective decision-making across various domains, from contract negotiations and conflict resolution to political discussions and citizens’ assemblies. The Habermas Machine offers a promising tool for finding agreement and promoting collective action in an increasingly divided world. AI helps people find common ground in collective deliberation. (Left) The AI mediator uses participants’ opinions to generate group statements and iteratively refines those statements through participants’ critiques. (Middle) Statements from the AI mediator (purple) garner stronger endorsement than those written by a human mediator (orange). (Right) AI mediation leaves groups less divided after deliberation, whereas simply sharing opinions with others does not.", "year": 2024, "publicationdate": "2024-10-18", "externalids": {"DOI": "10.1126/science.adq2852"}, "doi_lower": "10.1126/science.adq2852"}
{"paper_id": 270562425, "title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges", "author_names": ["Aman Singh Thakur", "Kartik Choudhary", "Venkat Srinik Ramayapally", "Sankaran Vaidyanathan", "Dieuwke Hupkes"], "venue": "arXiv.org", "abstract": "Offering a promising solution to the scalability challenges associated with human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an approach to evaluating large language models (LLMs). However, there are still many open questions about the strengths and weaknesses of this paradigm, and what potential biases it may hold. In this paper, we present a comprehensive study of the performance of various LLMs acting as judges, focusing on a clean scenario in which inter-human agreement is high. Investigating thirteen judge models of different model sizes and families, judging answers of nine different'examtaker models'- both base and instruction-tuned - we find that only the best (and largest) models achieve reasonable alignment with humans. However, they are still quite far behind inter-human agreement and their assigned scores may still differ with up to 5 points from human-assigned scores. In terms of their ranking of the nine exam-taker models, instead, also smaller models and even the lexical metric contains may provide a reasonable signal. Through error analysis and other studies, we identify vulnerabilities in judge models, such as their sensitivity to prompt complexity and length, and a tendency toward leniency. The fact that even the best judges differ from humans in this comparatively simple setup suggest that caution may be wise when using judges in more complex setups. Lastly, our research rediscovers the importance of using alignment metrics beyond simple percent alignment, showing that judges with high percent agreement can still assign vastly different scores.", "year": 2024, "publicationdate": "2024-06-18", "externalids": {"DOI": "10.48550/arXiv.2406.12624"}, "doi_lower": "10.48550/arxiv.2406.12624"}
{"paper_id": 266725532, "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models", "author_names": ["S. Tonmoy", "S. M. M. Zaman", "Vinija Jain", "Anku Rani", "Vipula Rawte", "Aman Chadha", "Amitava Das"], "venue": "arXiv.org", "abstract": "As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.", "year": 2024, "publicationdate": "2024-01-02", "externalids": {"DOI": "10.48550/arXiv.2401.01313"}, "doi_lower": "10.48550/arxiv.2401.01313"}
{"paper_id": 258108255, "title": "ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning", "author_names": ["Petter Törnberg"], "venue": "arXiv.org", "abstract": "This paper assesses the accuracy, reliability and bias of the Large Language Model (LLM) ChatGPT-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The LLM is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that ChatGPT-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that LLM will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.", "year": 2023, "publicationdate": "2023-04-13", "externalids": {"DOI": "10.48550/arXiv.2304.06588"}, "doi_lower": "10.48550/arxiv.2304.06588"}
{"paper_id": 259950998, "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "author_names": ["Hugo Touvron", "Louis Martin", "Kevin R. Stone", "Peter Albert", "Amjad Almahairi", "Yasmine Babaei", "Niko-lay Bashlykov", "Soumya Batra", "Prajjwal Bhargava", "Shruti Bhosale", "D. Bikel", "Lukas Blecher", "Cris-tian Cantón Ferrer", "Moya Chen", "Guillem Cucurull", "David Esiobu", "Jude Fernandes", "J. Fu", "Wenyin Fu", "Brian Fuller", "Cynthia Gao", "Vedanuj Goswami", "Naman Goyal", "A. Hartshorn", "Saghar Hosseini", "Rui Hou", "Hakan Inan", "Marcin Kardas", "Viktor Kerkez", "Madian Khabsa", "Isabel M. Kloumann", "A. Korenev", "Punit Singh Koura", "M. Lachaux", "Thibaut Lavril", "Jenya Lee", "Diana Liskovich", "Yinghai Lu", "Yuning Mao", "Xavier Martinet", "Todor Mihaylov", "Pushkar Mishra", "Igor Molybog", "Yixin Nie", "Andrew Poulton", "J. Reizenstein", "Rashi Rungta", "Kalyan Saladi", "A. Schelten", "Ruan Silva", "Eric Michael Smith", "R. Subramanian", "Xia Tan", "Binh Tang", "Ross Taylor", "Adina Williams", "Jian Xiang Kuan", "Puxin Xu", "Zhengxu Yan", "Iliyan Zarov", "Yuchen Zhang", "Angela Fan", "M. Kambadur", "Sharan Narang", "Aur'elien Rodriguez", "Robert Stojnic", "Sergey Edunov", "Thomas Scialom"], "venue": "arXiv.org", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "year": 2023, "publicationdate": "2023-07-18", "externalids": {}, "doi_lower": null}
{"paper_id": 271516633, "title": "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents", "author_names": ["H. Trivedi", "Tushar Khot", "Mareike Hartmann", "R. Manku", "Vinty Dong", "Edward Li", "Shashank Gupta", "Ashish Sabharwal", "Niranjan Balasubramanian"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built $\\textbf{AppWorld Engine}$, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created $\\textbf{AppWorld Benchmark}$ (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our 'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least 16% fewer. This highlights the benchmark's difficulty and AppWorld's potential to push the frontiers of interactive coding agents. The project website is available at https://appworld.dev/.", "year": 2024, "publicationdate": "2024-07-26", "externalids": {"DOI": "10.48550/arXiv.2407.18901"}, "doi_lower": "10.48550/arxiv.2407.18901"}
{"paper_id": 273228170, "title": "Self-rationalization improves LLM as a fine-grained judge", "author_names": ["Prapti Trivedi", "Aditya Gulati", "Oliver Molenschot", "M. Rajeev", "Rajkumar Ramamurthy", "Keith Stevens", "Tanveesh Singh Chaudhery", "Jahnavi Jambholkar", "James Zou", "Nazneen Rajani"], "venue": "arXiv.org", "abstract": "LLM-as-a-judge models have been used for evaluating both human and AI generated content, specifically by providing scores and rationales. Rationales, in addition to increasing transparency, help models learn to calibrate its judgments. Enhancing a model's rationale can therefore improve its calibration abilities and ultimately the ability to score content. We introduce Self-Rationalization, an iterative process of improving the rationales for the judge models, which consequently improves the score for fine-grained customizable scoring criteria (i.e., likert-scale scoring with arbitrary evaluation criteria). Self-rationalization works by having the model generate multiple judgments with rationales for the same input, curating a preference pair dataset from its own judgements, and iteratively fine-tuning the judge via DPO. Intuitively, this approach allows the judge model to self-improve by learning from its own rationales, leading to better alignment and evaluation accuracy. After just two iterations -- while only relying on examples in the training set -- human evaluation shows that our judge model learns to produce higher quality rationales, with a win rate of $62\\%$ on average compared to models just trained via SFT on rationale . This judge model also achieves high scoring accuracy on BigGen Bench and Reward Bench, outperforming even bigger sized models trained using SFT with rationale, self-consistency or best-of-$N$ sampling by $3\\%$ to $9\\%$.", "year": 2024, "publicationdate": "2024-10-07", "externalids": {"DOI": "10.48550/arXiv.2410.05495"}, "doi_lower": "10.48550/arxiv.2410.05495"}
{"paper_id": 273163168, "title": "Are Expert-Level Language Models Expert-Level Annotators?", "author_names": ["Yu-Min Tseng", "Wei-Lin Chen", "Chung-Chi Chen", "Hsin-Hsi Chen"], "venue": "arXiv.org", "abstract": "Data annotation refers to the labeling or tagging of textual data with relevant information. A large body of works have reported positive results on leveraging LLMs as an alternative to human annotators. However, existing studies focus on classic NLP tasks, and the extent to which LLMs as data annotators perform in domains requiring expert knowledge remains underexplored. In this work, we investigate comprehensive approaches across three highly specialized domains and discuss practical suggestions from a cost-effectiveness perspective. To the best of our knowledge, we present the first systematic evaluation of LLMs as expert-level data annotators.", "year": 2024, "publicationdate": "2024-10-04", "externalids": {"DOI": "10.48550/arXiv.2410.03254"}, "doi_lower": "10.48550/arxiv.2410.03254"}
{"paper_id": 14636783, "title": "Computing Machinery and Intelligence", "author_names": ["A. Turing"], "venue": "The Philosophy of Artificial Intelligence", "abstract": null, "year": 1950, "publicationdate": "1950-10-01", "externalids": {"DOI": "10.1093/MIND/LIX.236.433"}, "doi_lower": "10.1093/mind/lix.236.433"}
{"paper_id": 265213404, "title": "LLMs cannot find reasoning errors, but can correct them!", "author_names": ["Gladys Tyen", "Hassan Mansoor", "Peter Chen", "Tony Mak", "Victor Carbune"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023b; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we show that poor self-correction performance stems from LLMs' inability to find logical mistakes, rather than their ability to correct a known mistake. Firstly, we benchmark several state-of-the-art LLMs on their mistake-finding ability and demonstrate that they generally struggle with the task, even in highly objective, unambiguous cases. Secondly, we test the correction abilities of LLMs -- separately from mistake finding -- using a backtracking setup that feeds ground truth mistake location information to the model. We show that this boosts downstream task performance across our 5 reasoning tasks, indicating that LLMs' correction abilities are robust. Finally, we show that it is possible to obtain mistake location information without ground truth labels or in-domain training data. We train a small classifier with out-of-domain data, which exhibits stronger mistake-finding performance than prompting a large model. We release our dataset of LLM-generated logical mistakes, BIG-Bench Mistake, to enable further research into locating LLM reasoning mistakes.", "year": 2023, "publicationdate": "2023-11-14", "externalids": {"DOI": "10.48550/arXiv.2311.08516"}, "doi_lower": "10.48550/arxiv.2311.08516"}
{"paper_id": 263909251, "title": "Can Large Language Models Really Improve by Self-critiquing Their Own Plans?", "author_names": ["Karthik Valmeekam", "Matthew Marquez", "Subbarao Kambhampati"], "venue": "arXiv.org", "abstract": "There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation. Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.", "year": 2023, "publicationdate": "2023-10-12", "externalids": {"DOI": "10.48550/arXiv.2310.08118"}, "doi_lower": "10.48550/arxiv.2310.08118"}
{"paper_id": 269449458, "title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models", "author_names": ["Pat Verga", "Sebastian Hofstätter", "Sophia Althammer", "Yixuan Su", "Aleksandra Piktus", "Arkady Arkhangorodsky", "Minjie Xu", "Naomi White", "Patrick Lewis"], "venue": "arXiv.org", "abstract": "As Large Language Models (LLMs) have become more advanced, they have outpaced our abilities to accurately evaluate their quality. Not only is finding data to adequately probe particular model properties difficult, but evaluating the correctness of a model's freeform generation alone is a challenge. To address this, many evaluations now rely on using LLMs themselves as judges to score the quality of outputs from other LLMs. Evaluations most commonly use a single large model like GPT4. While this method has grown in popularity, it is costly, has been shown to introduce intramodel bias, and in this work, we find that very large models are often unnecessary. We propose instead to evaluate models using a Panel of LLm evaluators (PoLL). Across three distinct judge settings and spanning six different datasets, we find that using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive.", "year": 2024, "publicationdate": "2024-04-29", "externalids": {"DOI": "10.48550/arXiv.2404.18796"}, "doi_lower": "10.48550/arxiv.2404.18796"}
{"paper_id": 271213398, "title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "author_names": ["Tu Vu", "Kalpesh Krishna", "Salaheddin Alzubi", "C. Tar", "Manaal Faruqui", "Yun-Hsuan Sung"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "As large language models (LLMs) evolve, evaluating their output reliably becomes increasingly difficult due to the high cost of human evaluation. To address this, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on a diverse set of over 100 quality assessment tasks, incorporating 5M+ human judgments curated from publicly released human evaluations. FLAMe outperforms models like GPT-4 and Claude-3 on various held-out tasks, and serves as a powerful starting point for fine-tuning, as shown in our reward model evaluation case study (FLAMe-RM). On Reward-Bench, FLAMe-RM-24B achieves 87.8% accuracy, surpassing GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we introduce FLAMe-Opt-RM, an efficient tail-patch fine-tuning approach that offers competitive RewardBench performance using 25×fewer training datapoints. Our FLAMe variants outperform popular proprietary LLM-as-a-Judge models on 8 of 12 autorater benchmarks, covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis shows that FLAMe is significantly less biased than other LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark.", "year": 2024, "publicationdate": "2024-07-15", "externalids": {"DOI": "10.48550/arXiv.2407.10817"}, "doi_lower": "10.48550/arxiv.2407.10817"}
{"paper_id": 271270693, "title": "Halu-J: Critique-Based Hallucination Judge", "author_names": ["Binjie Wang", "Steffi Chern", "Ethan Chern", "Pengfei Liu"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) frequently generate non-factual content, known as hallucinations. Existing retrieval-augmented-based hallucination detection approaches typically address this by framing it as a classification task, evaluating hallucinations based on their consistency with retrieved evidence. However, this approach usually lacks detailed explanations for these evaluations and does not assess the reliability of these explanations. Furthermore, deficiencies in retrieval systems can lead to irrelevant or partially relevant evidence retrieval, impairing the detection process. Moreover, while real-world hallucination detection requires analyzing multiple pieces of evidence, current systems usually treat all evidence uniformly without considering its relevance to the content. To address these challenges, we introduce Halu-J, a critique-based hallucination judge with 7 billion parameters. Halu-J enhances hallucination detection by selecting pertinent evidence and providing detailed critiques. Our experiments indicate that Halu-J outperforms GPT-4o in multiple-evidence hallucination detection and matches its capability in critique generation and evidence selection. We also introduce ME-FEVER, a new dataset designed for multiple-evidence hallucination detection. Our code and dataset can be found in https://github.com/GAIR-NLP/factool .", "year": 2024, "publicationdate": "2024-07-17", "externalids": {"DOI": "10.48550/arXiv.2407.12943"}, "doi_lower": "10.48550/arxiv.2407.12943"}
{"paper_id": 273482423, "title": "Automated Genre-Aware Article Scoring and Feedback Using Large Language Models", "author_names": ["Chihang Wang", "Yuxin Dong", "Zhenhong Zhang", "Ruotong Wang", "Shuo Wang", "Jiajing Chen"], "venue": "2024 5th International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)", "abstract": "This paper focuses on the development of an advanced intelligent article scoring system that not only assesses the overall quality of written work but also offers detailed feature-based scoring tailored to various article genres. By integrating the pre-trained BERT model with the large language model Chat-GPT, the system gains a deep understanding of both the content and structure of the text, enabling it to provide a thorough evaluation along with targeted suggestions for improvement. Experimental results demonstrate that this system outperforms traditional scoring methods across multiple public datasets, particularly in feature-based assessments, offering a more accurate reflection of the quality of different article types. Moreover, the system generates personalized feedback to assist users in enhancing their writing skills, underscoring the potential and practical value of automated scoring technologies in educational contexts.", "year": 2024, "publicationdate": "2024-10-18", "externalids": {"DOI": "10.1109/ICBAIE63306.2024.11116923"}, "doi_lower": "10.1109/icbaie63306.2024.11116923"}
{"paper_id": 260704279, "title": "Learning Evaluation Models From Large Language Models for Sequence Generation", "author_names": ["Chenglong Wang", "Hang Zhou", "Kai-Chun Chang", "Tongran Liu", "Chunliang Zhang", "Quan Du", "Tong Xiao", "Jingbo Zhu"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "abstract": "Automatic evaluation of sequence generation, which has traditionally relied on metrics such as BLEU and ROUGE, often struggles to capture the semantic accuracy of generated text due to an overemphasis on n-gram overlap. A promising solution to this issue is the development of model-based metrics, such as BLEURT and COMET. However, these approaches are typically limited by the scarcity of labeled evaluation data, which is essential for training evaluation models. In this work, we address this challenge by proposing the Customized Sequence Evaluation Metric (CSEM), a three-stage model training method that leverages large language models to generate labeled data for metric development, eliminating the need for human-labeled data. Furthermore, we extend the capabilities of CSEM to support a range of evaluation types, including single-aspect, multi-aspect, reference-free, and reference-based evaluations. This flexibility allows for the customization of metrics to fit various real-world scenarios. Experimental results on the SummEval benchmark demonstrate that CSEM can effectively train an evaluation model without human-labeled data. Additional experiments in reinforcement learning and reranking show that metrics developed through CSEM outperform traditional evaluation metrics, leading to significant improvements in sequence quality, as assessed by both commonly used metrics and ChatGPT.", "year": 2023, "publicationdate": "2023-08-08", "externalids": {"DOI": "10.1109/TASLPRO.2025.3587460"}, "doi_lower": "10.1109/taslpro.2025.3587460"}
{"paper_id": 258960339, "title": "Large Language Models are not Fair Evaluators", "author_names": ["Peiyi Wang", "Lei Li", "Liang Chen", "Dawei Zhu", "Binghuai Lin", "Yunbo Cao", "Qi Liu", "Tianyu Liu", "Zhifang Sui"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the\"win/tie/lose\"outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \\url{https://github.com/i-Eval/FairEval} to facilitate future research.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {"DOI": "10.48550/arXiv.2305.17926"}, "doi_lower": "10.48550/arxiv.2305.17926"}
{"paper_id": 163015684, "title": "xiao yuan wang shu xue shi yan ping tai de she ji yu shi xian", "author_names": ["Shi Tongju"], "venue": "", "abstract": null, "year": 2008, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 260735852, "title": "Shepherd: A Critic for Language Model Generation", "author_names": ["Tianlu Wang", "Ping Yu", "Xiaoqing Tan", "Sean O'Brien", "Ramakanth Pasunuru", "Jane Dwivedi-Yu", "O. Yu. Golovneva", "Luke Zettlemoyer", "Maryam Fazel-Zarandi", "Asli Celikyilmaz"], "venue": "arXiv.org", "abstract": "As large language models improve, there is increasing interest in techniques that leverage these models' capabilities to refine their own outputs. In this work, we introduce Shepherd, a language model specifically tuned to critique responses and suggest refinements, extending beyond the capabilities of an untuned model to identify diverse errors and provide suggestions to remedy them. At the core of our approach is a high quality feedback dataset, which we curate from community feedback and human annotations. Even though Shepherd is small (7B parameters), its critiques are either equivalent or preferred to those from established models including ChatGPT. Using GPT-4 for evaluation, Shepherd reaches an average win-rate of 53-87% compared to competitive alternatives. In human evaluation, Shepherd strictly outperforms other models and on average closely ties with ChatGPT.", "year": 2023, "publicationdate": "2023-08-08", "externalids": {"DOI": "10.48550/arXiv.2308.04592"}, "doi_lower": "10.48550/arxiv.2308.04592"}
{"paper_id": 273350939, "title": "TestAgent: A Framework for Domain-Adaptive Evaluation of LLMs via Dynamic Benchmark Construction and Exploratory Interaction", "author_names": ["Wanying Wang", "Zeyu Ma", "Pengfei Liu", "Mingang Chen"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 21054674, "title": "Position Bias Estimation for Unbiased Learning to Rank in Personal Search", "author_names": ["Xuanhui Wang", "Nadav Golbandi", "Michael Bendersky", "Donald Metzler", "Marc Najork"], "venue": "Web Search and Data Mining", "abstract": null, "year": 2018, "publicationdate": "2018-02-02", "externalids": {"DOI": "10.1145/3159652.3159732"}, "doi_lower": "10.1145/3159652.3159732"}
{"paper_id": 247595263, "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "author_names": ["Xuezhi Wang", "Jason Wei", "D. Schuurmans", "Quoc Le", "Ed H. Chi", "Denny Zhou"], "venue": "International Conference on Learning Representations", "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).", "year": 2022, "publicationdate": "2022-03-21", "externalids": {}, "doi_lower": null}
{"paper_id": 259108266, "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization", "author_names": ["Yidong Wang", "Zhuohao Yu", "Zhengran Zeng", "Linyi Yang", "Cunxiang Wang", "Hao Chen", "Chaoya Jiang", "Rui Xie", "Jindong Wang", "Xingxu Xie", "Wei Ye", "Shi-Bo Zhang", "Yue Zhang"], "venue": "International Conference on Learning Representations", "abstract": "Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our results indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation ability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage. All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.48550/arXiv.2306.05087"}, "doi_lower": "10.48550/arxiv.2306.05087"}
{"paper_id": 273025954, "title": "HelpSteer2-Preference: Complementing Ratings with Preferences", "author_names": ["Zhilin Wang", "Alexander Bukharin", "Olivier Delalleau", "Daniel Egert", "Gerald Shen", "Jiaqi Zeng", "Oleksii Kuchaiev", "Yi Dong"], "venue": "International Conference on Learning Representations", "abstract": "Reward models are critical for aligning models to follow instructions, and are typically trained following one of two popular paradigms: Bradley-Terry style or Regression style. However, there is a lack of evidence that either approach is better than the other, when adequately matched for data. This is primarily because these approaches require data collected in different (but incompatible) formats, meaning that adequately matched data is not available in existing public datasets. To tackle this problem, we release preference annotations (designed for Bradley-Terry training) to complement existing ratings (designed for Regression style training) in the HelpSteer2 dataset. To improve data interpretability, preference annotations are accompanied with human-written justifications. Using this data, we conduct the first head-to-head comparison of Bradley-Terry and Regression models when adequately matched for data. Based on insights derived from such a comparison, we propose a novel approach to combine Bradley-Terry and Regression reward modeling. A Llama-3.1-70B-Instruct model tuned with this approach scores 94.1 on RewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. This reward model can then be used with REINFORCE algorithm (RLHF) to align an Instruct model to reach 85.0 on Arena Hard, which is No. 1 as of 1 Oct 2024. We open-source this dataset (CC-BY-4.0 license) at https://huggingface.co/datasets/nvidia/HelpSteer2#preferences-new -- 1-oct-2024 and openly release the trained Reward and Instruct models at https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward and https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct", "year": 2024, "publicationdate": "2024-10-02", "externalids": {"DOI": "10.48550/arXiv.2410.01257"}, "doi_lower": "10.48550/arxiv.2410.01257"}
{"paper_id": 265220723, "title": "HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM", "author_names": ["Zhilin Wang", "Yi Dong", "Jiaqi Zeng", "Virginia Adams", "Makesh Narsimhan Sreedhar", "Daniel Egert", "Olivier Delalleau", "Jane Scowcroft", "Neel Kant", "Aidan Swope", "Oleksii Kuchaiev"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Existing open-source helpfulness preference datasets do not specify what makes some responses more helpful and others less so. Models trained on these datasets can incidentally learn to model dataset artifacts (e.g. preferring longer but unhelpful responses only due to their length). To alleviate this problem, we collect HelpSteer, a multi-attribute helpfulness dataset annotated for the various aspects that make responses helpful. Specifically, our 37k-sample dataset has annotations for correctness, coherence, complexity, and verbosity in addition to overall helpfulness of responses. Training Llama 2 70B using the HelpSteer dataset with SteerLM technique produces a model that scores 7.54 on MT Bench, which is currently the highest score for open models that do not require training data from more powerful models (e.g. GPT-4). We release this dataset with CC-BY-4.0 license at https://huggingface.co/datasets/nvidia/HelpSteer", "year": 2023, "publicationdate": "2023-11-16", "externalids": {"DOI": "10.48550/arXiv.2311.09528"}, "doi_lower": "10.48550/arxiv.2311.09528"}
{"paper_id": 273375180, "title": "CREAM: Consistency Regularized Self-Rewarding Language Models", "author_names": ["Zhaoyang Wang", "Weilei He", "Zhiyuan Liang", "Xuchao Zhang", "Chetan Bansal", "Ying Wei", "Weitong Zhang", "Huaxiu Yao"], "venue": "International Conference on Learning Representations", "abstract": "Recent self-rewarding large language models (LLM) have successfully applied LLM-as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same LLM to act as both the policy model (which generates responses) and the reward model (which scores and ranks those responses). The ranked responses are then used as preference pairs to train the LLM via direct alignment technologies (e.g. DPO). However, it is noteworthy that throughout this process, there is no guarantee of accuracy in the rewarding and ranking, which is critical for ensuring accurate rewards and high-quality preference data. Empirical results from relatively small LLMs (e.g., 7B parameters) also indicate that improvements from self-rewarding may diminish after several iterations in certain situations, which we hypothesize is due to accumulated bias in the reward system. This bias can lead to unreliable preference data for training the LLM. To address this issue, we first formulate and analyze the generalized iterative preference fine-tuning framework for self-rewarding language model. We then introduce the regularization to this generalized framework to mitigate the overconfident preference labeling in the self-rewarding process. Based on this theoretical insight, we propose a Consistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages the consistency of rewards across different iterations to regularize the self-rewarding training, helping the model to learn from more reliable preference data. With this explicit regularization, our empirical results demonstrate the superiority of CREAM in improving both reward consistency and alignment performance. The code is publicly available at https://github.com/Raibows/CREAM.", "year": 2024, "publicationdate": "2024-10-16", "externalids": {"DOI": "10.48550/arXiv.2410.12735"}, "doi_lower": "10.48550/arxiv.2410.12735"}
{"paper_id": 54616504, "title": "Five ways to look at Cohen's kappa", "author_names": ["M. Warrens"], "venue": "", "abstract": null, "year": 2015, "publicationdate": "2015-07-28", "externalids": {"DOI": "10.4172/2161-0487.1000197"}, "doi_lower": "10.4172/2161-0487.1000197"}
{"paper_id": 270688323, "title": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data", "author_names": ["Ishaan Watts", "Varun Gumma", "Aditya Yadavalli", "Vivek Seshadri", "Manohar Swaminathan", "Sunayana Sitaram"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Evaluation of multilingual Large Language Models (LLMs) is challenging due to a variety of factors – the lack of benchmarks with sufficient linguistic diversity, contamination of popular benchmarks into LLM pre-training data and the lack of local, cultural nuances in translated benchmarks. In this work, we study human and LLM-based evaluation in a multilingual, multi-cultural setting. We evaluate 30 models across 10 Indic languages by conducting 90K human evaluations and 30K LLM-based evaluations and find that models such as GPT-4o and Llama-3 70B consistently perform best for most Indic languages. We build leaderboards for two evaluation settings - pairwise comparison and direct assessment and analyse the agreement between humans and LLMs. We find that humans and LLMs agree fairly well in the pairwise setting but the agreement drops for direct assessment evaluation especially for languages such as Bengali and Odia. We also check for various biases in human and LLM-based evaluation and find evidence of self-bias in the GPT-based evaluator. Our work presents a significant step towards scaling up multilingual evaluation of LLMs.", "year": 2024, "publicationdate": "2024-06-21", "externalids": {"DOI": "10.48550/arXiv.2406.15053"}, "doi_lower": "10.48550/arxiv.2406.15053"}
{"paper_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "author_names": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "Ed H. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "venue": "Neural Information Processing Systems", "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 268385144, "title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences", "author_names": ["M. Weyssow", "Aton Kamanda", "Xin Zhou", "H. Sahraoui"], "venue": "ACM Transactions on Software Engineering and Methodology", "abstract": "Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavor that requires a deep assessment of LLMs’ outputs. Existing methods and benchmarks rely primarily on automated metrics and static analysis tools, which often fail to capture the nuances of user instructions and LLM outputs. To address this gap, we introduce the LLM-as-a-Judge evaluation framework and present CodeUltraFeedback, a comprehensive dataset for assessing and improving LLM alignment with coding preferences. CodeUltraFeedback consists of 10,000 coding instructions, each annotated with four responses generated from a diverse pool of 14 LLMs. These responses are annotated using GPT-3.5 as a judge, with both ranking-based scores and detailed textual feedback across five distinct coding preferences. Our analysis reveals that responses from GPT-3.5 and GPT-4 are consistently rated higher than those from open-weight models, underscoring substantial alignment gaps between closed- and open-weight LLMs. In turn, we explore the usage of CodeUltraFeedback as feedback data to fine-tune and align CodeLlama-7B-Instruct using supervised fine-tuning (SFT) and reinforcement learning from AI feedback (RLAIF) with direct preference optimization (DPO). The resulting aligned model achieves an average alignment improvement of 22.7% and 29.7% when evaluated with GPT-3.5 and GPT-4 judges, respectively. Notably, our aligned CodeLlama-7B-Instruct surpasses much larger models, such as CodeLlama-13B and 34B, in alignment with coding preferences. Despite not being explicitly trained for functional correctness, it also achieves a 10.5% and 26.6% relative improvement in Pass@ \\(1\\) and Pass@ \\(10\\) on the HumanEval+ benchmark. Our contributions demonstrate the practical value of preference tuning in code generation and set the stage for further progress in model alignment and RLAIF for automated software engineering.", "year": 2024, "publicationdate": "2024-03-14", "externalids": {"DOI": "10.1145/3736407"}, "doi_lower": "10.1145/3736407"}
{"paper_id": 267406164, "title": "Continual Learning for Large Language Models: A Survey", "author_names": ["Tongtong Wu", "Linhao Luo", "Yuan-Fang Li", "Shirui Pan", "Thuy-Trang Vu", "Gholamreza Haffari"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.", "year": 2024, "publicationdate": "2024-02-02", "externalids": {"DOI": "10.48550/arXiv.2402.01364"}, "doi_lower": "10.48550/arxiv.2402.01364"}
{"paper_id": 271533411, "title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge", "author_names": ["Tianhao Wu", "Weizhe Yuan", "Olga Golovneva", "Jing Xu", "Yuandong Tian", "Jiantao Jiao", "Jason E. Weston", "Sainbayar Sukhbaatar"], "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "abstract": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can improve by judging their own responses instead of relying on human labelers. However, existing methods have primarily focused on improving model responses rather than judgment capabilities, resulting in rapid saturation during iterative training. To address this issue, we introduce a novel Meta-Rewarding step to the self-improvement process, where the model judges its own judgements and uses that feedback to refine its judgment skills. Surprisingly, this unsupervised approach improves the model's ability to judge {\\em and} follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on Arena-Hard. These results strongly suggest the potential for self-improving models without human supervision.", "year": 2024, "publicationdate": "2024-07-28", "externalids": {"DOI": "10.48550/arXiv.2407.19594"}, "doi_lower": "10.48550/arxiv.2407.19594"}
{"paper_id": 269005306, "title": "Evaluating Mathematical Reasoning Beyond Accuracy", "author_names": ["Shijie Xia", "Xuefeng Li", "Yixin Liu", "Tongshuang Wu", "Pengfei Liu"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "The leaderboard of Large Language Models (LLMs) in mathematical tasks has been continuously updated. However, the majority of evaluations focus solely on the final results, neglecting the quality of the intermediate steps. This oversight can mask underlying problems, such as logical errors or unnecessary steps in the reasoning process. To measure reasoning beyond final-answer accuracy, we introduce ReasonEval, a new methodology for evaluating the quality of reasoning steps. ReasonEval employs validity and redundancy to characterize the reasoning quality, as well as accompanying LLMs to assess them automatically. We explore different design options for the LLM-based evaluators and empirically demonstrate that ReasonEval, when instantiated with base models possessing strong mathematical knowledge and trained with high-quality labeled data, consistently outperforms baseline methods in the meta-evaluation datasets. We also highlight the strong generalization capabilities of ReasonEval. By utilizing ReasonEval to evaluate LLMs specialized in math, we find that an increase in final-answer accuracy does not necessarily guarantee an improvement in the overall quality of the reasoning steps for challenging mathematical problems. Additionally, we observe that ReasonEval can play a significant role in data selection. We open-source the best-performing model, meta-evaluation script, and all evaluation results to facilitate future research.", "year": 2024, "publicationdate": "2024-04-08", "externalids": {"DOI": "10.48550/arXiv.2404.05692"}, "doi_lower": "10.48550/arxiv.2404.05692"}
{"paper_id": 269899678, "title": "Language Models can Evaluate Themselves via Probability Discrepancy", "author_names": ["Tingyu Xia", "Bowen Yu", "Yuan Wu", "Yi Chang", "Chang Zhou"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In this paper, we initiate our discussion by demonstrating how Large Language Models (LLMs), when tasked with responding to queries, display a more even probability distribution in their answers if they are more adept, as opposed to their less skilled counterparts. Expanding on this foundational insight, we propose a new self-evaluation method ProbDiff for assessing the efficacy of various LLMs. This approach obviates the necessity for an additional evaluation model or the dependence on external, proprietary models like GPT-4 for judgment. It uniquely utilizes the LLMs being tested to compute the probability discrepancy between the initial response and its revised versions. A higher discrepancy for a given query between two LLMs indicates a relatively weaker capability. Our findings reveal that ProbDiff achieves results on par with those obtained from evaluations based on GPT-4, spanning a range of scenarios that include natural language generation (NLG) tasks such as translation, summarization, and our proposed Xiaohongshu blog writing task, and benchmarks for LLM evaluation like AlignBench, MT-Bench, and AlpacaEval, across LLMs of varying magnitudes.", "year": 2024, "publicationdate": "2024-05-17", "externalids": {"DOI": "10.48550/arXiv.2405.10516"}, "doi_lower": "10.48550/arxiv.2405.10516"}
{"paper_id": 259129602, "title": "PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance", "author_names": ["Qianqian Xie", "Weiguang Han", "Xiao Zhang", "Yanzhao Lai", "Min Peng", "Alejandro Lopez-Lira", "Jimin Huang"], "venue": "arXiv.org", "abstract": "Although large language models (LLMs) has shown great performance on natural language processing (NLP) in the financial domain, there are no publicly available financial tailtored LLMs, instruction tuning datasets, and evaluation benchmarks, which is critical for continually pushing forward the open-source development of financial artificial intelligence (AI). This paper introduces PIXIU, a comprehensive framework including the first financial LLM based on fine-tuning LLaMA with instruction data, the first instruction data with 136K data samples to support the fine-tuning, and an evaluation benchmark with 5 tasks and 9 datasets. We first construct the large-scale multi-task instruction data considering a variety of financial tasks, financial document types, and financial data modalities. We then propose a financial LLM called FinMA by fine-tuning LLaMA with the constructed dataset to be able to follow instructions for various financial tasks. To support the evaluation of financial LLMs, we propose a standardized benchmark that covers a set of critical financial tasks, including five financial NLP tasks and one financial prediction task. With this benchmark, we conduct a detailed analysis of FinMA and several existing LLMs, uncovering their strengths and weaknesses in handling critical financial tasks. The model, datasets, benchmark, and experimental results are open-sourced to facilitate future research in financial AI.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.48550/arXiv.2306.05443"}, "doi_lower": "10.48550/arxiv.2306.05443"}
{"paper_id": 270688409, "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "author_names": ["Tinghao Xie", "Xiangyu Qi", "Yi Zeng", "Yangsibo Huang", "Udari Madhushani Sehwag", "Kaixuan Huang", "Luxi He", "Boyi Wei", "Dacheng Li", "Ying Sheng", "Ruoxi Jia", "Bo Li", "Kai Li", "Danqi Chen", "Peter Henderson", "Prateek Mittal"], "venue": "International Conference on Learning Representations", "abstract": "Evaluating aligned large language models' (LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with SORRY-Bench, our proposed benchmark. First, existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics. For example, among the ten existing datasets that we evaluated, tests for refusals of self-harm instructions are over 3x less represented than tests for fraudulent activities. SORRY-Bench improves on this by using a fine-grained taxonomy of 44 potentially unsafe topics, and 440 class-balanced unsafe instructions, compiled through human-in-the-loop methods. Second, linguistic characteristics and formatting of prompts are often overlooked, like different languages, dialects, and more -- which are only implicitly considered in many evaluations. We supplement SORRY-Bench with 20 diverse linguistic augmentations to systematically examine these effects. Third, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation, which can be computationally expensive. We investigate design choices for creating a fast, accurate automated safety evaluator. By collecting 7K+ human annotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs, we show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale LLMs, with lower computational cost. Putting these together, we evaluate over 50 proprietary and open-weight LLMs on SORRY-Bench, analyzing their distinctive safety refusal behaviors. We hope our effort provides a building block for systematic evaluations of LLMs' safety refusal capabilities, in a balanced, granular, and efficient manner. Benchmark demo, data, code, and models are available through https://sorry-bench.github.io.", "year": 2024, "publicationdate": "2024-06-20", "externalids": {"DOI": "10.48550/arXiv.2406.14598"}, "doi_lower": "10.48550/arxiv.2406.14598"}
{"paper_id": 258426922, "title": "Self-Evaluation Guided Beam Search for Reasoning", "author_names": ["Yuxi Xie", "Kenji Kawaguchi", "Yiran Zhao", "Xu Zhao", "MingSung Kan", "Junxian He", "Qizhe Xie"], "venue": "Neural Information Processing Systems", "abstract": "Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge of uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. We propose a decoding algorithm integrating the self-evaluation guidance via stochastic beam search. The self-evaluation guidance serves as a better-calibrated automatic criterion, facilitating an efficient search in the reasoning space and resulting in superior prediction quality. Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness. Our approach surpasses the corresponding Codex-backboned baselines in few-shot accuracy by $6.34\\%$, $9.56\\%$, and $5.46\\%$ on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on arithmetic reasoning demonstrate the efficiency of our method in outperforming the baseline methods with comparable computational budgets. Further analysis in multi-step reasoning finds our self-evaluation guidance pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at https://guideddecoding.github.io/.", "year": 2023, "publicationdate": "2023-05-01", "externalids": {}, "doi_lower": null}
{"paper_id": 265220772, "title": "DocLens: Multi-aspect Fine-grained Evaluation for Medical Text Generation", "author_names": ["Yiqing Xie", "Sheng Zhang", "Hao Cheng", "Zelalem Gero", "Cliff Wong", "Tristan Naumann", "Hoifung Poon"], "venue": "", "abstract": "Medical text generation aims to assist with administrative work and highlight salient information to support decision-making. To reflect the specific requirements of medical text, in this paper, we propose a set of metrics to evaluate the completeness, conciseness, and attribution of the generated text at a fine-grained level. The metrics can be computed by various types of evaluators including instruction-following (both proprietary and open-source) and supervised entailment models. We demonstrate the effectiveness of the resulting framework, DocLens, with three evaluators on three tasks: clinical note generation, radiology report summarization, and patient question summarization. A comprehensive human study shows that DocLens exhibits substantially higher agreement with the judgments of medical experts than existing metrics. The results also highlight the need to improve open-source evaluators and suggest potential directions.", "year": 2023, "publicationdate": "2023-11-16", "externalids": {}, "doi_lower": null}
{"paper_id": 273549187, "title": "Improving Model Factuality with Fine-grained Critique-based Evaluator", "author_names": ["Yiqing Xie", "Wenxuan Zhou", "Pradyot Prakash", "Di Jin", "Yuning Mao", "Quintin Fettes", "Arya Talebzadeh", "Si-Yuan Wang", "Han Fang", "Carolyn P. Rosé", "Daniel Fried", "Hejia Zhang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Factuality evaluation aims to detect factual errors produced by language models (LMs) and hence guide the development of more factual models. Towards this goal, we train a factuality evaluator, FenCE, that provides LM generators with claim-level factuality feedback. We conduct data augmentation on a combination of public judgment datasets to train FenCE to (1) generate textual critiques along with scores and (2) make claim-level judgment based on diverse source documents obtained by various tools. We then present a framework that leverages FenCE to improve the factuality of LM generators by constructing training data. Specifically, we generate a set of candidate responses, leverage FenCE to revise and score each response without introducing lesser-known facts, and train the generator by preferring highly scored revised responses. Experiments show that our data augmentation methods improve the evaluator's accuracy by 2.9% on LLM-AggreFact. With FenCE, we improve Llama2-7B-chat and Llama3-8B-chat's factuality rate by 16.86% and 14.45% on FActScore, outperforming state-of-the-art factuality finetuning methods by 8.83% and 6.96%.", "year": 2024, "publicationdate": "2024-10-24", "externalids": {"DOI": "10.48550/arXiv.2410.18359"}, "doi_lower": "10.48550/arxiv.2410.18359"}
{"paper_id": 273098155, "title": "LLLaVA-Critic: Learning to Evaluate Multimodal Models", "author_names": ["Tianyi Xiong", "Xiyao Wang", "Dong Guo", "Qinghao Ye", "Haoqi Fan", "Quanquan Gu", "Heng Huang", "Chunyuan Li"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multi-modal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation criteria and scenarios. Our experiments demonstrate the model's effectiveness in two key areas: (i) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation scores, performing on par with or surpassing GPT models on multiple evaluation benchmarks; and (ii) Preference Learning, where it generates reward signals for preference learning, enhancing model alignment capabilities. This work underscores the potential of open-source LMMs in self-critique and evaluation, setting the stage for future research into scalable, superhuman alignment feedback mechanisms for LMMs.", "year": 2024, "publicationdate": "2024-10-03", "externalids": {"DOI": "10.1109/CVPR52734.2025.01271"}, "doi_lower": "10.1109/cvpr52734.2025.01271"}
{"paper_id": 258298159, "title": "WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions", "author_names": ["Can Xu", "Qingfeng Sun", "Kai Zheng", "Xiubo Geng", "Pu Zhao", "Jiazhan Feng", "Chongyang Tao", "Daxin Jiang"], "venue": "International Conference on Learning Representations", "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM", "year": 2023, "publicationdate": "2023-04-24", "externalids": {}, "doi_lower": null}
{"paper_id": 259983087, "title": "CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility", "author_names": ["Guohai Xu", "Jiayi Liu", "Mingshi Yan", "Haotian Xu", "Jinghui Si", "Zhuoran Zhou", "Peng Yi", "Xing Gao", "Jitao Sang", "Rong Zhang", "Ji Zhang", "Chao Peng", "Feiyan Huang", "Jingren Zhou"], "venue": "arXiv.org", "abstract": "With the rapid evolution of large language models (LLMs), there is a growing concern that they may pose risks or have negative social impacts. Therefore, evaluation of human values alignment is becoming increasingly important. Previous work mainly focuses on assessing the performance of LLMs on certain knowledge and reasoning abilities, while neglecting the alignment to human values, especially in a Chinese context. In this paper, we present CValues, the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria. As a result, we have manually collected adversarial safety prompts across 10 scenarios and induced responsibility prompts from 8 domains by professional experts. To provide a comprehensive values evaluation of Chinese LLMs, we not only conduct human evaluation for reliable comparison, but also construct multi-choice prompts for automatic evaluation. Our findings suggest that while most Chinese LLMs perform well in terms of safety, there is considerable room for improvement in terms of responsibility. Moreover, both the automatic and human evaluation are important for assessing the human values alignment in different aspects. The benchmark and code is available on ModelScope and Github.", "year": 2023, "publicationdate": "2023-07-19", "externalids": {"DOI": "10.48550/arXiv.2307.09705"}, "doi_lower": "10.48550/arxiv.2307.09705"}
{"paper_id": 273345628, "title": "Large Language Models Are Active Critics in NLG Evaluation", "author_names": ["Shuying Xu", "Junjie Hu", "Ming Jiang"], "venue": "arXiv.org", "abstract": "The conventional paradigm of using large language models (LLMs) for natural language generation (NLG) evaluation relies on pre-defined task definitions and evaluation criteria, positioning LLMs as\"passive critics\"that strictly follow developer-provided guidelines. However, human evaluators often apply implicit criteria, and their expectations in practice can vary widely based on specific end-user needs. Consequently, these rigid evaluation methods struggle to adapt to diverse scenarios without extensive prompt customization. To address this, we introduce Active-Critic, a novel LLM-based evaluator that transforms LLMs into\"active critics'' capable of adapting to diverse NLG tasks using limited example data. Active-Critic consists of two stages: (1) self-inferring the target NLG task and relevant evaluation criteria, and (2) dynamically optimizing prompts to produce human-aligned scores along with detailed justifications. Our experiments show that Active-Critic can generate nuanced, context-aware evaluation criteria, enabling it to achieve superior alignment with human judgments across multiple tasks.", "year": 2024, "publicationdate": "2024-10-14", "externalids": {"DOI": "10.48550/arXiv.2410.10724"}, "doi_lower": "10.48550/arxiv.2410.10724"}
{"paper_id": 272987127, "title": "The Perfect Blend: Redefining RLHF with Mixture of Judges", "author_names": ["Tengyu Xu", "Eryk Helenowski", "Karthik Abinav Sankararaman", "Di Jin", "Kaiyan Peng", "Eric Han", "Shaoliang Nie", "Chen Zhu", "Hejia Zhang", "Wenxuan Zhou", "Zhouhao Zeng", "Yun He", "Karishma Mandyam", "Arya Talabzadeh", "Madian Khabsa", "Gabriel Cohen", "Yuandong Tian", "Hao Ma", "Si-Yuan Wang", "Han Fang"], "venue": "arXiv.org", "abstract": "Reinforcement learning from human feedback (RLHF) has become the leading approach for fine-tuning large language models (LLM). However, RLHF has limitations in multi-task learning (MTL) due to challenges of reward hacking and extreme multi-objective optimization (i.e., trade-off of multiple and/or sometimes conflicting objectives). Applying RLHF for MTL currently requires careful tuning of the weights for reward model and data combinations. This is often done via human intuition and does not generalize. In this work, we introduce a novel post-training paradigm which we called Constrained Generative Policy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with cost-efficient constrained policy optimization with stratification, which can identify the perfect blend in RLHF in a principled manner. It shows strong empirical results with theoretical guarantees, does not require extensive hyper-parameter tuning, and is plug-and-play in common post-training pipelines. Together, this can detect and mitigate reward hacking behaviors while reaching a pareto-optimal point across an extremely large number of objectives. Our empirical evaluations demonstrate that CGPO significantly outperforms standard RLHF algorithms like PPO and DPO across various tasks including general chat, STEM questions, instruction following, and coding. Specifically, CGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in Arena-Hard (STEM&reasoning), and consistent gains in other domains like math and coding. Notably, PPO, while commonly used, is prone to severe reward hacking in popular coding benchmarks, which CGPO successfully addresses. This breakthrough in RLHF not only tackles reward hacking and extreme multi-objective optimization challenges but also advances the state-of-the-art in aligning general-purpose LLMs for diverse applications.", "year": 2024, "publicationdate": "2024-09-30", "externalids": {"DOI": "10.48550/arXiv.2409.20370"}, "doi_lower": "10.48550/arxiv.2409.20370"}
{"paper_id": 263830581, "title": "INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback", "author_names": ["Wenda Xu", "Danqing Wang", "Liangming Pan", "Zhenqiao Song", "Markus Freitag", "W. Wang", "Lei Li"], "venue": "", "abstract": "Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics can not explain their verdict or associate the scores with defects in generated text. To address this limitation, we present InstructScore, an explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate InstructScore on a variety of generation tasks, including translation, captioning, data-to-text and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {}, "doi_lower": null}
{"paper_id": 267751249, "title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement", "author_names": ["Wenda Xu", "Guanglei Zhu", "Xuandong Zhao", "Liangming Pan", "Lei Li", "W. Wang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others. We discovered that such a contrary is due to LLM's bias in evaluating their own output. In this paper, we formally define LLM's self-bias - the tendency to favor its own generation - using two statistics. We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks. The code and data are released at https://github.com/xu1998hz/llm_self_bias.", "year": 2024, "publicationdate": "2024-02-18", "externalids": {"DOI": "10.18653/v1/2024.acl-long.826"}, "doi_lower": "10.18653/v1/2024.acl-long.826"}
{"paper_id": 264406064, "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack", "author_names": ["Xilie Xu", "Keyi Kong", "Ninghao Liu", "Li-zhen Cui", "Di Wang", "Jingfeng Zhang", "Mohan S. Kankanhalli"], "venue": "arXiv.org", "abstract": "The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM's adversarial robustness. This paper proposes an efficient tool to audit the LLM's adversarial robustness via a prompt-based adversarial attack (PromptAttack). PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself. The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples. Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels. Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++. Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions.", "year": 2023, "publicationdate": "2023-10-20", "externalids": {"DOI": "10.48550/arXiv.2310.13345"}, "doi_lower": "10.48550/arxiv.2310.13345"}
{"paper_id": 265157805, "title": "Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration", "author_names": ["Zhenran Xu", "Senbao Shi", "Baotian Hu", "Jindi Yu", "Dongfang Li", "Min Zhang", "Yuxiang Wu"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in general natural language processing tasks but often fall short in complex reasoning tasks. Recent studies have explored human-like problem-solving strategies, such as self-correct, to push further the boundary of single-model reasoning ability. In this work, we let a single model\"step outside the box\"by engaging multiple models to correct each other. We introduce a multi-agent collaboration strategy that emulates the academic peer review process. Each agent independently constructs its own solution, provides reviews on the solutions of others, and assigns confidence levels to its reviews. Upon receiving peer reviews, agents revise their initial solutions. Extensive experiments on three different types of reasoning tasks show that our collaboration approach delivers superior accuracy across all ten datasets compared to existing methods. Further study underscores the effectiveness of integrating confidence in reviews, demonstrates the superiority of feedback exchange over mere solution sharing, and highlights the role of capability and diversity in fostering successful collaboration.", "year": 2023, "publicationdate": "2023-11-14", "externalids": {"DOI": "10.48550/arXiv.2311.08152"}, "doi_lower": "10.48550/arxiv.2311.08152"}
{"paper_id": 269214139, "title": "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing", "author_names": ["Le Yan", "Zhen Qin", "Honglei Zhuang", "R. Jagerman", "Xuanhui Wang", "Michael Bendersky", "Harrie Oosterhuis"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The powerful generative abilities of large language models (LLMs) show potential in generating relevance labels for search applications. Previous work has found that directly asking about relevancy, such as \"*How relevant is document A to query Q?*”, results in suboptimal ranking. Instead, the pairwise-ranking prompting (PRP) approach produces promising ranking performance through asking about pairwise comparisons, e.g., \"*Is document A more relevant than document B to query Q?*”. Thus, while LLMs are effective at their ranking ability, this is not reflected in their relevance label generation.In this work, we propose a post-processing method to consolidate the relevance labels generated by an LLM with its powerful ranking abilities. Our method takes both LLM generated relevance labels and pairwise preferences. The labels are then altered to satisfy the pairwise preferences of the LLM, while staying as close to the original values as possible. Our experimental results indicate that our approach effectively balances label accuracy and ranking performance. Thereby, our work shows it is possible to combine both the ranking and labeling abilities of LLMs through post-processing.", "year": 2024, "publicationdate": "2024-04-17", "externalids": {"DOI": "10.48550/arXiv.2404.11791"}, "doi_lower": "10.48550/arxiv.2404.11791"}
{"paper_id": 269214139, "title": "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing", "author_names": ["Le Yan", "Zhen Qin", "Honglei Zhuang", "R. Jagerman", "Xuanhui Wang", "Michael Bendersky", "Harrie Oosterhuis"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The powerful generative abilities of large language models (LLMs) show potential in generating relevance labels for search applications. Previous work has found that directly asking about relevancy, such as \"*How relevant is document A to query Q?*”, results in suboptimal ranking. Instead, the pairwise-ranking prompting (PRP) approach produces promising ranking performance through asking about pairwise comparisons, e.g., \"*Is document A more relevant than document B to query Q?*”. Thus, while LLMs are effective at their ranking ability, this is not reflected in their relevance label generation.In this work, we propose a post-processing method to consolidate the relevance labels generated by an LLM with its powerful ranking abilities. Our method takes both LLM generated relevance labels and pairwise preferences. The labels are then altered to satisfy the pairwise preferences of the LLM, while staying as close to the original values as possible. Our experimental results indicate that our approach effectively balances label accuracy and ranking performance. Thereby, our work shows it is possible to combine both the ranking and labeling abilities of LLMs through post-processing.", "year": 2024, "publicationdate": "2024-04-17", "externalids": {"DOI": "10.48550/arXiv.2404.11791"}, "doi_lower": "10.48550/arxiv.2404.11791"}
{"paper_id": 265221028, "title": "Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination", "author_names": ["Nakyeong Yang", "Taegwan Kang", "Stanley Jungkyu Choi", "Honglak Lee", "Kyomin Jung"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Instruction-following language models often show undesirable biases. These undesirable biases may be accelerated in the real-world usage of language models, where a wide range of instructions is used through zero-shot example prompting. To solve this problem, we first define the bias neuron, which significantly affects biased outputs, and prove its existence empirically. Furthermore, we propose a novel and practical bias mitigation method, CRISPR, to eliminate bias neurons of language models in instruction-following settings. CRISPR automatically determines biased outputs and categorizes neurons that affect the biased outputs as bias neurons using an explainability method. Experimental results demonstrate the effectiveness of our method in mitigating biases under zero-shot instruction-following settings without losing the model's task performance and existing knowledge. The experimental results reveal the generalizability of our method as it shows robustness under various instructions and datasets. Surprisingly, our method can mitigate the bias in language models by eliminating only a few neurons (at least three).", "year": 2023, "publicationdate": "2023-11-16", "externalids": {"DOI": "10.18653/v1/2024.acl-long.490"}, "doi_lower": "10.18653/v1/2024.acl-long.490"}
{"paper_id": 258762525, "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "author_names": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "T. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "venue": "Neural Information Processing Systems", "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {"DOI": "10.48550/arXiv.2305.10601"}, "doi_lower": "10.48550/arxiv.2305.10601"}
{"paper_id": 272366500, "title": "Self-Judge: Selective Instruction Following with Alignment Self-Evaluation", "author_names": ["Hai Ye", "Hwee Tou Ng"], "venue": "arXiv.org", "abstract": "Pre-trained large language models (LLMs) can be tailored to adhere to human instructions through instruction tuning. However, due to shifts in the distribution of test-time data, they may not always execute instructions accurately, potentially generating factual errors or misaligned content when acting as chat assistants. To enhance the reliability of LLMs in following instructions, we propose the study of selective instruction following, whereby the system declines to execute instructions if the anticipated response quality is low. We train judge models that can predict numerical quality scores for model responses. To address data scarcity, we introduce Self-J, a novel self-training framework for developing judge models without needing human-annotated quality scores. Our method leverages the model's inherent self-evaluation capability to extract information about response quality from labeled instruction-tuning data. It incorporates a gold reference answer to facilitate self-evaluation and recalibrates by assessing the semantic similarity between the response sample and the gold reference. During the training phase, we implement self-distillation as a regularization technique to enhance the capability of reference-free estimation. To validate alignment evaluation on general instruction-following tasks, we collect large-scale high-quality instructions from Hugging Face for model training and evaluation. Extensive experiments on five open-source models show that our method correlates much more with GPT-4 than strong baselines, e.g., supervised models distilled from GPT-4 and GPT-3.5-turbo. Our analysis shows our model's strong generalization across domains. Additionally, our judge models serve as good reward models, e.g., boosting WizardLM-13B-V1.2 from 89.17 to 92.48 and from 12.03 to 15.90 in version v1 and v2 of AlpacaEval respectively using best-of-32 sampling with our judge models.", "year": 2024, "publicationdate": "2024-09-02", "externalids": {"DOI": "10.48550/arXiv.2409.00935"}, "doi_lower": "10.48550/arxiv.2409.00935"}
{"paper_id": 273098639, "title": "Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge", "author_names": ["Jiayi Ye", "Yanbo Wang", "Yue Huang", "Dongping Chen", "Qihui Zhang", "Nuno Moniz", "Tian Gao", "Werner Geyer", "Chao Huang", "Pin-Yu Chen", "Nitesh V. Chawla", "Xiangliang Zhang"], "venue": "International Conference on Learning Representations", "abstract": "LLM-as-a-Judge has been widely utilized as an evaluation method in various benchmarks and served as supervised rewards in model training. However, despite their excellence in many domains, potential issues are under-explored, undermining their reliability and the scope of their utility. Therefore, we identify 12 key potential biases and propose a new automated bias quantification framework-CALM-which systematically quantifies and analyzes each type of bias in LLM-as-a-Judge by using automated and principle-guided modification. Our experiments cover multiple popular language models, and the results indicate that while advanced models have achieved commendable overall performance, significant biases persist in certain specific tasks. Empirical results suggest that there remains room for improvement in the reliability of LLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence of these biases and give some suggestions for the reliable application of LLM-as-a-Judge. Our work highlights the need for stakeholders to address these issues and remind users to exercise caution in LLM-as-a-Judge applications.", "year": 2024, "publicationdate": "2024-10-03", "externalids": {"DOI": "10.48550/arXiv.2410.02736"}, "doi_lower": "10.48550/arxiv.2410.02736"}
{"paper_id": 257900871, "title": "Self-Refine: Iterative Refinement with Self-Feedback", "author_names": ["Aman Madaan", "Niket Tandon", "Prakhar Gupta", "Skyler Hallinan", "Luyu Gao", "Sarah Wiegreffe", "Uri Alon", "Nouha Dziri", "Shrimai Prabhumoye", "Yiming Yang", "S. Welleck", "Bodhisattwa Prasad Majumder", "Shashank Gupta", "A. Yazdanbakhsh", "Peter Clark"], "venue": "Neural Information Processing Systems", "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {"DOI": "10.48550/arXiv.2303.17651"}, "doi_lower": "10.48550/arxiv.2303.17651"}
{"paper_id": 259991144, "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets", "author_names": ["Seonghyeon Ye", "Doyoung Kim", "Sungdong Kim", "Hyeonbin Hwang", "Seungone Kim", "Yongrae Jo", "James Thorne", "Juho Kim", "Minjoon Seo"], "venue": "International Conference on Learning Representations", "abstract": "Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlation between model-based and human-based evaluations. We publicly release the evaluation data and code implementation at https://github.com/kaistAI/FLASK.", "year": 2023, "publicationdate": "2023-07-20", "externalids": {"DOI": "10.48550/arXiv.2307.10928"}, "doi_lower": "10.48550/arxiv.2307.10928"}
{"paper_id": 273185729, "title": "Beyond Scalar Reward Model: Learning Generative Judge from Preference Data", "author_names": ["Ziyi Ye", "Xiangsheng Li", "Qiuchi Li", "Qingyao Ai", "Yujia Zhou", "Wei Shen", "Dong Yan", "Yiqun Liu"], "venue": "arXiv.org", "abstract": "Learning from preference feedback is a common practice for aligning large language models~(LLMs) with human value. Conventionally, preference data is learned and encoded into a scalar reward model that connects a value head with an LLM to produce a scalar score as preference or reward. However, scalar models lack interpretability and are known to be susceptible to biases in datasets. This paper investigates leveraging the generation capability of LLMs to address both limitations in one shot. Specifically, we prompt the pre-trained LLM to generate positive and negative judgments, both supported with rationales in natural language form. The self-generated contrastive judgment pairs are used to train the generative judge with Direct Preference Optimization (DPO). This proposal of training the generative Judge using self-generated Contrastive judgments (Con-J) ensures natural interpretability due to the generated rationales together with the judgments, as well as high robustness against bias without the need for an additional reward head. Experimental results show that the performance of Con-J is comparable to the scalar reward model trained on the same collection of preference data, and demonstrate its superior interpretability and robustness in encoding human preferences.", "year": 2024, "publicationdate": "2024-10-01", "externalids": {"DOI": "10.48550/arXiv.2410.03742"}, "doi_lower": "10.48550/arxiv.2410.03742"}
{"paper_id": 282212153, "title": "Low-Resource Fine-Tuning of LLMs for Domain-Specific Tasks", "author_names": ["Vamshikrishna Challa", "Abika Osinachi Bright"], "venue": "Universal Research Reports", "abstract": "Large Language Models (LLMs) such as GPT and LLaMA have demonstrated remarkable capabilities across diverse natural language processing (NLP) applications. However, their enormous computational and memory requirements hinder adoption by smaller research labs and enterprises. Full-scale fine-tuning of such models is often infeasible due to high GPU memory, storage, and energy consumption. Parameter-Efficient Fine-Tuning (PEFT) techniques, including Low-Rank Adaptation (LoRA), adapter-based methods, and prefix-tuning, present an alternative for adapting LLMs to downstream tasks under constrained budgets. Despite progress in PEFT for general NLP benchmarks, limited attention has been given to domain-specific applications such as healthcare and energy, where specialized knowledge is critical. This research investigates low-resource fine-tuning strategies for domain adaptation of LLMs, identifies the challenges of constrained environments, and evaluates practical frameworks that balance efficiency and performance. Experimental results demonstrate that LoRA- and adapter-based methods achieve competitive accuracy while drastically reducing trainable parameters and compute costs, making them highly suitable for resource-limited settings.", "year": 2025, "publicationdate": "2025-10-12", "externalids": {"DOI": "10.36676/urr.v12.i4.1621"}, "doi_lower": "10.36676/urr.v12.i4.1621"}
{"paper_id": 259587374, "title": "Overview of the Tenth Dialog System Technology Challenge: DSTC10", "author_names": ["Koichiro Yoshino", "Yun-Nung (Vivian) Chen", "Paul A. Crook", "Satwik Kottur", "Jinchao Li", "Behnam Hedayatnia", "Seungwhan Moon", "Zhengcong Fei", "Zekang Li", "Jinchao Zhang", "Yang Feng", "Jie Zhou", "Seokhwan Kim", "Yang Liu", "Di Jin", "A. Papangelis", "Karthik Gopalakrishnan", "Dilek Z. Hakkani-Tür", "Babak Damavandi", "A. Geramifard", "Chiori Hori", "Ankit Shah", "Chen Zhang", "Haizhou Li", "João Sedoc", "L. F. D’Haro", "Rafael E. Banchs", "Alexander I. Rudnicky"], "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing", "abstract": "This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.", "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.1109/TASLP.2023.3293030"}, "doi_lower": "10.1109/taslp.2023.3293030"}
{"paper_id": 267897557, "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models", "author_names": ["Zhuohao Yu", "Chang Gao", "Wenjin Yao", "Yidong Wang", "Wei Ye", "Jindong Wang", "Xing Xie", "Yue Zhang", "Shikun Zhang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered\"interactor\"role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KIEval's effectiveness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.", "year": 2024, "publicationdate": "2024-02-23", "externalids": {"DOI": "10.48550/arXiv.2402.15043"}, "doi_lower": "10.48550/arxiv.2402.15043"}
{"paper_id": 267035293, "title": "Self-Rewarding Language Models", "author_names": ["Weizhe Yuan", "Richard Yuanzhe Pang", "Kyunghyun Cho", "Sainbayar Sukhbaatar", "Jing Xu", "Jason E. Weston"], "venue": "International Conference on Machine Learning", "abstract": "We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still to explore, this work opens the door to the possibility of models that can continually improve in both axes.", "year": 2024, "publicationdate": "2024-01-18", "externalids": {"DOI": "10.48550/arXiv.2401.10020"}, "doi_lower": "10.48550/arxiv.2401.10020"}
{"paper_id": 262064568, "title": "DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services", "author_names": ["Shengbin Yue", "Wei Chen", "Siyuan Wang", "Bingxuan Li", "Chenchen Shen", "Shujun Liu", "Yuxuan Zhou", "Yao Xiao", "Song Yun", "Wei Lin", "Xuanjing Huang", "Zhongyu Wei"], "venue": "arXiv.org", "abstract": "We propose DISC-LawLLM, an intelligent legal system utilizing large language models (LLMs) to provide a wide range of legal services. We adopt legal syllogism prompting strategies to construct supervised fine-tuning datasets in the Chinese Judicial domain and fine-tune LLMs with legal reasoning capability. We augment LLMs with a retrieval module to enhance models' ability to access and utilize external legal knowledge. A comprehensive legal benchmark, DISC-Law-Eval, is presented to evaluate intelligent legal systems from both objective and subjective dimensions. Quantitative and qualitative results on DISC-Law-Eval demonstrate the effectiveness of our system in serving various users across diverse legal scenarios. The detailed resources are available at https://github.com/FudanDISC/DISC-LawLLM.", "year": 2023, "publicationdate": "2023-09-20", "externalids": {"DOI": "10.48550/arXiv.2309.11325"}, "doi_lower": "10.48550/arxiv.2309.11325"}
{"paper_id": 258587884, "title": "Automatic Evaluation of Attribution by Large Language Models", "author_names": ["Xiang Yue", "Boshi Wang", "Kai Zhang", "Ziru Chen", "Yu Su", "Huan Sun"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem.", "year": 2023, "publicationdate": "2023-05-10", "externalids": {"DOI": "10.48550/arXiv.2305.06311"}, "doi_lower": "10.48550/arxiv.2305.06311"}
{"paper_id": 264963902, "title": "STaR: Bootstrapping Reasoning With Reasoning", "author_names": ["E. Zelikman", "Yuhuai Wu", "Noah D. Goodman"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2203.14465"}, "doi_lower": "10.48550/arxiv.2203.14465"}
{"paper_id": 270213007, "title": "Automatic Instruction Evolving for Large Language Models", "author_names": ["Weihao Zeng", "Can Xu", "Yingxiu Zhao", "Jian-Guang Lou", "Weizhu Chen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that evolves instruction datasets using large language models without any human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for the given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process. Our extensive experiments demonstrate that the best method optimized by Auto Evol-Instruct outperforms human-designed methods on various benchmarks, including MT-Bench, AlpacaEval, GSM8K, and HumanEval.", "year": 2024, "publicationdate": "2024-06-02", "externalids": {"DOI": "10.48550/arXiv.2406.00770"}, "doi_lower": "10.48550/arxiv.2406.00770"}
{"paper_id": 241035539, "title": "Automatic Evaluation and Moderation of Open-domain Dialogue Systems", "author_names": ["Chen Zhang", "João Sedoc", "L. F. D’Haro", "Rafael E. Banchs", "Alexander I. Rudnicky"], "venue": "arXiv.org", "abstract": "The development of Open-Domain Dialogue Systems (ODS)is a trending topic due to the large number of research challenges, large societal and business impact, and advances in the underlying technology. However, the development of these kinds of systems requires two important characteristics:1) automatic evaluation mechanisms that show high correlations with human judgements across multiple dialogue evaluation aspects (with explainable features for providing constructive and explicit feedback on the quality of generative models' responses for quick development and deployment)and 2) mechanisms that can help to control chatbot responses,while avoiding toxicity and employing intelligent ways to handle toxic user comments and keeping interaction flow and engagement. This track at the 10th Dialogue System Technology Challenge (DSTC10) is part of the ongoing effort to promote scalable and toxic-free ODS. This paper describes the datasets and baselines provided to participants, as well as submission evaluation results for each of the two proposed subtasks.", "year": 2021, "publicationdate": "2021-11-03", "externalids": {}, "doi_lower": null}
{"paper_id": 271218325, "title": "TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot", "author_names": ["Kaiqi Zhang", "Shuai Yuan", "Honghan Zhao"], "venue": "arXiv.org", "abstract": "With the rapid development of large language models (LLM), the evaluation of LLM becomes increasingly important. Measuring text generation tasks such as summarization and article creation is very difficult. Especially in specific application domains (e.g., to-business or to-customer service), in-house evaluation criteria have to meet not only general standards (correctness, helpfulness and creativity, etc.) but also specific needs of customers and business security requirements at the same time, making the evaluation more difficult. So far, the evaluation of LLM in business scenarios has mainly relied on manual, which is expensive and time-consuming. In this paper, we propose a model-based evaluation method: TALEC, which allows users to flexibly set their own evaluation criteria, and uses in-context learning (ICL) to teach judge model these in-house criteria. In addition, we try combining zero-shot and few-shot to make the judge model focus on more information. We also propose a prompt paradigm and an engineering approach to adjust and iterate the shots ,helping judge model to better understand the complex criteria. We then compare fine-tuning with ICL, finding that fine-tuning can be replaced by ICL. TALEC demonstrates a strong capability to accurately reflect human preferences and achieves a correlation of over 80% with human judgments, outperforming even the inter-human correlation in some tasks. The code is released in https://github.com/zlkqz/auto_eval", "year": 2024, "publicationdate": "2024-06-25", "externalids": {"DOI": "10.48550/arXiv.2407.10999"}, "doi_lower": "10.48550/arxiv.2407.10999"}
{"paper_id": 273185916, "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References", "author_names": ["Qiyuan Zhang", "Yufei Wang", "Tiezheng Yu", "Yuxin Jiang", "Chuhan Wu", "Liangyou Li", "Yasheng Wang", "Xin Jiang", "Lifeng Shang", "Ruiming Tang", "Fuyuan Lyu", "Chen Ma"], "venue": "International Conference on Learning Representations", "abstract": "With significant efforts in recent studies, LLM-as-a-Judge has become a cost-effective alternative to human evaluation for assessing text generation quality in a wide range of tasks. However, there still remains a reliability gap between LLM-as-a-Judge and human evaluation. One important reason is the lack of guided oracles in the evaluation process. Motivated by the role of reference pervasively used in classic text evaluation, we introduce RevisEval, a novel text generation evaluation paradigm via the response-adapted references. RevisEval is driven by the key observation that an ideal reference should maintain the necessary relevance to the response to be evaluated. Specifically, RevisEval leverages the text revision capabilities of large language models (LLMs) to adaptively revise the response, then treat the revised text as the reference (response-adapted reference) for the subsequent evaluation. Extensive experiments demonstrate that RevisEval outperforms traditional reference-free and reference-based evaluation paradigms that use LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks. More importantly, our response-adapted references can further boost the classical text metrics, e.g., BLEU and BERTScore, compared to traditional references and even rival the LLM-as-a-Judge. A detailed analysis is also conducted to confirm RevisEval's effectiveness in bias reduction, the impact of inference cost, and reference relevance.", "year": 2024, "publicationdate": "2024-10-07", "externalids": {"DOI": "10.48550/arXiv.2410.05193"}, "doi_lower": "10.48550/arxiv.2410.05193"}
{"paper_id": 264814421, "title": "LLMaAA: Making Large Language Models as Active Annotators", "author_names": ["Ruoyu Zhang", "Yanzeng Li", "Yongliang Ma", "Ming Zhou", "Lei Zou"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.", "year": 2023, "publicationdate": "2023-10-30", "externalids": {"DOI": "10.48550/arXiv.2310.19596"}, "doi_lower": "10.48550/arxiv.2310.19596"}
{"paper_id": 6869582, "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?", "author_names": ["Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "J. Weston"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.", "year": 2018, "publicationdate": "2018-01-22", "externalids": {"DOI": "10.18653/v1/P18-1205"}, "doi_lower": "10.18653/v1/p18-1205"}
{"paper_id": 270258334, "title": "Large Language Models as Evaluators for Recommendation Explanations", "author_names": ["Xiaoyu Zhang", "Yishan Li", "Jiayin Wang", "Bowen Sun", "Weizhi Ma", "Peijie Sun", "Min Zhang"], "venue": "ACM Conference on Recommender Systems", "abstract": "The explainability of recommender systems has attracted significant attention in academia and industry. Many efforts have been made for explainable recommendations, yet evaluating the quality of the explanations remains a challenging and unresolved issue. In recent years, leveraging LLMs as evaluators presents a promising avenue in Natural Language Processing tasks (e.g., sentiment classification, information extraction), as they perform strong capabilities in instruction following and common-sense reasoning. However, evaluating recommendation explanatory texts is different from these NLG tasks, as its criteria are related to human perceptions and are usually subjective. In this paper, we investigate whether LLMs can serve as evaluators of recommendation explanations. To answer the question, we utilize real user feedback on explanations given from previous work and additionally collect third-party annotations and LLM evaluations. We design and apply a 3-level meta-evaluation strategy to measure the correlation between evaluator labels and the ground truth provided by users. Our experiments reveal that LLMs, such as GPT4, can provide comparable evaluations with appropriate prompts and settings. We also provide further insights into combining human labels with the LLM evaluation process and utilizing ensembles of multiple heterogeneous LLM evaluators to enhance the accuracy and stability of evaluations. Our study verifies that utilizing LLMs as evaluators can be an accurate, reproducible and cost-effective solution for evaluating recommendation explanation texts. Our code is available here1.", "year": 2024, "publicationdate": "2024-06-05", "externalids": {"DOI": "10.1145/3640457.3688075"}, "doi_lower": "10.1145/3640457.3688075"}
{"paper_id": 260438863, "title": "Wider and Deeper LLM Networks are Fairer LLM Evaluators", "author_names": ["Xinghua Zhang", "Yu Bowen", "Haiyang Yu", "Yangyu Lv", "Tingwen Liu", "Fei Huang", "Hongbo Xu", "Yongbin Li"], "venue": "arXiv.org", "abstract": "Measuring the quality of responses generated by LLMs is a challenging task, particularly when it comes to evaluating whether the response is aligned with human preference. A novel approach involves using the LLM itself to make evaluation and stabilizing the results through multiple independent evaluations, similar to a single-layer narrow LLM network. This network consists of a fixed number of neurons, with each neuron being the same LLM. In this paper, we draw upon the extensive research on deep neural networks to explore whether deeper and wider networks can lead to fairer evaluations. Specifically, inspired by the observation that different neurons in a neural network are responsible for detecting different concepts, we first adaptively generate as many neuron roles as possible for each evaluation sample. Each perspective corresponds to the role of a specific LLM neuron in the first layer. In subsequent layers, we follow the idea that higher layers in deep networks are responsible for more comprehensive features, each layer receives representations from all neurons in the previous layer, integrating the locally learned evaluation information to obtain a more comprehensive evaluation result. Interestingly, this network design resembles the process of academic paper reviewing. To validate the effectiveness of our method, we construct the largest and most diverse English evaluation benchmark LLMEval$^2$ for LLM evaluators, comprising 15 tasks, 8 abilities, and 2,553 samples. Experimental results demonstrate that a wider network (involving many reviewers) with 2 layers (one round of discussion) performs the best, improving kappa correlation coefficient from 0.28 to 0.34. We also leverage WideDeep to aid in the assessment of Chinese LLMs, which has accelerated the evaluation time by 4.6 times, resulting in a 60% cost saving. WideDeep achieves a remarkable 93% agreement level among humans.", "year": 2023, "publicationdate": "2023-08-03", "externalids": {"DOI": "10.48550/arXiv.2308.01862"}, "doi_lower": "10.48550/arxiv.2308.01862"}
{"paper_id": 276569369, "title": "Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions", "author_names": ["Ruochen Zhao", "Wenxuan Zhang", "Yew Ken Chia", "Deli Zhao", "Li Bing"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2405.20267"}, "doi_lower": "10.48550/arxiv.2405.20267"}
{"paper_id": 257900969, "title": "A Survey of Large Language Models", "author_names": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Z. Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "J. Nie", "Ji-rong Wen"], "venue": "arXiv.org", "abstract": "Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.", "year": 2023, "publicationdate": "2023-03-31", "externalids": {}, "doi_lower": null}
{"paper_id": 271769567, "title": "Measuring the Inconsistency of Large Language Models in Preferential Ranking", "author_names": ["Xiutian Zhao", "Ke Wang", "Wei Peng"], "venue": "KNOWLLM", "abstract": "Despite large language models’ (LLMs’) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent and preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLMs reveal their inability to meet these criteria, indicating a strong positional bias and poor transitivity, with preferences easily swayed by irrelevant alternatives. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.", "year": 2024, "publicationdate": "2024-10-11", "externalids": {"DOI": "10.18653/v1/2024.knowllm-1.14"}, "doi_lower": "10.18653/v1/2024.knowllm-1.14"}
{"paper_id": 261101090, "title": "Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models", "author_names": ["Yachao Zhao", "Bo Wang", "Dongming Zhao", "Kun Huang", "Yan Wang", "Ruifang He", "Yuexian Hou"], "venue": "arXiv.org", "abstract": "Recent researches indicate that Pre-trained Large Language Models (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs. This paper focuses on explicit and implicit social bias, a distinctive two-level cognitive construct in psychology. It posits that individuals' explicit social bias, which is their conscious expression of bias in the statements, may differ from their implicit social bias, which represents their unconscious bias. We propose a two-stage approach and discover a parallel phenomenon in LLMs known as\"re-judge inconsistency\"in social bias. In the initial stage, the LLM is tasked with automatically completing statements, potentially incorporating implicit social bias. However, in the subsequent stage, the same LLM re-judges the biased statement generated by itself but contradicts it. We propose that this re-judge inconsistency can be similar to the inconsistency between human's unaware implicit social bias and their aware explicit social bias. Experimental investigations on ChatGPT and GPT-4 concerning common gender biases examined in psychology corroborate the highly stable nature of the re-judge inconsistency. This finding may suggest that diverse cognitive constructs emerge as LLMs' capabilities strengthen. Consequently, leveraging psychological theories can provide enhanced insights into the underlying mechanisms governing the expressions of explicit and implicit constructs in LLMs.", "year": 2023, "publicationdate": "2023-08-24", "externalids": {"DOI": "10.48550/arXiv.2308.12578"}, "doi_lower": "10.48550/arxiv.2308.12578"}
{"paper_id": 231979430, "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "author_names": ["Tony Zhao", "Eric Wallace", "Shi Feng", "D. Klein", "Sameer Singh"], "venue": "International Conference on Machine Learning", "abstract": "GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.", "year": 2021, "publicationdate": "2021-02-19", "externalids": {}, "doi_lower": null}
{"paper_id": 261582594, "title": "Large Language Models Are Not Robust Multiple Choice Selectors", "author_names": ["Chujie Zheng", "Hao Zhou", "Fandong Meng", "Jie Zhou", "Minlie Huang"], "venue": "International Conference on Learning Representations", "abstract": "Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent\"selection bias\", namely, they prefer to select specific option IDs as answers (like\"Option A\"). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs' token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model's prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.", "year": 2023, "publicationdate": "2023-09-07", "externalids": {"DOI": "10.48550/arXiv.2309.03882"}, "doi_lower": "10.48550/arxiv.2309.03882"}
{"paper_id": 259129398, "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena", "author_names": ["Lianmin Zheng", "Wei-Lin Chiang", "Ying Sheng", "Siyuan Zhuang", "Zhanghao Wu", "Yonghao Zhuang", "Zi Lin", "Zhuohan Li", "Dacheng Li", "E. Xing", "Haotong Zhang", "Joseph E. Gonzalez", "Ion Stoica"], "venue": "Neural Information Processing Systems", "abstract": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.", "year": 2023, "publicationdate": "2023-06-09", "externalids": {}, "doi_lower": null}
{"paper_id": 273233252, "title": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates", "author_names": ["Xiaosen Zheng", "Tianyu Pang", "Chao Du", "Qian Liu", "Jing Jiang", "Min Lin"], "venue": "International Conference on Learning Representations", "abstract": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a\"null model\"that always outputs a constant response (irrelevant to input instructions) can cheat automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench. Moreover, the crafted cheating outputs are transferable because we assume that the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are private and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. The code is available at https://github.com/sail-sg/Cheating-LLM-Benchmarks.", "year": 2024, "publicationdate": "2024-10-09", "externalids": {"DOI": "10.48550/arXiv.2410.07137"}, "doi_lower": "10.48550/arxiv.2410.07137"}
{"paper_id": 272881277, "title": "Mitigating the Bias of Large Language Model Evaluation", "author_names": ["Hongli Zhou", "Hui Huang", "Yunfei Long", "Bing Xu", "Conghui Zhu", "Hailong Cao", "Muyun Yang", "Tiejun Zhao"], "venue": "China National Conference on Chinese Computational Linguistics", "abstract": "“Recently, there has been a trend of evaluating the Large Language Model (LLM) quality in theflavor of LLM-as-a-Judge, namely leveraging another LLM to evaluate the current output qual-ity. However, existing judges are proven to be biased, namely they would favor answers whichpresent better superficial quality (such as verbosity, fluency) while ignoring the instruction fol-lowing ability. In this work, we propose systematic research about the bias of LLM-as-a-Judge.Specifically, for closed-source judge models, we apply calibration to mitigate the significance ofsuperficial quality, both on probability level and prompt level. For open-source judge models, wepropose to mitigate the bias by contrastive training, with curated negative samples that deviatefrom instruction but present better superficial quality. We apply our methods on the bias evalu-ation benchmark, and experiment results show our methods mitigate the bias by a large marginwhile maintaining a satisfactory evaluation accuracy.”", "year": 2024, "publicationdate": "2024-09-25", "externalids": {"DOI": "10.48550/arXiv.2409.16788"}, "doi_lower": "10.48550/arxiv.2409.16788"}
{"paper_id": 270560949, "title": "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "author_names": ["Han Zhou", "Xingchen Wan", "Yinhong Liu", "Nigel Collier", "Ivan Vuli'c", "Anna Korhonen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.", "year": 2024, "publicationdate": "2024-06-17", "externalids": {"DOI": "10.48550/arXiv.2406.11370"}, "doi_lower": "10.48550/arxiv.2406.11370"}
{"paper_id": 263310485, "title": "Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering", "author_names": ["Han Zhou", "Xingchen Wan", "Lev Proleev", "Diana Mincu", "Jilin Chen", "Katherine A. Heller", "Subhrajit Roy"], "venue": "International Conference on Learning Representations", "abstract": "Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allow it to learn the contextual bias from labeled data. We validate the effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.", "year": 2023, "publicationdate": "2023-09-29", "externalids": {"DOI": "10.48550/arXiv.2309.17249"}, "doi_lower": "10.48550/arxiv.2309.17249"}
{"paper_id": 269803977, "title": "Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks", "author_names": ["Ruiyang Zhou", "Lu Chen", "Kai Yu"], "venue": "International Conference on Language Resources and Evaluation", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 264289186, "title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents", "author_names": ["Xuhui Zhou", "Hao Zhu", "Leena Mathur", "Ruohong Zhang", "Haofei Yu", "Zhengyang Qi", "Louis-philippe Morency", "Yonatan Bisk", "Daniel Fried", "Graham Neubig", "Maarten Sap"], "venue": "International Conference on Learning Representations", "abstract": "Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.", "year": 2023, "publicationdate": "2023-10-18", "externalids": {"DOI": "10.48550/arXiv.2310.11667"}, "doi_lower": "10.48550/arxiv.2310.11667"}
{"paper_id": 269983268, "title": "Calibrated Self-Rewarding Vision Language Models", "author_names": ["Yiyang Zhou", "Zhiyuan Fan", "Dongjie Cheng", "Sihan Yang", "Zhaorun Chen", "Chenhang Cui", "Xiyao Wang", "Yun Li", "Linjun Zhang", "Huaxiu Yao"], "venue": "Neural Information Processing Systems", "abstract": "Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality. Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization. These approaches may not effectively reflect the target LVLM's preferences, making the curated preferences easily distinguishable. Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning. In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input. Empirical results demonstrate that CSR enhances performance and reduces hallucinations across ten benchmarks and tasks, achieving substantial improvements over existing methods by 7.62%. Our empirical results are further supported by rigorous theoretical analysis, under mild assumptions, verifying the effectiveness of introducing visual constraints into the self-rewarding paradigm. Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning. Our data and code are available at https://github.com/YiyangZhou/CSR.", "year": 2024, "publicationdate": "2024-05-23", "externalids": {"DOI": "10.48550/arXiv.2405.14622"}, "doi_lower": "10.48550/arxiv.2405.14622"}
{"paper_id": 273229041, "title": "Rational Metareasoning for Large Language Models", "author_names": ["Nicolò De Sabbata", "T. Sumers", "Badr AlKhamissi", "Thomas L. Griffiths"], "venue": "arXiv.org", "abstract": "Being prompted to engage in reasoning has emerged as a core technique for using large language models (LLMs), deploying additional inference-time compute to improve task performance. However, as LLMs increase in both size and adoption, inference costs are correspondingly becoming increasingly burdensome. How, then, might we optimize reasoning's cost-performance tradeoff? This work introduces a novel approach based on computational models of metareasoning used in cognitive science, training LLMs to selectively use intermediate reasoning steps only when necessary. We first develop a reward function that incorporates the Value of Computation by penalizing unnecessary reasoning, then use this reward function with Expert Iteration to train the LLM. Compared to few-shot chain-of-thought prompting and STaR, our method significantly reduces inference costs (20-37\\% fewer tokens generated across three models) while maintaining task performance across diverse datasets.", "year": 2024, "publicationdate": "2024-10-07", "externalids": {"DOI": "10.48550/arXiv.2410.05563"}, "doi_lower": "10.48550/arxiv.2410.05563"}
{"paper_id": 264490588, "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges", "author_names": ["Lianghui Zhu", "Xinggang Wang", "Xinlong Wang"], "venue": "International Conference on Learning Representations", "abstract": "Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, multi-turn chat, etc. Code is available at https://github.com/baaivision/JudgeLM.", "year": 2023, "publicationdate": "2023-10-26", "externalids": {"DOI": "10.48550/arXiv.2310.17631"}, "doi_lower": "10.48550/arxiv.2310.17631"}
{"paper_id": 264146620, "title": "A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models", "author_names": ["Shengyao Zhuang", "Honglei Zhuang", "B. Koopman", "G. Zuccon"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "We propose a novel zero-shot document ranking approach based on Large Language Models (LLMs): the Setwise prompting approach. Our approach complements existing prompting approaches for LLM-based zero-shot ranking: Pointwise, Pairwise, and Listwise. Through the first-of-its-kind comparative evaluation within a consistent experimental framework and considering factors like model size, token consumption, latency, among others, we show that existing approaches are inherently characterised by trade-offs between effectiveness and efficiency. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. Our Setwise approach, instead, reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, compared to previous methods. This significantly improves the efficiency of LLM-based zero-shot ranking, while also retaining high zero-shot ranking effectiveness. We make our code and results publicly available at https://github.com/ielab/llm-rankers.", "year": 2023, "publicationdate": "2023-10-14", "externalids": {"DOI": "10.1145/3626772.3657813"}, "doi_lower": "10.1145/3626772.3657813"}
{"paper_id": 273350802, "title": "Agent-as-a-Judge: Evaluate Agents with Agents", "author_names": ["Mingchen Zhuge", "Changsheng Zhao", "Dylan R. Ashley", "Wenyi Wang", "Dmitrii Khizbullin", "Yunyang Xiong", "Zechun Liu", "Ernie Chang", "Raghuraman Krishnamoorthi", "Yuandong Tian", "Yangyang Shi", "Vikas Chandra", "Jurgen Schmidhuber"], "venue": "arXiv.org", "abstract": "Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.", "year": 2024, "publicationdate": "2024-10-14", "externalids": {"DOI": "10.48550/arXiv.2410.10934"}, "doi_lower": "10.48550/arxiv.2410.10934"}
{"paper_id": 258352761, "title": "ICE-Score: Instructing Large Language Models to Evaluate Code", "author_names": ["Terry Yue Zhuo"], "venue": "Findings", "abstract": "Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text. Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code intelligence tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code intelligence tasks. Moreover, utilizing human-written test suites to evaluate functional correctness can be challenging in domains with low resources. To overcome these obstacles, we propose ICE-Score, a new evaluation metric via instructing large language models (LLMs) for code assessments. Our metric addresses the limitations of existing approaches by achieving superior correlations with functional correctness and human preferences, without the need for test oracles or references. We evaluate the efficacy of our metric on two different aspects (human preference and execution success) and four programming languages. Our results demonstrate that our metric surpasses state-of-the-art metrics for code generation, delivering high levels of accuracy and consistency across various programming languages and tasks. We also make our evaluation metric and datasets available to the public, encouraging further research in evaluating code intelligence tasks.", "year": 2023, "publicationdate": "2023-04-27", "externalids": {}, "doi_lower": null}
{"paper_id": 260202961, "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models", "author_names": ["Andy Zou", "Zifan Wang", "J. Z. Kolter", "Matt Fredrikson"], "venue": "arXiv.org", "abstract": "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.", "year": 2023, "publicationdate": "2023-07-27", "externalids": {}, "doi_lower": null}
