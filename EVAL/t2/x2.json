{
  "survey": "Large Language Models (LLMs) have become integral to AI evaluation, serving as advanced judges in assessing both human and AI-generated outputs. This survey explores the methodologies for deploying LLMs as evaluative tools, emphasizing their role in enhancing natural language processing (NLP) and AI performance assessment. LLMs offer cost-effective solutions to critical bottlenecks, such as the scarcity of high-quality annotations, by aligning evaluations closely with human judgment. Innovative frameworks like Prometheus and benchmarks such as Topical-Chat and TREC Deep Learning track illustrate the diverse applications of LLMs in evaluative contexts. Despite these advancements, challenges persist, including biases, interpretability issues, and resource constraints. The survey highlights the need for robust evaluation frameworks that mitigate these limitations, ensuring that LLMs can effectively complement human judgment. Future directions include enhancing LLM capabilities, integrating multimodal data, and refining feedback processes to align evaluations with evolving human standards. This comprehensive review underscores the transformative potential of LLMs in AI evaluation, advocating for continued research to refine methodologies and expand datasets, ensuring that LLMs remain relevant and effective across diverse application domains.\n\nIntroduction Concept of LLMs as Judges Large Language Models (LLMs) have become essential in AI evaluation, acting as sophisticated judges that assess both human and AI-generated outputs. Their capacity to generate informative critiques is crucial for evaluating the quality of texts produced by language models [1]. In legal contexts, the evaluation of LLMs necessitates stringent standards of accuracy, reliability, and fairness, highlighting their importance in high-stakes applications [2]. TrustorEsc emphasizes the need for evaluation methods that closely mirror human judgment, advocating for approaches that reflect human evaluative processes [3]. Prometheus, an open-source LLM, exemplifies advancements in evaluation methods by assessing long-form responses, thus enhancing evaluation depth and precision [4]. The challenge of creating open-domain conversations that leverage world knowledge is illustrated by the Topical-Chat benchmark, showcasing diverse LLM applications in evaluation contexts [5]. In document retrieval, LLMs serve as judges requiring nuanced analyses for determining document relevance, which is vital for refining search algorithms [6]. The TREC Deep Learning track aims to improve passage and document ranking evaluations, reflecting ongoing efforts to optimize LLM performance in complex information retrieval scenarios [7]. Despite their capabilities, LLMs face challenges as evaluators. The Bayesian Calibration study identifies issues with unreliable win rate estimation, stemming from intrinsic biases and inaccuracies in LLM evaluations [8]. Additionally, the vulnerabilities and performance limitations of various LLMs as judges reveal complexities in replicating human-like judgment [9]. These findings underscore the necessity for robust evaluation frameworks that mitigate biases and enhance the reliability of LLM-based assessments. Significance in AI Evaluation The integration of LLMs within AI evaluation frameworks marks significant advancements, particularly in improving the accuracy and efficiency of evaluative processes. LLMs address critical bottlenecks, such as the lack of high-quality annotations in machine learning, by providing cost-effective evaluation methods [10]. However, challenges persist, including the generation of hallucinated content, reliance on outdated information, and the opacity of reasoning processes [11]. To combat these issues, critiques using LLMs have been developed to produce detailed evaluations, thereby enhancing overall performance [1]. The importance of LLMs is further underscored by efforts to align evaluations with human judgments, thereby increasing trust and accuracy [3]. Nonetheless, reliance on LLMs for generating relevance judgments can be inadequate if not properly managed, potentially hindering the evaluation process [12]. The Prometheus framework distinguishes itself by providing a customizable evaluation tool for long-form text, overcoming limitations associated with proprietary LLMs [4]. Advanced evaluation benchmarks, such as Topical-Chat, have become integral to open-domain conversational AI research, offering datasets that simulate human-to-human interactions [5]. The application of Bayesian calibration methods further highlights LLMs' potential to refine evaluation accuracy and efficiency [8]. AgentBench is also recognized as a pivotal benchmark for enhancing the understanding and capabilities of LLMs as interactive agents [13]. Innovative frameworks and benchmarks, including the development of retrieval models, enhance evaluations by distinguishing between various approaches tailored for efficiency and accuracy [7]. These advancements collectively establish a transformative role for LLMs in AI evaluation, ensuring frameworks are robust and aligned with evolving human standards. Objectives of the Paper This survey aims to critically evaluate the effectiveness of LLMs in evaluative roles, focusing on approaches like Eval-Instruct to refine the quality and informativeness of critiques for LLM outputs [1]. Another key goal is to explore frameworks such as Cascaded Selective Evaluation, which seeks to ensure LLM-based assessments align with human judgments, thereby enhancing reliability and acceptance [3]. The survey highlights the Prometheus framework, emphasizing its utility in evaluating long-form text using custom scoring rubrics and advocating for an open-source evaluation benchmark for broader accessibility [4]. It also discusses agentic rerankers like JudgeRank, designed to mimic human cognitive processes for improving document relevance assessment [6]. To address challenges related to win-rate estimation inaccuracies in LLM evaluations, advanced calibration techniques such as Bayesian Win-Rate Sampling (BWRS) and Bayesian Dawid-Skene are presented as essential for enhancing evaluative metrics [8]. The survey evaluates various judge models within the LLM-as-a-judge paradigm, examining their performance against multiple exam-taker models, thereby providing insights into the robustness and versatility of LLM evaluations [9]. Ultimately, the survey aims to strengthen LLMs' roles in AI evaluation by aligning evaluations with human standards and advancing methodologies across diverse application domains. It addresses the limitations of traditional metrics like BLEU and ROUGE, which struggle with creative or high-quality LLM outputs, emphasizing the need for human input to ensure robust evaluations. The review highlights comprehensive methods, the importance of domain-specific benchmarks, and the challenges of bias in LLM evaluations, particularly in retrieval-augmented generation tasks, underscoring the necessity for careful validation against human judgments to enhance the reliability of LLMs [14,15,16,17,18]. Structure of the Survey This survey is structured to guide readers through the multifaceted role of LLMs as judges in AI evaluation. The paper begins by defining the problem and introducing the concept of LLMs as judges, setting the stage for understanding their significance and objectives within AI evaluation. Following this, the survey provides background insights into AI evaluation in natural language processing (NLP), tracing the evolution and relevance of LLMs. The core section explores LLM-based evaluation methods, highlighting innovative techniques and frameworks that leverage LLMs for enhanced assessment capabilities. This includes a detailed examination of benchmarking and evaluation frameworks, alongside innovative techniques addressing bias mitigation and calibration challenges. The paper offers a comparative analysis between evaluations conducted by LLMs and humans, emphasizing performance metrics and methodologies. It discusses the limitations of traditional metrics like BLEU and ROUGE in assessing creative outputs and the challenges of scaling human evaluations. The study delves into the promising yet complex role of LLMs as evaluators, noting concerns about trust and reliability while underscoring the importance of integrating human input to align evaluations with human intent. Findings from various studies are presented, including variability in LLM evaluations across tasks and biases observed in LLMs, particularly their self-preference in rating tasks. The paper introduces novel methodologies to enhance transparency, diversity, and effectiveness in LLM evaluation systems, such as domain-specific evaluation sets and recalibration procedures to mitigate biases, ultimately aiming to improve alignment with human preferences [19,14,15,16,17]. Subsequent sections address inherent challenges and limitations in using LLMs as judges, discussing biases, interpretability, complexity, and resource constraints, along with the importance of data quality and security risks. The survey illustrates practical applications and case studies showcasing real-world scenarios where LLMs have been deployed as judges, including their roles in code generation, text summarization, and multimodal conversational AI. Finally, the paper concludes with future directions, suggesting enhancements in LLM capabilities and benchmarks, integration with multimodal data, advancements in feedback processes, and exploration of novel applications and emerging trends. This comprehensive organization, complemented by the multi-agent debate framework ChatEval, presents experimental results that underscore the effectiveness of LLMs in comparison to human evaluations [20].The following sections are organized as shown in . Background and Preliminaries History of AI Evaluation in NLP AI evaluation in natural language processing (NLP) has evolved from human-centric methods to sophisticated automated approaches, addressing scalability and cost challenges associated with human-generated data, particularly in text summarization and dialog systems [21]. Structured datasets like DailyDialog were introduced to support dialog system development [22]. As the field advanced, benchmarks became crucial for generative tasks involving complex cognitive processes, such as mathematical reasoning [23]. Hierarchical Storytelling benchmarks were established to generate coherent narratives [24], while the TREC NeuCLIR track evaluated neural methods in cross-language information retrieval, emphasizing global information access [25]. Despite advancements, the lack of standardized benchmarks hindered effective metric assessment and comparison [26]. Historical evaluations in AI summarization faced challenges due to binary treatments of factuality, necessitating nuanced approaches [27]. The TREC Deep Learning track addressed document and passage ranking challenges in information retrieval systems [28]. AI evaluation historically focused on knowledge and reasoning, often neglecting alignment with human values, crucial for comprehensive evaluation [29]. The Topical-Chat benchmark enhanced topical coverage, addressing existing dataset limitations [5]. Research on hallucination metrics and mitigation strategies contributed to structuring the evaluation landscape [30]. The evolution of AI evaluation in NLP reflects ongoing efforts to refine methodologies, transitioning from static to dynamic approaches that adapt to model changes during training, essential in contemporary AI evaluation [31]. Evolution and Relevance of LLMs The development of Large Language Models (LLMs) marks a significant milestone in AI evaluation methodologies, emphasizing their relevance in enhancing evaluative processes across domains. Initially limited in reasoning capabilities, LLMs improved through techniques like chain of thought prompting, enhancing decision-making [32]. Prototypical networks further exemplified robust classification tasks [33]. Prometheus advanced LLMs by offering evaluation capabilities similar to proprietary models, allowing user-defined scoring rubrics for customized assessments [4]. This adaptability aligns LLM evaluations with specific domain requirements, as demonstrated by LexEval, which introduces a taxonomy of legal cognitive abilities within the largest Chinese legal evaluation dataset [2]. Domain-specific enhancements underscore cultural and linguistic considerations in AI evaluations. Benchmarks like Topical-Chat expanded LLM evaluations by removing explicit conversational roles and encompassing a broader array of topics [5]. The Overview benchmark enhanced reliability by focusing on passage retrieval and significantly increasing dataset size [7]. These advancements reflect efforts to optimize LLM capabilities in complex information retrieval tasks. The AgentBench benchmark highlights performance disparities between commercial and open-source LLMs, underscoring the need for quantitative assessments addressing these differences [13]. The PHUDGE framework illustrates that simpler models, when systematically experimented with and thoughtfully augmented, can yield superior results, challenging the assumption that complexity is inherently beneficial [34]. Despite advancements, challenges remain in specialized domains like medical text evaluation, where existing benchmarks inadequately address unique needs [35]. The evolution of LLMs aims to overcome obstacles by developing specialized metrics catering to domain-specific requirements. Collectively, these developments highlight LLMs' critical role in advancing AI evaluation methodologies, ensuring alignment with real-world standards and enhancing AI systems' efficacy across diverse applications. Current Challenges in AI Evaluation The integration of Large Language Models (LLMs) as evaluators in AI systems presents complex challenges affecting evaluation reliability and effectiveness. A key issue is the inadequacy of judge models, which may not outperform evaluated models, limiting current debiasing methods' effectiveness [10]. Using LLMs as proxies for human judges establishes a performance ceiling, constraining evaluation efficacy [12]. LLMs struggle with long-term reasoning, decision-making, and instruction-following, as revealed by the AgentBench benchmark [13]. Generating informative critiques that effectively differentiate AI-generated texts remains challenging, with existing models often misaligned with human evaluative standards [1]. This misalignment is exacerbated by over-reliance on model preferences, leading to evaluations diverging from human judgments [3]. LLMs' inability to perform nuanced analyses in reasoning-intensive tasks, such as document relevance judgments, underscores current evaluators' limitations [6]. Inaccurate comparisons of generative systems using LLM evaluators result in misleading conclusions about performance, challenging AI evaluations' credibility [8]. Significant scoring discrepancies between LLM and human judges have been noted, with judge models sensitive to prompt complexity and length, complicating evaluation [9]. Existing benchmarks often lack capacity to accurately assess specialized fields, such as medical text generation, resulting in insufficient evaluations [35]. These challenges highlight the need for refined evaluation frameworks. To ensure LLMs' sustainable development as robust evaluative tools, advancing systematic bias measurement, enhancing reasoning capabilities, and improving alignment with human evaluative standards is essential. Recent studies reveal significant biases in LLMs, particularly in evaluative tasks favoring self-generated content. Within retrieval-augmented generation (RAG) frameworks, factual accuracy is prioritized over stylistic elements, indicating less self-preference bias. Both human and LLM judges exhibit biases, such as misinformation oversight and gender bias, impacting evaluation reliability. Human-centered design approaches, like EvaluLLM, emphasize integrating human input to ensure evaluation criteria align with human intent and expectations, fostering trust and reliability in LLM-as-a-judge systems. Addressing biases and enhancing human-LLM collaboration are pivotal for developing robust, unbiased, and effective evaluation systems [17,36,16]. In the realm of evaluating large language models (LLMs), it is crucial to understand the various methodologies that have emerged. These methodologies can be systematically categorized to provide a clearer understanding of their contributions to the field. illustrates the hierarchical structure of LLM-based evaluation methods, categorizing innovations, benchmarking frameworks, evaluation techniques, and bias mitigation strategies. Each category is further divided into specific methods and frameworks, highlighting their contributions to enhancing precision, reliability, and fairness in LLM evaluations. This structured approach not only aids researchers in navigating the complex landscape of LLM evaluations but also underscores the importance of comprehensive assessment in advancing model development and application. LLM-based Evaluation Methods Innovations in Evaluation Methods Recent advancements in LLM-based evaluation methods have introduced sophisticated frameworks that enhance precision and reliability. The PHUDGE method employs a generalized Earth Movers Distance as a loss function for grading tasks, refining evaluation processes [34]. Eval-Instruct combines pseudo references with multi-path prompting, improving the quality of LLM-generated critiques [1]. TrustorEsc's Cascaded Selective Evaluation method integrates confidence estimation with selective model evaluation, offering a reliable assessment framework [3]. Prometheus allows for customizable scoring rubrics, facilitating tailored assessments aligned with specific evaluative needs [4]. JudgeRank enhances document relevance assessments through query analysis, document analysis, and relevance judgment [6]. The Overview benchmark expands dataset sizes, providing a comprehensive evaluation basis [7]. Bayesian inference techniques offer more accurate metrics, distinguishing LLM evaluations from traditional approaches [8]. A framework categorizing judge models based on performance and alignment with human scores provides insights into LLM evaluation effectiveness [9]. In specialized domains like medical text evaluation, DOCLENS addresses unique requirements where traditional metrics fall short [35]. illustrates the innovations in LLM-based evaluation methods, categorized into advanced methods, customizable frameworks, and domain-specific tools. Each category highlights key methodologies and benchmarks that have contributed to enhancing precision, reliability, and domain applicability in large language model evaluations. These innovations collectively represent a significant leap in LLM-based evaluation methodologies, equipping practitioners with robust frameworks for nuanced, fair, and reliable assessments across diverse domains. Benchmarking and Evaluation Frameworks Benchmarking and evaluation frameworks are crucial for systematically assessing LLM performance across various tasks and domains. Table provides a detailed overview of representative benchmarks used in the systematic assessment of LLM performance across multiple domains and task formats. Prometheus benchmarks its performance against proprietary models such as GPT-4 and ChatGPT, establishing evaluative capabilities through detailed metrics [4]. This comparative approach is vital for understanding the relative strengths and weaknesses of models in real-world applications. The Overview benchmark evaluates models based on ranking performance using specified metrics across test collections, emphasizing standardized criteria for consistent evaluations [7]. DocLens introduces multi-aspect fine-grained metrics focusing on completeness, conciseness, and attribution, providing a nuanced evaluation framework suited to specialized domains [35]. These frameworks and benchmarks signify advancements in LLM evaluations by introducing domain-specific evaluation sets, comprehensive surveys, and innovative methodologies like automated debate frameworks. They enhance reliability, comprehensiveness, and relevance, addressing existing benchmarks' limitations by ensuring high separability and alignment with human preferences. These advancements contribute to the responsible development of LLMs, maximizing societal benefits while minimizing potential risks [37,38,14]. Innovative Evaluation Techniques Recent advancements in evaluation techniques for NLP tasks utilizing LLMs have introduced sophisticated methods aimed at enhancing assessment accuracy and effectiveness. Table presents a comprehensive comparison of recent evaluation methods for natural language processing tasks, showcasing their distinct approaches to enhancing reasoning and multi-dimensional assessment. Chain of Thought prompting improves reasoning performance by providing structured guidance, enabling LLMs to tackle complex problems with enhanced clarity [32]. HotFlip generates adversarial examples through strategic token swapping based on gradients, aiding in evaluating LLM robustness against perturbations [39]. ALLURE leverages iterative in-context learning to refine accuracy, epitomizing cutting-edge methodological progress [40]. Creative Backend Evaluation combines response generation with post-evaluation validation, enhancing the interplay between generation and evaluation processes [41]. The FineSurE framework comprehensively evaluates summaries across multiple dimensions, emphasizing diverse metrics in assessing LLM outputs [42]. Qwen models demonstrate superior performance in specialized tasks, surpassing existing benchmarks [43]. Prototypical networks calibrate predictions based on the likelihood of assigned cluster prototypes, enhancing predictive accuracy [33]. The ChatEval framework exemplifies a multi-agent approach, leveraging multiple LLMs to enhance evaluation quality through diversified perspectives [20]. Accurate model performance evaluation is exemplified by assessments quantifying correctly solved problems based on generated code execution results, providing a practical measure of functional accuracy [44]. These techniques signify a transformative shift in LLM-based NLP evaluation, broadening the scope and depth of assessment capabilities in evolving AI applications. Bias Mitigation and Calibration Strategies for bias mitigation and calibration in LLM evaluations are crucial for advancing AI systems' robustness and fairness. Self-recognition capability assessments aim to reduce self-preference bias in LLM decision-making processes [45]. CalibraEval addresses selection bias by aligning observed prediction distributions with unbiased distributions, enhancing LLM output credibility [46]. PromptAttack enables precise manipulation of LLM outputs through structured prompt designs, preserving original meanings and ensuring faithful evaluations [47]. The PBMM framework employs entropy regularization and bias ensembling to mitigate position bias during training, refining LLM evaluative outputs [48]. CRISPR automatically identifies biased outputs and categorizes associated neurons as bias neurons, streamlining bias identification and rectification processes [49]. These interventions strengthen LLM evaluation frameworks by addressing inherent biases and enabling calibrated, objective assessments. They enhance interpretability and fairness in evaluative judgments while contributing to AI systems that better align with human expectations and ethical standards. Techniques such as bias correction, explicit error analysis, and selective evaluation frameworks improve automated evaluators' accuracy and reliability, fostering AI models consistent with human values. Recalibration procedures have shown significant improvements in aligning language model assessments with human evaluations, while cascaded selective evaluation frameworks ensure strong human agreement, even with cost-effective models. These methodologies emphasize integrating human input to refine evaluation criteria, ensuring AI systems are evaluated in alignment with human preferences and ethical considerations [19,3,50,15,17]. Comparison with Human-Based Evaluation In the realm of artificial intelligence, particularly in evaluating language models, discerning the comparative effectiveness of human-based assessments versus those conducted by Large Language Models (LLMs) is essential. This section investigates the fundamental distinctions and overlaps between human evaluations and LLM assessments, analyzing their methodologies to identify respective strengths and weaknesses, thereby deepening our understanding of their roles in the evaluation landscape. Human vs. LLM Evaluations Comparing human evaluations with LLM assessments reveals both clear differences and notable similarities. Models like Flan-UL2, InstructGPT, and ChatGPT, with substantial parameter counts, have been rigorously tested against human evaluations, showing varying degrees of alignment with human judgment [51]. This underscores the complexity of replicating human evaluative standards within AI systems. The USR framework, when applied to dialog generation models, shows a strong correlation with human judgment, indicating LLMs' potential to mirror human evaluative processes in specific contexts [52]. This correlation is crucial for establishing LLMs as credible evaluators, especially in dialog systems where nuanced understanding is vital. Experiments with TigerScore facilitate comparisons between human and LLM evaluations, identifying strengths and areas for improvement in LLM methodologies [53]. Such analyses are vital for pinpointing gaps in LLM performance and guiding the refinement of evaluation techniques to align more closely with human standards. VideoAutoArena has demonstrated efficacy in distinguishing among models, providing insights into strengths and weaknesses critical for enhancing LLM evaluation frameworks [54]. This differentiation ensures that LLM assessments are comprehensive and reflective of diverse evaluative criteria. In specialized domains like medical text evaluation, DocLens has shown significantly higher agreement with medical expert judgments compared to traditional metrics, illustrating LLMs' potential to achieve domain-specific alignment with human standards [35]. Such alignment is crucial for advancing the reliability of LLMs in fields where expert judgment is paramount. Recent studies emphasize refining evaluation methodologies that integrate human and LLM assessments to enhance LLMs' accuracy, reliability, and credibility across diverse domains. Traditional metrics like BLEU and ROUGE often fall short for the nuanced outputs of LLMs, advocating for human involvement to ensure robust evaluations. Initiatives like JUDGE-BENCH provide extensive datasets for validating LLMs against human annotations, revealing variability in model reliability across tasks. Domain-specific evaluation sets for LLM-as-a-Judge frameworks highlight gaps in current benchmarks, proposing novel pipelines that improve transparency and effectiveness in model evaluation. Studies addressing biases in LLM evaluations within retrieval-augmented generation frameworks suggest factual accuracy is critical, challenging previous assumptions about LLM self-preference effects. As illustrated in , this figure highlights the key frameworks, metrics, and challenges in comparing human and LLM evaluations, emphasizing the alignment of LLM outputs with human judgments across various domains. Collectively, these explorations advance the methodologies underpinning the evaluation and trustworthiness of LLM systems, addressing bias concerns and aligning with practitioner expectations in varied applications [17,15,16,14]. Alignment and Discrepancies The alignment between LLM evaluations and human judgments is a critical aspect of AI assessment, revealing concordances and divergences in evaluative outcomes. The USR framework has shown a strong correlation with human judgment, particularly in contexts requiring nuanced understanding and conversational coherence, suggesting LLMs can effectively replicate certain aspects of human judgment [52]. Conversely, discrepancies arise where LLMs struggle to capture the depth of human evaluative criteria. For example, DocLens has demonstrated higher agreement with medical expert judgments than traditional metrics, yet gaps remain in accurately reflecting the complex reasoning employed by human experts [35]. This divergence underscores the challenges faced by LLMs in specialized domains where expert knowledge and contextual understanding are critical. Further analysis using TigerScore identifies areas where LLM evaluations diverge from human assessments, particularly in tasks requiring subjective interpretation and cultural sensitivity [53]. Such discrepancies highlight the limitations of LLMs in fully understanding and replicating human evaluative standards across diverse contexts. The VideoAutoArena framework provides valuable insights into these alignment and discrepancy scenarios, emphasizing the need for continuous refinement in LLM methodologies to bridge the gap between AI and human evaluations [54]. This ongoing effort is crucial for enhancing the credibility and reliability of LLMs as evaluative tools, ensuring they effectively complement human judgment in complex assessment tasks. Performance Metrics and Evaluation Methods Evaluating LLMs against human assessments necessitates sophisticated performance metrics and evaluation methods capturing both qualitative and quantitative aspects. This comprehensive approach spans diverse domains, including natural language processing, ethics, and multilingual capabilities. Current evaluation frameworks emphasize assessing LLMs for task-specific performance and broader societal impacts, including potential risks like data privacy concerns and harmful content generation. Innovative methods, such as LLM-based evaluators and debate-based benchmarking, are explored to address scalability and bias issues, particularly in multilingual contexts. These developments underscore the need for continuous refinement of evaluation strategies to ensure the safe and beneficial evolution of LLMs, maximizing societal benefits while minimizing risks [55,37,18,38]. Key performance metrics include task completion accuracy, alignment with human evaluative standards, and reliability across various domains. The challenge in comparing LLMs with human evaluations lies in establishing standardized metrics that effectively measure qualitative aspects of assessment processes. Metrics like Fleiss kappa, evaluating inter-rater reliability, are pivotal for assessing the consistency of LLM judgments relative to human evaluations [51]. This statistical measure provides critical insights into the degree of agreement between different evaluation agents, offering a quantitative basis for assessing LLM effectiveness. Frameworks like USR have implemented multidimensional metrics that gauge dialog system evaluations through parameters such as coherence, specificity, and engagement, aligning closely with human qualitative assessment criteria [52]. This comprehensive approach ensures evaluation metrics encompass a broad spectrum of qualitative factors essential for effective judgment replication by LLMs. Advanced evaluation methods, such as those employed in VideoAutoArena, leverage automated assessment tools to differentiate model capabilities while maintaining relevance and depth akin to human assessments [54]. These methodologies are essential for capturing intricate details that may be overlooked in traditional metric frameworks, thereby providing a holistic perspective on model performance. The alignment of quantitative metrics with qualitative judgment criteria is further emphasized by evaluation frameworks like DocLens, which have demonstrated efficacy in domains requiring nuanced understanding, such as medical text evaluations [35]. By incorporating domain-specific metrics and ensuring alignment with expert criteria, these methods enhance the fidelity of LLM evaluations. The development of multifaceted performance metrics and robust evaluation methods is crucial for bridging the gap between LLM and human evaluations, ensuring AI systems can accurately and effectively complement human judgment across diverse applications. Traditional metrics like BLEU and ROUGE are inadequate for assessing the creative outputs of LLMs, particularly when reference outputs are unavailable. While human evaluation is an option, it is costly and challenging to scale. Recent innovations, such as LLM-as-a-judge frameworks, show promise but require integrating human input to align evaluation criteria with human intent and ensure consistency. Efforts to construct domain-specific evaluation sets and automated benchmarking frameworks, including debates judged by LLMs, enhance the transparency, diversity, and effectiveness of LLM evaluations. These advancements provide valuable insights into model performance across various domains, emphasizing the importance of evaluation as a critical discipline for advancing LLM development [17,37,18,14]. Challenges and Limitations of LLMs as Judges Biases and Limitations in LLM Evaluations LLMs as evaluators face significant biases and limitations that affect their reliability. Intrinsic biases compromise win rate estimation, impacting evaluation credibility [8]. The disparity between LLM and human judgments exacerbates these issues, as LLMs are influenced by factors that hinder replication of human standards [9]. Frameworks like JudgeRank demonstrate how model capabilities can skew evaluation outcomes [6]. Specific tasks, such as passage retrieval, often fail to capture document ranking complexities, limiting comprehensiveness [7]. Datasets like Topical-Chat reveal challenges in generalizability, affecting applicability across diverse contexts [5]. Instruction-following models may produce biased outputs, necessitating robust mitigation strategies [49]. The need for improved open-source evaluators highlights challenges in matching proprietary model performance in specialized domains [35]. These limitations emphasize the need for transparent, comprehensive evaluation methodologies to enhance correlation between metrics and human judgments, ensuring responsible AI development [56,50,38]. Challenges in Replicating Human Understanding Replicating human understanding through LLMs is challenging due to methodological limitations that hinder alignment with human judgment standards [3]. Debiasing methods often fail to fully mitigate biases, limiting LLMs' capacity to replicate nuanced human understanding [10]. Reliance on positional cues impairs emulation of human processes [48]. Task complexity restricts benchmarks designed for simpler models, posing challenges in achieving reliable evaluations [57]. AgentBench findings reveal weaknesses in replicating human cognitive processes, particularly in agent-based scenarios [13]. LLMs struggle with values alignment tasks, failing to mirror comprehensive human criteria [12]. Identifying and eliminating neurons responsible for biased outputs remains unresolved, hindering unbiased human-like understanding [49]. illustrates these primary challenges in replicating human understanding through large language models (LLMs), categorizing them into methodological limitations, task complexity, and bias/neuron challenges. Each category highlights specific issues identified in recent research, emphasizing the need for ongoing improvements in LLM alignment with human standards. These challenges necessitate ongoing research to enhance LLM alignment with human standards, especially in complex fields. Interpretability and Complexity LLM evaluations face interpretability and complexity challenges in AI assessment frameworks. Effective methods for reliable relevance judgments are lacking, leading to ambiguous assessments [58]. Self-bias degrades output quality, affecting LLM objectivity [59]. CRISPR offers solutions by identifying and eliminating bias neurons, enhancing interpretability [49]. Task complexity requires frameworks that navigate intricacies while ensuring clarity and transparency. Addressing the scarcity of studies on evaluation metrics, establishing multi-dimensional protocols, and implementing innovative benchmarking methods are essential. Tailoring fine-grained metrics to specific domains enhances alignment with human judgments [56,37,35,60,50]. Resource and Scalability Constraints Deploying LLMs as evaluators involves significant resource and scalability challenges. Computational complexity impedes scalability, especially in tasks like estimating Gaussian mixtures [33]. ReST$^{EM}$ uses scalar feedback for scalable solutions [61], while REFINER improves reasoning without costly human-in-the-loop data [62]. Efficient Selective Search Frameworks reduce computational demands [63]. Speculative Rejection showcases high efficiency, aligning outputs without extensive resources [64]. BOBBA improves query efficiency, enabling effective adversarial attacks [65]. Cross-Code Evaluation demands significant resources, posing challenges [66]. Creative Backend Evaluation may require extensive resources, highlighting the need for optimization. Sample size savings from LLM judges are modest, indicating a need for efficient strategies [10]. In-domain word availability impacts evaluation methods, underscoring the importance of context-specific models [67]. Developing specialized continual learning strategies is crucial for robust, efficient LLM functioning across scenarios [68]. Advancements emphasize scalable methodologies for deploying LLMs in AI evaluations, with domain-specific sets and novel relevance judgment approaches enhancing transparency, diversity, and effectiveness [58,14]. Dependence on Data Quality LLM evaluation effectiveness is closely tied to data quality in training and assessment phases. The PKU-SafeRLHF dataset exemplifies high-quality data's role in reliable outcomes [69]. Robust datasets in domains like self-driving are crucial for covering diverse scenarios [70]. Few-shot demonstration quality significantly influences evaluation effectiveness [71]. The Prometheus framework highlights score rubrics and instruction quality's impact on consistency [4]. Bayesian inference methods rely on high-quality data to account for uncertainties and biases [8]. Typographical errors in datasets, as seen in Adv-BERT, assess model robustness against noise [72]. Ensuring data quality involves comprehensive protocols, addressing biases, and aligning metrics with human judgments for fair, accurate assessments [56,35,27,19,50]. Manipulation and Security Risks LLM deployment as evaluators introduces manipulation and security risks compromising evaluative integrity. LLM-generated relevance judgments may misalign with human standards, skewing outcomes [12]. Bias susceptibility poses security risks, distorting evaluations. CRISPR offers solutions by identifying bias neurons, enhancing security [49]. Future research could expand CRISPR's capabilities across architectures. Traditional metrics like BLEU and ROUGE fall short in assessing creative LLM outputs, necessitating advanced methodologies. Human evaluations are reliable but costly, prompting interest in LLMs as evaluators. Trust and reliability concerns highlight the need for human input integration for alignment. Comprehensive evaluations categorized into knowledge, alignment, and safety guide responsible LLM development. Frameworks like FBI reveal shortcomings in current Evaluator LLMs, emphasizing robust, human-assisted systems across domains [17,73,38]. Applications and Case Studies The examination of Large Language Models (LLMs) in practical applications and case studies reveals their transformative potential across diverse fields, showcasing their adaptability and enhancing evaluative processes. This section investigates specific real-world applications in finance, education, and autonomous systems, offering insights into methodologies and implications for future developments in LLM technology. Applications in Real-World Scenarios LLMs serve as effective evaluative tools across various domains. In finance, Pixiu provides benchmarks tailored for financial LLMs, improving evaluation precision [74]. MM-Eval ensures comprehensive multilingual LLM assessments, addressing cultural nuances [75]. The Self-Judge method enhances LLM output accuracy by selectively executing instructions based on predicted response quality [76]. In autonomous driving, CODA-LM evaluates corner cases, tackling LVLM assessment challenges [70]. The PKU-SafeRLHF dataset underscores the importance of high-quality data for reliable evaluations in safety-critical contexts [69]. Prometheus facilitates the evaluation of long-form responses, enhancing accessibility across domains [4]. The Topical-Chat dataset aids in understanding LLM performance in nuanced language tasks [5]. The Cascaded Selective Evaluation method aligns LLM evaluations with human judgments, boosting credibility [3]. Studies on judge models highlight the significance of error analysis for LLM refinement [9]. Collectively, these applications demonstrate LLMs' transformative impact on evaluative processes, making assessments more precise and scalable. As illustrated in , the key applications of Large Language Models (LLMs) in diverse evaluation contexts showcase their impact in finance, autonomous driving, and multilingual assessments. Specifically, this figure highlights benchmarks and methods such as Pixiu, Self-Judge, CODA-LM, PKU-SafeRLHF, MM-Eval, and Prometheus, which contribute to enhancing the precision and scalability of LLM evaluations. Despite challenges in reliability and trustworthiness, LLMs present a promising alternative to traditional metrics, particularly in specialized domains requiring expert knowledge. By automating evaluations, LLMs reduce human fatigue and scale assessment processes, advancing the development of proficient models [77,73,17,78,18]. Case Studies and Applications Case studies highlight LLMs' transformative potential across domains. In the legal sector, LexEval offers a taxonomy of legal cognitive abilities, facilitating the development of Chinese legal systems and LLM evaluation pipelines [2]. Tailored evaluation frameworks are crucial for domain-specific needs. Further research seeks to optimize few-shot learning and explore additional legal applications, enhancing effectiveness in nuanced evaluations [58]. In education, experiments with datasets aligned with Bloom's Taxonomy involve thousands of evaluations, showcasing LLMs' potential in educational assessments [36]. These case studies underscore LLMs' versatility in addressing domain-specific challenges through robust evaluative tools. Despite their promise, LLMs face limitations in specialized tasks, as shown by varying agreement rates between LLM judges and subject matter experts in fields like dietetics and mental health. Human involvement remains crucial to ensure evaluations align with human intent, as LLMs may lack depth for complex tasks. Recent efforts to construct domain-specific evaluation sets have improved benchmark separability and alignment with human preferences, emphasizing the value of integrating human input into LLM evaluation workflows [17,78,14]. Evaluation of Language Model Outputs Evaluating language model outputs with LLMs involves diverse methodologies and datasets assessing generated text quality across tasks. The PHUDGE method evaluates latency and throughput, correlating with human annotators to ensure alignment with human judgment [34]. This approach highlights LLMs' role in enhancing evaluation precision. In critique generation, the Eval-Instruct method enables nuanced LLM performance assessments through critique generation and revision [1]. Bayesian calibration techniques validated across six datasets, including story generation and summarization, provide a structured framework for evaluating LLM outputs [8]. The DOCLENS dataset, comprising clinical notes and patient questions, offers a basis for assessing LLM performance in medical contexts [35]. These methodologies emphasize LLMs' transformative role in evaluating outputs through diverse strategies, ensuring rigorous, reliable assessments aligned with human standards across application domains, including multilingual contexts and ethical considerations. By leveraging LLMs and integrating human input, evaluations address traditional metrics' limitations, guiding responsible LLM development and deployment while maximizing societal benefits [37,17,55,18,38]. Code Generation and Software Development LLMs have enhanced software engineering processes in code generation and development tasks. AIME utilizes datasets from coding challenge platforms like LeetCodeHard and HumanEval, focusing on code generation complexity and variety [79]. These datasets provide a robust foundation for assessing LLM capabilities in code generation. Integrating LLMs into software development workflows enhances performance metrics through automated evaluations, reducing human effort and ensuring consistent evaluations, although human input is crucial for aligning criteria with human intent [17,77,15,80]. By leveraging LLM capabilities, developers can automate code evaluations, enhancing productivity in software development environments. LLMs promote adaptive algorithms responding to evolving coding standards. Frameworks like EvalGen and SWE-bench demonstrate adaptability, addressing biases and real-world issues while aligning evaluations with human preferences [81,19,82,83]. These advancements highlight LLMs' transformative impact in code generation and software development, offering scalable solutions meeting modern programming demands. LLMs enhance software artifact summarization and contribute to retrieval-augmented generation tasks, prioritizing factual accuracy. Despite concerns about biases and risks, evaluations in various domains reveal LLMs' capacity to align with human-centered design principles, maximizing societal benefits while minimizing potential risks [77,80,16,17,38]. Text Summarization and Quality Assessment LLMs significantly contribute to text summarization and quality assessment, employing diverse methodologies and datasets to improve summarization outputs. The dataset in [84] includes submissions from eleven LLMs on summarization tasks, illustrating diverse approaches in generating concise summaries, serving as a foundational resource for evaluating LLM effectiveness in summarization contexts. FineSurE offers a comprehensive framework for evaluating text summarization models, focusing on multiple dimensions of summarization quality [42]. TACNN's role in generating concise summaries is explored in [85], highlighting its applicability across domains. The benchmark in [27] emphasizes factual accuracy in summarization tasks, ensuring alignment with human evaluative standards. Summeval provides a large collection of model-generated summaries and human judgments, offering a diverse evaluation resource for text summarization [56]. These methodologies underscore LLMs' transformative role in enhancing text summarization and quality assessment. LLMs offer an automated, scalable alternative to human evaluation, particularly in software artifact summarization and legal case relevance judgments. They reduce evaluator fatigue and improve consistency, although challenges such as bias and the need for human-aligned evaluation criteria remain. By integrating human input and expert reasoning, LLMs can deliver rigorous evaluations aligned with human standards across diverse application domains [17,77,16,58]. Multimodal and Conversational AI LLMs have significantly advanced interaction quality in multimodal and conversational AI settings. Their integration into multimodal frameworks enhances AI interactions by processing both textual and visual data, addressing complexities in multimodal communication and ensuring effective interpretation and response. In legal case retrieval, LLMs assist in generating transparent relevance assessments, improving interaction quality despite challenges in judgment consistency and bias [80,16,58,86]. In conversational AI, LLMs facilitate coherent dialogues, leveraging advanced language processing to simulate human-like interactions. Frameworks like ChatEval exemplify this potential, utilizing multi-agent approaches to diversify evaluative perspectives and enhance interaction quality [20]. The development of benchmarks such as Topical-Chat expands conversational AI evaluations by encompassing a broad range of topics, providing a versatile platform for assessing LLM performance in open-domain dialogues [5]. VideoAutoArena employs automated assessment tools to differentiate model capabilities while maintaining relevance and depth akin to human assessments [54]. Exploring LLM advancements highlights their pivotal role in transforming multimodal and conversational AI by offering sophisticated frameworks for nuanced, fair, and reliable assessments across diverse application domains. These innovations facilitate expert-aligned relevance judgments, promote human-centered evaluation systems to mitigate biases, and support creating diverse, domain-specific benchmarks. They also address bias concerns in clinical decision support systems, contributing to a more robust and equitable AI landscape [80,58,14,16,17]. Ongoing refinement of LLM methodologies continues to enhance their applicability in complex AI settings, ensuring alignment with human standards and expectations. Future Directions The evolution of artificial intelligence necessitates exploring future directions for Large Language Models (LLMs). This section outlines pathways for enhancing LLM capabilities and evaluation frameworks, addressing challenges and opportunities. The following subsections focus on improving LLM capabilities and developing benchmarks to meet diverse application needs. Enhancements in LLM Capabilities and Benchmarks Enhancing LLM capabilities and establishing comprehensive benchmarks are crucial for addressing emerging evaluation needs and improving prediction accuracy. Future research should refine evaluation metrics, explore advanced retrieval techniques, and develop robust integration frameworks to enhance Retrieval-Augmented Generation (RAG) efficacy [11]. Optimizing the PHUDGE method's loss function for varied tasks could enhance its applicability [34]. In critique generation, improving pseudo reference generation and exploring additional prompting strategies are essential for enhancing critique quality [1]. Prometheus would benefit from diverse rubrics and instructions, advancing LLM capabilities through richer assessments [4]. Improving JudgeRank by exploring different architectures and addressing limitations in document retrieval tasks could enhance its effectiveness [6]. Refining Bayesian calibration methods for various evaluators could significantly improve assessment precision [8]. In legal domains, expanding LexEval to encompass more scenarios and updating datasets to reflect evolving practices will ensure evaluations remain relevant [2]. Developing alignment metrics and examining prompt complexity impact are crucial for advancing evaluation reliability [9]. For medical applications, refining DOCLENS metrics and expanding it to include more medical text types will enhance evaluation scope and accuracy [35]. Integrating LLMs into relevance assessments without compromising judgment quality is crucial [12]. These directions emphasize continuous refinement of LLM capabilities and the development of robust benchmarks. Traditional metrics like BLEU and ROUGE often fall short in assessing nuanced outputs, particularly in creative contexts. Innovative approaches, such as LLMs as evaluators, require careful integration of human input to maintain trust. Constructing diverse evaluation sets tailored to specific domains enhances alignment with human preferences. Automated benchmarking, such as debates judged by LLMs, provides scalable alternatives to human evaluations, capturing skills beyond domain knowledge. These efforts ensure LLMs effectively complement human judgment across fields, enhancing transparency and effectiveness in evaluation methodologies [37,14,15,87,17]. Integration with Multimodal and Real-World Data Integrating LLMs with multimodal and real-world data enhances AI evaluation accuracy and applicability. This integration allows LLMs to process diverse data types, enriching evaluative capabilities. Leveraging multimodal data improves contextual understanding, crucial for nuanced interpretation in tasks like legal case retrieval and multimodal evaluation. In legal contexts, LLMs can generate interpretable relevance judgments, enhancing analysis accuracy despite biases [58,12,16,17,86]. Incorporating real-world data enables realistic assessments, aligning AI outputs with human expectations. This is beneficial in domains like autonomous driving, where real-world data provides context for evaluating corner cases [70]. The CODA-LM framework exemplifies this by addressing challenges in evaluating LVLMs, highlighting LLMs' role in enhancing safety and reliability. As illustrated in , the integration of LLMs with multimodal and real-world data showcases key areas such as multimodal data integration for improved context and interpretation, real-world data utilization for realistic assessments and autonomous driving, and advancements in LLM methodologies for expert-aligned judgments and bias addressing. This visual representation underscores the significance of these advancements in refining LLM capabilities. Multimodal data integration facilitates robust AI models that adapt to diverse input scenarios, ensuring evaluations remain relevant [69]. This adaptability is crucial for maintaining alignment with industry requirements and ensuring AI systems complement human judgment in complex environments. These advancements underscore the potential of integrating LLMs with multimodal and real-world data, equipping practitioners with tools for nuanced, fair, and reliable assessments. Refining LLM methodologies enhances their applicability by improving their ability to generate expert-aligned judgments. This ensures alignment with human standards across domains, including legal retrieval and clinical decision support. By incorporating human-centered design principles and addressing biases, LLMs are becoming more reliable and transparent, facilitating integration into tasks requiring nuanced reasoning and accuracy. These advancements demonstrate LLMs' potential to replicate human judgments while emphasizing continuous validation and expert involvement to optimize performance [80,58,15,16,17]. Advancements in Feedback and Annotation Processes Advancements in feedback and annotation processes for LLMs have improved iterative refinement capabilities, enabling more accurate evaluations. The Self-Refine framework exemplifies this by allowing LLMs to iteratively enhance outputs through feedback mechanisms, improving evaluation quality [88]. Future research should explore methods to mitigate criteria drift and enhance evaluation assistants' robustness [81]. Exploring additional NLP tasks has improved annotation processes, facilitating comprehensive assessments and refining explanation generation for clearer insights [89]. This refinement is crucial for achieving transparent evaluations in complex domains. In safety-related areas, further enhancements to modular designs are essential for reliable assessments [90]. Integrating multimodal data promises advancements in areas like speech emotion recognition, allowing richer evaluative contexts [91]. Integrating knowledge graphs with LLMs addresses scalability challenges, enhancing evaluative robustness and facilitating deeper integrations [92]. Expanding datasets to include diverse political contexts and languages enhances evaluations' comprehensiveness and relevance [93]. Future research should focus on refining thought generation processes, leveraging frameworks like ToT for enhanced evaluative capabilities [94]. Improving confidence estimation methods remains a priority for advancing evaluation techniques [3]. These advancements highlight the potential of enhanced feedback and annotation processes in LLM evaluations, equipping practitioners with frameworks for nuanced, fair, and reliable assessments. Future research should delve deeper into evaluating text summarization and machine translation by refining methodologies like MQM and addressing biases. Efforts should focus on expanding datasets to ensure evaluation metrics align closely with human judgments. Developing robust human-aligned AI evaluation systems and exploring self-training methods will reduce reliance on human data, ensuring models meet evolving standards and expectations [56,19,21,61,50]. Exploration of Novel Applications and Emerging Trends Exploring novel applications and emerging trends in LLMs drives transformative advancements in AI evaluation. Expanding evaluation criteria within frameworks like Prometheus is crucial for ensuring evaluations remain relevant across domains [95]. Emerging trends emphasize refining prompting methodologies, pivotal in enhancing LLM output precision [96]. Developing frameworks to categorize and assess prompt engineering techniques is essential for advancing evaluation efficacy. Applying FullAnno to other datasets presents a promising avenue for improving annotation quality and expanding evaluation scope [97]. By integrating additional features, FullAnno can enhance annotations' robustness, contributing to nuanced evaluations. These trends and applications underscore LLMs' potential in AI evaluation, equipping practitioners with tools and methodologies aligning with evolving standards. Future research should delve deeper into evaluating LLMs by refining methodologies and expanding datasets to address challenges across domains, including NLP, reasoning, ethics, and domain-specific areas like law and medicine. Emphasis should be on developing domain-specific evaluation sets, enhancing benchmark separability and agreement with human preferences. By focusing on creating representative and robust frameworks, researchers can better assess performance and potential risks, ensuring models remain cutting-edge and relevant in academic and real-world contexts [18,14]. Conclusion The survey elucidates the pivotal role of Large Language Models (LLMs) in reshaping AI evaluation frameworks, highlighting notable advancements in methodologies across diverse domains. By integrating LLMs with Knowledge Graphs (KGs), inference and knowledge representation are significantly enhanced, contributing to improved model performance. Length-controlled evaluation strategies, exemplified by AlpacaEval, demonstrate substantial alignment with human preferences, notably advancing Spearman correlation metrics. These innovations underscore the importance of context-aware approaches in refining LLM evaluations. The introduction of frameworks like FLASK provides a more nuanced evaluation methodology, offering insights into model performance concerning specific competencies. This approach is complemented by recalibration processes that enhance the alignment of LLM evaluators with human assessments, promoting fair and dependable evaluations. Additionally, systems such as AGAS outperform traditional methods, particularly in feature-based evaluations, offering valuable feedback for improving written content. Despite the advancements in automated metrics, discrepancies between human and automated evaluations persist, underscoring the need for standardized methodologies in fields like machine translation. The Shepherd framework exemplifies efforts to refine critique and enhancement of language model outputs, achieving competitive win rates. This capability is vital for ensuring comprehensive evaluations that align with human standards. A human-centered approach to LLM evaluation is emphasized as crucial for bolstering the reliability and trustworthiness of outputs. The ALLURE method showcases improvements in evaluation capabilities while minimizing reliance on human annotators. Nevertheless, challenges such as subjectivity and criteria drift necessitate ongoing refinement of evaluation processes. These findings collectively highlight the significant contributions of LLMs to AI evaluation processes, underscoring the need for continuous refinement and validation to ensure alignment with human standards across varied applications. The proposed benchmark framework addresses limitations of previous benchmarks, offering a more reliable assessment methodology for foundational models. Moreover, the conclusion identifies notable limitations within the LLM-as-a-judge paradigm, particularly when evaluated models exceed the capabilities of the judges. Such insights suggest that the new benchmark offers a more dependable evaluation framework for passage retrieval tasks.",
  "reference": {
    "1": "2311.18702v2",
    "2": "2409.20288v4",
    "3": "2407.18370v1",
    "4": "2310.08491v2",
    "5": "2308.11995v1",
    "6": "2411.00142v1",
    "7": "2507.10865v1",
    "8": "2411.04424v1",
    "9": "2406.12624v6",
    "10": "2410.13341v2",
    "11": "2312.10997v5",
    "12": "2409.15133v2",
    "13": "2308.03688v3",
    "14": "2408.08808v3",
    "15": "2406.18403v3",
    "16": "2410.20833v2",
    "17": "2407.03479v1",
    "18": "2307.03109v9",
    "19": "2407.12847v1",
    "20": "2308.07201v1",
    "21": "2208.11646v4",
    "22": "1710.03957v1",
    "23": "2409.04168v2",
    "24": "1805.04833v1",
    "25": "2404.08071v1",
    "26": "2105.08920v1",
    "27": "2104.13346v2",
    "28": "2507.08191v1",
    "29": "2307.09705v1",
    "30": "2202.03629v7",
    "31": "2402.04792v2",
    "32": "2201.11903v6",
    "33": "2205.10183v2",
    "34": "2405.08029v2",
    "35": "2311.09581v3",
    "36": "2402.10669v5",
    "37": "2406.11044v2",
    "38": "2310.19736v3",
    "39": "1712.06751v2",
    "40": "2309.13701v2",
    "41": "2405.00099v4",
    "42": "2407.00908v3",
    "43": "2309.16609v1",
    "44": "2107.03374v2",
    "45": "2404.13076v1",
    "46": "2410.15393v1",
    "47": "2310.13345v1",
    "48": "2004.14602v4",
    "49": "2311.09627v2",
    "50": "2104.14478v1",
    "51": "2306.17563v2",
    "52": "2005.00456v1",
    "53": "2310.00752v4",
    "54": "2411.13281v2",
    "55": "2309.07462v2",
    "56": "2007.12626v4",
    "57": "2407.18901v1",
    "58": "2403.18405v3",
    "59": "2402.11436v2",
    "60": "2306.01200v1",
    "61": "2312.06585v4",
    "62": "2304.01904v2",
    "63": "2411.17760v1",
    "64": "2410.20290v2",
    "65": "2206.08575v1",
    "66": "2310.11248v2",
    "67": "2305.19148v3",
    "68": "2402.01364v2",
    "69": "2406.15513v3",
    "70": "2404.10595v5",
    "71": "2304.05128v2",
    "72": "2003.04985v1",
    "73": "2406.13439v2",
    "74": "2306.05443v1",
    "75": "2410.17578v2",
    "76": "2409.00935v1",
    "77": "2409.00630v1",
    "78": "2410.20266v1",
    "79": "2410.03131v3",
    "80": "2404.15149v1",
    "81": "2404.12272v1",
    "82": "2310.06770v3",
    "83": "2310.05470v2",
    "84": "2401.15641v2",
    "85": "1808.08745v1",
    "86": "2402.04788v3",
    "87": "2408.08896v1",
    "88": "2303.17651v2",
    "89": "2303.16854v2",
    "90": "2410.08442v2",
    "91": "2307.06090v3",
    "92": "2306.08302v3",
    "93": "2304.06588v1",
    "94": "2305.10601v2",
    "95": "2405.01535v2",
    "96": "2402.07927v2",
    "97": "2409.13540v1"
  },
  "chooseref": {
    "1": "2406.11050v2",
    "2": "2401.01313v3",
    "3": "2308.04592v1",
    "4": "2112.00861v3",
    "5": "2307.03109v9",
    "6": "2402.07927v2",
    "7": "2410.03131v3",
    "8": "2309.13701v2",
    "9": "2310.11689v2",
    "10": "2003.04985v1",
    "11": "2308.03688v3",
    "12": "2311.18743v4",
    "13": "2402.11253v3",
    "14": "2407.12847v1",
    "15": "2403.16950v5",
    "16": "2310.13345v1",
    "17": "2308.08747v5",
    "18": "2303.16854v2",
    "19": "2407.18901v1",
    "20": "2309.07462v2",
    "21": "2311.09476v2",
    "22": "2410.14165v1",
    "23": "2404.10595v5",
    "24": "2411.04424v1",
    "25": "2309.17012v3",
    "26": "2306.04181v2",
    "27": "2410.03742v2",
    "28": "2312.06585v4",
    "29": "2404.15149v1",
    "30": "2407.10241v2",
    "31": "2310.10632v1",
    "32": "2410.15393v1",
    "33": "2309.13308v1",
    "34": "2307.06090v3",
    "35": "2310.08118v1",
    "36": "2201.11903v6",
    "37": "2303.15056v2",
    "38": "2308.07201v1",
    "39": "2304.06588v1",
    "40": "2403.09032v3",
    "41": "2404.06503v1",
    "42": "2410.16256v1",
    "43": "2404.11791v1",
    "44": "2408.08808v3",
    "45": "2402.01364v2",
    "46": "2410.12735v5",
    "47": "2405.00099v4",
    "48": "2311.18702v2",
    "49": "2310.11248v2",
    "50": "2307.09705v1",
    "51": "2311.09581v3",
    "52": "1710.03957v1",
    "53": "2402.06782v4",
    "54": "2402.04792v2",
    "55": "2305.18290v3",
    "56": "2309.11325v2",
    "57": "2409.15133v2",
    "58": "1808.08745v1",
    "59": "2405.05894v3",
    "60": "2411.17760v1",
    "61": "2305.14233v1",
    "62": "2404.05692v2",
    "63": "2107.03374v2",
    "64": "2310.19736v3",
    "65": "2406.11044v2",
    "66": "2209.02128v1",
    "67": "2310.13800v1",
    "68": "2104.14478v1",
    "69": "2410.20290v2",
    "70": "2406.13439v2",
    "71": "2407.00908v3",
    "72": "2307.10928v4",
    "73": "2407.10817v1",
    "74": "2409.04168v2",
    "75": "2409.13540v1",
    "76": "2311.09204v3",
    "77": "2310.05470v2",
    "78": "2302.04166v2",
    "79": "2308.09687v4",
    "80": "2402.15754v1",
    "81": "2407.12943v1",
    "82": "2410.01257v2",
    "83": "2311.09528v1",
    "84": "1805.04833v1",
    "85": "1712.06751v2",
    "86": "2407.03479v1",
    "87": "2402.10669v5",
    "88": "2402.16795v2",
    "89": "2211.09527v1",
    "90": "2410.18359v3",
    "91": "2407.07061v2",
    "92": "2402.14016v2",
    "93": "2411.00142v1",
    "94": "2406.12624v6",
    "95": "2410.08442v2",
    "96": "2410.02736v2",
    "97": "2402.15043v2",
    "98": "2309.03118v1",
    "99": "2408.08896v1",
    "100": "2410.20833v2",
    "101": "2409.00630v1",
    "102": "2311.08516v3",
    "103": "2410.12869v4",
    "104": "2405.10516v2",
    "105": "2407.05216v2",
    "106": "2410.10724v2",
    "107": "2306.17563v2",
    "108": "2405.01724v1",
    "109": "2305.17926v2",
    "110": "2305.08845v2",
    "111": "2302.00093v3",
    "112": "2310.01798v2",
    "113": "2304.03245v3",
    "114": "2308.11483v1",
    "115": "2308.04386v3",
    "116": "2310.17609v1",
    "117": "2404.04475v2",
    "118": "2403.18405v3",
    "119": "2409.20288v4",
    "120": "2410.20266v1",
    "121": "2410.13341v2",
    "122": "2311.00686v1",
    "123": "2410.02712v2",
    "124": "2404.13076v1",
    "125": "2305.13711v1",
    "126": "2406.18403v3",
    "127": "2004.14602v4",
    "128": "2410.17578v2",
    "129": "2407.19594v2",
    "130": "2311.09627v2",
    "131": "2305.19148v3",
    "132": "2402.04788v3",
    "133": "2404.09682v3",
    "134": "2306.01200v1",
    "135": "2006.16176v4",
    "136": "2208.11646v4",
    "137": "2407.06551v2",
    "138": "2307.09288v2",
    "139": "2406.07545v1",
    "140": "2105.08920v1",
    "141": "2310.18122v2",
    "142": "2507.08191v1",
    "143": "2507.10865v1",
    "144": "2404.08071v1",
    "145": "2406.15053v2",
    "146": "2405.08029v2",
    "147": "2406.15513v3",
    "148": "2306.05443v1",
    "149": "2307.02762v3",
    "150": "2401.15641v2",
    "151": "2402.11436v2",
    "152": "2405.01535v2",
    "153": "2310.08491v2",
    "154": "2310.10077v1",
    "155": "2205.10183v2",
    "156": "2206.08575v1",
    "157": "2309.16609v1",
    "158": "2410.16184v1",
    "159": "2410.04838v2",
    "160": "2311.10947v2",
    "161": "2304.01904v2",
    "162": "2404.18796v2",
    "163": "2005.11401v4",
    "164": "2312.10997v5",
    "165": "2210.11416v5",
    "166": "2410.06961v1",
    "167": "2409.00935v1",
    "168": "2203.11171v4",
    "169": "2312.09300v1",
    "170": "2310.11511v1",
    "171": "2410.05495v1",
    "172": "2303.17651v2",
    "173": "2401.10020v3",
    "174": "2310.00074v3",
    "175": "2408.10902v3",
    "176": "2210.08459v2",
    "177": "2007.12626v4",
    "178": "2202.03629v7",
    "179": "2310.06770v3",
    "180": "2304.05128v2",
    "181": "2409.20370v1",
    "182": "2406.18365v2",
    "183": "2310.00752v4",
    "184": "2308.11995v1",
    "185": "2409.01941v1",
    "186": "2311.08152v2",
    "187": "2305.10601v2",
    "188": "2407.18370v1",
    "189": "2109.07958v2",
    "190": "2310.01377v2",
    "191": "2005.00456v1",
    "192": "2104.13346v2",
    "193": "2306.08302v3",
    "194": "2411.13281v2",
    "195": "2406.04770v2",
    "196": "2404.12272v1",
    "197": "2311.08788v2",
    "198": "1605.05362v1",
    "199": "2307.08487v3"
  }
}