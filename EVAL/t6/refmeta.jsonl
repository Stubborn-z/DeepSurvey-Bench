{"paper_id": 265295011, "title": "GPT-4V(ision) for Robotics: Multimodal Task Planning From Human Demonstration", "author_names": ["Naoki Wake", "Atsushi Kanehira", "Kazuhiro Sasabuchi", "Jun Takamatsu", "K. Ikeuchi"], "venue": "IEEE Robotics and Automation Letters", "abstract": "We introduce a pipeline that enhances a general-purpose Vision Language Model, GPT-4V(ision), to facilitate one-shot visual teaching for robotic manipulation. This system analyzes videos of humans performing tasks and outputs executable robot programs that incorporate insights into affordances. The process begins with GPT-4 V analyzing the videos to obtain textual explanations of environmental and action details. A GPT-4-based task planner then encodes these details into a symbolic task plan. Subsequently, vision systems spatially and temporally ground the task plan in the videos—objects are identified using an open-vocabulary object detector, and hand-object interactions are analyzed to pinpoint moments of grasping and releasing. This spatiotemporal grounding allows for the gathering of affordance information (e.g., grasp types, waypoints, and body postures) critical for robot execution. Experiments across various scenarios demonstrate the method's efficacy in enabling real robots to operate from one-shot human demonstrations. Meanwhile, quantitative tests have revealed instances of hallucination in GPT-4 V, highlighting the importance of incorporating human supervision within the pipeline.", "year": 2023, "publicationdate": "2023-11-20", "externalids": {"DOI": "10.1109/LRA.2024.3477090"}, "doi_lower": "10.1109/lra.2024.3477090"}
{"paper_id": 276969503, "title": "Engineering material failure analysis report generation based on QWen and Llama2", "author_names": ["Sijie Chang", "Meng Wan", "Jiaxiang Wang", "Hao Du", "Pufen Zhang", "Peng Shi"], "venue": "Results in Engineering", "abstract": null, "year": 2025, "publicationdate": "2025-03-01", "externalids": {"DOI": "10.1016/j.rineng.2025.104532"}, "doi_lower": "10.1016/j.rineng.2025.104532"}
{"paper_id": 269187943, "title": "Many-Shot In-Context Learning", "author_names": ["Rishabh Agarwal", "Avi Singh", "Lei M. Zhang", "Bernd Bohnet", "Stephanie Chan", "Biao Zhang", "Ankesh Anand", "Zaheer Abbas", "Azade Nova", "John D. Co-Reyes", "Eric Chu", "Feryal M. P. Behbahani", "Aleksandra Faust", "Hugo Larochelle"], "venue": "Neural Information Processing Systems", "abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. We also find that inference cost increases linearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL to varying degrees. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.", "year": 2024, "publicationdate": "2024-04-17", "externalids": {"DOI": "10.48550/arXiv.2404.11018"}, "doi_lower": "10.48550/arxiv.2404.11018"}
{"paper_id": 258999480, "title": "Transformers learn to implement preconditioned gradient descent for in-context learning", "author_names": ["Kwangjun Ahn", "Xiang Cheng", "Hadi Daneshmand", "S. Sra"], "venue": "Neural Information Processing Systems", "abstract": "Several recent works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate iterations of gradient descent. Going beyond the question of expressivity, we ask: Can transformers learn to implement such algorithms by training over random problem instances? To our knowledge, we make the first theoretical progress on this question via an analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with $L$ attention layers, we prove certain critical points of the training objective implement $L$ iterations of preconditioned gradient descent. Our results call for future theoretical studies on learning algorithms by training transformers.", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.48550/arXiv.2306.00297"}, "doi_lower": "10.48550/arxiv.2306.00297"}
{"paper_id": 259108565, "title": "In-Context Learning through the Bayesian Prism", "author_names": ["Kabir Ahuja", "Madhuri Panwar", "Navin Goyal"], "venue": "International Conference on Learning Representations", "abstract": "In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning-like ICL setups have been devised that train transformers on sequences of input-output pairs $(x, f(x))$. The function $f$ comes from a function class and generalization is checked by evaluating on sequences generated from unseen functions from the same class. One of the main discoveries in this line of research has been that for several function classes, such as linear regression, transformers successfully generalize to new functions in the class. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. In this paper we empirically examine how far this Bayesian perspective can help us understand ICL. To this end, we generalize the previous meta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple task families. We instantiate this setup on a diverse range of linear and nonlinear function families and find that transformers can do ICL in this setting as well. Where Bayesian inference is tractable, we find evidence that high-capacity transformers mimic the Bayesian predictor. The Bayesian perspective provides insights into the inductive bias of ICL and how transformers perform a particular task when they are trained on multiple tasks. We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor. We examine these deviations in more depth offering new insights and hypotheses.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.48550/arXiv.2306.04891"}, "doi_lower": "10.48550/arxiv.2306.04891"}
{"paper_id": 274790044, "title": "MGH Radiology Llama: A Llama 3 70B Model for Radiology", "author_names": ["Yucheng Shi", "Peng Shu", "Zheng Liu", "Zihao Wu", "Quanzheng Li", "Tianming Liu", "Ninghao Liu", "Xiang Li"], "venue": "", "abstract": "In recent years, the field of radiology has increasingly harnessed the power of artificial intelligence (AI) to enhance diagnostic accuracy, streamline workflows, and improve patient care. Large language models (LLMs) have emerged as particularly promising tools, offering significant potential in assisting radiologists with report generation, clinical decision support, and patient communication. This paper presents an advanced radiology-focused large language model: MGH Radiology Llama. It is developed using the Llama 3 70B model, building upon previous domain-specific models like Radiology-GPT and Radiology-Llama2. Leveraging a unique and comprehensive dataset from Massachusetts General Hospital, comprising over 6.5 million de-identified medical reports across various imaging modalities, the model demonstrates significant improvements in generating accurate and clinically relevant radiology impressions given the corresponding findings. Our evaluation, incorporating both traditional metrics and a GPT-4-based assessment, highlights the enhanced performance of this work over general-purpose LLMs.", "year": 2024, "publicationdate": "2024-08-13", "externalids": {}, "doi_lower": null}
{"paper_id": 254043800, "title": "What learning algorithm is in-context learning? Investigations with linear models", "author_names": ["Ekin Akyürek", "D. Schuurmans", "Jacob Andreas", "Tengyu Ma", "Denny Zhou"], "venue": "International Conference on Learning Representations", "abstract": "Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary, and converging to Bayesian estimators for large widths and depths. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners' late layers non-linearly encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may rediscover standard estimation algorithms. Code and reference implementations are released at https://github.com/ekinakyurek/google-research/blob/master/incontext.", "year": 2022, "publicationdate": "2022-11-28", "externalids": {"DOI": "10.48550/arXiv.2211.15661"}, "doi_lower": "10.48550/arxiv.2211.15661"}
{"paper_id": 248476411, "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "author_names": ["Jean-Baptiste Alayrac", "Jeff Donahue", "Pauline Luc", "Antoine Miech", "Iain Barr", "Yana Hasson", "Karel Lenc", "A. Mensch", "Katie Millican", "Malcolm Reynolds", "Roman Ring", "Eliza Rutherford", "Serkan Cabi", "Tengda Han", "Zhitao Gong", "Sina Samangooei", "Marianne Monteiro", "Jacob Menick", "Sebastian Borgeaud", "Andy Brock", "Aida Nematzadeh", "Sahand Sharifzadeh", "Mikolaj Binkowski", "Ricardo Barreira", "O. Vinyals", "Andrew Zisserman", "K. Simonyan"], "venue": "Neural Information Processing Systems", "abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.", "year": 2022, "publicationdate": "2022-04-29", "externalids": {}, "doi_lower": null}
{"paper_id": 258558112, "title": "How Do In-Context Examples Affect Compositional Generalization?", "author_names": ["Shengnan An", "Zeqi Lin", "Qiang Fu", "B. Chen", "Nanning Zheng", "Jian-Guang Lou", "D. Zhang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Compositional generalization–understanding unseen combinations of seen primitives–is an essential reasoning capability in human intelligence.The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning–the prevailing few-shot paradigm based on large language models–exhibits compositional generalization.In this paper, we present CoFe, a test suite to investigate in-context compositional generalization.We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization.We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple.Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus.We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm.", "year": 2023, "publicationdate": "2023-05-08", "externalids": {"DOI": "10.48550/arXiv.2305.04835"}, "doi_lower": "10.48550/arxiv.2305.04835"}
{"paper_id": 259095794, "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection", "author_names": ["Yu Bai", "Fan Chen", "Haiquan Wang", "Caiming Xiong", "Song Mei"], "venue": "Neural Information Processing Systems", "abstract": "Neural sequence models based on the transformer architecture have demonstrated remarkable \\emph{in-context learning} (ICL) abilities, where they can perform new tasks when prompted with training and test examples, without any parameter update to the model. This work first provides a comprehensive statistical theory for transformers to perform ICL. Concretely, we show that transformers can implement a broad class of standard machine learning algorithms in context, such as least squares, ridge regression, Lasso, learning generalized linear models, and gradient descent on two-layer neural networks, with near-optimal predictive power on various in-context data distributions. Using an efficient implementation of in-context gradient descent as the underlying mechanism, our transformer constructions admit mild size bounds, and can be learned with polynomially many pretraining sequences. Building on these ``base'' ICL algorithms, intriguingly, we show that transformers can implement more complex ICL procedures involving \\emph{in-context algorithm selection}, akin to what a statistician can do in real life -- A \\emph{single} transformer can adaptively select different base ICL algorithms -- or even perform qualitatively different tasks -- on different input sequences, without any explicit prompting of the right algorithm or task. We both establish this in theory by explicit constructions, and also observe this phenomenon experimentally. In theory, we construct two general mechanisms for algorithm selection with concrete examples: pre-ICL testing, and post-ICL validation. As an example, we use the post-ICL validation mechanism to construct a transformer that can perform nearly Bayes-optimal ICL on a challenging task -- noisy linear models with mixed noise levels. Experimentally, we demonstrate the strong in-context algorithm selection capabilities of standard transformer architectures.", "year": 2023, "publicationdate": "2023-06-07", "externalids": {"DOI": "10.48550/arXiv.2306.04637"}, "doi_lower": "10.48550/arxiv.2306.04637"}
{"paper_id": 251979350, "title": "Visual Prompting via Image Inpainting", "author_names": ["Amir Bar", "Yossi Gandelsman", "Trevor Darrell", "A. Globerson", "Alexei A. Efros"], "venue": "Neural Information Processing Systems", "abstract": "How does one adapt a pre-trained visual model to novel downstream tasks without task-specific finetuning or any model modification? Inspired by prompting in NLP, this paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples. We show that posing this problem as simple image inpainting - literally just filling in a hole in a concatenated visual prompt image - turns out to be surprisingly effective, provided that the inpainting algorithm has been trained on the right data. We train masked auto-encoders on a new dataset that we curated - 88k unlabeled figures from academic papers sources on Arxiv. We apply visual prompting to these pretrained models and demonstrate results on various downstream image-to-image tasks, including foreground segmentation, single object detection, colorization, edge detection, etc.", "year": 2022, "publicationdate": "2022-09-01", "externalids": {"DOI": "10.48550/arXiv.2209.00647"}, "doi_lower": "10.48550/arxiv.2209.00647"}
{"paper_id": 123329493, "title": "A Markovian Decision Process", "author_names": ["R. Bellman"], "venue": "", "abstract": null, "year": 1957, "publicationdate": "1957-04-18", "externalids": {"DOI": "10.1512/IUMJ.1957.6.56038"}, "doi_lower": "10.1512/iumj.1957.6.56038"}
{"paper_id": 269484643, "title": "In-Context Learning with Long-Context Models: An In-Depth Exploration", "author_names": ["Amanda Bertsch", "Maor Ivgi", "Uri Alon", "Jonathan Berant", "Matthew R. Gormley", "Graham Neubig"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "As model context lengths continue to increase, the number of demonstrations that can be provided in-context approaches the size of entire training datasets. We study the behavior of in-context learning (ICL) at this extreme scale on multiple datasets and models. We show that, for many datasets with large label spaces, performance continues to increase with thousands of demonstrations. We contrast this with example retrieval and finetuning: example retrieval shows excellent performance at low context lengths but has diminished gains with more demonstrations; finetuning is more data hungry than ICL but can exceed long-context ICL performance with additional data. We use the ICL setting to study several properties of both in-context learning and long-context models. We show that long-context ICL is less sensitive to random input shuffling than short-context ICL, that grouping of same-label examples negatively impacts performance, and that the performance boosts do not arise from cumulative gain from encoding many examples together. We conclude that long-context ICL can be an effective tool, and may not require long-context for encoding the demonstration set at all.", "year": 2024, "publicationdate": "2024-04-30", "externalids": {"DOI": "10.48550/arXiv.2405.00200"}, "doi_lower": "10.48550/arxiv.2405.00200"}
{"paper_id": 258999187, "title": "Birth of a Transformer: A Memory Viewpoint", "author_names": ["A. Bietti", "Vivien A. Cabannes", "Diane Bouchacourt", "H. Jégou", "L. Bottou"], "venue": "Neural Information Processing Systems", "abstract": "Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties.", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.48550/arXiv.2306.00802"}, "doi_lower": "10.48550/arxiv.2306.00802"}
{"paper_id": 195574530, "title": "Zhenyu Ma, Wei Wu, Wei Zhang, Jianping Wang, and Fusheng Liu", "author_names": ["Zhen-Yu Ma", "Wei Wu", "Wei Zhang", "Jian-Ping Wang", "Fu-Sheng Liu"], "venue": "", "abstract": null, "year": 2019, "publicationdate": "2019-05-01", "externalids": {}, "doi_lower": null}
{"paper_id": 14604520, "title": "A large annotated corpus for learning natural language inference", "author_names": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.", "year": 2015, "publicationdate": "2015-08-21", "externalids": {"DOI": "10.18653/v1/D15-1075"}, "doi_lower": "10.18653/v1/d15-1075"}
{"paper_id": 218971783, "title": "Language Models are Few-Shot Learners", "author_names": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "R. Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Ma-teusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "I. Sutskever", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "year": 2020, "publicationdate": "2020-05-28", "externalids": {}, "doi_lower": null}
{"paper_id": 266174713, "title": "ICL Markup: Structuring In-Context Learning using Soft-Token Tags", "author_names": ["Marc-Etienne Brunet", "Ashton Anderson", "R. Zemel"], "venue": "arXiv.org", "abstract": "Large pretrained language models (LLMs) can be rapidly adapted to a wide variety of tasks via a text-to-text approach, where the instruction and input are fed to the model in natural language. Combined with in-context learning (ICL), this paradigm is impressively flexible and powerful. However, it also burdens users with an overwhelming number of choices, many of them arbitrary. Inspired by markup languages like HTML, we contribute a method of using soft-token tags to compose prompt templates. This approach reduces arbitrary decisions and streamlines the application of ICL. Our method is a form of meta-learning for ICL; it learns these tags in advance during a parameter-efficient fine-tuning ``warm-up'' process. The tags can subsequently be used in templates for ICL on new, unseen tasks without any additional fine-tuning. Our experiments with this approach yield promising initial results, improving LLM performance on important enterprise applications such as few-shot and open-world intent detection, as well as text classification in news and legal domains.", "year": 2023, "publicationdate": "2023-12-12", "externalids": {"DOI": "10.48550/arXiv.2312.07405"}, "doi_lower": "10.48550/arxiv.2312.07405"}
{"paper_id": 248665718, "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers", "author_names": ["Stephanie C. Y. Chan", "Adam Santoro", "Andrew Kyle Lampinen", "Jane X. Wang", "Aaditya K Singh", "Pierre H. Richemond", "J. Mcclelland", "Felix Hill"], "venue": "Neural Information Processing Systems", "abstract": "Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having large numbers of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. In sum, our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and how future work might encourage both in-context and in-weights learning in domains beyond language.", "year": 2022, "publicationdate": "2022-04-22", "externalids": {"DOI": "10.48550/arXiv.2205.05055"}, "doi_lower": "10.48550/arxiv.2205.05055"}
{"paper_id": 269899641, "title": "Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks", "author_names": ["Anwoy Chatterjee", "Eshaan Tanwar", "Subhabrata Dutta", "Tanmoy Chakraborty"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large Language Models (LLMs) have transformed NLP with their remarkable In-context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal models excel in zero-shot performance, their computational demands limit widespread use, and smaller language models struggle without context. This paper investigates whether LLMs can generalize from labeled examples of predefined tasks to novel tasks. Drawing inspiration from biological neurons and the mechanistic interpretation of the Transformer architecture, we explore the potential for information sharing across tasks. We design a cross-task prompting setup with three LLMs and show that LLMs achieve significant performance improvements despite no examples from the target task in the context. Cross-task prompting leads to a remarkable performance boost of 107% for LLaMA-2 7B, 18.6% for LLaMA-2 13B, and 3.2% for GPT 3.5 on average over zero-shot prompting, and performs comparable to standard in-context learning. The effectiveness of generating pseudo-labels for in-task examples is demonstrated, and our analyses reveal a strong correlation between the effect of cross-task examples and model activation similarities in source and target input tokens. This paper offers a first-of-its-kind exploration of LLMs' ability to solve novel tasks based on contextual signals from different task examples.", "year": 2024, "publicationdate": "2024-05-17", "externalids": {"DOI": "10.48550/arXiv.2405.10548"}, "doi_lower": "10.48550/arxiv.2405.10548"}
{"paper_id": 266843853, "title": "Grimoire is All You Need for Enhancing Large Language Models", "author_names": ["Ding Chen", "Shichao Song", "Qingchen Yu", "Zhiyu Li", "Wenjin Wang", "Feiyu Xiong", "Bo Tang"], "venue": "arXiv.org", "abstract": "In-context Learning (ICL) is one of the key methods for enhancing the performance of large language models on specific tasks by providing a set of few-shot examples. However, the ICL capability of different types of models shows significant variation due to factors such as model architecture, volume of learning data, and the size of parameters. Generally, the larger the model's parameter size and the more extensive the learning data, the stronger its ICL capability. In this paper, we propose a method SLEICL that involves learning from examples using strong language models and then summarizing and transferring these learned skills to weak language models for inference and application. This ensures the stability and effectiveness of ICL. Compared to directly enabling weak language models to learn from prompt examples, SLEICL reduces the difficulty of ICL for these models. Our experiments, conducted on up to eight datasets with five language models, demonstrate that weak language models achieve consistent improvement over their own zero-shot or few-shot capabilities using the SLEICL method. Some weak language models even surpass the performance of GPT4-1106-preview (zero-shot) with the aid of SLEICL.", "year": 2024, "publicationdate": "2024-01-07", "externalids": {"DOI": "10.48550/arXiv.2401.03385"}, "doi_lower": "10.48550/arxiv.2401.03385"}
{"paper_id": 248512524, "title": "Improving In-Context Few-Shot Learning via Self-Supervised Training", "author_names": ["Mingda Chen", "Jingfei Du", "Ramakanth Pasunuru", "Todor Mihaylov", "Srini Iyer", "Ves Stoyanov", "Zornitsa Kozareva"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Self-supervised pretraining has made few-shot learning possible for many NLP tasks. But the pretraining objectives are not typically adapted specifically for in-context few-shot learning. In this paper, we propose to use self-supervision in an intermediate training stage between pretraining and downstream few-shot usage with the goal to teach the model to perform in-context few shot learning. We propose and evaluate four self-supervised objectives on two benchmarks. We find that the intermediate self-supervision stage produces models that outperform strong baselines. Ablation study shows that several factors affect the downstream performance, such as the amount of training data and the diversity of the self-supervised objectives. Human-annotated cross-task supervision and self-supervision are complementary. Qualitative analysis suggests that the self-supervised-trained models are better at following task requirements.", "year": 2022, "publicationdate": "2022-05-03", "externalids": {"DOI": "10.48550/arXiv.2205.01703"}, "doi_lower": "10.48550/arxiv.2205.01703"}
{"paper_id": 247951931, "title": "PaLM: Scaling Language Modeling with Pathways", "author_names": ["A. Chowdhery", "Sharan Narang", "Jacob Devlin", "Maarten Bosma", "Gaurav Mishra", "Adam Roberts", "P. Barham", "Hyung Won Chung", "Charles Sutton", "Sebastian Gehrmann", "Parker Schuh", "Kensen Shi", "Sasha Tsvyashchenko", "Joshua Maynez", "Abhishek Rao", "Parker Barnes", "Yi Tay", "Noam Shazeer", "Vinodkumar Prabhakaran", "Emily Reif", "Nan Du", "Ben Hutchinson", "Reiner Pope", "James Bradbury", "Jacob Austin", "M. Isard", "Guy Gur-Ari", "Pengcheng Yin", "Toju Duke", "Anselm Levskaya", "S. Ghemawat", "Sunipa Dev", "H. Michalewski", "Xavier García", "Vedant Misra", "Kevin Robinson", "L. Fedus", "Denny Zhou", "Daphne Ippolito", "D. Luan", "Hyeontaek Lim", "Barret Zoph", "A. Spiridonov", "Ryan Sepassi", "David Dohan", "Shivani Agrawal", "Mark Omernick", "Andrew M. Dai", "Thanumalayan Sankaranarayana Pillai", "Marie Pellat", "Aitor Lewkowycz", "Erica Moreira", "R. Child", "Oleksandr Polozov", "Katherine Lee", "Zongwei Zhou", "Xuezhi Wang", "Brennan Saeta", "Mark Díaz", "Orhan Firat", "Michele Catasta", "Jason Wei", "K. Meier-Hellstern", "D. Eck", "J. Dean", "Slav Petrov", "Noah Fiedel"], "venue": "Journal of machine learning research", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "year": 2022, "publicationdate": "2022-04-05", "externalids": {}, "doi_lower": null}
{"paper_id": 263672156, "title": "Fine-tune Language Models to Approximate Unbiased In-context Learning", "author_names": ["Timothy Chu", "Zhao Song", "Chiwun Yang"], "venue": "arXiv.org", "abstract": "In-context learning (ICL) is an astonishing emergent ability of large language models (LLMs). By presenting a prompt that includes multiple input-output pairs as examples and introducing a new query input, models can generate the corresponding output. However, the performance of models heavily relies on the quality of the input prompt when implementing in-context learning. Biased or imbalanced input prompts can significantly degrade the performance of language models. To address this issue, we introduce a reweighted algorithm called RICL (Reweighted In-context Learning). This algorithm fine-tunes language models using an unbiased validation set to determine the optimal weight for each input-output example to approximate unbiased in-context learning. Furthermore, we also introduce a low-cost reweighted algorithm, a linear optimal weight approximation algorithm called LARICL (Linear Approximation of Reweighted In-context Learning). This algorithm requires minimal training cost while providing effective results. We prove the convergence of our algorithm and validate its performance through experiments conducted on a numerical dataset. The experimental findings reveal a substantial improvement in comparison to benchmarks including the performance of casual prompt-based in-context learning and the performance of a classic fine-tuning method.", "year": 2023, "publicationdate": "2023-10-05", "externalids": {"DOI": "10.48550/arXiv.2310.03331"}, "doi_lower": "10.48550/arxiv.2310.03331"}
{"paper_id": 253018554, "title": "Scaling Instruction-Finetuned Language Models", "author_names": ["Hyung Won Chung", "Le Hou", "S. Longpre", "Barret Zoph", "Yi Tay", "W. Fedus", "Eric Li", "Xuezhi Wang", "Mostafa Dehghani", "Siddhartha Brahma", "Albert Webson", "S. Gu", "Zhuyun Dai", "Mirac Suzgun", "Xinyun Chen", "A. Chowdhery", "Dasha Valter", "Sharan Narang", "Gaurav Mishra", "Adams Wei Yu", "Vincent Zhao", "Yanping Huang", "Andrew M. Dai", "Hongkun Yu", "Slav Petrov", "Ed H. Chi", "J. Dean", "Jacob Devlin", "Adam Roberts", "Denny Zhou", "Quoc V. Le", "Jason Wei"], "venue": "Journal of machine learning research", "abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.", "year": 2022, "publicationdate": "2022-10-20", "externalids": {"DOI": "10.48550/arXiv.2210.11416"}, "doi_lower": "10.48550/arxiv.2210.11416"}
{"paper_id": 254877715, "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers", "author_names": ["Damai Dai", "Yutao Sun", "Li Dong", "Y. Hao", "Zhifang Sui", "Furu Wei"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large pretrained language models have shown surprising In-Context Learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without additional parameter updates. Despite the great success in performance, the working mechanism of ICL still remains an open problem. In order to better understand how ICL works, this paper explains language models as meta-optimizers and understands ICL as a kind of implicit ﬁnetuning. Theoretically, we ﬁgure out that the Transformer attention has a dual form of gradient descent based optimization. On top of it, we understand ICL as follows: GPT ﬁrst produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. Experimentally, we comprehensively compare the behavior of ICL and explicit ﬁnetuning based on real tasks to provide empirical evidence that supports our understanding. The results prove that ICL behaves similarly to explicit ﬁnetuning at the prediction level, the representation level, and the attention behavior level. Further, inspired by our understanding of meta-optimization, we design a momentum-based attention by analogy with the momentum-based gradient descent algorithm. Its consistently better performance over vanilla attention supports our understanding again from another aspect, and more impor-tantly, it shows the potential to utilize our understanding for future model designing.", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2212.10559"}, "doi_lower": "10.48550/arxiv.2212.10559"}
{"paper_id": 258615266, "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", "author_names": ["Wenliang Dai", "Junnan Li", "Dongxu Li", "A. Tiong", "Junqi Zhao", "Weisheng Wang", "Boyang Albert Li", "Pascale Fung", "Steven C. H. Hoi"], "venue": "Neural Information Processing Systems", "abstract": "Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.", "year": 2023, "publicationdate": "2023-05-11", "externalids": {"DOI": "10.48550/arXiv.2305.06500"}, "doi_lower": "10.48550/arxiv.2305.06500"}
{"paper_id": 233289412, "title": "Editing Factual Knowledge in Language Models", "author_names": ["Nicola De Cao", "Wilker Aziz", "Ivan Titov"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix ‘bugs’ or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor’s efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a ‘probe’ revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor", "year": 2021, "publicationdate": "2021-04-16", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.522"}, "doi_lower": "10.18653/v1/2021.emnlp-main.522"}
{"paper_id": 254877171, "title": "Is GPT-3 a Good Data Annotator?", "author_names": ["Bosheng Ding", "Chengwei Qin", "Linlin Liu", "Lidong Bing", "Shafiq R. Joty", "Boyang Albert Li"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Data annotation is the process of labeling data that could be used to train machine learning models. Having high quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI, has demonstrated im- impressive zero- and few-shot performance on a wide range of NLP tasks. It is therefore natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10450"}, "doi_lower": "10.48550/arxiv.2212.10450"}
{"paper_id": 260887420, "title": "CausalLM is not optimal for in-context learning", "author_names": ["Nan Ding", "Tomer Levinboim", "Jialin Wu", "Sebastian Goodman", "Radu Soricut"], "venue": "International Conference on Learning Representations", "abstract": "Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic and real tasks and using various types of transformers. Our experiments verify that causalLM consistently underperforms prefixLM in all settings.", "year": 2023, "publicationdate": "2023-08-14", "externalids": {}, "doi_lower": null}
{"paper_id": 258762315, "title": "Statistical Knowledge Assessment for Large Language Models", "author_names": ["Qingxiu Dong", "Jingjing Xu", "Lingpeng Kong", "Zhifang Sui", "Lei Li"], "venue": "Neural Information Processing Systems", "abstract": "Given varying prompts regarding a factoid question, can a large language model (LLM) reliably generate factually correct answers? Existing LLMs may generate distinct responses for different prompts. In this paper, we study the problem of quantifying knowledge contained in an LLM regarding a given set of facts. We propose KaRR, a statistical approach to assess factual knowledge for LLMs. The main idea is to estimate the ratio of LLM generating text corresponding to the answer entity given diverse prompts of the subject and the querying relation, versus it generating by random chances. Our assessment suite contains a comprehensive set of 994,123 entities and 600 relations, with 1,395,905 text aliases. We use our method to evaluate 20 LLMs of various sizes, including LLaMA, Alpaca, OPT, etc. Experiments show that our results have a strong correlation (0.43 Kendall's $\\tau$) with the results of human assessment on LLMs. Our results reveal that the knowledge in LLMs with the same backbone architecture adheres to the scaling law, while tuning on instruction-following data sometimes compromises the model's capability to generate factually correct text reliably.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {}, "doi_lower": null}
{"paper_id": 259187776, "title": "Trained Transformers Learn Linear Models In-Context", "author_names": ["Ruiqi Zhang", "Spencer Frei", "P. Bartlett"], "venue": "Journal of machine learning research", "abstract": "Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares. Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this global minimum, when given a test prompt of labeled examples from a new prediction task, the transformer achieves prediction error competitive with the best linear predictor over the test prompt distribution. We additionally characterize the robustness of the trained transformer to a variety of distribution shifts and show that although a number of shifts are tolerated, shifts in the covariate distribution of the prompts are not. Motivated by this, we consider a generalized ICL setting where the covariate distributions can vary across prompts. We show that although gradient flow succeeds at finding a global minimum in this setting, the trained transformer is still brittle under mild covariate shifts. We complement this finding with experiments on large, nonlinear transformer architectures which we show are more robust under covariate shifts.", "year": 2023, "publicationdate": "2023-06-16", "externalids": {"DOI": "10.48550/arXiv.2306.09927"}, "doi_lower": "10.48550/arxiv.2306.09927"}
{"paper_id": 259342611, "title": "In-Context Learning for Attention Scheme: from Single Softmax Regression to Multiple Softmax Regression via a Tensor Trick", "author_names": ["Yeqi Gao", "Zhao Song", "Shenghao Xie"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have brought significant and transformative changes in human society. These models have demonstrated remarkable capabilities in natural language understanding and generation, leading to various advancements and impacts across several domains. We consider the in-context learning under two formulation for attention related regression in this work. Given matrices $A_1 \\in \\mathbb{R}^{n \\times d}$, and $A_2 \\in \\mathbb{R}^{n \\times d}$ and $B \\in \\mathbb{R}^{n \\times n}$, the purpose is to solve some certain optimization problems: Normalized version $\\min_{X} \\| D(X)^{-1} \\exp(A_1 X A_2^\\top) - B \\|_F^2$ and Rescaled version $\\| \\exp(A_1 X A_2^\\top) - D(X) \\cdot B \\|_F^2$. Here $D(X) := \\mathrm{diag}( \\exp(A_1 X A_2^\\top) {\\bf 1}_n )$. Our regression problem shares similarities with previous studies on softmax-related regression. Prior research has extensively investigated regression techniques related to softmax regression: Normalized version $\\| \\langle \\exp(Ax) , {\\bf 1}_n \\rangle^{-1} \\exp(Ax) - b \\|_2^2$ and Resscaled version $\\| \\exp(Ax) - \\langle \\exp(Ax), {\\bf 1}_n \\rangle b \\|_2^2 $ In contrast to previous approaches, we adopt a vectorization technique to address the regression problem in matrix formulation. This approach expands the dimension from $d$ to $d^2$, resembling the formulation of the regression problem mentioned earlier. Upon completing the lipschitz analysis of our regression function, we have derived our main result concerning in-context learning.", "year": 2023, "publicationdate": "2023-07-05", "externalids": {"DOI": "10.48550/arXiv.2307.02419"}, "doi_lower": "10.48550/arxiv.2307.02419"}
{"paper_id": 251253368, "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes", "author_names": ["Shivam Garg", "Dimitris Tsipras", "Percy Liang", "G. Valiant"], "venue": "Neural Information Processing Systems", "abstract": "In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn\"most\"functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .", "year": 2022, "publicationdate": "2022-08-01", "externalids": {"DOI": "10.48550/arXiv.2208.01066"}, "doi_lower": "10.48550/arxiv.2208.01066"}
{"paper_id": 254408772, "title": "Demystifying Prompts in Language Models via Perplexity Estimation", "author_names": ["Hila Gonen", "Srini Iyer", "Terra Blevins", "Noah A. Smith", "Luke Zettlemoyer"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Language models can be prompted to perform a wide variety of zero- and few-shot learning problems. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens or how to pick the best prompts. In this work, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is coupled with the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt is, the better the prompt is able to perform the task. As a result, we devise a method for creating prompts: (1) automatically extend a small seed set of manually written prompts by paraphrasing using GPT3 and backtranslation and (2) choose the lowest perplexity prompts to get significant gains in performance.", "year": 2022, "publicationdate": "2022-12-08", "externalids": {"DOI": "10.48550/arXiv.2212.04037"}, "doi_lower": "10.48550/arxiv.2212.04037"}
{"paper_id": 258715048, "title": "Pre-Training to Learn in Context", "author_names": ["Yuxian Gu", "Li Dong", "Furu Wei", "Minlie Huang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pre-training for In-Context Learning), a framework to enhance the language models’ in-context learning ability by pre-training the model on a large collection of “intrinsic tasks” in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widely-used text classification datasets and the Super-NaturalInstrctions benchmark, which contains 100+ NLP tasks formulated to text generation. Our experiments show that PICL is more effective and task-generalizable than a range of baselines, outperforming larger language models with nearly 4x parameters. The code is publicly available at https://github.com/thu-coai/PICL.", "year": 2023, "publicationdate": "2023-05-16", "externalids": {"DOI": "10.48550/arXiv.2305.09137"}, "doi_lower": "10.48550/arxiv.2305.09137"}
{"paper_id": 257505037, "title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction", "author_names": ["Michael Hahn", "Navin Goyal"], "venue": "arXiv.org", "abstract": "Scaling large language models (LLMs) leads to an emergent capacity to learn in-context from example demonstrations. Despite progress, theoretical understanding of this phenomenon remains limited. We argue that in-context learning relies on recombination of compositional operations found in natural language data. We derive an information-theoretic bound showing how in-context learning abilities arise from generic next-token prediction when the pretraining distribution has sufficient amounts of compositional structure, under linguistically motivated assumptions. A second bound provides a theoretical justification for the empirical success of prompting LLMs to output intermediate steps towards an answer. To validate theoretical predictions, we introduce a controlled setup for inducing in-context learning; unlike previous approaches, it accounts for the compositional nature of language. Trained transformers can perform in-context learning for a range of tasks, in a manner consistent with the theoretical results. Mirroring real-world LLMs in a miniature setup, in-context learning emerges when scaling parameters and data, and models perform better when prompted to output intermediate steps. Probing shows that in-context learning is supported by a representation of the input's compositional structure. Taken together, these results provide a step towards theoretical understanding of emergent behavior in large language models.", "year": 2023, "publicationdate": "2023-03-14", "externalids": {"DOI": "10.48550/arXiv.2303.07971"}, "doi_lower": "10.48550/arxiv.2303.07971"}
{"paper_id": 258833197, "title": "Understanding Emergent In-Context Learning from a Kernel Regression Perspective", "author_names": ["Chi Han", "Ziqi Wang", "H. Zhao", "Heng Ji"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Large language models (LLMs) have initiated a paradigm shift in transfer learning. In contrast to the classic pretraining-then-finetuning procedure, in order to use LLMs for downstream prediction tasks, one only needs to provide a few demonstrations, known as in-context examples, without adding more or updating existing model parameters. This in-context learning (ICL) capability of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs acquire such capabilities. In this paper, we investigate the reason why a transformer-based language model can accomplish in-context learning after pre-training on a general language corpus by proposing a kernel-regression perspective of understanding LLMs'ICL bahaviors when faced with in-context examples. More concretely, we first prove that Bayesian inference on in-context prompts can be asymptotically understood as kernel regression $\\hat y = \\sum_i y_i K(x, x_i)/\\sum_i K(x, x_i)$ as the number of in-context demonstrations grows. Then, we empirically investigate the in-context behaviors of language models. We find that during ICL, the attention and hidden features in LLMs match the behaviors of a kernel regression. Finally, our theory provides insights into multiple phenomena observed in the ICL field: why retrieving demonstrative samples similar to test samples can help, why ICL performance is sensitive to the output formats, and why ICL accuracy benefits from selecting in-distribution and representative samples. Code and resources are publicly available at https://github.com/Glaciohound/Explain-ICL-As-Kernel-Regression.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {}, "doi_lower": null}
{"paper_id": 259262608, "title": "Understanding In-Context Learning via Supportive Pretraining Data", "author_names": ["Xiaochuang Han", "Daniel Simig", "Todor Mihaylov", "Yulia Tsvetkov", "Asli Celikyilmaz", "Tianlu Wang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In-context learning (ICL) improves language models’ performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining data that supports ICL. We observe that a continued pretraining on this small subset significantly improves the model’s ICL ability, by up to 18%. We then compare the supportive subset constrastively with random subsets of pretraining data and discover: (1) The supportive pretraining data to ICL do not have a higher domain relevance to downstream tasks. (2) The supportive pretraining data have a higher mass of rarely occurring, long-tail tokens. (3) The supportive pretraining data are challenging examples where the information gain from long-range context is below average, indicating learning to incorporate difficult long-range context encourages ICL. Our work takes a first step towards understanding ICL via analyzing instance-level pretraining data. Our insights have a potential to enhance the ICL ability of language models by actively guiding the construction of pretraining data in the future.", "year": 2023, "publicationdate": "2023-06-26", "externalids": {"DOI": "10.48550/arXiv.2306.15091"}, "doi_lower": "10.48550/arxiv.2306.15091"}
{"paper_id": 249626024, "title": "Language Models are General-Purpose Interfaces", "author_names": ["Y. Hao", "Haoyu Song", "Li Dong", "Shaohan Huang", "Zewen Chi", "Wenhui Wang", "Shuming Ma", "Furu Wei"], "venue": "arXiv.org", "abstract": "Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.", "year": 2022, "publicationdate": "2022-06-13", "externalids": {"DOI": "10.48550/arXiv.2206.06336"}, "doi_lower": "10.48550/arxiv.2206.06336"}
{"paper_id": 254591686, "title": "Structured Prompting: Scaling In-Context Learning to 1, 000 Examples", "author_names": ["Y. Hao", "Yutao Sun", "Li Dong", "Zhixiong Han", "Yuxian Gu", "Furu Wei"], "venue": "arXiv.org", "abstract": "Large language models have exhibited intriguing in-context learning capability, achieving promising zero- and few-shot performance without updating the parameters. However, conventional in-context learning is usually restricted by length constraints, rendering it ineffective to absorb supervision from a large number of examples. In order to go beyond few shots, we introduce structured prompting that breaks the length limit and scales in-context learning to thousands of examples. Speciﬁcally, demonstration examples are separately encoded with well-designed position embeddings, and then they are jointly attended by the test example using a rescaled attention mechanism. So we can scale the number of exemplars with linear complexity instead of quadratic complexity with respect to length. Experimental results on a diverse set of tasks show that our approach improves end-task performance and reduces evaluation variance over conventional in-context learning as the number of demonstration examples increases. Code has been released at https://aka.ms/structured-prompting .", "year": 2022, "publicationdate": "2022-12-13", "externalids": {"DOI": "10.48550/arXiv.2212.06713"}, "doi_lower": "10.48550/arxiv.2212.06713"}
{"paper_id": 257427632, "title": "ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction", "author_names": ["Jiabang He", "Lei Wang", "Yingpeng Hu", "Ning Liu", "Hui-juan Liu", "Xingdong Xu", "Hengtao Shen"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated remarkable results in various natural language processing (NLP) tasks with in-context learning, which involves inference based on a few demonstration examples. Despite their successes in NLP tasks, no investigation has been conducted to assess the ability of LLMs to perform document information extraction (DIE) using in-context learning. Applying LLMs to DIE poses two challenges: the modality and task gap. To this end, we propose a simple but effective in-context learning framework called ICL-D3IE, which enables LLMs to perform DIE with different types of demonstration examples. Specifically, we extract the most difficult and distinct segments from hard training documents as hard demonstrations for benefiting all test instances. We design demonstrations describing relationships that enable LLMs to understand positional relationships. We introduce formatting demonstrations for easy answer extraction. Additionally, the framework improves diverse demonstrations by updating them iteratively. Our experiments on three widely used benchmark datasets demonstrate that the ICL-D3IE framework enables Davinci-003/ChatGPT to achieve superior performance when compared to previous pre-trained methods fine-tuned with full training in both the in-distribution (ID) setting and in the out-of-distribution (OOD) setting. Code is available at https://github.com/MAEHCM/ICL-D3IE.", "year": 2023, "publicationdate": "2023-03-09", "externalids": {"DOI": "10.1109/ICCV51070.2023.01785"}, "doi_lower": "10.1109/iccv51070.2023.01785"}
{"paper_id": 268819792, "title": "Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models", "author_names": ["Wei He", "Shichun Liu", "Jun Zhao", "Yiwen Ding", "Yi Lu", "Zhiheng Xi", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "venue": "NAACL-HLT", "abstract": "Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query. Thorough experiments on our dataset and two public math benchmarks have shown that our method can outperform state-of-the-art baselines in the OOD setting. Moreover, we conduct a range of analyses to validate Self-Demos's generalization and provide more insights.", "year": 2024, "publicationdate": "2024-04-01", "externalids": {"DOI": "10.48550/arXiv.2404.00884"}, "doi_lower": "10.48550/arxiv.2404.00884"}
{"paper_id": 269362836, "title": "Continual Learning of Large Language Models: A Comprehensive Survey", "author_names": ["Haizhou Shi", "Zihao Xu", "Hengyi Wang", "Weiyi Qin", "Wenyuan Wang", "Yibin Wang", "Hao Wang"], "venue": "ACM Computing Surveys", "abstract": "The challenge of effectively and efficiently adapting statically pre-trained Large Language Models (LLMs) to ever-evolving data distributions remains predominant. When tailored for specific needs, pre-trained LLMs often suffer from significant performance degradation in previous knowledge domains – a phenomenon known as “catastrophic forgetting”. While extensively studied in the Continual Learning (CL) community, this problem presents new challenges in the context of LLMs. In this survey, we provide a comprehensive overview and detailed discussion of the current research progress on LLMs within the context of CL. Besides the introduction of the preliminary knowledge, this survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3). Following vertical continuity, we summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4). We then provide an overview of evaluation protocols for continual learning with LLMs, along with currently available data sources (Section 5). Finally, we discuss intriguing questions related to continual learning for LLMs (Section 6). This survey sheds light on the relatively understudied domain of continually pre-training, adapting, and fine-tuning large language models, suggesting the necessity for greater attention from the community. Key areas requiring immediate focus include the development of practical and accessible evaluation benchmarks, along with methodologies specifically designed to counter forgetting and enable knowledge transfer within the evolving landscape of LLM learning paradigms. The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey.", "year": 2024, "publicationdate": "2024-04-25", "externalids": {"DOI": "10.1145/3735633"}, "doi_lower": "10.1145/3735633"}
{"paper_id": 248986755, "title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions", "author_names": ["Or Honovich", "Uri Shaham", "Samuel R. Bowman", "Omer Levy"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7% of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8% of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space.", "year": 2022, "publicationdate": "2022-05-22", "externalids": {"DOI": "10.48550/arXiv.2205.10782"}, "doi_lower": "10.48550/arxiv.2205.10782"}
{"paper_id": 258832444, "title": "PRODIGY: Enabling In-context Learning Over Graphs", "author_names": ["Qian Huang", "Hongyu Ren", "Peng Chen", "Gregor Krvzmanc", "D. Zeng", "Percy Liang", "J. Leskovec"], "venue": "Neural Information Processing Systems", "abstract": "In-context learning is the ability of a pretrained model to adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters. While large language models have demonstrated this ability, how in-context learning could be performed over graphs is unexplored. In this paper, we develop \\textbf{Pr}etraining \\textbf{O}ver \\textbf{D}iverse \\textbf{I}n-Context \\textbf{G}raph S\\textbf{y}stems (PRODIGY), the first pretraining framework that enables in-context learning over graphs. The key idea of our framework is to formulate in-context learning over graphs with a novel \\emph{prompt graph} representation, which connects prompt examples and queries. We then propose a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives. With PRODIGY, the pretrained model can directly perform novel downstream classification tasks on unseen graphs via in-context learning. We provide empirical evidence of the effectiveness of our framework by showcasing its strong in-context learning performance on tasks involving citation networks and knowledge graphs. Our approach outperforms the in-context learning accuracy of contrastive pretraining baselines with hard-coded adaptation by 18\\% on average across all setups. Moreover, it also outperforms standard finetuning with limited data by 33\\% on average with in-context learning.", "year": 2023, "publicationdate": "2023-05-21", "externalids": {"DOI": "10.48550/arXiv.2305.12600"}, "doi_lower": "10.48550/arxiv.2305.12600"}
{"paper_id": 257219775, "title": "Language Is Not All You Need: Aligning Perception with Language Models", "author_names": ["Shaohan Huang", "Li Dong", "Wenhui Wang", "Y. Hao", "Saksham Singhal", "Shuming Ma", "Tengchao Lv", "Lei Cui", "O. Mohammed", "Qiang Liu", "Kriti Aggarwal", "Zewen Chi", "Johan Bjorck", "Vishrav Chaudhary", "Subhojit Som", "Xia Song", "Furu Wei"], "venue": "Neural Information Processing Systems", "abstract": "A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.", "year": 2023, "publicationdate": "2023-02-27", "externalids": {"DOI": "10.48550/arXiv.2302.14045"}, "doi_lower": "10.48550/arxiv.2302.14045"}
{"paper_id": 246823378, "title": "The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention", "author_names": ["Kazuki Irie", "R'obert Csord'as", "J. Schmidhuber"], "venue": "International Conference on Machine Learning", "abstract": "Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public.", "year": 2022, "publicationdate": "2022-02-11", "externalids": {}, "doi_lower": null}
{"paper_id": 255096269, "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization", "author_names": ["S. Iyer", "Xi Victoria Lin", "Ramakanth Pasunuru", "Todor Mihaylov", "Daniel Simig", "Ping Yu", "Kurt Shuster", "Tianlu Wang", "Qing Liu", "Punit Singh Koura", "Xian Li", "Brian O'Horo", "Gabriel Pereyra", "Jeff Wang", "Christopher Dewan", "Asli Celikyilmaz", "Luke S. Zettlemoyer", "Veselin Stoyanov"], "venue": "arXiv.org", "abstract": "Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.", "year": 2022, "publicationdate": "2022-12-22", "externalids": {}, "doi_lower": null}
{"paper_id": 258236342, "title": "A Latent Space Theory for Emergent Abilities in Large Language Models", "author_names": ["Hui Jiang"], "venue": "arXiv.org", "abstract": "Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.", "year": 2023, "publicationdate": "2023-04-19", "externalids": {"DOI": "10.48550/arXiv.2304.09960"}, "doi_lower": "10.48550/arxiv.2304.09960"}
{"paper_id": 258686689, "title": "Exploring In-Context Learning Capabilities of Foundation Models for Generating Knowledge Graphs from Text", "author_names": ["H. Khorashadizadeh", "Nandana Mihindukulasooriya", "S. Tiwari", "Jinghua Groppe", "Sven Groppe"], "venue": "TEXT2KG/BiKE@ESWC", "abstract": "Knowledge graphs can represent information about the real-world using entities and their relations in a structured and semantically rich manner and they enable a variety of downstream applications such as question-answering, recommendation systems, semantic search, and advanced analytics. However, at the moment, building a knowledge graph involves a lot of manual effort and thus hinders their application in some situations and the automation of this process might benefit especially for small organizations. Automatically generating structured knowledge graphs from a large volume of natural language is still a challenging task and the research on sub-tasks such as named entity extraction, relation extraction, entity and relation linking, and knowledge graph construction aims to improve the state of the art of automatic construction and completion of knowledge graphs from text. The recent advancement of foundation models with billions of parameters trained in a self-supervised manner with large volumes of training data that can be adapted to a variety of downstream tasks has helped to demonstrate high performance on a large range of Natural Language Processing (NLP) tasks. In this context, one emerging paradigm is in-context learning where a language model is used as it is with a prompt that provides instructions and some examples to perform a task without changing the parameters of the model using traditional approaches such as fine-tuning. This way, no computing resources are needed for re-training/fine-tuning the models and the engineering effort is minimal. Thus, it would be beneficial to utilize such capabilities for generating knowledge graphs from text.", "year": 2023, "publicationdate": "2023-05-15", "externalids": {}, "doi_lower": null}
{"paper_id": 249712501, "title": "Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator", "author_names": ["Hyuhng Joon Kim", "Hyunsoo Cho", "Junyeob Kim", "Taeuk Kim", "Kang Min Yoo", "Sang-goo Lee"], "venue": "arXiv.org", "abstract": "Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e., in-context learning), however, naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper, we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration. We conduct experiments on four different text classification tasks and show SG-ICL significantly outperforms zero-shot learning and is generally worth approximately 0.6 gold training samples. Moreover, our generated demonstrations show more consistent performance with low variance compared to randomly selected demonstrations from the training dataset.", "year": 2022, "publicationdate": "2022-06-16", "externalids": {"DOI": "10.48550/arXiv.2206.08082"}, "doi_lower": "10.48550/arxiv.2206.08082"}
{"paper_id": 263867991, "title": "In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning", "author_names": ["Jannik Kossen", "Tom Rainforth", "Y. Gal"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2307.12375"}, "doi_lower": "10.48550/arxiv.2307.12375"}
{"paper_id": 258547300, "title": "Otter: A Multi-Modal Model With In-Context Instruction Tuning", "author_names": ["Bo Li", "Yuanhan Zhang", "Liangyu Chen", "Jinghao Wang", "Jingkang Yang", "Ziwei Liu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "Recent advances in Large Multimodal Models (LMMs) have unveiled great potential as visual assistants. However, most existing works focus on responding to individual instructions or using previous dialogues for contextual understanding. There is little discussion on employing both images and text as in-context examples to enhance the instruction following capability. To bridge this gap, we introduce the Otter model to leverage both textual and visual in-context examples for instruction tuning. Specifically, Otter builds upon Flamingo with Perceiver architecture, and has been instruction tuned for general purpose multi-modal assistant. Otter seamlessly processes multi-modal inputs, supporting modalities including text, multiple images, and dynamic video content. To support the training of Otter, we present the MIMIC-IT (MultI-Modal In-Context Instruction Tuning) dataset, which encompasses over 3 million multi-modal instruction-response pairs, including approximately 2.2 million unique instructions across a broad spectrum of images and videos. MIMIC-IT has been carefully curated to feature a diverse array of in-context examples for each entry. Comprehensive evaluations suggest that instruction tuning with these in-context examples substantially enhances model convergence and generalization capabilities. Notably, the extensive scenario coverage provided by the MIMIC-IT dataset empowers the Otter model to excel in tasks involving complex video and multi-image understanding.", "year": 2023, "publicationdate": "2023-05-05", "externalids": {"DOI": "10.1109/TPAMI.2025.3571946"}, "doi_lower": "10.1109/tpami.2025.3571946"}
{"paper_id": 263897368, "title": "Towards Enhancing In-Context Learning for Code Generation", "author_names": ["Jia Li", "Yunfei Zhao", "Yongming Li", "Ge Li", "Zhi Jin"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2303.17780"}, "doi_lower": "10.48550/arxiv.2303.17780"}
{"paper_id": 269899527, "title": "Feature-Adaptive and Data-Scalable In-Context Learning", "author_names": ["Jiahao Li", "Quan Wang", "L. Zhang", "Guoqing Jin", "Zhendong Mao"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In-context learning (ICL), which promotes inference with several demonstrations, has become a widespread paradigm to stimulate LLM capabilities for downstream tasks. Due to context length constraints, it cannot be further improved in spite of more training data, and general features directly from LLMs in ICL are not adaptive to the specific downstream task. In this paper, we propose a feature-adaptive and data-scalable in-context learning framework (FADS-ICL), which can leverage task-adaptive features to promote inference on the downstream task, with the supervision of beyond-context samples. Specifically, it first extracts general features of beyond-context samples via the LLM with ICL input form one by one, and introduces a task-specific modulator to perform feature refinement and prediction after fitting a specific downstream task. We conduct extensive experiments on FADS-ICL under varying data settings (4$\\sim$128 shots) and LLM scale (0.8$\\sim$70B) settings. Experimental results show that FADS-ICL consistently outperforms previous state-of-the-art methods by a significant margin under all settings, verifying the effectiveness and superiority of FADS-ICL. For example, under the 1.5B and 32 shots setting, FADS-ICL can achieve \\textbf{+14.3} average accuracy from feature adaptation over vanilla ICL on 10 datasets, with \\textbf{+6.2} average accuracy over the previous state-of-the-art method, and the performance can further improve with increasing training data. Code and data are publicly available at \\url{https://github.com/jiahaozhenbang/FADS-ICL}.", "year": 2024, "publicationdate": "2024-05-17", "externalids": {"DOI": "10.48550/arXiv.2405.10738"}, "doi_lower": "10.48550/arxiv.2405.10738"}
{"paper_id": 258331729, "title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression", "author_names": ["Shuai Li", "Zhao Song", "Yu Xia", "Tong Yu", "Tianyi Zhou"], "venue": "Neural Information Processing Systems", "abstract": "Large language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human life-related or even job-related tasks. The attention mechanism in the Transformer architecture is a critical component of LLMs, as it allows the model to selectively focus on specific input parts. The softmax unit, which is a key part of the attention mechanism, normalizes the attention scores. Hence, the performance of LLMs in various NLP tasks depends significantly on the crucial role played by the attention mechanism with the softmax unit. In-context learning, as one of the celebrated abilities of recent LLMs, is an important concept in querying LLMs such as ChatGPT. Without further parameter updates, Transformers can learn to predict based on few in-context examples. However, the reason why Transformers becomes in-context learners is not well understood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the in-context learning from a mathematical perspective based on a linear regression formulation $\\min_x\\| Ax - b \\|_2$, which show Transformers' capability of learning linear functions in context. In this work, we study the in-context learning based on a softmax regression formulation $\\min_{x} \\| \\langle \\exp(Ax), {\\bf 1}_n \\rangle^{-1} \\exp(Ax) - b \\|_2$ of Transformer's attention mechanism. We show the upper bounds of the data transformations induced by a single self-attention layer and by gradient-descent on a $\\ell_2$ regression loss for softmax prediction function, which imply that when training self-attention-only Transformers for fundamental regression tasks, the models learned by gradient-descent and Transformers show great similarity.", "year": 2023, "publicationdate": "2023-04-26", "externalids": {"DOI": "10.48550/arXiv.2304.13276"}, "doi_lower": "10.48550/arxiv.2304.13276"}
{"paper_id": 268857023, "title": "Long-context LLMs Struggle with Long In-context Learning", "author_names": ["Tianle Li", "Ge Zhang", "Quy Duc Do", "Xiang Yue", "Wenhu Chen"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Large Language Models (LLMs) have made significant strides in handling long sequences. Some models like Gemini could even to be capable of dealing with millions of tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their true abilities in more challenging, real-world scenarios. We introduce a benchmark (LongICLBench) for long in-context learning in extreme-label classification using six datasets with 28 to 174 classes and input lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct predictions. We evaluate on 15 long-context LLMs and find that they perform well on less challenging classification tasks with smaller label space and shorter demonstrations. However, they struggle with more challenging task like Discovery with 174 labels, suggesting a gap in their ability to process long, context-rich sequences. Further analysis reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LongICLBench could serve as a more realistic evaluation for the future long-context LLMs.", "year": 2024, "publicationdate": "2024-04-02", "externalids": {"DOI": "10.48550/arXiv.2404.02060"}, "doi_lower": "10.48550/arxiv.2404.02060"}
{"paper_id": 258557751, "title": "Unified Demonstration Retriever for In-Context Learning", "author_names": ["Xiaonan Li", "Kai Lv", "Hang Yan", "Tianya Lin", "Wei Zhu", "Yuan Ni", "G. Xie", "Xiaoling Wang", "Xipeng Qiu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown sensitive to the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works train task-specific retrievers for several tasks separately, these methods are hard to transfer and scale on various tasks, and separately trained retrievers will cause a lot of parameter storage and deployment cost. In this paper, we propose Unified Demonstration Retriever (UDR), a single model to retrieve demonstrations for a wide range of tasks. To train UDR, we cast various tasks’ training signals into a unified list-wise ranking formulation by language model’s feedback. Then we propose a multi-task list-wise ranking training framework with an iterative mining strategy to find high-quality candidates, which can help UDR fully incorporate various tasks’ signals. Experiments on 30+ tasks across 13 task families and multiple data domains show that UDR significantly outperforms baselines. Further analyses show the effectiveness of each proposed component and UDR’s strong ability in various scenarios including different LMs (1.3B 175B), unseen datasets, varying demonstration quantities, etc. We will release the code and model checkpoint after review.", "year": 2023, "publicationdate": "2023-05-07", "externalids": {"DOI": "10.48550/arXiv.2305.04320"}, "doi_lower": "10.48550/arxiv.2305.04320"}
{"paper_id": 257219778, "title": "Finding Support Examples for In-Context Learning", "author_names": ["Xiaonan Li", "Xipeng Qiu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Additionally, the strong dependency among in-context examples makes it an NP-hard combinatorial optimization problem and enumerating all permutations is infeasible. Hence we propose LENS, a fiLter-thEN-Search method to tackle this challenge in two stages: First we filter the dataset to obtain informative in-context examples individually. Specifically, we propose a novel metric, InfoScore, to evaluate the example's in-context informativeness based on the language model's feedback, and further propose a progressive filtering process to filter out uninformative examples. Then we propose diversity-guided example search which iteratively refines and evaluates the selected example permutations, to find examples that fully depict the task. The experimental results show that LENS significantly outperforms a wide range of baselines.", "year": 2023, "publicationdate": "2023-02-27", "externalids": {"DOI": "10.18653/v1/2023.findings-emnlp.411"}, "doi_lower": "10.18653/v1/2023.findings-emnlp.411"}
{"paper_id": 268363458, "title": "MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning", "author_names": ["Yichuan Li", "Xiyao Ma", "Sixing Lu", "Kyumin Lee", "Xiaohu Liu", "Chenlei Guo"], "venue": "International Conference on Learning Representations", "abstract": "Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations). Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism. Existing solutions attempt to distill lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise LLM's in-context learning performance. To mitigate these challenges, we present Meta dEmonstratioN Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task. We exploit the knowledge distillation to enhance alignment between MEND and LLM, achieving both efficiency and effectiveness simultaneously. MEND is endowed with the meta-knowledge of distilling demonstrations through a two-stage training process, which includes meta-distillation pretraining and fine-tuning. Comprehensive evaluations across seven diverse ICL task partitions using decoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not only matches but often outperforms the Vanilla ICL as well as other state-of-the-art distillation models, while significantly reducing the computational demands. This innovation promises enhanced scalability and efficiency for the practical deployment of large language models", "year": 2024, "publicationdate": "2024-03-11", "externalids": {"DOI": "10.48550/arXiv.2403.06914"}, "doi_lower": "10.48550/arxiv.2403.06914"}
{"paper_id": 256616253, "title": "Transformers as Algorithms: Generalization and Stability in In-context Learning", "author_names": ["Yingcong Li", "M. E. Ildiz", "Dimitris Papailiopoulos", "Samet Oymak"], "venue": "International Conference on Machine Learning", "abstract": "In-context learning (ICL) is a type of prompting where a transformer model operates on a sequence of (input, output) examples and performs inference on-the-fly. In this work, we formalize in-context learning as an algorithm learning problem where a transformer model implicitly constructs a hypothesis function at inference-time. We first explore the statistical aspects of this abstraction through the lens of multitask learning: We obtain generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system. The crux of our analysis is relating the excess risk to the stability of the algorithm implemented by the transformer. We characterize when transformer/attention architecture provably obeys the stability condition and also provide empirical verification. For generalization on unseen tasks, we identify an inductive bias phenomenon in which the transfer learning risk is governed by the task complexity and the number of MTL tasks in a highly predictable manner. Finally, we provide numerical evaluations that (1) demonstrate transformers can indeed implement near-optimal algorithms on classical regression problems with i.i.d. and dynamic data, (2) provide insights on stability, and (3) verify our theoretical predictions.", "year": 2023, "publicationdate": "2023-01-17", "externalids": {}, "doi_lower": null}
{"paper_id": 262460726, "title": "A Practical Survey on Zero-Shot Prompt Design for In-Context Learning", "author_names": ["Yinheng Li"], "venue": "Recent Advances in Natural Language Processing", "abstract": "The remarkable advancements in large language models (LLMs) have brought about significant improvements in Natural Language Processing(NLP) tasks. This paper presents a comprehensive review of in-context learning techniques, focusing on different types of prompts, including discrete, continuous, few-shot, and zero-shot, and their impact on LLM performance. We explore various approaches to prompt design, such as manual design, optimization algorithms, and evaluation methods, to optimize LLM performance across diverse tasks. Our review covers key research studies in prompt engineering, discussing their methodologies and contributions to the field. We also delve into the challenges faced in evaluating prompt performance, given the absence of a single “best” prompt and the importance of considering multiple metrics. In conclusion, the paper highlights the critical role of prompt design in harnessing the full potential of LLMs and provides insights into the combination of manual design, optimization techniques, and rigorous evaluation for more effective and efficient use of LLMs in various NLP tasks.", "year": 2023, "publicationdate": "2023-09-22", "externalids": {"DOI": "10.26615/978-954-452-092-2_069"}, "doi_lower": "10.26615/978-954-452-092-2_069"}
{"paper_id": 269983389, "title": "Implicit In-context Learning", "author_names": ["Zhuowei Li", "Zihao Xu", "Ligong Han", "Yunhe Gao", "Song Wen", "Di Liu", "Hao Wang", "Dimitris N. Metaxas"], "venue": "International Conference on Learning Representations", "abstract": "In-context Learning (ICL) empowers large language models (LLMs) to swiftly adapt to unseen tasks at inference-time by prefixing a few demonstration examples before queries. Despite its versatility, ICL incurs substantial computational and memory overheads compared to zero-shot learning and is sensitive to the selection and order of demonstration examples. In this work, we introduce Implicit In-context Learning (I2CL), an innovative paradigm that reduces the inference cost of ICL to that of zero-shot learning with minimal information loss. I2CL operates by first generating a condensed vector representation, namely a context vector, extracted from the demonstration examples. It then conducts an inference-time intervention through injecting a linear combination of the context vector and query activations back into the model's residual streams. Empirical evaluation on nine real-world tasks across three model architectures demonstrates that I2CL achieves few-shot level performance at zero-shot inference cost, and it exhibits robustness against variations in demonstration examples. Furthermore, I2CL facilitates a novel representation of task-ids, enhancing task similarity detection and fostering effective transfer learning. We also perform a comprehensive analysis and ablation study on I2CL, offering deeper insights into its internal mechanisms. Code is available at https://github.com/LzVv123456/I2CL.", "year": 2024, "publicationdate": "2024-05-23", "externalids": {"DOI": "10.48550/arXiv.2405.14660"}, "doi_lower": "10.48550/arxiv.2405.14660"}
{"paper_id": 258179774, "title": "Visual Instruction Tuning", "author_names": ["Haotian Liu", "Chunyuan Li", "Qingyang Wu", "Yong Jae Lee"], "venue": "Neural Information Processing Systems", "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.", "year": 2023, "publicationdate": "2023-04-17", "externalids": {"DOI": "10.48550/arXiv.2304.08485"}, "doi_lower": "10.48550/arxiv.2304.08485"}
{"paper_id": 241574664, "title": "Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures", "author_names": [], "venue": "", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2021.deelio-1"}, "doi_lower": "10.18653/v1/2021.deelio-1"}
{"paper_id": 259360665, "title": "Lost in the Middle: How Language Models Use Long Contexts", "author_names": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "F. Petroni", "Percy Liang"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.", "year": 2023, "publicationdate": "2023-07-06", "externalids": {"DOI": "10.1162/tacl_a_00638"}, "doi_lower": "10.1162/tacl_a_00638"}
{"paper_id": 236493269, "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing", "author_names": ["Pengfei Liu", "Weizhe Yuan", "Jinlan Fu", "Zhengbao Jiang", "Hiroaki Hayashi", "Graham Neubig"], "venue": "ACM Computing Surveys", "abstract": "This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia–Pretrain including constantly updated survey and paperlist.", "year": 2021, "publicationdate": "2021-07-28", "externalids": {"DOI": "10.1145/3560815"}, "doi_lower": "10.1145/3560815"}
{"paper_id": 265149781, "title": "In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering", "author_names": ["Sheng Liu", "Haotian Ye", "Lei Xing", "James Y. Zou"], "venue": "International Conference on Machine Learning", "abstract": "Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3) it reduces the length of the prompt by removing the in-context demonstrations; 4) ICV is computationally much more efficient than fine-tuning. We demonstrate that ICV achieves better performance compared to standard in-context learning and fine-tuning on diverse tasks including safety, style transfer, role-playing and formatting. Moreover, we show that we can flexibly teach LLM to simultaneously follow different types of instructions by simple vector arithmetics on the corresponding ICVs.", "year": 2023, "publicationdate": "2023-11-11", "externalids": {"DOI": "10.48550/arXiv.2311.06668"}, "doi_lower": "10.48550/arxiv.2311.06668"}
{"paper_id": 267740729, "title": "Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning", "author_names": ["Yinpeng Liu", "Jiawei Liu", "Xiang Shi", "Qikai Cheng", "Wei Lu"], "venue": "arXiv.org", "abstract": "Demonstration ordering, which is an important strategy for in-context learning (ICL), can significantly affects the performance of large language models (LLMs). However, most of the current approaches of ordering require high computational costs to introduce the priori knowledge. In this paper, inspired by the human learning process, we propose a simple but effective demonstration ordering method for ICL, named the few-shot In-Context Curriculum Learning (ICCL). The ICCL implies gradually increasing the complexity of prompt demonstrations during the inference process. The difficulty can be assessed by human experts or LLMs-driven metrics, such as perplexity. Then we design extensive experiments to discuss the effectiveness of the ICCL at both corpus-level and instance-level. Moreover, we also investigate the formation mechanism of LLM's ICCL capability. Experimental results demonstrate that ICCL, developed during the instruction-tuning stage, is effective for representative open-source LLMs. To facilitate further research and applications by other scholars, we make the code publicly available.", "year": 2024, "publicationdate": "2024-02-16", "externalids": {"DOI": "10.48550/arXiv.2402.10738"}, "doi_lower": "10.48550/arxiv.2402.10738"}
{"paper_id": 260815690, "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time", "author_names": ["Zichang Liu", "Jue Wang", "Tri Dao", "Tianyi Zhou", "Binhang Yuan", "Zhao Song", "Anshumali Shrivastava", "Ce Zhang", "Yuandong Tian", "Christopher Ré", "Beidi Chen"], "venue": "International Conference on Machine Learning", "abstract": "Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardware-aware implementation that speeds up LLM inference. We validate that DejaVu can reduce the inference latency of OPT-175B by over 2X compared to the state-of-the-art FasterTransformer, and over 6X compared to the widely used Hugging Face implementation, without compromising model quality. The code is available at https://github.com/FMInference/DejaVu.", "year": 2023, "publicationdate": "2023-10-26", "externalids": {"DOI": "10.48550/arXiv.2310.17157"}, "doi_lower": "10.48550/arxiv.2310.17157"}
{"paper_id": 233296494, "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity", "author_names": ["Yao Lu", "Max Bartolo", "Alastair Moore", "Sebastian Riedel", "Pontus Stenetorp"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are “fantastic” and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.", "year": 2021, "publicationdate": "2021-04-18", "externalids": {"DOI": "10.18653/v1/2022.acl-long.556"}, "doi_lower": "10.18653/v1/2022.acl-long.556"}
{"paper_id": 259375820, "title": "One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention", "author_names": ["Arvind V. Mahankali", "Tatsunori Hashimoto", "Tengyu Ma"], "venue": "International Conference on Learning Representations", "abstract": "Recent works have empirically analyzed in-context learning and shown that transformers trained on synthetic linear regression tasks can learn to implement ridge regression, which is the Bayes-optimal predictor, given sufficient capacity [Aky\\\"urek et al., 2023], while one-layer transformers with linear self-attention and no MLP layer will learn to implement one step of gradient descent (GD) on a least-squares linear regression objective [von Oswald et al., 2022]. However, the theory behind these observations remains poorly understood. We theoretically study transformers with a single layer of linear self-attention, trained on synthetic noisy linear regression data. First, we mathematically show that when the covariates are drawn from a standard Gaussian distribution, the one-layer transformer which minimizes the pre-training loss will implement a single step of GD on the least-squares linear regression objective. Then, we find that changing the distribution of the covariates and weight vector to a non-isotropic Gaussian distribution has a strong impact on the learned algorithm: the global minimizer of the pre-training loss now implements a single step of $\\textit{pre-conditioned}$ GD. However, if only the distribution of the responses is changed, then this does not have a large effect on the learned algorithm: even when the response comes from a more general family of $\\textit{nonlinear}$ functions, the global minimizer of the pre-training loss still implements a single step of GD on a least-squares linear regression objective.", "year": 2023, "publicationdate": "2023-07-07", "externalids": {"DOI": "10.48550/arXiv.2307.03576"}, "doi_lower": "10.48550/arxiv.2307.03576"}
{"paper_id": 264829036, "title": "Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection", "author_names": ["Costas Mavromatis", "Balasubramaniam Srinivasan", "Zhengyuan Shen", "Jiani Zhang", "H. Rangwala", "Christos Faloutsos", "G. Karypis"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) can adapt to new tasks via in-context learning (ICL). ICL is efficient as it does not require any parameter updates to the trained LLM, but only few annotated examples as input for the LLM. In this work, we investigate an active learning approach for ICL, where there is a limited budget for annotating examples. We propose a model-adaptive optimization-free algorithm, termed AdaICL, which identifies examples that the model is uncertain about, and performs semantic diversity-based example selection. Diversity-based sampling improves overall effectiveness, while uncertainty sampling improves budget efficiency and helps the LLM learn new information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage problem, that dynamically adapts based on the model's feedback and can be approximately solved via greedy algorithms. Extensive experiments on nine datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy points over SOTA (7.7% relative improvement), is up to 3x more budget-efficient than performing annotations uniformly at random, while it outperforms SOTA with 2x fewer ICL examples.", "year": 2023, "publicationdate": "2023-10-30", "externalids": {"DOI": "10.48550/arXiv.2310.20046"}, "doi_lower": "10.48550/arxiv.2310.20046"}
{"paper_id": 256503647, "title": "Using In-Context Learning to Improve Dialogue Safety", "author_names": ["Nicholas Meade", "Spandana Gella", "Devamanyu Hazarika", "Prakhar Gupta", "Di Jin", "Siva Reddy", "Yang Liu", "Dilek Z. Hakkani-Tür"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. We investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with strong baselines without requiring training. For instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking procedure which can further improve response safeness.", "year": 2023, "publicationdate": "2023-02-02", "externalids": {"DOI": "10.48550/arXiv.2302.00871"}, "doi_lower": "10.48550/arxiv.2302.00871"}
{"paper_id": 236956577, "title": "Noisy Channel Language Model Prompting for Few-Shot Text Classification", "author_names": ["Sewon Min", "Michael Lewis", "Hannaneh Hajishirzi", "Luke Zettlemoyer"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive models (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.", "year": 2021, "publicationdate": "2021-08-09", "externalids": {"DOI": "10.18653/v1/2022.acl-long.365"}, "doi_lower": "10.18653/v1/2022.acl-long.365"}
{"paper_id": 240288835, "title": "MetaICL: Learning to Learn In Context", "author_names": ["Sewon Min", "M. Lewis", "Luke Zettlemoyer", "Hannaneh Hajishirzi"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different meta-training/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task training data, and outperforms much bigger models with nearly 8x parameters.", "year": 2021, "publicationdate": "2021-10-29", "externalids": {"DOI": "10.18653/v1/2022.naacl-main.201"}, "doi_lower": "10.18653/v1/2022.naacl-main.201"}
{"paper_id": 247155069, "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?", "author_names": ["Sewon Min", "Xinxi Lyu", "Ari Holtzman", "Mikel Artetxe", "M. Lewis", "Hannaneh Hajishirzi", "Luke Zettlemoyer"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.", "year": 2022, "publicationdate": "2022-02-25", "externalids": {"DOI": "10.18653/v1/2022.emnlp-main.759"}, "doi_lower": "10.18653/v1/2022.emnlp-main.759"}
{"paper_id": 237421373, "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions", "author_names": ["Swaroop Mishra", "Daniel Khashabi", "Chitta Baral", "Hannaneh Hajishirzi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.", "year": 2021, "publicationdate": "2021-04-18", "externalids": {"DOI": "10.18653/v1/2022.acl-long.244"}, "doi_lower": "10.18653/v1/2022.acl-long.244"}
{"paper_id": 257078624, "title": "In-context Example Selection with Influences", "author_names": ["Nguyen Tai", "Eric Wong"], "venue": "arXiv.org", "abstract": "In-context learning (ICL) is a powerful paradigm emerged from large language models (LLMs). Despite its promises, ICL performance is known to be highly sensitive to input examples. In this work, we use $\\textit{in-context influences}$ to analyze few-shot ICL performance directly from the in-context examples. Our proposed influence-based example selection method can identify both positive and negative examples, outperforming several baselines when evaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\\%$ performance gap between using the most negative in-context examples compared to the most positive. In a case study, we apply our influence-based framework to quantify the phenomena of recency bias in example ordering for few-shot ICL.", "year": 2023, "publicationdate": "2023-02-21", "externalids": {"DOI": "10.48550/arXiv.2302.11042"}, "doi_lower": "10.48550/arxiv.2302.11042"}
{"paper_id": 162256654, "title": "CLARK, Tom. Jack Kerouac: A Biography.", "author_names": ["George F. Wedge"], "venue": "", "abstract": null, "year": 1992, "publicationdate": null, "externalids": {"DOI": "10.1080/0895769X.1992.10542722"}, "doi_lower": "10.1080/0895769x.1992.10542722"}
{"paper_id": 257532815, "title": "GPT-4 Technical Report", "author_names": ["OpenAI Josh Achiam", "Steven Adler", "Sandhini Agarwal", "Lama Ahmad", "Ilge Akkaya", "Florencia Leoni Aleman", "Diogo Almeida", "Janko Altenschmidt", "Sam Altman", "Shyamal Anadkat", "Red Avila", "Igor Babuschkin", "S. Balaji", "Valerie Balcom", "Paul Baltescu", "Haim-ing Bao", "Mo Bavarian", "J. Belgum", "Irwan Bello", "Jake Berdine", "Gabriel Bernadett-Shapiro", "Christopher Berner", "Lenny Bogdonoff", "Oleg Boiko", "Made-laine Boyd", "Anna-Luisa Brakman", "Greg Brockman", "Tim Brooks", "Miles Brundage", "Kevin Button", "Trevor Cai", "Rosie Campbell", "Andrew Cann", "Brittany Carey", "Chelsea Carlson", "Rory Carmichael", "Brooke Chan", "Che Chang", "Fotis Chantzis", "Derek Chen", "Sully Chen", "Ruby Chen", "Jason Chen", "Mark Chen", "Benjamin Chess", "Chester Cho", "Casey Chu", "Hyung Won Chung", "Dave Cummings", "Jeremiah Currier", "Yunxing Dai", "Cory Decareaux", "Thomas Degry", "Noah Deutsch", "Damien Deville", "Arka Dhar", "David Dohan", "Steve Dowling", "Sheila Dunning", "Adrien Ecoffet", "Atty Eleti", "Tyna Eloundou", "David Farhi", "L. Fedus", "Niko Felix", "Sim'on Posada Fishman", "Juston Forte", "Is-abella Fulford", "Leo Gao", "Elie Georges", "C. Gibson", "Vik Goel", "Tarun Gogineni", "Gabriel Goh", "Raphael Gontijo-Lopes", "Jonathan Gordon", "Morgan Grafstein", "Scott Gray", "Ryan Greene", "Joshua Gross", "S. Gu", "Yufei Guo", "Chris Hallacy", "Jesse Han", "Jeff Harris", "Yuchen He", "Mike Heaton", "Johannes Heidecke", "Chris Hesse", "Alan Hickey", "W. Hickey", "Peter Hoeschele", "Brandon Houghton", "Kenny Hsu", "Shengli Hu", "Xin Hu", "Joost Huizinga", "Shantanu Jain", "Shawn Jain", "Joanne Jang", "Angela Jiang", "Roger Jiang", "Haozhun Jin", "Denny Jin", "Shino Jomoto", "B. Jonn", "Heewoo Jun", "Tomer Kaftan", "Lukasz Kaiser", "Ali Kamali", "I. Kanitscheider", "N. Keskar", "Tabarak Khan", "Logan Kilpatrick", "Jong Wook Kim", "Christina Kim", "Yongjik Kim", "Hendrik Kirchner", "J. Kiros", "Matthew Knight", "Daniel Kokotajlo", "Lukasz Kondraciuk", "Andrew Kondrich", "Aris Konstantinidis", "Kyle Kosic", "Gretchen Krueger", "Vishal Kuo", "Michael Lampe", "Ikai Lan", "Teddy Lee", "Jan Leike", "Jade Leung", "Daniel Levy", "Chak Li", "Rachel Lim", "Molly Lin", "Stephanie Lin", "Ma-teusz Litwin", "Theresa Lopez", "Ryan Lowe", "Patricia Lue", "A. Makanju", "Kim Malfacini", "Sam Manning", "Todor Markov", "Yaniv Markovski", "Bianca Martin", "Katie Mayer", "Andrew Mayne", "Bob McGrew", "S. McKinney", "Christine McLeavey", "Paul McMillan", "Jake McNeil", "David Medina", "Aalok Mehta", "Jacob Menick", "Luke Metz", "An-drey Mishchenko", "Pamela Mishkin", "Vinnie Monaco", "Evan Morikawa", "Daniel P. Mossing", "Tong Mu", "Mira Murati", "O. Murk", "David M'ely", "Ashvin Nair", "Reiichiro Nakano", "Rajeev Nayak", "Arvind Neelakantan", "Richard Ngo", "Hyeonwoo Noh", "Ouyang Long", "Cullen O'Keefe", "J. Pachocki", "A. Paino", "Joe Palermo", "Ashley Pantuliano", "Giambattista Parascandolo", "J. Parish", "Emy Parparita", "Alexandre Passos", "Mikhail Pavlov", "Andrew Peng", "Adam Perelman", "Filipe de Avila Belbute Peres", "Michael Petrov", "Henrique Pondé de Oliveira Pinto", "Michael Pokorny", "Michelle Pokrass", "Vitchyr H. Pong", "Tolly Powell", "Alethea Power", "Boris Power", "Elizabeth Proehl", "Raul Puri", "Alec Radford", "Jack W. Rae", "Aditya Ramesh", "Cameron Raymond", "Francis Real", "Kendra Rimbach", "Carl Ross", "Bob Rotsted", "Henri Roussez", "N. Ryder", "M. Saltarelli", "Ted Sanders", "Shibani Santurkar", "Girish Sastry", "Heather Schmidt", "David Schnurr", "John Schulman", "Daniel Selsam", "Kyla Sheppard", "Toki Sherbakov", "Jessica Shieh", "Sarah Shoker", "Pranav Shyam", "Szymon Sidor", "Eric Sigler", "Maddie Simens", "Jordan Sitkin", "Katarina Slama", "Ian Sohl", "Benjamin Sokolowsky", "Yang Song", "Natalie Staudacher", "F. Such", "Natalie Summers", "I. Sutskever", "Jie Tang", "N. Tezak", "Madeleine Thompson", "P. Tillet", "Amin Tootoonchian", "Elizabeth Tseng", "Preston Tuggle", "Nick Turley", "Jerry Tworek", "Juan Felipe Cer'on Uribe", "Andrea Vallone", "Arun Vijayvergiya", "Chelsea Voss", "Carroll L. Wainwright", "Justin Jay Wang", "Alvin Wang", "Ben Wang", "Jonathan Ward", "Jason Wei", "CJ Weinmann", "Akila Welihinda", "Peter Welinder", "Jiayi Weng", "Lilian Weng", "Matt Wiethoff", "Dave Willner", "Clemens Winter", "Samuel Wolrich", "Hannah Wong", "Lauren Workman", "Sherwin Wu", "Jeff Wu", "Michael Wu", "Kai Xiao", "Tao Xu", "Sarah Yoo", "Kevin Yu", "Qim-ing Yuan", "Wojciech Zaremba", "Rowan Zellers", "Chong Zhang", "Marvin Zhang", "Shengjia Zhao", "Tianhao Zheng", "Juntang Zhuang", "William Zhuk", "Barret Zoph"], "venue": "", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {}, "doi_lower": null}
{"paper_id": 258740972, "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning", "author_names": ["Jane Pan", "Tianyu Gao", "Howard Chen", "Danqi Chen"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations -- even without ground-truth labels -- and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquire TL as the model scales, and TL's performance consistently improves with more demonstrations in context. Our findings unravel two different forces behind ICL and we advocate for discriminating them in future ICL research due to their distinct nature.", "year": 2023, "publicationdate": "2023-05-16", "externalids": {"DOI": "10.48550/arXiv.2305.09731"}, "doi_lower": "10.48550/arxiv.2305.09731"}
{"paper_id": 263872623, "title": "Differentially Private In-Context Learning", "author_names": ["Ashwinee Panda", "Tong Wu", "Jiachen T. Wang", "Prateek Mittal"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.01639"}, "doi_lower": "10.48550/arxiv.2305.01639"}
{"paper_id": 264146526, "title": "In-Context Learning with Iterative Demonstration Selection", "author_names": ["Chengwei Qin", "Aston Zhang", "Anirudh Dagar", "Wenming Ye"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.", "year": 2023, "publicationdate": "2023-10-15", "externalids": {"DOI": "10.48550/arXiv.2310.09881"}, "doi_lower": "10.48550/arxiv.2310.09881"}
{"paper_id": 160025533, "title": "Language Models are Unsupervised Multitask Learners", "author_names": ["Alec Radford", "Jeff Wu", "R. Child", "D. Luan", "Dario Amodei", "I. Sutskever"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 47018994, "title": "Know What You Don’t Know: Unanswerable Questions for SQuAD", "author_names": ["Pranav Rajpurkar", "Robin Jia", "Percy Liang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.", "year": 2018, "publicationdate": "2018-06-11", "externalids": {"DOI": "10.18653/v1/P18-2124"}, "doi_lower": "10.18653/v1/p18-2124"}
{"paper_id": 259261789, "title": "Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression", "author_names": ["Allan Ravent'os", "Mansheej Paul", "Feng Chen", "S. Ganguli"], "venue": "Neural Information Processing Systems", "abstract": "Pretrained transformers exhibit the remarkable ability of in-context learning (ICL): they can learn tasks from just a few examples provided in the prompt without updating any weights. This raises a foundational question: can ICL solve fundamentally $\\textit{new}$ tasks that are very different from those seen during pretraining? To probe this question, we examine ICL's performance on linear regression while varying the diversity of tasks in the pretraining dataset. We empirically demonstrate a $\\textit{task diversity threshold}$ for the emergence of ICL. Below this threshold, the pretrained transformer cannot solve unseen regression tasks as it behaves like a Bayesian estimator with the $\\textit{non-diverse pretraining task distribution}$ as the prior. Beyond this threshold, the transformer significantly outperforms this estimator; its behavior aligns with that of ridge regression, corresponding to a Gaussian prior over $\\textit{all tasks}$, including those not seen during pretraining. These results highlight that, when pretrained on data with task diversity greater than the threshold, transformers $\\textit{can}$ solve fundamentally new tasks in-context. Importantly, this capability hinges on it deviating from the Bayes optimal estimator with the pretraining distribution as the prior. This study underscores, in a concrete example, the critical role of task diversity, alongside data and model scale, in the emergence of ICL. Code is available at https://github.com/mansheej/icl-task-diversity.", "year": 2023, "publicationdate": "2023-06-26", "externalids": {"DOI": "10.48550/arXiv.2306.15063"}, "doi_lower": "10.48550/arxiv.2306.15063"}
{"paper_id": 245218561, "title": "Learning To Retrieve Prompts for In-Context Learning", "author_names": ["Ohad Rubin", "Jonathan Herzig", "Jonathan Berant"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.", "year": 2021, "publicationdate": "2021-12-16", "externalids": {"DOI": "10.18653/v1/2022.naacl-main.191"}, "doi_lower": "10.18653/v1/2022.naacl-main.191"}
{"paper_id": 252693237, "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought", "author_names": ["Abulhair Saparov", "He He"], "venue": "International Conference on Learning Representations", "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.", "year": 2022, "publicationdate": "2022-10-03", "externalids": {"DOI": "10.48550/arXiv.2210.01240"}, "doi_lower": "10.48550/arxiv.2210.01240"}
{"paper_id": 252735112, "title": "Language Models are Multilingual Chain-of-Thought Reasoners", "author_names": ["Freda Shi", "Mirac Suzgun", "Markus Freitag", "Xuezhi Wang", "Suraj Srivats", "Soroush Vosoughi", "Hyung Won Chung", "Yi Tay", "Sebastian Ruder", "Denny Zhou", "Dipanjan Das", "Jason Wei"], "venue": "International Conference on Learning Representations", "abstract": "We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.", "year": 2022, "publicationdate": "2022-10-06", "externalids": {"DOI": "10.48550/arXiv.2210.03057"}, "doi_lower": "10.48550/arxiv.2210.03057"}
{"paper_id": 248427215, "title": "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model", "author_names": ["Seongjin Shin", "Sang-Woo Lee", "Hwijeen Ahn", "Sungdong Kim", "Hyoungseok Kim", "Boseop Kim", "Kyunghyun Cho", "Gichang Lee", "W. Park", "Jung-Woo Ha", "Nako Sung"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From our in-depth investigation, we introduce the following observations: (1) in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning, (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora, even when each corpus does not result in in-context learning on its own, (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in the few-shot setting, and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate: e.g., low perplexity does not always imply high in-context few-shot learning performance.", "year": 2022, "publicationdate": "2022-04-28", "externalids": {"DOI": "10.48550/arXiv.2204.13509"}, "doi_lower": "10.48550/arxiv.2204.13509"}
{"paper_id": 258833064, "title": "Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations", "author_names": ["Chenglei Si", "Dan Friedman", "Nitish Joshi", "Shi Feng", "Danqi Chen", "He He"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In-context learning (ICL) is an important paradigm for adapting large language models (LLMs) to new tasks, but the generalization behavior of ICL remains poorly understood. We investigate the inductive biases of ICL from the perspective of feature bias: which feature ICL is more likely to use given a set of underspecified demonstrations in which two features are equally predictive of the labels. First, we characterize the feature biases of GPT-3 models by constructing underspecified demonstrations from a range of NLP datasets and feature combinations. We find that LLMs exhibit clear feature biases—for example, demonstrating a strong bias to predict labels according to sentiment rather than shallow lexical features, like punctuation. Second, we evaluate the effect of different interventions that are designed to impose an inductive bias in favor of a particular feature, such as adding a natural language instruction or using semantically relevant label words. We find that, while many interventions can influence the learner to prefer a particular feature, it can be difficult to overcome strong prior biases. Overall, our results provide a broader picture of the types of features that ICL may be more likely to exploit and how to impose inductive biases that are better aligned with the intended task.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.48550/arXiv.2305.13299"}, "doi_lower": "10.48550/arxiv.2305.13299"}
{"paper_id": 990233, "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author_names": ["R. Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "A. Ng", "Christopher Potts"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.", "year": 2013, "publicationdate": "2013-10-01", "externalids": {"DOI": "10.18653/v1/d13-1170"}, "doi_lower": "10.18653/v1/d13-1170"}
{"paper_id": 990233, "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author_names": ["R. Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "A. Ng", "Christopher Potts"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.", "year": 2013, "publicationdate": "2013-10-01", "externalids": {"DOI": "10.18653/v1/d13-1170"}, "doi_lower": "10.18653/v1/d13-1170"}
{"paper_id": 263625818, "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models", "author_names": ["Aarohi Srivastava", "Abhinav Rastogi", "Abhishek Rao", "Abu Awal Md Shoeb", "Abubakar Abid", "Adam Fisch", "Adam R. Brown", "Adam Santoro", "Aditya Gupta", "Adrià Garriga-Alonso", "Agnieszka Kluska", "Aitor Lewkowycz", "Akshat Agarwal", "Alethea Power", "Alex Ray", "Alex Warstadt", "Alexander W. Kocurek", "Ali Safaya", "Ali Tazarv", "Alice Xiang", "Alicia Parrish", "Allen Nie", "Aman Hussain", "Amanda Askell", "A. Dsouza", "Ambrose Slone", "Ameet Rahane", "Anantharaman S. Iyer", "Anders Andreassen", "Andrea Madotto", "Andrea Santilli", "Andreas Stuhlmuller", "Andrew M. Dai", "A. La", "Andrew Kyle Lampinen", "Andy Zou", "Angela Jiang", "Angelica Chen", "Anh Vuong", "Animesh Gupta", "Anna Gottardi", "Antonio Norelli", "Anu Venkatesh", "Arash Gholamidavoodi", "A. Tabassum", "Arul Menezes", "Arun Kirubarajan", "A. Mullokandov", "Ashish Sabharwal", "Austin Herrick", "Avia Efrat", "Aykut Erdem", "Ayla Karakacs", "B. R. Roberts", "B. S. Loe", "Barret Zoph", "Bartlomiej Bojanowski", "Batuhan Ozyurt", "Behnam Hedayatnia", "Behnam Neyshabur", "Benjamin Inden", "Benno Stein", "Berk Ekmekci", "Bill Yuchen Lin", "B. Howald", "Bryan Orinion", "Cameron Diao", "Cameron Dour", "Catherine Stinson", "Cedrick Argueta", "C'esar Ferri Ram'irez", "Chandan Singh", "Charles Rathkopf", "Chenlin Meng", "Chitta Baral", "Chiyu Wu", "Chris Callison-Burch", "Chris Waites", "Christian Voigt", "Christopher D. Manning", "Christopher Potts", "Cindy Ramirez", "Clara E. Rivera", "Clemencia Siro", "Colin Raffel", "Courtney Ashcraft", "Cristina Garbacea", "Damien Sileo", "Dan Garrette", "Dan Hendrycks", "D. Kilman", "Dan Roth", "Daniel Freeman", "Daniel Khashabi", "Daniel Levy", "D. Gonz'alez", "Danielle R. Perszyk", "Danny Hernandez", "Danqi Chen", "Daphne Ippolito", "Dar Gilboa", "David Dohan", "D. Drakard", "David Jurgens", "Debajyoti Datta", "Deep Ganguli", "Denis Emelin", "Denis Kleyko", "Deniz Yuret", "Derek Chen", "Derek Tam", "Dieuwke Hupkes", "Diganta Misra", "Dilyar Buzan", "Dimitri Coelho Mollo", "Diyi Yang", "Dong-Ho Lee", "Dylan Schrader", "Ekaterina Shutova", "E. D. Cubuk", "Elad Segal", "Eleanor Hagerman", "Elizabeth Barnes", "Elizabeth Donoway", "Ellie Pavlick", "Emanuele Rodolà", "Emma Lam", "Eric Chu", "Eric Tang", "Erkut Erdem", "Ernie Chang", "Ethan A. Chi", "Ethan Dyer", "E. Jerzak", "Ethan Kim", "Eunice Engefu Manyasi", "Evgenii Zheltonozhskii", "Fanyue Xia", "Fatemeh Siar", "Fernando Mart'inez-Plumed", "Francesca Happ'e", "François Chollet", "Frieda Rong", "Gaurav Mishra", "Genta Indra Winata", "Gerard de Melo", "Germán Kruszewski", "Giambattista Parascandolo", "Giorgio Mariani", "Gloria Xinyue Wang", "Gonzalo Jaimovitch-L'opez", "Gregor Betz", "Guy Gur-Ari", "Hana Galijasevic", "Hannah Kim", "Hannah Rashkin", "Hannaneh Hajishirzi", "Harsh Mehta", "H. Bogar", "Henry Shevlin", "Hinrich Schutze", "H. Yakura", "Hongming Zhang", "Hugh Mee Wong", "Ian Ng", "Isaac Noble", "Jaap Jumelet", "Jack Geissinger", "John Kernion", "Jacob Hilton", "Jaehoon Lee", "J. Fisac", "James B. Simon", "James Koppel", "James Zheng", "James Zou", "Jan Koco'n", "Jana Thompson", "Janelle Wingfield", "Jared Kaplan", "Jarema Radom", "Jascha Narain Sohl-Dickstein", "Jason Phang", "Jason Wei", "J. Yosinski", "Jekaterina Novikova", "Jelle Bosscher", "Jennifer Marsh", "Jeremy Kim", "Jeroen Taal", "Jesse Engel", "Jesujoba Oluwadara Alabi", "Jiacheng Xu", "Jiaming Song", "Jillian Tang", "Jane W Waweru", "John Burden", "John Miller", "John U. Balis", "Jonathan Batchelder", "Jonathan Berant", "Jorg Frohberg", "Jos Rozen", "J. Hernández-Orallo", "Joseph Boudeman", "J. Guerr", "Joseph Jones", "Joshua B. Tenenbaum", "Joshua S. Rule", "Joyce Chua", "Kamil Kanclerz", "Karen Livescu", "K. Krauth", "Karthik Gopalakrishnan", "Katerina Ignatyeva", "K. Markert", "Kaustubh D. Dhole", "Kevin Gimpel", "Kevin Omondi", "K. Mathewson", "Kristen Chiafullo", "Ksenia Shkaruta", "Kumar Shridhar", "Kyle McDonell", "Kyle Richardson", "Laria Reynolds", "Leo Gao", "Li Zhang", "Liam Dugan", "Lianhui Qin", "Lidia Contreras-Ochando", "Louis-philippe Morency", "Luca Moschella", "Luca Lam", "Lucy Noble", "Ludwig Schmidt", "Luheng He", "Luis Oliveros Col'on", "Luke Metz", "Lutfi Kerem cSenel", "Maarten Bosma", "Maarten Sap", "Maartje ter Hoeve", "Maheen Farooqi", "Manaal Faruqui", "Mantas Mazeika", "Marco Baturan", "Marco Marelli", "Marco Maru", "Maria Jose Ram’irez Quintana", "M. Tolkiehn", "Mario Giulianelli", "Martha Lewis", "Martin Potthast", "Matthew L. Leavitt", "Matthias Hagen", "M. Schubert", "Medina Baitemirova", "Melody Arnaud", "M. McElrath", "Michael A. Yee", "Michael Cohen", "Michael Gu", "Michael Ivanitskiy", "Michael Starritt", "M. Strube", "Michal Swkedrowski", "Michele Bevilacqua", "Michihiro Yasunaga", "Mihir Kale", "Mike Cain", "Mimee Xu", "Mirac Suzgun", "Mitch Walker", "Monica Tiwari", "Mohit Bansal", "Moin Aminnaseri", "Mor Geva", "Mozhdeh Gheini", "T. MukundVarma", "Nanyun Peng", "Nathan A. Chi", "Nayeon Lee", "Neta Gur-Ari Krakover", "Nicholas Cameron", "Nicholas Roberts", "Nick Doiron", "Nicole Martinez", "Nikita Nangia", "Niklas Deckers", "Niklas Muennighoff", "N. Keskar", "Niveditha Iyer", "Noah Constant", "Noah Fiedel", "Nuan Wen", "Oliver Zhang", "Omar Agha", "Omar Elbaghdadi", "Omer Levy", "Owain Evans", "Pablo Antonio Moreno Casares", "P. Doshi", "Pascale Fung", "Paul Pu Liang", "Paul Vicol", "Pegah Alipoormolabashi", "Peiyuan Liao", "Percy Liang", "Peter Chang", "P. Eckersley", "Phu Mon Htut", "P. Hwang", "P. Milkowski", "P. Patil", "Pouya Pezeshkpour", "Priti Oli", "Qiaozhu Mei", "Qing Lyu", "Qinlang Chen", "Rabin Banjade", "Rachel Etta Rudolph", "Raefer Gabriel", "Rahel Habacker", "Ramon Risco", "Raphael Milliere", "Rhythm Garg", "Richard Barnes", "R. Saurous", "Riku Arakawa", "Robbe Raymaekers", "Robert Frank", "Rohan Sikand", "Roman Novak", "Roman Sitelew", "Ronan Le Bras", "Rosanne Liu", "Rowan Jacobs", "Rui Zhang", "R. Salakhutdinov", "Ryan Chi", "Ryan Lee", "Ryan Stovall", "R. Teehan", "Rylan Yang", "Sahib Singh", "Saif Mohammad", "Sajant Anand", "Sam Dillavou", "Sam Shleifer", "Sam Wiseman", "Samuel Gruetter", "Samuel R. Bowman", "S. Schoenholz", "Sanghyun Han", "Sanjeev Kwatra", "Sarah A. Rous", "Sarik Ghazarian", "Sayan Ghosh", "Sean Casey", "Sebastian Bischoff", "Sebastian Gehrmann", "Sebastian Schuster", "Sepideh Sadeghi", "Shadi S. Hamdan", "Sharon Zhou", "Shashank Srivastava", "Sherry Shi", "Shikhar Singh", "Shima Asaadi", "S. Gu", "Shubh Pachchigar", "Shubham Toshniwal", "Shyam Upadhyay", "Shyamolima Debnath", "Siamak Shakeri", "Simon Thormeyer", "S. Melzi", "Siva Reddy", "S. Makini", "Soo-Hwan Lee", "Spencer Bradley Torene", "Sriharsha Hatwar", "S. Dehaene", "Stefan Divic", "Stefano Ermon", "Stella Biderman", "Stephanie Lin", "Stephen Prasad", "Steven T Piantadosi", "Stuart M. Shieber", "Summer Misherghi", "S. Kiritchenko", "Swaroop Mishra", "Tal Linzen", "Tal Schuster", "Tao Li", "Tao Yu", "Tariq Ali", "Tatsunori Hashimoto", "Te-Lin Wu", "T. Desbordes", "Theodore Rothschild", "Thomas Phan", "Tianle Wang", "Tiberius Nkinyili", "Timo Schick", "T. Kornev", "T. Tunduny", "Tobias Gerstenberg", "T. Chang", "Trishala Neeraj", "Tushar Khot", "Tyler Shultz", "Uri Shaham", "Vedant Misra", "Vera Demberg", "Victoria Nyamai", "Vikas Raunak", "V. Ramasesh", "Vinay Uday Prabhu", "Vishakh Padmakumar", "Vivek Srikumar", "W. Fedus", "W. Saunders", "William Zhang", "Wout Vossen", "Xiang Ren", "Xiaoyu Tong", "Xinran Zhao", "Xinyi Wu", "Xudong Shen", "Yadollah Yaghoobzadeh", "Yair Lakretz", "Yangqiu Song", "Yasaman Bahri", "Yejin Choi", "Yichi Yang", "Yiding Hao", "Yifu Chen", "Yonatan Belinkov", "Yu Hou", "Yu Hou", "Yuntao Bai", "Zachary Seid", "Zhuoye Zhao", "Zijian Wang", "Zijie J. Wang", "Zirui Wang", "Ziyi Wu"], "venue": "arXiv.org", "abstract": "Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit\"breakthrough\"behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.", "year": 2022, "publicationdate": "2022-06-09", "externalids": {}, "doi_lower": null}
{"paper_id": 252089424, "title": "Selective Annotation Makes Language Models Better Few-Shot Learners", "author_names": ["Hongjin Su", "Jungo Kasai", "Chen Henry Wu", "Weijia Shi", "Tianlu Wang", "Jiayi Xin", "Rui Zhang", "Mari Ostendorf", "Luke Zettlemoyer", "Noah A. Smith", "Tao Yu"], "venue": "International Conference on Learning Representations", "abstract": "Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9%/11.4% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks. Our code is available at https://github.com/HKUNLP/icl-selective-annotation.", "year": 2022, "publicationdate": "2022-09-05", "externalids": {"DOI": "10.48550/arXiv.2209.01975"}, "doi_lower": "10.48550/arxiv.2209.01975"}
{"paper_id": 258048653, "title": "Exploring Effective Factors for Improving Visual In-Context Learning", "author_names": ["Yanpeng Sun", "Qiang Chen", "Jian Wang", "Jingdong Wang", "Zechao Li"], "venue": "IEEE Transactions on Image Processing", "abstract": "The In-Context Learning (ICL) is to understand a new task via a few demonstrations (aka. prompt) and predict new inputs without tuning the models. While it has been widely studied in NLP, it is still a relatively new area of research in computer vision. To reveal the factors influencing the performance of visual in-context learning, this paper shows that Prompt Selection and Prompt Fusion are two major factors that have a direct impact on the inference performance of visual in-context learning. Prompt selection is the process of selecting the most suitable prompt for query image. This is crucial because high-quality prompts assist large-scale visual models in rapidly and accurately comprehending new tasks. Prompt fusion involves combining prompts and query images to activate knowledge within large-scale visual models. However, altering the prompt fusion method significantly impacts its performance on new tasks. Based on these findings, we propose a simple framework prompt-SelF to improve visual in-context learning. Specifically, we first use the pixel-level retrieval method to select a suitable prompt, and then use different prompt fusion methods to activate diverse knowledge stored in the large-scale vision model, and finally, ensemble the prediction results obtained from different prompt fusion methods to obtain the final prediction results. We conducted extensive experiments on single-object segmentation and detection tasks to demonstrate the effectiveness of prompt-SelF. Remarkably, prompt-SelF has outperformed OSLSM method-based meta-learning in 1-shot segmentation for the first time. This indicated the great potential of visual in-context learning. The source code and models will be available at https://github.com/syp2ysy/prompt-SelF.", "year": 2023, "publicationdate": "2023-04-10", "externalids": {"DOI": "10.1109/TIP.2025.3554410"}, "doi_lower": "10.1109/tip.2025.3554410"}
{"paper_id": 252917648, "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them", "author_names": ["Mirac Suzgun", "Nathan Scales", "Nathanael Scharli", "Sebastian Gehrmann", "Yi Tay", "Hyung Won Chung", "A. Chowdhery", "Quoc V. Le", "Ed H. Chi", "Denny Zhou", "Jason Wei"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.", "year": 2022, "publicationdate": "2022-10-17", "externalids": {"DOI": "10.48550/arXiv.2210.09261"}, "doi_lower": "10.48550/arxiv.2210.09261"}
{"paper_id": 53296520, "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge", "author_names": ["Alon Talmor", "Jonathan Herzig", "Nicholas Lourie", "Jonathan Berant"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/N19-1421"}, "doi_lower": "10.18653/v1/n19-1421"}
{"paper_id": 265607950, "title": "In-context Learning of Large Language Models for Controlled Dialogue Summarization: A Holistic Benchmark and Empirical Analysis", "author_names": ["Yuting Tang", "Ratish Puduppully", "Zhengyuan Liu", "Nancy Chen"], "venue": "NEWSUM", "abstract": "Large Language Models (LLMs) have shown significant performance in numerous NLP tasks, including summarization and controlled text generation. A notable capability of LLMs is in-context learning (ICL), where the model learns new tasks using input-output pairs in the prompt without any parameter update. However, the performance of LLMs in the context of few-shot abstractive dialogue summarization remains underexplored. This study evaluates various state-of-the-art LLMs on the SAMSum dataset within a few-shot framework. We assess these models in both controlled (entity control, length control, and person-focused planning) and uncontrolled settings, establishing a comprehensive benchmark in few-shot dialogue summarization. Our findings provide insights into summary quality and model controllability, offering a crucial reference for future research in dialogue summarization.", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.newsum-1.6"}, "doi_lower": "10.18653/v1/2023.newsum-1.6"}
{"paper_id": 258588286, "title": "Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment", "author_names": ["Eshaan Tanwar", "Manish Borthakur", "Subhabrata Dutta", "Tanmoy Chakraborty"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy — Cross-lingual In-context Source Target Alignment (X-InSTA). With an injected coherence in the semantics of the input examples and a task-based alignment across the source and target languages, X-InSTA is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross-lingual pairs.", "year": 2023, "publicationdate": "2023-05-10", "externalids": {"DOI": "10.48550/arXiv.2305.05940"}, "doi_lower": "10.48550/arxiv.2305.05940"}
{"paper_id": 257219404, "title": "LLaMA: Open and Efficient Foundation Language Models", "author_names": ["Hugo Touvron", "Thibaut Lavril", "Gautier Izacard", "Xavier Martinet", "M. Lachaux", "Timothée Lacroix", "Baptiste Rozière", "Naman Goyal", "Eric Hambro", "Faisal Azhar", "Aur'elien Rodriguez", "Armand Joulin", "Edouard Grave", "Guillaume Lample"], "venue": "arXiv.org", "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.", "year": 2023, "publicationdate": "2023-02-27", "externalids": {}, "doi_lower": null}
{"paper_id": 259950998, "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "author_names": ["Hugo Touvron", "Louis Martin", "Kevin R. Stone", "Peter Albert", "Amjad Almahairi", "Yasmine Babaei", "Niko-lay Bashlykov", "Soumya Batra", "Prajjwal Bhargava", "Shruti Bhosale", "D. Bikel", "Lukas Blecher", "Cris-tian Cantón Ferrer", "Moya Chen", "Guillem Cucurull", "David Esiobu", "Jude Fernandes", "J. Fu", "Wenyin Fu", "Brian Fuller", "Cynthia Gao", "Vedanuj Goswami", "Naman Goyal", "A. Hartshorn", "Saghar Hosseini", "Rui Hou", "Hakan Inan", "Marcin Kardas", "Viktor Kerkez", "Madian Khabsa", "Isabel M. Kloumann", "A. Korenev", "Punit Singh Koura", "M. Lachaux", "Thibaut Lavril", "Jenya Lee", "Diana Liskovich", "Yinghai Lu", "Yuning Mao", "Xavier Martinet", "Todor Mihaylov", "Pushkar Mishra", "Igor Molybog", "Yixin Nie", "Andrew Poulton", "J. Reizenstein", "Rashi Rungta", "Kalyan Saladi", "A. Schelten", "Ruan Silva", "Eric Michael Smith", "R. Subramanian", "Xia Tan", "Binh Tang", "Ross Taylor", "Adina Williams", "Jian Xiang Kuan", "Puxin Xu", "Zhengxu Yan", "Iliyan Zarov", "Yuchen Zhang", "Angela Fan", "M. Kambadur", "Sharan Narang", "Aur'elien Rodriguez", "Robert Stojnic", "Sergey Edunov", "Thomas Scialom"], "venue": "arXiv.org", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "year": 2023, "publicationdate": "2023-07-18", "externalids": {}, "doi_lower": null}
{"paper_id": 235658331, "title": "Multimodal Few-Shot Learning with Frozen Language Models", "author_names": ["M. Tsimpoukelli", "Jacob Menick", "Serkan Cabi", "S. Eslami", "O. Vinyals", "Felix Hill", "Zacharias Janssen"], "venue": "Neural Information Processing Systems", "abstract": "When trained at sufficient scale, auto-regressive language models exhibit the notable ability to learn a new language task after being prompted with just a few examples. Here, we present a simple, yet effective, approach for transferring this few-shot learning ability to a multimodal setting (vision and language). Using aligned image and caption data, we train a vision encoder to represent each image as a sequence of continuous embeddings, such that a pre-trained, frozen language model prompted with this prefix generates the appropriate caption. The resulting system is a multimodal few-shot learner, with the surprising ability to learn a variety of new tasks when conditioned on examples, represented as a sequence of multiple interleaved image and text embeddings. We demonstrate that it can rapidly learn words for new objects and novel visual categories, do visual question-answering with only a handful of examples, and make use of outside knowledge, by measuring a single model on a variety of established and new benchmarks.", "year": 2021, "publicationdate": "2021-06-25", "externalids": {}, "doi_lower": null}
{"paper_id": 249889477, "title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change", "author_names": ["Karthik Valmeekam", "Alberto Olmo", "S. Sreedharan", "Subbarao Kambhampati"], "venue": "Neural Information Processing Systems", "abstract": "Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.", "year": 2022, "publicationdate": "2022-06-21", "externalids": {}, "doi_lower": null}
{"paper_id": 254685643, "title": "Transformers learn in-context by gradient descent", "author_names": ["J. Oswald", "Eyvind Niklasson", "E. Randazzo", "J. Sacramento", "A. Mordvintsev", "A. Zhmoginov", "Max Vladymyrov"], "venue": "International Conference on Machine Learning", "abstract": "At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .", "year": 2022, "publicationdate": "2022-12-15", "externalids": {"DOI": "10.48550/arXiv.2212.07677"}, "doi_lower": "10.48550/arxiv.2212.07677"}
{"paper_id": 143424870, "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "author_names": ["Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "venue": "Neural Information Processing Systems", "abstract": "In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at this http URL.", "year": 2019, "publicationdate": "2019-05-02", "externalids": {}, "doi_lower": null}
{"paper_id": 258832435, "title": "GPT-SW3: An Autoregressive Language Model for the Nordic Languages", "author_names": ["Ariel Ekgren", "Amaru Cuba Gyllensten", "F. Stollenwerk", "Joey Öhman", "T. Isbister", "Evangelia Gogoulou", "F. Carlsson", "Alice Heiman", "Judit Casademont", "Magnus Sahlgren"], "venue": "arXiv.org", "abstract": "This paper details the process of developing the first native large generative language model for the Nordic languages, GPT-SW3. We cover all parts of the development process, from data collection and processing, training configuration and instruction finetuning, to evaluation and considerations for release strategies. We hope that this paper can serve as a guide and reference for other researchers that undertake the development of large generative models for smaller languages.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.48550/arXiv.2305.12987"}, "doi_lower": "10.48550/arxiv.2305.12987"}
{"paper_id": 253098851, "title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought", "author_names": ["Boshi Wang", "Xiang Deng", "Huan Sun"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "While Pre-trained Language Models (PLMs) internalize a great amount of world knowledge, they have been shown incapable of recalling these knowledge to solve tasks requiring complex & multi-step reasoning. Similar to how humans develop a “chain of thought” for these tasks, how can we equip PLMs with such abilities? In this work, we explore an iterative prompting framework, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference. We identify key limitations of existing prompting methods, namely they are either restricted to queries with a single identifiable relation/predicate, or being agnostic to input contexts, which makes it difficult to capture variabilities across different inference steps. We propose an iterative context-aware prompter, which addresses these limitations by learning to dynamically synthesize prompts conditioned on the current step’s contexts. Experiments on three datasets involving multi-step reasoning show the effectiveness of the iterative scheme and the context-aware prompter design.", "year": 2022, "publicationdate": "2022-03-16", "externalids": {"DOI": "10.18653/v1/2022.emnlp-main.174"}, "doi_lower": "10.18653/v1/2022.emnlp-main.174"}
{"paper_id": 255440307, "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers", "author_names": ["Chengyi Wang", "Sanyuan Chen", "Yu Wu", "Zi-Hua Zhang", "Long Zhou", "Shujie Liu", "Zhuo Chen", "Yanqing Liu", "Huaming Wang", "Jinyu Li", "Lei He", "Sheng Zhao", "Furu Wei"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 50 k hours of English speech which is hundreds of times larger than existing systems. VALL-E emerges in-context learning capability and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. Experiment results show that VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find VALL-E could preserve the speaker's emotion and acoustic environment from the prompt in synthesis.", "year": 2023, "publicationdate": "2023-01-05", "externalids": {"DOI": "10.1109/TASLPRO.2025.3530270"}, "doi_lower": "10.1109/taslpro.2025.3530270"}
{"paper_id": 237363383, "title": "Want To Reduce Labeling Cost? GPT-3 Can Help", "author_names": ["Shuohang Wang", "Yang Liu", "Yichong Xu", "Chenguang Zhu", "Michael Zeng"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-specific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 175 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We find that, to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance with limited labeling budget. These results present a cost-effective data labeling methodology that is generalizable to many practical applications.", "year": 2021, "publicationdate": "2021-08-30", "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.354"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.354"}
{"paper_id": 254246343, "title": "Images Speak in Images: A Generalist Painter for In-Context Visual Learning", "author_names": ["Xinlong Wang", "Wen Wang", "Yue Cao", "Chunhua Shen", "Tiejun Huang"], "venue": "Computer Vision and Pattern Recognition", "abstract": "In-context learning, as a new paradigm in NLP, allows the model to rapidly adapt to various tasks with only a handful of prompts and examples. But in computer vision, the difficulties for in-context learning lie in that tasks vary significantly in the output representations, thus it is unclear how to define the general-purpose task prompts that the vision model can understand and transfer to out-of-domain tasks. In this work, we present Painter, a generalist model which addresses these obstacles with an “image”-centric solution, that is, to redefine the output of core vision tasks as images, and specify task prompts as also images. With this idea, our training process is extremely simple, which performs standard masked image modeling on the stitch of input and output image pairs. This makes the model capable of performing tasks conditioned on visible image patches. Thus, during inference, we can adopt a pair of input and output images from the same task as the input condition, to indicate which task to perform. Without bells and whistles, our generalist Painter can achieve competitive performance compared to well-established task-specific models, on seven representative vision tasks ranging from high-level visual understanding to low-level image processing. In addition, Painter significantly outperforms recent generalist models on several challenging tasks.", "year": 2022, "publicationdate": "2022-12-05", "externalids": {"DOI": "10.1109/CVPR52729.2023.00660"}, "doi_lower": "10.1109/cvpr52729.2023.00660"}
{"paper_id": 267024015, "title": "SegGPT: Towards Segmenting Everything In Context", "author_names": ["Xinlong Wang", "Xiaosong Zhang", "Yue Cao", "Wen Wang", "Chunhua Shen", "Tiejun Huang"], "venue": "IEEE International Conference on Computer Vision", "abstract": "We present SegGPT, a generalist model for segmenting everything in context. We unify various segmentation tasks into a generalist in-context learning framework that accommodates different kinds of segmentation data by transforming them into the same format of images. The training of SegGPT is formulated as an in-context coloring problem with random color mapping for each data sample. The objective is to accomplish diverse tasks according to the context, rather than relying on specific colors. After training, SegGPT can perform arbitrary segmentation tasks in images or videos via in-context inference, such as object instance, stuff, part, contour, and text. SegGPT is evaluated on a broad range of tasks, including few-shot semantic segmentation, video object segmentation, semantic segmentation, and panoptic segmentation. Our results show strong capabilities in segmenting in-domain and out-of-domain targets, either qualitatively or quantitatively.", "year": 2023, "publicationdate": "2023-10-01", "externalids": {"DOI": "10.1109/ICCV51070.2023.00110"}, "doi_lower": "10.1109/iccv51070.2023.00110"}
{"paper_id": 270973586, "title": "Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning", "author_names": ["Xinyi Wang", "Wanrong Zhu", "William Yang Wang"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2301.11916"}, "doi_lower": "10.48550/arxiv.2301.11916"}
{"paper_id": 131773911, "title": "Few-shot Learning: A Survey", "author_names": ["Yaqing Wang", "Quanming Yao"], "venue": "arXiv.org", "abstract": null, "year": 2019, "publicationdate": "2019-04-10", "externalids": {}, "doi_lower": null}
{"paper_id": 254877310, "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "author_names": ["Yizhong Wang", "Yeganeh Kordi", "Swaroop Mishra", "Alisa Liu", "Noah A. Smith", "Daniel Khashabi", "Hannaneh Hajishirzi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10560"}, "doi_lower": "10.48550/arxiv.2212.10560"}
{"paper_id": 253098274, "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks", "author_names": ["Yizhong Wang", "Swaroop Mishra", "Pegah Alipoormolabashi", "Yeganeh Kordi", "Amirreza Mirzaei", "Anjana Arunkumar", "Arjun Ashok", "Arut Selvan Dhanasekaran", "Atharva Naik", "David Stap", "Eshaan Pathak", "Giannis Karamanolakis", "H. Lai", "I. Purohit", "Ishani Mondal", "Jacob Anderson", "Kirby Kuznia", "Krima Doshi", "Maitreya Patel", "Kuntal Kumar Pal", "M. Moradshahi", "Mihir Parmar", "Mirali Purohit", "Neeraj Varshney", "Phani Rohitha Kaza", "Pulkit Verma", "Ravsehaj Singh Puri", "Rushang Karia", "Shailaja Keyur Sampat", "Savan Doshi", "Siddhartha Mishra", "Sujan Reddy", "Sumanta Patro", "Tanay Dixit", "Xudong Shen", "Chitta Baral", "Yejin Choi", "Noah A. Smith", "Hannaneh Hajishirzi", "Daniel Khashabi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions—training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.", "year": 2022, "publicationdate": "2022-04-16", "externalids": {"DOI": "10.18653/v1/2022.emnlp-main.340"}, "doi_lower": "10.18653/v1/2022.emnlp-main.340"}
{"paper_id": 258437037, "title": "In-Context Learning Unlocked for Diffusion Models", "author_names": ["Zhendong Wang", "Yifan Jiang", "Yadong Lu", "Yelong Shen", "Pengcheng He", "Weizhu Chen", "Zhangyang Wang", "Mingyuan Zhou"], "venue": "Neural Information Processing Systems", "abstract": "We present Prompt Diffusion, a framework for enabling in-context learning in diffusion-based generative models. Given a pair of task-specific example images, such as depth from/to image and scribble from/to image, and a text guidance, our model automatically understands the underlying task and performs the same task on a new query image following the text guidance. To achieve this, we propose a vision-language prompt that can model a wide range of vision-language tasks and a diffusion model that takes it as input. The diffusion model is trained jointly over six different tasks using these prompts. The resulting Prompt Diffusion model is the first diffusion-based vision-language foundation model capable of in-context learning. It demonstrates high-quality in-context generation on the trained tasks and generalizes effectively to new, unseen vision tasks with their respective prompts. Our model also shows compelling text-guided image editing results. Our framework aims to facilitate research into in-context learning for computer vision. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Prompt-Diffusion.", "year": 2023, "publicationdate": "2023-05-01", "externalids": {"DOI": "10.48550/arXiv.2305.01115"}, "doi_lower": "10.48550/arxiv.2305.01115"}
{"paper_id": 237416585, "title": "Finetuned Language Models Are Zero-Shot Learners", "author_names": ["Jason Wei", "Maarten Bosma", "Vincent Zhao", "Kelvin Guu", "Adams Wei Yu", "Brian Lester", "Nan Du", "Andrew M. Dai", "Quoc V. Le"], "venue": "International Conference on Learning Representations", "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.", "year": 2021, "publicationdate": "2021-09-03", "externalids": {}, "doi_lower": null}
{"paper_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "author_names": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "Ed H. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "venue": "Neural Information Processing Systems", "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 257378479, "title": "Larger language models do in-context learning differently", "author_names": ["Jerry W. Wei", "Jason Wei", "Yi Tay", "Dustin Tran", "Albert Webson", "Yifeng Lu", "Xinyun Chen", "Hanxiao Liu", "Da Huang", "Denny Zhou", "Tengyu Ma"], "venue": "arXiv.org", "abstract": "We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former.", "year": 2023, "publicationdate": "2023-03-07", "externalids": {"DOI": "10.48550/arXiv.2303.03846"}, "doi_lower": "10.48550/arxiv.2303.03846"}
{"paper_id": 257505009, "title": "The Learnability of In-Context Learning", "author_names": ["Noam Wies", "Yoav Levine", "A. Shashua"], "venue": "Neural Information Processing Systems", "abstract": "In-context learning is a surprising and important phenomenon that emerged when modern language models were scaled to billions of learned parameters. Without modifying a large language model's weights, it can be tuned to perform various downstream natural language tasks simply by including concatenated training examples of these tasks in its input. Though disruptive for many practical applications of large language models, this emergent learning paradigm is not well understood from a theoretical perspective. In this paper, we propose a first-of-its-kind PAC based framework for in-context learnability, and use it to provide the first finite sample complexity results for the in-context learning setup. Our framework includes an initial pretraining phase, which fits a function to the pretraining distribution, and then a second in-context learning phase, which keeps this function constant and concatenates training examples of the downstream task in its input. We use our framework in order to prove that, under mild assumptions, when the pretraining distribution is a mixture of latent tasks (a model often considered for natural language pretraining), these tasks can be efficiently learned via in-context learning, even though the model's weights are unchanged and the input significantly diverges from the pretraining distribution. Our theoretical analysis reveals that in this setting, in-context learning is more about identifying the task than about learning it, a result which is in line with a series of recent empirical findings. We hope that the in-context learnability framework presented in this paper will facilitate future progress towards a deeper understanding of this important new learning paradigm.", "year": 2023, "publicationdate": "2023-03-14", "externalids": {"DOI": "10.48550/arXiv.2303.07895"}, "doi_lower": "10.48550/arxiv.2303.07895"}
{"paper_id": 257365525, "title": "OpenICL: An Open-Source Framework for In-context Learning", "author_names": ["Zhenyu Wu", "Yaoxiang Wang", "Jiacheng Ye", "Jiangtao Feng", "Jingjing Xu", "Yu Qiao", "Zhiyong Wu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In recent years, In-context Learning (ICL) has gained increasing attentionand emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates.However, the implementation of ICL is sophisticated due to the diverse retrieval and inference methods involved, as well as the varying pre-processing requirements for different models, datasets, and tasks. A unified and flexible framework for ICL is urgently needed to ease the implementation of the aforementioned components.To facilitate ICL research, we introduce OpenICL, an open-source toolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly flexible architecture that users can easily combine different components to suit their needs.It also provides various state-of-the-art retrieval and inference methods to streamline the process of adapting ICL to cutting-edge research.The effectiveness of OpenICL has been validated on a wide range of NLP tasks, including classification, QA, machine translation, and semantic parsing. As a side-product, we found OpenICL to be an efficient yet robust tool for LLMs evaluation. OpenICL is released at https://github.com/Shark-NLP/OpenICL.", "year": 2023, "publicationdate": "2023-03-06", "externalids": {"DOI": "10.48550/arXiv.2303.02913"}, "doi_lower": "10.48550/arxiv.2303.02913"}
{"paper_id": 254877590, "title": "Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering", "author_names": ["Zhiyong Wu", "Yaoxiang Wang", "Jiacheng Ye", "Lingpeng Kong"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example organization (i.e., selection and permutation) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code will be released to facilitate future research.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10375"}, "doi_lower": "10.48550/arxiv.2212.10375"}
{"paper_id": 241035330, "title": "An Explanation of In-context Learning as Implicit Bayesian Inference", "author_names": ["Sang Michael Xie", "Aditi Raghunathan", "Percy Liang", "Tengyu Ma"], "venue": "International Conference on Learning Representations", "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.", "year": 2021, "publicationdate": "2021-11-03", "externalids": {}, "doi_lower": null}
{"paper_id": 267069067, "title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey", "author_names": ["an Luo", "Xin Xu", "Yue Liu", "Panupong Pasupat", "Mehran Kazemi"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training procedures, and inference algorithms.", "year": 2024, "publicationdate": "2024-01-21", "externalids": {"DOI": "10.48550/arXiv.2401.11624"}, "doi_lower": "10.48550/arxiv.2401.11624"}
{"paper_id": 254926784, "title": "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning", "author_names": ["Zhiyang Xu", "Ying Shen", "Lifu Huang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric – Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task.", "year": 2022, "publicationdate": "2022-12-21", "externalids": {"DOI": "10.48550/arXiv.2212.10773"}, "doi_lower": "10.48550/arxiv.2212.10773"}
{"paper_id": 264935329, "title": "Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models", "author_names": ["Steve Yadlowsky", "Lyric Doshi", "Nilesh Tripuraneni"], "venue": "arXiv.org", "abstract": "Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of $(x, f(x))$ pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks. Together our results highlight that the impressive ICL abilities of high-capacity sequence models may be more closely tied to the coverage of their pretraining data mixtures than inductive biases that create fundamental generalization capabilities.", "year": 2023, "publicationdate": "2023-11-01", "externalids": {"DOI": "10.48550/arXiv.2311.00871"}, "doi_lower": "10.48550/arxiv.2311.00871"}
{"paper_id": 265221442, "title": "Auto-ICL: In-Context Learning without Human Supervision", "author_names": ["Jinghan Yang", "Shuming Ma", "Furu Wei"], "venue": "arXiv.org", "abstract": "With in-context learning ability, the performance of large language models can be significantly boosted when provided with appropriate context. However, existing in-context learning methods mainly rely on human-provided contexts, such as labeled examples and explicit instructions. Writing context by humans is labor-intensive on various tasks and limits the model to tasks manageable by humans. To overcome these limitations, we propose Automatic In-Context Learning framework that enables the model to autonomously generate examples and instructions for problem-solving. With experiments across various models and datasets, results show that model-generated contexts outperform human-annotated contexts, including Few-Shot and Few-Shot-CoT methods, and surpass existing self-generated context methods like Zero-CoT and Auto-CoT.", "year": 2023, "publicationdate": "2023-11-15", "externalids": {"DOI": "10.48550/arXiv.2311.09263"}, "doi_lower": "10.48550/arxiv.2311.09263"}
{"paper_id": 263909494, "title": "Not All Demonstration Examples are Equally Beneficial: Reweighting Demonstration Examples for In-Context Learning", "author_names": ["Zhe Yang", "Damai Dai", "Peiyi Wang", "Zhifang Sui"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large Language Models (LLMs) have recently gained the In-Context Learning (ICL) ability with the models scaling up, allowing them to quickly adapt to downstream tasks with only a few demonstration examples prepended in the input sequence. Nonetheless, the current practice of ICL treats all demonstration examples equally, which still warrants improvement, as the quality of examples is usually uneven. In this paper, we investigate how to determine approximately optimal weights for demonstration examples and how to apply them during ICL. To assess the quality of weights in the absence of additional validation data, we design a masked self-prediction (MSP) score that exhibits a strong correlation with the final ICL performance. To expedite the weight-searching process, we discretize the continuous weight space and adopt beam search. With approximately optimal weights obtained, we further propose two strategies to apply them to demonstrations at different model positions. Experimental results on 8 text classification tasks show that our approach outperforms conventional ICL by a large margin. Our code are publicly available at https:github.com/Zhe-Young/WICL.", "year": 2023, "publicationdate": "2023-10-12", "externalids": {"DOI": "10.48550/arXiv.2310.08309"}, "doi_lower": "10.48550/arxiv.2310.08309"}
{"paper_id": 256826793, "title": "Compositional Exemplars for In-context Learning", "author_names": ["Jiacheng Ye", "Zhiyong Wu", "Jiangtao Feng", "Tao Yu", "Lingpeng Kong"], "venue": "International Conference on Machine Learning", "abstract": "Large pretrained language models (LMs) have shown impressive In-Context Learning (ICL) ability, where the model learns to do an unseen task via a prompt consisting of input-output examples as the demonstration, without any parameter updates. The performance of ICL is highly dominated by the quality of the selected in-context examples. However, previous selection methods are mostly based on simple heuristics, leading to sub-optimal performance. In this work, we formulate in-context example selection as a subset selection problem. We propose CEIL (Compositional Exemplars for In-context Learning), which is instantiated by Determinantal Point Processes (DPPs) to model the interaction between the given input and in-context examples, and optimized through a carefully-designed contrastive learning objective to obtain preference from LMs. We validate CEIL on 12 classification and generation datasets from 7 distinct NLP tasks, including sentiment analysis, paraphrase detection, natural language inference, commonsense reasoning, open-domain question answering, code generation, and semantic parsing. Extensive experiments demonstrate not only the state-of-the-art performance but also the transferability and compositionality of CEIL, shedding new light on effective and efficient in-context learning. Our code is released at https://github.com/HKUNLP/icl-ceil.", "year": 2023, "publicationdate": "2023-02-11", "externalids": {"DOI": "10.48550/arXiv.2302.05698"}, "doi_lower": "10.48550/arxiv.2302.05698"}
{"paper_id": 249062718, "title": "Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations", "author_names": ["Junyeob Kim", "Hyuhng Joon Kim", "Hyunsoo Cho", "Hwiyeol Jo", "Sang-Woo Lee", "Sang-goo Lee", "Kang Min Yoo", "Taeuk Kim"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Despite recent explosion of interests in in-context learning, the underlying mechanism and the precise impact of the quality of demonstrations remain elusive.Intuitively, ground-truth labels should have as much impact in in-context learning (ICL) as supervised learning, but recent work reported that the input-label correspondence is significantly less important than previously thought.Intrigued by this counter-intuitive observation, we re-examine the importance of ground-truth labels in in-context learning.With the introduction of two novel metrics, namely Label-Correctness Sensitivity and Ground-truth Label Effect Ratio (GLER), we were able to conduct quantifiable analysis on the impact of ground-truth label demonstrations.Through extensive analyses, we find that the correct input-label mappings can have varying impacts on the downstream in-context learning performances, depending on the experimental configuration.Through additional studies, we identify key components, such as the verbosity of prompt templates and the language model size, as the controlling factor to achieve more noise-resilient ICL.", "year": 2022, "publicationdate": "2022-05-25", "externalids": {"DOI": "10.48550/arXiv.2205.12685"}, "doi_lower": "10.48550/arxiv.2205.12685"}
{"paper_id": 368182, "title": "Character-level Convolutional Networks for Text Classification", "author_names": ["Xiang Zhang", "J. Zhao", "Yann LeCun"], "venue": "Neural Information Processing Systems", "abstract": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.", "year": 2015, "publicationdate": "2015-09-04", "externalids": {}, "doi_lower": null}
{"paper_id": 253420743, "title": "Active Example Selection for In-Context Learning", "author_names": ["Yiming Zhang", "Shi Feng", "Chenhao Tan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "With a handful of demonstration examples, large-scale language models demonstrate strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a 5.8% improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models.", "year": 2022, "publicationdate": "2022-11-08", "externalids": {"DOI": "10.48550/arXiv.2211.04486"}, "doi_lower": "10.48550/arxiv.2211.04486"}
{"paper_id": 256416477, "title": "What Makes Good Examples for Visual In-Context Learning?", "author_names": ["Yuanhan Zhang", "Kaiyang Zhou", "Ziwei Liu"], "venue": "Neural Information Processing Systems", "abstract": "Large-scale models trained on broad data have recently become the mainstream architecture in computer vision due to their strong generalization performance. In this paper, the main focus is on an emergent ability in large vision models, known as in-context learning, which allows inference on unseen tasks by conditioning on in-context examples (a.k.a.~prompt) without updating the model parameters. This concept has been well-known in natural language processing but has only been studied very recently for large vision models. We for the first time provide a comprehensive investigation on the impact of in-context examples in computer vision, and find that the performance is highly sensitive to the choice of in-context examples. To overcome the problem, we propose a prompt retrieval framework to automate the selection of in-context examples. Specifically, we present (1) an unsupervised prompt retrieval method based on nearest example search using an off-the-shelf model, and (2) a supervised prompt retrieval method, which trains a neural network to choose examples that directly maximize in-context learning performance. The results demonstrate that our methods can bring non-trivial improvements to visual in-context learning in comparison to the commonly-used random selection.", "year": 2023, "publicationdate": "2023-01-31", "externalids": {"DOI": "10.48550/arXiv.2301.13670"}, "doi_lower": "10.48550/arxiv.2301.13670"}
{"paper_id": 258987402, "title": "What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization", "author_names": ["Yufeng Zhang", "Fengzhuo Zhang", "Zhuoran Yang", "Zhaoran Wang"], "venue": "International Conference on Artificial Intelligence and Statistics", "abstract": "In this paper, we conduct a comprehensive study of In-Context Learning (ICL) by addressing several open questions: (a) What type of ICL estimator is learned by large language models? (b) What is a proper performance metric for ICL and what is the error rate? (c) How does the transformer architecture enable ICL? To answer these questions, we adopt a Bayesian view and formulate ICL as a problem of predicting the response corresponding to the current covariate, given a number of examples drawn from a latent variable model. To answer (a), we show that, without updating the neural network parameters, ICL implicitly implements the Bayesian model averaging algorithm, which is proven to be approximately parameterized by the attention mechanism. For (b), we analyze the ICL performance from an online learning perspective and establish a $\\mathcal{O}(1/T)$ regret bound for perfectly pretrained ICL, where $T$ is the number of examples in the prompt. To answer (c), we show that, in addition to encoding Bayesian model averaging via attention, the transformer architecture also enables a fine-grained statistical analysis of pretraining under realistic assumptions. In particular, we prove that the error of pretrained model is bounded by a sum of an approximation error and a generalization error, where the former decays to zero exponentially as the depth grows, and the latter decays to zero sublinearly with the number of tokens in the pretraining dataset. Our results provide a unified understanding of the transformer and its ICL ability with bounds on ICL regret, approximation, and generalization, which deepens our knowledge of these essential aspects of modern language models.", "year": 2023, "publicationdate": "2023-05-30", "externalids": {"DOI": "10.48550/arXiv.2305.19420"}, "doi_lower": "10.48550/arxiv.2305.19420"}
{"paper_id": 252762275, "title": "Automatic Chain of Thought Prompting in Large Language Models", "author_names": ["Zhuosheng Zhang", "Aston Zhang", "Mu Li", "Alexander J. Smola"], "venue": "International Conference on Learning Representations", "abstract": "Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like\"Let's think step by step\"to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the\"Let's think step by step\"prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot", "year": 2022, "publicationdate": "2022-10-07", "externalids": {}, "doi_lower": null}
{"paper_id": 257378493, "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling", "author_names": ["Zi-Hua Zhang", "Long Zhou", "Chengyi Wang", "Sanyuan Chen", "Yu Wu", "Shujie Liu", "Zhuo Chen", "Yanqing Liu", "Huaming Wang", "Jinyu Li", "Lei He", "Sheng Zhao", "Furu Wei"], "venue": "arXiv.org", "abstract": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.", "year": 2023, "publicationdate": "2023-03-07", "externalids": {"DOI": "10.48550/arXiv.2303.03926"}, "doi_lower": "10.48550/arxiv.2303.03926"}
{"paper_id": 231979430, "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "author_names": ["Tony Zhao", "Eric Wallace", "Shi Feng", "D. Klein", "Sameer Singh"], "venue": "International Conference on Machine Learning", "abstract": "GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.", "year": 2021, "publicationdate": "2021-02-19", "externalids": {}, "doi_lower": null}
{"paper_id": 253553151, "title": "Teaching Algorithmic Reasoning via In-context Learning", "author_names": ["Hattie Zhou", "Azade Nova", "H. Larochelle", "Aaron C. Courville", "Behnam Neyshabur", "Hanie Sedghi"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have shown increasing in-context learning capabilities through scaling up model and data size. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems. While providing a rationale with the final answer has led to further improvements in multi-step reasoning problems, Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved. In this work, we identify and study four key stages for successfully teaching algorithmic reasoning to LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously (skill accumulation), (3) teaching how to combine skills (skill composition) and (4) teaching how to use skills as tools. We show that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which we refer to as algorithmic prompting. We evaluate our approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrate significant boosts in performance over existing prompting techniques. In particular, for long parity, addition, multiplication and subtraction, we achieve an error reduction of approximately 10x, 9x, 5x and 2x respectively compared to the best available baselines.", "year": 2022, "publicationdate": "2022-11-15", "externalids": {"DOI": "10.48550/arXiv.2211.09066"}, "doi_lower": "10.48550/arxiv.2211.09066"}
{"paper_id": 253265328, "title": "Large Language Models Are Human-Level Prompt Engineers", "author_names": ["Yongchao Zhou", "Andrei Ioan Muresanu", "Ziwen Han", "Keiran Paster", "Silviu Pitis", "Harris Chan", "Jimmy Ba"], "venue": "International Conference on Learning Representations", "abstract": "By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the\"program,\"optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.", "year": 2022, "publicationdate": "2022-11-03", "externalids": {"DOI": "10.48550/arXiv.2211.01910"}, "doi_lower": "10.48550/arxiv.2211.01910"}
{"paper_id": 264832783, "title": "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "author_names": ["Yuxiang Zhou", "Jiazheng Li", "Yanzheng Xiang", "Hanqi Yan", "Lin Gui", "Yulan He"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Understanding in-context learning (ICL) capability that enables large language models (LLMs) to excel in proficiency through demonstration examples is of utmost importance. This importance stems not only from the better utilization of this capability across various tasks, but also from the proactive identification and mitigation of potential risks, including concerns regarding truthfulness, bias, and toxicity, that may arise alongside the capability. In this paper, we present a thorough survey on the interpretation and analysis of in-context learning. First, we provide a concise introduction to the background and definition of in-context learning. Then, we give an overview of advancements from two perspectives: 1) a theoretical perspective, emphasizing studies on mechanistic interpretability and delving into the mathematical foundations behind ICL; and 2) an empirical perspective, concerning studies that empirically analyze factors associated with ICL. We conclude by discussing open questions and the challenges encountered, and suggesting potential avenues for future research. We believe that our work establishes the basis for further exploration into the interpretation of in-context learning. To aid this effort, we have created a repository containing resources that will be continually updated.", "year": 2023, "publicationdate": "2023-11-01", "externalids": {"DOI": "10.18653/v1/2024.emnlp-main.795"}, "doi_lower": "10.18653/v1/2024.emnlp-main.795"}
{"paper_id": 258291930, "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "author_names": ["Deyao Zhu", "Jun Chen", "Xiaoqian Shen", "Xiang Li", "Mohamed Elhoseiny"], "venue": "International Conference on Learning Representations", "abstract": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.", "year": 2023, "publicationdate": "2023-04-20", "externalids": {"DOI": "10.48550/arXiv.2304.10592"}, "doi_lower": "10.48550/arxiv.2304.10592"}
{"paper_id": 258048937, "title": "Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis", "author_names": ["Wenhao Zhu", "Hongyi Liu", "Qingxiu Dong", "Jingjing Xu", "Lingpeng Kong", "Jiajun Chen", "Lei Li", "Shujian Huang"], "venue": "NAACL-HLT", "abstract": "Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating massive languages? 2) Which factors affect LLMs' performance in translation? We thoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our empirical results show that translation capabilities of LLMs are continually involving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system like Google Translate, especially on low-resource languages. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, LLM can acquire translation ability in a resource-efficient way and generate moderate translation even on zero-resource languages. Second, instruction semantics can surprisingly be ignored when given in-context exemplars. Third, cross-lingual exemplars can provide better task guidance for low-resource translation than exemplars in the same language pairs. Code will be released at: https://github.com/NJUNLP/MMT-LLM.", "year": 2023, "publicationdate": "2023-04-10", "externalids": {"DOI": "10.48550/arXiv.2304.04675"}, "doi_lower": "10.48550/arxiv.2304.04675"}
