{
  "authors": [
    "Qingxiu Dong",
    "Lei Li",
    "Damai Dai",
    "Ce Zheng",
    "Zhiyong Wu",
    "Baobao Chang",
    "Xu Sun",
    "Jingjing Xu",
    "Lei Li",
    "Zhifang Sui"
  ],
  "literature_review_title": "A Survey on In-context Learning",
  "year": "2022",
  "date": "2022-12-31",
  "category": "cs.CL",
  "abstract": "With the increasing capabilities of large language models (LLMs), in-context\nlearning (ICL) has emerged as a new paradigm for natural language processing\n(NLP), where LLMs make predictions based on contexts augmented with a few\nexamples. It has been a significant trend to explore ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress and challenges of ICL. We first present a formal definition of ICL\nand clarify its correlation to related studies. Then, we organize and discuss\nadvanced techniques, including training strategies, prompt designing\nstrategies, and related analysis. Additionally, we explore various ICL\napplication scenarios, such as data engineering and knowledge updating.\nFinally, we address the challenges of ICL and suggest potential directions for\nfurther research. We hope that our work can encourage more research on\nuncovering how ICL works and improving ICL.",
  "structure": [
    {
      "section_title": "Introduction/Pre-section",
      "level": "1",
      "content": "\\pdfoutput=1 \\documentclass[11pt]{article} \\usepackage[preprint]{acl} teaserblue{RGB}{242, 242, 255} times latexsym booktabs \\usepackage[T1]{fontenc} \\usepackage[utf8]{inputenc} microtype inconsolata graphicx color \\usepackage[ruled,linesnumbered]{algorithm2e} xspace tabularx makecell amssymb microtype inconsolata amsmath amsfonts amssymb booktabs multirow paralist mdwlist \\xx{x} \\uu{u} \\zz{z} \\yy{y} \\hh{h} \\zy[1]{green{\\bf \\small [#1 --zy]}} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\usepackage[tikz]{bclogo} color xcolor lipsum tikz \\usepackage[edges]{forest} hidden-draw{RGB}{20,68,106} hidden-pink{RGB}{255,245,247} \\leiModify[1]{orange{#1 --Lei}} \\qingxiu[1]{blue{#1}} A Survey on In-context Learning Qingxiu Dong\\textsuperscript{\\rm1, Lei Li\\rm1, Damai Dai\\rm1, Ce Zheng\\rm1, Jingyuan Ma\\rm1, Rui Li\\rm1, Heming Xia\\rm2,\\\\ Jingjing Xu\\textsuperscript{\\rm3, Zhiyong Wu\\rm4, Tianyu Liu\\rm5, Baobao Chang\\rm1, Xu Sun\\rm1, Lei Li\\rm6 and Zhifang Sui\\rm1} \\\\ \\rm 1 Peking University \\rm 2 The Hong Kong Polytechnic University \\\\ \\rm 3 ByteDance \\rm 4 Shanghai AI Lab \\ \\ \\rm 5 Alibaba Group \\rm 6 Carnegie Mellon University \\\\ dqx@stu.pku.edu.cn, szf@pku.edu.cn } document \\maketitle \\footnotetext[1]{We list the author contributions and roles in Appendix~app:authers.}",
      "origin_cites_number": 0
    },
    {
      "section_title": "Introduction",
      "level": "1",
      "content": "With the scaling of model size and data size~gpt3, chowdhery2022palm, openai:2023gpt4, Hugo:2023llama, Hugo:2023llama2, large language models (LLMs) demonstrate the in-context learning (ICL) ability, that is, learning from a few examples in the context. Many studies have shown that LLMs can perform a series of complex tasks through ICL, such as solving mathematical reasoning problems~cot. These strong abilities have been widely verified as emerging abilities for large language models~wei2022emergent. The key idea of in-context learning is to learn from analogy. Figure~fig:icl gives an example that describes how language models make decisions via ICL. First, ICL requires a few demonstration examples to form a prompt context. These examples are usually written in natural language templates. Then, ICL concatenates a query question and the piece of prompt context together to form the input, which is then fed into the language model for prediction. Different from supervised learning, which requires a training stage that uses backward gradients to update model parameters, ICL does not perform parameter updates. The model is expected to learn the pattern hidden in the demonstration and accordingly make the right prediction. % The inference is performed in the form of text completion by reusing the language model head learned during the large-scale pretraining. my-box=[ rectangle, draw=hidden-draw, rounded corners, text opacity=1, minimum height=1.5em, minimum width=5em, inner sep=2pt, align=center, fill opacity=.5, line width=0.8pt, ] leaf=[my-box, minimum height=1.5em, fill=hidden-pink!80, text=black, align=left,font=\\normalsize, inner xsep=2pt, inner ysep=4pt, line width=0.8pt, ] figure*[t!] -1.0cm \\centering \\textwidth{!}{ forest forked edges, for tree={ grow=east, reversed=true, anchor=base west, parent anchor=east, child anchor=west, base=left, font=\\large, rectangle, draw=hidden-draw, rounded corners, align=left, minimum width=4em, edge+={darkgray, line width=1pt}, s sep=3pt, inner xsep=2pt, inner ysep=3pt, line width=0.8pt, ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center}, }, where level=1{text width=4em,font=\\normalsize,}{}, where level=2{text width=7.5em,font=\\normalsize,}{}, where level=3{text width=6.0em,font=\\normalsize,}{}, where level=4{text width=6.0em,font=\\normalsize,}{}, [ In-context Learning, ver [ Training, ver [ Pre-training (\\S sec:pretraining) [ PICL~picl{, }MEND~Li2023mend{, }ICLM~Shi2023iclm, leaf, text width=45.0em ] ] [ Warmup (\\S sec:warmup) [ MetaICL~metaicl{, }OPT-IML~optiml{, }Super-NaturalInstructions~natural{, }\\\\FLAN~flan{, }Scaling Instruction~chung{, }Self-supervised ICL~selfsupericl{, }\\\\Symbol Tuning~symboltuning{, }RICL~Chu2023Finetune {, }ICL Markup~Brunet2023Markup, leaf, text width=45.0em ] ] ] [ Inference, ver [ Demonstration\\\\ (\\S sec:demonstration_org) [ Selection \\\\ (\\S sec:select) [ Unsupervised [ KATE~liu2022close{, }SG-ICL~kim2022self{, }Self-Adaptive\\\\Wu2022SelfadaptiveIL{, }PPL~gonen2022demystifying{, }MI~sorensen2022information{, }\\\\Informative Score~li2023supporting{, }IDS~qin2023incontext{,}\\\\Votek~su2022selective , leaf, text width=29.7em ] ] [ Supervised [ EPR~rubin2022learning{, }Q-Learning~zhang2022active{, }\\\\AdaICL~mavromatis2023examples{, }Topic~topic{, }\\\\UDR~udr , leaf, text width=29.7em ] ] ] [ Reformatting \\\\ (\\S sec:reformatting) [ SG-ICL~kim2022self{, }Structrured Prompting~hao2022structured{, }\\\\AutoICL~yang2023autoicl{, }WICL~yang2023demonstration{, }ICV~liu2024incontext, leaf, text width=37.3em ] ] [ Ordering \\\\ (\\S sec:order) [ GlobalE\\&LocalE~lu2022order{,} ICCL~liu2024lets , leaf, text width=37.3em ] ] ] [ Instruction (\\S sec:instruction) [ Instruction Induction~induct{, }Self-Instruct~wang2022self{, }APE~zhou2022large{, }\\\\Grimoire~chen2024grimoire , leaf, text width=45.0em ] ] [ Scoring \\\\ Function (\\S sec:scoring) [ Calibrate~calibrate{, }Channel Models~min2022noisy{, }$k$NN-Prompting~knnPrompting, leaf, text width=45.0em ] ] ] [ Analysis, ver [ Influencing \\\\Factors (\\S sec:inf_factors) [ Pre-training \\\\Stage (\\S sec:inf_factors_pre) [Pre-Training\\\\ Data [Distribution~distribution,wies2024learnability{, }Domain\\\\shin2022corpora, Han2023UnderstandingIL{, }Diversity~Yadlowsky2023PretrainingDM, leaf, text width=29.6em ] ] [Model and\\\\Training [Architecture ~Ding2023CausalLMIN{, }Pre-training steps~wei2022emergent{, }\\\\Parameters~gpt3,wei2022emergent, leaf, text width=29.6em ] ] ] [ Inference\\\\ Stage (\\S sec:inf_factors_infe) [ Input Labels [ Mapping~ground_truth,Pan2023WhatIL,Tang2023LargeLM{, }\\\\Settings~min2022rethinking, leaf, text width=29.6em ] ] [Demonstration\\\\ Examples [Diversity and Simplicity~compositional_generalization{, }Query Similarity\\\\liu2022close, compositional_generalization{, }Feature bias~inductive_bias{, }\\\\Order~lu2022order, Zhang2022ActiveES,Liu2023LostIT, leaf, text width=29.6em ] ] ] ] [ Learning \\\\Mechanism (\\S sec:mech) [ Functional\\\\ Modules\\\\(\\S sec:mech_fun) [ Induction Heads~olsson2022induction, Bietti2023BirthOA {, }\\\\Computational Layers~label_anchor{, }Attention Modules~icl_weight_shifting, leaf, text width=37.3em ] ] [ Theoretical\\\\ Interpretation\\\\(\\Ssec:mech_theo) [ Bayesian Framework ~bayesian, topic, jiang2023latent{, }\\\\Gradient Descent~dai2022iclft, Irie2022TheDF, mahankali2023one{, }\\\\Others~garg2022linear, akyurek2022algorithm, trm_as_alg, tr_and_tl, leaf, text width=37.3em ] ] ] ] ] forest } Taxonomy of in-context learning. figure* As a new paradigm, ICL has multiple attractive advantages. %ICL opens new directions for natural language processing research. First, since the demonstration is written in natural language, it provides an interpretable interface to communicate with LLMs~gpt3. This paradigm makes it much easier to incorporate human knowledge into LLMs by changing the demonstration and templates~liu2022close, lu2022order, cot, Wu2022SelfadaptiveIL. Second, in-context learning is similar to the decision process of human beings by learning from analogy~winston1980learningByAnalogy. %, i.e., a human makes a prediction based on a few instructions and utilizes the experience from the past to generalize to new tasks.learning by analogy (ju yi fan san) Third, compared to supervised training, ICL is a training-free learning framework. %ICL is data-efficient This high data efficiency of ICL makes this paradigm more ideal than previous data-hungry tuning methods for low-resource applications and scenarios~calibrate. This could not only greatly reduce the computational costs for adapting the model to new tasks, but also make language-model-as-a-service~sun2022black possible and can be easily applied to large-scale real-world tasks. Despite being promising, there are also interesting questions and intriguing properties that require further investigation in ICL. Although a range of vanilla GPT models show excellent ICL capability, several studies have found that this capability can be significantly improved through adaptation during pretraining~metaicl, Li2023mend. Moreover, the performance of ICL is sensitive to specific settings, including the prompt template, the selection and order of demonstration examples, and other factors~topic, liu2024lets. Additionally, optimizing the conciseness of demonstration examples and improving the computational efficiency of ICL are critical areas of ongoing research~liu2024incontext. Furthermore, despite preliminary explanations~dai2022iclft, jiang2023latent, the underlying working mechanism of ICL remains unclear and requires further investigation. With the rapid growth of studies in ICL, our survey aims to sensitize the community toward the current progress. In the following sections, we delve into an in-depth discussion of related studies, and we summarize the taxonomy in Figure~taxo_of_icl and the key findings in Appendix~app:takeaway. We highlight the challenges and potential directions and hope our work provide a useful roadmap for beginners interested in this area and shed light on future research.",
      "origin_cites_number": 69
    },
    {
      "section_title": "Definition and Formulation",
      "level": "1",
      "content": "Following gpt3, we here provide a formal definition of in-context learning: quote In-context learning is a paradigm that allows language models to learn tasks given only a few examples in the form of demonstration. quote Formally, given a query input text $x$ and a set of candidate answers $Y = \\{y_1, \\ldots, y_m\\}$, a pretrained language model $M$ takes the candidate answer with the maximum score as the prediction,$Y$ could be class labels or a set of free-text phrases. conditioned a demonstration set $C$. $C$ contains an optional task instruction $I$ and $k$ demonstration examples, thus $C = \\{ I, s(x_1, y_1), \\ldots, s(x_k, y_k) \\}$ or $C = \\{ s^{\\prime}(x_1, y_1, I), \\ldots, s^{\\prime}(x_k, y_k, I) \\}$, where $s^{\\prime}(x_i, y_i, I)$ is an example written in natural language according to the task. Depending on whether $k$ and the demonstration examples belong to the same task, it can be categorized as task-specific ICL and cross-task ICL. In the latter, different examples have their own instructions. The likelihood of a candidate answer $y_j$ comes from a scoring function $f$ on the whole input sequence: equation P( y_j \\mid x) \\triangleq f_M ( y_j, C, x) equation The final predicted label $\\hat y$ is the candidate answer with the highest probability: equation \\hat y = \\arg\\max_{y_j \\in Y } P(y_j \\mid x). equation According to the definition, we can see that ICL differs from related concepts as follows: (1) Prompt Learning: prompts can be discrete templates or soft parameters that encourage the model to predict the desired output. ICL can be regarded as a subclass of prompt tuning where the demonstration examples are part of the prompt. liu2021pre made a thorough survey on prompt learning, but ICL was not included in their study. (2) Few-shot Learning: few-shot learning is a general machine learning approach that involves adapting model parameters to perform a task with a limited number of supervised examples~wang2019few. In contrast, ICL does not require parameter updates and is directly performed on pretrained LLMs.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Model Training",
      "level": "1",
      "content": "Although LLMs have demonstrated promising ICL capability directly, many studies revealed that these ICL capabilities can be further enhanced through specialized training before inference~selfsupericl, picl, Shi2023iclm.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Pretraining",
      "level": "2",
      "content": "One straightforward direction to boost the ICL capability of LLMs is through pretraining or continual pretraining. For instance, picl and Shi2023iclm proposed to reorganize pretraining corpora by aggregating related contexts, making models learn to reason across prior demonstrations. Differently, Li2023mend introduced a meta-distillation pretraining process, which allows LLMs to reason with distilled demonstration vectors, thereby enhancing ICL efficiency without compromising its effectiveness.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Warmup",
      "level": "2",
      "content": "Another way to enhance ICL ability is adding a continual training stage between pretraining and ICL inference, which we call model warmup for short. Warmup is an optional procedure for ICL, which adjusts LLMs before inference by modifying or adding parameters. As most pretraining data are not tailored for ICL~selfsupericl, researchers have introduced various warmup strategies to bridge the gap between pretraining and ICL inference. Both metaicl and natural proposed to continually finetune LLMs on a broad range of tasks with multiple demonstration examples, which boosts ICL abilities. To encourage the model to learn input-label mappings from the context, symboltuning proposed symbol tuning, which substitutes natural language labels (e.g., ``positive/negative sentiment'') with arbitrary symbols (e.g., ``foo/bar''). selfsupericl proposed a self-supervised method to align raw text with ICL formats in downstream tasks. Besides, multiple studies have indicated the potential value of instructions~mishra2021cross, flan. Tuning the 137B LaMDA-PT~lamda on over 60 datasets verbalized via natural language instruction templates, FLAN~flan improves the ability of LLMs to follow instructions, boosting both the zero-shot and few-shot ICL performance. chung and natural proposed to further scale up instruction tuning with more than 1000+ task instructions.",
      "origin_cites_number": 10
    },
    {
      "section_title": "Prompt Designing",
      "level": "1",
      "content": "table*[t!] \\centering -1.0cm \\tabcolsep{2pt} \\small \\linewidth{!}{tabular{@{}lcccc@{}} % Added one more 'c' for the new column \\toprule \\bf Category & \\bf Methods & \\bf Demonstration Acquisition & \\bf LLMs & \\bf Features \\\\ \\midrule 6{*}{ Demonstration \\\\ Selection} % Updated the row span to include all sub-entries & KATE~liu2022close & Human design & GPT-3 & KNN Selection \\\\ & MI~sorensen2022information &Human design & GPT-3 & Mutual Information \\\\ & EPR~rubin2022learning & Human design & GPT-\\{J, 3\\}/CodeX & Score-based Retrieval \\\\ & IDS~qin2023incontext & Human design & GPT-3.5 & Iterative Selection \\\\ & AdaICL~mavromatis2023examples &Human design & GPT-\\{J, Neo\\} & Selective Demonstration \\\\ & UDR~udr &Human design & GPT-Neo-2.7B & Unified Retrieval \\\\\\midrule 4{*}{ Demonstration \\\\ Reformatting} & SG-ICL~kim2022self & LM generated & GPT-J & Auto Demonstration Generation\\\\ & AutoICL~yang2023autoicl &LM generated & GPT-3.5-Turbo-0301 & Reasoning Path Generation \\\\ & MSP~yang2023demonstration &Human design & GPT series & Adjusting Demonstration Weight \\\\ & ICV~liu2024incontext &Human design & Falcon-7b / Llama-7b & Demonstration Embedding \\\\\\midrule 2{*}{Demonstration \\\\ Ordering} & GlobalE \\& LocalE~lu2022order & Human design & GPT-\\{2, 3\\} & Best Order Selection\\\\ & ICCL~liu2024lets &Human design & Llama2/Mixtral/Qwen & Ordering from Simple to Complex \\\\ \\bottomrule tabular} Summary of representative demonstration designing methods. table* In this section, we focus on the principles of ICL during inference, including demonstration organization~(\\S sec:demonstration_org) and instruction formatting~(\\S sec:instruction) .",
      "origin_cites_number": 12
    },
    {
      "section_title": "Demonstration Organization",
      "level": "2",
      "content": "Many studies have shown that the performance of ICL strongly relies on the demonstration surface, including the selection, formatting, and ordering of demonstration examples~calibrate, lu2022order. In this subsection, we survey demonstration organization strategies and classify them into three categories, as shown in Table~tab:promptmethods.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Demonstration Selection",
      "level": "3",
      "content": "Demonstrations selection aims to answer a fundamental question: Which samples are good examples for ICL? We categorize the related studies into two approaches: unsupervised methods based on predefined metrics and supervised methods. Unsupervised Method A straightforward approach to selecting ICL examples is to choose the nearest neighbors of input instances based on their similarities~liu2022close, tanwar2023multilingual, qin2023incontext. Distance metrics, such as L2 distance or cosine similarity based on sentence embeddings, are commonly used for this purpose. For example, liu2022close proposed KATE, the first $k$NN-based unsupervised retriever for selecting in-context examples. Similarly, $k$-NN cross-lingual demonstrations can be retrieved for multi-lingual ICL to strengthen source-target language alignment~tanwar2023multilingual. su2022selective proposed to combine graphs and confidence scores to select diverse and representative examples. In addition to distance metrics, mutual information~sorensen2022information and perplexity~gonen2022demystifying have proven valuable for prompt selection without labeled examples or specific LLMs. Furthermore, using output scores of LLMs as unsupervised metrics has shown effectiveness in demonstration selection~Wu2022SelfadaptiveIL, nguyen2023influence, li2023supporting. Particularly, Wu2022SelfadaptiveIL selected the best subset permutation of $k$NN examples based on the code length for data transmission to compress label $y$ given $x$ and $C$. li2023supporting used infoscore, i.e., the average of $P(y|x_i,y_i,x) P(y|x)$ for all $(x,y)$ pairs in a validation set with a diversity regularization. %These methods collectively push the boundaries of how in-context examples are optimized to enhance the efficacy of model inference. Supervised Method Though off-the-shelf retrievers offer convenient services for extensive NLP tasks, they are heuristic and sub-optimal due to the lack of task-specific supervision. To address this issue, numerous supervised methods have been developed~rubin2022learning, ye2023compositional, topic, zhang2022active. EPR~rubin2022learning introduced a two-stage method to train a dense retriever for demonstration selection. For a specific input, it first utilized unsupervised methods (e.g., BM25) to recall similar examples as candidates and then used this data to build a supervised dense retriever. Following EPR, udr adopted a unified demonstration retriever to select demonstrations across different tasks. Unlike prior work that retrieves individual demonstrations, ye2023compositional proposed retrieving entire demonstration sets to model inter-relationships between examples. Additionally, mavromatis2023examples introduced AdaICL, a model-adaptive method that employs LLM to predict the unlabeled data set, generating an uncertainty score for each instance. Based on prompt tuning, topic viewed LLMs as topic models that can infer concepts $\\theta$ from a few demonstrations and generate tokens based on these concepts. They represent latent concepts with task-related concept tokens, which are learned to maximize $P(y|x,\\theta)$. Demonstrations are selected based on their likelihood to infer the concept variable using $P(\\theta|x,y)$. Additionally, reinforcement learning was introduced by zhang2022active for example selection. They formulated demonstration selection as a Markov decision process~bellman1957markovian and selected demonstrations via Q-learning. The action is choosing an example, and the reward is defined as the accuracy of a labeled validation set. In order to have a more intuitive comparison of the performance of several unsupervised methods, we select topk~liu2022close, votek~su2022selective, mdl~Wu2022SelfadaptiveIL to conduct experiments. The result is shown in Table 2. The details of the experiment can be found in Appendix app:experiment.",
      "origin_cites_number": 23
    },
    {
      "section_title": "Demonstration Reformatting",
      "level": "3",
      "content": "In addition to directly selecting examples from training data, another research trend involves utilizing LLMs to reformat the representation of existing demonstrations~kim2022self, yang2023autoicl, hao2022structured, yang2023demonstration, liu2024incontext, li2024featureadaptive. For instance, kim2022self proposed generating demonstrations directly from LLMs to reduce the reliance on external demonstration data. Structured Prompting hao2022structured proposed to encode demonstration examples separately with special positional embeddings, which are then provided to the test examples using a rescaled attention mechanism. Diverging from these methods, other approaches focus on modifying the latent representation of demonstrations~liu2024incontext, li2024featureadaptive. Specifically, liu2024incontext developed In-Context Vectors (ICVs) derived from the latent embeddings of demonstration examples in LLMs. These ICVs are used during inference to adjust the latent states of the LLM, thereby enhancing the model's ability to follow the demonstrations more effectively.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Demonstration Ordering",
      "level": "3",
      "content": "Ordering the selected demonstration examples is also an important aspect of demonstration organization. lu2022order have proven that order sensitivity is a common problem and always exists for various models. To handle this problem, previous studies have proposed several training-free methods for sorting demonstration examples. Particularly, liu2022close arranged examples based on their proximity to the input, positioning the closest example as the rightmost demonstration. lu2022order introduced global and local entropy metrics, finding a positive correlation between these metrics and the ICL performance. Consequently, they utilized the entropy metric to determine the optimal demonstration ordering. Additionally, ICCL~liu2024lets suggested ranking demonstrations from simple to complex, thereby gradually increasing the complexity of demonstration examples during the inference process.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Instruction Formatting",
      "level": "2",
      "content": "A common way to format demonstrations is concatenating examples $(x_1, y_1), \\ldots, (x_k, y_k)$ with a template $T$ directly. However, in some tasks that need complex reasoning (e.g., math word problems and commonsense reasoning), it is not easy to learn the mapping from $x_i$ to $y_i$ with only $k$ demonstrations. Although template engineering has been studied in prompting~liu2021pre, some researchers aim to design a better format of demonstrations for ICL by describing tasks with the instruction $I$. induct found that given several demonstration examples, LLMs can generate task instructions themselves. Considering the generation abilities of LLMs, zhou2022large proposed an Automatic Prompt Engineer for automatic instruction generation and selection. To further improve the quality of the automatically generated instructions, several strategies have proposed using LLMs to bootstrap off its own generations~wang2022self, chen2024grimoire. Additionally, chain-of-thought (CoT)~cot introduces intermediate reasoning steps between inputs and outputs to enhance problem-solving and comprehension. Recent advancements have also emphasized the process of enhancing step-by-step reasoning in models~autocot, wang2022iteratively, least.",
      "origin_cites_number": 6
    },
    {
      "section_title": "Scoring Function",
      "level": "2",
      "content": "The scoring function determines how to transform the predictions of a language model into an estimation of the likelihood of a specific answer. The Direct method uses the conditional probability of candidate answers represented by tokens in the model's vocabulary gpt3. The answer with the highest probability is selected as the final answer, but this method restricts template design by requiring answer tokens to be at the end of input sequences. Perplexity (PPL) is another commonly used metric that computes the sentence perplexity of the entire input sequence \\( S_j = \\{ C, s(x, y_j, I) \\} \\), which includes tokens from demonstration examples \\( C \\), the input query \\( x \\), and the candidate label \\( y_j \\). PPL evaluates the probability of the sentence, eliminating token position limitations but requiring additional computation time. min2022noisy proposed using channel models (Channel) to compute the conditional probability in reverse, estimating the likelihood of the input query given the label. This approach requires language models to generate every token in the input, potentially boosting performance under imbalanced training data. We summarize all three scoring functions in Table~tab:score_func. Note that in Table~tab:score_func, `Efficiency' refers to the language model inference latency; `Coverage' reflects whether the method utilizes the output probability of the local or all token positions in the input sequence; and `Stability' indicates whether the in-context learning ability is easily affected by changes in the demonstration examples.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Analysis",
      "level": "1",
      "content": "To understand ICL, recent studies attempt to investigate what influence ICL performance~shin2022corpora, ground_truth, Kossen2023InContextLL and why ICL works~dai2022iclft, Irie2022TheDF. In this section, we present a detailed elaboration of influencing factors~(\\S sec:inf_factors) and learning mechanisms~(\\S sec:mech) of ICL, as illustrated in Figure~fig:factor. figure* \\centering -1.0cm \\includegraphics[width=0.9\\textwidth]{fig/icl_ana.pdf} Summary of factors that have a relatively strong correlation to ICL performance and different perspectives to explain why ICL works. figure*",
      "origin_cites_number": 2
    },
    {
      "section_title": "Influencing Factors",
      "level": "2",
      "content": "We discuss relevant research addressing what influences ICL performance, including factors both in the pretraining stage and in the inference stage.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Pretraining Stage",
      "level": "3",
      "content": "We first introduce factors that influence the pretraining stage. The diversity of pretraining corpora significantly impacts ICL performance~shin2022corpora, Yadlowsky2023PretrainingDM, Raventos2023PretrainingTD. In particular, shin2022corpora found that the source domain is more important than the corpus size, suggesting that combining multiple corpora may lead to the emergence of ICL ability. Similarly, Raventos2023PretrainingTD empirically identified a task diversity threshold beyond which LLMs exhibit strong ICL capabilities in unseen tasks. Another line of research investigates the impact of data distribution on ICL~distribution, wies2024learnability. For instance, distribution demonstrated that ICL capability emerges when the training data exhibits specific distributional properties, such as burstiness, wherein items appear in clusters rather than being uniformly distributed over time. Beyond these works, several studies have investigated the impact of model architecture and training process on ICL performance~wei2022emergent, gpt3, Ding2023CausalLMIN. wei2022emergent investigated the emergent abilities of many large-scale models on multiple tasks. They suggested that a pretrained model acquires some emergent ICL abilities when it reaches a large scale of pretraining steps or model parameters. Ding2023CausalLMIN pointed out that the in-context samples should attend to each other during inference, indicating that current causal LLMs may lead to suboptimal ICL performance.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Inference Stage",
      "level": "3",
      "content": "During inference, there are also multiple properties of demonstration examples that influence ICL performance. min2022rethinking proved that input-label settings such as the pairing format, the exposure of label space, and the input distribution contribute substantially to ICL performance. However, contrary to the conclusion in min2022rethinking that input-label mapping matters little to ICL, latter studies showed that the accurate mapping influence ICL performance significantly~ground_truth, Pan2023WhatIL, Tang2023LargeLM. llm_icl_differently further pointed that flipped or semantically-unrelated input-label mapping also can be learned. From the perspective of demonstration construction, recent literature focuses on the diversity and simplicity of demonstrations~compositional_generalization, the order of samples~lu2022order, Zhang2022ActiveES, Liu2023LostIT, and the similarity between demonstrations and queries~liu2022close. For example, liu2022close found that demonstration samples with embeddings closer to those of the query samples typically yield better performance than those with more distant embeddings. Notably, despite efforts to refine demonstrations to optimize the performance, there still remain clear feature biases during ICL inference~inductive_bias. Overcoming strong prior biases and ensuring the model gives equal weight to all contextual information remain challenges~Kossen2023InContextLL.",
      "origin_cites_number": 10
    },
    {
      "section_title": "Learning Mechanism",
      "level": "2",
      "content": "From a learning mechanism perspective, we delve into the research addressing why ICL is effective.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Functional Modules",
      "level": "3",
      "content": "The ICL capability is intimately connected to specific functional modules within Transformers. As one of the core components, the attention module is a focal point in the study of ICL mechanism~olsson2022induction, Bietti2023BirthOA,dai2022iclft, Irie2022TheDF, icl_weight_shifting,Gao2023InContextLF,zhang2023and. Particularly, olsson2022induction identified specific attention heads, referred to as ``induction heads'', that can replicate previous patterns for next-token prediction, thus progressively developing ICL capabilities. Additionally, label_anchor focused on the information flow in Transformers and found that during the ICL process, demonstration label words serve as anchors, which aggregate and distribute key information for the final prediction.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Theoretical Interpretation",
      "level": "3",
      "content": "In this subsection, we introduce the theoretical interpretations of ICL from different views. Bayesian View In the Bayesian framework, ICL is explained as implicit Bayesian inference, where models perform ICL by identifying a shared latent concept among examples~bayesian,wies2024learnability,ahuja2023context,jiang2023latent,topic. Additional perspectives suggest that LLMs encode the Bayesian Model Averaging algorithm via the attention mechanism~zhang2023and. As the number of in-context examples increases, implicit Bayesian inference becomes analogous to kernel regression~Han2023ExplainingEI. Gradient Descent View Gradient descent offers another valuable lens for understanding ICL. ~dai2022iclft identified a dual form between Transformer attention and gradient descent, finding that GPT-based ICL behaves similarly to explicit fine-tuning from multiple perspectives. Other studies have attempted to establish connections between ICL and gradient descent in simplified regression settings~icl_gd, Ahn2023TransformersLT, mahankali2023one, icl_weight_shifting. For instance, icl_gd showed that linear attention-only Transformers with manually constructed parameters are closely related to models learned by gradient descent. icl_weight_shifting found that self-attention-only Transformers exhibit similarities with models trained via gradient descent. However, the simplified settings used in these studies have led to debates about the direct applicability of these connections in real-world contexts~Shen2023RevisitingTH. fu2023transformers argued that Transformers perform ICL on linear regression using higher-order optimization techniques rather than gradient descent. Other Views Beyond connecting ICL with a single algorithm, researchers have analyzed it from various perspectives, including ability decoupling, algorithmic learning, and information theory. tr_and_tl decoupled ICL capabilities into task recognition ability and task learning ability, each manifesting under different conditions. Another typical theory abstracts ICL as an algorithmic learning problem~akyurek2022algorithm, garg2022linear, trm_as_alg, Bai2023TransformersAS, where Transformers dynamically select algorithms, such as gradient descent and ridge regression, tailored to different ICL instances. Moreover, implicit_structure_induction utilized information theory to show an error bound for ICL under linguistically motivated assumptions, explaining how next-token prediction can bring about the ICL ability. These analytical studies have taken an essential step to explain ICL. However, most of them focused on simple tasks and small models. Extending analysis on extensive tasks and large models may be the next step to be considered.",
      "origin_cites_number": 12
    },
    {
      "section_title": "Application",
      "level": "1",
      "content": "Given its user-friendly interface and lightweight prompting method, ICL has broad applications on traditional NLP tasks~kim2022self,metaicl,zhu2023multilingual. Particularly, by using demonstrations that explicitly guide the reasoning process, ICL manifests remarkable effects on tasks requiring complex reasoning~cot,li2023code,teachalgo and compositional generalization~least. We explore several emerging and prevalent applications of ICL, including data engineering, model augmentation, and knowledge updating. 1) Data Engineering: Unlike traditional methods such as human annotation and noisy automatic annotation, ICL generates relatively high-quality data at a lower cost, leading to improved performance.~want, khorashadizadeh2023exploring, annotation. 2) Model Augmentation: The context-flexible nature of ICL shows promise in model augmentation. It can enhance retrieval-augmented methods by prepending grounding documents to the input~ram2023context. Additionally, ICL for retrieval demonstrates potential in steering models toward safer outputs~dpicl, meade2023using. 3) Knowledge Updating: LLMs often contain outdated or incorrect knowledge~NEURIPS2023_5f0a4cd2. ICL has demonstrated efficacy in revising such knowledge through carefully crafted demonstrations, yielding higher success rates compared to gradient-based methods~editingfact. As mentioned above, ICL has yielded significant benefits on both traditional and emergent NLP applications. The tremendous success of ICL in NLP has inspired researchers to explore its potential in various modalities beyond text (elaborated in Appendix~app:vision), including vision ~bar2022visual_icl,wang2023imagesPainter, vision-language~tsimpoukelli2021frozen, alayrac2022flamingo, as well as speech applications~wang2023neural,zhang2023valle-x.",
      "origin_cites_number": 11
    },
    {
      "section_title": "Challenges and Future Directions",
      "level": "1",
      "content": "In this section, we review existing challenges and discuss future directions for ICL. Efficiency and Scalability The use of demonstrations in ICL introduces two challenges: (1) higher computational costs with an increasing number of demonstrations (efficiency), and (2) fewer learnable samples due to the maximum input length of LLMs (scalability). Prior research has attempted to mitigate these issues by distilling lengthy demonstrations into compact vectors~Li2024ImplicitIL,Li2023mend or expediting LLM inference times~Liu2023dejavu. However, these methods often involve a trade-off in performance or necessitate access to model parameters, which is impractical for closed-source models like ChatGPT and Claude~Zhou2023Efficient. Thus, enhancing the scalability and efficiency of ICL with more demonstrations remains a significant challenge. Generalization ICL heavily relies on high-quality demonstrations selected from annotated examples, which are often scarce in low-resource languages and tasks. This scarcity poses a challenge to the generalization ability of ICL~He2024SelfDemos. Given that there is a substantial discrepancy in the availability of annotated high-resource data and low-resource data, the potential to leverage high-resource data to address low-resource tasks is highly appealing~Chatterjee2024LanguageMC, tanwar2023multilingual. Long-context ICL Recent advances in context-extended LLMs have spurred research into the impact of ICL when using an increasing number of demonstration examples~agarwal2024manyshot,Bertsch2024InContextLW. However, researchers have found that increasing the number of demonstrations does not necessarily enhance performance and may even be detrimental. These performance declines indicate a need for further investigation. Additionally, Li2024LongcontextLS developed LongICLBench, which includes diverse extreme-label classification tasks, revealing further weaknesses of LLMs in comprehending extended demonstrations.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Conclusion",
      "level": "1",
      "content": "In this paper, we comprehensively review the existing literature on ICL, examining advanced techniques, conducting analytical studies, discussing relevant applications, and identifying critical challenges and potential directions for future research. To our knowledge, this is the first comprehensive survey dedicated to ICL. We aim to highlight the current state of research in ICL and provide insights to guide future work in this promising area.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Limitations",
      "level": "1",
      "content": "This paper offers a comprehensive examination and summary of current methodologies and analyses in the area of In-Context Learning (ICL). However, given the extensive body of related work, particularly in demonstration design and the principle analysis of ICL, we may have overlooked some equally valuable contributions. Additionally, we outline several future directions for research in ICL, including long-context ICL, efficiency and scalability in ICL, etc. We plan to leave these aspects for future work. Furthermore, many papers covered by this survey did not utilize the most up-to-date models while running experiments. We advocate for more thorough and up-to-date research to provide actionable insights for practitioners.",
      "origin_cites_number": 0
    }
  ],
  "literature_review_id": 255372865,
  "meta_info": {
    "cite_counts": 162,
    "Conference_journal_name": "{\"name\": null, \"pages\": \"1107-1128\", \"volume\": null}",
    "influentialcitationcount": 44,
    "Author_info": {
      "Publicationsh": 34,
      "h_index": 17,
      "Citations": 2501,
      "Highly Influential Citations": 0
    },
    "all_cites_title": [
      "Human design GPT-Neo-2.7B Unified Retrieval Demonstration Reformatting SG-ICL (Kim et al., 2022) LM generated GPT-J Auto Demonstration Generation AutoICL (Yang et al., 2023a) LM generated GPT-3.5-Turbo-0301 Reasoning Path Generation MSP (Yang et al., 2023b) Human design GPT series Adjusting Demonstration Weight ICV (Liu et al., 2024a) Human design Falcon-7b / Llama-7b Demonstration Embedding Demonstration Ordering GlobalE",
      "Llama2/Mixtral/Qwen Ordering from Simple to Complex References",
      "Many-shot incontext learning",
      "Transformers learn to implement preconditioned gradient descent for in-context learning",
      "Kabir Ahuja, Madhur Panwar, and Navin Goyal. 2023. In-context learning through the bayesian prism. CoRR, abs/2306.04891.",
      "Llama 3 model card",
      "What learning algorithm is in-context learning? investigations with linear models",
      "Flamingo: a visual language model for few-shot learning",
      "How do in-context examples affect compositional generalization?",
      "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng- guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx- uan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023a. Qwen technical report. arXiv preprint arXiv:2309.16609.",
      "2023b. Transformers as statisticians: Provable in-context learning with in-context algorithm selection",
      "Visual prompting via image inpainting",
      "A markovian decision process",
      "In-context learning with long-context models: An in-depth exploration",
      "Birth of a transformer: A memory viewpoint",
      "Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Dem- szky, Chris Donahue, Moussa Doumbouya, Esin Dur- mus, Stefano Ermon, John Etchemendy, Kawin Etha- yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lau- ren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khat- tab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Ma- lik, Christopher D. Manning, Suvir P. Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel Orr, Isabel Papadim- itriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Re- ich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher R'e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram√®r, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Ya- sunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021. On the opportunities and risks of foundation models. ArXiv.",
      "A large annotated corpus for learning natural language inference",
      "Language models are few-shot learners",
      "ICL markup: Structuring incontext learning using soft-token tags",
      "Data distributional properties drive emergent in-context learning in transformers",
      "Language models can exploit cross-task in-context learning for datascarce novel tasks",
      "Grimoire is all you need for enhancing large language models",
      "Improving in-context few-shot learning via self-supervised training",
      "Palm: Scaling language modeling with pathways",
      "Fine-tune language models to approximate unbiased in-context learning",
      "Scaling instruction-finetuned language models",
      "2023a. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers",
      "Instructblip: Towards general-purpose visionlanguage models with instruction tuning",
      "Editing factual knowledge in language models",
      "Is GPT-3 a good data annotator?",
      "CausalLM is not optimal for in-context learning",
      "Statistical knowledge assessment for large language models",
      "Transformers learn higher-order optimization methods for in-context learning: A study with linear models",
      "Incontext learning for attention scheme: from single softmax regression to multiple softmax regression via a tensor trick",
      "What can transformers learn incontext? A case study of simple function classes",
      "Demystifying prompts in language models via perplexity estimation",
      "Pre-training to learn in context",
      "A theory of emergent in-context learning as implicit structure induction",
      "2023a. Explaining emergent in-context learning as kernel regression",
      "2023b. Understanding in-context learning via supportive pretraining data",
      "Language models are general-purpose interfaces",
      "2022b. Structured prompting: Scaling in-context learning to 1,000 examples",
      "ICL-D3IE: in-context learning with diverse demonstrations updating for document information extraction",
      "Self-demos: Eliciting out-of-demonstration generalizability in large language models",
      "In-context learning in large language models: A comprehensive survey",
      "Instruction induction: From few examples to natural language task descriptions",
      "PRODIGY: enabling in-context learning over graphs",
      "Language is not all you need: Aligning perception with language models",
      "The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention",
      "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "A latent space theory for emergent abilities in large language models",
      "Exploring in-context learning capabilities of foundation models for generating knowledge graphs from text",
      "Self-generated in-context learning: Leveraging autoregressive language models as a demonstration generator",
      "In-context learning in large language models learns label relationships but is not conventional learning",
      "A multi-modal model with in-context instruction tuning",
      "2023b. Towards enhancing in-context learning for code generation",
      "2024a. Feature-adaptive and data-scalable in-context learning",
      "The closeness of in-context learning and weight shifting for softmax regression",
      "Quy Duc Do, Xiang Yue, and Wenhu Chen. 2024b. Long-context llms struggle with long in-context learning",
      "Xiaoling Wang, and Xipeng Qiu. 2023d. Unified demonstration retriever for incontext learning",
      "Finding supporting examples for in-context learning",
      "2024c. MEND: meta demonstration distillation for efficient and effective in-context learning",
      "2023e. Transformers as algorithms: Generalization and stability in in-context learning",
      "A practical survey on zero-shot prompt design for in-context learning",
      "2024d. Implicit in-context learning",
      "Visual instruction tuning",
      "What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
      "2023b. Lost in the middle: How language models use long contexts",
      "2023c. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "2024a. In-context vectors: Making in context learning more effective and controllable through latent space steering",
      "2024b. Let's learn step by step: Enhancing in-context learning ability with curriculum learning",
      "Deja vu: Contextual sparsity for efficient llms at inference time",
      "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
      "One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention",
      "Which examples to annotate for in-context learning? towards effective and efficient selection",
      "Using in-context learning to improve dialogue safety",
      "2022a. Noisy channel language model prompting for few-shot text classification",
      "MetaICL: Learning to learn in context",
      "Rethinking the role of demonstrations: What makes in-context learning work?",
      "Cross-task generalization via natural language crowdsourcing instructions",
      "In-context example selection with influences",
      "Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Con- erly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jack- son Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2022. In-context learn- ing and induction heads. CoRR, abs/2209.11895.",
      "GPT-4 technical report",
      "2023a. What in-context learning \"learns\" in-context: Disentangling task recognition and task learning",
      "2023b. What in-context learning \"learns\" in-context: Disentangling task recognition and task learning",
      "Differentially private in-context learning",
      "In-context learning with iterative demonstration selection",
      "Language models are unsupervised multitask learners",
      "Know what you don't know: Unanswerable questions for squad",
      "Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models",
      "Pretraining task diversity and the emergence of non-bayesian in-context learning for regression",
      "Learning to retrieve prompts for in-context learning",
      "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
      "Do pretrained transformers learn in-context by gradient descent?",
      "Language models are multilingual chain-of-thought reasoners",
      "In-context pretraining: Language modeling beyond document boundaries",
      "On the effect of pretraining corpora on in-context learning by a large-scale language model",
      "Measuring inductive biases of in-context learning with underspecified demonstrations",
      "Recursive deep models for semantic compositionality over a sentiment treebank",
      "Recursive deep models for semantic compositionality over a sentiment treebank",
      "An information-theoretic approach to prompt engineering without ground truth labels",
      "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
      "Selective annotation makes language models better few-shot learners",
      "Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service",
      "Exploring effective factors for improving visual in-context learning",
      "Challenging big-bench tasks and whether chain-of-thought can solve them",
      "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
      "2023a. Large language models can be lazy learners: Analyze shortcuts in in-context learning",
      "2023b. In-context learning of large language models for controlled dialogue summarization: A holistic benchmark and empirical analysis",
      "Multilingual llms are better cross-lingual in-context learners with alignment",
      "Lamda: Language models for dialog applications",
      "Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models",
      "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton- Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, An- thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di- ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar- tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly- bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subrama- nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur√©lien Ro- driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.",
      "Multimodal few-shot learning with frozen language models",
      "Large language models still can't plan (a benchmark for llms on planning and reasoning about change)",
      "Transformers learn in-context by gradient descent",
      "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
      "2022a. Iteratively prompt pre-trained language models for chain of thought",
      "2023a. Neural codec language models are zero-shot text to speech synthesizers",
      "Label words are anchors: An information flow perspective for understanding in-context learning",
      "Want to reduce labeling cost? GPT-3 can help",
      "Images speak in images: A generalist painter for in-context visual learning",
      "Seggpt: Towards segmenting everything in context",
      "Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning",
      "Few-shot learning: A survey",
      "Self-instruct: Aligning language models with self-generated instructions",
      "Tanay Dixit, and Xudong Shen. 2022b. Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks",
      "2023g. In-context learning unlocked for diffusion models",
      "2022a. Finetuned language models are zero-shot learners",
      "Emergent abilities of large language models",
      "Chain-of-thought prompting elicits reasoning in large language models",
      "Symbol tuning improves in-context learning in language models",
      "Larger language models do in-context learning differently",
      "The learnability of in-context learning",
      "Learning and reasoning by analogy",
      "2023a. Openicl: An open-source framework for in-context learning",
      "Self-adaptive in-context learning: An information compression perspective for incontext example selection and ordering",
      "An explanation of in-context learning as implicit bayesian inference",
      "Qiaoqiao She, and Yongdong Zhang. 2023a. k nn prompting: Learning beyond the context with nearest neighbor inference",
      "-context learning with retrieved demonstrations for language models: A survey",
      "Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning",
      "Pretraining data mixtures enable narrow model selection capabilities in transformer models",
      "2023a. Auto-icl: In-context learning without human supervision",
      "2023b. Not all demonstration examples are equally beneficial: Reweighting demonstration examples for in-context learning",
      "Compositional exemplars for in-context learning",
      "Ground-truth labels matter: A deeper look into input-label demonstrations",
      "Character-level convolutional networks for text classification",
      "Active example selection for in-context learning",
      "Active example selection for in-context learning",
      "2023a. What makes good examples for visual in-context learning?",
      "2023b. What and how does incontext learning learn? bayesian model averaging, parameterization, and generalization",
      "Automatic chain of thought prompting in large language models",
      "Speak foreign languages with your own voice: Cross-lingual neural codec language modeling",
      "Calibrate before use: Improving few-shot performance of language models",
      "2023a. Least-to-most prompting enables complex reasoning in large language models",
      "Teaching algorithmic reasoning via in-context learning",
      "Ryan Cotterell, and Mrinmaya Sachan. 2023b. Efficient prompting via dynamic in-context learning",
      "Large language models are human-level prompt engineers",
      "The mystery and fascination of llms: A comprehensive survey on the interpretation and analysis of emergent abilities",
      "2023a. Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "2023b. Multilingual machine translation with large language models: Empirical results and analysis"
    ]
  }
}