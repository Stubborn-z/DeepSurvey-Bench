{
  "survey": "This survey provides a comprehensive examination of advanced techniques in artificial intelligence, focusing on in-context learning, prompt engineering, few-shot learning, and their applications in natural language processing (NLP). The primary objective is to elucidate how these methodologies enhance machine understanding and generation of human language. By exploring the adaptation of pre-trained models to novel tasks without specific finetuning and the optimization of large language models through effective prompt design, the survey highlights the pivotal role these strategies play in AI development. The survey systematically explores the mechanisms of in-context learning, emphasizing its ability to enable models to learn from context without explicit parameter updates. It delves into prompt engineering strategies, including discrete, continuous, few-shot, and zero-shot prompts, emphasizing their impact on model performance. Few-shot learning is addressed with a focus on techniques that enhance learning capabilities from minimal examples, crucial for reducing data requirements. The survey synthesizes these methodologies within NLP, illustrating their roles in enhancing machine understanding and generation of human language. Key findings underscore the significance of these techniques in advancing AI capabilities, with future research directions proposed to optimize methodologies and address current challenges. Collectively, the survey emphasizes the transformative potential of these advanced techniques in enhancing AI adaptability, reliability, and performance across diverse tasks and domains.\n\nIntroduction Objectives and Overview This survey provides a comprehensive examination of advanced techniques in artificial intelligence, specifically in-context learning, prompt engineering, and few-shot learning, along with their applications in natural language processing (NLP). The primary objective is to elucidate how these methodologies enhance machine understanding and generation of human language. It explores the adaptation of pre-trained models to novel tasks without specific finetuning, as highlighted in visual prompting techniques [1], and the optimization of large language models through effective prompt design [2]. Furthermore, the survey addresses challenges in few-shot learning, particularly the limitations of in-context learning without meta-training [3]. The paper is structured into distinct sections, each focusing on a specific technique, followed by their applications in NLP, thereby providing a holistic view of the current landscape and future directions in these domains. Structure of the Survey The survey is systematically organized into several key sections, each dedicated to distinct methodologies and their applications within artificial intelligence. It begins with foundational concepts of in-context learning, prompt engineering, and few-shot learning, providing definitions and explanations that underscore their significance in AI. The subsequent section examines the mechanisms of in-context learning, illustrating how models learn from context without explicit parameter updates, which influences AI development. The prompt engineering section explores various strategies, including discrete, continuous, few-shot, and zero-shot prompts [2], emphasizing their impact on model performance and the challenges of effective prompt design. Following this, the survey focuses on few-shot learning techniques that enhance learning from minimal examples, highlighting the importance of this approach in reducing data requirements. The applications section synthesizes these methodologies within NLP, demonstrating their roles in improving machine understanding and generation of human language. The survey concludes by summarizing key findings and proposing future research directions, emphasizing the potential of these techniques to advance AI capabilities and address current challenges.The following sections are organized as shown in . Background and Definitions Definitions and Explanations of Key Concepts In-context learning (ICL) is a pivotal feature of large language models (LLMs) that allows them to perform tasks with minimal examples, bypassing the need for explicit weight adjustments. This is achieved through pretrained transformers, which facilitate task adaptation by interpreting input-output pairs without additional tuning. The quality of data annotations is critical for optimizing ICL performance [4], and its application in retrieving safe responses in sensitive dialogues highlights its potential for enhancing model safety [5]. Understanding semantic priors and input-label mappings is crucial for evaluating ICL's effectiveness [6]. Few-shot learning empowers models to generalize with limited examples, reducing the dependency on extensive datasets [7]. This is especially valuable in resource-intensive data annotation contexts. Research into transformers executing preconditioned gradient descent algorithms offers promising enhancements for few-shot learning [8]. Evaluating models' ability to generate instructions from minimal examples serves as a benchmark for few-shot learning [9]. Prompt engineering involves designing prompts to optimize model responses across tasks [2], facilitating cross-task generalization. Developing benchmarks to assess different prompting strategies is crucial for advancement [9]. While successful in NLP, instruction tuning in vision and multimodal contexts remains underexplored, presenting future research opportunities [10]. These concepts drive innovation in NLP, addressing challenges in neural network interpretability and decision-making. The MGSM benchmark evaluates multilingual reasoning in LLMs [11], and ICL's application in document information extraction (DIE) exemplifies its utility in processing complex data [12]. Benchmarks addressing dialog system safety and factual grounding ensure alignment with human values [13], essential for pretrained language model performance [14]. Significance in AI In-context learning, prompt engineering, and few-shot learning significantly reshape AI model training and deployment, addressing key developmental challenges. ICL enhances task generalization by enabling adaptive predictors from examples, circumventing traditional extensive parameter updates [3]. This is crucial for practical applications, allowing models to utilize demonstrations effectively [15], though challenges exist when tasks fall outside pretraining data [3]. Weaker models' inconsistent ICL capabilities limit specific task performance [15], and accurate annotations, akin to supervised learning, are vital [12]. ICL's role in generating safer outputs enhances model safety and user trust [13]. Prompt engineering is key for cross-task generalization via textual instructions, enhancing model versatility [2]. Input prompt quality heavily influences model performance, with no universally accepted 'best' prompt complicating evaluations [2]. This approach addresses inefficiencies and costs of dataset annotation for language tasks [3]. Multimodal learning emphasizes AI models' need to understand and generate text from diverse inputs, addressing current benchmark limitations [16]. Instruction finetuning advances pretrained model usability and performance, improving task handling [2]. Few-shot learning reduces data needs, enabling generalization from minimal examples, crucial in resource-constrained settings [3]. Human-labeled data reliance limits training scalability, highlighting few-shot learning's importance in AI development [3]. Insights into transformers learning optimization algorithms support few-shot learning advancements [15]. Collectively, these strategies and benchmarks advance AI capabilities, ensuring models are performant, reliable, and adaptable across tasks and domains [16], mitigating social harms through improved language model understanding. Interrelation of Concepts The interrelation of in-context learning, prompt engineering, and few-shot learning is vital for enhancing AI model adaptability and resource-efficient learning. ICL and few-shot learning both aim to enable effective operation with minimal examples but through distinct methodologies. ICL uses LLMs to interpret and generate responses based on input-output pairs, facilitating task adaptation without weight updates [16]. Challenges remain in selecting informative in-context examples from numerous permutations, essential for effective language model learning [17]. Few-shot learning focuses on task generalization with limited data, critical in resource-intensive annotation scenarios. Prompt engineering bridges these methods by optimizing input prompts to enhance model performance across tasks [16]. Effective prompt design is crucial for cross-task generalization, affecting language models' inductive biases and feature preferences in underspecified demonstrations. Integrating multimodal tasks requires a unified instruction tuning approach, combining causal and non-causal modeling to advance few-shot learning by learning from diverse inputs and extending applicability across domains [16]. The benchmark by [16] underscores robust methodologies for real-world complexities through diverse task incorporation. Advanced LLMs like GPT-3 serve as effective data annotators, bridging traditional methods and AI capabilities. This interrelation highlights the need for strategies enhancing neural network interpretability and decision-making, ensuring models are performant, reliable, and adaptable across tasks and domains. Task recognition and learning in ICL contribute to this interrelation, offering insights for improved model performance [16]. In-context Learning In artificial intelligence, in-context learning (ICL) is pivotal for enabling models to leverage contextual information for task adaptation. This section explores ICL mechanisms, emphasizing task recognition (TR) and task learning (TL), which allow models to discern task patterns and learn mappings from minimal examples in prompts. As illustrated in , the hierarchical structure of ICL mechanisms is depicted, categorizing key concepts such as task recognition and learning, model adaptability, efficiency, emergent abilities, performance factors, and innovative techniques. This figure underscores the significance of these elements in enhancing AI model capabilities across diverse domains. Table provides a comparative analysis of different in-context learning methods, illustrating their distinct approaches to task recognition, example selection, and adaptability. The following subsection delves into these mechanisms, highlighting their role in enhancing large language models (LLMs) across various domains. Mechanisms of In-context Learning ICL represents a significant advancement in AI, allowing models to adapt to new tasks with minimal examples embedded in prompts. The core mechanisms involve task recognition (TR) and task learning (TL), functioning independently to help models identify task patterns and learn mappings [18]. LLMs facilitate this by inferring latent concepts from input contexts, enabling efficient adaptation [16]. illustrates the hierarchical structure of in-context learning mechanisms, categorizing the primary concepts into task recognition and learning, methodologies and applications, and performance enhancement techniques. Each category highlights key methods and frameworks that contribute to the understanding and advancement of in-context learning within AI systems. Methods like retrieval-based in-context learning (RIL) enhance safety in dialogues by using safe response demonstrations [5]. The LENS method addresses the challenge of selecting informative examples through a two-stage filtering process [17]. IDS iteratively selects demonstrations based on their correlation with test samples, refining the ICL process [19]. ICL-D3IE improves LLM performance in document information extraction by leveraging diverse demonstrations [12]. ICL-FSUL examines language models' adaptability to manipulated label semantics [6]. SLEICL enhances ICL by enabling weaker models to learn from stronger ones [15]. The LaMDA approach integrates external knowledge to improve safety and factual accuracy [13]. Self-Instruct generates and filters its own instruction data for fine-tuning, enhancing instruction-following capabilities [14]. MGSM highlights the need for benchmarks reflecting diverse reasoning abilities across languages [11]. These methodologies deepen our understanding of ICL mechanisms, enabling AI systems to perform a wide array of tasks with minimal examples, enhancing generalization and reliability. Relevance of In-context Learning ICL is crucial in AI for enhancing model adaptability with minimal examples in prompts, supported by pretraining data's compositional structure [20]. The distinction between TR and TL allows LLMs to function as associative memories, adapting to new tasks without extensive updates. ICL improves model safety and reliability, as retrieval-based approaches enhance chatbot response safety by using safe dialogue examples [5]. Methods like LENS ensure informative and diverse examples, crucial for effective ICL [17]. Larger models can override semantic priors with contradictory examples, indicating advanced ICL capabilities [6]. ICL optimizes annotation processes, with GPT-3 performing competitively as a data annotator [4]. Iterative updating in ICL-D3IE refines understanding of positional relationships, essential for information extraction [12]. IDS enhances contextual understanding, enabling models to adapt to specific tasks [19]. SLEICL boosts weaker models' performance by leveraging stronger models [15]. LaMDA framework experiments show improvements in safety and factual grounding [13]. Self-Instruct contributes to creating diverse training datasets, essential for robust ICL [14]. Multilingual benchmarks indicate superior reasoning abilities in larger models, emphasizing ICL's role in enhancing multilingual capabilities [11]. ICL advancements enhance model reliability, scalability, and adaptability across diverse domains. Models can generate knowledge graphs from text, automating tasks like named entity extraction. ICL reduces computational demands, integrating retrieved demonstrations tailored to queries, improving efficiency and scalability while mitigating biases [21,22,23]. These methodologies illustrate ICL's potential to drive AI developments, ensuring models address real-world complexities with accuracy and efficiency. Emergent Abilities and Performance Factors Emergent abilities of LLMs using ICL have drawn attention for tackling complex tasks with minimal examples in prompts. These abilities are linked to the interplay between TR and TL, contributing to ICL performance [18]. Theoretical analyses suggest ICL manages latent tasks under specific pretraining conditions, enhancing adaptability [24]. ICL development is driven by next-token prediction mechanisms, forming the basis for context-specific reasoning [20]. The 'induction head' mechanism facilitates context-specific knowledge acquisition, enhancing adaptability to novel tasks [25]. Identifying a task diversity threshold during pretraining is crucial, enabling transformers to surpass Bayesian estimators [26]. Performance factors in ICL are influenced by example selection, vital for optimizing compositional generalization [27]. IDS uses zero-shot chain-of-thought reasoning for demonstration selection, enhancing ICL effectiveness [19]. Chain-of-thought prompting improves performance on challenging tasks, highlighting strategic prompt configurations [9]. Larger models perform linear classification in semantically unrelated label settings, demonstrating superior capabilities [6]. Retrieval-based ICL methods generate safer responses without additional training, enhancing reliability in sensitive applications [5]. Emergent abilities and performance factors underscore dependencies on pretraining data, example selection, and task configuration. Recent advancements in ICL and cross-task generalization improve AI model capabilities, enabling models to generate structured knowledge graphs and handle diverse tasks efficiently. Table provides a detailed overview of the representative benchmarks used to quantify the capabilities of large language models in the context of emergent abilities and performance factors. Benchmarks like BIG-bench quantify capabilities, revealing improvements and limitations as models scale, highlighting areas for future research to ensure models are prepared for various applications while mitigating social biases [21,28,29]. Advancements in In-context Learning Techniques Recent advancements in ICL techniques have enhanced language models through innovative frameworks, strategic selection processes, and empirical analyses. The IDS method employs zero-shot chain-of-thought reasoning for task-specific demonstration selection, enhancing ICL effectiveness [19]. Experiments on reasoning, question answering, and topic classification demonstrate IDS's superiority, emphasizing strategic example selection's importance. Empirical analyses reveal inconsistencies between ICL and gradient descent, providing insights into practical applications [30]. Symbol tuning significantly improves performance on ICL tasks, leveraging symbolic representations for adaptability [31]. RICL, a reweighted algorithm, fine-tunes models using an unbiased validation set to determine optimal weights, addressing training dataset bias [32]. Preconditioning matrix adaptation enhances ICL robustness, allowing models to handle diverse scenarios [8]. Evaluations show ICL can be achieved without ground truth demonstrations by leveraging structural aspects [33]. LENS filters datasets for informative examples, optimizing ICL performance [17]. SLEICL leverages strong models' abilities to improve weak models' performance [15]. Advancements in ICL techniques emphasize empirical analyses, symbol tuning, strategic example selection, and collaborative learning as pivotal factors in enhancing AI model performance. Retrieved demonstrations tailored to queries improve learning processes, reducing manual selection biases. Active example selection, enhanced through reinforcement learning, improves generalization to unseen tasks, particularly in smaller models like GPT-2. AdaICL optimizes selection by focusing on semantic diversity and uncertainty, yielding performance gains and budget efficiency. ICL's potential extends to applications like automatically generating knowledge graphs, highlighting foundation models' transformative impact across NLP tasks without parameter updates [21,22,34,35]. Prompt Engineering Strategies and Methodologies Prompt engineering employs diverse strategies to optimize input prompts, enhancing AI model performance across tasks. A key approach involves integrating multimodal data inputs, emphasizing comprehensive data integration for robust model understanding [16]. Chain of thought prompting enhances reasoning capabilities by incorporating intermediate steps, aiding tasks like structured knowledge graph generation and algorithmic reasoning [36,37,38,21,39]. Auto-CoT automates reasoning chain generation, reducing manual intervention and improving efficiency. The Automatic Prompt Engineer (APE) optimizes performance by generating and selecting instructions using an information-theoretic approach, maximizing mutual information between inputs and outputs, thus reducing reliance on labeled data [2,40]. Pretraining models on sequences of related documents enhances in-context learning, integrating manual design and optimization algorithms to improve performance in complex scenarios [41,42]. Selecting prompt templates by maximizing mutual information aligns models with tasks without labeled examples, achieving high accuracy without ground truth labels [2,40,43]. The PAC-based framework offers insights into in-context learning's learnability, optimizing prompt design for long-context scenarios [44,45]. These strategies reflect prompt engineering's dynamic evolution, emphasizing multimodal data integration, automation, and theoretical insights as key drivers in optimizing AI capabilities [29,39,46]. Impact on Model Performance Prompt engineering significantly impacts model performance by optimizing input prompts for desired responses. The KaRR framework organizes prompt design into discrete, continuous, few-shot, and zero-shot types, enhancing model performance [36,2]. As illustrated in , this figure highlights the various types of prompts, optimization techniques, and efficiency drivers that contribute to enhancing AI models. Automatic paraphrasing and backtranslation diversify linguistic structures, improving performance [43]. Chain of thought prompting enhances reasoning accuracy by facilitating deeper cognitive processing [47]. APE matches or exceeds traditional methods, optimizing performance across NLP tasks [48,46]. Mutual information-based prompt selection improves efficiency and model efficacy without extensive datasets [40]. These methodologies highlight prompt design's profound impact on AI model performance, emphasizing automation, linguistic diversity, and mutual information as key optimization drivers [2,40,46]. Challenges in Prompt Design Prompt design faces challenges in optimizing input prompts for desired AI model responses. Uncertainty about prompt structure and model familiarity complicates optimization [43]. Inherent biases influence model outputs, necessitating mitigation strategies [49]. Inefficiencies in current methods limit scalability and adaptability, relying heavily on manual effort [46,48]. Dependency on labeled data and model parameters restricts applicability and efficiency [40]. Addressing these challenges is crucial for advancing prompt engineering methodologies, aiming to align models with tasks without heavy reliance on labeled data or parameters. Techniques like maximizing mutual information and leveraging in-context learning can enhance AI capabilities and efficiency, addressing language model limitations highlighted by benchmarks like BIG-bench [21,28,40,39]. Recent Advancements and Innovations Recent advancements in prompt engineering have improved language models' ability to learn from context and generalize across tasks. The PICL framework leverages contextual information for enhanced task performance [50]. Linking prompt performance to perplexity provides a systematic approach for generating and selecting prompts [43]. APE optimizes task performance by combining manual design, optimization techniques, and evaluation methods [46,2]. Approximate algorithms for nearest neighbor search improve input context construction [27]. Auto-CoT automates reasoning chain generation, enhancing demonstration quality without manual input [48]. Information-Theoretic Prompt Selection (ITPS) optimizes prompt selection without labeled data, broadening applicability [40]. These advancements emphasize empirical insights, automation, and optimization techniques as key drivers in enhancing AI model capabilities, optimizing alignment with tasks without reliance on labeled data or model parameters [2,40,46]. Case Studies and Applications Prompt engineering has demonstrated effectiveness across various case studies and applications, transforming AI models in multiple domains. APE enhances NLP task accuracy and scalability, outperforming traditional methods with automatic instruction generation [43]. ITPS enables prompt selection without ground truth labels, achieving high accuracy [40]. Chain-of-thought prompting improves reasoning task accuracy, exemplified by a model achieving state-of-the-art performance on GSM8K math word problems using few exemplars [47,48]. These case studies highlight prompt engineering's versatility and effectiveness in advancing AI capabilities, equipping models to tackle diverse tasks with heightened accuracy and efficiency. Future work could explore applications across languages and domains, developing sophisticated prompt generation techniques [43]. Few-shot Learning Techniques to Enhance Few-shot Learning Few-shot learning in AI is enhanced through diverse methodologies optimizing model adaptation under limited data conditions. Iterative Demonstration Selection (IDS) refines few-shot learning by selecting examples relevant to the test sample's reasoning path, improving efficacy [19]. Teaching large language models (LLMs) to formulate and utilize algorithms expands adaptability across tasks. AI-generated labels combined with human input reduce data requirements, enhancing training efficiency. For instance, leveraging GPT-3 for pseudo labels with selective human annotations decreases labeling costs by up to 96 A diverse multimodal dataset serves as a resource for instruction tuning, crucial for enhancing few-shot learning. AdaICL employs a model-adaptive algorithm to select examples based on uncertainty and diversity, improving in-context learning (ICL) effectiveness and budget efficiency. Extensive testing shows a 4.4 MetaICL, a meta-training framework, tunes a pretrained language model for in-context learning across tasks, enabling efficient adaptation without parameter updates. By tuning across 142 NLP datasets, MetaICL surpasses traditional in-context learning, emphasizing the importance of a diverse meta-training set [21,3]. These techniques illustrate diverse approaches to enhance few-shot learning, emphasizing strategic example selection, skill composition, and meta-training frameworks as key drivers in optimizing performance in resource-constrained environments. Significance of Few-shot Learning Few-shot learning is crucial in AI, enabling models to generalize effectively from limited examples, thus mitigating extensive data annotation challenges [12]. It allows AI systems to extend applicability across tasks with minimal data input, leveraging in-context examples to enhance performance in tasks requiring nuanced understanding. Integrating multimodal data within few-shot learning frameworks enhances efficiency, enabling models to leverage diverse inputs for improved performance. This integration is essential for expanding few-shot learning applications, addressing limitations in traditional models reliant on word-based representations [28,22,51,52]. Frameworks like ICL-D3IE improve LLMs' ability to extract information from complex documents, emphasizing selective data annotation and strategic example selection [12]. Techniques leveraging pre-training on diverse tasks further enhance few-shot learning by improving models' ability to learn from limited examples. Few-shot learning reduces data requirements and enhances model generalization, advancing AI capabilities across domains. Techniques such as selective annotation enable language models to perform new tasks with minimal examples, achieving performance comparable to traditional fine-tuning methods at reduced annotation costs [53,54,55]. Multimodal Few-shot Learning Multimodal few-shot learning integrates diverse data modalities to enhance model performance in limited-example scenarios. This approach addresses challenges in traditional methods that fail to leverage frozen language models' capabilities with limited multimodal inputs [52]. Integrating multimodal data facilitates comprehensive understanding and response generation, expanding few-shot learning applicability across domains. Query-aware demo generation allows flexible adaptation of LLMs to new tasks, enhancing performance on out-of-domain queries [56]. This method optimizes model adaptability in handling novel tasks. Integrating few-shot learning across modalities underscores the need for frameworks combining visual and textual inputs, enhancing models' generalization from minimal examples [21,53,29,57]. Selective annotation enables high performance with lower costs, demonstrated by a 12.9 These advancements highlight multimodal few-shot learning's impact on AI capabilities, emphasizing query-aware demo generation and diverse data modalities as key drivers in optimizing performance. Enhancing methodologies is crucial for advancing AI systems to tackle complex tasks accurately and efficiently across domains [28,29,37,21,39]. Enhancements in Few-shot Learning Recent advancements in few-shot learning enhance AI models' ability to generalize from minimal data through innovative methodologies and scaling strategies. An intermediate self-supervision stage improves few-shot learning tasks, emphasizing self-supervised learning's role in enhancing model adaptability [54]. In multimodal learning, developing a vision encoder for processing by a frozen language model facilitates few-shot learning in multimodal contexts [52]. The Flamingo model exemplifies state-of-the-art performance by leveraging multimodal inputs [58]. Scaling language models improves few-shot learning capabilities, demonstrated by models like PaLM with 540 billion parameters, achieving state-of-the-art results on language benchmarks [59,60]. Instruction fine-tuning dramatically improves performance, as seen with Flan-PaLM 540B, outperforming PaLM 540B across tasks. Scaling strategies reduce dependency on labeled datasets, facilitating efficient training. Self-Demos outperform existing methods in out-of-domain settings, showcasing strategic demonstration generation's effectiveness in enhancing LLMs' generalization [56]. Influence-based example selection provides quantitative assessment, enhancing few-shot learning efficacy by aligning examples with task outcomes [44]. These advancements highlight few-shot learning's dynamic evolution, showcasing self-supervised training stages, multimodal integration, scaling strategies, and strategic example selection as pivotal roles. These elements enhance AI model performance and generalization across domains, optimizing example selection and improving task performance with minimal annotation costs [22,53,54,55]. Applications in Natural Language Processing Applications in Natural Language Processing Recent advancements in natural language processing (NLP) have been significantly driven by methodologies such as in-context learning, prompt engineering, and few-shot learning, which enable models to handle complex linguistic tasks with minimal examples. In-context learning enhances model generalization across diverse applications, especially when specific training data distributions are utilized. For example, retrieval-based in-context learning has improved the safety and factual grounding of conversational agents, as demonstrated by LaMDA's success in addressing dialog challenges [13]. The ICL-D3IE framework has also shown improvements in document information extraction, enhancing models like Davinci-003/ChatGPT [12]. Prompt engineering, exemplified by the Automatic Prompt Engineer (APE), surpasses prior LLM baselines and achieves performance comparable to human-generated instructions across various tasks, demonstrating strong zero-shot capabilities on unseen multimodal tasks [1]. Self-Instruct has further improved the instruction-following abilities of pretrained models, achieving results akin to those trained with human annotations [14]. Few-shot learning methodologies have advanced machine comprehension and language generation, notably through transformers. Models like PaLM have excelled in multi-step reasoning tasks, while the SLEICL method consistently enhances weaker models' capabilities [15]. The use of GPT-3 for data labeling offers substantial cost reductions compared to human labeling, highlighting the economic benefits of few-shot learning [4]. The MGSM benchmark, with its 250 grade-school math problems, illustrates the diverse linguistic structures and reasoning challenges these methodologies can address [11]. These advancements reflect the dynamic evolution of NLP methodologies, driven by empirical insights and optimization strategies that enhance AI model capabilities. Continuous refinement of AI approaches is crucial for developing systems that manage complex language tasks with increased accuracy and efficiency. Leveraging resources like the Stanford Natural Language Inference corpus enhances semantic representation by providing a rich collection of human-annotated sentence pairs. Benchmarks such as the Beyond the Imitation Game (BIG-bench) are essential for assessing language models' capabilities and limitations, improving performance while addressing challenges like social bias [28,37]. Multimodal Applications in NLP The integration of in-context learning, prompt engineering, and few-shot learning in multimodal NLP tasks has significantly enhanced AI models' ability to process and understand diverse data types, including text and images. Large language models (LLMs) employing various prompt design strategies—such as discrete, continuous, few-shot, and zero-shot techniques—optimize performance through manual design and rigorous evaluation. These methodologies enable the generation of structured knowledge graphs from unstructured text and enhance models' adaptability to new tasks with minimal computational resources by retrieving optimal prompts tailored to specific queries, reducing biases and improving efficiency across numerous NLP applications [40,42,21,2,22]. In few-shot learning, MetaICL exemplifies the integration of multimodal data to boost performance across tasks, approaching the effectiveness of fully finetuned models and providing a robust framework for adapting to novel tasks with limited examples [3]. This capability is particularly beneficial in multimodal contexts, where diverse data integration is crucial for achieving high accuracy and reliability. Exploring diverse datasets for training retrievers in multimodal contexts presents a promising avenue for future research, potentially enhancing adaptability and applicability across a broader range of NLP tasks [42]. Incorporating varied datasets not only improves model performance but also ensures AI systems can address a wide array of real-world scenarios, increasing their utility and effectiveness. These advancements underscore the transformative impact of integrating in-context learning, prompt engineering, and few-shot learning techniques in multimodal NLP tasks. Ongoing refinement of methodologies is set to significantly enhance AI capabilities, particularly in processing and understanding complex multimodal inputs with greater precision and efficiency. This includes advancements in generating structured knowledge graphs from text, leveraging foundation models with billions of parameters for tasks like named entity extraction and relation linking, and employing in-context learning to minimize computational resource requirements. The development of large annotated corpora, such as the Stanford Natural Language Inference corpus, further improves semantic representations for natural language inference. Additionally, the integration of language models with multimodal perception, as demonstrated by Kosmos-1, facilitates a convergence of language understanding, multimodal dialogue, and vision tasks, advancing artificial general intelligence through cross-modal knowledge transfer [21,51,37]. Conclusion The survey highlights the significant advancements in artificial intelligence brought about by in-context learning, prompt engineering, and few-shot learning, particularly within the realm of natural language processing (NLP). These approaches have proven to enhance model adaptability and generalization, contributing to improved performance across a wide array of tasks. The scaling of large language models (LLMs), exemplified by models such as GPT-4, showcases their potential to achieve high levels of performance, rivaling human expertise and demonstrating versatility across multiple domains. Future research should focus on refining self-supervised objectives and exploring scalability to optimize few-shot learning techniques. Investigating in-context learning in complex datasets and identifying additional influencing factors will further enhance the application of these methodologies. Expanding prompt graph representations and exploring diverse graph types can broaden the scope of prompt engineering. Additionally, optimizing model architectures and training paradigms to boost generalization remains a crucial area for development. The MGSM benchmark offers valuable insights into the multilingual reasoning capabilities of language models, indicating the need for future studies to integrate complex semantic relationships to overcome current limitations. Exploring dual formulations for larger datasets and various neural network architectures could unlock new potentials in AI development. Refining benchmarks to better assess emerging capabilities and address limitations is essential. Collectively, these directions underscore the potential of advanced techniques to overcome existing challenges, ensuring that models are not only high-performing but also adaptable and reliable across diverse tasks and domains. Further research should investigate diverse corpus combinations, delve into in-context learning mechanisms, and explore emerging trends in language model training. Enhancing LLMs' proof planning abilities and expanding datasets to encompass complex reasoning tasks are also promising avenues. Addressing challenges in commonsense question answering and integrating commonsense knowledge will be crucial for future model improvements. Additionally, insights from in-context learning should be applied to non-linear models and various neural architectures, while enhancing Painter's capabilities for specialized tasks. Future studies should focus on optimizing ground-truth label effectiveness and robustness in in-context learning, exploring its applicability across fields, and improving pseudo label quality generated by GPT-3.",
  "reference": {
    "1": "2209.00647v1",
    "2": "2309.13205v1",
    "3": "2110.15943v2",
    "4": "2212.10450v2",
    "5": "2302.00871v3",
    "6": "2303.03846v2",
    "7": "2108.13487v1",
    "8": "2306.00297v2",
    "9": "2210.09261v1",
    "10": "2212.10773v3",
    "11": "2210.03057v1",
    "12": "2303.05063v4",
    "13": "2201.08239v3",
    "14": "2212.10560v2",
    "15": "2401.03385v2",
    "16": "2303.08774v6",
    "17": "2302.13539v3",
    "18": "2305.09731v1",
    "19": "2310.09881v4",
    "20": "2303.07971v1",
    "21": "2305.08804v1",
    "22": "2401.11624v5",
    "23": "2305.14160v4",
    "24": "2303.07895v1",
    "25": "2306.00802v2",
    "26": "2306.15063v2",
    "27": "2310.10638v6",
    "28": "2206.04615v3",
    "29": "2104.08773v4",
    "30": "2310.08540v5",
    "31": "2305.08298v2",
    "32": "2310.03331v1",
    "33": "2202.12837v2",
    "34": "2211.04486v1",
    "35": "2310.20046v1",
    "36": "2305.10519v2",
    "37": "1508.05326v1",
    "38": "2210.01240v4",
    "39": "2211.09066v1",
    "40": "2203.11364v1",
    "41": "2304.04748v2",
    "42": "2112.08633v2",
    "43": "2212.04037v2",
    "44": "2302.11042v2",
    "45": "2405.00200v2",
    "46": "2211.01910v2",
    "47": "2201.11903v6",
    "48": "2210.03493v1",
    "49": "2102.09690v2",
    "50": "2305.09137v1",
    "51": "2209.01975v1",
    "52": "2005.14165v4",
    "53": "2302.14045v2",
    "54": "2106.13884v2",
    "55": "2205.01703v2",
    "56": "2404.00884v1",
    "57": "2205.10782v1",
    "58": "2204.14198v2",
    "59": "2204.02311v5",
    "60": "2210.11416v5"
  },
  "chooseref": {
    "1": "2401.11624v5",
    "2": "2305.09731v1",
    "3": "2305.09731v1",
    "4": "1508.05326v1",
    "5": "2304.09960v3",
    "6": "2305.03726v2",
    "7": "2309.13205v1",
    "8": "2303.07971v1",
    "9": "2211.04486v1",
    "10": "2111.02080v6",
    "11": "2203.11364v1",
    "12": "2210.03493v1",
    "13": "2206.04615v3",
    "14": "2306.00802v2",
    "15": "2102.09690v2",
    "16": "2201.11903v6",
    "17": "2210.09261v1",
    "18": "1509.01626v3",
    "19": "1811.00937v2",
    "20": "2302.05698v3",
    "21": "2104.08773v4",
    "22": "2205.05055v6",
    "23": "2212.04037v2",
    "24": "2310.08540v5",
    "25": "2104.08164v2",
    "26": "2206.07682v2",
    "27": "2304.04748v2",
    "28": "2305.08804v1",
    "29": "2302.13539v3",
    "30": "2310.03331v1",
    "31": "2204.14198v2",
    "32": "2303.08774v6",
    "33": "2401.03385v2",
    "34": "2205.12685v2",
    "35": "2305.04835v3",
    "36": "2303.05063v4",
    "37": "2212.02499v2",
    "38": "2205.01703v2",
    "39": "2302.11042v2",
    "40": "2310.09881v4",
    "41": "2405.00200v2",
    "42": "2310.10638v6",
    "43": "2205.10782v1",
    "44": "2212.10450v2",
    "45": "1806.03822v1",
    "46": "2305.14160v4",
    "47": "2201.08239v3",
    "48": "2302.14045v2",
    "49": "2005.14165v4",
    "50": "2206.06336v1",
    "51": "2210.01240v4",
    "52": "2210.03057v1",
    "53": "2211.01910v2",
    "54": "2303.03846v2",
    "55": "2112.08633v2",
    "56": "2305.13299v1",
    "57": "2110.15943v2",
    "58": "2212.10773v3",
    "59": "2305.05940v3",
    "60": "2106.13884v2",
    "61": "2204.13509v2",
    "62": "2307.03576v1",
    "63": "2212.12017v3",
    "64": "2305.12600v1",
    "65": "2204.02311v5",
    "66": "2305.09137v1",
    "67": "2311.00871v1",
    "68": "2306.15063v2",
    "69": "2202.12837v2",
    "70": "2210.11416v5",
    "71": "2209.01975v1",
    "72": "2404.00884v1",
    "73": "2212.10560v2",
    "74": "2303.03926v1",
    "75": "2305.10519v2",
    "76": "1905.00537v3",
    "77": "2305.08298v2",
    "78": "2211.09066v1",
    "79": "2304.13276v1",
    "80": "2202.05798v2",
    "81": "2303.07895v1",
    "82": "2212.07677v2",
    "83": "2306.00297v2",
    "84": "2302.00871v3",
    "85": "2304.08485v2",
    "86": "2209.00647v1",
    "87": "2108.13487v1",
    "88": "2211.15661v3",
    "89": "2310.20046v1"
  }
}