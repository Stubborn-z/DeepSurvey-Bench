{
  "authors": [
    "Xiangtai Li",
    "Henghui Ding",
    "Wenwei Zhang",
    "Haobo Yuan",
    "Jiangmiao Pang",
    "Guangliang Cheng",
    "Kai Chen",
    "Ziwei Liu",
    "Chen Change Loy"
  ],
  "literature_review_title": "Transformer-Based Visual Segmentation: A Survey",
  "year": "2023",
  "date": "2023-04-19",
  "category": "cs.CV",
  "abstract": "Visual segmentation seeks to partition images, video frames, or point clouds\ninto multiple segments or groups. This technique has numerous real-world\napplications, such as autonomous driving, image editing, robot sensing, and\nmedical analysis. Over the past decade, deep learning-based methods have made\nremarkable strides in this area. Recently, transformers, a type of neural\nnetwork based on self-attention originally designed for natural language\nprocessing, have considerably surpassed previous convolutional or recurrent\napproaches in various vision processing tasks. Specifically, vision\ntransformers offer robust, unified, and even simpler solutions for various\nsegmentation tasks. This survey provides a thorough overview of\ntransformer-based visual segmentation, summarizing recent advancements. We\nfirst review the background, encompassing problem definitions, datasets, and\nprior convolutional methods. Next, we summarize a meta-architecture that\nunifies all recent transformer-based approaches. Based on this\nmeta-architecture, we examine various method designs, including modifications\nto the meta-architecture and associated applications. We also present several\nclosely related settings, including 3D point cloud segmentation, foundation\nmodel tuning, domain-aware segmentation, efficient segmentation, and medical\nsegmentation. Additionally, we compile and re-evaluate the reviewed methods on\nseveral well-established datasets. Finally, we identify open challenges in this\nfield and propose directions for future research. The project page can be found\nat https://github.com/lxtGH/Awesome-Segmentation-With-Transformer. We will also\ncontinually monitor developments in this rapidly evolving field.",
  "structure": [
    {
      "section_title": "Introduction/Pre-section",
      "level": "1",
      "content": "\\documentclass[10pt,journal,compsoc]{IEEEtran} \\ifCLASSOPTIONcompsoc \\usepackage[nocompress]{cite} \\else cite \\fi \\ifCLASSINFOpdf \\else \\fi url times epsfig graphicx amsmath amssymb multirow colortbl color threeparttable booktabs adjustbox subfig caption \\usepackage[misc]{ifsym} xcolor,colortbl makecell \\newcommand*{\\belowrulesepcolor}[1]{% % \\kern-\\belowrulesep \\begingroup \\color{#1% \\hrule height\\belowrulesep \\endgroup -0.03mm }% } \\newcommand*{\\aboverulesepcolor}[1]{% % \\vspace{-0.03mm \\begingroup #1% \\hrule height\\aboverulesep \\endgroup \\kern-\\aboverulesep }% } \\vspacefigtext{-3mm} \\vspacesection{-2.2mm} \\pp{p} \\RR{R} \\FF{F} \\XX{X} \\YY{Y} \\hatd{d} \\haty{y} \\hatp{p} \\hatm{m} \\hatc{c} \\noobject{\\varnothing} \\denc{d_{\\rm enc}} \\ddec{d_{\\rm dec}} \\Re{R} \\hy{y} \\hb{b} \\hp{p} \\ty{y} pifont% http://ctan.org/pkg/pifont \\cmark{51}% \\xmark{55}% xspace \\makeatletter \\DeclareRobustCommand\\futurelet\\@let@token\\@onedot \\def\\@onedot{\\ifx\\@let@token.\\else.\\null\\fi\\xspace} \\def\\emph{e.g\\onedot} \\def\\emph{E.g\\onedot} \\def\\emph{i.e\\onedot} \\def\\emph{I.e\\onedot} \\def\\emph{c.f\\onedot} \\def\\emph{C.f\\onedot} \\def\\emph{etc\\onedot} \\def\\emph{vs\\onedot} \\defw.r.t\\onedot \\defd.o.f\\onedot \\def\\emph{et al\\onedot} \\usepackage[linesnumbered,lined,boxed,commentsnumbered,ruled]{algorithm2e} \\usepackage[pagebackref,breaklinks=true,colorlinks,citecolor=blue,urlcolor=blue,linkcolor=blue,bookmarks=false]{hyperref} \\def\\textit{et al.} \\cavan[1]{{magenta(cavan: {#1})}} % cavan's comments \\lxt[1]{{red(lxt: {#1})}} % xiangtai li's comments \\dingh[1]{{magenta(Ding: {#1})}} % henghui's comments \\zww[1]{{cyan(zww: {#1})}} %zww's comments \\ck[1]{{blue(kaichen: {#1})}} % kaichen's comments \\jiangmiao[1]{{blue(jiangmiao: {#1})}} %Jiangmiao's comments \\haobo[1]{{violet(haobo): {#1})}} % haobo's comments \\gl[1]{{blue(guangliang: {#1})}} % guangliang's comments \\newcommandbookmarks=true,bookmarksnumbered=true, pdfpagemode={UseOutlines,plainpages=false,pdfpagelabels=true, colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black}, pdftitle={Dynamic Spatial Temporal Learning via Temporal Pyramid Routing},%<!CHANGE! pdfsubject={Typesetting},%<!CHANGE! pdfauthor={Michael D. Shell},%<!CHANGE! pdfkeywords={Computer Society, IEEEtran, journal, LaTeX, paper, template}}%<^!CHANGE! op-tical net-works semi-conduc-tor document Transformer-Based Visual Segmentation:\\\\ A Survey Xiangtai Li, Henghui Ding, Haobo Yuan, Wenwei Zhang, Jiangmiao Pang, \\\\ Guangliang Cheng, Kai Chen, Ziwei Liu, Chen Change Loy \\IEEEcompsocitemizethanks{\\IEEEcompsocthanksitem X. Li, H. Ding, W. Zhang, H. Yuan, and C. Loy are with the S-Lab, Nanyang Technological University, Singapore. xiangtai94@gmail.com, henghui.ding@gmail.com, \\{wenwei.zhang, ccloy\\@ntu.edu.sg. \\IEEEcompsocthanksitem J. Pang and K. Chen are with Shanghai AI Laboratory, Shanghai, China. pangjiangmiao@gmail.com, chenkai@pjlab.org.cn. \\IEEEcompsocthanksitem G. Cheng is with the University of Liverpool, UK. \\IEEEcompsocthanksitem Corresponding: Guangliang Cheng.(Guangliang.Cheng@liverpool.ac.uk) }% <-this % stops a space } IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE {Shell \\textit{et al.}: Bare Advanced Demo of IEEEtran.cls for IEEE Computer Society Journals} \\begin{IEEEkeywords Vision Transformer Review, Dense Prediction, Image Segmentation, Video Segmentation, Scene Understanding IEEEkeywords} \\maketitle \\IEEEdisplaynontitleabstractindextext \\IEEEpeerreviewmaketitle",
      "origin_cites_number": 0
    },
    {
      "section_title": "Introduction",
      "level": "1",
      "content": "V{isual} segmentation aims to group pixels of the given image or video into a set of semantic regions. It is a fundamental problem in computer vision and involves numerous real-world applications, such as robotics, automated surveillance, image/video editing, social media, autonomous driving, etc. Starting from the hand-crafted features~malik2001contour,shi2000normalized and classical machine learning models~stella2003multiclass,schroff2008object,kass1988snakes, segmentation problems have been involved with a lot of research efforts. During the last ten years, deep neural networks, Convolution Neural Networks (CNNs)~russakovsky2015imagenet,resnet,simonyan2014very, such as Fully Convolutional Networks (FCNs)~long2015fully,chen2017deeplab,zhao2017pyramid,ding2018context have achieved remarkable successes for different segmentation tasks and led to much better results. Compared to traditional segmentation approaches, CNNs based approaches have better generalization ability. Because of their exceptional performance, CNNs and FCN architecture have been the basic components in the segmentation research works. Recently, with the success of natural language processing (NLP), transformer~vaswani2017attention is introduced as a replacement for recurrent neural networks~LSTM. Transformer contains a novel self-attention design and can process various tokens in parallel. Then, based on transformer design, BERT~BERT and GPT-3~GPT3 scale the model parameters up and pre-train with huge unlabeled text information. They achieve strong performance on many NLP tasks, accelerating the development of transformers into the vision community. Recently, researchers applied transformers to computer vision (CV) tasks. Early methods~zhao2018psanet,wang2018nonlocal combine the self-attention layers to augment CNNs. Meanwhile, several works~zhao2020exploring,hu2019local used pure self-attention layers to replace convolution layers. After that, two remarkable methods boost the CV tasks. One is vision transformer (ViT)~VIT, which is a pure transformer that directly takes the sequences of image patches to classify the full image. It achieves state-of-the-art performance on multiple image recognition datasets. Another is detection transformer (DETR)~detr, which introduces the concept of object query. Each object query represents one instance. The object query replaces the complex anchor design in the previous detection framework, which simplifies the pipeline of detection and segmentation. Then, the following works adopt improved designs on various vision tasks, including representation learning~liu2021swin,MaskedAutoencoders2021, object detection~zhu2020deformabledetr, segmentation~wang2020maxDeeplab, low-level image processing~chen2021pre, video understanding~bertasius2021space, 3D scene understanding~point_transformer, and image/video generation~pan20213d. As for visual segmentation, recent state-of-the-art methods are all based on transformer architecture. Compared with CNN approaches, most transformer-based approaches have simpler pipelines but stronger performance. Because of a rapid upsurge in transformer-based vision models, there are several surveys on vision transformer~han2022survey,khan2022transformers,lin2022survey. However, most of them mainly focus on general transformer design and its application on several specific vision tasks~selva2022videotransformer_survey,lahoud20223d_transformer_survey,xu2022multimodal. Meanwhile, there are previous surveys on the deep-learning-based segmentation~minaee2021image,hao2020brief,zhou2023survey. However, to the best of our knowledge, there are no surveys focusing on using vision transformers for visual segmentation or query-based object detection. We believe it would be beneficial for the community to summarize these works and keep tracking this evolving field. figure*[!t] \\centering \\includegraphics[width=0.95\\linewidth]{figs/survey_pipline_new.pdf} A diagram that summarizes this survey. Different colors represent specific sections. Best viewed in color. figure* 0.5cm \\noindent$\\bullet$ Contribution. In this survey, we systematically introduce recent advances in transformer-based visual segmentation methods. We start by defining the task, datasets, and CNN-based approaches and then move on to transformer-based approaches, covering existing methods and future work directions. Our survey groups existing representative works from a more technical perspective of the method details. In particular, for the main review part, we first summarize the core framework of existing approaches into a meta-architecture in Sec.~sec:method_meta, which is an extension of DETR~detr. By changing the components of the meta-architecture, we divide existing approaches into six categories in Sec.~sec:method_categorization, including Representation Learning, Interaction Design in Decoder, Optimizing Object Query, Using Query For Association, and Conditional Query Generation. Moreover, we also survey closely related specific subfields, including point cloud segmentation, tuning foundation models, domain-aware segmentation, data/model efficient segmentation, class agnostic segmentation and tracking, and medical segmentation. We also evaluate the performance of influential works published in top-tier conferences and journals on several widely used segmentation benchmarks. Additionally, we provide an overview of previous CNN-based models and relevant literature in other areas, such as object detection, object tracking, and referring segmentation in the background section. \\noindent$\\bullet$ Scope. This survey will cover several mainstream segmentation tasks, including semantic segmentation, instance segmentation, panoptic segmentation, and their variants, such as video and point cloud segmentation. Additionally, we cover related subfields in Sec.~sec:method_downstream_beyond. We focus on transformer-based approaches and only review a few closely related CNN-based approaches for reference. Although there are many preprints or published works, we only include the most representative works. \\noindent$\\bullet$ Organization. The rest of the survey is organized as follows. Overall, Fig.~fig:survey_pipline shows the pipeline of our survey. We first introduce the background knowledge on problem definition, datasets, and CNN-based approaches in Sec.~sec:background. Then, we review representative papers on transformer-based segmentation methods in Sec.~sec:method_survey and Sec.~sec:method_downstream_beyond. We compare the experiment results in Sec.~sec:benchmark. Finally, we raise the future directions in Sec.~sec:future_work and conclude the survey in Sec.~sec:conclusion. We provide more benchmarks and details in the appendix. -2mm",
      "origin_cites_number": 23
    },
    {
      "section_title": "Background",
      "level": "1",
      "content": "In this section, we first present a unified problem definition of different segmentation tasks. Then, we detail the common datasets and evaluation metrics. Next, we present a summary of previous approaches before the transformer. Finally, we present a review of basic concepts in transformers. To facilitate understanding of this survey, we list the brief notations in Tab.~tab:conception_notation for reference.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Problem Definition",
      "level": "2",
      "content": "\\noindent$\\bullet$ Image Segmentation. Given an input image $ I \\in R^{H\\times {W}\\times 3}$, the goal of image segmentation is to output a group of masks $\\{y_i\\}_{i=1}^G = \\{(m_i, c_i)\\}_{i=1}^G \\,$ where $c_i$ denotes the ground truth class label of the binary mask $m_i$ and $G$ is the number of masks, ${H}\\times {W}$ are the spatial size. According to the scope of class labels and masks, image segmentation can be divided into three different tasks, including semantic segmentation (SS), instance segmentation (IS), and panoptic segmentation (PS), as shown in Fig.~fig:seg_tasks (a). For SS, the classes may be foreground objects (thing) or background (stuff), and each class only has one binary mask that indicates the pixels belonging to this class. Each SS mask does not overlap with other masks. For IS, each class may have more than one binary mask, and all the classes are foreground objects. Some IS masks may overlap with others. For PS, depending on the class definition, each class may have a different number of masks. For the countable thing class, each class may have multiple masks for different instances. For the uncountable stuff class, each class only has one mask. Each PS mask does not overlap with other masks. One can understand image segmentation from the pixel view. Given an input $I\\inR^{H\\times {W}\\times 3}$, the output of image segmentation is a two-channel dense segmentation map $S= \\{k_{j},c_{j}\\}_{j=1}^{H\\times W}$. In particular, $k$ indicates the identity of the pixel $j$, and $c$ means the class label of pixel $j$. For SS, the identities of all pixels are zero. For IS, each instance has a unique identity. For PS, the pixels belonging to the thing classes have a unique identity. The pixel identities of the stuff class are zero. From both two perspectives, the PS unifies both SS and IS. We present the visual examples in Fig.~fig:seg_tasks. \\noindent$\\bullet$ Video Segmentation. Given a video clip input as $ V \\in R^{T\\times H\\times {W}\\times 3}$, where $T$ represents the frame number, the goal of video segmentation is to obtain a mask tube $\\{y_i\\}_{i=1}^N = \\{(m_i, c_i)\\}_{i=1}^N \\,$, where $N$ is the number of the tube masks $m_i \\in {\\{0,1\\}}^{{T}\\times {H}\\times {W}}$, and $c_i$ denotes the class label of the tube $m_i$. Video panoptic segmentation (VPS) requires temporally consistent segmentation and tracking results for each pixel. Each tube mask can be classified into countable thing classes and countless stuff classes. Each thing tube mask also has a unique ID for evaluating tracking performance. For stuff masks, the tracking is zero by default. When $N=C$ and the task only contains stuff classes, and all thing classes have no IDs, VPS turns into video semantic segmentation (VSS). If ${\\{y_i\\}_{i=1}^N}$ overlap and $C$ only contains the thing classes and all stuff classes are ignored, VPS turns into video instance segmentation (VIS). We present the visual examples that summarize the difference among VPS, VIS, and VSS with $T=2$ in Fig.~fig:seg_tasks (b). \\noindent$\\bullet$ Related Problems. Object detection and instance-wise segmentation (IS/VIS/VPS) are closely related tasks. Object detection involves predicting object bounding boxes, which can be considered a coarse form of IS. After introducing the DETR model, many works have treated object detection and IS as the same task, as IS can be achieved by adding a simple mask prediction head to object detection. Similarly, video object detection (VOD) aims to detect objects in every video frame. In our survey, we also examine query-based object detectors for both object detection and VOD. Point cloud segmentation is another segmentation task, where the goal is to segment each point in a point cloud into pre-defined categories. We can apply the same definitions of semantic segmentation, instance segmentation, and panoptic segmentation to this task, resulting in point cloud semantic segmentation (PCSS), point cloud instance segmentation (PCIS), and point cloud panoptic segmentation (PCPS). Referring segmentation is a task that aims to segment objects described in natural language text input. There are two subtasks in referring segmentation: referring image segmentation (RIS), which performs language-driven segmentation, and referring video object segmentation (RVOS), which segments and tracks a specific object in a video based on required text inputs. Finally, video object segmentation (VOS) involves tracking an object in a video by predicting pixel-wise masks in every frame, given a mask of the object in the first frame.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Datasets and Metrics",
      "level": "2",
      "content": "table*[t] \\centering \\tiny Commonly used datasets and metric for Transformer-based segmentation-2mm adjustbox{width=1.\\textwidth} tabular{c c c c c c} \\toprule[0.15em] Dataset & Samples (train/val) & Task & Evaluation Metrics & Characterization \\\\ \\toprule[0.15em] Pascal VOC~pascalvoc & 1,464 / 1,449 & SS & mIoU & PASCAL Visual Object Classes (VOC) 2012 dataset contains 20 object categories. \\\\ Pascal Context~pascalcontext & 4,998 / 5,105 & SS & mIoU & PASCAL Context dataset is an extension of PASCAL VOC containing 400+ classes (usually 59 most frequently). \\\\ COCO~coco_dataset & 118k / 5k & SS / IS / PS & mIoU / mAP / PQ & MS COCO dataset is a large-scale dataset with 80 thing categories and 91 stuff categories.\\\\ ADE20k~ADE20K & 20,210 / 2,000 & SS / IS / PS & mIoU / mAP / PQ & ADE20k dataset is a large-scale dataset exhaustively annotated with pixel-level objects and object part labels. \\\\ Cityscapes~cordts2016cityscapes & 2,975 / 500 & SS / IS / PS & mIoU / mAP / PQ & Cityscapes dataset focuses on semantic understanding of urban street scenes, captured in 50 cities.\\\\ Mapillary~neuhold2017mapillary & 18k / 2k & SS / PS & mIoU / PQ & Mapillary dataset is a large-scale dataset with accurate high-resolution annotations. \\\\ RefCOCO~RefCOCO & 42k / 4k & RIS & mIoU & A large-scale dataset for classic reference segmentation based on the COCO. \\\\ gRefCOCO~GRES & 79k / 8k & RIS & mIoU & A large-scale dataset for generalized referring segmentation based on the COCO. \\\\ \\hline VSPW~miao2021vspw & 2,906 / 343 & VSS & mIoU & VPSW is a large-scale high-resolution dataset with long videos focusing on VSS.\\\\ Youtube-VIS-2019~vis_dataset & 2,238 / 302 & VIS & AP & Extending from Youtube-VOS, Youtube-VIS is with exhaustive instance labels.\\\\ OVIS~OVIS & 607 / 140 & VIS & AP & A large-scale occluded video instance segmentation benchmark.\\\\ VIP-Seg~miao2022large & 2,806 / 343 & VPS & VPQ \\& STQ & Extending from VSPW, VIP-Seg adds extra instance labels for VPS task.\\\\ Cityscape-VPS~kim2020vps & 2,400 / 300 & VPS & VPQ & Cityscapes-VPS dataset extracts from the val split of Cityscapes dataset, adding temporal annotations.\\\\ KITTI-STEP~STEP & 5,027 / 2,981 & VPS & STQ & KITTI-STEP focuses on the long videos in the urban scenes. \\\\ DAVIS-2017~davis2017& 4,219 / 2,023 & VOS & J / F / J\\&F& DAVIS focuses on video object segmentation. \\\\ Youtube-VOS~vos2018& 3,471 / 474 & VOS & J / F / J\\&F& A large-scale video object segmentation benchmark. \\\\ MOSE~MOSE& 1,507 / 311 & VOS & J / F / J\\&F& Tracking and segmenting objects in complex environments.\\\\ MeViS~MeViS& 1,712 / 140 & RVOS & J / F / J\\&F& Tracking and segmenting target objects referred by motion expressions.\\\\ \\bottomrule[0.1em] tabular adjustbox table* \\noindent$\\bullet$ Commonly Used Datasets. For image segmentation, the most commonly used datasets are COCO~coco_dataset, ADE20k~ADE20K and Cityscapes~cordts2016cityscapes. For video segmentation, the most used datasets are VSPW~miao2021vspw and Youtube-VIS~vis_dataset. We will compare several dataset results in Sec.~sec:benchmark. More datasets are listed in the Tab.~tab:dataset_summary. \\noindent$\\bullet$ Common Metric. For SS and VSS, the commonly used metric is mean intersection over union (mIoU), which calculates the pixel-wised Union of Interest between output image and video masks and ground truth masks. For IS, the metric is mask mean average precision (mAP), which is extended from the object detection via replacing box IoU with mask IoU. For VIS, the metric is 3D mAP, which extends mask mAP in a spatial-temporal manner. For PS, the metric is the panoptic quality (PQ), which unifies both thing and stuff prediction by setting a fixed threshold 0.5. For VPS, the commonly used metrics are video panoptic quality (VPQ) and segmentation tracking quality (STQ). The former extends PQ into temporal window calculation, while the latter decouples the segmentation and tracking in a per-pixel-wised manner. Note that there are other metrics, including pixel accuracy and temporal consistency. For simplicity, we only report the primary metrics used in the literature. We present the detailed formulation of these metrics in the supplementary material.",
      "origin_cites_number": 23
    },
    {
      "section_title": "Segmentation Approaches Before Transformer",
      "level": "2",
      "content": "\\noindent$\\bullet$ Semantic Segmentation. Prior to the emergence of {ViT} and {DETR}. SS was typically approached as a dense pixel classification problem, as initially proposed by FCN. Then, the following works are all based on the FCN framework. These methods can be divided into the following aspects, including better encoder-decoder frameworks~yu2018learning,ding2020semantic, larger kernels~peng2017large,SVCNet, multiscale pooling~zhao2017pyramid,deeplabv3, multiscale feature fusion~ding2018context,shuai2018toward,li2020gated,li2021global, non-local modeling~wang2018nonlocal,ocrnet,zhangli_dgcn, efficient modeling~sfnet,Li2022SFNetFA,BiSeNet, and better boundary delineation~kirillov2020pointrend,BoundaryAware,li2020improving,ebl_he_iccv. After the transformer was proposed, with the goal of global context modeling, several works design variants of self-attention operators to replace the CNN prediction heads~DAnet,ocrnet. \\noindent$\\bullet$ Instance Segmentation. IS aims to detect and segment each object, which goes beyond object detection. Most IS approaches focus on how to represent instance masks beyond object detection, which can be divided into two categories: top-down approaches~maskrcnn,tian2020conditional and bottom-up approaches~neven2019instanceSeg,de2017semanticInstanceLoss. The former extends the object detector with an extra mask head. The designs of mask heads are various, including FCN heads~maskrcnn,htc, diverse mask encodings~zhang2020MEInst, and dynamic kernels~tian2020conditional,bolya2019yolact. The latter performs instance clustering from semantic segmentation maps to form instance masks. The performance of top-down approaches is closely related to the choice of detector ~qiao2021detectors, while bottom-up approaches depend on both semantic segmentation results and clustering methods~cheng2020panoptic. Besides, there are also several approaches~chen2019tensormask,wang2020solov2 using gird representation to learn instance masks directly. The ideas using kernels and different mask encodings are also extended into several transformer-based approaches, which will be detailed in Sec.~sec:method_survey. \\noindent$\\bullet$ Panoptic Segmentation. Previous works for PS mainly focus on how to fuse the results of both SS and IS, which treats PS as two independent tasks. Based on IS subtask, the previous works can also be divided into two categories: top-down approaches~xiong2019upsnet,li2020panopticFCN and bottom-up approaches~axialDeeplab,cheng2020panoptic, according to the way to generate instance masks. Several works use a shared backbone with multitask heads to jointly learn IS and SS, focusing on mutual task association. Meanwhile, several bottom-up approaches~axialDeeplab,cheng2020panoptic use the sequential pipeline by performing instance clustering from semantic segmentation results and then fusing both. In summary, most PS methods include complex pipelines and are highly engineered. \\noindent$\\bullet$ Video Segmentation. The research for VSS mainly focuses on better spatial-temporal fusion~gadde2017semantic or acceleration using extra cues~shelhamer2016clockwork,DFF in the video. VIS requires segmenting and tracking each instance. Most VIS approaches~mask_pro_vis,fu2021compfeat,kim2020vps,li2022improving focus on learning instance-wised spatial, temporal relation, and feature fusion. Several works learn the 3D temporal embeddings. Like PS, VPS~kim2020vps can also be top-down~kim2020vps and bottom-up approaches~vip_deeplab. The top-down approaches learn to link the temporal features and then perform instance association online. In contrast, the bottom-up approaches predict the center map of the near frame and perform instance association in a separate stage. Most of these approaches are highly engineering. For example, MaskPro~mask_pro_vis adopts state-of-the-art IS segmentation models~htc, deformable CNN~deformablev2, and offline mask propagation in one system. There are also several video segmentation tasks, including video object segmentation (VOS)~VOS_data,MOSE, referring video segmentation~MeViS, multi-Object tracking, and segmentation (MOTS)~voigtlaender2019mots. \\noindent$\\bullet$ Point Cloud Segmentation. This task aims to group point clouds into semantic or instance categories, similar to image and video segmentation. Depending on the input scene, it is typically categorized as either indoor or outdoor scenes. Indoor scene segmentation mainly includes point cloud semantic segmentation (PSS) and point cloud instance segmentation (PIS). PSS is commonly achieved using the Point-Net~qi2017pointnet,qi2017pointnet++, while PIS can be achieved through two approaches: top-down approaches~yang2019learning,yi2019gspn and bottom-up approaches~wang2018sgpn,jiang2020pointgroup. The former extracts 3D bounding boxes and uses a mask learning branch to predict masks, while the latter predicts semantic labels and utilizes point embedding to group points into different instances. For outdoor scenes, point cloud segmentation can be divided into point-based~qi2017pointnet,mao2019interpolated and voxel-based~hu2020randla,cheng20212 approaches. Point-based methods focus on processing individual points, while voxel-based methods divide the point cloud into 3D grids and apply 3D convolution. Like panoptic segmentation, most 3D panoptic segmentation methods~zhou2021panoptic,xu2022sparse,Hong_2021_CVPR,aygun20214d,Zhu_2021_CVPR first predict semantic segmentation results, separate instances based on these predictions and fuse the two results to obtain the final results.",
      "origin_cites_number": 37
    },
    {
      "section_title": "Transformer Basics",
      "level": "2",
      "content": "\\noindent$\\bullet$ Vanilla Transformer~vaswani2017attention is a seminal model in the transformer-based research field. It is an encoder-decoder structure that takes tokenized inputs and consists of stacked transformer blocks. Each block has two sub-layers: a multi-head self-attention (MHSA) layer and a position-wise fully-connected feed-forward network (FFN). The MHSA layer allows the model to attend to different parts of the input sequence while the FFN processes the output of the MHSA layer. Both sub-layers use residual connections and layer normalization for better optimization. In the vanilla transformer, the encoder and decoder both use the same architecture. However, the decoder is modified to include a mask that prevents it from attending to future tokens during training. Additionally, the decoder uses sine and cosine functions to produce positional embeddings, which allow the model to understand the order of the input sequence. Subsequent models such as BERT and GPT-2 have built upon its architecture and achieved state-of-the-art results on a wide range of natural language processing tasks. \\noindent$\\bullet$ Self-Attention. The core operator of the vanilla transformer is the self-attention (SA) operation. Suppose the input data is a set of tokens $X =[ x_{1}, x_{2}, ..., x_{N}] \\in R^{N \\times c}$. $N$ is the token number and $c$ is token dimension. The positional encoding $P$ may be added into $I = X + P$. The input embedding $I$ goes through three linear projection layers ($W^{q} \\in R^{c \\times d}, W^{k} \\in R^{c \\times d}, W^{v} \\in R^{c \\times d}$) to generate Query (Q), Key (K), and Value (V): equation Q = IW^{q}, K=IW^{k}, V=IW^{v}, equation where $d$ is the hidden dimension. The Query and Key are usually used to generate the attention map in SA. Then the SA is performed as follows: equation O = SA(Q,K,V) = Softmax(QK^\\intercal)V. equation According to Equ.~equ:self_atten, given an input $X$, self-attention allows each token $x_{i}$ to attend to all the other tokens. Thus, it has the ability of global perception compared with local CNN operator. Motivated by this, several works~wang2018nonlocal,chen2018a2net treat it as a fully-connected graph or a non-local module for visual recognition task. \\noindent$\\bullet$ Multi-Head Self-Attention. In practice, multi-head self-attention (MHSA) is more commonly used. The idea of MHSA is to stack multiple SA sub-layer in parallel, and the concatenated outputs are fused by a projection matrix $W^{fuse} \\in R^{d \\times c}$: equation O = MHSA(Q,K,V) = concat([SA_{i,..SA_{H}}])W^{fuse}, equation where $SA_{i}= SA(Q_{i}, K_{i}, V_{i})$ and $H$ is the number of the head. Different heads have individual parameters. Thus, MHSA can be viewed as an ensemble of SA. \\noindent$\\bullet$ Feed-Forward Network. The goal of feed-forward network (FFN) is to enhance the non-linearity of attention layer outputs. It is also called multi-layer perceptron (MLP) since it consists of two successive linear layers with non-linear activation layers.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Methods: A Survey",
      "level": "1",
      "content": "In this section, based on DETR-like meta-architecture, we review the key techniques of transformer-based segmentation. As shown in Fig.~fig:meta_architec, the meta-architecture contains a feature extractor, object query, and a transformer decoder. Then, according to the meta-architecture, we survey existing methods by considering the modification or improvements to each component of the meta-architecture in Sec.~sec:method_strong_representation, Sec.~sec:method_interaction_design and Sec.~sec:extra_cue_query_learning. Finally, based on such meta-architecture, we present several detailed applications in Sec.~sec:query_association and Sec.~sec:conditional_query_generation.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Meta-Architecture",
      "level": "2",
      "content": "\\noindent$\\bullet$ Backbone. Before ViTs, CNNs were the standard approach for feature extraction in computer vision tasks. To ensure a fair comparison, many research works~maskrcnn,ren2015faster,detr used the same CNN models, such as ResNet50~resnet. Some researchers~wang2018nonlocal,axialDeeplab also explored the combination of CNNs with self-attention layers to model long-range dependencies. ViT, on the other hand, utilizes a standard transformer encoder for feature extraction. It has a specific input pipeline for images, where the input image is split into fixed-size patches, such as 16 $\\times$ 16 patches. These patches are then processed through a linear embedding layer. Then, the positional embeddings are added to each patch. Afterward, a standard transformer encoder encodes all patches. It contains multiple multi-head self-attention and feed-forward layers. For instance, given an image $I \\in R^{H \\times W \\times 3}$, ViT first reshapes it into a sequence of flattened 2D patches: $I_{p} \\in R^{N \\times P^{2} \\times 3}$, where $N$ is the number of patches and $P$ is the patch size. With patch embedding operations, the final input is $I_{in} \\in R^{N \\times P^{2} \\times C}$, where $C$ is the embedding channel. To perform classification, an extra learnable embedding ``classification token\" ($CLS$) is added to the sequence of embedded patches. After the standard transformer for all patches, $I_{out} \\in R^{N \\times P^{2} \\times C}$ is obtained. For segmentation tasks, ViT is used as a feature extractor, meaning that $I_{out}$ is resized back to a dense map $F \\in R^{H\\times W \\times C}$. \\noindent$\\bullet$ Neck. Feature pyramid network (FPN) has been shown effective in object detection and instance segmentation~fpn,focal_loss,tian2021fcos for scale variation modeling. FPN maps the features from different stages into the same channel dimension $C$ for the decoder. Several works~nasfpn,qiao2021detectors design stronger FPNs via cross-scale modeling using dilation or deformable convolution. For example, Deformable DETR~zhu2020deformabledetr proposes a deformable FPN to model cross-scale fusion using deformable attention. Lite-DETR~li2023litedetr further refines the deformable cross-scale attention design by efficiently sampling high-level features and low-level features in an interleaved manner. The output features are used for decoding the boxes and masks. The role of FPN is the same as previous detection-based or FCN-based segmentation methods. The FPN generates multi-scale features to handle and balance both small and large objects in the scene. For the transformer-based method, FPN architecture is often used to refine object queries from different scales, which can lead to stronger results than single-scale refinement. \\noindent$\\bullet$ Object Query. Object query is first introduced in DETR~detr. It plays as the dynamic anchors that are used in detectors~maskrcnn,ren2015faster. In practice, it is a learnable embedding $Q_{obj} \\in R^{N_{ins} \\times d}$. $N_{ins}$ represents the maximum instance number. The query dimension $d$ is usually the same as feature channel $c$. Object query is refined by the cross-attention layers. Each object query represents one instance of the image. During the training, each ground truth is assigned with one corresponding query for learning. During the inference, the queries with high scores are selected as output. Thus, object query simplifies the design of detection and segmentation models by eliminating the need for hand-crafted components such as non-maximum suppression (NMS). The flexible design of object query has led to many research works exploring its usage in different contexts, which will be discussed in more detail in Sec.~sec:method_categorization. \\noindent$\\bullet$ Transformer Decoder. Transformer decoder is a crucial architecture component in transformer-based segmentation and detection models. Its main operation is cross-attention, which takes in the object query $Q_{obj}$ and the image/video feature $F$. It outputs a refined object query, denoted as $Q_{out}$. The cross-attention operation is derived from the vanilla transformer architecture, where $Q_{obj}$ serves as the query, and $F$ is used as the key and value in the self-attention mechanism. After obtaining the refined object query $Q_{out}$, it is passed through a prediction FFN, which typically consists of a 3-layer perceptron with a ReLU activation layer and a linear projection layer. The FFN outputs the final prediction, which depends on the specific task. For example, for classification, the refined query is mapped directly to class prediction via a linear layer. For detection, the FFN predicts the normalized center coordinates, height, and width of the object bounding box. For segmentation, the output embedding is used to perform dot product with feature $F$, which results in the binary mask logits. The transformer decoder iteratively repeats cross-attention and FFN operations to refine the object query and obtain the final prediction. The intermediate predictions are used for auxiliary losses during training and discarded during inference. The outputs from the last stage of the decoder are taken as the final detection or segmentation results. We show the detailed process in Fig.~fig:meta_architec (b). \\noindent$\\bullet$ Mask Prediction Representation. Transformer-based segmentation approaches adopt two formats to represent the mask prediction: pixel-wise prediction as FCNs and per-mask-wise prediction as DETR. The former is used in semantic-aware segmentation tasks, including SS, VSS, VOS, and \\etc. The latter is used in instance-aware segmentation tasks, including IS, VIS, and VPS, where each query represents each instance. \\noindent$\\bullet$ Bipartite Matching and Loss Function. Object query is usually combined with bipartite matching~kuhn1955hungarian during training, uniquely assigning predictions with ground truth. This means each object query builds the one-to-one matching during training. Such matching is based on the {matching cost} between ground truth and predictions. The {matching cost} is defined as the distance between prediction and ground truth, including labels, boxes, and masks. By minimizing the cost with the Hungarian algorithm~kuhn1955hungarian, each object query is assigned by its corresponding ground truth. For object detection, each object query is trained with classification and box regression loss~ren2015faster. For instance-aware segmentation, each object query is trained via both mask classification loss and segmentation loss. The output masks are obtained via the inner product between object query and decoder features. The segmentation loss usually contains binary cross-entropy loss and dice loss~dice_loss. \\noindent$\\bullet$ Discussion on Scope of Meta-Architecture. We admit our meta-architecture may not cover all transformer-based segmentation methods. In semantic segmentation, methods such as Segformer~xie2021segformer and SETR~SETR employ a fully connected layer and predict each pixel as previous FCN-based methods~long2015fully,deeplabv3,deeplabv3plus. These methods concentrate on enhanced feature representation. We argue that this represents a basic form of our meta-architecture, wherein each query corresponds to a class category. The cascaded cross-attention layers are omitted, and bipartite matching is removed. Thus, the object query plays the same role as a fully connected layer. In addition, meta-architecture represents the latest design philosophy. Nearly all recent state-of-the-art methods~OMGSeg,athar2023tarvis,yuan2021polyphonicformer,wu2023uniref++ adopt this meta-architecture. In particular, different methods may add more components to adapt their tasks and requirements. Thus, we review recent works by modifying each component based on this meta-architecture. table*[t] \\centering \\small Transformer-Based Segmentation Method Categorization. We select the representative works for reference. adjustbox{width=0.90\\textwidth} tabular{l c l c} \\toprule[0.15em] Method Categorization & Tasks & Reference \\\\ \\toprule[0.15em] Representation Learning (Sec.~sec:method_strong_representation) & \\\\ \\quad $\\bullet$Better ViTs Design & SS / IS & ~VIT,DeiT,fan2021mvitv1,li2022mvitv2,lee2022mpvit,ali2021xcit,Wang_2021_ICCV_PVT,chen2021crossvit, zhang2020feature,Xu_2021_ICCV \\\\ \\quad $\\bullet$Hybrid CNNs / transformers / MLPs & SS / IS &~liu2021swin,xie2021segformer,guo2021cmt,chu2021Twins,wu2021cvt,xu2021vitae,liu2022convnet,han2021connection,guo2022segnext,yu2022metaformer,dai2022demystify \\\\ \\quad $\\bullet$Self-Supervised Learning & SS / IS &~chen2020simple,he2020momentum,chen2021mocov3,bao2021beit,MaskedAutoencoders2021,wei2022maskedfeat,gandelsman2022test,hu2022exploring,gao2022convmae,radford2021learning_clip,li2022scaling \\\\ \\hline Cross-Attention Design in Decoder (Sec.~sec:method_interaction_design) & \\\\ \\quad $\\bullet$Improved Cross-Attention Design & SS / IS / PS &~zhu2020deformabledetr,peize2020sparse,QueryInst,hu2021ISTR,dong2021solq,he2021boundarysqueeze,zhang2021knet,cheng2021maskformer,li2021panoptic \\\\ \\quad $\\bullet$Spatial-Temporal Cross-Attention Design & VSS / VIS / VPS &~VIS_TR,zhou2022transvod,yang2022tevit,cheng2021mask2formervis,IFC_21,seqformer,li2022videoknet,kim2022tubeformer \\\\ \\hline Optimizing Object Query (Sec.~sec:extra_cue_query_learning) & \\\\ \\quad $\\bullet$Adding Position Information into Query & IS / PS &~meng2021conditional,chen2022conditional,wang2022anchor,liu2022dabdetr \\\\ \\quad $\\bullet$Adding Extra Supervision into Query. & IS / PS &~li2022dn,zhang2022dino,li2022maskdino,Instance_Unique_Querying,jia2022detrs,chen2022group,zong2022detrs \\\\ \\hline Using Query For Association (Sec.~sec:query_association)\\\\ \\quad $\\bullet$Query for Instance Association & VIS / VPS &~meinhardt2021trackformer,transtrack,zeng2021motr,huang2022minvis,li2022videoknet,IDOL \\\\ \\quad $\\bullet$Query for Linking Multi-Tasks & VPS / DVPS / PS / PPS / IS &~panopticpartformer,yuan2021polyphonicformer,gao2022panopticdepth,xu2022fashionformer,xu2022multi,invpt2022 \\\\ \\hline Conditional Query Generation (Sec.~sec:conditional_query_generation) \\\\ \\quad $\\bullet$Conditional Query Fusion on Language Features & RIS / RVOS &~VLT_iccv2021,MeViS,LAVT_22cvpr,ReSTR_2022_CVPR,CRIS_2022_CVPR,botach2022end,ding2022language,GRES,wu2022towards_robust_ris,wu2022language \\\\ \\quad $\\bullet$Conditional Query Fusion on Cross Image Features & SS/ VOS / SS / Few Shot SS &~zhang2021few,yang2021associating,park2022matteformer,shi2022transformer,lin2022structtoken,yu2022batman,jiao2022mask \\\\ \\bottomrule[0.1em] tabular adjustbox table*",
      "origin_cites_number": 28
    },
    {
      "section_title": "Method Categorization",
      "level": "2",
      "content": "In this section, we review five aspects of transformer-based segmentation methods. Rather than classifying the literature by the task settings, our goal is to extract the {essential and common techniques} used in the literature. We summarize the methods, techniques, related tasks, and corresponding references in Tab.~tab:method_categorization. Most approaches are based on the meta-architecture described in Sec.~sec:method_meta. We list the comparison of representative works in Tab.~tab:method_classification.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Strong Representations",
      "level": "3",
      "content": "Learning a strong feature representation always leads to better segmentation results. Taking the SS task as an example, SETR~SETR is the first to replace CNN backbone with the ViT backbone. It achieves state-of-the-art results on the ADE20k dataset without bells and whistles. After ViTs, researchers start to design better vision transformers. We categorize the related works into three aspects: better vision transformer design, hybrid CNNs/transformers/MLPs, and self-supervised learning. \\noindent$\\bullet$ Better ViTs Design. Rather than introducing local bias, these works follow the original ViTs design and process feature using the original MHSA for token mixing. DeiT~DeiT proposes knowledge distillation and provides strong data augmentation to train ViT efficiently. Starting from DeiT, nearly all ViTs adopt the stronger training procedure. MViT-V1~fan2021mvitv1 introduces the multiscale feature representation and pooling strategies to reduce the computation cost in MHSA. MViT-V2~li2022mvitv2 further incorporates decomposed relative positional embeddings and residual pooling design in MViT-V1, which leads to better representation. Motivated by MViT, from the architecture level, MPViT~lee2022mpvit introduces multiscale patch embedding and multi-path structure to explore tokens of different scales jointly. Meanwhile, from the operator level, XCiT~ali2021xcit operates across feature channels rather than token inputs and proposes cross-covariance attention, which has linear complexity in the number of tokens. This design makes it easy to adapt to segmentation tasks, which always have high-resolution inputs. Pyramid ViT~Wang_2021_ICCV_PVT is the first work to build multiscale features for detection and segmentation tasks. There are also several works~chen2021crossvit,zhang2020feature,Xu_2021_ICCV exploring cross-scale modeling via MHSA, which exchange long-range information on different feature pyramids. \\noindent$\\bullet$ Hybrid CNNs/Transformers/MLPs. Rather than modifying the ViTs, many works focus on introducing local bias into ViT or using CNNs with large kernels directly. To build a multi-stage pipeline, Swin~liu2021swin,liu2022swin adopts shift-window attention in a CNN style. They also scale up the models to large sizes and achieve significant improvements on many vision tasks. From an efficient perspective, Segformer~xie2021segformer designs a light-weight transformer encoder. It contains a sequence reduction during MHSA and a light-weight MLP decoder. Segformer achieves better speed and accuracy trade-off for SS. Meanwhile, several works~guo2021cmt,chu2021Twins,wu2021cvt,xu2021vitae directly add CNN layers to a transformer to explore the local context. Several works~chen2021cyclemlp,tolstikhin2021mlp explore the pure MLPs design to replace the transformer. With specific designs such as shifting and fusion~chen2021cyclemlp, MLP models can also achieve comparable results with ViTs. Later, several works~liu2022convnet,han2021connection point out that CNNs can achieve stronger results than ViTs if using the same data augmentation pipeline. In particular, DWNet~han2021connection re-visits the training pipeline of ViTs and proposes dynamic depth-wise convolution. Then, ConvNeXt~liu2022convnet uses the larger kernel depth-wise convolution and a stronger data training pipeline. It achieves stronger results than Swin~liu2021swin. Motivated by ConvNeXt, SegNext~guo2022segnext designs a CNN-like backbone with linear self-attention and performs strongly on multiple SS benchmarks. Meanwhile, Meta-Former~yu2022metaformer shows that the meta-architecture of ViT is the key to achieving stronger results. Such meta-architecture contains a token mixer, a MLP, and residual connections. The token mixer is a simple MHSA layer. Meta-Former shows that the token mixer is not as important as meta-architecture. Using simple pooling as a token mixer can achieve stronger results. Following the Meta-Former, recent work~dai2022demystify re-benchmarks several previous works using a unified architecture to eliminate unfair engineering techniques. However, under stronger settings, the authors find the spatial token mixer design still matters. Meanwhile, several works~chen2022cyclemlp,guo2021hire explore the MLP-like architecture for dense prediction. \\noindent$\\bullet$ Self-Supervised Learning (SSL). SSL has achieved huge progress in recent years~chen2020simple,he2020momentum,chen2023context. Compared with supervised learning, SSL exploits unlabeled data via specially designed pseudo tasks and can be easily scaled up. MoCo-v3~chen2021mocov3 is the first study that trains ViTs in SSL. It freezes the patch projection layer to stabilize the training process. Motivated by BERT, BEiT~bao2021beit proposes the BERT-like per-training (Mask Image Modeling, MIM) of vision transformers. After BEiT, MAE~MaskedAutoencoders2021 shows that ViTs can be trained with the simplest MIM style. By masking a portion of input tokens and reconstructing the RGB images, MAE achieves better results than supervised training. As a concurrent work, MaskFeat~wei2022maskedfeat mainly studies reconstructing targets of the MIM framework, such as the histogram of oriented gradient (HOG) features. The following works focus on improving the MIM framework~gandelsman2022test,hu2022exploring or replacing the backbone of ViTs with CNN architecture~gao2022convmae,tian2023designing. DINO series~caron2021emergingDINO find the self-supervised learned feature itself has grouping effects, which is always used in unsupervised learning contexts. (Sec.~sec:label_efficient) Recently, several works~radford2021learning_clip,jia2021scaling_align on VLM also adopt SSL by utilizing easily obtained text-image pairs. Recent work~li2022scaling demonstrates the effectiveness of VLM in downstream tasks, including IS and SS. Moreover, several recent works~li2022uniperceiver_v2 adopt multi-modal SSL pre-training and design a unified model for many vision tasks. For video representation learning, most current works~videomae,MaskedAutoencodersSpatiotemporal2022,liu2022video verify such representation learning on action or motion learning, such as action recognition. Several works~wu2022language,ding2022vlt adopt a video backbone for video segmentation. However, for video segmentation, from the method design perspective, most works focus on matching and association of entities or pixels, which is discussed in Sec.~sec:method_interaction_design and Sec.~sec:query_association.",
      "origin_cites_number": 34
    },
    {
      "section_title": "Cross-Attention Design in Decoder",
      "level": "3",
      "content": "In this section, we review the new transformer decoder designs. We categorize the decoder design into two groups: one for improved cross-attention design in image segmentation and the other for spatial-temporal cross-attention design in video segmentation. The former focuses on designing a better decoder to refine the original decoder in the original DETR. The latter extends the query-based object detector and segmenter into the video domain for VOD, VIS, and VPS, focusing on modeling temporal consistency and association. \\noindent$\\bullet$ Improved Cross-Attention Design. Cross-attention is the core operation of meta-architecture for segmentation and detection. Current solutions for improved cross-attention mainly focus on designing new or enhanced cross-attention operators and improved decoder architectures. Following DETR, Deformable DETR~zhu2020deformabledetr proposes deformable attention to efficiently sample point features and perform cross-attention with object query jointly. Meanwhile, several works bring object queries into previous RCNN frameworks. Sparse-RCNN~peize2020sparse uses RoI pooled features to refine the object query for object detection. They also propose a new dynamic convolution and self-attention to enhance object query without extra cross-attention. In particular, the pooled query features reweight the object query, and then self-attention is applied to the object query to obtain the global view. After that, several works~QueryInst,hu2021ISTR add the extra mask heads for IS. QueryInst~QueryInst adds mask heads and refines mask query with dynamic convolution. Meanwhile, several works~yu2022soit,dong2021solq extend Deformable DETR by directly applying MLP on the shared query. Inspired by MEInst~zhang2020MEInst, SOLQ~dong2021solq utilizes mask encodings on object query via MLP. By applying the strong Deformable DETR detector and Swin transformer~liu2021swin backbone, it achieves remarkable results on IS. However, these works still need extra box supervision, which makes the system complex. Moreover, most RoI-based approaches for IS have low mask quality issues since the mask resolution is limited within the boxes~kirillov2020pointrend. To fix the issues of extra box heads, several works remove the box prediction and adopt pure mask-based approaches. Earlier work, OCRNet~ocrnet characterizes a pixel by exploiting the representation of the corresponding object class that forms a category query. Then, Segmenter~strudel2021_segmenter adopts a strong ViT backbone with the class query to directly decode class-wise masks. Pure mask-based approaches directly generate segmentation masks from high-resolution features and naturally have better mask quality. Max-Deeplab~wang2020maxDeeplab is the first to remove the box head and design a pure-mask-based segmenter for PS. It also achieves stronger performance than box-based PS method~qiao2021detectors. It combines a CNN-transformer hybrid encoder~axialDeeplab and a transformer decoder as an extra path. Max-Deeplab still needs extra auxiliary loss functions, such as semantic segmentation loss, and instance discriminative loss. K-Net~zhang2021knet uses mask pooling to group the mask features and designs a gated dynamic convolution to update the corresponding query. By viewing the segmentation tasks as convolution with different kernels, K-Net is the first to unify all three image segmentation tasks, including SS, IS, and PS. % Meanwhile, MaskFormer~cheng2021maskformer extends the original DETR by removing the box head and transferring the object query into the mask query via MLPs. It proves simple mask classification can work well enough for all three segmentation tasks. Compared to MaskFormer, K-Net is good at training data efficiency. This is because K-Net adopts mask pooling to localize object features and then update object queries accordingly. Motivated by this, Mask2Former~cheng2021mask2former proposes masked cross-attention and replaces the cross-attention in MaskFormer. Masked cross-attention makes object query only attend to the object area, guided by the mask outputs from previous stages. Mask2Former also adopts a stronger Deformable FPN backbone~zhu2020deformabledetr, stronger data augmentation~detectron2, and multiscale mask decoding. The above works only consider updating object queries. To handle this, CMT-Deeplab~yu2022cmt proposes an alternating procedure for object query and decoder features. It jointly updates object queries and pixel features. After that, inspired by the k-means clustering algorithm, kMaX-DeepLab~kmax_deeplab_2022 proposes k-means cross-attention by introducing cluster-wise argmax operation in the cross-attention operation. Meanwhile, PanopticSegformer~li2021panoptic proposes a decoupling query strategy and deeply supervised mask decoder to speed up the training process. For real-time segmentation setting, SparseInst~Cheng2022SparseInst proposes a sparse set of instance activation maps highlighting informative regions for each foreground object. Besides segmentation tasks, several works speed up the convergence of DETR by introducing new decoder designs, and most approaches can be extended into IS. Several works bring such semantic priors in the DETR decoder. SAM-DETR~zhang2022_SAMDETR projects object queries into semantic space and searches salient points with the most discriminative features. SMAC~gao2021fast conducts location-aware co-attention by sampling features of high near estimated bounding box locations. Several works adopt dynamic feature re-weights. From the multiscale feature perspective, AdaMixer~gao2022adamixer samples feature over space and scales using the estimated offsets. It dynamically decodes sampled features with an MLP, which builds a fast-converging query-based detector. ACT-DETR~zheng2020end clusters the query features adaptively using a locality-sensitive hashing and replaces the query-key interaction with the prototype-key interaction to reduce cross-attention cost. From the feature re-weighting view, Dynamic-DETR~dai2021dynamic introduces dynamic attention to both the encoder and decoder parts of DETR using RoI-wise dynamic convolution. Motivated by the sparsity of the decoder feature, Sparse-DETR~roh2022sparse selectively updates the referenced tokens from the decoder and proposes an auxiliary detection loss on the selected tokens in the encoder to keep the sparsity. In summary, dynamically assigning features into query learning speeds up the convergence of DETR. table*[ht] \\centering \\small Representative works summarization and comparison in Sec.~\\ref{sec:method_survey.} \\tabcolsep{3.0pt} 0.75{ tabular{p{0.14\\textwidth}p{0.10\\textwidth}p{0.20\\textwidth}p{0.20\\textwidth}p{0.60\\textwidth}} \\toprule gray!30! gray!30! Method & Task & Input/Output & Transformer Architecture & \\ \\ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Highlight \\\\ gray!30! \\midrule gray!15! gray!15!5{c}{Strong Representations (Sec.~\\ref{sec:method_strong_representation)}} \\\\ gray!15! \\midrule SETR~SETR & SS & Image/Semantic Masks & Pure transformer + CNN decoder & the first vision transformer to replace CNN backbone in SS. \\\\ gray!10! Segformer~xie2021segformer & SS & Image/Semantic Masks & Pure transformer + MLP head & a light-weight transformer backbone with simple MLP prediction head. \\\\ MAE~MaskedAutoencoders2021 & SS/IS & Image/Semantic Masks & Pure transformer + CNN decoder & a MIM pretraining framework for plain ViTs, which achieves better results than supervised training. \\\\ gray!10! SegNext~guo2022segnext & SS & Image/Semantic Masks & Transformer + CNN & a large kernel CNN backbone with linear self-attention layer. \\\\ \\midrule gray!15! gray!15!5{c}{Cross-Attention Design in Decoder (Sec.~\\ref{sec:method_interaction_design)}} \\\\ gray!15! \\midrule Deformable DETR~zhu2020deformabledetr & OD & Image/Box & CNN + query decoder & a new multi-scale deformable attention and a new encoder-decoder framework. \\\\ OCRNet~ocrnet & SS & Image/Semantic Masks & CNN + query decoder & introduces category queries and uses one cross-attention layer to model global context efficiently. \\\\ Segmenter~strudel2021_segmenter & SS & Image/Semantic Masks & ViT + query decoder & uses ViT backbone and category queries to directly output each class mask. \\\\ gray!10! Sparse-RCNN~peize2020sparse & OD & Image/Box & CNN + query decoder & a new dynamic convolution layer and combine object query with RoI-based detector. \\\\ AdaMixer~gao2022adamixer & OD & Image/Box & CNN + query decoder & a new multiscale query-based decoder and refine query with multiscale features. \\\\ gray!10! Max-Deeplab~wang2020maxDeeplab & PS & Image/Panoptic Masks & CNN + attention + query decoder & the first pure mask supervised panoptic segmentation method and a two path framework (query and CNN features). \\\\ K-Net~zhang2021knet & SS/IS/PS & Image/Panoptic Masks & CNN + query decoder & the first work using kernels to unify image segmentation tasks and a new mask-based dynamic kernel update module. \\\\ gray!10! Mask2Former~cheng2021mask2former & SS/IS/PS & Image/Panoptic Masks & CNN + query decoder & design masked cross-attention and fully utilize the multiscale features in the decoder. \\\\ kMax-Deeplab~kmax_deeplab_2022 & SS/PS & Image/Panoptic Masks & CNN + query decoder & proposes a new k-mean style cross-attention by replacing softmax with argmax operation. \\\\ gray!10! VisTR~VIS_TR & VIS & Video/Instance Masks & CNN + query decoder & the first end-to-end VIS method and each query represent a tracked object in a clip. \\\\ VITA~heo2022vita & VIS & Video/Instance Masks & CNN + query decoder & use the fixed object detector and process all frame queries with extra encoder-decoder and global queries. \\\\ gray!10! TubeFormer~kim2022tubeformer & VSS/VIS/VPS & Video/Panoptic Masks & CNN + query decoder & a tube-like decoder with a token exchanging mechanism within the tube, which unifies three video segmentation tasks in one framework. \\\\ Video K-Net~li2022videoknet & VSS/VIS/VPS & Video/Panoptic Masks & CNN + query decoder & unified online video segmentation and adopt object query for association and linking. \\\\ \\midrule gray!15! gray!15!5{c}{Optimizing Object Query (Sec.~\\ref{sec:extra_cue_query_learning)}}\\\\ gray!15!\\midrule Conditional DETR~meng2021conditional & OD & Image/Box & CNN + query decoder & add a spatial query to explore the extremity regions to speed up the DETR training. \\\\ gray!10!DN-DETR~li2022dn & OD & Image/Box & CNN + query decoder & add noisy boxes and de-noisy loss to stable query matching and improve the coverage of DETR. \\\\ Group-DETR~chen2022group & OD & Image/Box & CNN + query decoder & introduce one-to-many assignment by extending more queries into groups. \\\\ gray!10!Mask-DINO~li2022maskdino & IS/PS & Image/Panoptic Masks & CNN + query decoder & boost instance/panoptic segmentation with object detection datasets. \\\\ \\midrulegray!15! gray!15!5{c}{Using Query For Association (Sec.~\\ref{sec:query_association)}} \\\\ gray!15! \\midrule gray!10! MOTR~zeng2021motr & MOT & Video/Box & CNN + query decoder & design an extra tracking query for object association. \\\\ MiniVIS~huang2022minvis & VIS & Video/Instance Masks & CNN/transformer + query decoder & perform video instance segmentation with image level pretraining and image object query for tracking. \\\\ gray!10! Polyphonicformer yuan2021polyphonicformer & D-VPS & Video/(Depth+Panoptic Masks) & CNN/transformer + query decoder & use object query and depth query to model instance-wise mask and depth prediction jointly. \\\\ X-Decoder~zou2022xdecoder & SS/PS & Image /Panoptic Masks & CNN/transformer + query decoder & jointly pre-train image segmentation and language model and perform zero-shot inference on multiple segmentation tasks. \\\\ gray!10!LMPM~MeViS & RVOS & (Video+Text)/Instance Masks & CNN/transformer + query decoder & capture motion by associating frame-level object tokens from an off-the-shelf instance segmentation model. \\\\ \\midrulegray!15! gray!15!5{c}{Conditional Query Generation (Sec.~\\ref{sec:conditional_query_generation)}} \\\\ gray!15! \\midrule VLT~VLT_iccv2021,ding2022vlt & RIS & (Image+Text)/Instance Masks & CNN + transformer decoder & design a query generation module to produce language conditional queries for transformer decoder. \\\\ gray!10! LAVT~LAVT_22cvpr & RIS & (Image+Text)/Instance Masks & Transformer + CNN decoder & design gated cross-attention between pyramid features and language features. \\\\ MTTR~botach2022end & RVOS & (Video+Text)/Instance Masks & Transformer + query decoder & perform spatial-temporal cross-attention between language features and object query. \\\\ gray!10! X-DETR~cai2022x & OD & (Image+Text)/Box & Transformer + query decoder & perform directly alignment between language features and object query. \\\\ CyCTR~zhang2021few & Few-Shot SS & (Image+Masks)/Instance Masks & Transformer + query decoder & design a cycle cross-attention between features in support images and query images. \\\\ \\bottomrule tabular } \\vspacefigtext table* \\noindent$\\bullet$ Spatial-Temporal Cross-Attention Design. After extending the object query in the video domain, each object query represents a tracked object across different frames, which is shown in Fig.~fig:video_query. The simplest extension is proposed by VisTR~VIS_TR for VIS. VisTR extends the cross-attention in DETR into multiple frames by stacking all clip features into flattened spatial-temporal features. The spatial-temporal features also involve temporal embeddings. During inference, one object query can directly output spatial-temporal masks without extra tracking. Meanwhile, TransVOD~zhou2022transvod proposes to link object query and corresponding features across the temporal domain. It splits the clips into sub-clips and performs clip-wise object detection. TransVOD utilizes the local temporal information and achieves better speed and accuracy trade-off. IFC~IFC_21 adopts message tokens to exchange temporal context among different frames. The message tokens are similar to learnable queries, which perform cross-attention with features in each frame and self-attention among the tokens. After that, TeViT~yang2022tevit proposes a novel messenger shift mechanism for temporal fusion and a shared spatial-temporal query interaction mechanism to utilize both frame-level and instance-level temporal context information. Seqformer~seqformer combines Deformable-DETR and VisTR in one framework. It also proposes to use image datasets to augment video segmentation training. Mask2Former-VIS~cheng2021mask2formervis extends masked cross-attention in Mask2Former~cheng2021mask2former into temporal masked cross-attention. Following VisTR, it also directly outputs spatial-temporal masks. In addition to VIS, several works~zhang2021knet,cheng2021mask2former,li2021panoptic have shown that query-based methods can naturally unify different segmentation tasks. Following this pipeline, there are also several works~li2022videoknet,kim2022tubeformer solving multiple video segmentation tasks in one framework. In particular, based on K-Net~zhang2021knet, Video K-Net~li2022videoknet proposes to unify VPS/VIS/VSS via tracking and linking kernels and works in an online manner. Meanwhile, TubeFormer~kim2022tubeformer extends Max-Deeplab~wang2020maxDeeplab into the temporal domain by obtaining the mask tubes. Cross-attention is carried out in a clip-wise manner. During inference, the instance association is performed by mask-based matching. Moreover, several works~heo2022vita propose the local temporal window to refine the global spatial-temporal cross-attention. For example, VITA~heo2022vita aggregates the local temporal query on top of an off-the-shelf transformer-based image instance segmentation model~cheng2021mask2former. Recently, several works~shin2023video,tubelink have explored the cross-clip association for video segmentation. In particular, Tube-Link~tubelink designs a universal video segmentation framework via learning cross-tube relations. It performs better than task-specific methods in VSS, VIS, and VPS.",
      "origin_cites_number": 78
    },
    {
      "section_title": "Optimizing Object Query",
      "level": "3",
      "content": "Compared with Faster-RCNN~ren2015faster, DETR~detr needs a much longer schedule for convergence. Due to the critical role of object query, several approaches have launched studies on speeding up training schedules and improving performance. According to the methods for the object query, we divide the following literature into two aspects: {adding position information} and {adopting extra supervision}. The position information provides the cues to sample the query feature for faster training. The extra supervision focuses on designing specific loss functions in addition to default ones in DETR. \\noindent$\\bullet$ Adding Position Information into Query. Conditional DETR~meng2021conditional finds cross-attention in DETR relies highly on the content embeddings for localizing the four extremities. The authors introduce conditional spatial query to explore the extremity regions explicitly. Conditional DETR V2~chen2022conditional introduces the box queries from the image content to improve detection results. The box queries are directly learned from image content, which is dynamic with various image inputs. The image-dependent box query helps locate the object and improve the performance. Motivated by previous anchor designs in object detectors, several works bring anchor priors in DETR. The Efficient DETR~yao2021efficientdetr adopts hybrid designs by including query-based and dense anchor-based predictions in one framework. Anchor DETR~wang2022anchor proposes to use anchor points to replace the learnable query and also designs an efficient self-attention head for faster training. Each object query predicts multiple objects at one position. DAB-DETR~liu2022dabdetr finds the localization issues of the learnable query and proposes dynamic anchor boxes to replace the learnable query. Dynamic anchor boxes make the query learning more explainable and explicitly decouple the localization and content part, further improving the detection performance. \\noindent$\\bullet$ Adding Extra Supervision into Query. DN-DETR~li2022dn finds that the instability of bipartite graph matching causes the slow convergence of DETR and proposes a denoising loss to stabilize query learning. In particular, the authors feed GT bounding boxes with noises into the transformer decoder and train the model to reconstruct the original boxes. Motivated by DN-DETR, based on Mask2Former, MP-Former~mp_former finds inconsistent predictions between consecutive layers. It further adds class embeddings of both ground truth class labels and masks to reconstruct the masks and labels. Meanwhile, DINO~zhang2022dino improves DN-DETR via a contrastive way of denoising training and a mixed query selection for better query initialization. Mask DINO~li2022maskdino extends DINO by adding an extra query decoding head for mask prediction. Mask DINO~li2022maskdino proposes a unified architecture and joint training process for both object detection and instance segmentation. By sharing the training data, Mask DINO can scale up and fully utilize the detection annotations to improve IS results. Meanwhile, motivated by contrastive learning, IUQ~Instance_Unique_Querying introduces two extra supervisions, including cross-image contrastive query loss via extra memory blocks and equivalent loss against geometric transformations. Both losses can be naturally adapted into query-based detectors. Meanwhile, there are also several works~wang2022towards,jia2022detrs,chen2022group,zong2022detrs exploring query supervision from the target assignment perspective. In particular, since DETR lacks the capability of exploiting multiple positive object queries, DE-DETR~wang2022towards first introduces one-to-many label assignment in query-based instance perception framework, to provide richer supervision for model training. Group DETR~chen2022group proposes group-wise one-to-many assignments during training. H-DETR~jia2022detrs adds auxiliary queries that use one-to-many matching loss during training. Rather than adding more queries, Co-DETR~zong2022detrs proposes a collaborative hybrid training scheme using parallel auxiliary heads supervised by one-to-many label assignments. All these approaches drop the extra supervision heads during inference. These extra supervision designs can be easily extended to query-based segmentation methods~cheng2021mask2former,zhang2021knet.",
      "origin_cites_number": 19
    },
    {
      "section_title": "Using Query For Association",
      "level": "3",
      "content": "Benefiting from the simplicity of query representation, several recent works have adopted it as an association tool to solve downstream tasks. There are mainly two usages: one for instance-level association and the other for task-level association. The former adopts the idea of instance discrimination, for instance-wise matching problems in video, such as joint segmentation and tracking. The latter adopts queries to link features for multitask learning. \\noindent$\\bullet$ Using Query for Instance Association. The research in this area can be divided into two aspects: one for designing extra tracking queries and the other for using object queries directly. TrackFormer~meinhardt2021trackformer is the first to treat multi-object tracking as a set prediction problem by performing joint detection and tracking-by-attention. TransTrack~transtrack uses the object query from the last frame as a new track query and outputs tracking boxes from the shared decoder. MOTR~zeng2021motr introduces the extra track query to model the tracked instances of the entire video. In particular, MOTR proposes a new tracklet-awared label assignment to train track queries and a temporal aggregation module to fuse temporal features. There are also several works~huang2022minvis,MeViS,li2022videoknet,IDOL,tubelink adopting object query solely for tracking. In particular, MiniVIS~huang2022minvis directly uses object query for matching without extra tracking head modeling for VIS, where it adopts image instance segmentation training. Both Video K-Net~li2022videoknet and IDOL~IDOL learn the association embeddings directly from the object query using a temporal contrastive loss. During inference, the learned association embeddings are used to match instances across frames. These methods are usually verified in VIS and VPS tasks. All methods pre-train their image baseline on image datasets, including COCO and Cityscapes, and fine-tune their video architecture in the video datasets. \\noindent$\\bullet$ Using Query for Linking Multi-Tasks. Several works~panopticpartformer,yuan2021polyphonicformer,li2023panopticpartformer++,DsHmp use object query to link features across different tasks to achieve mutual benefits. Rather than directly fusing multitask features, using object query fusion not only selects the most discriminative parts to fuse but also is more efficient than dense feature fusion. In particular, Panoptic-PartFormer~panopticpartformer links part and panoptic features via different object queries into an end-to-end framework, where joint learning leads to better part segmentation results. Several works~yuan2021polyphonicformer,gao2022panopticdepth combine segmentation features, and depth features using the MHSA layer on corresponding depth query and segmentation query, which unify the depth prediction and panoptic segmentation prediction via shared masks. Both works find the mutual effect for both segmentation and depth prediction. Recently, several works~xu2022multi,invpt2022 have adopted the vision transformers with multiple task-aware queries for multi-task dense prediction tasks. In particular, they treat object queries as task-specific hidden features for fusion and perform cross-task reasoning using MSHA on task queries. Moreover, in addition to dense prediction tasks, FashionFormer~xu2022fashionformer unifies fashion attribute prediction and instance part segmentation in one framework. It also finds the mutual effect on instance segmentation and attribute prediction via query sharing. Recently, X-Decoder~zou2022xdecoder uses two different queries for segmentation and language generation tasks. The authors jointly pre-train two different queries using large-scale vision language datasets, where they find both queries can benefit corresponding tasks, including visual segmentation and caption generation.",
      "origin_cites_number": 13
    },
    {
      "section_title": "Conditional Query Fusion",
      "level": "3",
      "content": "In addition to using object query for multitask prediction, several works adopt conditional query design for cross-modal and cross-image tasks. The query is conditional on the task inputs, and the decoder head uses such a conditional query to obtain the corresponding segmentation masks. Based on the source of different inputs, we split these works into two aspects: language features and image features. \\noindent$\\bullet$ Conditional Query Fusion From Language Feature. Several works~VLT_iccv2021,ding2022vlt,MeViS,GRES,LAVT_22cvpr,GRES,ReSTR_2022_CVPR,D2Zero,DsHmp,botach2022end,RIE,ding2022language,wu2022towards_robust_ris adopt conditional query fusion according to input language feature for both referring image segmentation (RIS)~GRES and referring video object segmentation (RVOS)~MeViS tasks. In particular, VLT~VLT_iccv2021,ding2022vlt firstly adopts the vision transformer for the RIS task and proposes a query generation module to produce multiple sets of language-conditional queries, which enhances the diversified comprehensions of the language. Then, it adaptively selects the output features of these queries via the proposed query balance module. Following the same idea, LAVT~LAVT_22cvpr designs a new gated cross-attention fusion where the image features are the query inputs of a self-attention layer in the encoder part. Compared with previous CNN approaches~ISFP,MCN, using a vision transformer significantly improves the language-driven segmentation quality. With the help of CLIP's knowledge, CRIS~CRIS_2022_CVPR proposes vision-language decoding and contrastive learning for achieving text-to-pixel alignment. Meanwhile, several works~wu2022multilevel_ref_video_seg,botach2022end,MeViS,wu2022language adopt video detection transformer in Sec.~sec:method_interaction_design for the RVOS task. MTTR~botach2022end models the RVOS task as a sequence prediction problem and proposes both language and video features jointly. Recently, several works~DsHmp,MeViS explore referring VOS under fast motion condition settings. Each object query in each frame combines the language features before sending it into the decoder. To speed up the query learning, ReferFormer~wu2022language designs a small set of object queries conditioned on the language as the input to the transformer. The conditional queries are transformed into dynamic kernels to generate tracked object masks in the decoder. With the same design as VisTR, ReferFormer can segment and track object masks with given language inputs. In this way, each object tracklet is controlled by a given language input. In addition to referring segmentation tasks, MDETR~MDETR presents an end-to-end modulated detector that detects objects in an image conditioned on a raw text query. In particular, they fuse the text embedding directly into visual features and jointly train the fused feature and object query. X-DETR~cai2022x proposes an effective architecture for instance-wise vision-language tasks via using dot-product to align vision and language. In summary, these works fully utilize the interaction of language features and query features. \\noindent$\\bullet$ Condition Query Fusion From Image Feature. Several tasks take multiple images as references and refine corresponding object masks of the main image. The multiple images can be support images in few shot segmentation~cao2022prototype,jiao2022mask,zhang2021few or the same input image in matting~park2022matteformer,DIIM and semantic segmentation~shi2022transformer,lin2022structtoken. These works aim to model the correspondences between the main image and other images via condition query fusion. For SS, StructToken~lin2022structtoken presents a new framework by doing interactions between a set of learnable structure tokens and the image features, where the image features are the spatial priors. In the video, BATMAN~yu2022batman fuses optical flow features and previous frame features into mixed features and uses such features as a query to decode the current frame outputs. For few-shot segmentation, CyCTR~zhang2021few aggregates pixel-wise support features into query features. In particular, CyCTR performs cross-attention between features from different images in a cycle manner, where support image features and query image features are the query inputs of the transformer jointly. Meanwhile, MM-Former~jiao2022mask adopts a class-agnostic method~cheng2021mask2former to decompose the query image into multiple segment proposals. Then, the support and query image features are used to select the correct masks via a transformer module. Then, for few-shot instance segmentation, RefTwice~han2023referencetwice proposes an object query enhanced framework to weight query image features via object queries from support queries. In image matting, MatteFormer~park2022matteformer designs a new attention layer called prior-attentive window self-attention based on Swin~liu2021swin. The prior token represents the global context feature of each trimap region, which is the query input of window self-attention. The prior token introduces spatial cues and achieves thinner matting results. In summary, according to the different tasks, the image features play as the decoder features in previous Sec.~sec:method_interaction_design, which enhance the features in the main images.",
      "origin_cites_number": 24
    },
    {
      "section_title": "Specific Subfields",
      "level": "1",
      "content": "In this section, we revisit several related subfields that adopt vision transformers for segmentation tasks. The subfields include point cloud segmentation, domain-aware segmentation, label and model efficient segmentation, class agnostic segmentation, tracking, and medical segmentation.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Segmentation",
      "level": "2",
      "content": "\\noindent$\\bullet$ Semantic Level Point Cloud Segmentation. Like image segmentation and video semantic segmentation, adopting transformers for semantic level processing mainly focuses on learning a strong representation (Sec.~sec:method_strong_representation). The works~guo2021pct,point_transformer focus on transferring the success in image/video representation learning into the point cloud. Early works~point_transformer directly use modified self-attention as backbone networks and design U-Net-like architectures for segmentation. In particular, Point-Transformer~point_transformer proposes vector self-attention and subtraction relation to aggregate local features progressively. The concurrent work PCT~guo2021pct also adopts a self-attention operation and enhances input embedding with the support of farthest point sampling and nearest neighbor searching. However, the ability to model long-range context and cross-scale interaction is still limited. Stratified-Transformer~lai2022stratified extends the idea of Swin Transformer~liu2021swin into the point cloud and dived 3D inputs into cubes. It proposes a mixed key sampling method for attention input and enlarges the effective receptive field via merging different cube outputs. Meanwhile, several works also focus on better pre-training or distilling the knowledge of 2D pre-trained models. PointBert~yu2022pointBERT designs the first Masked Point Modeling (MPM) task to pre-train point cloud transformers. It divides a point cloud into several local point patches as the input of a standard transformer. Moreover, it also pre-trains a point cloud Tokenizer with a discrete variational autoEncoder to encode the semantic contents and train an extra decoder using the reconstruction loss. Following MAE~MaskedAutoencoders2021, several works~pang2022masked,zhang2022point_m2ae simply the MIM pretraining process. Point-MAE~pang2022masked divides the input point cloud into irregular point patches and randomly masks them at a high ratio. Then, it uses a standard transformer-based autoencoder to reconstruct the masked points. Point-M2AE~zhang2022point_m2ae designs a multiscale MIM pretraining by making the encoder and decoder into pyramid architectures to model spatial geometries and multilevel semantics progressively. Meanwhile, benefiting from the same transformer architecture for point cloud and image, several works adopt image pre-trained standard transformer by distilling the knowledge from large-scale image dataset pre-trained models. \\noindent$\\bullet$ Instance Level Point Cloud Segmentation. As shown in Sec.~sec:background, previous PCIS / PCPS approaches are based on manually-tuned components, including a voting mechanism that predicts hand-selected geometric features for top-down approaches and heuristics for clustering the votes for bottom-up approaches. Both approaches involve many hand-crafted components and post-processing, The usage of transformers in instance-level point cloud segmentation is similar to the image or video domain, and most works use bipartite matching for instance-level masks for indoor and outdoor scenes. For example, Mask3D~Schult23ICRAMask3D proposes the first Transformer-based approach for 3D semantic instance segmentation. It models each object instance as an instance query and uses the transformer decoder to refine each instance query by attending to point cloud features at different scales. Meanwhile, SPFormer~sun2022superpoint learns to group the potential features from point clouds into super-points~landrieu2018large_supper_points, and directly predicts instances through instance query with a masked-based transformer decoder. The super-points utilize geometric regularities to represent homogeneous neighboring points, which is more efficient than all point features. The transformer decoder works similarly to Mask2Former, where the cross-attention between instance query and super-point features is guided by the attention mask from the previous stage. PUPS~su2023pups proposes a unified PPS system for outdoor scenes. It presents two types of learnable queries named semantic score and grouping score. The former predicts the class label for each point, while the latter indicates the probability of grouping ID for each point. Then, both queries are refined via grouped point features, which share the same ideas from previous Sparse-RCNN~peize2020sparse and K-Net~zhang2021knet. Moreover, PUPS also presents a context-aware mixing to balance the training instance samples, which achieves the new state-of-the-art results~semantic_kitti.",
      "origin_cites_number": 18
    },
    {
      "section_title": "Tuning Foundation Models",
      "level": "2",
      "content": "We divide this section into two aspects: vision adapter design and open vocabulary learning. The former introduces new ways to adapt the pre-trained large-scale foundation models for downstream tasks. The latter tries to detect and segment unknown objects with the help of the pre-trained vision language model and zero-shot knowledge transfer on unseen segmentation datasets. The core idea for vision adapter design is to extract the knowledge of foundation models and design better ways to fit the downstream settings. For open vocabulary learning, the core idea is to align pre-trained VLM features into current detectors to achieve novel class classification. \\noindent$\\bullet$ Vision Adapter and Prompting Modeling. Following the idea of prompt tuning in NLP, early works~zhou2022cocoop,zhang2021tip_adapter adopt learnable parameters with the frozen foundation models to better transfer the downstream datasets. These works use small image classification datasets for verification and achieve better results than original zero-shot results~jia2021scaling_align. Meanwhile, there are several works~lin2022frozen designing adapter and frozen foundation models for video recognition tasks. In particular, the pre-trained parameters are frozen, and only a few learnable parameters or layers are tuned. Following the idea of learnable tuning, recent works~chen2022vitadapter, rao2022denseclip extend the vision adapter into dense prediction tasks, including segmentation and detection. In particular, ViT-Adapter~chen2022vitadapter proposes a spatial prior module to solve the issue of the location prior assumptions in ViTs. The authors design a two-stream adaption framework using deformable attention and achieve comparable results in downstream tasks. From the CLIP knowledge usage view, DenseCLIP~rao2022denseclip converts the original image-text in CLIP to a pixel-text matching problem and uses the pixel-text score maps to guide the learning of dense prediction models. From the task prompt view, CLIPSeg~lueddecke22_clip_seg_prompt builds a system to generate image segmentations based on arbitrary prompts at test time. A prompt can be a text or an image where the CLIP visual model is frozen during training. In this way, the segmentation model can be turned into a different task driven by the task prompt. Previous works only focus on a single task. OneFormer~jain2022oneformer extends the Mask2Former with multiple target training setting and perform segmentation driven by the task prompt. Moreover, using a vision adapter and text prompt can easily reduce the taxonomy problems of each dataset and learn a more general representation for different segmentation tasks. Recently, SAM~kirillov2023segment proposes more generalized prompting methods, including mask, points, box, and text. The authors build a larger dataset with 1 billion masks. SAM achieves good zero-shot performance in various segmentation datasets. \\noindent$\\bullet$ Open Vocabulary Learning. Recent studies~zareian2021open, ViLD, D2Zero, detic, OV-DETR,PAD, zhou2023rethinking focus on the open vocabulary and open world setting, where their goal is to detect and segment novel classes, which are not seen during the training. Different from zero-shot learning, an open vocabulary setting assumes that large vocabulary data or knowledge can provide cues for final classification. Most models are trained by leveraging pre-trained language-text pairs, including captions and text prompts, or with the help of VLM. Then, trained models can detect and segment the novel classes with the help of weakly annotated captions or existing publicly available VLM. In particular, VilD~ViLD distills the knowledge from a trained open vocabulary image classification model CLIP into a two-stage detector. However, VilD still needs an extra visual CLIP encoder for visual distillation. To handle this, Forzen-VLM~kuo2022fvlm adopts the frozen visual clip model and combines the scores of both learned visual embedding and CLIP embedding for novel class detection. From the data augmentation view, MViT~Maaz2022Multimodal combines the Deformable DETR and CLIP text encoder for the open world class-agnostic detection, where the authors build a large dataset by mixing existing detection datasets. Motivated by the more balanced samples from image classification datasets, Detic~detic improves the performance of the novel classes with existing image classification datasets by supervising the max-size proposal with all image labels. OV-DETR~OV-DETR designs the first query-based open vocabulary framework by learning conditional matching between class text embedding and query features. Besides these open vocabulary detection settings, recent works~OpenSeg, LSeg perform open vocabulary segmentation. In particular, L-Seg~LSeg presents a new setting for language-driven semantic image segmentation and proposes a transformer-based image encoder that computes dense per-pixel embeddings according to the language inputs. OpenSeg~OpenSeg learns to generate segmentation masks for possible candidates using a DETR-like transformer. Then it performs visual-semantic alignments by aligning each word in a caption to one or a few predicted masks. BetrayedCaption~wu2023betrayed presents a unified transformer framework by joint segmentation and caption learning, where the caption part contains both caption generation and caption grounding. The novel class information is encoded into the network during training. With the goal of unifying different segmentation with text prompts, FreeSeg~qin2023freeseg adopts a similar pipeline as OpenSeg to crop frozen CLIP features for novel class classification. Meanwhile, open set segmentation~wang2021unidentified requires the model to output class agnostic masks and enhance the generality of segmentation models. Recently, ODISE~xu2023odise uses a frozen diffusion model as the feature extractor, a Mask2Former head, and joint training with caption data to perform open vocabulary panoptic segmentation. There are also several works~gupta2021ow focusing on open-world object detection, where the task detects a known set of object categories while simultaneously identifying unknown objects. In particular, OW-DETR~gupta2021ow adopts the DETR as the base detector and proposes several improvements, including attention-driven pseudo-labeling, novelty classification, and objectness scoring. In summary, most approaches~qin2023freeseg,xu2023side adopt the idea of region proposal network~ren2015faster to generate class-agnostic mask proposals via different approaches, including anchor-based and query-based decoders in Sec.~sec:method_meta. Then, the open vocabulary problem turns into a region-level matching problem to match the visual region features with pre-trained VLM language embedding.",
      "origin_cites_number": 26
    },
    {
      "section_title": "Domain-aware Segmentation",
      "level": "2",
      "content": "\\noindent$\\bullet$ Domain Adaption. Unsupervised Domain Adaptation (UDA) aims at adapting the network trained with source (synthetic) domain into target (real) domain~cordts2016cityscapes,liu2020open without access to target labels. UDA has two different settings, including semantic segmentation and object detection. Before ViTs, the previous works~yang2020fda mainly design domain-invariant representation learning strategies. DAFormer~hoyer2022daformer replaces the outdated backbone with the advanced transformer backbone~xie2021segformer and proposes three training strategies, including rare class sampling, thing-class ImageNet feature loss, and a learning rate warm-up method. It achieves new state-of-the-art results and is a strong baseline for UDA segmentation. Then, HRDA~hoyer2022hrda improves DAFormer via a multi-resolution training approach and uses various crops to preserve fine segmentation details and long-range contexts. Motivated by MIM~MaskedAutoencoders2021, MIC~hoyer2022mic proposes a masked image consistency to learn spatial context relations of the target domain as additional clues. MIC enforces the consistency between predictions of masked target images and pseudo-labels via a teacher-student framework. It is a plug-in module that is verified among various UDA settings. For detection transformers on UDA, SFA~wang2021exploring finds feature distribution alignment on CNN brings limited improvements. Instead, it proposes a domain query-based feature alignment and a token-wise feature alignment module to enhance. In particular, the alignment is achieved by introducing a domain query and performing the domain classification on the decoder. DA-DETR~zhang2021da_detr proposes a hybrid attention module (HAM), which contains a coordinate attention module and a level attention module along with the transformer encoder. A single domain-aware discriminator supervises the output of HAM. MTTrans~yu2022mttrans presents a teacher-student framework and a shared object query strategy. Meanwhile, SePiCo~xie2023sepico introduces a new framework that extracts the semantic meaning of individual pixels to learn class-discriminative and class-balanced pixel representations. It supports both CNN and Transformer architecture. The image and object features between source and target domains are aligned at local, global, and instance levels. \\noindent$\\bullet$ Multi-Dataset Segmentation. The goal of multi-dataset segmentation is to learn a universal segmentation model on various domains. MSeg~lambert2020mseg re-defines the taxonomies and aligns the pixel-level annotations by relabeling several existing semantic segmentation benchmarks. Then, the following works try to avoid taxonomy conflicts via various approaches. For example, Sentence-Seg~yin2022devil replaces each class label with a vector-valued embedding. The embedding is generated by a language model~BERT. To further handle inflexible one-hot common taxonomy, LMSeg~lmseg extends such embedding with learnable tokens~zhou2022cocoop and proposes a dataset-specific augmentation for each dataset. It dynamically aligns the segment queries in MaskFormer~cheng2021maskformer with the category embeddings for both SS and PS tasks. Meanwhile, there are several works on multi-dataset object detection~Zhou_2022_CVPR_mutl_data_det,meng2022detectionhub. In particular, Detection-Hub~meng2022detectionhub proposes to adapt object queries on language embedding of categories per dataset. Rather than previously shared embedding for all datasets, it learns semantic bias for each dataset based on the common language embedding to avoid the domain gap. Meanwhile, several works~hoyer2023domain,zhao2023style focus on segmentation domain generation, which directly transfers learned knowledge from one domain to the remaining domains. TarVIS~athar2023tarvis jointly pre-trains one video segmentation model for different tasks spanning multiple benchmarks, where it extends Mask2Former into the video domain and adopts the unified image datasets pretraining and video fine-tuning. Recently, OMG-Seg~OMGSeg has unified multi-dataset segmentation, image/video segmentation, and open-vocabulary segmentation in one shared model and achieved using one model to segment all entities.",
      "origin_cites_number": 22
    },
    {
      "section_title": "Label and Model Efficient Segmentation",
      "level": "2",
      "content": "\\noindent$\\bullet$ Weakly Supervised Segmentation. Weakly supervised segmentation methods learn segmentation with weaker annotations, such as image labels and object boxes. For weakly supervised semantic segmentation, previous works~xu2022multitoke_wsss,wang2020self improve the typical CNN pipeline with class activation maps (CAM) and use refined CAM as training labels, which requires an extra model for training. ViT-PCM~rossetti2022max shows the self-supervised transformers~chen2021mocov3 with a global max pooling can leverage patch features to negotiate pixel-label probability and achieve end-to-end training and test with one model. MCTformer~xu2022multitoke_wsss adopts the idea that the attended regions of the one-class token in the vision transformer can be leveraged to form a class-agnostic localization map. It extends to multiple classes by using multiple class tokens to learn interactions between the class tokens and the patch tokens to generate the segmentation labels. For weakly supervised instance segmentation, previous works~hsu2019bbtp,lan2021discobox,tian2021boxinst mainly leverage the box priors to supervise mask heads. Recently, MAL~hsu2019bbtp shows that vision transformers are good mask auto-labelers. It takes the box-cropped images as inputs and adopts a teacher-student framework, where the two vision transformers are trained with multiple instances loss~hsu2019bbtp. MAL proves the zero-shot segmentation ability and achieves nearly mask-supervised performance on various baselines. Meanwhile, several works~xu2022groupvit, yi2023simple explore the text-only supervision for semantic segmentation. One representative work, GroupViT~xu2022groupvit adopts ViT to group image regions into progressively larger shaped segments. \\noindent$\\bullet$ Unsupervised Segmentation. Unsupervised segmentation performs segmentation without any labels~maskfreevis. Before ViTs, recent progress~van2021unsupervised leverages the ideas from self-supervised learning. DINO~caron2021emergingDINO finds that the self-supervised ViT features naturally contain explicit information on the segmentation of input image. It finds that the attention maps between the $CLS$ token and feature to describe the segmentation of objects. Instead of using the CLS token, LOST~simeoni2021localizingLOST solves unsupervised object discovery by using the key component of the last attention layer for computing the similarities between the different patches. Several works are aiming at finding the semantic correspondence of multiple images. Then, by utilizing the correspondence maps as guidance, they achieve better performance than DINO. Given a pair of images, SETGO~hamilton2022unsupervisedSETGO finds the self-supervised learned features of DINO have semantically consistent correlations. It proposes to distill unsupervised features into high-quality discrete semantic labels. Motivated by the success of VLM, ReCo~shin2022reco adopts the language-image pre-trained model, CLIP, to retrieve large unlabeled images by leveraging the correspondences in deep representation. Then, it performs co-segmentation among both input and retrieved images. There are also several works adopting sequential pipelines. MaskDistill~van2022discoveringMaskDistill firstly identifies groups of pixels that likely belong to the same object with a bottom-up model. Then, it clusters the object masks and uses the result as pseudo ground truths to train an extra model. Finally, the output masks are selected from the offline model according to the object score. FreeSOLO~wang2022freesolo firstly adopts an extra self-supervised trained model to obtain the coarse masks. Then, it trains a SOLO-based instance segmentation model via weak supervision. CutLER~wang2023cut proposes a new framework for multiple object mask generation. It first designs the MaskCut to discover multiple coarse masks based on the self-supervised features (DINO). Then, it adopts a detector to recall the missing masks via a loss-dropping strategy. Finally, it further refines mask quality via self-training \\noindent$\\bullet$ Mobile Segmentation. Most transformer-based segmentation methods have huge computational costs and memory requirements, which make these methods unsuitable for mobile devices. Different from previous real-time segmentation methods~BiSeNet,zhao2017icnet,sfnet, the mobile segmentation methods need to be deployed on mobile devices with considering both power cost and latency. Several earlier works~maaz2023edgenext,mehta2021mobilevit,zhang2023rethinking,liang2022expediting,zhou2023edgesam,xu2024rapsam focus on a more efficient transformer backbone. In particular, Mobile-ViT~mehta2021mobilevit introduces the first transformer backbone for mobile devices. It reduces image patches via MLPs before performing MHSA and shows better task-level generalization properties. There have also been several works on designing mobile semantic segmentation using transformers. TopFormer~zhang2022topformer proposes a token pyramid module that takes the tokens from various scales as input to produce the scale-aware semantic feature. SeaFormer~wan2023seaformer proposes a squeeze-enhanced axial transformer that contains a generic attention block. The block mainly contains two branches: a squeeze axial attention layer to model efficient global context and a detail enhancement module to preserve the details. RAP-SAM~xu2024rapsam proposes a new unified setting to put real-time interactive segmentation, panoptic segmentation, and video segmentation into one framework. -2mm",
      "origin_cites_number": 24
    },
    {
      "section_title": "Class Agnostic Segmentation and Tracking",
      "level": "2",
      "content": "\\noindent$\\bullet$ Fine-grained Object Segmentation. Several applications, such as image and video editing, often need fine-grained details of object mask boundaries. Earlier CNN-based works focus on refining the object masks with extra convolution modules~kirillov2020pointrend, or extra networks~cheng2020cascadepsp. Most transformer-based approaches~transfiner,video_transfiner,liu2022simpleclick,SegRefiner, song2024basam adopt vision transformers due to their fine-grained multiscale features and long-range context modeling. Transfiner~transfiner refines the region of the coarse mask via a quad-tree transformer. By considering multiscale point features, it produces more natural boundaries while revealing details for the objects. Then, Video-Transfiner~video_transfiner refines the spatial-temporal mask boundaries by applying Transfiner~transfiner to the video segmentation method~VIS_TR. It can refine the existing video instance segmentation datasets~vis_dataset. PatchDCT~wen2023patchdct adopts the idea of ViT by making object masks into patches. Then, each mask is encoded into a DCT vector~shen2021dct, and PatchDCT designs a classifier and a regressor to refine each encoded patch. Entity segmentation~qi2022openentity aims to segment all visual entities without predicting their semantic labels. Its goal is to obtain high-quality and generalized segmentation results. \\noindent$\\bullet$ Video Object Segmentation. Recent approaches for VOS mainly focus on designing better memory-based matching methods~stm_vos. Inspired by the Non-local network~wang2018nonlocal in image recognition tasks, the representative work STM~stm_vos is the first to adopt cross-frame attention, where previous features are seen as memory. Then, the following works~yang2021associating design a better memory-matching process. associating objects with transformers (AOT)~yang2021associating matches and decodes multiple objects jointly. The authors propose a novel hierarchical matching and propagation, named long short-term transformer, where they joint persevere an identity bank and long-short term attention. XMem~cheng2022xmem proposes a mixed memory design to handle the long video inputs. The mixed memory design is also based on the self-attention architecture. Meanwhile, Clip-VOS~park2022perclip_vos introduces per-clip memory matching for inference efficiency. Recently, to enhance instance-level context, Wang~\\etal~Wang2022LookBY adds an extra query from Mask2Former into memory matching for VOS.",
      "origin_cites_number": 19
    },
    {
      "section_title": "Medical Image Segmentation",
      "level": "2",
      "content": "CNNs have achieved milestones in medical image analysis. In particular, the U-shaped architecture and skip-connections~ronneberger2015_unet,isensee2021nnu have been widely applied in various medical image segmentation tasks. With the success of ViTs, recent representative works~chen2021transunet,cao2021swinunet adopt vision transformers into the U-Net architecture and achieve better results. TransUNet~chen2021transunet merges transformer and U-Net, where the transformer encodes tokenized image patches to build the global context. Then decoder upsamples the encoded features, which are then combined with the high-resolution CNN feature maps to enable precise localization. Swin-Unet~cao2021swinunet designs a symmetric Swin-like~liu2021swin decoder to recover fine details. TransFuse~zhang2021transfuse combines transformers and CNNs in a parallel style, where global dependency and low-level spatial details can be efficiently captured jointly. UNETR~hatamizadeh2022unetr focuses on 3D input medical images and designs a similar U-Net-like architecture. The encoded representations of different layers in the transformer are extracted and merged with a decoder via skip connections to get the final 3D mask outputs.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Benchmark Results",
      "level": "1",
      "content": "In this section, we report recent transformer-based visual segmentation and tabulate the performance of previously discussed algorithms. For each reviewed field, the most widely used datasets are selected for performance benchmark in Sec.~sec:main_image_results and Sec.~sec:main_results_video_segmentation_benchmarks. We further re-benchmark several representative works in Sec.~sec:our_re_becnmarking using {the same data augmentations} and {feature extractor}. Note that we only list \\textit{published works} for reference. For simplicity, we have excluded several works on representation learning and only present specific segmentation methods. For a comprehensive method comparison, please refer to the supplementary material that provides a more detailed analysis. In addition, several works~zhang2023simple,zou2023segment,HIPIE achieve better results. However, due to the extra datasets~objects365 they used, we do not list them here.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Main Results on Image Segmentation Datasets",
      "level": "2",
      "content": "\\noindent$\\bullet$ \\noindent Results On Semantic Segmentation Datasets. In Tab.~tab:sem_seg_datasets_results, Mask2Former~cheng2021mask2former and OneFormer~jain2022oneformer perform the best on Cityscapes and ADE20K dataset, while SegNext~guo2022segnext achieves the best results on COCO-Stuff and Pascal-Context datasets. \\noindent$\\bullet$ \\noindent Results on COCO Instance Segmentation. In Tab.~tab:instance_seg_results, Mask DINO~li2022maskdino achieves the best results on the COCO instance segmentation with both ResNet and Swin-L backbones. \\noindent$\\bullet$ \\noindent Results on Panoptic Segmentation. In Tab.~tab:panoptic_seg_results, for panoptic segmentation, Mask DINO~li2022maskdino and K-Max Deeplab~kmax_deeplab_2022 achieve the best results on the COCO dataset. K-Max Deeplab also achieves the best results on Cityscapes. OneFormer~jain2022oneformer performs the best on ADE20K.",
      "origin_cites_number": 43
    },
    {
      "section_title": "Re-Benchmarking For Image Segmentation",
      "level": "2",
      "content": "\\noindent$\\bullet$ Motivation. We perform re-benchmarking on two segmentation tasks: semantic segmentation and panoptic segmentation on four public datasets, including ADE20K, COCO, Cityscapes, and COCO-Stuff datasets. In particular, we want to explore the effect of the transformer decoder. Thus, we use the same encoder~resnet and neck architecture~zhu2020deformabledetr for a fair comparison. \\noindent$\\bullet$ Results on Semantic Segmentation. As shown in Tab.~tab:experiments_res_sem_seg, we carry out re-benchmark experiments for SS. In particular, using the same neck architecture, Segformer+~xie2021segformer achieves the best results on COCO-Stuff and Cityscapes. Mask2Former achieves the best result on the ADE-20k dataset. \\noindent$\\bullet$ Results on Instance Segmentation. In Tab.~tab:experiments_res_ins_seg, we also explore the instance segmentation methods on COCO datasets. Under the same neck architecture, we observe gains on both K-Net and MaskFormer, compared with origin results in Tab.~tab:instance_seg_results. Mask2Former achieve the best results. \\noindent$\\bullet$ Results on Panoptic Segmentation. In Tab.~tab:experiments_res_pano_seg, we present the re-benchmark results for PS. In particular, Mask2Former achieves the best results on all three datasets. Compared with K-Net and MaskFormer, both K-Net+ and MaskFormer+ achieve over 3-4\\% improvements due to the usage of stronger neck~zhu2020deformabledetr, which close the gaps between their original results and Mask2Former.",
      "origin_cites_number": 19
    },
    {
      "section_title": "Main Results for Video Segmentation Datasets",
      "level": "2",
      "content": "\\noindent$\\bullet$ \\noindent Results On Video Semantic Segmentation In Tab.~tab:video_semantic_seg_results, we report VSS results on VPSW. Among the methods, TubeFormer~kim2022tubeformer achieves the best results. \\noindent$\\bullet$ \\noindent Results on Video Instance Segmentation In Tab.~tab:video_instance_seg_results, for VIS, CTVIS~ying2023ctvis achieves the best result on YT-VIS-2019 and YT-VIS-2021 using ResNet50 backbone. GenVIS~heo2022generalized achieves better results on OVIS using ResNet50 backbone. When adopting Swin-L backbone, CTVIS~ying2023ctvis achieves the best results. \\noindent$\\bullet$ Results on Video Panoptic Segmentation In Tab.~tab:video_panoptic_seg_results, for VPS, SLOT-VPS~zhou2022slot achieves the best results on Cityscapes-VPS. TubeLink~tubelink achieves the best results on the VIP-Seg dataset. Video K-Net~li2022videoknet achieves the best results on the KITTI-STEP dataset.",
      "origin_cites_number": 32
    },
    {
      "section_title": "Future Directions",
      "level": "1",
      "content": "\\noindent$\\bullet$ General and Unified Image/Video Segmentation. \\if 0 Using Transformer to unify different segmentation tasks is a trend. Recent works~zhang2021knet,li2022videoknet,wang2020maxDeeplab,kim2022tubeformer,jain2022oneformer use the query-based transformer to perform different segmentation tasks using one architecture. One possible research direction is unifying image and video segmentation tasks via only one model on various segmentation datasets. These universal models can achieve general and robust segmentation in various scenes, \\eg, detecting and segmenting rare classes in various scenes help the robot to make better decisions. These will be more practical and robust in several applications, including robot navigation and self-driving cars. \\fi {The trend of using transformers to unify diverse segmentation tasks is gaining traction. Recent studies~zhang2021knet,li2022videoknet,wang2020maxDeeplab,kim2022tubeformer,jain2022oneformer,OMGSeg,wu2023uniref++ have employed query-based transformers for various segmentation tasks within a unified architecture. A promising research avenue is the integration of image and video segmentation tasks in a universal model across different datasets. Such models may achieve general, robust segmentation capabilities in multiple scenarios, like detecting rare classes for improved robotic decision-making. This approach holds significant practical value, particularly in applications like robot navigation and autonomous vehicles.} \\noindent$\\bullet$ Joint Learning with Multi-Modality. \\if 0 The lack of inductive biases makes Transformers versatile in handling any modality. Thus, using Transformer to unify the vision and language tasks is a general trend. Segmentation tasks provide pixel-level cues, which may also benefit the related vision language tasks, including text-image retrieval and caption generation~phraseclick. Recent works~lu2022unified,zou2022xdecoder jointly learn the segmentation and visual language tasks in one universal transformer architecture, which provides a direction to combining segmentation learning across multi-modality. \\fi {Transformers' inherent flexibility in handling various modalities positions them as ideal for unifying vision and language tasks. Segmentation tasks, which offer pixel-level information, can enhance associated vision-language tasks such as text-image retrieval and caption generation~phraseclick. Recent studies~lu2022unified,zou2022xdecoder,qi2024generalizable,yuan2024open demonstrate the potential of a universal transformer architecture that concurrently learns segmentation alongside visual language tasks, paving the way for integrated multi-modal segmentation learning.} \\noindent$\\bullet$ \\noindent Life-Long Learning for Segmentation. Existing segmentation methods are usually benchmarked on closed-world datasets with a set of predefined categories, \\ie, assuming that the training and testing samples have the same categories and feature spaces that are known beforehand. However, realistic scenarios are usually open-world and non-stationary, where novel classes may occur continuously~zhang2021prototypical. For example, unseen situations can occur unexpectedly in self-driving vehicles and medical diagnoses. There is a distinct gap between the performance and capabilities of existing methods in realistic and open-world settings. Thus, it is desired to gradually and continuously incorporate novel concepts into the existing knowledge base of segmentation models, making the model capable of lifelong learning. \\noindent$\\bullet$ Long Video Segmentation in Dynamic Scenes. Long videos introduce several challenges~MOSE,MeViS,zhou2024dvis. First, existing video segmentation methods are designed to work with short video inputs and may struggle to associate instances over longer periods. Thus, new methods must incorporate long-term memory design and consider the association of instances over a more extended period. Second, maintaining segmentation mask consistency over long periods can be difficult, especially when instances move in and out of the scene. This requires new methods to incorporate temporal consistency constraints and update the segmentation masks over time. Third, heavy occlusion can occur in long videos, making it challenging to segment all instances accurately. New methods should incorporate occlusion reasoning and detection to improve segmentation accuracy. Finally, long video inputs often involve various scene inputs, which can bring domain robustness challenges for video segmentation models. New methods must incorporate domain adaptation techniques to ensure the model can handle diverse scene inputs. In short, addressing these challenges requires the development of new long video segmentation models that incorporate advanced memory design, temporal consistency constraints, occlusion reasoning, and detection techniques. \\noindent$\\bullet$ Generative Segmentation. With the rise of stronger generative models, recent works~chen2022generalist, wang2024explore, xie2023mosaicfusion solve image segmentation problems via generative modeling, inspired by a stronger transformer decoder and high-resolution representation in the diffusion model~rombach2021highresolution. Adopting a generative design avoids the transformer decoder and object query design, which makes the entire framework simpler. However, these generative models typically introduce a complicated training pipeline. A simpler training pipeline is needed for further research. \\noindent$\\bullet$ Segmentation with Visual Reasoning. Visual reasoning~johnson2015image,pvsg,yang_psg,psg4d,wang2023pair requires the robot to understand the connections between objects in the scene, and this understanding plays a crucial role in motion planning. Previous research has explored using segmentation results as input to visual reasoning models for various applications, such as object tracking and scene understanding. Joint segmentation and visual reasoning can be a promising direction, with the potential for mutual benefits for both segmentation and relation classification. By incorporating visual reasoning into the segmentation process, researchers can leverage the power of reasoning to improve the segmentation accuracy, while segmentation can provide better input for visual reasoning. -4mm",
      "origin_cites_number": 11
    },
    {
      "section_title": "Conclusion",
      "level": "1",
      "content": "This survey provides a comprehensive review of recent advancements in transformer-based visual segmentation, which, to our knowledge, is the first of its kind. The paper covers essential background knowledge and an overview of previous works before transformers and summarizes more than 120 deep-learning models for various segmentation tasks. The recent works are grouped into six categories based on the meta-architecture of the segmenter. Additionally, the paper reviews five specific subfields and reports the results of several representative segmentation methods on widely-used datasets. To ensure fair comparisons, we also re-benchmark several representative works under the same settings. Finally, we conclude by pointing out future research directions for transformer-based visual segmentation. \\noindent Acknowledgement. This work is supported by The Alan Turing Institute (UK) through the project 'Turing-DSO Labs Singapore Collaboration' (SDCfP2\\textbackslash100009). This study is also supported under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative and Singapore MOE AcRF Tier 1 (RG16/21). , as well as cash and in-kind contributions from the industry partner(s).",
      "origin_cites_number": 0
    }
  ],
  "literature_review_id": 258212528,
  "meta_info": {
    "cite_counts": 407,
    "Conference_journal_name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "influentialcitationcount": 0,
    "Author_info": {
      "Publicationsh": 51,
      "h_index": 27,
      "Citations": 3045,
      "Highly Influential Citations": 0
    },
    "all_cites_title": [
      "Contour and texture analysis for image segmentation",
      "Normalized cuts and image segmentation",
      "Multiclass spectral clustering",
      "Object class segmentation using random forests",
      "Snakes: Active contour models",
      "Imagenet large scale visual recognition challenge",
      "Deep residual learning for image recognition",
      "Very deep convolutional networks for large-scale image recognition",
      "Fully convolutional networks for semantic segmentation",
      "DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs",
      "Pyramid scene parsing network",
      "Context contrasted feature and gated multi-scale aggregation for scene segmentation",
      "Attention is all you need",
      "Long short-term memory",
      "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "Language models are few-shot learners",
      "PSANet: Point-wise spatial attention network for scene parsing",
      "Non-local neural networks",
      "Exploring self-attention for image recognition",
      "Local relation networks for image recognition",
      "An image is worth 16x16 words: Transformers for image recognition at scale",
      "End-to-end object detection with transformers",
      "Swin transformer: Hierarchical vision transformer using shifted windows",
      "Masked autoencoders are scalable vision learners",
      "Deformable DETR: Deformable transformers for end-to-end object detection",
      "MaX-DeepLab: End-to-end panoptic segmentation with mask transformers",
      "Pre-trained image processing transformer",
      "Is space-time attention all you need for video understanding",
      "Point transformer",
      "3D object detection with pointformer",
      "A survey on vision transformer",
      "Transformers in vision: A survey",
      "A survey of transformers",
      "Video transformers: A survey",
      "3D vision with transformers: A survey",
      "Multimodal learning with transformers: A survey",
      "Image segmentation using deep learning: A survey",
      "A brief survey on semantic segmentation with deep learning",
      "A survey on deep learning technique for video segmentation",
      "Largescale video panoptic segmentation in the wild: A benchmark",
      "The PASCAL visual object classes (voc) challenge",
      "The role of context for object detection and semantic segmentation in the wild",
      "Microsoft COCO: Common objects in context",
      "Semantic understanding of scenes through the ADE20K dataset",
      "The cityscapes dataset for semantic urban scene understanding",
      "The mapillary vistas dataset for semantic understanding of street scenes",
      "Modeling context in referring expressions",
      "GRES: Generalized referring expression segmentation",
      "VSPW: A largescale dataset for video scene parsing in the wild",
      "Video instance segmentation",
      "Occluded video instance segmentation: A benchmark",
      "Video panoptic segmentation",
      "STEP: Segmenting and tracking every pixel",
      "The 2017 davis challenge on video object segmentation",
      "Youtube-vos: A large-scale video object segmentation benchmark",
      "MOSE: A new dataset for video object segmentation in complex scenes",
      "MeViS: A large-scale benchmark for video segmentation with motion expressions",
      "Learning a discriminative feature network for semantic segmentation",
      "Semantic segmentation with context encoding and multi-path decoding",
      "Large kernel mattersimprove semantic segmentation by global convolutional network",
      "Semantic correlation promoted shape-variant context for segmentation",
      "Rethinking atrous convolution for semantic image segmentation",
      "Toward achieving robust low-level and high-level scene parsing",
      "Gated fully fusion for semantic segmentation",
      "Global aggregation then local distribution for scene parsing",
      "Object-contextual representations for semantic segmentation",
      "Dual graph convolutional network for semantic segmentation",
      "Semantic flow for fast and accurate scene parsing",
      "Sfnet: Faster, accurate, and domain agnostic semantic segmentation via semantic flow",
      "BiSeNet: Bilateral segmentation network for real-time semantic segmentation",
      "Pointrend: Image segmentation as rendering",
      "Boundary-aware feature propagation for scene segmentation",
      "Improving semantic segmentation via decoupled body and edge supervision",
      "Enhanced boundary learning for glass-like object segmentation",
      "Dual attention network for scene segmentation",
      "Conditional convolutions for instance segmentation",
      "Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth",
      "Semantic instance segmentation with a discriminative loss function",
      "Hybrid task cascade for instance segmentation",
      "Mask encoding for single shot instance segmentation",
      "YOLACT: Real-time instance segmentation",
      "Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution",
      "Panoptic-DeepLab: A simple, strong, and fast baseline for bottom-up panoptic segmentation",
      "Tensormask: A foundation for dense object segmentation",
      "SOLOv2: Dynamic and fast instance segmentation",
      "UPSNet: A unified panoptic segmentation network",
      "Fully convolutional networks for panoptic segmentation",
      "Axial-DeepLab: Stand-alone axial-attention for panoptic segmentation",
      "Semantic video cnns through representation warping",
      "Clockwork convnets for video semantic segmentation",
      "Deep feature flow for video recognition",
      "Classifying, segmenting, and tracking object instances in video with mask propagation",
      "CompFeat: Comprehensive feature aggregation for video instance segmentation",
      "Improving video instance segmentation via temporal pyramid routing",
      "Vip-deeplab: Learning visual perception with depth-aware video panoptic segmentation",
      "Deformable convnets v2: More deformable, better results",
      "A benchmark dataset and evaluation methodology for video object segmentation",
      "MOTS: Multi-object tracking and segmentation",
      "PointNet: Deep learning on point sets for 3d classification and segmentation",
      "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
      "Learning object bounding boxes for 3d instance segmentation on point clouds",
      "GSPN: Generative shape proposal network for 3d instance segmentation in point cloud",
      "SGPN: Similarity group proposal network for 3d point cloud instance segmentation",
      "PointGroup: Dual-set point grouping for 3d instance segmentation",
      "Interpolated convolutional networks for 3d point cloud understanding",
      "RandLA-Net: Efficient semantic segmentation of largescale point clouds",
      "(AF)2-S3Net: Attentive feature fusion with adaptive feature selection for sparse semantic segmentation network",
      "Panoptic-PolarNet: Proposal-free lidar point cloud panoptic segmentation",
      "Sparse cross-scale attention network for efficient lidar panoptic segmentation",
      "LiDAR-based panoptic segmentation via dynamic shifting network",
      "panoptic lidar segmentation",
      "Cylindrical and asymmetrical 3d convolution networks for lidar segmentation",
      "A2-Nets: Double attention networks",
      "Faster R-CNN: Towards realtime object detection with region proposal networks",
      "Feature pyramid networks for object detection",
      "Focal loss for dense object detection",
      "FCOS: A simple and strong anchor-free object detector",
      "NAS-FPN: Learning scalable feature pyramid architecture for object detection",
      "Lite DETR: An interleaved multi-scale encoder for efficient detr",
      "The hungarian method for the assignment problem",
      "V-Net: Fully convolutional neural networks for volumetric medical image segmentation",
      "SegFormer: Simple and efficient design for semantic segmentation with transformers",
      "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers",
      "Encoderdecoder with atrous separable convolution for semantic image segmentation",
      "Omg-seg: Is one model good enough for all segmentation",
      "TarViS: A unified approach for target-based video segmentation",
      "Polyphonicformer: Unified query learning for depth-aware video panoptic segmentation",
      "Uniref++: Segment every reference object in spatial and temporal spaces",
      "Training data-efficient image transformers & distillation through attention",
      "Multiscale vision transformers",
      "MViTv2: Improved multiscale vision transformers for classification and detection",
      "MPViT: Multi-path vision transformer for dense prediction",
      "XCiT: Crosscovariance image transformers",
      "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
      "Crossvit: Cross-attention multiscale vision transformer for image classification",
      "Feature pyramid transformer",
      "Co-scale conv-attentional image transformers",
      "CMT: Convolutional neural networks meet vision transformers",
      "Twins: Revisiting the design of spatial attention in vision transformers",
      "CvT: Introducing convolutions to vision transformers",
      "ViTAE: Vision transformer advanced by exploring intrinsic inductive bias",
      "A ConvNet for the 2020s",
      "On the connection between local attention and dynamic depth-wise convolution",
      "SegNeXt: Rethinking convolutional attention design for semantic segmentation",
      "MetaFormer is actually what you need for vision",
      "Demystify transformers & convolutions in modern image deep networks",
      "A simple framework for contrastive learning of visual representations",
      "Momentum contrast for unsupervised visual representation learning",
      "An empirical study of training selfsupervised vision transformers",
      "BEiT: BERT pre-training of image transformers",
      "Masked feature prediction for self-supervised visual pre-training",
      "Test-time training with masked autoencoders",
      "Exploring long-sequence masked autoencoders",
      "ConvMAE: Masked convolution meets masked autoencoders",
      "Learning transferable visual models from natural language supervision",
      "Scaling languageimage pre-training via masking",
      "SparseR-CNN: End-to-end object detection with learnable proposals",
      "Instances as queries",
      "ISTR: End-to-end instance segmentation via transformers",
      "SOLQ: Segmenting objects by learning queries",
      "Boundarysqueeze: Image segmentation as boundary squeezing",
      "K-Net: Towards unified image segmentation",
      "Per-pixel classification is not all you need for semantic segmentation",
      "Panoptic segformer: Delving deeper into panoptic segmentation with transformers",
      "End-to-end video instance segmentation with transformers",
      "TransVOD: End-to-end video object detection with spatial-temporal transformers",
      "Temporally efficient vision transformer for video instance segmentation",
      "Mask2former for video instance segmentation",
      "Video instance segmentation using inter-frame communication transformers",
      "SeqFormer: Sequential transformer for video instance segmentation",
      "Video K-Net: A simple, strong, and unified baseline for video segmentation",
      "TubeFormer-DeepLab: Video mask transformer",
      "Conditional detr for fast training convergence",
      "Conditional detr v2: Efficient detection transformer with box queries",
      "Anchor DETR: Query design for transformer-based detector",
      "DAB-DETR: Dynamic anchor boxes are better queries for DETR",
      "DN-DETR: Accelerate detr training by introducing query denoising",
      "DINO: DETR with improved denoising anchor boxes for end-to-end object detection",
      "Mask DINO: Towards a unified transformer-based framework for object detection and segmentation",
      "Learning equivariant segmentation with instance-unique querying",
      "DETRs with hybrid matching",
      "Group DETR: Fast detr training with group-wise one-to-many assignment",
      "Detrs with collaborative hybrid assignments training",
      "Track-Former: Multi-object tracking with transformers",
      "TransTrack: Multiple-object tracking with transformer",
      "MOTR: End-to-end multiple-object tracking with transformer",
      "MinVIS: A minimal video instance segmentation framework without video-based training",
      "In defense of online models for video instance segmentation",
      "Panopticpartformer: Learning a unified model for panoptic part segmentation",
      "PanopticDepth: A unified framework for depth-aware panoptic segmentation",
      "Fashionformer: A simple, effective and unified baseline for human fashion segmentation and recognition",
      "Multi-task learning with multi-query transformer for dense prediction",
      "Inverted pyramid multi-task transformer for dense scene understanding",
      "Vision-language transformer and query generation for referring segmentation",
      "ReSTR: Convolutionfree referring image segmentation using transformers",
      "CRIS: CLIP-driven referring image segmentation",
      "End-to-end referring video object segmentation with multimodal transformers",
      "Languagebridged spatial-temporal interaction for referring video object segmentation",
      "Towards robust referring image segmentation",
      "Language as queries for referring video object segmentation",
      "Few-shot segmentation via cycle-consistent transformer",
      "Associating objects with transformers for video object segmentation",
      "Matteformer: Transformer-based image matting via prior-tokens",
      "A transformer-based decoder for semantic segmentation with multi-level context mining",
      "Structtoken: Rethinking semantic segmentation with structural prior",
      "BATMAN: Bilateral attention transformer in motion-appearance neighboring space for video object segmentation",
      "Mask matching transformer for few-shot segmentation",
      "Swin transformer v2: Scaling up capacity and resolution",
      "CycleMLP: A mlp-like architecture for dense prediction",
      "MLPmixer: An all-MLP architecture for vision",
      "CycleMLP: A MLP-like architecture for dense prediction",
      "Hire-MLP: Vision MLP via hierarchical rearrangement",
      "Context autoencoder for self-supervised representation learning",
      "Designing bert for convolutional networks: Sparse and hierarchical masked modeling",
      "Emerging properties in self-supervised vision transformers",
      "Scaling up visual and vision-language representation learning with noisy text supervision",
      "Uni-Perceiver v2: A generalist model for large-scale vision and vision-language tasks",
      "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "Masked autoencoders as spatiotemporal learners",
      "Video swin transformer",
      "VLT: Vision-language transformer and query generation for referring segmentation",
      "SOIT: Segmenting objects with instance-aware transformers",
      "Segmenter: Transformer for semantic segmentation",
      "Masked-attention mask transformer for universal image segmentation",
      "CMT-DeepLab: Clustering mask transformers for panoptic segmentation",
      "k-means mask transformer",
      "Sparse instance activation for real-time instance segmentation",
      "Accelerating DETR convergence via semantic-aligned matching",
      "Fast convergence of detr with spatially modulated co-attention",
      "AdaMixer: A fast-converging query-based object detector",
      "Endto-end object detection with adaptive clustering transformer",
      "Dynamic DETR: End-to-end object detection with dynamic attention",
      "Sparse detr: Efficient end-to-end object detection with learnable sparsity",
      "VITA: Video instance segmentation via object token association",
      "Generalized decoding for pixel, image and language",
      "X-DETR: A versatile architecture for instance-wise visionlanguage tasks",
      "Video-kMaX: A simple unified approach for online and near-online video panoptic segmentation",
      "Tubelink: A flexible cross tube baseline for universal video segmentation",
      "Efficient DETR: improving end-toend object detector with dense prior",
      "MP-Former: Mask-piloted transformer for image segmentation",
      "Towards data-efficient detection transformers",
      "Panopticpartformer++: A unified and decoupled view for panoptic part segmentation",
      "Decoupling static and hierarchical motion perception for referring video segmentation",
      "Semantic-promoted debiasing and background disambiguation for zero-shot instance segmentation",
      "Referring image editing: Object-level image editing via referring expressions",
      "Instance-specific feature propagation for referring segmentation",
      "Multi-task collaborative network for joint referring expression comprehension and segmentation",
      "Multi-level representation learning with semantic alignment for referring video object segmentation",
      "MDETR-modulated detection for end-to-end multi-modal understanding",
      "Prototype as query for few shot semantic segmentation",
      "Deep interactive image matting with feature propagation",
      "Reference twice: A simple and unified baseline for few-shot instance segmentation",
      "PCT: Point cloud transformer",
      "Stratified transformer for 3d point cloud segmentation",
      "Point-BERT: Pre-training 3D point cloud transformers with masked point modeling",
      "Masked autoencoders for point cloud self-supervised learning",
      "Point-M2AE: Multi-scale masked autoencoders for hierarchical point cloud pre-training",
      "Mask3D for 3D Semantic Instance Segmentation",
      "Superpoint transformer for 3d scene instance segmentation",
      "Large-scale point cloud semantic segmentation with superpoint graphs",
      "PUPS: Point cloud unified panoptic segmentation",
      "A dataset for semantic segmentation of point cloud sequences",
      "Conditional prompt learning for vision-language models",
      "Tip-Adapter: Training-free clip-adapter for better visionlanguage modeling",
      "Frozen clip models are efficient video learners",
      "Vision transformer adapter for dense predictions",
      "DenseCLIP: Language-guided dense prediction with contextaware prompting",
      "Image segmentation using text and image prompts",
      "OneFormer: One transformer to rule universal image segmentation",
      "Segment anything",
      "Open-vocabulary object detection using captions",
      "Open-vocabulary object detection via vision and language knowledge distillation",
      "Detecting twenty-thousand classes using image-level supervision",
      "Open-vocabulary detr with conditional matching",
      "Primitive generation and semanticrelated alignment for universal zero-shot segmentation",
      "Rethinking evaluation metrics of open-vocabulary segmentaion",
      "F-VLM: Open-vocabulary object detection upon frozen vision and language models",
      "Class-agnostic object detection with multi-modal transformer",
      "Scaling open-vocabulary image segmentation with image-level labels",
      "Language-driven semantic segmentation",
      "Betrayed by captions: Joint caption grounding and generation for open vocabulary instance segmentation",
      "FreeSeg: Unified, universal and open-vocabulary image segmentation",
      "Unidentified video objects: A benchmark for dense, open-world segmentation",
      "Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models",
      "OW-DETR: Open-world detection transformer",
      "Side adapter network for open-vocabulary semantic segmentation",
      "Open compound domain adaptation",
      "FDA: Fourier domain adaptation for semantic segmentation",
      "DAFormer: Improving network architectures and training strategies for domain-adaptive semantic segmentation",
      "HRDA: Context-aware high-resolution domain-adaptive semantic segmentation",
      "MIC: Masked image consistency for context-enhanced domain adaptation",
      "Exploring sequence feature alignment for domain adaptive detection transformers",
      "DA-DETR: Domain adaptive detection transformer by hybrid attention",
      "MTTrans: Cross-domain object detection with mean teacher transformer",
      "Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation",
      "MSeg: A composite dataset for multi-domain semantic segmentation",
      "The devil is in the labels: Semantic segmentation from sentences",
      "LMSeg: Language-guided multi-dataset segmentation",
      "Simple multi-dataset detection",
      "Detection hub: Unifying object detection datasets via query adaptation on language embedding",
      "Domain adaptive and generalizable network architectures and training strategies for semantic image segmentation",
      "Style-hallucinated dual consistency learning: A unified framework for visual domain generalization",
      "Multiclass token transformer for weakly supervised semantic segmentation",
      "Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation",
      "Max pooling with vision transformers reconciles class and shape in weakly supervised semantic segmentation",
      "Weakly supervised instance segmentation using the bounding box tightness prior",
      "DiscoBox: Weakly supervised instance segmentation and semantic correspondence from box supervision",
      "BoxInst: High-performance instance segmentation with box annotations",
      "Groupvit: Semantic segmentation emerges from text supervision",
      "A simple framework for text-supervised semantic segmentation",
      "Maskfree video instance segmentation",
      "Unsupervised semantic segmentation by contrasting object mask proposals",
      "Localizing objects with selfsupervised transformers and no labels",
      "Unsupervised semantic segmentation by distilling feature correspondences",
      "ReCo: Retrieve and co-segment for zero-shot transfer",
      "Discovering object masks with transformers for unsupervised semantic segmentation",
      "FreeSOLO: Learning to segment objects without annotations",
      "Cut and learn for unsupervised object detection and instance segmentation",
      "ICNet for real-time semantic segmentation on high-resolution images",
      "EdgeNeXt: efficiently amalgamated cnn-transformer architecture for mobile vision applications",
      "Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer",
      "Rethinking mobile block for efficient neural models",
      "Expediting large-scale vision transformer for dense prediction without fine-tuning",
      "Edgesam: Prompt-in-theloop distillation for on-device deployment of sam",
      "Rap-sam: Towards real-time allpurpose segment anything",
      "TopFormer: Token pyramid transformer for mobile semantic segmentation",
      "SeaFormer: Squeezeenhanced axial transformer for mobile semantic segmentation",
      "CascadePSP: Toward class-agnostic and very high-resolution segmentation via global and local refinement",
      "Mask transfiner for high-quality instance segmentation",
      "Video mask transfiner for high-quality video instance segmentation",
      "SimpleClick: Interactive image segmentation with simple vision transformers",
      "Segrefiner: Towards model-agnostic segmentation refinement with discrete diffusion process",
      "Ba-sam: Scalable bias-mode attention mask for segment anything model",
      "Patchdct: Patch refinement for high quality instance segmentation",
      "Dct-mask: Discrete cosine transform mask representation for instance segmentation",
      "Open world entity segmentation",
      "Video object segmentation using space-time memory networks",
      "XMem: Long-term video object segmentation with an atkinson-shiffrin memory model",
      "Per-clip video object segmentation",
      "Look before you match: Instance understanding matters in video object segmentation",
      "U-Net: Convolutional networks for biomedical image segmentation",
      "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
      "Transunet: Transformers make strong encoders for medical image segmentation",
      "Swin-Unet: Unet-like pure transformer for medical image segmentation",
      "Transfuse: Fusing transformers and cnns for medical image segmentation",
      "UNETR: Transformers for 3d medical image segmentation",
      "A simple framework for open-vocabulary segmentation and detection",
      "Segment everything everywhere all at once",
      "Hierarchical open-vocabulary universal image segmentation",
      "Objects365: A large-scale, high-quality dataset for object detection",
      "Clustseg: Clustering for universal segmentation",
      "Coarse-to-fine feature mining for video semantic segmentation",
      "Mining relations among cross-frame affinities for video semantic segmentation",
      "You only segment once: Towards real-time panoptic segmentation",
      "Ctvis: Consistent training for online video instance segmentation",
      "A generalized framework for video instance segmentation",
      "Slot-VPS: Object-centric representation learning for video panoptic segmentation",
      "Phraseclick: toward achieving flexible interactive segmentation by phrase and click",
      "Unified-IO: A unified model for vision, language, and multi-modal tasks",
      "Generalizable entity grounding via assistance of large language model",
      "Openvocabulary sam: Segment and recognize twenty-thousand classes interactively",
      "Prototypical matching and open set rejection for zero-shot semantic segmentation",
      "Dvis-daq: Improving video segmentation via dynamic anchor queries",
      "A generalist framework for panoptic segmentation of images and videos",
      "Explore in-context segmentation via latent diffusion models",
      "Mosaicfusion: Diffusion models as data augmenters for large vocabulary instance segmentation",
      "Highresolution image synthesis with latent diffusion models",
      "Image retrieval using scene graphs",
      "Panoptic video scene graph generation",
      "Panoptic scene graph generation",
      "4d panoptic scene graph generation",
      "Pair then relation: Pair-net for panoptic scene graph generation",
      "HOTA: A higher order metric for evaluating multi-object tracking",
      "Oneformer3d: One transformer for unified point cloud segmentation",
      "Point transformer",
      "Point transformer v2: Grouped vector attention and partition-based pooling",
      "Mask3d: Mask transformer for 3d semantic instance segmentation",
      "Superpoint transformer for 3d scene instance segmentation",
      "Towards open vocabulary learning: A survey",
      "Decoupling zero-shot semantic segmentation",
      "A simple baseline for open-vocabulary semantic segmentation with pre-trained vision-language model",
      "DenseCLIP: Extract free dense labels from clip",
      "Open-vocabulary panoptic segmentation with maskclip",
      "Open-vocabulary semantic segmentation with mask-adapted clip",
      "Global knowledge calibration for fast openvocabulary segmentation",
      "Side adapter network for open-vocabulary semantic segmentation",
      "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "Im2text: Describing images using 1 million captioned photographs",
      "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "Microsoft coco captions: Data collection and evaluation server",
      "Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling",
      "Mask-free ovis: Open-vocabulary instance segmentation without manual mask annotations",
      "A simple baseline for open-vocabulary semantic segmentation with pre-trained vision-language model",
      "Weakly-supervised semantic segmentation with visual words learning and hybrid pooling",
      "Mctformer+: Multi-class token transformer for weakly supervised semantic segmentation",
      "Token contrast for weaklysupervised semantic segmentation",
      "Weaktr: Exploring plain vision transformer for weakly-supervised semantic segmentation",
      "Causal unsupervised semantic segmentation",
      "Simple copy-paste is a strong data augmentation method for instance segmentation",
      "MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark",
      "Mmdetection: Open mmlab detection toolbox and benchmark",
      "Afformer: Head-free lightweight semantic segmentation with linear transformer",
      "Vision transformers are good mask auto-labelers",
      "Style-hallucinated dual consistency learning for domain generalized semantic segmentation",
      "Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening",
      "Style-hallucinated dual consistency learning: A unified framework for visual domain generalization",
      "Lidar-based 4d panoptic segmentation via dynamic shifting network"
    ]
  }
}