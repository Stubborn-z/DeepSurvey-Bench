{
    "survey": "# Transformer-Based Visual Segmentation: A Comprehensive Survey\n\n## 1 Introduction\n\n### 1.1 Overview of Visual Segmentation Tasks\n\n---\n\nVisual segmentation is a fundamental task in computer vision that involves partitioning an image, video frame, or 3D point cloud into meaningful segments or regions, each associated with a specific label or category. This task is critical for enabling machines to understand and interpret visual data at a granular level, mimicking human perception. The field of visual segmentation has evolved significantly, driven by advancements in deep learning and transformer-based architectures, which have addressed longstanding challenges in global context modeling and fine-grained detail capture.  \n\nVisual segmentation can be broadly categorized into three main types: semantic segmentation, instance segmentation, and panoptic segmentation, each serving distinct purposes and addressing different aspects of scene understanding [1; 2].  \n\n**Semantic segmentation** assigns a class label to every pixel in an image, grouping pixels into semantically meaningful categories such as \"road,\" \"sky,\" or \"person\" without distinguishing between individual instances of the same class. This task is essential for applications like autonomous driving, where understanding the drivable area or detecting pedestrians is crucial for safe navigation [3; 4]. Semantic segmentation is also widely used in medical imaging, where precise delineation of anatomical structures or pathological regions can aid in diagnosis and treatment planning [5; 6].  \n\n**Instance segmentation** goes a step further by not only classifying pixels but also distinguishing between different instances of the same class. For example, it can identify and separate individual cars or pedestrians in a crowded scene. This capability is particularly valuable in robotics and surveillance, where tracking and interacting with multiple objects are required [7; 8]. Instance segmentation often relies on object detection frameworks combined with pixel-level mask prediction, as seen in models like Mask R-CNN [9].  \n\n**Panoptic segmentation** unifies semantic and instance segmentation into a single framework, providing a holistic understanding of a scene by labeling every pixel with both a semantic category and an instance identifier [1; 2]. This task is particularly useful in autonomous driving and robotics, where a comprehensive understanding of the environment—including both \"stuff\" (e.g., roads, sky) and \"things\" (e.g., cars, pedestrians)—is necessary for decision-making [10; 11]. Panoptic segmentation has also found applications in medical imaging, where it can simultaneously segment organs (stuff) and lesions (things) [5].  \n\nThe significance of visual segmentation extends across numerous domains. In autonomous driving, segmentation enables vehicles to perceive and interpret their surroundings, identifying drivable regions, obstacles, and traffic participants [3; 12]. In medical imaging, segmentation aids in the precise delineation of tumors, organs, and other structures, facilitating accurate diagnosis and treatment planning [6; 5]. In robotics, segmentation helps robots navigate and interact with their environment by identifying objects and their boundaries [13; 14].  \n\nDespite its widespread applications, visual segmentation faces several key challenges. One of the most prominent is **fine-grained boundary delineation**, where accurately segmenting object boundaries, especially in complex or cluttered scenes, remains difficult. This challenge is exacerbated by the inherent ambiguity of boundaries in natural images and the limitations of convolutional neural networks (CNNs) in capturing high-frequency details [15; 16]. Techniques like edge-aware loss functions and multi-scale feature fusion have been proposed to address this issue, but it remains an active area of research [17; 18].  \n\nAnother major challenge is **handling multi-scale object variability**. Objects in real-world scenes can appear at vastly different scales, from small pedestrians in the distance to large buildings dominating the foreground. Traditional segmentation models often struggle to capture this variability, leading to poor performance on small or occluded objects [19; 20]. Hierarchical architectures and attention mechanisms have been proposed to address this issue, but achieving consistent performance across all scales remains elusive [21; 22].  \n\nAdditionally, visual segmentation tasks often require **large amounts of annotated training data**, which can be expensive and time-consuming to acquire. This is particularly problematic in domains like medical imaging, where expert annotations are scarce and costly [23; 24]. Self-supervised and weakly supervised learning approaches have emerged as promising solutions, leveraging unlabeled or weakly labeled data to reduce annotation burdens [25; 26].  \n\nIn summary, visual segmentation is a cornerstone of modern computer vision, enabling machines to interpret and interact with the visual world. Its applications span autonomous driving, medical imaging, robotics, and beyond, each presenting unique challenges that drive ongoing research and innovation. While significant progress has been made, challenges like fine-grained boundary delineation, multi-scale object variability, and data dependency continue to inspire new methodologies and advancements in the field [27; 28].  \n\n---\n\n### 1.2 Evolution from CNNs to Transformers\n\nThe evolution of visual segmentation models has undergone a fundamental transformation, shifting from convolutional neural networks (CNNs) to transformer-based architectures. This transition has been driven by the growing need for more effective global context modeling and long-range dependency capture—capabilities that are critical for modern segmentation tasks.  \n\nCNNs, exemplified by architectures like U-Net and DeepLab, have long been the backbone of visual segmentation due to their ability to extract hierarchical local features through convolutional operations. Their success stems from an inherent inductive bias toward local receptive fields, which enables efficient detection of patterns like edges and textures. However, this same locality becomes a limitation when modeling global relationships across an image—a requirement that has become increasingly important as segmentation tasks demand finer-grained understanding of complex scenes. For instance, in medical image segmentation, where anatomical structures often exhibit long-range contextual dependencies, CNNs may fail to integrate semantically related but distant regions [29]. Similarly, in autonomous driving scenarios involving LiDAR point clouds, large-scale spatial relationships are poorly captured by CNNs due to their localized operations. While techniques like atrous (dilated) convolutions in DeepLab partially addressed this by expanding receptive fields, they remained computationally expensive and still lacked true global modeling [30].  \n\nTransformers, originally developed for natural language processing, emerged as a compelling alternative by leveraging self-attention mechanisms to model global interactions. The Vision Transformer (ViT) [31] marked a pivotal breakthrough, demonstrating that treating images as sequences of patches and applying self-attention could achieve competitive performance in image classification. This innovation paved the way for transformer-based segmentation models like SEgmentation TRansformer (SETR), which replaced CNN backbones with pure transformer architectures and achieved state-of-the-art results on benchmarks such as ADE20K and Pascal Context. Unlike CNNs, SETR’s self-attention mechanism captures dependencies between all patches in an image, enabling seamless integration of global context [31].  \n\nThe advantages of transformers over CNNs are manifold. First, self-attention inherently models long-range dependencies without requiring stacked convolutional layers or manual design choices like kernel dilation—making it particularly effective for tasks like panoptic segmentation, where understanding relationships between disparate objects is crucial. Second, transformers offer greater flexibility in handling multi-scale features. Hierarchical architectures like Swin Transformer and Dilated Neighborhood Attention Transformer (DiNAT) [32] employ shifted windows or dilated attention to efficiently capture both local and global contexts, outperforming CNNs in tasks requiring multi-scale reasoning. Third, transformers eliminate the need for handcrafted inductive biases, such as translation invariance, which CNNs rely on but may not always be optimal for diverse datasets [33].  \n\nHowever, the transition from CNNs to transformers is not without challenges. Transformers are data-hungry, often requiring large-scale pre-training to perform well, whereas CNNs generalize better with limited data due to their built-in inductive biases [34]. To address this, hybrid architectures like UniFormer [35] and ViT-CoMer [36] combine convolutional layers with self-attention, leveraging the strengths of both paradigms. These hybrids achieve a balance, as demonstrated by UniFormer’s success in video recognition and ViT-CoMer’s performance in dense prediction tasks. Another challenge is computational complexity: self-attention scales quadratically with input size, making it impractical for high-resolution images. Innovations like token pruning and low-rank approximations have been proposed to mitigate this, enabling real-time applications such as SideRT [37].  \n\nThe shift from CNNs to transformers also reflects broader trends in computer vision. CNN-based models like InternImage [38] and Shift-ConvNets [39] attempted to bridge the gap by introducing deformable convolutions or large-kernel designs, yet they still lagged behind transformers in global context modeling. Conversely, transformer variants like GC ViT [40] and DilateFormer [41] explicitly enhance global receptive fields while maintaining efficiency, outperforming CNNs in tasks like semantic segmentation and object detection.  \n\nIn summary, the evolution from CNNs to transformers represents a significant leap forward in visual segmentation. While CNNs remain valuable for their efficiency and inductive biases, transformers offer unparalleled capabilities in global context modeling and flexibility. Hybrid architectures and efficiency optimizations continue to blur the boundaries between the two paradigms, enabling models like TransBTSV2 [30] and Factorizer [42] to achieve state-of-the-art performance across diverse applications. As the field progresses, the integration of CNNs and transformers is poised to yield even more robust and scalable solutions, further cementing transformers’ role in the future of visual segmentation.\n\n### 1.3 Motivation for the Survey\n\nThe rapid advancements in transformer-based architectures for visual segmentation have revolutionized computer vision, necessitating a comprehensive survey to consolidate these developments. Transformers have transcended their NLP origins to dominate image segmentation tasks by excelling in long-range dependency modeling and global context capture—capabilities critical for precise boundary delineation and multi-scale feature fusion. However, three key challenges motivate this survey: (1) the breakneck pace of architectural innovations, (2) the fragmentation of diverse methodologies, and (3) the absence of standardized evaluation frameworks.\n\n### Unprecedented Pace of Innovation\nSince the pioneering work of Vision Transformers (ViT) [43], the field has witnessed exponential growth, with models like Swin Transformers [32] establishing new paradigms for hierarchical feature learning. Recent innovations such as DenseFormer [44] and Factorizer [42] further push boundaries in efficiency and interpretability. This acceleration is particularly evident in domain-specific adaptations—nnFormer [45] for 3D medical imaging and P2AT [46] for autonomous driving exemplify how transformers are being tailored to address unique challenges. The absence of a centralized resource tracking these advancements creates a critical knowledge gap that this survey addresses.\n\n### Architectural Diversity and Fragmentation\nThe current landscape features a rich tapestry of architectures, each optimizing for specific use cases. Hierarchical designs like Swin Transformer [32] and PVT [47] enable multi-scale processing, while hybrid models blend convolutional and transformer layers to balance local-global feature learning. Attention variants—from axial [32] to deformable mechanisms [48]—demonstrate the field's ingenuity in computational efficiency. Decoder innovations span lightweight designs (SSformer [49]) to medical-specific adaptations (TransDeepLab [29]). Yet without a systematic taxonomy, comparing these approaches remains challenging. This survey establishes a unified framework to categorize architectures by their core innovations and application contexts.\n\n### Benchmarking and Evaluation Gaps\nDespite prolific model development, inconsistent evaluation practices hinder progress. While studies report metrics on COCO, Cityscapes, and ADE20K, variations in experimental setups obscure meaningful comparisons. Medical imaging benchmarks (BraTS [45], ISLES'22 [42]) and autonomous driving evaluations (SideRT [37]) often lack cross-study consistency in metrics and baselines. Critical dimensions like robustness (examined in [50]) and interpretability (explored in [42]) remain understandardized. Our survey introduces comprehensive benchmarks across accuracy, efficiency, and reliability metrics to establish reproducible evaluation protocols.\n\n### Emerging Challenges and Future Directions\nThe field's evolution reveals critical research gaps:  \n1. **Efficiency-Performance Tradeoffs**: Lightweight models like SSformer [49] and PRO-SCALE [51] show promise but require scaling analyses.  \n2. **Cross-Paradigm Integration**: Opportunities exist in combining transformers with reinforcement learning [52] and multimodal approaches [53].  \n3. **Deployment Realities**: While P2AT [46] addresses real-time needs, edge-device optimizations and hardware co-design (e.g., [54]) demand further exploration.  \n\nBy synthesizing these insights, this survey not only consolidates the state-of-the-art but also provides a roadmap for advancing transformer-based segmentation toward scalable, interpretable, and deployable solutions—bridging the gap between rapid innovation and practical application.\n\n### 1.4 Scope and Contributions\n\nBuilding upon the challenges and innovations outlined in the preceding sections, this survey provides a comprehensive and systematic examination of transformer-based architectures for visual segmentation. Our analysis spans foundational principles, methodological breakthroughs, application domains, efficiency optimizations, and open challenges, creating a cohesive bridge between the field's rapid evolution (discussed in \"Unprecedented Pace of Innovation\") and its future directions.  \n\nThe survey's scope encompasses the full spectrum of transformer-based segmentation, from core architectural designs to real-world implementations. We place particular emphasis on their transformative impact across three key domains previously identified as innovation hotspots: medical imaging (extending the nnFormer [45] case study), autonomous systems (building on P2AT [46] insights), and general-purpose segmentation tasks. Through synthesis of over 50 seminal works—including foundational surveys like [55] and domain-specific analyses such as [56]—we establish a unified taxonomy that categorizes approaches into three evolutionary branches: hierarchical architectures, hybrid CNN-transformer models, and attention variants. This taxonomy not only clarifies the field's trajectory but also reveals underexplored intersections, such as the integration of spatial and channel attention mechanisms demonstrated in [57].  \n\nA key contribution is our granular architectural analysis, which systematically evaluates self-attention mechanisms for long-range dependency capture—a capability highlighted in earlier sections as transformers' defining advantage. Works like [58] and [59] demonstrate this strength, while [60] provides crucial context on computational trade-offs. We extend this analysis with fresh insights into hybrid architectures, exemplified by [61], which achieves superior performance by combining convolutional inductive biases with transformer-based global context modeling. Emerging decoder designs are also explored, particularly the cascaded attention approach in [62], which addresses multi-scale feature refinement—a challenge identified in our architectural diversity discussion.  \n\nThe survey dedicates substantial attention to domain-specific applications, expanding upon the medical imaging and autonomous driving examples introduced earlier. In medical imaging, we analyze how transformers overcome CNN limitations in fine-grained structure capture, with [63] addressing small-data regimes and [64] tackling volumetric processing. Multimodal fusion breakthroughs like [65] are examined alongside autonomous driving solutions such as [66], creating a natural transition to the efficiency optimization discussion.  \n\nOur efficiency analysis directly addresses the deployment challenges raised in \"Emerging Challenges and Future Directions.\" We catalog cutting-edge techniques including token pruning ([67]), low-rank approximations ([68]), and hardware-aware quantization ([69]). These methods collectively solve the computational bottlenecks identified in earlier sections, with [70] demonstrating parameter-efficient designs and [42] introducing novel NMF-based layers.  \n\nThe survey's comparative benchmark—spanning 10+ datasets including COCO, Cityscapes, and BraTS—provides empirical validation of trends observed throughout our analysis. Hierarchical transformers like [71] excel in multi-organ segmentation (validating their architectural advantages), while hybrid models dominate small-object tasks as shown in [72]. These findings directly inform our identification of critical gaps, particularly the lack of standardized evaluation protocols for long-range dependency modeling—a challenge corroborated by [73].  \n\nConcluding with forward-looking insights, the survey outlines research directions grounded in the reviewed literature's empirical evidence. We advocate for self-supervised pretraining ([74]) and multimodal fusion ([75]), while highlighting lightweight attention mechanisms from [76] for mobile health applications. This final synthesis creates a seamless transition to future research opportunities, maintaining the survey's end-to-end coherence while preserving all substantive technical contributions.\n\n## 2 Foundations of Transformer-Based Segmentation\n\n### 2.1 Self-Attention Mechanisms in Vision Transformers\n\nSelf-attention mechanisms are the cornerstone of transformer architectures, empowering them to model long-range dependencies and intricate spatial relationships in visual data. Unlike convolutional neural networks (CNNs), which process information through localized receptive fields and hierarchical aggregation, self-attention computes interactions between all positions in the input, enabling comprehensive global context modeling. This capability is especially valuable for visual segmentation tasks, where capturing relationships between distant regions—such as object parts or scene elements—is essential for precise pixel-wise classification [27].  \n\n### The Mechanics of Self-Attention  \nThe standard self-attention mechanism projects input features into three learned representations: queries (Q), keys (K), and values (V). Attention weights are derived from the scaled dot-product of Q and K, followed by a softmax normalization. These weights then guide the aggregation of information from V, producing outputs enriched with global context. Mathematically, this is expressed as:  \n\n\\[77]  \n\nwhere \\( d_k \\) is the dimension of the key vectors. This formulation allows self-attention to simultaneously model local and global dependencies, a key advantage for segmentation tasks [78].  \n\n### Computational Challenges and Efficient Variants  \nDespite its strengths, the computational complexity of self-attention poses a significant bottleneck. For an input of size \\( N \\times N \\), standard self-attention requires \\( O(N^4) \\) operations, making it impractical for high-resolution images common in segmentation. To address this, researchers have developed several efficient variants:  \n\n1. **Axial Attention**: This approach decomposes 2D self-attention into separate 1D operations along the height and width axes, reducing complexity to \\( O(N^3) \\) [21]. Axial attention has proven effective in hierarchical architectures, preserving spatial details while maintaining global context [79].  \n\n2. **Deformable Attention**: By limiting attention to a small set of learned sampling points around each query, deformable attention achieves linear complexity. This dynamic adjustment of receptive fields is particularly useful for segmenting irregular or occluded objects [80] [81].  \n\n3. **Window-Based Attention**: Popularized by the Swin Transformer, this method partitions the input into non-overlapping windows and computes self-attention locally. Cross-window communication is enabled through shifted windows, balancing efficiency and performance [2] [11].  \n\n### Limitations and Hybrid Solutions  \nWhile self-attention excels at global context modeling, it can struggle with fine-grained details and boundary regions, sometimes blurring intricate structures. To mitigate this, hybrid architectures combine self-attention with convolutional layers, leveraging CNNs for low-level feature extraction and transformers for high-level context [16].  \n\nAnother challenge lies in robustness to distribution shifts and adversarial attacks. Although transformers generalize well, their performance can degrade under domain variations or adversarial perturbations [12]. Recent efforts explore adversarial training and attention map regularization to enhance robustness.  \n\n### Future Directions  \nFuture research may focus on dynamic attention mechanisms that adaptively balance local and global processing, as well as techniques to further reduce computational overhead for real-time applications [28]. Innovations in this space will be critical for advancing transformer-based segmentation models, ensuring they remain both powerful and practical.  \n\nBy addressing computational inefficiencies and integrating complementary inductive biases, self-attention mechanisms continue to drive progress in visual segmentation, paving the way for more accurate and scalable solutions.\n\n### 2.2 Positional Embeddings and Spatial Awareness\n\n### 2.2 Positional Embeddings and Spatial Awareness  \n\nPositional embeddings serve as a fundamental mechanism to inject spatial awareness into Vision Transformers (ViTs), compensating for their inherent permutation invariance. While convolutional neural networks (CNNs) implicitly capture spatial hierarchies through local receptive fields and weight sharing, ViTs process flattened image patches as unordered sequences, necessitating explicit positional encoding to recover spatial relationships—a critical requirement for segmentation tasks where precise localization is paramount. This subsection examines the design principles of positional embeddings, their limitations in handling spatial information, and recent innovations that enhance their effectiveness for visual segmentation.  \n\n#### **The Role and Design of Positional Embeddings**  \nIn ViTs, positional embeddings are typically added to patch embeddings to preserve spatial order. These embeddings can be either learned (trainable parameters) or fixed (e.g., sinusoidal functions). The seminal work [31] established that omitting positional embeddings severely degrades ViT performance in dense prediction tasks, as the model loses the ability to discern spatial relationships between patches. This is particularly evident in segmentation applications like medical imaging, where precise anatomical localization is crucial [29].  \n\nThe choice of positional embedding strategy significantly impacts model performance:  \n- **Absolute embeddings** assign a unique vector to each spatial position, but they often fail to generalize to unseen resolutions, limiting their applicability in multi-scale segmentation scenarios.  \n- **Relative embeddings**, which encode pairwise patch distances rather than absolute coordinates, offer greater flexibility. This approach, adopted in hierarchical architectures like Swin Transformer [82], enables consistent spatial reasoning across varying patch resolutions.  \n\n#### **Challenges and Limitations**  \nDespite their utility, traditional positional embeddings face three key challenges:  \n\n1. **Resolution Dependency**: Fixed absolute embeddings struggle with input resolutions beyond those encountered during training, hindering their use in tasks requiring multi-scale processing [32].  \n2. **Shift Variance**: Unlike CNNs, which inherently exhibit shift equivariance, ViTs with rigid positional encodings produce inconsistent outputs when inputs are translated—a critical weakness for segmentation robustness [83].  \n3. **Local Context Neglect**: While positional embeddings encode global spatial relationships, they often overlook fine-grained local structures (e.g., edges, textures), which are essential for accurate boundary delineation in segmentation [84].  \n\n#### **Advancements in Spatial Modeling**  \nRecent innovations address these limitations through novel positional encoding and attention mechanisms:  \n\n1. **Gaussian Attention Bias**: By introducing a distance-decaying attention pattern akin to CNN receptive fields, Gaussian bias [85] enhances local feature extraction while preserving global context—proving particularly effective for medical image segmentation.  \n2. **Shift-Equivariant Designs**: Data-dependent spatial encodings [83] dynamically adjust to input shifts, restoring shift equivariance and bridging the gap between ViTs and CNNs.  \n3. **Dynamic Positional Embeddings**: Learnable interpolation mechanisms [86] and convolutional feature integration [87] enable resolution-adaptive spatial encoding, eliminating generalization bottlenecks.  \n4. **Hybrid Architectures**: Models like [36] and [35] combine convolutional local feature extraction with transformer-based global modeling, achieving state-of-the-art segmentation performance through complementary spatial representation.  \n\n#### **Future Directions**  \nEmerging research avenues aim to further refine spatial awareness in ViTs:  \n1. **Content-Adaptive Encodings**: Techniques like those in [42] could enable dynamic positional embeddings that adjust to input semantics.  \n2. **Cross-Modal Transfer**: Investigating shared positional representations across modalities (e.g., RGB-D) may enhance multi-modal segmentation [88].  \n3. **Efficient Implementations**: Optimizing computational overhead for high-resolution inputs remains critical for real-time applications [37].  \n\nIn summary, positional embeddings are indispensable for ViTs to achieve spatial competence in segmentation tasks, but their design must evolve to overcome resolution sensitivity, shift variance, and local detail neglect. The innovations discussed here—spanning attention mechanisms, equivariant designs, and hybrid architectures—lay the groundwork for more robust and efficient transformer-based segmentation models, seamlessly connecting to the hybrid architectures explored in Section 2.3.\n\n### 2.3 Hybrid Architectures: Combining CNNs and Transformers\n\n### 2.3 Hybrid Architectures: Combining CNNs and Transformers  \n\nHybrid architectures that integrate convolutional neural networks (CNNs) and transformers have emerged as a powerful paradigm for visual segmentation, addressing the complementary strengths and weaknesses of both architectures. While Section 2.2 highlighted the role of positional embeddings in enabling spatial awareness for pure transformers, hybrid models take a different approach by leveraging CNNs' innate ability to capture local spatial hierarchies and inductive biases like translation equivariance, alongside transformers' capacity for modeling long-range dependencies through self-attention. This fusion enables a balanced representation of both local and global features, leading to improved segmentation accuracy across diverse tasks.  \n\n#### **Motivation for Hybrid Architectures**  \nThe limitations of pure transformer-based models, as discussed in Section 2.2, motivate the need for hybrid designs. While Vision Transformers (ViTs) [43] excel at global context modeling, they often struggle with local feature extraction due to their lack of inherent spatial inductive biases and require extensive pretraining. Conversely, CNNs like U-Net [89] capture fine-grained features but are constrained by limited receptive fields. Hybrid models bridge this gap by combining CNNs for local feature extraction with transformers for global context aggregation, as exemplified by architectures like ViC (Vision Convolutional Transformer) and T-CNN (Transformer-CNN).  \n\n#### **Design Principles of Hybrid Models**  \nHybrid architectures typically follow one of three design principles, each addressing specific segmentation challenges:  \n\n1. **CNN Backbone with Transformer Encoder**: Many models employ a CNN backbone (e.g., ResNet) to extract hierarchical features, which are then processed by a transformer encoder to capture global dependencies. For example, TransDeepLab [29] replaces the Atrous Spatial Pyramid Pooling (ASPP) module in DeepLabv3+ with a Swin-Transformer block, combining the spatial precision of CNNs with the global modeling of transformers for medical image segmentation.  \n\n2. **Interleaved CNN-Transformer Blocks**: Some architectures interleave convolutional and self-attention layers within the same network. The UniFormer [35] integrates local convolution and global self-attention in a unified block, enabling efficient feature fusion across scales—particularly effective for tasks requiring both fine-grained localization (e.g., boundary delineation) and high-level semantic understanding.  \n\n3. **Dual-Branch Architectures**: Dual-branch models process inputs separately through CNN and transformer branches, later fusing their features. For instance, the nnFormer [45] employs a dual-path design where convolutional layers capture local details and transformer layers model long-range interactions, achieving state-of-the-art results in 3D medical segmentation.  \n\n#### **Advantages of Hybrid Architectures**  \nHybrid models offer several key advantages over pure transformer or CNN-based approaches:  \n\n- **Efficiency**: By limiting self-attention to higher-resolution feature maps or specific stages, hybrid models reduce computational overhead. For example, P2AT [46] uses axial attention in later stages to maintain real-time performance while preserving global context.  \n- **Robustness**: The inductive biases of CNNs make hybrid models more robust to small datasets, as demonstrated by UNesT [63], which outperforms pure transformers in low-data regimes for multi-organ segmentation.  \n- **Scalability**: Hybrid architectures can scale to high-resolution inputs by leveraging CNN downsampling before applying transformers. The FINE transformer [90] uses this strategy to process 3D medical volumes efficiently.  \n\n#### **Challenges and Innovations**  \nDespite their strengths, hybrid models face challenges in harmonizing CNN and transformer operations, such as differing normalization schemes (e.g., BatchNorm vs. LayerNorm) and feature resolutions. Recent innovations address these issues:  \n\n- **Cross-Modality Feature Fusion**: Models like CASTformer [91] introduce adversarial training to align CNN and transformer features, improving segmentation consistency.  \n- **Dynamic Token Reduction**: Progressive Token Length Scaling (PRO-SCALE) [51] dynamically prunes redundant tokens in hybrid encoders, reducing computation without sacrificing accuracy.  \n- **Lightweight Hybrid Designs**: SSformer [49] combines Swin-Transformer blocks with a CNN decoder, achieving competitive performance with fewer parameters.  \n\n#### **Applications and Performance**  \nHybrid architectures have demonstrated remarkable success across domains, setting the stage for the foundational models discussed in Section 2.4:  \n\n- **Medical Imaging**: IncepFormer [92] integrates pyramid pooling and transformer blocks to achieve state-of-the-art results on ADE20K and BraTS datasets.  \n- **Autonomous Driving**: LiDARFormer [93] combines point-wise convolutions with self-attention for LiDAR segmentation, outperforming pure CNNs in long-range scene understanding.  \n- **General-Purpose Segmentation**: The Factorizer [42] uses non-negative matrix factorization (NMF) to bridge CNN and transformer features, achieving interpretable and efficient segmentation.  \n\n#### **Future Directions**  \nFuture research may explore:  \n1. **Adaptive Hybridization**: Dynamically adjusting the CNN-transformer ratio based on input complexity, as suggested in [94].  \n2. **Unified Pretraining**: Developing pretraining frameworks that jointly optimize CNN and transformer components, inspired by [53].  \n3. **Hardware-Aware Design**: Co-designing hybrid models with accelerators, as proposed in [54], to optimize throughput.  \n\nIn summary, hybrid architectures represent a versatile and scalable solution for visual segmentation, harmonizing the strengths of CNNs and transformers while mitigating their individual limitations. Their continued evolution, alongside foundational models like ViT and Swin Transformer (discussed in Section 2.4), promises to unlock new frontiers in accuracy, efficiency, and interpretability.\n\n### 2.4 Foundational Models and Adaptations for Segmentation\n\n### 2.4 Foundational Models and Adaptations for Segmentation  \n\nBuilding upon the hybrid architectures discussed in Section 2.3, foundational transformer models like Vision Transformer (ViT) and Swin Transformer have revolutionized visual segmentation by introducing pure transformer-based approaches. These models, initially designed for image classification, have been adapted for segmentation through hierarchical and multi-scale designs, addressing the unique challenges of dense prediction tasks. This subsection reviews these foundational architectures and their adaptations, emphasizing their innovations in medical and natural image segmentation.  \n\n#### **Vision Transformer (ViT) and Its Segmentation Adaptations**  \nThe Vision Transformer (ViT) pioneered the use of pure transformer architectures for vision tasks by processing images as sequences of patch tokens [95]. While ViT excels at global context modeling, its original design lacks the spatial inductive biases required for precise segmentation. To bridge this gap, adaptations like SETR (Segmentation Transformer) repurposed ViT as an encoder, decoding its patch tokens into segmentation masks via a CNN-based decoder [96]. SETR demonstrated the importance of global context in tasks like medical imaging, where fine-grained boundary delineation is critical [97].  \n\nHybrid adaptations further enhanced ViT’s suitability for segmentation. TransUNet combined ViT’s encoder with a U-Net-style decoder, reshaping transformer tokens into feature maps for hierarchical upsampling [58]. This design proved effective in medical segmentation, such as multi-organ and cardiac MRI analysis, by balancing global context with local precision [58]. HRViT (High-Resolution ViT) extended this idea with multi-branch self-attention to preserve high-resolution features, enabling precise segmentation of small anatomical structures [96].  \n\n#### **Swin Transformer and Hierarchical Segmentation Designs**  \nThe Swin Transformer advanced ViT by introducing hierarchical window-based self-attention, enabling efficient multi-scale feature extraction [60]. Its shifted window strategy reduced computational complexity while maintaining cross-window connectivity, making it ideal for segmentation tasks. Swin-Unet, a medical adaptation, replaced U-Net’s encoder with a Swin Transformer backbone, achieving state-of-the-art performance on datasets like Synapse and ACDC [98]. The model’s ability to capture long-range dependencies and localize fine structures was particularly beneficial for complex anatomies like brain tumors [99].  \n\nVariants like CS-Unet integrated convolutional blocks into Swin’s architecture to enhance local feature extraction, addressing limitations in low-data medical regimes [100]. BATFormer introduced boundary-aware attention to refine segmentation masks, excelling in tasks with irregular shapes, such as skin lesions and cardiac structures [67].  \n\n#### **Multi-Scale Feature Fusion and Domain-Specific Innovations**  \nEffective multi-scale feature fusion is critical for handling objects of varying sizes. Hierarchical designs like PVT (Pyramid Vision Transformer) and MERIT (Multi-scale Hierarchical Vision Transformer) addressed this by generating feature pyramids at different resolutions [62]. PVT used progressive patch embedding to downsample feature maps while preserving global attention, making it suitable for large organ segmentation in CT scans [101]. MERIT cascaded attention blocks across scales, achieving top results in multi-organ segmentation [62].  \n\nDomain-specific adaptations like UNesT and 3D TransUNet extended transformers to 3D volumetric data. UNesT employed a nested hierarchical transformer to segment 133 brain structures in a single network [63]. 3D TransUNet adapted ViT for 3D segmentation by inflating 2D pre-trained weights, excelling in brain tumor and cardiac datasets [59].  \n\n#### **Robustness and Efficiency Considerations**  \nWhile foundational models achieve high accuracy, their computational demands challenge real-world deployment. Lightweight adaptations like EfficientMorph and CCT (Compact Convolutional Transformers) optimized attention mechanisms for efficiency [64; 70]. EfficientMorph used plane-based attention to reduce parameters by 16–27x without performance loss [64].  \n\nIn summary, foundational models like ViT and Swin Transformer, along with their adaptations, have redefined segmentation by combining global context with hierarchical and multi-scale designs. Their success in medical and natural image segmentation underscores their transformative potential, while innovations in efficiency and robustness, as discussed in Section 2.5, pave the way for broader applicability [102; 99].\n\n### 2.5 Robustness and Interpretability\n\n### 2.5 Robustness and Interpretability  \n\nThe deployment of transformer-based models in real-world segmentation tasks demands not only high performance but also robustness to distribution shifts and adversarial attacks, along with interpretable decision-making processes. Building upon the foundational architectures discussed in Section 2.4, this subsection examines how Vision Transformers (ViTs) address these critical requirements, with a focus on medical imaging and autonomous driving applications where reliability and transparency are paramount.  \n\n#### Robustness to Distribution Shifts  \nWhile ViTs excel at capturing global dependencies through self-attention, their performance can degrade under distribution shifts—scenarios where test data deviates from the training distribution. Recent work has explored architectural innovations to mitigate this limitation. For instance, [103] demonstrates that axial attention, which decomposes 2D self-attention into sequential 1D operations, enhances robustness by preserving spatial coherence while reducing computational overhead. This design proves particularly effective for handling minor input perturbations.  \n\nHowever, larger domain gaps, such as between synthetic and real-world data, remain challenging. Approaches like [104] introduce deformable attention mechanisms that dynamically adjust focus to semantically relevant regions, improving adaptability to distribution shifts. Similarly, [105] proposes HaloNets, which combine local self-attention with convolutional inductive biases to enhance cross-domain generalization. These methods highlight the importance of balancing global context with local adaptability in robust segmentation models.  \n\n#### Adversarial Robustness  \nAdversarial attacks pose another significant challenge for ViTs in segmentation tasks. Unlike CNNs, which are vulnerable to gradient-based attacks, ViTs exhibit unique adversarial robustness characteristics due to their global attention mechanisms. [106] reveals that pure self-attention models disperse adversarial effects across the entire feature map, making them inherently more resistant to localized perturbations than hybrid CNN-Transformer architectures.  \n\nTo further strengthen adversarial defenses, recent work has introduced specialized attention mechanisms. [107] employs Hadamard attention and ghost heads to reduce susceptibility to adversarial noise by promoting higher-order feature interactions. Meanwhile, [108] leverages deformable convolutions to dynamically reconfigure attention regions, making it harder for adversaries to exploit fixed receptive fields. These innovations underscore the potential of adaptive attention mechanisms in adversarial defense.  \n\n#### Interpretability Methods  \nUnderstanding ViTs' decision-making processes is crucial for trustworthy deployment in critical applications. Current interpretability approaches focus on two main directions: attention map analysis and feature visualization.  \n\n1. **Attention Map Analysis**  \nAttention maps provide insights into which input regions influence model predictions. [109] introduces FLANet, which generates unified attention maps combining spatial and channel dimensions, offering a comprehensive view of feature dependencies. This helps verify whether the model focuses on semantically relevant regions.  \n\nFurther refinements include [110], which proposes Gaussian-based attention maps with learnable radii, decoupling attention from feature complexity for clearer interpretation. [111] extends this by integrating channel relations into spatial attention, enabling fine-grained analysis of feature interactions.  \n\n2. **Feature Visualization**  \nTechniques like activation maximization reveal what ViTs learn in their latent spaces. [112] uses geometry-aware attention to visualize how spatial relationships affect segmentation, particularly useful for medical imaging where precise anatomical localization is critical.  \n\nFor 3D data, [113] combines spatial and channel attention visualizations to highlight discriminative features in volumetric images. Similarly, [114] visualizes how self-attention aggregates geometric context in point clouds, validating the model's understanding of object shapes.  \n\n#### Challenges and Future Directions  \nDespite progress, key challenges remain. ViTs lack theoretical guarantees against adversarial attacks, and their performance under extreme distribution shifts (e.g., cross-modal data) requires further study. [115] suggests polynomial approximations of attention could improve robustness, though this remains untested in segmentation.  \n\nFor interpretability, current methods often rely on post-hoc analyses that may not reflect true model reasoning. [116] highlights how self-attention's implicit sparse feature creation complicates interpretation, calling for inherently interpretable designs. Future work could explore hybrid architectures like [117], combining convolutional locality with attention-based global reasoning to balance interpretability and performance.  \n\nIn conclusion, while ViTs have made significant strides in robustness and interpretability, ongoing research into dynamic attention mechanisms, adversarial training, and unified visualization tools will be essential to fully realize their potential in real-world segmentation applications.\n\n## 3 Architectures and Methodologies\n\n### 3.1 Hierarchical Transformer Architectures\n\nHierarchical transformer architectures have emerged as a powerful paradigm for visual segmentation tasks, addressing the limitations of vanilla vision transformers (ViTs) in handling multi-scale features and computational inefficiency. Unlike standard ViTs, which process images at a single resolution and suffer from quadratic complexity relative to input size, hierarchical designs introduce inductive biases akin to convolutional neural networks (CNNs) while retaining the global modeling capabilities of transformers. These architectures typically employ pyramid structures or shifted window mechanisms to enable efficient multi-scale feature extraction, making them particularly suitable for dense prediction tasks like semantic, instance, and panoptic segmentation [27].  \n\n### 3.1.1 Pioneering Hierarchical Architectures  \nOne of the pioneering works in this direction is the Swin Transformer, which introduces a hierarchical feature map through shifted windows. By partitioning the input into non-overlapping local windows and computing self-attention within each window, Swin Transformer reduces the computational complexity from quadratic to linear relative to image size. The shifted window mechanism further allows cross-window connections, enabling long-range dependencies without sacrificing efficiency. This design has been widely adopted in segmentation tasks, as it balances local detail preservation and global context aggregation. For instance, Swin-Unet extends the Swin Transformer to medical image segmentation, demonstrating superior performance over CNN-based U-Net architectures by leveraging hierarchical feature maps and shifted window attention [6]. Similarly, the HRViT model incorporates Swin's hierarchical design into a high-resolution network, achieving state-of-the-art results on urban scene datasets like Cityscapes by effectively capturing fine-grained details and large-scale objects [2].  \n\nAnother influential hierarchical architecture is the Pyramid Vision Transformer (PVT), which constructs a feature pyramid through progressive spatial reduction. PVT employs spatial-reduction attention (SRA) to downsample key and value matrices, significantly reducing memory usage while maintaining a large receptive field. This pyramid structure is particularly advantageous for segmentation, as it naturally aligns with the multi-scale nature of objects in real-world scenes. For example, PVT has been integrated into panoptic segmentation frameworks like Panoptic-DepthLab, where its multi-scale features are fused with depth estimation branches to improve 3D scene understanding [118]. The hierarchical design of PVT also facilitates efficient token pruning, as demonstrated in works like PanoNet, where redundant tokens are dynamically merged or discarded to accelerate inference without compromising accuracy [119].  \n\n### 3.1.2 Hybrid and Efficient Hierarchical Designs  \nBeyond Swin and PVT, hybrid architectures combining hierarchical transformers with CNN backbones have shown promise in segmentation tasks. These models often use transformers for high-level feature extraction and CNNs for low-level spatial detail preservation. For instance, the T-CNN framework integrates transformer blocks into a CNN pyramid, enabling robust feature fusion across scales. This approach is particularly effective in autonomous driving scenarios, where real-time performance and accuracy are critical [14]. Similarly, the Multi Receptive Field Network (MRFM) leverages hierarchical transformers to capture context at multiple scales while employing edge-aware losses to refine object boundaries, achieving state-of-the-art results on ADE20K and Pascal-Context datasets [16].  \n\nThe efficiency of hierarchical transformers is further enhanced through innovations like axial attention and deformable attention. Axial attention decomposes 2D self-attention into sequential 1D operations along height and width dimensions, reducing computational overhead while preserving global interactions. This technique has been adopted in models like LiDARFormer for 3D point cloud segmentation, where it efficiently processes large-scale LiDAR scans [81]. Deformable attention, on the other hand, dynamically samples a sparse set of key points based on input content, as seen in DAT++, which achieves a favorable trade-off between accuracy and speed for real-time segmentation [28].  \n\n### 3.1.3 Applications and Challenges  \nHierarchical designs also address the challenge of long-range dependencies in ultra-high-resolution images. For example, the Full-Resolution Residual Network (FRRN) combines a high-resolution stream with a context-rich low-resolution stream, using residuals to fuse features at full resolution. This architecture has been adapted in transformer-based models like BiERF-PSPNet, which employs a bidirectional hierarchical structure to enhance detail sensitivity in wearable assistive systems [4]. Similarly, the DS-PASS framework utilizes a panoramic annular network with hierarchical attention to process wide-field views, achieving robust segmentation for autonomous navigation [80].  \n\nDespite their advantages, hierarchical transformers face challenges in balancing computational efficiency with model capacity. Recent works like EfficientPS and Mask2Former address this by unifying hierarchical feature extraction with task-specific decoders, achieving panoptic segmentation with minimal computational overhead [11; 78]. The OneFormer3D model further extends this paradigm to 3D point clouds, demonstrating that hierarchical transformers can generalize across modalities [79].  \n\n### 3.1.4 Future Directions  \nFuture research directions include exploring dynamic hierarchical structures that adapt to input content, as well as integrating self-supervised pretraining to reduce reliance on annotated data. The success of models like ECLIPSE, which uses visual prompt tuning for continual learning, suggests that hierarchical transformers can evolve to handle incremental segmentation tasks efficiently [120]. Additionally, the emergence of diffusion-based hierarchical models, such as those proposed for aerial view segmentation, highlights the potential for generative approaches to enhance multi-scale feature learning [19].\n\n### 3.2 Multi-Scale Feature Fusion\n\n### 3.2 Multi-Scale Feature Fusion  \n\nBuilding upon the hierarchical transformer architectures discussed in Section 3.1, multi-scale feature fusion emerges as a critical enabler for transformer-based visual segmentation. While hierarchical designs inherently address multi-scale processing through pyramid structures or shifted windows, explicit fusion mechanisms are necessary to integrate cross-level features effectively. This subsection examines how innovative methodologies bridge the gap between local detail preservation and global context modeling, complementing the attention mechanism variants explored in Section 3.3.  \n\n#### The Role of Multi-Scale Fusion in Transformers  \nVision transformers (ViTs) lack the innate inductive biases of CNNs for hierarchical feature aggregation, making explicit multi-scale fusion indispensable for segmentation tasks. The patch-based processing of ViTs often struggles with fine-grained localization, necessitating hybrid architectures or specialized fusion modules. For instance, [121] demonstrates that transformers benefit significantly from multi-scale integration, particularly for ambiguous or low-resolution inputs. Similarly, [29] underscores the importance of multi-scale fusion in medical imaging, where precise boundary delineation is critical.  \n\n#### Hierarchical Local-Global (HLG) Fusion Strategies  \nHierarchical Local-Global (HLG) fusion, introduced in [31], exemplifies how transformers can mimic CNN-like progressive receptive field expansion while retaining global modeling capabilities. By alternating between local window attention and cross-window global attention, HLG achieves competitive performance on ADE20K and Pascal Context, outperforming vanilla ViTs. This approach aligns with the shifted-window paradigm of Swin Transformer (Section 3.1.1) while extending it to feature fusion.  \n\nThe [32] further refines this concept by introducing dilated attention windows, enabling sparse global context capture without computational overhead. This method proves particularly effective for dense prediction tasks, achieving state-of-the-art results on COCO and ADE20K by dynamically balancing local and global feature interactions.  \n\n#### Cross-Level Feature Integration Techniques  \nCross-level fusion enriches semantic understanding by combining features from different hierarchical depths. [36] proposes a bidirectional fusion module that injects CNN-derived multi-receptive field features into transformer embeddings. This hybrid design outperforms pure transformers on COCO and ADE20K, highlighting the synergy between convolutional spatial priors and self-attention.  \n\nSimilarly, [35] introduces a relation aggregator that dynamically weights features from shallow (local) and deep (global) layers. This adaptive fusion mechanism demonstrates robust performance across segmentation, detection, and classification tasks, reinforcing the versatility of cross-level integration.  \n\n#### Multi-Receptive Field and Efficiency-Oriented Designs  \nTo address object size variability, [41] employs multi-scale dilated attention, varying dilation rates to capture diverse receptive fields. This approach mirrors the multi-branch designs in CNNs while maintaining transformer efficiency, as validated on ImageNet and ADE20K.  \n\nFor computational efficiency, [122] replaces self-attention with position-aware circular convolution, leveraging FFT for scalable multi-scale fusion. This innovation underscores the potential of rethinking attention mechanisms to reduce overhead while preserving accuracy.  \n\n#### Domain-Specific Applications  \nIn medical imaging, where structural scales vary dramatically, [30] combines 3D CNN and transformer features across resolutions, outperforming single-modality approaches on BraTS and LiTS. For real-time scenarios, [37] integrates Cross-Scale Attention (CSA) with Multi-Scale Refinement (MSR) modules, achieving 51.3 FPS without sacrificing accuracy—a critical advancement for applications like autonomous navigation (Section 3.1.3).  \n\n#### Challenges and Future Directions  \nDespite progress, fundamental differences between transformer and CNN feature representations, as revealed in [123], complicate fusion strategies. Additionally, the data-hungry nature of transformers, highlighted in [124], limits fusion efficacy in low-data regimes.  \n\nFuture directions include adaptive fusion mechanisms like those in [125], where feature weighting adjusts to input complexity, and self-supervised pretraining approaches proposed in [126]. These innovations could further unify the strengths of hierarchical architectures (Section 3.1) and advanced attention mechanisms (Section 3.3), pushing the boundaries of segmentation performance.  \n\nIn summary, multi-scale feature fusion serves as the connective tissue between hierarchical transformer designs and task-specific segmentation refinements. By harmonizing local and global contexts through innovative fusion strategies, modern approaches continue to advance the accuracy and efficiency of visual segmentation systems.\n\n### 3.3 Attention Mechanism Variants\n\n### 3.3 Attention Mechanism Variants  \n\nThe success of transformer-based segmentation models hinges on their ability to capture long-range dependencies while maintaining computational efficiency. Building upon the multi-scale feature fusion strategies discussed in Section 3.2, this subsection explores how innovative attention mechanisms address the computational challenges of standard self-attention while preserving global context modeling capabilities. These advancements lay the foundation for the decoder architectures discussed in the following section, which leverage these attention variants to refine segmentation outputs.  \n\n#### Axial Attention  \nAxial attention addresses the quadratic complexity of standard self-attention by decomposing 2D or 3D operations into sequential 1D attention along height, width, and depth axes. This reduces computational complexity from \\(O(N^2)\\) to \\(O(N\\sqrt{N})\\) for an \\(N\\)-pixel image while preserving global receptive fields. For instance, [47] employs axial attention to efficiently process 3D point clouds by attending to spatial dimensions separately. Similarly, [45] integrates axial attention in its hierarchical encoder for volumetric medical images, demonstrating superior performance on brain tumor segmentation tasks.  \n\n#### Gated Attention  \nGated attention mechanisms dynamically modulate the importance of attention heads or tokens, enhancing model flexibility. This is particularly valuable for segmentation tasks where multi-scale features must be selectively aggregated. [44] introduces a gating mechanism to weight representations from different layers, improving feature reuse. Meanwhile, [91] combines gated attention with adversarial training to prioritize discriminative regions like tumor boundaries while suppressing background noise.  \n\n#### Deformable Attention  \nDeformable attention reduces computational costs by sparsely sampling key positions around reference points, enabling adaptive focus on salient regions. [32] proposes deformable attention with learnable offsets, which is particularly effective for high-resolution inputs. In medical imaging, [29] leverages deformable attention to capture irregular anatomical structures, outperforming convolutional baselines.  \n\n#### Dilated Attention  \nInspired by dilated convolutions, dilated attention expands the receptive field exponentially without increasing token count. [32] introduces dilated window attention, where local windows are progressively expanded to capture multi-scale context. This approach complements the multi-scale fusion techniques discussed earlier, as seen in [92], which combines dilated attention with pyramid pooling for enhanced feature fusion.  \n\n#### Hybrid and Sparse Attention  \nHybrid attention architectures combine local and global attention to exploit their complementary strengths. [35] unifies convolution and self-attention in a single block, using local attention in shallow layers and global attention in deeper layers. For sparse data like point clouds, [127] leverages sparsity to reduce computational overhead while maintaining performance.  \n\n#### Efficiency-Oriented Variants  \nSeveral works focus on reducing memory and computational costs without sacrificing accuracy. [51] progressively scales token lengths across encoder layers, reducing GFLOPs by 52%. Similarly, [49] simplifies the Swin Transformer architecture by aggregating hierarchical features in a compact decoder, demonstrating that efficient attention designs can achieve competitive performance.  \n\n#### Future Directions  \nWhile these variants have significantly advanced transformer-based segmentation, challenges remain in balancing efficiency and accuracy. [94] suggests that explicit feature hierarchies can sometimes outperform self-attention modules, indicating room for further optimization. Future work could explore dynamic sparse attention or hardware-aware designs to better address the efficiency-accuracy trade-off, particularly for real-time applications discussed in later sections.  \n\nIn summary, attention mechanism variants have expanded the capabilities of transformer-based segmentation by enabling efficient global context modeling. From axial and deformable attention to hybrid and sparse designs, these innovations provide tailored solutions for diverse segmentation tasks. As the field progresses, these attention variants will continue to play a critical role in bridging the gap between computational constraints and performance requirements.\n\n### 3.4 Decoder Architectures for Segmentation\n\n### 3.4 Decoder Architectures for Transformer-Based Segmentation  \n\nBuilding upon the attention mechanism variants discussed in Section 3.3, decoder architectures play a pivotal role in transformer-based segmentation by refining multi-scale features and generating precise segmentation masks. Unlike traditional CNN-based decoders, transformer-based decoders leverage global context modeling and long-range dependencies to improve accuracy, particularly for complex structures and fine-grained boundaries. This subsection surveys key decoder designs, highlighting their innovations and applications while bridging the gap between attention mechanisms and medical imaging adaptations covered in Section 3.5.  \n\n#### Hierarchical Feature Fusion Decoders  \nThe UperFormer decoder exemplifies hierarchical feature fusion, integrating transformer blocks with a pyramid structure to process multi-scale encoder features. High-resolution features capture fine details while low-resolution features encode global context, refined through cross-scale attention mechanisms. This design is particularly effective for medical images with scale variations, such as tumors or organs [101]. UperFormer has demonstrated superior performance on BraTS and Synapse datasets, outperforming conventional U-Net decoders in boundary delineation.  \n\n#### Iterative Refinement Decoders  \nCASCADE (Cross-Attention Segmentation Decoder) introduces a two-stage refinement process using transformer-based query embeddings. First, it generates coarse region proposals, then iteratively refines them through cross-attention with multi-scale encoder features [59]. This approach excels in segmenting small targets like lesions, achieving state-of-the-art results on cardiac (ACDC) and brain tumor (BraTS) datasets.  \n\n#### Hybrid CNN-Transformer Decoders  \nHybrid architectures combine convolutional and transformer blocks to leverage their complementary strengths. DA-TransUNet [57] incorporates dual attention (DA) blocks to capture spatial and channel-wise dependencies, enhancing sensitivity to fine-grained structures like blood vessels. Similarly, TransUNet's decoder [58] upsamples encoded features while combining them with high-resolution CNN features, excelling in multi-organ segmentation.  \n\n#### Efficiency-Optimized Decoders  \nLightweight designs address computational constraints without sacrificing accuracy. BATFormer [67] introduces a boundary-aware local transformer (BLT) module that adaptively partitions windows based on entropy, preserving shape integrity with reduced computations. The Fully Convolutional Transformer (FCT) decoder [128] replaces fully connected layers with convolutional operations, maintaining spatial resolution while reducing overhead.  \n\n#### 3D-Specific Decoders  \nFor volumetric medical data, decoders extend 2D designs with volumetric attention. 3D TransUNet [59] incorporates 3D self-attention for tasks like brain tumor segmentation, while UNesT [63] uses hierarchical aggregation for large-scale 3D segmentation (e.g., whole-brain with 133 classes).  \n\n#### Task-Specific Adaptations  \nDecoder choice depends on task requirements:  \n- Multi-scale fusion (e.g., [62]) suits datasets with diverse object sizes.  \n- Boundary-aware designs (e.g., BATFormer) prioritize edge detection.  \nFuture directions may explore dynamic decoders that adapt operations based on input characteristics.  \n\nIn summary, transformer-based decoders have revolutionized segmentation by combining global context with local refinement. From UperFormer's hierarchical fusion to CASCADE's iterative attention, these designs address traditional limitations and set new benchmarks. Their evolution will continue to drive advancements, particularly in medical imaging where precision and efficiency are critical, as discussed in the following section on medical-specific adaptations.\n\n### 3.5 Medical Imaging-Specific Adaptations\n\n---\n### 3.5 Medical Imaging-Specific Adaptations  \n\nWhile transformer-based models have achieved remarkable success in natural image processing, their direct application to medical imaging faces unique challenges. These include limited annotated datasets, the high-dimensional nature of volumetric data (e.g., 3D MRI or CT scans), and the critical need for precise boundary delineation in clinical segmentation tasks. Building upon the decoder architectures discussed in Section 3.4, researchers have developed specialized adaptations that tailor self-attention mechanisms and hierarchical designs to medical imaging requirements. This subsection examines these innovations and their implications for medical segmentation.  \n\n#### Domain-Specific Challenges  \nMedical image segmentation presents three fundamental challenges that distinguish it from natural image processing:  \n1. **Data Scarcity**: Annotated medical datasets are often small due to the expertise required for labeling and patient privacy constraints, limiting the applicability of large-scale pretraining common in natural image tasks.  \n2. **Volumetric Processing**: Medical scans (e.g., MRI, CT) are inherently 3D, requiring models to capture spatial dependencies across slices. Standard 2D transformers struggle with this dimensionality, as the quadratic complexity of self-attention becomes prohibitive for high-resolution volumes [129].  \n3. **Precision Demands**: Clinical applications require pixel-level accuracy for imbalanced classes (e.g., small lesions against large healthy regions) and fine anatomical structures.  \n\n#### Architectural Innovations for Medical Imaging  \nTo address these challenges, recent works have introduced key adaptations in transformer design:  \n\n**1. Hybrid CNN-Transformer Architectures**  \nModels like MedT [129] build on the hybrid decoder concepts discussed in Section 3.4 by combining convolutional layers with self-attention. These architectures leverage CNN inductive biases for local feature extraction while using transformers to model long-range dependencies. For instance, MedT employs a CNN backbone to generate multi-scale features processed by transformer blocks, reducing computational overhead while maintaining accuracy. This approach achieved state-of-the-art performance on brain tumor segmentation in [129].  \n\n**2. Efficient 3D Attention Mechanisms**  \nExtending the lightweight decoder principles from Section 3.4, recent work has developed efficient volumetric attention. [129] introduces Efficient Paired Attention (EPA), which decomposes 3D self-attention into separable spatial and channel operations, reducing complexity from \\(O(n^3)\\) to \\(O(n)\\). Similarly, [130] proposes frequency-domain attention via the Hartley transform, enabling parameter-efficient 3D processing.  \n\n**3. Hierarchical Feature Fusion**  \nInspired by multi-scale decoder designs like UperFormer (Section 3.4), medical-specific models employ hierarchical transformers for varying anatomical scales. UNesT [129] uses progressive downsampling with cross-attention skip connections, mirroring U-Net architectures while enhancing global context modeling.  \n\n**4. Data-Efficient Training**  \nAddressing small dataset challenges, techniques like self-supervised pretraining and attention-guided feature reuse have emerged. [129] employs masked autoencoding on unlabeled volumes, while [113] combines spatial and channel attention to reduce annotation dependence.  \n\n#### Case Studies and Performance  \n1. **MedT**: Integrating axial attention [103], this model processes 3D volumes slice-by-slice, achieving 87.2% Dice on Synapse multi-organ segmentation [129].  \n2. **UNesT**: With tokenized skip connections, it outperforms CNNs in neonatal brain MRI segmentation using 30% fewer parameters [129].  \n3. **DAE-Former**: Its dual-attention design enables robust segmentation of skin lesions and cardiac structures without pretraining [113].  \n\n#### Future Directions and Open Challenges  \nWhile current adaptations show promise, three key challenges persist:  \n- **Scalability**: Ultra-high-resolution volumes demand further optimization, potentially through innovations like [131].  \n- **Interpretability**: Clinical adoption requires explainable attention mechanisms, building on techniques from [110].  \n- **Multimodal Fusion**: Integrating diverse imaging modalities (MRI, CT) remains underexplored, though [132] offers preliminary insights.  \n\nThese medical-specific adaptations demonstrate how transformer architectures can be refined for clinical applications, setting the stage for further efficiency optimizations discussed in Section 3.6 on real-time deployment. By addressing domain constraints while preserving the strengths of global context modeling, they advance the practicality of AI-assisted medical diagnostics.  \n---\n\n### 3.6 Real-Time and Mobile Solutions\n\n---\n### 3.6 Real-Time and Mobile Solutions  \n\nBuilding upon the medical imaging adaptations discussed in Section 3.5, which addressed domain-specific challenges through architectural innovations, this section explores the deployment of transformer-based segmentation models in real-time and resource-constrained environments. The computational overhead of self-attention mechanisms poses unique challenges for mobile devices and edge computing platforms, necessitating specialized efficiency-focused approaches. This subsection examines lightweight architectures, token reduction strategies, and hardware-aware optimizations that enable practical deployment while maintaining performance.  \n\n#### Lightweight Architectures for Mobile Deployment  \n\nRecent advances in mobile-friendly transformer design focus on reducing self-attention complexity while preserving global modeling capabilities. [133] introduces a convolution-free ViT variant that uses bi-dimensional channel and spatial attention to reduce redundancy, achieving competitive accuracy with only 0.7G FLOPs. Similarly, [134] merges local (CNN-derived) and global (self-attention-derived) tokens, cutting FLOPs by 50% while maintaining spatial awareness.  \n\nFor extreme efficiency, [135] replaces attention layers with parameter-free shift operations. By exchanging channel subsets between patches, ShiftViT matches Swin Transformer performance while eliminating attention computations entirely—ideal for mobile devices due to reduced memory and energy consumption.  \n\n#### Real-Time Segmentation with Token Pruning and Dynamic Attention  \n\nTo achieve real-time performance, dynamic token pruning and sparse attention techniques have emerged. [136] employs a two-phase localization and focus mechanism, reducing FLOPs by 63% while doubling inference speed. For video segmentation, [137] linearizes attention complexity via Taylor expansion and sparsity, yielding 3× faster inference and energy savings.  \n\n#### Hardware-Aware Optimization and Quantization  \n\nEfficient mobile deployment often requires algorithm-hardware co-design. [138] binarizes weights and activations while using learnable scaling factors to mitigate attention distortion, achieving 61.5× FLOPs reduction. [139] replaces quadratic attention with linear-angular kernels during inference, cutting MACs by 40% and improving COCO mAP by 1.2 points.  \n\n#### Mobile-Friendly Hybrid Architectures  \n\nHybrid designs combine convolutional efficiency with transformer flexibility. [86] partitions computation across nodes, reducing parameters while achieving 10% higher accuracy than vanilla ViTs. For medical applications, [140] introduces gated axial-attention, minimizing FLOPs while maintaining precision on BraTS—bridging the gap between Sections 3.5 and 3.6.  \n\n#### Benchmarking and Performance Trade-offs  \n\nReal-time models must balance speed and accuracy. [141] rectifies inefficiencies in windowed attention, improving COCO mAP by 1.7% while reducing latency. [142] shows that minimizing redundancy in embeddings and attention maps can double throughput without accuracy loss.  \n\n#### Challenges and Future Directions  \n\nKey challenges persist:  \n- **Critical Attention Heads**: [143] reveals that pruning certain heads harms spatial reasoning, suggesting dynamic head selection as a solution.  \n- **Localization Integration**: [144] proposes coordinate prediction tasks to reduce dense attention dependence.  \n- **Robustness**: Balancing efficiency with robustness to geometric variations (e.g., [145]) remains open.  \n\nIn summary, real-time transformer-based segmentation advances through lightweight architectures (e.g., [146]), dynamic computation (e.g., [147]), and hardware co-design (e.g., [138]). These innovations, building on medical adaptations from Section 3.5, pave the way for edge deployment—setting the stage for broader applications discussed in subsequent sections.\n\n## 4 Applications in Visual Segmentation\n\n### 4.1 Medical Image Segmentation\n\n### 4.1 Medical Image Segmentation  \n\nMedical image segmentation plays a pivotal role in modern healthcare, enabling precise diagnosis, treatment planning, and surgical guidance. The advent of transformer-based models has significantly advanced this field by addressing key challenges such as anatomical variability, low-contrast imaging, and the demand for pixel-level precision. This subsection explores the transformative impact of transformer architectures across three critical medical imaging applications: lung cancer detection, brain tumor segmentation, and multimodal head-and-neck tumor analysis, while also discussing persistent challenges and future directions.  \n\n#### **Lung Cancer Detection**  \nLung cancer, a leading global cause of cancer-related mortality, requires early and accurate detection for effective intervention. While convolutional neural networks (CNNs) have traditionally dominated lung nodule segmentation in computed tomography (CT) scans, their limited receptive fields often fail to capture long-range dependencies in high-resolution images. Transformer-based models like the Swin Transformer have overcome this limitation through hierarchical architectures and shifted window mechanisms, enabling efficient global context modeling [5]. Recent adaptations incorporate multi-scale feature fusion to handle nodules of varying sizes, achieving state-of-the-art performance on benchmarks such as LUNA16. These models leverage self-attention to delineate subtle nodule boundaries often missed by CNNs [27], demonstrating the clinical value of transformers in early cancer detection.  \n\n#### **Brain Tumor Segmentation**  \nThe heterogeneous appearance of gliomas in magnetic resonance imaging (MRI) presents unique segmentation challenges. Hybrid architectures like TransBTSV2 combine 3D convolutional layers with transformer blocks to capture both local features and long-range spatial relationships in volumetric MRI data [6]. This approach addresses the computational constraints of pure transformers while outperforming CNN-based methods on the BraTS benchmark through axial attention mechanisms that preserve spatial resolution [5]. Further innovations include vision transformers with deformable attention, which adapt receptive fields to irregular tumor shapes, improving Dice scores by up to 5% for diffuse tumor regions [9]. The integration of panoptic segmentation frameworks also enables comprehensive tumor analysis by unifying semantic and instance segmentation [2].  \n\n#### **Multimodal Head-and-Neck Tumor Segmentation**  \nComplex head-and-neck tumor segmentation benefits from transformer architectures' ability to fuse multi-modal data (CT, MRI, PET) through cross-modal attention. MedT exemplifies this capability with a dual-branch design that processes different imaging modalities independently before combining features via shared self-attention layers [27]. Such approaches demonstrate robust performance even with limited annotated data. Recent progress in few-shot learning further addresses data scarcity, with vision transformers pretrained on large datasets and fine-tuned using minimal annotations [5]. Hybrid models like UNesT combine CNNs and transformers to preserve fine details while maintaining global context awareness [6], offering enhanced accuracy for intricate anatomical structures.  \n\n#### **Challenges and Future Directions**  \nThree key challenges persist in transformer-based medical segmentation: computational intensity, data scarcity, and domain adaptation. Sparse attention mechanisms [81] and low-rank approximations [28] show promise for reducing computational overhead. Self-supervised learning and synthetic data generation [24] may alleviate annotation bottlenecks. Future research should focus on:  \n1. Lightweight architectures for real-time applications [148]  \n2. Multimodal integration, including temporal data from medical video [26]  \n3. Domain adaptation techniques for clinical deployment [149]  \n\nIn summary, transformer-based models have revolutionized medical image segmentation by overcoming CNN limitations in global context modeling and multimodal fusion. Their demonstrated success in lung cancer, brain tumor, and head-and-neck applications underscores their potential to enhance diagnostic precision and patient care. Continued innovation in efficiency, generalization, and clinical integration will be crucial for realizing their full impact in healthcare.\n\n### 4.2 Autonomous Driving and LiDAR Segmentation\n\n### 4.2 Autonomous Driving and LiDAR Segmentation  \n\nThe evolution of transformer architectures has significantly advanced perception systems for autonomous driving, particularly in processing LiDAR (Light Detection and Ranging) point clouds. As a bridge between medical imaging (Section 4.1) and general-purpose segmentation (Section 4.3), this domain demands unique solutions for sparse, irregular 3D data while requiring real-time performance. This subsection examines how transformers address these challenges through three key aspects: LiDAR point cloud segmentation, panoptic segmentation, and efficiency optimizations for autonomous systems.  \n\n#### **Transformer-Based LiDAR Point Cloud Segmentation**  \n\nLiDAR data's sparse and unstructured nature poses fundamental challenges that transformer architectures are uniquely equipped to handle. Unlike CNNs constrained by grid-based processing, transformers excel at modeling arbitrary point relationships through self-attention. [29] pioneers this approach with a hierarchical transformer architecture that combines sparse attention patterns for computational efficiency with global context modeling, achieving state-of-the-art performance on SemanticKITTI. The method's success lies in its dynamic receptive fields, which adapt to varying point densities—a critical capability for autonomous driving scenarios.  \n\nFurther innovations address occlusion challenges prevalent in urban environments. [150] introduces an occupancy-aware attention mechanism that explicitly models occluded regions, improving segmentation accuracy for partially visible objects by 12% over CNN baselines. This aligns with broader trends in medical imaging (Section 4.1), where transformers similarly overcome visibility constraints in complex anatomies. Hybrid architectures also show promise, as seen in [29]'s integration of CNN layers for local feature extraction with transformer blocks for long-range dependency modeling—a design philosophy later echoed in general-purpose segmentation (Section 4.3).  \n\n#### **Panoptic Segmentation for Comprehensive Scene Understanding**  \n\nThe transition to transformer-based architectures has been particularly transformative for panoptic segmentation, which unifies semantic (\"stuff\") and instance (\"things\") segmentation. [31] establishes a new paradigm by replacing traditional hand-crafted components with masked attention mechanisms, achieving a 15% improvement in segmentation quality on Cityscapes. This mirrors advancements in medical panoptic segmentation [2], where transformers similarly unify diverse segmentation tasks.  \n\nEfficiency remains paramount for real-time autonomous systems. The [71] adapts its shifted window approach to reduce computational complexity by 40% while maintaining global receptive fields—a strategy later adopted in general-purpose models (Section 4.3). Meanwhile, [150] demonstrates how unified convolution-self-attention blocks can balance local feature extraction with global context modeling, particularly for fine-grained object boundaries. These innovations collectively address autonomous driving's dual needs for accuracy and speed.  \n\n#### **Challenges and Cross-Domain Solutions**  \n\nThree critical challenges persist in applying transformers to autonomous driving:  \n\n1. **Computational Efficiency**: High-resolution LiDAR processing demands innovations like [151]'s real-time transformer for depth estimation and [32]'s sparse attention patterns—techniques that parallel medical imaging efficiency gains (Section 4.1).  \n\n2. **Data Scarcity**: Limited annotated LiDAR datasets motivate solutions from other domains, such as [124]'s self-supervised learning and [152]'s CNN-inspired inductive biases.  \n\n3. **Multimodal Fusion**: Following medical imaging's success with cross-modal attention [27], frameworks like [153] suggest promising directions for integrating LiDAR with camera data.  \n\nFuture research could leverage generative transformers ([154]) for synthetic data augmentation while adapting efficiency techniques from general-purpose segmentation (Section 4.3), such as progressive token scaling [51].  \n\n#### **Conclusion**  \n\nTransformer-based methods have redefined LiDAR segmentation by overcoming CNN limitations in handling sparse 3D data and complex occlusions. Through architectures like [29] and [150], they enable more accurate and robust perception for autonomous vehicles. While challenges in efficiency and data scarcity remain, solutions emerging from medical imaging and general-purpose segmentation point to a convergent future where transformers power perception systems across domains. The next section (4.3) will explore how these innovations extend to broader segmentation tasks.\n\n### 4.3 General-Purpose Semantic and Instance Segmentation\n\n---\n### 4.3 General-Purpose Semantic and Instance Segmentation  \n\nBuilding upon the success of transformer-based methods in specialized domains like autonomous driving (Section 4.2), these architectures have similarly revolutionized general-purpose semantic and instance segmentation tasks. By leveraging their inherent ability to model global context and long-range dependencies, transformers have demonstrated superior performance over traditional convolutional approaches across various benchmarks and applications. This subsection systematically examines these advancements through three key perspectives: architectural innovations in semantic segmentation, breakthroughs in instance segmentation, and efficiency optimizations for practical deployment.\n\n#### Architectural Innovations in Semantic Segmentation  \n\nSemantic segmentation requires dense pixel-wise classification while maintaining fine-grained spatial understanding - a task particularly suited for transformers' global attention mechanisms. The field has evolved through several key architectural paradigms:\n\n1. **Pure Transformer Approaches**: Segmenter [43] pioneered this direction by formulating semantic segmentation as a sequence-to-sequence prediction task. Using patch embeddings and class tokens, it demonstrated how global attention could outperform convolutional networks in capturing whole-image contextual relationships.\n\n2. **Hierarchical Designs**: Swin-Unet [43] combined the Swin Transformer's shifted window approach with U-Net's decoder structure. This hybrid achieved multi-scale feature fusion while maintaining computational efficiency, setting new benchmarks on ADE20K (53.5% mIoU) and Cityscapes through its precise boundary delineation capabilities.\n\n3. **Enhanced Feature Propagation**: Recent innovations like DenseFormer [44] addressed the vanishing gradient problem in deep transformers through depth-weighted averaging. Similarly, IncepFormer [92] integrated pyramid pooling with transformer blocks, achieving superior multi-scale context modeling.\n\n#### Breakthroughs in Instance Segmentation  \n\nThe transition to transformer-based architectures has particularly transformed instance segmentation, where models must simultaneously detect and segment individual objects:\n\n- **Mask Classification Paradigm**: Mask4Former [27] revolutionized the field by unifying detection and segmentation into an end-to-end mask classification framework. This approach eliminated traditional hand-crafted components like anchor boxes while achieving 58.0% PQ on COCO.\n\n- **Query Optimization**: kMaX-DeepLab [48] advanced this paradigm by replacing cross-attention with k-means clustering for query generation. This innovation improved both computational efficiency (reducing FLOPs by 18%) and mask quality, particularly for small objects.\n\n#### Efficiency Optimizations and Practical Deployment  \n\nWhile transformer-based models achieve remarkable accuracy, their computational demands have spurred significant efficiency innovations:\n\n1. **Hybrid Architectures**: Models like P2AT [46] combine axial attention with lightweight CNN backbones, achieving real-time performance (48 FPS) on Cityscapes without sacrificing accuracy.\n\n2. **Computation Reduction**: Techniques like progressive token scaling in PRO-SCALE [51] reduce encoder computations by 52%, enabling deployment on edge devices.\n\n3. **Structural Optimizations**: SSformer [49] demonstrates how architectural modifications (grouped self-attention, skip connections) can maintain competitive performance while reducing parameters by 40%.\n\n#### Future Directions and Cross-Domain Potential  \n\nThe success of these transformer-based approaches in natural images (achieving 50.9% mIoU on ADE20K with DenseFormer) suggests promising directions:\n\n- **Self-Supervised Learning**: Techniques from [55] could enhance generalization with limited labeled data.\n\n- **Multimodal Integration**: Approaches like those in [53] may improve robustness through complementary data fusion.\n\n- **Scalability**: Addressing challenges in ultra-high-resolution processing and occlusion handling will be crucial for applications ranging from autonomous systems (extending Section 4.2) to medical imaging (anticipating Section 4.4).\n\nThis progression from specialized architectures to general-purpose solutions demonstrates transformers' transformative potential across the segmentation landscape. Their ability to balance global context modeling with computational efficiency continues to push the boundaries of what's possible in computer vision.\n\n### 4.4 Video Scene Parsing and Temporal Segmentation\n\n### 4.4 Video Scene Parsing and Temporal Segmentation  \n\nBuilding upon transformer architectures' success in general-purpose segmentation (Section 4.3), their application to video scene parsing and temporal segmentation has opened new frontiers in dynamic scene understanding. These tasks demand joint modeling of spatial features and temporal dependencies – a challenge where transformers' global attention mechanisms provide distinct advantages over traditional CNNs' limited receptive fields. This subsection systematically examines transformer-based approaches through three key dimensions: video semantic segmentation, 4D panoptic segmentation, and emerging solutions to computational challenges.\n\n#### Transformers for Video Semantic Segmentation  \n\nThe extension of semantic segmentation to video sequences requires consistent pixel-level classification while maintaining temporal coherence. Transformer architectures address this through two principal innovations:\n\n1. **Spatiotemporal Attention**: Video-SwinUNet adapts the hierarchical Swin Transformer architecture to video data through shifted window-based self-attention across both spatial and temporal dimensions. This design achieves 78.4% mIoU on Cityscapes-VPS by efficiently modeling local motion patterns and global scene dynamics.\n\n2. **Volumetric Processing**: For medical video analysis, [155] introduces a 3D transformer with Fusion-Head Self-Attention (FHSA), capturing inter-frame dependencies in dynamic MRI sequences. The architecture demonstrates 12% improvement over 3D CNNs in tumor boundary delineation while maintaining clinically feasible inference speeds.\n\n#### 4D Panoptic Segmentation with Transformers  \n\nThe unification of instance tracking and segmentation in video finds an elegant solution through transformer-based query mechanisms:\n\n- **Mask Classification Extension**: Mask4Former extends its image-based framework to video by incorporating temporal consistency into query-key matching. This approach achieves 51.2% VPQ on KITTI-STEP while eliminating traditional heuristic tracking components.\n\n- **Medical Motion Analysis**: Adapting these principles, [58] demonstrates precise cardiac structure segmentation across cardiac phases. The temporal attention mechanism captures cyclic motion patterns with 0.92 Dice similarity in 4D ultrasound analysis.\n\n#### Computational Challenges and Innovative Solutions  \n\nWhile transformers excel at spatiotemporal modeling, three key challenges have driven architectural innovations:\n\n1. **Efficiency Optimization**: [64] reduces attention redundancy through grouped spatial-temporal heads, cutting FLOPs by 35% without performance degradation.\n\n2. **Data Scarcity Mitigation**: Techniques from [156] show how cross-modal pretraining (e.g., RGB-thermal pairs) can compensate for limited annotated video data.\n\n3. **Real-Time Deployment**: Lightweight designs like those in [67] achieve 28 FPS on 1024×2048 surgical videos through depthwise separable attention.\n\n#### Future Directions and Cross-Modal Synergies  \n\nThe evolution of video transformers points to several promising avenues that bridge to emerging domains:\n\n- **Interpretability**: Methods from [157] could extend to video attention visualization, crucial for safety-critical applications like autonomous systems (extending Section 4.2).\n\n- **Multimodal Fusion**: Integrating approaches from [53] may enhance robustness through complementary sensor streams (LiDAR, thermal).\n\n- **Self-Supervised Learning**: Techniques like those in [55] could address annotation bottlenecks, particularly for medical video analysis (anticipating Section 4.5).\n\nThe progression from static image segmentation to dynamic video understanding demonstrates transformers' unique capacity to unify spatial and temporal modeling. As efficiency improvements and multimodal integration mature, these architectures are poised to become the foundation for next-generation video analysis systems across automotive, medical, and surveillance domains.\n\n## 5 Efficiency and Optimization Techniques\n\n### 5.1 Token Pruning and Merging\n\n### 5.1 Token Pruning and Merging  \n\nTo address the computational challenges of transformer-based segmentation models, token pruning and merging have emerged as essential techniques for improving efficiency while maintaining segmentation accuracy. These methods directly tackle the quadratic complexity of self-attention by reducing redundant or less informative tokens, enabling more scalable processing of high-resolution images and videos.  \n\n#### **Token Pruning: Dynamic Removal of Redundant Tokens**  \nToken pruning selectively eliminates tokens that contribute minimally to the segmentation output, typically by scoring tokens based on their importance. A common approach leverages attention scores as a proxy for token relevance, where tokens with low attention weights are pruned. For instance, [78] uses masked attention to focus computation only on salient regions, effectively discarding irrelevant tokens. Similarly, [11] introduces a dynamic pruning mechanism based on the entropy of attention maps, achieving real-time performance on high-resolution datasets like Cityscapes and KITTI.  \n\nAdaptive methods further refine this process by learning token importance scores. [80] employs a lightweight gating network to predict token relevance dynamically, which is particularly effective for panoramic segmentation where large portions of the input may contain low-information regions.  \n\n#### **Token Merging: Aggregating Similar Tokens**  \nToken merging reduces redundancy by combining semantically similar tokens into a compact set of representative tokens, preserving global context while lowering computational costs. Clustering-based approaches, such as those in [81], group LiDAR point cloud tokens into instance-aware clusters, significantly reducing token counts for 3D segmentation tasks. Similarly, [158] merges tokens using contrastive learning to align similar embeddings, enabling efficient panoptic segmentation.  \n\nHierarchical merging strategies, exemplified by [71], progressively combine adjacent patches in multi-stage architectures. This approach has been successfully adapted for segmentation tasks, as discussed in [2], where it demonstrates superior efficiency compared to flat tokenization schemes.  \n\n#### **Hybrid Approaches: Combining Pruning and Merging**  \nRecent advances integrate pruning and merging to maximize efficiency. [79] introduces a unified framework that first prunes semantically irrelevant tokens and then merges remaining tokens using a learned affinity matrix, achieving a 30% reduction in computational costs without sacrificing performance. Similarly, [120] combines pruning with prompt-based merging, retaining only task-critical tokens and compressing them into compact prompts for efficient continual learning.  \n\n#### **Challenges and Future Directions**  \nWhile token pruning and merging significantly enhance efficiency, challenges remain in balancing computational savings with segmentation accuracy. Over-aggressive pruning or merging can degrade performance, particularly at object boundaries, as noted in [15]. Future work could explore adaptive thresholds or reinforcement learning to dynamically optimize this trade-off.  \n\nScalability to video segmentation is another critical challenge, where spatio-temporal redundancy must be addressed. Techniques like optical flow-guided pruning, as proposed in [26], show promise for efficient video processing. Additionally, integrating these methods with other efficiency techniques, such as quantization and knowledge distillation—as suggested in [28]—could further advance real-time segmentation capabilities.  \n\nIn summary, token pruning and merging are pivotal for deploying transformer-based segmentation models in resource-constrained environments. By dynamically reducing computational overhead, these techniques enable efficient processing without compromising accuracy, making them indispensable for applications in autonomous driving, robotics, and medical imaging. Their continued refinement will be crucial for bridging the gap between high-performance segmentation and real-world deployment.\n\n### 5.2 Low-Rank Approximations and Sparse Attention\n\n### 5.2 Low-Rank Approximations and Sparse Attention  \n\nBuilding on the token efficiency techniques discussed in Section 5.1, this subsection explores low-rank approximations and sparse attention mechanisms—two complementary approaches to address the quadratic complexity of transformer self-attention in visual segmentation. These methods enable efficient global context modeling while maintaining segmentation accuracy, laying the groundwork for the knowledge distillation techniques covered in Section 5.3.  \n\n#### **Low-Rank Factorization of Attention**  \nGiven the inherent redundancy in full attention matrices, low-rank factorization has emerged as a powerful tool to reduce computational overhead. By decomposing attention into compact representations, these methods preserve essential interactions while eliminating redundant computations. For instance, [159] demonstrates that low-rank approximations maintain performance in diffusion models, while [87] integrates them into hybrid architectures for efficient deep-layer processing.  \n\nKernelized attention further refines this approach by reformulating softmax attention into linear operations. [160] shows how kernel approximations achieve competitive segmentation accuracy while avoiding the memory bottlenecks of traditional attention—particularly valuable for high-resolution feature maps.  \n\n#### **Sparse Attention Patterns**  \nSparse methods strategically limit token interactions to computationally tractable subsets. Fixed patterns like local windows (e.g., [32]) balance receptive field size with efficiency through dilation, while dynamic approaches like [41] alternate between local and global layers. These hybrid sparse-dense patterns are especially effective for segmentation, where multi-scale fusion is critical for boundary precision.  \n\n#### **Linear Attention Transformations**  \nReformulating attention as linear operations enables near-linear complexity without explicit matrix construction. [139] decomposes attention into linear terms and residuals, retaining only essential components during inference. Similarly, [122] leverages FFT-based circular convolutions to mimic global attention efficiently—bridging convolutional and transformer paradigms for real-time segmentation.  \n\n#### **Applications in Segmentation**  \nThese techniques have proven particularly impactful in segmentation architectures. [29] replaces ASPP with low-rank attention, demonstrating that multi-scale context can be captured efficiently without sacrificing accuracy. Such adaptations highlight how these methods enable transformers to meet the dual demands of precision and scalability in dense prediction tasks.  \n\n#### **Challenges and Future Directions**  \nKey challenges remain in balancing approximation fidelity with efficiency gains. As [33] notes, overly aggressive approximations may degrade localization-sensitive tasks. Architectural integration also requires careful design, as discussed in [161], to avoid disrupting hierarchical feature learning.  \n\nPromising directions include:  \n1. **Dynamic sparsity**: Adaptive patterns based on input content ([40])  \n2. **Hardware co-design**: Combining these methods with quantization for edge deployment ([162])  \n3. **Cross-layer optimization**: Coordinating low-rank and sparse approaches across transformer stages  \n\nBy continuing to refine these approximations while addressing their limitations, the field can further unlock scalable transformer-based segmentation for real-world applications—from medical imaging to autonomous systems.\n\n### 5.3 Knowledge Distillation Strategies\n\n---\n### 5.3 Knowledge Distillation for Efficient Transformer Segmentation  \n\nBuilding upon the efficiency improvements from low-rank approximations and sparse attention discussed in Section 5.2, knowledge distillation (KD) has emerged as a complementary strategy for compressing large transformer-based segmentation models into smaller, more efficient variants while preserving performance. This subsection explores distillation techniques specifically adapted for transformer architectures in visual segmentation, with a focus on robustness-reinforced approaches and correlation-based loss formulations. These methods address the unique challenges of distilling transformers' global context modeling and hierarchical feature representations, while paving the way for the lightweight architectures discussed in Section 5.4.  \n\n#### Foundations of Knowledge Distillation for Transformers  \nTraditional KD frameworks transfer knowledge by minimizing the divergence between teacher and student outputs or features. However, transformer-based segmentation models like [45] and [63] introduce complexities due to their multi-scale self-attention and long-range dependencies. As shown in [94], simply mimicking final outputs often fails to capture transformers' rich intermediate representations, necessitating specialized distillation approaches.  \n\n#### Robustness-Reinforced Distillation  \nThese methods enhance student models' resilience to distribution shifts while compressing the model—a critical capability for applications like autonomous driving and medical imaging. [50] introduces adversarial examples during distillation, training the student to match both clean and perturbed teacher outputs. This dual-objective learning improves generalization, yielding 6.9% higher robustness on medical benchmarks.  \n\n[91] further refines this approach with class-specific weighting, prioritizing critical regions (e.g., tumors). This reduces the mIoU drop from 4.2% to 1.8% when compressing a transformer to 1/5th its original size.  \n\n#### Correlation-Based Distillation  \nThese methods preserve structural relationships between features rather than absolute values. [47] aligns teacher and student token similarity matrices via cross-attention correlation loss, achieving 84.8% accuracy on Kinetics-600 with a 3x smaller model.  \n\n[44] decomposes features into low-frequency (global) and high-frequency (local) components, applying separate distillation losses. This dual-stream approach reduces FLOPs by 2.5x with only 0.5% mIoU degradation on Cityscapes.  \n\n#### Attention-Specific Distillation  \nGiven transformers' reliance on attention mechanisms, several works directly distill attention maps. [128] proposes multi-head attention alignment, where student heads approximate weighted combinations of teacher heads. This achieves 50.9% mIoU on BraTS with 40% fewer parameters.  \n\nFor 3D tasks, [127] uses optimal transport to distill dynamic window attention patterns, improving nuScenes performance by 3.2%.  \n\n#### Efficiency-Aware Distillation Frameworks  \nRecent work optimizes the distillation process itself. [51] gradually reduces student tokens, yielding 52% GFLOPs reduction without accuracy loss on COCO. [163] introduces a zero-shot distillation metric, reducing carbon footprint by 5x.  \n\n#### Challenges and Future Directions  \nKey challenges include distilling small-data transformers ([164]) and hardware-aware hybrid approaches ([54]).  \n\nFuture directions may explore:  \n1. Dynamic distillation based on input complexity ([46])  \n2. Cross-modal techniques ([53])  \n3. Self-distillation leveraging self-attention analysis ([165])  \n\nThese advances will be critical for deploying efficient transformer-based segmentation across resource-constrained applications.\n\n### 5.4 Lightweight Architecture Design\n\n### 5.4 Lightweight Architecture Design  \n\nFollowing the knowledge distillation techniques discussed in Section 5.3, lightweight architecture design represents another critical approach to improving the efficiency of transformer-based segmentation models. These innovations—spanning hybrid CNN-transformer architectures, efficient self-attention variants, and parameter-efficient designs—address the computational challenges of pure transformer models while maintaining performance, paving the way for the quantization techniques explored in Section 5.5.  \n\n#### Hybrid CNN-Transformer Architectures  \n\nHybrid architectures synergize the local feature extraction strengths of CNNs with the global context modeling of transformers, offering a balanced solution for efficiency and accuracy. For instance, [58] introduces a U-Net-like framework where a transformer encoder processes tokenized CNN feature maps, reducing computational overhead while preserving precise localization. Similarly, [61] demonstrates that convolutional architectures enhanced with transformer-inspired designs, such as long-range skip connections, can rival pure transformer models in efficiency.  \n\nFurther advancements include [72], which integrates transformer blocks into CNNs to improve boundary delineation in dermatological images by combining fine-grained spatial details with long-range dependencies. [98] refines this approach through cascaded multi-scale self-attention and convolutional layers, enabling hierarchical feature fusion for complex structures like tumors.  \n\n#### Efficient Self-Attention Variants  \n\nTo mitigate the quadratic complexity of standard self-attention, several efficient variants have emerged. [67] proposes a boundary-aware local transformer (BLT) module that adaptively partitions windows based on entropy, reducing redundant computations while preserving structural details. This approach achieves superior efficiency on datasets like ACDC and ISIC 2018 with minimal performance trade-offs.  \n\n[60] systematically evaluates attention mechanisms, highlighting axial and deformable attention as particularly effective for reducing costs. Axial attention processes input along separate spatial axes, lowering memory usage, while deformable attention dynamically focuses on relevant regions. [42] replaces self-attention with non-negative matrix factorization (NMF), a linear-scaling alternative that achieves competitive results on BraTS and ISLES’22 datasets.  \n\n#### Parameter-Efficient Designs  \n\nParameter reduction is critical for clinical deployment. [68] introduces low-rank adaptation, where small trainable \"plug-ins\" are added to a frozen transformer backbone, reducing trainable parameters by 99.83% without performance loss. [64] optimizes 3D registration with plane-based and cascaded group attention, achieving state-of-the-art results on OASIS with 16–27× fewer parameters. [63] further demonstrates scalability by segmenting 133 brain structures in a single hierarchical network.  \n\n#### Real-Time and Mobile Solutions  \n\nFor real-time applications, compact designs are essential. [70] shows that compact convolutional transformers (CCTs) perform well on small datasets, ideal for mobile health. [69] introduces a high-resolution convolutional transformer (HCT) with linear self-attention approximation, enabling efficient mammography analysis.  \n\n#### Challenges and Future Directions  \n\nKey challenges include balancing hybrid components to avoid over-parameterization [94] and improving interpretability for clinical trust [157]. Future directions may involve dynamic architecture search [102] and hardware-aware optimizations [156].  \n\nIn summary, lightweight transformer-based segmentation architectures—through hybrid designs, efficient attention, and parameter reduction—deliver high performance with minimal computational cost, bridging the gap between efficiency and accuracy for practical deployment.\n\n### 5.5 Quantization and Mixed-Precision Techniques\n\n### 5.5 Quantization and Mixed-Precision Techniques  \n\nQuantization and mixed-precision techniques have become indispensable for optimizing transformer-based visual segmentation models, particularly as the field shifts toward deployment in resource-constrained environments. These methods address the computational and memory bottlenecks of transformers by reducing the bit-width of weights and activations while preserving model accuracy. The two primary paradigms—post-training quantization (PTQ) and quantization-aware training (QAT)—offer distinct trade-offs between efficiency and performance, with recent advances bridging the gap between them.  \n\n#### Post-Training Quantization (PTQ)  \n\nPTQ converts pre-trained full-precision models (typically 32-bit floating-point) into lower-bit representations (e.g., 8-bit integers) without retraining, offering immediate computational benefits. However, its simplicity comes at the cost of potential accuracy degradation due to precision loss. Recent innovations have mitigated this by refining calibration techniques and dynamic range adjustment. For example, [106] shows that self-attention mechanisms remain robust under aggressive quantization when attention weights and activations are carefully calibrated.  \n\nA key challenge in PTQ for transformers is the sensitivity of self-attention operations to quantization errors. [103] tackles this with per-channel quantization for attention weights, preserving dynamic ranges and achieving 4× memory savings without compromising panoptic segmentation performance. Similarly, [166] introduces a mobile-optimized 4-bit quantization scheme for additive attention, enabling real-time inference with minimal accuracy loss.  \n\n#### Quantization-Aware Training (QAT)  \n\nQAT integrates quantization into the training loop, allowing models to adapt to lower precision during optimization. While computationally intensive, QAT typically outperforms PTQ in accuracy by simulating quantization effects through \"fake quantization\" operations during forward passes. [109] demonstrates this with an 8-bit QAT-trained model that matches floating-point accuracy in semantic segmentation.  \n\nFor attention-based models, QAT’s adaptability is particularly valuable. [104] combines QAT with mixed-precision, retaining 16-bit precision for critical attention heads while quantizing less sensitive operations to 8-bit. This hybrid approach balances efficiency and accuracy, achieving state-of-the-art results on ImageNet and COCO.  \n\n#### Mixed-Precision Techniques  \n\nMixed-precision quantization dynamically assigns bit-widths based on layer sensitivity, optimizing efficiency without uniform precision loss. [105] employs a hierarchical scheme: early self-attention layers use 16-bit precision to capture fine details, while later stages operate at 8-bit. This preserves accuracy while reducing computational overhead.  \n\nAsymmetric quantization further refines this idea. [167] quantizes key-query matrices to 4-bit (tolerant to precision reduction) while keeping value matrices at 8-bit, enabling linear-time complexity with minimal accuracy drop. Similarly, [115] shows 4-bit-quantized polynomial attention approximations can maintain global context modeling, making them viable for high-resolution segmentation.  \n\n#### Challenges and Future Directions  \n\nDespite progress, key challenges persist. The interplay between quantization and dynamic attention mechanisms (e.g., deformable attention [104]) remains underexplored. Current mixed-precision methods often rely on heuristics; automated approaches like reinforcement learning [108] could optimize bit-width allocation.  \n\nIntegration with other efficiency techniques also holds promise. For instance, [131] combines 4-bit quantization with linear attention for sub-linear complexity, enabling edge-device deployment. Future work should unify quantization with sparsity and pruning while developing theoretical frameworks to guide precision allocation in attention architectures.  \n\nIn summary, quantization and mixed-precision techniques are pivotal for practical transformer-based segmentation, offering substantial gains in efficiency without sacrificing accuracy. As hardware-aware optimizations gain traction (discussed in Section 5.6), these methods will play a central role in enabling real-time, energy-efficient deployments across diverse platforms.\n\n### 5.6 Hardware-Aware Optimization\n\n### 5.6 Hardware-Aware Optimization  \n\nBuilding upon the quantization and mixed-precision techniques discussed in Section 5.5, hardware-aware optimization represents the next critical frontier for deploying transformer-based visual segmentation models efficiently. This subsection explores the co-design of algorithms and hardware accelerators to address the remaining challenges in computational complexity, memory bottlenecks, and energy efficiency, particularly for resource-constrained edge devices and real-time applications.  \n\n#### Challenges in Hardware Deployment  \nThe deployment of vision transformers (ViTs) on hardware platforms faces several key challenges rooted in their architectural design. First, the self-attention mechanism’s quadratic complexity with respect to sequence length (e.g., image patches) creates prohibitive computational overhead for high-resolution inputs. Second, the memory footprint of storing attention matrices and intermediate activations strains hardware resources, limiting scalability. Third, the irregular memory access patterns of attention operations reduce the efficiency of conventional hardware architectures like GPUs and CPUs. These challenges necessitate hardware-algorithm co-design strategies that optimize both the model’s computational graph and the underlying hardware architecture.  \n\n#### Algorithm-Hardware Co-Design Strategies  \n\n**1. Linearized Attention and Low-Rank Approximations**  \nTo mitigate the quadratic cost of self-attention, recent works propose hardware-friendly linearized attention mechanisms. For instance, [137] introduces a linear Taylor attention approximation, reducing the attention computation to a low-rank form while preserving accuracy. This approach leverages first-order Taylor expansion to approximate softmax attention, enabling efficient matrix operations that align with hardware parallelism. Similarly, [139] replaces quadratic attention with linear-angular attention during inference, achieving significant FLOPs reduction without sacrificing performance. These methods are particularly amenable to hardware acceleration due to their structured computation patterns.  \n\n**2. Token Pruning and Dynamic Sparsity**  \nDynamic token pruning techniques, such as those explored in [136], identify and discard redundant tokens early in the pipeline, reducing the effective sequence length for subsequent layers. This strategy not only cuts computation but also minimizes memory bandwidth requirements. Hardware accelerators can exploit this sparsity by integrating dynamic scheduling units that skip computations for pruned tokens, as demonstrated in [135], where shift operations replace attention heads to reduce hardware overhead.  \n\n**3. Quantization and Mixed-Precision Techniques**  \nComplementing the methods in Section 5.5, hardware-aware quantization further optimizes ViTs for deployment. [138] achieves fully binarized ViTs by addressing gradient vanishing and ranking disorder in attention maps, enabling 61.5× FLOPs reduction compared to full-precision models. Mixed-precision quantization, where critical layers (e.g., attention heads) retain higher precision, balances accuracy and efficiency while leveraging hardware support like NVIDIA’s Tensor Cores.  \n\n**4. Hardware-Specific Attention Variants**  \nCustom attention mechanisms tailored for hardware constraints have emerged. For example, [168] replaces global attention with windowed selective scans, reducing memory access latency by localizing computations. Similarly, [134] integrates convolutional inductive biases into attention layers, simplifying hardware implementation while maintaining global context. These designs often leverage systolic arrays or spatial architectures to exploit parallelism.  \n\n#### Hardware Accelerator Architectures  \n\n**1. Spatial Accelerators for Attention**  \nSpecialized accelerators, such as those proposed in [137], employ spatial architectures to exploit the parallelism in linearized attention. These accelerators feature dedicated units for low-rank matrix multiplications and sparse-dense attention fusion, achieving 3× energy efficiency gains over GPU baselines.  \n\n**2. Memory-Centric Designs**  \nMemory bandwidth remains a critical bottleneck for ViTs due to large intermediate activations. [133] addresses this by decoupling invariant and equivariant tasks, reducing memory traffic through hierarchical feature aggregation. Hardware designs like [86] further optimize memory access by partitioning workloads across multiple nodes, enabling collaborative inference.  \n\n**3. Energy-Efficient Inference**  \nEnergy efficiency is paramount for edge deployment. Techniques like [142] reduce redundancy in ViTs, enabling smaller, energy-efficient models without sacrificing accuracy. Hardware platforms like FPGAs and ASICs benefit from these optimizations by minimizing dynamic power consumption through sparsity-aware scheduling.  \n\n#### Case Studies and Benchmarks  \n\n1. **ViTALiTy**: This framework demonstrates the synergy between linear Taylor attention and hardware acceleration, achieving 3× faster inference and 3× energy savings on ImageNet compared to vanilla ViTs [137].  \n2. **Bi-ViT**: Fully binarized ViTs achieve 22.1% higher accuracy than baselines on DeiT-Tiny, with 61.5× FLOPs reduction [138].  \n3. **LocalMamba**: By combining windowed scans with hardware-aware sparsity, this model reduces latency by 40% on GPU deployments [168].  \n\n#### Future Directions  \nFuture research should explore:  \n1. **Dynamic Hardware Reconfiguration**: Adapting accelerator architectures on-the-fly to varying input resolutions or sparsity patterns.  \n2. **Cross-Stack Optimization**: Jointly optimizing compiler toolchains (e.g., TVM, Halide) with hardware-aware ViT variants.  \n3. **Energy-Quality Trade-offs**: Techniques like [126] could inspire energy-efficient pretraining for hardware-aware models.  \n\nIn summary, hardware-aware optimization for transformer-based segmentation requires a holistic approach, blending algorithmic innovations like linearized attention and token pruning with tailored hardware architectures. The co-design paradigm exemplified by [137] and [138] underscores the potential to unlock real-time, energy-efficient deployment across diverse platforms. This sets the stage for broader discussions on system-level integration, as explored in subsequent sections.\n\n## 6 Challenges and Open Problems\n\n### 6.1 Data Dependency and Generalization\n\n---\n### 6.1 Data Dependency and Generalization  \n\nTransformer-based segmentation models have achieved state-of-the-art performance across various visual tasks, but their success is heavily dependent on large-scale annotated datasets. Unlike convolutional neural networks (CNNs), which leverage inherent spatial inductive biases to generalize effectively with moderate data, transformers require extensive training data due to their self-attention mechanisms and lack of built-in locality priors [27]. This data dependency poses significant challenges in domains where labeled data is scarce or domain-specific, such as medical imaging or specialized industrial applications [17; 6].  \n\n#### Limitations of Data Dependency  \n\nThe high data requirements of transformer-based models manifest in several key limitations:  \n\n1. **Poor Generalization on Small Datasets**:  \n   Transformers struggle to achieve competitive performance when trained on limited data, particularly in domains like medical imaging where expert annotations are costly and time-consuming [6]. While CNNs such as U-Net excel in these settings by exploiting local spatial hierarchies, transformers often require extensive data augmentation or pretraining on external datasets to compensate [17]. Similarly, in autonomous driving, transformer-based models demand massive LiDAR and camera datasets for robust performance, which may be impractical for niche applications or regions with limited infrastructure [12].  \n\n2. **Computational and Resource Barriers**:  \n   Training transformers on large-scale datasets necessitates substantial computational resources, creating accessibility challenges for researchers and practitioners without high-performance hardware [11]. For instance, state-of-the-art models on benchmarks like COCO and ADE20K often require thousands of GPU hours for training, limiting their adoption in resource-constrained settings [78; 79].  \n\n#### Challenges in Domain Adaptation and Long-Tailed Learning  \n\nBeyond data scarcity, transformer-based models face additional generalization challenges:  \n\n1. **Domain-Specific Adaptation**:  \n   Performance often degrades when models trained on general-purpose benchmarks (e.g., Cityscapes, Pascal VOC) are deployed in specialized domains like agriculture or forestry, where object appearance varies significantly across environments [169]. The lack of domain-specific pretraining data exacerbates this issue, as transformers rely heavily on pretraining to learn transferable features [170].  \n\n2. **Long-Tailed Data Distributions**:  \n   Transformers, like CNNs, exhibit bias toward majority classes in imbalanced datasets, leading to poor segmentation of rare but critical objects (e.g., construction equipment in urban scenes) [2]. While techniques like class-balanced sampling or focal loss can mitigate this, they often require manual tuning and may not fully resolve the imbalance [7].  \n\n#### Mitigation Strategies and Future Directions  \n\nTo address these challenges, researchers have explored several promising approaches:  \n\n1. **Self-Supervised and Weakly Supervised Learning**:  \n   Methods like [23] use synthetic or unlabeled data to generate pseudo-labels, reducing annotation dependency. Similarly, [25] employs weak supervision via class proportions, though these methods often trade accuracy for reduced labeling effort [24].  \n\n2. **Domain Adaptation and Transfer Learning**:  \n   Techniques such as [149] leverage multi-modal data for cross-domain generalization, while [120] adapts pretrained models with minimal target-domain data. However, these approaches still assume some annotated target data, which may not always be available [169].  \n\n3. **Open Research Problems**:  \n   Key unresolved challenges include:  \n   - **Efficient Pretraining**: Lightweight strategies to reduce data needs without sacrificing performance [11].  \n   - **Cross-Domain Robustness**: Architectures that generalize natively across diverse domains [149].  \n   - **Few-Shot Learning**: Enabling transformers to learn from few examples, akin to CNNs [171].  \n   - **Imbalance-Aware Training**: Novel loss functions or sampling methods for long-tailed data [2].  \n\nIn summary, while transformer-based models have pushed the boundaries of segmentation accuracy, their reliance on large-scale data and limited generalization to niche or imbalanced datasets remain critical hurdles. Advances in self-supervision, domain adaptation, and efficiency will be essential to broaden their applicability in real-world scenarios.  \n\n---\n\n### 6.2 Robustness to Adversarial Attacks\n\n### 6.2 Robustness to Adversarial Attacks  \n\nWhile transformer-based models have achieved state-of-the-art performance in visual segmentation, their robustness to adversarial attacks remains a critical limitation, particularly when compared to convolutional neural networks (CNNs). This vulnerability stems from their reliance on global self-attention mechanisms, which lack the inherent spatial inductive biases that provide CNNs with some degree of natural robustness. This subsection examines the unique vulnerabilities of transformer-based segmentation models, discusses current defense strategies, and outlines open challenges in developing robust transformer architectures.  \n\n#### Unique Vulnerabilities of Transformer-Based Models  \n\nThe self-attention mechanism in vision transformers (ViTs), while effective for capturing long-range dependencies, introduces distinct attack surfaces. Unlike CNNs, where perturbations are often localized, adversarial attacks on ViTs can exploit their global receptive field to propagate distortions across the entire image. For instance, [31] demonstrates that ViTs' patch-based tokenization makes them highly sensitive to perturbations in critical patches, disproportionately affecting segmentation outputs. This sensitivity is further amplified by ViTs' lack of spatial invariance, as shown in [123], which reveals that ViTs exhibit heightened susceptibility to spatial shifts and adversarial noise, especially in shallow layers where local features are less robustly encoded.  \n\nAdditionally, the quadratic complexity of self-attention creates opportunities for computationally intensive adversarial attacks. [172] illustrates how manipulating token interactions in attention maps can induce misclassifications, while [33] demonstrates that ViTs are more vulnerable to gradient-based attacks (e.g., FGSM and PGD) due to their attention mechanisms amplifying small perturbations.  \n\n#### Current Defense Strategies  \n\nTo address these vulnerabilities, researchers have developed several defense approaches, including adversarial training, attention mechanism modifications, and hybrid architectures.  \n\n1. **Adversarial Training**:  \n   Training ViTs with adversarial examples can improve robustness by enforcing local inductive biases. [152] shows that pre-training with adversarial examples enhances resilience, though this approach demands significant computational resources and may not generalize to novel attack types.  \n\n2. **Attention Mechanism Enhancements**:  \n   Modifying self-attention to reduce adversarial susceptibility is a promising direction. [32] introduces sparse global context sampling to mitigate noise impact, while [125] proposes dynamic attention weighting based on token robustness scores. These methods aim to preserve global attention benefits while reducing vulnerabilities.  \n\n3. **Hybrid Architectures**:  \n   Combining ViTs with CNNs leverages the strengths of both architectures. [173] integrates convolutional layers to bolster local feature robustness, and [150] unifies convolution and self-attention to balance local and global feature extraction. Such hybrids often demonstrate improved adversarial performance.  \n\n4. **Robust Tokenization**:  \n   Enhancing patch embedding can stabilize ViTs against localized perturbations. [174] employs progressive token aggregation to reduce sensitivity, aligning with findings from [84], which shows convolutional feature extraction can improve adversarial stability.  \n\n#### Persistent Challenges  \n\nDespite progress, key challenges remain:  \n\n1. **Computational Overhead**:  \n   Many defenses, such as adversarial training and attention modifications, incur high computational costs. [175] highlights the trade-off between robustness and inference speed, complicating real-world deployment.  \n\n2. **Attack Generalization**:  \n   Current defenses often target specific attack types (e.g., gradient-based) but struggle against evolving adversaries like patch-based or semantic attacks. [176] underscores the need for adaptive defenses.  \n\n3. **Interpretability Gaps**:  \n   The opacity of ViTs makes diagnosing robustness failures difficult. While [29] proposes attention map visualization for vulnerability identification, more interpretable robustness metrics are needed.  \n\n4. **Domain-Specific Robustness**:  \n   Adversarial robustness in critical domains (e.g., medical imaging) remains understudied. [153] and [30] emphasize the life-critical implications of attacks in medical segmentation.  \n\n#### Future Directions  \n\nPromising research avenues include:  \n- **Adaptive Defenses**: Developing real-time defenses that adjust to attack types, as explored in [172].  \n- **Self-Supervised Robustness**: Leveraging self-supervised learning to pre-train on adversarial examples, suggested by [124].  \n- **Hardware-Co-Design**: Co-optimizing robust ViTs with hardware accelerators to minimize overhead, as discussed in [175].  \n\nIn summary, while transformers have advanced visual segmentation, their adversarial robustness lags behind CNNs. Addressing this gap requires a multifaceted approach, combining architectural innovations, adaptive defenses, and domain-specific solutions to ensure reliable deployment in real-world applications.\n\n### 6.3 Interpretability and Transparency\n\n### 6.3 Interpretability and Transparency  \n\nThe \"black-box\" nature of transformer-based models poses significant challenges in understanding their decision-making processes, particularly in critical applications like medical imaging and autonomous driving, where interpretability is paramount. Unlike convolutional neural networks (CNNs), which rely on spatially localized filters, transformers leverage self-attention mechanisms to capture global dependencies, making their internal workings less intuitive. This subsection explores methods to enhance interpretability and transparency in transformer-based visual segmentation, focusing on attention map visualization, feature hierarchy analysis, and emerging techniques to demystify these models.  \n\n#### Attention Map Visualization  \nAttention maps serve as a primary tool for interpreting transformer models, revealing the regions of an image that influence segmentation decisions. For instance, [177] demonstrates how attention maps in interactive segmentation models align with user-provided clicks, enabling clinicians to validate the model's focus areas. Similarly, [91] shows that class-aware attention modules prioritize semantically meaningful regions, such as tumor boundaries, enhancing trust in the model's outputs.  \n\nHowever, raw attention maps often exhibit noise or diffuseness, limiting their interpretability. To address this, [32] introduces dilated neighborhood attention, which produces sparser yet more meaningful attention patterns by focusing on selective global contexts. This refinement clarifies how transformers balance local and global information. Additionally, [42] employs non-negative matrix factorization (NMF) to decompose attention maps into interpretable components, offering deeper insights into multi-scale feature processing.  \n\n#### Feature Hierarchy Analysis  \nTransformers' hierarchical designs, such as those in [45], inherently encode multi-scale features, enabling analysis of how the model transitions from low-level textures to high-level semantics. For example, [63] reveals that shallow layers capture local anatomical structures (e.g., tissue boundaries), while deeper layers aggregate global context (e.g., organ shapes), aligning with human intuition.  \n\nDespite this alignment, challenges remain in linking feature hierarchies to specific segmentation outcomes. [94] critiques the replaceability of transformer-learned features, showing that some hierarchical representations can be mimicked by simpler linear operators, raising questions about their uniqueness. To mitigate this, [128] proposes a hybrid architecture where convolutional inductive biases guide feature learning, ensuring interpretability while retaining the benefits of global attention.  \n\n#### Emerging Techniques for Transparency  \nRecent advances have introduced novel methods to enhance transformer interpretability. For instance, [50] evaluates transformer robustness through uncertainty estimation, revealing that attention mechanisms can overconfidently focus on spurious correlations. This underscores the need for calibration techniques, such as temperature scaling or Bayesian attention, to improve model reliability. Similarly, [178] highlights discrepancies between attention maps and model performance, suggesting that attention alone may not fully explain transformer behavior.  \n\nSurrogate models offer another promising direction, as explored in [164]. Here, simpler proxy models approximate transformer decisions, providing human-readable approximations of complex attention dynamics. Meanwhile, [90] introduces memory tokens to track long-range dependencies, offering tangible representations of how global context influences segmentation.  \n\n#### Open Challenges  \nDespite progress, several challenges persist:  \n1. **Attention-Output Misalignment**: Attention maps do not always correlate with model predictions, as noted in [178], necessitating better metrics to quantify interpretability.  \n2. **Scalability**: High-resolution 3D segmentation models, such as [179], face computational bottlenecks when generating attention maps, limiting real-time interpretability.  \n3. **Domain-Specific Interpretability**: Applications like medical imaging and autonomous driving require tailored explanations. For example, [55] emphasizes the need for lesion-specific attention visualizations, while [93] highlights the importance of real-time attention monitoring for safety-critical systems.  \n\n#### Future Directions  \nFuture research could explore:  \n- **Dynamic Attention Visualization**: Techniques like [51] could adaptively highlight relevant regions during inference.  \n- **Cross-Modal Interpretability**: Multimodal transformers, as discussed in [53], could leverage text or audio cues to explain visual attention.  \n- **Benchmarking Interpretability**: Standardized benchmarks, akin to those in [27], are needed to evaluate interpretability methods across tasks.  \n\nIn conclusion, while transformers' opacity remains a challenge, advances in attention visualization, feature hierarchy analysis, and surrogate modeling are steadily improving transparency. Addressing open challenges will require interdisciplinary collaboration, combining insights from computer vision, cognitive science, and domain-specific applications.\n\n### 6.4 Handling Long-Range Dependencies and Multi-Scale Features\n\n---\n### 6.4 Handling Long-Range Dependencies and Multi-Scale Features  \n\nBuilding on the interpretability challenges discussed in Section 6.3, transformer-based models face additional complexities when modeling long-range dependencies and multi-scale features—a capability central to their success in visual segmentation. While self-attention theoretically enables global context modeling, practical limitations emerge in handling high-resolution inputs, particularly in medical imaging where fine-grained anatomical structures require both local precision and global coherence. This subsection examines these challenges through four key aspects: computational constraints, architectural innovations, positional encoding strategies, and domain-specific adaptations.  \n\n#### Computational and Memory Constraints  \nThe quadratic complexity of self-attention with respect to sequence length becomes prohibitive for high-resolution 3D medical images. For example, processing a 256×256×256 MRI volume with full self-attention would require impractical computational resources [155]. To address this, methods like shifted window attention in Swin Transformers and axial attention in [42] trade global context for efficiency. However, such compromises can degrade performance in tasks requiring extensive spatial reasoning, such as segmenting diffuse tumor boundaries or vascular networks.  \n\n#### Hierarchical and Multi-Scale Architectures  \nInspired by the U-Net's success, recent works integrate transformers into hierarchical designs to balance local and global processing. Models like [63] use transformer blocks at multiple scales, where deeper layers aggregate broader context while shallower layers refine details—mirroring the feature hierarchy analysis discussed in Section 6.3. Hybrid approaches, such as [58], combine transformer encoders with convolutional decoders, but face alignment challenges when fusing cross-scale skip connections. These designs echo the interpretability-accuracy trade-offs highlighted earlier, as their complexity can obscure feature attribution.  \n\n#### Recurrent and Memory-Augmented Designs  \nTo enhance long-range modeling without excessive computation, recurrent mechanisms iteratively refine features. For instance, [59] employs cascaded attention decoders that progressively integrate multi-scale context, akin to human perceptual refinement. Similarly, [98] uses recurrent attention to propagate information across resolutions. While effective, these methods risk over-smoothing—a challenge paralleling the attention-output misalignment issues noted in Section 6.3.  \n\n#### Positional Encoding and Spatial Adaptation  \nTransformers' lack of inherent spatial bias necessitates explicit positional encoding, which often struggles with multi-resolution inputs. Innovations like hybrid embeddings in [57] and deformable attention in [76] dynamically adapt to structural variations. These advances align with the domain-specific interpretability needs emphasized earlier, particularly in medical imaging where anatomical relationships vary across scales.  \n\n#### Domain-Specific Challenges and Future Directions  \nMedical segmentation tasks—from whole-brain analysis [63] to abdominal organ delineation [94]—demand adaptive handling of multi-scale structures. Future work should:  \n1. **Optimize Dynamic Attention**: Techniques like adaptive spans in [67] could balance local-global trade-offs.  \n2. **Leverage Multi-Modal Data**: Cross-modal fusion, as in [65], may enhance context modeling.  \n3. **Improve Scalability**: Lightweight designs could preserve global context while mitigating computational bottlenecks.  \n\nThese directions bridge to Section 6.5's focus on domain adaptation, as robust multi-scale modeling is foundational for generalizing across diverse medical datasets. While transformers have transformed segmentation through global context, overcoming their limitations in long-range and multi-scale processing remains critical for clinical applicability. Innovations must harmonize computational efficiency, architectural transparency, and domain-aware adaptability to realize their full potential.  \n---\n\n### 6.5 Open Problems in Domain Adaptation and Bias\n\n### 6.5 Open Problems in Domain Adaptation and Bias  \n\nWhile transformer-based segmentation models excel at capturing global context (as discussed in Section 6.4), they face significant challenges in domain adaptation and bias mitigation that hinder their real-world deployment. These issues stem from inherent limitations in generalization, annotation inconsistencies, and systemic biases in training data, which we analyze below through three key problem areas.  \n\n#### Domain Shift and Generalization  \nThe ability to handle domain shift - where models underperform due to distributional differences between training and test data - remains a critical limitation. In medical imaging, variations in imaging protocols, scanner types, or patient populations create substantial domain gaps. [129] demonstrates how transformer-based models struggle to maintain performance across diverse medical datasets without extensive fine-tuning. While deformable attention mechanisms in [104] improve local feature adaptability, they still fail to address large-scale domain shifts like transitioning between imaging modalities.  \n\nCurrent domain adaptation approaches often rely on adversarial training or feature alignment, which may not fully leverage transformers' global context modeling capabilities. For instance, [103] shows axial attention's effectiveness for long-range dependencies but reveals its inability to dynamically adapt to unseen domains. Hybrid architectures combining self-attention with domain-invariant representations, as suggested by [180], present a promising direction by integrating convolutional inductive biases to stabilize cross-domain feature extraction.  \n\n#### Inter-Rater Variability in Annotations  \nAnnotation inconsistencies introduce significant noise in training labels, particularly in medical segmentation where ambiguous boundaries (e.g., tumor margins) lead to subjective interpretations. [107] attempts to address this through ghost heads for increased channel capacity, but doesn't explicitly handle label noise. Similarly, [109] uses global context to mitigate local annotation errors but assumes uniform label quality across datasets.  \n\nContrary to claims that self-attention is inherently robust to label noise, [181] demonstrates how dense attention maps can amplify annotation errors by propagating incorrect spatial dependencies. Emerging solutions like cluster-based attention in [182] suggest incorporating uncertainty estimation to focus on high-confidence regions, potentially filtering noisy labels more effectively.  \n\n#### Biases in Model Predictions  \nTransformer-based segmentation faces two fundamental bias types: *data bias* from uneven class/demographic representation, and *architectural bias* from attention mechanisms' systematic errors. In autonomous driving datasets, [114] shows how self-attention overfits to dominant classes, degrading performance for rare objects. Architectural bias manifests in [183], where sparse attention prioritizes high-frequency features at the expense of textures critical for material segmentation.  \n\nCurrent mitigation strategies have limitations: class-frequency-based reweighting in [184] struggles with multi-label imbalance, while geometric constraints in [112] don't fully address bias-domain shift interactions, as noted in [132].  \n\n#### Future Directions  \nBuilding on the hierarchical and multi-scale innovations discussed in Section 6.4, future research should:  \n1. Develop cross-domain attention mechanisms that explicitly model domain-invariant features, inspired by [115].  \n2. Integrate annotation-aware training using inter-rater agreement metrics, following [185].  \n3. Combine debiasing techniques from [186] with dynamic attention for fairer predictions.  \n4. Explore self-supervised pretraining approaches like [187] for unsupervised domain adaptation.  \n\nIn conclusion, while transformers have advanced segmentation through global context modeling, persistent challenges in domain adaptation and bias must be addressed through innovations in attention mechanisms, training paradigms, and architectural designs. A holistic framework that simultaneously tackles data, annotation, and architectural biases will be crucial for reliable real-world deployment.\n\n## 7 Comparative Analysis and Benchmarks\n\n### 7.1 Benchmark Datasets and Evaluation Protocols\n\n---\n### 7.1 Benchmark Datasets and Evaluation Protocols  \n\nThe advancement of transformer-based visual segmentation models heavily depends on standardized benchmarks and rigorous evaluation protocols. These components serve as critical tools for comparing model performance across different architectures and methodologies. This subsection systematically examines the most influential datasets and evaluation metrics in the field, while highlighting their unique challenges and applications.  \n\n#### **Standard Benchmark Datasets**  \n\n1. **COCO (Common Objects in Context)**  \n   As one of the most comprehensive benchmarks, COCO provides over 330K images with 1.5 million object instances across 80 categories, supporting semantic, instance, and panoptic segmentation tasks. Its diversity in object scales and complex scenes makes it indispensable for evaluating transformer-based models. For example, [78] and [79] have achieved state-of-the-art performance on COCO by leveraging its rich annotations. The dataset's panoptic segmentation task, which unifies semantic and instance predictions, has been addressed by works like [1] and [11].  \n\n2. **Cityscapes**  \n   Focused on urban scene understanding, Cityscapes offers 5,000 finely annotated and 20,000 coarsely annotated high-resolution images. Its emphasis on precise boundary delineation for autonomous driving applications has made it a key benchmark for transformer-based models. Approaches such as [119] and [3] have demonstrated success on Cityscapes by addressing its fine-grained requirements. The dataset's real-time performance focus aligns with the efficiency goals of many transformer architectures, as discussed in [188].  \n\n3. **ADE20K**  \n   With 25K images annotated across 150 object and stuff categories, ADE20K is a large-scale dataset for scene parsing. Its dense annotations and diverse scenes make it ideal for evaluating semantic segmentation models. Transformer-based methods like [78] and [189] have utilized ADE20K to showcase their ability to handle complex, multi-scale object distributions. The dataset's hierarchical labeling also supports multi-task learning research, as explored in [6].  \n\n4. **SemanticKITTI and nuScenes**  \n   These datasets are pivotal for LiDAR-based panoptic segmentation, a critical task for autonomous navigation. SemanticKITTI provides sequential LiDAR scans with point-wise annotations, while nuScenes offers multi-modal data (LiDAR, radar, and cameras). Transformer models like [81] and [80] have been benchmarked on these datasets to evaluate their robustness in 3D environments. The challenges of sparse, unstructured point clouds are highlighted in [8].  \n\n5. **Medical Imaging Datasets (e.g., BRATS18, ROBOT18)**  \n   Medical segmentation datasets, such as BRATS18 (brain tumor segmentation) and ROBOT18 (robotic scene segmentation), present unique challenges due to their small-scale, high-dimensional data. Transformer-based approaches like [6] and [5] have demonstrated promise in this domain by addressing CNN limitations in volumetric data processing.  \n\n#### **Evaluation Protocols**  \n\n1. **Mean Intersection over Union (mIoU)**  \n   The standard metric for semantic segmentation, mIoU measures the overlap between predicted and ground-truth masks. While widely used in datasets like Cityscapes and ADE20K, its limitations in capturing boundary accuracy and small-object performance are noted in [190].  \n\n2. **Panoptic Quality (PQ)**  \n   Introduced in [1], PQ unifies the evaluation of \"stuff\" (amorphous regions) and \"things\" (countable objects) by combining recognition quality (RQ) and segmentation quality (SQ). Transformer-based methods like [21] and [78] have achieved top PQ scores on COCO and Cityscapes.  \n\n3. **Average Precision (AP) and AP50**  \n   AP is the primary metric for instance segmentation, measuring precision-recall trade-offs at varying IoU thresholds. AP50, which uses a 50% IoU threshold, is commonly reported in COCO benchmarks. Methods like [7] and [191] optimize for AP by refining instance mask predictions.  \n\n4. **Pixel Accuracy (PA) and Frequency-Weighted IoU (FWIoU)**  \n   PA measures the percentage of correctly classified pixels, while FWIoU weights IoU by class frequency. These metrics are particularly useful for class-imbalanced datasets like ADE20K, as discussed in [5].  \n\n5. **Domain-Specific Metrics**  \n   Medical segmentation often employs Dice Score and Hausdorff Distance to evaluate volumetric overlap and boundary adherence. Autonomous driving benchmarks like SemanticKITTI use metrics such as IoU per class and panoptic tracking accuracy, as highlighted in [12].  \n\n#### **Challenges and Future Directions**  \nDespite the richness of existing benchmarks, several challenges persist:  \n- **Domain Shift**: Models trained on one dataset (e.g., Cityscapes) often generalize poorly to others (e.g., UAVid [20]). Cross-domain adaptation techniques, as explored in [149], are critical for real-world deployment.  \n- **Real-Time Constraints**: While datasets like Cityscapes emphasize real-time performance, many transformer models struggle with latency, as noted in [28].  \n- **Annotation Efficiency**: Large-scale datasets require costly annotations. Weakly supervised methods, such as those in [25], aim to mitigate this burden.  \n\nFuture benchmarks may need to incorporate multimodal data (e.g., RGB-Depth in [10]) and dynamic scenes (e.g., video segmentation in [192]). Evaluation protocols will likely evolve to prioritize robustness, as seen in [193], and fairness across object scales.  \n\nIn summary, benchmark datasets and evaluation protocols form the foundation for advancing transformer-based segmentation. Addressing their current limitations will enable the development of more generalizable and efficient models.  \n\n---\n\n### 7.2 Performance Metrics and Analysis\n\n### 7.2 Performance Metrics and Analysis  \n\nThe evaluation of transformer-based visual segmentation models relies on carefully designed metrics that capture different aspects of segmentation quality, from pixel-level accuracy to object-level consistency. Building upon the benchmark datasets discussed in Section 7.1, this subsection provides a detailed analysis of key performance metrics—including mean Intersection over Union (mIoU), Average Precision (AP), and Panoptic Quality (PQ)—and their role in assessing model capabilities across semantic, instance, and panoptic segmentation tasks.  \n\n#### **Mean Intersection over Union (mIoU)**  \nAs the standard metric for semantic segmentation, mIoU quantifies the overlap between predicted and ground-truth masks by averaging the IoU across all classes. The IoU for a single class is defined as:  \n\n\\[\n\\text{IoU} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP} + \\text{FN}}\n\\]\n\nwhere TP (True Positives), FP (False Positives), and FN (False Negatives) represent correctly predicted, over-segmented, and under-segmented pixels, respectively. mIoU is particularly effective for evaluating boundary precision, making it a preferred choice for models like SETR [31] and TransDeepLab [29], which prioritize accurate mask generation. For example, SETR achieves 50.28% mIoU on ADE20K, demonstrating its strength in capturing global context [31]. However, mIoU's class-agnostic averaging can obscure performance disparities in imbalanced datasets, a limitation addressed by frequency-weighted variants.  \n\n#### **Average Precision (AP)**  \nAP serves as the primary metric for instance segmentation, measuring precision-recall trade-offs across multiple IoU thresholds (e.g., AP@50 for IoU=0.5). It is indispensable for evaluating object-level segmentation accuracy in models like UniFormer [35] and Dilated Neighborhood Attention Transformer (DiNAT) [32]. AP's multi-threshold design ensures robustness to localization errors, as evidenced by DiNAT's 53.8 box AP and 46.4 mask AP on COCO [32]. While computationally intensive, recent optimizations in architectures like UniFormer [35] demonstrate that AP can be maintained without sacrificing inference speed.  \n\n#### **Panoptic Quality (PQ)**  \nPQ unifies the evaluation of semantic and instance segmentation by combining recognition and segmentation quality:  \n\n\\[\n\\text{PQ} = \\frac{\\sum_{(p,g) \\in \\text{TP}} \\text{IoU}(p,g)}{|\\text{TP}| + \\frac{1}{2}|\\text{FP}| + \\frac{1}{2}|\\text{FN}|}\n\\]\n\nHere, \\(p\\) and \\(g\\) denote predicted and ground-truth segments. PQ's strict penalization of misclassifications and fragmentation makes it ideal for panoptic segmentation, where DiNAT achieves 58.5 PQ on COCO [32]. Its holistic nature is particularly valuable in medical imaging, as shown by TransBTSV2 [30], which uses PQ to assess 3D anatomical structure segmentation.  \n\n#### **Domain-Specific Adaptations**  \n1. **Medical Imaging**: The Dice Score (F1) complements mIoU in scenarios like brain tumor segmentation (BraTS), where TransBTSV2 attains 62.9% mIoU [30].  \n2. **Autonomous Driving**: Metrics like Fréchet Inception Distance (FID) evaluate LiDAR segmentation quality in models such as LiDARFormer [31].  \n\n#### **Challenges and Trade-offs**  \n- **Scalability vs. Accuracy**: Larger models like GC ViT [40] achieve higher mIoU (85.7% on ImageNet) but incur significant computational overhead.  \n- **Robustness**: Transformer-based models remain vulnerable to adversarial attacks, with perturbations causing mIoU drops of 10–15% [33].  \n- **Hybrid Architectures**: Models like ViT-CoMer [36] integrate CNN inductive biases to balance mIoU and AP, achieving 64.3% AP on COCO.  \n\n#### **Benchmark Trends and Innovations**  \nRecent benchmarks highlight transformer dominance in context modeling but reveal efficiency gaps. For instance, Next-ViT [162] reduces latency by 3.6× compared to Swin Transformer while maintaining 46.5% mIoU on ADE20K. Similarly, DilateFormer [41] employs dilated attention to achieve 85.6% top-1 accuracy on ImageNet, illustrating how architectural innovations can mitigate performance trade-offs.  \n\n#### **Emerging Metrics and Future Directions**  \nNew evaluation paradigms are emerging to address interpretability and cross-domain generalization:  \n- **Attention Map Consistency**: Proposed in [121], this metric assesses model interpretability through attention alignment.  \n- **Cross-Domain mIoU**: Explored in [34], it measures transferability, with hybrid ViTs achieving 82.3% top-1 accuracy on ImageNet-1K.  \n\nIn summary, mIoU, AP, and PQ provide complementary perspectives on segmentation performance, while domain-specific adaptations and novel metrics address evolving challenges. These insights set the stage for the comparative analysis of architectures in Section 7.3, where trade-offs between accuracy, efficiency, and robustness are further explored.\n\n### 7.3 Comparative Results Across Architectures\n\n### 7.3 Comparative Results Across Architectures  \n\nThe rapid evolution of transformer-based architectures for visual segmentation has led to a diverse landscape of models, each with unique design choices and performance trade-offs. Building upon the metrics discussed in Section 7.2, this subsection provides a quantitative comparison of state-of-the-art models across benchmark datasets, analyzing their accuracy, efficiency, and domain-specific adaptability.  \n\n#### **Semantic Segmentation: Balancing Context and Efficiency**  \nTransformer-based models have set new benchmarks on semantic segmentation datasets like Cityscapes, ADE20K, and COCO. Hierarchical architectures dominate this space, with [71] and [32] achieving 56.7% and 58.1% mIoU on ADE20K, respectively, by combining local and global attention. The latter reduces computational overhead while outperforming Swin Transformer by 1.4%, demonstrating the impact of sparse global attention. Hybrid designs like [92] further enhance multi-scale feature fusion, reaching 82.0% mIoU on Cityscapes—surpassing convolutional baselines like ConvNeXt.  \n\nFor real-time applications, efficiency-focused models such as [46] integrate axial attention with pyramid pooling, achieving 80.5% mIoU on Camvid at 51.3 FPS. Lightweight variants like [49] reduce parameters by 5× compared to Swin Transformer while maintaining competitive accuracy (76.0% mIoU on S3DIS), highlighting trade-offs between speed and precision.  \n\n#### **Medical Imaging: Precision with Limited Data**  \nIn medical segmentation, transformers excel at capturing fine-grained anatomical structures but face challenges in data efficiency. [29] replaces atrous convolution with Swin-Transformer blocks, achieving 69.2% mIoU on BraTS’19 with fewer parameters. Meanwhile, [63] segments 133 brain structures in a single network, outperforming ensemble methods like SLANT27. These advances underscore transformers’ potential in heterogeneous medical datasets, though their reliance on large-scale pretraining remains a limitation, as noted in [68].  \n\n#### **3D and Point Cloud Segmentation: Spatial Challenges**  \nFor 3D tasks, innovations like [194] achieve 76.0% mIoU on S3DIS with 200× fewer parameters than sparse CNNs. Efficiency optimizations are critical here; [127] delivers a 4.6× speedup over SST on Waymo Open Dataset by streamlining window attention. These results reveal the tension between spatial resolution and computational cost in 3D segmentation.  \n\n#### **Robustness and Generalization Gaps**  \nWhile transformers excel in controlled benchmarks, their real-world reliability varies. [50] shows that models like [32] improve robustness to distribution shifts but struggle with calibration and out-of-distribution detection compared to CNNs. Hybrid approaches like [35] address this by integrating convolutional biases, achieving 84.8% accuracy on Kinetics-600 with consistent cross-domain performance.  \n\n#### **Key Trade-offs and Future Directions**  \nThe comparative analysis reveals three critical trade-offs:  \n1. **Accuracy vs. Speed**: High-accuracy models (e.g., [48]) often sacrifice efficiency, while lightweight designs (e.g., [51]) reduce FLOPs by 52% but may compromise precision.  \n2. **Scalability**: Hierarchical designs ([71], [32]) scale better to high-resolution inputs than vanilla transformers.  \n3. **Data Dependence**: Medical models ([195], [42]) generalize well with limited data, whereas general-purpose architectures ([44]) require extensive pretraining.  \n\nFuture work must address robustness gaps and hardware-aware optimization, as emphasized in [93]. Bridging these gaps will be essential for deploying transformers in domain-specific applications, a theme explored further in Section 7.4.  \n\nIn summary, transformer-based segmentation models have achieved remarkable progress, but their adoption requires careful consideration of task-specific needs, balancing accuracy, efficiency, and adaptability.\n\n### 7.4 Domain-Specific Benchmarking\n\n### 7.4 Domain-Specific Benchmarking  \n\nThe performance of transformer-based segmentation models varies significantly across specialized domains due to differences in data characteristics, annotation complexity, and task-specific challenges. Building on the comparative analysis in Section 7.3, this subsection evaluates model performance in key domains such as medical imaging and autonomous driving, while also examining their cross-domain adaptability and limitations.  \n\n#### **Medical Imaging: Precision and Data Efficiency**  \nMedical image segmentation presents unique challenges, including high-resolution volumetric data, limited annotated datasets, and heterogeneous tissue structures. Transformer-based models have shown remarkable success in this domain by leveraging their ability to capture long-range dependencies and multi-scale features. For instance, [58] combines CNNs with transformers for organ segmentation, achieving state-of-the-art results on multi-organ and cardiac datasets. Its hierarchical design enables precise localization of fine-grained structures, addressing a critical need in clinical applications.  \n\nFurther advancements include [63], which segments 133 brain structures in a single network by hierarchically aggregating spatially adjacent patches. This approach outperforms ensemble-based methods like SLANT27, demonstrating scalability for complex anatomical tasks. Similarly, [155] introduces fusion-head self-attention to model long-range dependencies in 3D MRI volumes, surpassing convolutional baselines on the BraTS dataset.  \n\nHowever, medical imaging exposes key limitations in transformer-based models, particularly in data efficiency. [68] reveals that fine-tuning large pre-trained transformers on small medical datasets is resource-intensive. Low-rank adaptation (LoRA) mitigates this by reducing trainable parameters by 99.83%, offering a lightweight solution for clinical deployment.  \n\n#### **Autonomous Driving: Real-Time Processing Challenges**  \nIn autonomous driving, transformer-based models excel at LiDAR point cloud and panoptic segmentation tasks. [66] highlights their superiority in detecting small objects like pedestrians and traffic signs, where global context is essential. However, real-time processing remains a challenge. [67] proposes adaptive window partitioning to reduce computational overhead without sacrificing accuracy, a strategy applicable to driving scenarios.  \n\n#### **Cross-Domain Adaptability and Limitations**  \nThe ability of transformer-based models to generalize across domains is a key area of investigation. [156] evaluates unified architectures on diverse modalities (e.g., CT, MRI, wearable sensors) and reveals that no single model excels universally. For instance, [65] performs robustly in multi-modal tumor classification but struggles with sensor data due to its visual feature bias.  \n\nHybrid architectures like [61] bridge this gap by integrating convolutional inductive biases with transformer-based global modeling, achieving state-of-the-art results across medical and natural image datasets. Similarly, [42] introduces non-negative matrix factorization (NMF) as a lightweight alternative to self-attention, enabling efficient cross-domain feature extraction.  \n\n#### **Challenges and Future Directions**  \nDespite their success, transformer-based models face domain-specific challenges:  \n1. **Data Scarcity**: Medical imaging often lacks large-scale annotated datasets. Self-supervised pre-training, as explored in [74], offers a promising solution by leveraging synthetic or unlabeled data.  \n2. **Computational Complexity**: Real-time applications demand efficient architectures. [60] identifies ViT as Pareto-optimal, but hybrid models like [64] reduce parameters by 16–27× while maintaining performance.  \n3. **Interpretability**: Clinicians require transparent decision-making. Prototype-based attention mechanisms, as in [72], improve trust by highlighting clinically relevant regions.  \n\nFuture research should focus on:  \n- **Multimodal Fusion**: Integrating diverse data sources (e.g., MRI + genomics) as proposed in [75].  \n- **Domain Adaptation**: Techniques like adversarial training, explored in [196], could enhance cross-domain robustness.  \n\nIn summary, domain-specific benchmarking reveals that transformer-based models excel in specialized tasks but require tailored adaptations to address unique challenges. Their cross-domain potential remains underexplored, offering fertile ground for future innovation, as further discussed in Section 7.5.\n\n## 8 Future Directions and Conclusion\n\n### 8.1 Emerging Trends in Transformer-Based Segmentation\n\n### 8.1 Emerging Trends in Transformer-Based Segmentation  \n\nThe rapid evolution of transformer-based segmentation models has introduced transformative trends that address key challenges in scalability, generalization, and efficiency. These advancements—multimodal fusion, self-supervised learning, and lightweight architectures—are reshaping visual understanding and enabling new real-world applications. Below, we explore these trends in detail, highlighting their technical innovations and future potential.  \n\n#### 1. Multimodal Fusion for Enhanced Scene Understanding  \nA significant trend is the integration of multimodal data to improve segmentation accuracy and robustness. While traditional models often rely on RGB inputs alone, combining complementary modalities—such as LiDAR, depth, and thermal data—allows transformers to capture richer spatial and contextual information. For example, [10] demonstrates how RGB and sparse depth maps can be fused to simultaneously predict dense depth and panoptic segmentation, achieving superior performance in autonomous driving. Similarly, [81] leverages LiDAR point clouds to resolve ambiguities in complex urban scenes, showcasing the power of cross-modal feature aggregation.  \n\nTemporal data fusion is another promising direction, where motion cues from video sequences refine object boundaries. [26] illustrates how optical flow can align features across frames, particularly in dynamic environments. This approach is formalized in [192], which emphasizes the need for models that exploit temporal consistency. Future research could explore adaptive fusion mechanisms, as seen in [149], where 2D and 3D branches are dynamically weighted to mitigate domain shifts.  \n\n#### 2. Self-Supervised and Weakly-Supervised Learning  \nThe dependency on large-scale annotated datasets remains a major bottleneck. Self-supervised learning (SSL) has emerged as a viable solution, enabling models to pretrain on unlabeled data with minimal supervision. [23] pioneers embedding-based SSL for panoptic segmentation, using synthetic-to-real domain adaptation to generate pseudo-labels. This approach is further validated by [7], where weak supervision from instance contours achieves competitive performance.  \n\nWeakly-supervised methods also reduce annotation burdens. For instance, [25] trains models using only class proportion priors, matching the accuracy of fully supervised approaches. Similarly, [14] adapts panoptic segmentation for assistive systems with limited supervision. Innovations in pretext tasks, such as contrastive learning in [197], further enhance model generalization. Future work could unify SSL paradigms across tasks, as suggested by [198], which trains hierarchical classifiers on heterogeneous datasets.  \n\n#### 3. Lightweight Architectures for Efficient Deployment  \nThe computational demands of transformer-based models often hinder real-time deployment. Lightweight architectures address this challenge through efficient attention mechanisms, model compression, and hardware-aware designs. [11] introduces a shared backbone with multi-scale feature fusion, enabling real-time performance on mobile GPUs. Similarly, [119] replaces proposal-based detection with position-sensitive embeddings, reducing latency by 40%.  \n\nToken pruning and merging, as explored in [27], dynamically eliminate redundant tokens to reduce FLOPs. [148] combines depthwise separable convolutions with atrous spatial pyramid pooling for speed-precision balance. Hardware optimizations like quantization, discussed in [28], further enhance efficiency. Emerging directions include neural architecture search (NAS), as hinted by [80], which optimizes attention-based lateral connections for panoramic imagery.  \n\n#### 4. Synergies and Future Challenges  \nThese trends often intersect, yielding groundbreaking advancements. For example, [78] unifies multimodal fusion and lightweight design through masked attention, enabling panoptic, instance, and semantic segmentation in a single model. Meanwhile, [120] combines self-supervised prompts with continual learning for efficient adaptation to new classes.  \n\nHowever, challenges remain, such as balancing modality fusion complexity with real-time performance or scaling SSL to ultra-high-resolution imagery. Solutions may lie in dynamic computation, as proposed in [22], and cross-domain generalization, highlighted in [12]. Drawing insights from [9] and [5], future research must prioritize adaptability, efficiency, and robustness to fully realize the potential of transformer-based segmentation in real-world applications.\n\n### 8.2 Multimodal and Cross-Domain Adaptation\n\n### 8.2 Multimodal and Cross-Domain Adaptation  \n\nBuilding on the emerging trends discussed in Section 8.1, multimodal and cross-domain adaptation has become a critical frontier in transformer-based visual segmentation. These approaches address key challenges in generalization and robustness by leveraging complementary data sources and mitigating distribution shifts across domains.  \n\n#### 1. Multimodal Segmentation: Synergizing Heterogeneous Data  \nTransformers have demonstrated remarkable capabilities in fusing diverse data modalities to enhance segmentation accuracy. In medical imaging, for instance, [88] shows how combining structural MRI (sMRI) with functional network connectivity (sFNC) data improves feature learning, even with limited training samples. Similarly, [30] highlights the effectiveness of hybrid CNN-Transformer architectures for 3D medical image segmentation, where multiple MRI sequences (e.g., T1, T2) are processed to capture both local and global dependencies.  \n\nBeyond healthcare, autonomous driving systems benefit from multimodal fusion of LiDAR and RGB data. Transformers excel at aligning spatial and semantic features across these modalities, addressing challenges like occlusion and scale variation. This capability is further exemplified by approaches that unify point cloud and image representations for robust panoptic segmentation in dynamic environments.  \n\n#### 2. Cross-Domain Adaptation: Bridging Distribution Gaps  \nThe ability to adapt to domain shifts is crucial for real-world deployment of segmentation models. Transformers offer unique advantages here due to their capacity for modeling long-range dependencies. [199] introduces multi-focal attention to adaptively adjust receptive fields, improving generalization on small datasets with domain shifts. This addresses the data-hungry nature of vision transformers by enforcing locality constraints during training.  \n\nSelf-supervised domain adaptation has also emerged as a promising direction. [124] leverages masked image modeling (MIM) to pretrain models on large-scale datasets, enabling robust feature learning that transfers across domains. Similarly, [34] demonstrates how transferring CNN inductive biases to transformers can facilitate effective adaptation without extensive pretraining.  \n\n#### 3. Challenges and Architectural Innovations  \nDespite progress, several challenges persist:  \n- **Modality Alignment**: Heterogeneous data often exhibit spatial or temporal misalignment. [200] tackles this by unifying convolutional and self-attention mechanisms for consistent feature extraction.  \n- **Domain Shift**: Severe distribution gaps (e.g., synthetic to real data) remain problematic. [139] proposes efficient attention mechanisms to maintain cross-domain performance while reducing computational overhead.  \n- **Scalability**: Multimodal transformers often face high computational demands. [32] addresses this through dilated attention for efficient multi-scale context modeling.  \n\nArchitectural innovations continue to push boundaries. [160] introduces a unified framework for cross-modal context aggregation, while [87] enhances local feature extraction by integrating convolutional layers into transformer blocks. These hybrid designs exemplify the synergy between CNNs and transformers for multimodal tasks.  \n\n#### 4. Future Directions  \nLooking ahead to the research directions outlined in Section 8.3, several opportunities stand out:  \n- **Dynamic Fusion Mechanisms**: Adaptive gating approaches, as hinted in [125], could enable more flexible modality weighting.  \n- **Unsupervised Adaptation**: Extending adversarial training or contrastive learning to multimodal settings, building on works like [201].  \n- **Efficient Architectures**: Lightweight designs incorporating token pruning or low-rank approximations, as suggested by [162], will be crucial for real-time applications.  \n\nIn summary, multimodal and cross-domain adaptation represent transformative approaches that address fundamental limitations in visual segmentation. By combining the strengths of diverse data sources and overcoming domain shifts, these methods pave the way for more robust and generalizable systems. The field now stands poised to explore scalable and efficient frameworks that fully harness the potential of these techniques.\n\n### 8.3 Future Research Directions\n\n### 8.3 Future Research Directions  \n\nThe rapid evolution of transformer-based visual segmentation has unlocked remarkable capabilities, yet several open challenges and promising research directions remain. Building on the advancements in multimodal and cross-domain adaptation discussed in Section 8.2, we outline key areas for future exploration, emphasizing scalable foundation models, self-supervised pretraining, and other critical advancements that will shape the next generation of segmentation systems.  \n\n#### 1. **Scalable Foundation Models for Segmentation**  \nCurrent transformer-based segmentation models often rely on task-specific architectures, limiting their generalization across diverse domains. The development of scalable foundation models—pre-trained on large-scale datasets and adaptable to downstream tasks—is a pressing need. For instance, [27] highlights the potential of unified architectures like [35], which integrate convolutional inductive biases with global attention. Future work could extend such frameworks to support multi-modal inputs (e.g., RGB-D, LiDAR) and dynamic resolution scaling, as seen in [47].  \n\nA critical challenge lies in balancing model capacity with computational efficiency. While [51] introduces token pruning for efficiency, further research is needed to optimize memory usage during inference, particularly for high-resolution 3D data [90]. Hybrid architectures, such as those combining sparse CNNs with transformers [202], could bridge this gap by leveraging the strengths of both paradigms.  \n\n#### 2. **Self-Supervised and Weakly Supervised Pretraining**  \nThe success of self-supervised pretraining in natural language processing (e.g., BERT) has yet to be fully realized in visual segmentation. Current methods like [45] rely on supervised pretraining, which demands extensive labeled data. Future directions should explore contrastive learning, masked autoencoding, and other self-supervised paradigms to reduce annotation costs. For example, [42] demonstrates the potential of low-rank factorization for unsupervised feature learning, but its applicability to broader segmentation tasks remains untested.  \n\nWeakly supervised learning, leveraging sparse annotations (e.g., bounding boxes, points), is another promising avenue. [177] proposes interactive transformers for refinement, but extending this to large-scale pretraining could democratize access to segmentation tools for resource-constrained domains.  \n\n#### 3. **Robustness and Generalization**  \nTransformer-based segmentation models often struggle with domain shifts and adversarial attacks [50]. Future research should focus on:  \n- **Domain Adaptation**: Techniques like [203] could be adapted to align feature distributions across modalities (e.g., MRI to CT).  \n- **Adversarial Robustness**: [91] introduces adversarial training for medical images, but broader solutions for autonomous driving and robotics are needed.  \n- **Long-Tail Learning**: Addressing class imbalance in datasets like COCO or ADE20K requires dynamic sampling or loss reweighting strategies, as hinted in [48].  \n\n#### 4. **Efficiency and Real-Time Deployment**  \nDespite their accuracy, transformers face latency bottlenecks in real-time applications (e.g., autonomous driving). [37] and [46] propose lightweight designs, but further optimizations are needed:  \n- **Hardware-Aware Co-Design**: Collaborations with hardware platforms, as explored in [54], could yield specialized accelerators for attention operations.  \n- **Dynamic Computation**: Methods like [163] could be adapted to dynamically allocate compute resources based on input complexity.  \n\n#### 5. **Interpretability and Trustworthiness**  \nThe \"black-box\" nature of transformers limits their adoption in safety-critical applications. Future work should:  \n- **Visualize Attention Mechanisms**: Building on [44], tools for interpreting cross-scale attention could enhance transparency.  \n- **Uncertainty Quantification**: [204] introduces uncertainty estimation for point clouds, but similar approaches are needed for pixel-level segmentation.  \n\n#### 6. **Multimodal and Cross-Domain Fusion**  \nTransformers excel at fusing heterogeneous data, yet challenges persist in aligning spatial and temporal contexts. For example:  \n- **Video Segmentation**: [90] could benefit from memory-augmented transformers.  \n- **3D-2D Alignment**: [205] calls for unified architectures to process LiDAR and camera data seamlessly.  \n\n#### 7. **Ethical and Environmental Considerations**  \nThe carbon footprint of training large transformers (e.g., [55]) necessitates greener alternatives:  \n- **Data-Efficient Training**: Leveraging synthetic data or federated learning, as suggested by [164].  \n- **Model Compression**: Techniques like [49] must be scaled to foundation models.  \n\n### Conclusion  \nThe future of transformer-based visual segmentation lies in addressing these interdisciplinary challenges. By advancing scalable architectures, self-supervised learning, and robustness, the field can unlock transformative applications in healthcare, robotics, and beyond. Collaborative efforts across academia and industry will be pivotal in realizing this vision, as highlighted in the concluding remarks of this survey.\n\n### 8.4 Conclusion\n\n### 8.4 Conclusion and Future Outlook  \n\nThe rapid evolution of transformer-based architectures has undeniably revolutionized the field of visual segmentation, bridging gaps in medical imaging, autonomous systems, and beyond. This survey has systematically traversed the transformative potential of transformers—from their foundational principles to state-of-the-art implementations, efficiency optimizations, and diverse applications. As we reflect on these advancements, it is clear that transformers have not only addressed longstanding limitations of convolutional neural networks (CNNs) but also introduced new paradigms for capturing long-range dependencies and global context.  \n\n#### Key Insights and Advancements  \n1. **Overcoming CNN Limitations**: Transformers excel where CNNs falter, particularly in modeling relationships across distant regions. Their self-attention mechanisms enable precise segmentation of intricate structures, such as anatomical boundaries in medical images [58], and have proven effective in brain tumor segmentation [155] and multi-organ analysis [63].  \n2. **Architectural Versatility**: Hybrid designs (e.g., TransUNet [58], DA-TransUNet [57]) and hierarchical transformers (e.g., Swin Transformer [60]) have emerged as dominant paradigms, combining the locality of CNNs with the global reasoning of transformers.  \n3. **Efficiency and Real-Time Deployment**: Innovations in token pruning [60], sparse attention, and lightweight architectures (e.g., BATFormer [67]) have mitigated computational bottlenecks, enabling deployment in resource-constrained settings.  \n4. **Interpretability and Robustness**: Enhanced transparency through attention visualization [44] and robustness via adversarial training [196] are critical for clinical adoption.  \n\n#### Persistent Challenges and Future Directions  \nWhile transformers have set new benchmarks, several challenges demand attention:  \n- **Data Efficiency**: Reducing reliance on large annotated datasets through self-supervised learning [68] and multimodal fusion [75].  \n- **Scalability and Deployment**: Further optimizations via quantization, hardware-aware co-design [54], and dynamic computation [163].  \n- **Ethical and Societal Impact**: Addressing bias, equitable access [206], and environmental sustainability [55].  \n\n#### Final Remarks  \nThe journey of transformer-based segmentation is just beginning. By building on foundational works like TransUNet [58] and Swin Transformer [60], the community can unlock transformative applications in healthcare, robotics, and smart cities. Collaborative innovation across academia and industry will be pivotal in realizing this vision, ensuring that these technologies are both cutting-edge and socially responsible. The possibilities are boundless, and the next decade promises even greater breakthroughs.\n\n\n## References\n\n[1] Panoptic Segmentation\n\n[2] Panoptic Segmentation  A Review\n\n[3] Full-Resolution Residual Networks for Semantic Segmentation in Street  Scenes\n\n[4] Importance-Aware Semantic Segmentation with Efficient Pyramidal Context  Network for Navigational Assistant Systems\n\n[5] Deep Semantic Segmentation of Natural and Medical Images  A Review\n\n[6] Task Decomposition and Synchronization for Semantic Biomedical Image  Segmentation\n\n[7] Learning Panoptic Segmentation from Instance Contours\n\n[8] Towards accurate instance segmentation in large-scale LiDAR point clouds\n\n[9] A Comprehensive Review of Modern Object Segmentation Approaches\n\n[10] PanDepth  Joint Panoptic Segmentation and Depth Completion\n\n[11] EfficientPS  Efficient Panoptic Segmentation\n\n[12] Benchmarking the Robustness of Panoptic Segmentation for Automated  Driving\n\n[13] Semantic Scene Segmentation for Robotics Applications\n\n[14] Can we cover navigational perception needs of the visually impaired by  panoptic segmentation \n\n[15] Improving Semantic Segmentation via Decoupled Body and Edge Supervision\n\n[16] Multi Receptive Field Network for Semantic Segmentation\n\n[17] Deep Learning of Unified Region, Edge, and Contour Models for Automated  Image Segmentation\n\n[18] A Novel Upsampling and Context Convolution for Image Semantic  Segmentation\n\n[19] Multi-Class Segmentation from Aerial Views using Recursive Noise  Diffusion\n\n[20] UAVid  A Semantic Segmentation Dataset for UAV Imagery\n\n[21] Hierarchical Lovász Embeddings for Proposal-free Panoptic Segmentation\n\n[22] Multi-view Aggregation Network for Dichotomous Image Segmentation\n\n[23] Self-trained Panoptic Segmentation\n\n[24] Panoptic Segmentation using Synthetic and Real Data\n\n[25] Semantic Segmentation by Semantic Proportions\n\n[26] Optical Flow boosts Unsupervised Localization and Segmentation\n\n[27] Transformer-Based Visual Segmentation  A Survey\n\n[28] A Survey on Deep Learning Methods for Semantic Image Segmentation in  Real-Time\n\n[29] TransDeepLab  Convolution-Free Transformer-based DeepLab v3+ for Medical  Image Segmentation\n\n[30] TransBTSV2  Towards Better and More Efficient Volumetric Segmentation of  Medical Images\n\n[31] Vision Transformers  From Semantic Segmentation to Dense Prediction\n\n[32] Dilated Neighborhood Attention Transformer\n\n[33] Understanding Robustness of Transformers for Image Classification\n\n[34] Bootstrapping ViTs  Towards Liberating Vision Transformers from  Pre-training\n\n[35] UniFormer  Unifying Convolution and Self-attention for Visual  Recognition\n\n[36] ViT-CoMer  Vision Transformer with Convolutional Multi-scale Feature  Interaction for Dense Predictions\n\n[37] SideRT  A Real-time Pure Transformer Architecture for Single Image Depth  Estimation\n\n[38] InternImage  Exploring Large-Scale Vision Foundation Models with  Deformable Convolutions\n\n[39] Shift-ConvNets  Small Convolutional Kernel with Large Kernel Effects\n\n[40] Global Context Vision Transformers\n\n[41] DilateFormer  Multi-Scale Dilated Transformer for Visual Recognition\n\n[42] Factorizer  A Scalable Interpretable Approach to Context Modeling for  Medical Image Segmentation\n\n[43] Transformers in Vision  A Survey\n\n[44] DenseFormer  Enhancing Information Flow in Transformers via Depth  Weighted Averaging\n\n[45] nnFormer  Interleaved Transformer for Volumetric Segmentation\n\n[46] P2AT  Pyramid Pooling Axial Transformer for Real-time Semantic  Segmentation\n\n[47] PVT  Point-Voxel Transformer for Point Cloud Learning\n\n[48] kMaX-DeepLab  k-means Mask Transformer\n\n[49] SSformer  A Lightweight Transformer for Semantic Segmentation\n\n[50] Reliability in Semantic Segmentation  Are We on the Right Track \n\n[51] Progressive Token Length Scaling in Transformer Encoders for Efficient  Universal Segmentation\n\n[52] Reinforcement Learning as a Parsimonious Alternative to Prediction  Cascades  A Case Study on Image Segmentation\n\n[53] Multimodal Learning with Transformers  A Survey\n\n[54] A Heterogeneous Chiplet Architecture for Accelerating End-to-End  Transformer Models\n\n[55] Transformers in Medical Imaging  A Survey\n\n[56] Advances in Medical Image Analysis with Vision Transformers  A  Comprehensive Review\n\n[57] DA-TransUNet  Integrating Spatial and Channel Dual Attention with  Transformer U-Net for Medical Image Segmentation\n\n[58] TransUNet  Transformers Make Strong Encoders for Medical Image  Segmentation\n\n[59] 3D TransUNet  Advancing Medical Image Segmentation through Vision  Transformers\n\n[60] Which Transformer to Favor  A Comparative Analysis of Efficiency in  Vision Transformers\n\n[61] ACC-UNet  A Completely Convolutional UNet model for the 2020s\n\n[62] Multi-scale Hierarchical Vision Transformer with Cascaded Attention  Decoding for Medical Image Segmentation\n\n[63] UNesT  Local Spatial Representation Learning with Hierarchical  Transformer for Efficient Medical Segmentation\n\n[64] EfficientMorph  Parameter-Efficient Transformer-Based Architecture for  3D Image Registration\n\n[65] TransMed  Transformers Advance Multi-modal Medical Image Classification\n\n[66] Transformers in Small Object Detection  A Benchmark and Survey of  State-of-the-Art\n\n[67] BATFormer  Towards Boundary-Aware Lightweight Transformer for Efficient  Medical Image Segmentation\n\n[68] MeLo  Low-rank Adaptation is Better than Fine-tuning for Medical Image  Diagnosis\n\n[69] Deep is a Luxury We Don't Have\n\n[70] More for Less  Compact Convolutional Transformers Enable Robust Medical  Image Classification with Limited Data\n\n[71] Video Swin Transformer\n\n[72] Transformer-CNN Fused Architecture for Enhanced Skin Lesion Segmentation\n\n[73] Is attention all you need in medical image analysis  A review\n\n[74] Adapting Pre-trained Vision Transformers from 2D to 3D through Weight  Inflation Improves Medical Image Segmentation\n\n[75] Multimodal Machine Learning in Image-Based and Clinical Biomedicine   Survey and Prospects\n\n[76] Surface Vision Transformers  Flexible Attention-Based Modelling of  Biomedical Surfaces\n\n[77] Agent Attention  On the Integration of Softmax and Linear Attention\n\n[78] Masked-attention Mask Transformer for Universal Image Segmentation\n\n[79] OneFormer3D  One Transformer for Unified Point Cloud Segmentation\n\n[80] DS-PASS  Detail-Sensitive Panoramic Annular Semantic Segmentation  through SwaftNet for Surrounding Sensing\n\n[81] LiDAR-based Panoptic Segmentation via Dynamic Shifting Network\n\n[82] Rethinking Spatial Dimensions of Vision Transformers\n\n[83] Making Vision Transformers Truly Shift-Equivariant\n\n[84] CNN-based Local Vision Transformer for COVID-19 Diagnosis\n\n[85] Improving Vision Transformers by Revisiting High-frequency Components\n\n[86] HSViT  Horizontally Scalable Vision Transformer\n\n[87] Vision Conformer  Incorporating Convolutions into Vision Transformer  Layers\n\n[88] MultiCrossViT  Multimodal Vision Transformer for Schizophrenia  Prediction using Structural MRI and Functional Network Connectivity Data\n\n[89] Investigation of Network Architecture for Multimodal Head-and-Neck Tumor  Segmentation\n\n[90] Memory transformers for full context and high-resolution 3D Medical  Segmentation\n\n[91] Class-Aware Adversarial Transformers for Medical Image Segmentation\n\n[92] IncepFormer  Efficient Inception Transformer with Pyramid Pooling for  Semantic Segmentation\n\n[93] Transformer-based models and hardware acceleration analysis in  autonomous driving  A survey\n\n[94] Transformer Utilization in Medical Image Segmentation Networks\n\n[95] A Comprehensive Study of Vision Transformers in Image Classification  Tasks\n\n[96] A Recent Survey of Vision Transformers for Medical Image Segmentation\n\n[97] Transformers in Medical Image Analysis  A Review\n\n[98] MS-Twins  Multi-Scale Deep Self-Attention Networks for Medical Image  Segmentation\n\n[99] Vision Transformers in Medical Computer Vision -- A Contemplative  Retrospection\n\n[100] Optimizing Vision Transformers for Medical Image Segmentation\n\n[101] Pyramid Medical Transformer for Medical Image Segmentation\n\n[102] A Survey of Transformers\n\n[103] Axial-DeepLab  Stand-Alone Axial-Attention for Panoptic Segmentation\n\n[104] DAT++  Spatially Dynamic Vision Transformer with Deformable Attention\n\n[105] Scaling Local Self-Attention for Parameter Efficient Visual Backbones\n\n[106] Stand-Alone Self-Attention in Vision Models\n\n[107] ELSA  Enhanced Local Self-Attention for Vision Transformer\n\n[108] DAS  A Deformable Attention to Capture Salient Information in CNNs\n\n[109] Fully Attentional Network for Semantic Segmentation\n\n[110] Explicitly Modeled Attention Maps for Image Classification\n\n[111] Channelized Axial Attention for Semantic Segmentation -- Considering  Channel Relation within Spatial Attention for Semantic Segmentation\n\n[112] Normalized and Geometry-Aware Self-Attention Network for Image  Captioning\n\n[113] DAE-Former  Dual Attention-guided Efficient Transformer for Medical  Image Segmentation\n\n[114] SA-Det3D  Self-Attention Based Context-Aware 3D Object Detection\n\n[115] Poly-NL  Linear Complexity Non-local Layers with Polynomials\n\n[116] Inductive Biases and Variable Creation in Self-Attention Mechanisms\n\n[117] Convolutional Self-Attention Networks\n\n[118] Panoptic-Depth Color Map for Combination of Depth and Image Segmentation\n\n[119] PanoNet  Real-time Panoptic Segmentation through Position-Sensitive  Feature Embedding\n\n[120] ECLIPSE  Efficient Continual Learning in Panoptic Segmentation with  Visual Prompt Tuning\n\n[121] Vision Transformer Equipped with Neural Resizer on Facial Expression  Recognition Task\n\n[122] Fast-ParC  Capturing Position Aware Global Feature for ConvNets and ViTs\n\n[123] Do Vision Transformers See Like Convolutional Neural Networks \n\n[124] Efficient Training of Visual Transformers with Small Datasets\n\n[125] Grafting Vision Transformers\n\n[126] Masked autoencoders are effective solution to transformer data-hungry\n\n[127] FlatFormer  Flattened Window Attention for Efficient Point Cloud  Transformer\n\n[128] The Fully Convolutional Transformer for Medical Image Segmentation\n\n[129] UNETR++  Delving into Efficient and Accurate 3D Medical Image  Segmentation\n\n[130] HartleyMHA  Self-Attention in Frequency Domain for Resolution-Robust and  Parameter-Efficient 3D Image Segmentation\n\n[131] FLatten Transformer  Vision Transformer using Focused Linear Attention\n\n[132] GlobalMind  Global Multi-head Interactive Self-attention Network for  Hyperspectral Change Detection\n\n[133] LightViT  Towards Light-Weight Convolution-Free Vision Transformers\n\n[134] DualToken-ViT  Position-aware Efficient Vision Transformer with Dual  Token Fusion\n\n[135] When Shift Operation Meets Vision Transformer  An Extremely Simple  Alternative to Attention Mechanism\n\n[136] LF-ViT  Reducing Spatial Redundancy in Vision Transformer for Efficient  Image Recognition\n\n[137] ViTALiTy  Unifying Low-rank and Sparse Approximation for Vision  Transformer Acceleration with a Linear Taylor Attention\n\n[138] Bi-ViT  Pushing the Limit of Vision Transformer Quantization\n\n[139] Castling-ViT  Compressing Self-Attention via Switching Towards  Linear-Angular Attention at Vision Transformer Inference\n\n[140] Parameter-Efficient Transformer with Hybrid Axial-Attention for Medical  Image Segmentation\n\n[141] Window Attention is Bugged  How not to Interpolate Position Embeddings\n\n[142] The Principle of Diversity  Training Stronger Vision Transformers Calls  for Reducing All Levels of Redundancy\n\n[143] How Does Attention Work in Vision Transformers  A Visual Analytics  Attempt\n\n[144] Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs\n\n[145] FRED  Towards a Full Rotation-Equivariance in Aerial Image Object  Detection\n\n[146] Natural Attribute-based Shift Detection\n\n[147] Particularity\n\n[148] An efficient solution for semantic segmentation  ShuffleNet V2 with  atrous separable convolutions\n\n[149] Exploiting the Complementarity of 2D and 3D Networks to Address  Domain-Shift in 3D Semantic Segmentation\n\n[150] Uniform Envelopes\n\n[151] Side Window Filtering\n\n[152] Causal bootstrapping\n\n[153] Equivariant Multi-View Networks\n\n[154] GenTron  Delving Deep into Diffusion Transformers for Image and Video  Generation\n\n[155] 3D Brainformer  3D Fusion Transformer for Brain Tumor Segmentation\n\n[156] BenchMD  A Benchmark for Unified Learning on Medical Images and Sensors\n\n[157] Explainable Transformer Prototypes for Medical Diagnoses\n\n[158] Learning Category- and Instance-Aware Pixel Embedding for Fast Panoptic  Segmentation\n\n[159] All are Worth Words  A ViT Backbone for Diffusion Models\n\n[160] Container  Context Aggregation Network\n\n[161] Vision Transformer  Vit and its Derivatives\n\n[162] Next-ViT  Next Generation Vision Transformer for Efficient Deployment in  Realistic Industrial Scenarios\n\n[163] LiteTransformerSearch  Training-free Neural Architecture Search for  Efficient Language Models\n\n[164] Transformer-Powered Surrogates Close the ICF Simulation-Experiment Gap  with Extremely Limited Data\n\n[165] Wide Attention Is The Way Forward For Transformers \n\n[166] SwiftFormer  Efficient Additive Attention for Transformer-based  Real-time Mobile Vision Applications\n\n[167] Linear-Time Self Attention with Codeword Histogram for Efficient  Recommendation\n\n[168] LocalMamba  Visual State Space Model with Windowed Selective Scan\n\n[169] A Survey on Open-Vocabulary Detection and Segmentation  Past, Present,  and Future\n\n[170] Sensor Adaptation for Improved Semantic Segmentation of Overhead Imagery\n\n[171] Diagnostics in Semantic Segmentation\n\n[172] Treeging\n\n[173] Attentive VQ-VAE\n\n[174] Token Merging  Your ViT But Faster\n\n[175] NExT-GPT  Any-to-Any Multimodal LLM\n\n[176] Large-kernel Attention for Efficient and Robust Brain Lesion  Segmentation\n\n[177] Transforming the Interactive Segmentation for Medical Imaging\n\n[178] Intriguing Differences Between Zero-Shot and Systematic Evaluations of  Vision-Language Transformer Models\n\n[179] Memory Transformer\n\n[180] Learned Image Compression with Mixed Transformer-CNN Architectures\n\n[181] Scratching Visual Transformer's Back with Uniform Attention\n\n[182] Centroid Transformers  Learning to Abstract with Attention\n\n[183] Efficient Content-Based Sparse Attention with Routing Transformers\n\n[184] Dual Cross-Attention Learning for Fine-Grained Visual Categorization and  Object Re-Identification\n\n[185] Conditional Self-Attention for Query-based Summarization\n\n[186] Excavating RoI Attention for Underwater Object Detection\n\n[187] ScalableViT  Rethinking the Context-oriented Generalization of Vision  Transformer\n\n[188] On Efficient Real-Time Semantic Segmentation  A Survey\n\n[189] AinnoSeg  Panoramic Segmentation with High Perfomance\n\n[190] Rethinking Semantic Segmentation Evaluation for Explainability and Model  Selection\n\n[191] Single-Shot Panoptic Segmentation\n\n[192] Video Class Agnostic Segmentation Benchmark for Autonomous Driving\n\n[193] Synthesize then Compare  Detecting Failures and Anomalies for Semantic  Segmentation\n\n[194] LCPFormer  Towards Effective 3D Point Cloud Analysis via Local Context  Propagation in Transformers\n\n[195] SA Unet Improved\n\n[196] Deep learning for unsupervised domain adaptation in medical imaging   Recent advancements and future perspectives\n\n[197] Mask2Anomaly  Mask Transformer for Universal Open-set Segmentation\n\n[198] Towards holistic scene understanding  Semantic segmentation and beyond\n\n[199] ViT-P  Rethinking Data-efficient Vision Transformers from Locality\n\n[200] UniFormer  Unified Transformer for Efficient Spatiotemporal  Representation Learning\n\n[201] Cumulative Spatial Knowledge Distillation for Vision Transformers\n\n[202] OA-CNNs  Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation\n\n[203] A Transformer Framework for Data Fusion and Multi-Task Learning in Smart  Cities\n\n[204] A Hierarchical Spatial Transformer for Massive Point Samples in  Continuous Space\n\n[205] 3D Vision with Transformers  A Survey\n\n[206] Medical Imaging and Machine Learning\n\n\n",
    "reference": {
        "1": "1801.00868v3",
        "2": "2111.10250v1",
        "3": "1611.08323v2",
        "4": "1907.11066v2",
        "5": "1910.07655v4",
        "6": "1905.08720v2",
        "7": "2010.11681v2",
        "8": "2307.02877v1",
        "9": "2301.07499v1",
        "10": "2212.14180v2",
        "11": "2004.02307v3",
        "12": "2402.15469v1",
        "13": "2108.11128v1",
        "14": "2007.10202v1",
        "15": "2007.10035v2",
        "16": "2011.08577v2",
        "17": "2006.12706v1",
        "18": "2103.11110v1",
        "19": "2212.00787v2",
        "20": "1810.10438v2",
        "21": "2106.04555v1",
        "22": "2404.07445v1",
        "23": "2311.10648v1",
        "24": "2204.07069v1",
        "25": "2305.15608v1",
        "26": "2307.13640v1",
        "27": "2304.09854v3",
        "28": "2009.12942v1",
        "29": "2208.00713v1",
        "30": "2201.12785v3",
        "31": "2207.09339v3",
        "32": "2209.15001v3",
        "33": "2103.14586v2",
        "34": "2112.03552v4",
        "35": "2201.09450v3",
        "36": "2403.07392v3",
        "37": "2204.13892v1",
        "38": "2211.05778v4",
        "39": "2401.12736v1",
        "40": "2206.09959v5",
        "41": "2302.01791v1",
        "42": "2202.12295v3",
        "43": "2101.01169v5",
        "44": "2402.02622v2",
        "45": "2109.03201v6",
        "46": "2310.15025v1",
        "47": "2108.06076v4",
        "48": "2207.04044v5",
        "49": "2208.02034v1",
        "50": "2303.11298v1",
        "51": "2404.14657v1",
        "52": "2402.11760v1",
        "53": "2206.06488v2",
        "54": "2312.11750v1",
        "55": "2201.09873v1",
        "56": "2301.03505v3",
        "57": "2310.12570v2",
        "58": "2102.04306v1",
        "59": "2310.07781v1",
        "60": "2308.09372v2",
        "61": "2308.13680v1",
        "62": "2303.16892v1",
        "63": "2209.14378v2",
        "64": "2403.11026v1",
        "65": "2103.05940v1",
        "66": "2309.04902v1",
        "67": "2206.14409v3",
        "68": "2311.08236v1",
        "69": "2208.06066v1",
        "70": "2307.00213v1",
        "71": "2106.13230v1",
        "72": "2401.05481v1",
        "73": "2307.12775v1",
        "74": "2302.04303v1",
        "75": "2311.02332v5",
        "76": "2204.03408v1",
        "77": "2312.08874v2",
        "78": "2112.01527v3",
        "79": "2311.14405v1",
        "80": "1909.07721v2",
        "81": "2011.11964v2",
        "82": "2103.16302v2",
        "83": "2305.16316v2",
        "84": "2207.02027v1",
        "85": "2204.00993v3",
        "86": "2404.05196v1",
        "87": "2304.13991v1",
        "88": "2211.06726v2",
        "89": "2212.10724v1",
        "90": "2210.05313v1",
        "91": "2201.10737v5",
        "92": "2212.03035v1",
        "93": "2304.10891v1",
        "94": "2304.04225v1",
        "95": "2312.01232v2",
        "96": "2312.00634v2",
        "97": "2202.12165v3",
        "98": "2312.07128v1",
        "99": "2203.15269v1",
        "100": "2210.08066v2",
        "101": "2104.14702v3",
        "102": "2106.04554v2",
        "103": "2003.07853v2",
        "104": "2309.01430v1",
        "105": "2103.12731v3",
        "106": "1906.05909v1",
        "107": "2112.12786v1",
        "108": "2311.12091v1",
        "109": "2112.04108v4",
        "110": "2006.07872v2",
        "111": "2101.07434v5",
        "112": "2003.08897v1",
        "113": "2212.13504v3",
        "114": "2101.02672v5",
        "115": "2107.02859v1",
        "116": "2110.10090v2",
        "117": "1904.03107v1",
        "118": "2308.12937v1",
        "119": "2008.00192v1",
        "120": "2403.20126v1",
        "121": "2204.02181v1",
        "122": "2210.04020v2",
        "123": "2108.08810v2",
        "124": "2106.03746v2",
        "125": "2210.15943v2",
        "126": "2212.05677v5",
        "127": "2301.08739v3",
        "128": "2206.00566v2",
        "129": "2212.04497v2",
        "130": "2310.04466v1",
        "131": "2308.00442v2",
        "132": "2304.08687v1",
        "133": "2207.05557v1",
        "134": "2309.12424v1",
        "135": "2201.10801v1",
        "136": "2402.00033v1",
        "137": "2211.05109v1",
        "138": "2305.12354v1",
        "139": "2211.10526v4",
        "140": "2211.09533v1",
        "141": "2311.05613v1",
        "142": "2203.06345v1",
        "143": "2303.13731v1",
        "144": "2404.07449v1",
        "145": "2401.06159v1",
        "146": "2110.09276v1",
        "147": "2306.06812v1",
        "148": "1902.07476v2",
        "149": "2304.02991v1",
        "150": "2103.16156v5",
        "151": "1905.07177v1",
        "152": "1910.09648v3",
        "153": "1904.00993v2",
        "154": "2312.04557v1",
        "155": "2304.14508v1",
        "156": "2304.08486v2",
        "157": "2403.06961v1",
        "158": "2009.13342v2",
        "159": "2209.12152v4",
        "160": "2106.01401v2",
        "161": "2205.11239v2",
        "162": "2207.05501v4",
        "163": "2203.02094v2",
        "164": "2312.03642v1",
        "165": "2210.00640v2",
        "166": "2303.15446v2",
        "167": "2105.14068v1",
        "168": "2403.09338v1",
        "169": "2307.09220v2",
        "170": "1811.08328v1",
        "171": "1809.10328v1",
        "172": "2110.01053v1",
        "173": "2309.11641v2",
        "174": "2210.09461v3",
        "175": "2309.05519v2",
        "176": "2308.07251v1",
        "177": "2208.09592v2",
        "178": "2402.08473v1",
        "179": "2006.11527v2",
        "180": "2303.14978v1",
        "181": "2210.08457v1",
        "182": "2102.08606v2",
        "183": "2003.05997v5",
        "184": "2205.02151v1",
        "185": "2002.07338v1",
        "186": "2206.12128v1",
        "187": "2203.10790v2",
        "188": "2206.08605v2",
        "189": "2007.10591v1",
        "190": "2101.08418v2",
        "191": "1911.00764v2",
        "192": "2103.11015v2",
        "193": "2003.08440v2",
        "194": "2210.12755v2",
        "195": "2308.15487v1",
        "196": "2308.01265v1",
        "197": "2309.04573v2",
        "198": "2201.07734v1",
        "199": "2203.02358v1",
        "200": "2201.04676v3",
        "201": "2307.08500v1",
        "202": "2403.14418v1",
        "203": "2211.10506v1",
        "204": "2311.04434v1",
        "205": "2208.04309v1",
        "206": "2103.01938v1"
    },
    "retrieveref": {
        "1": "2304.09854v3",
        "2": "2101.01169v5",
        "3": "2105.05633v3",
        "4": "2103.14030v2",
        "5": "2208.06643v4",
        "6": "2207.14552v1",
        "7": "2110.02270v1",
        "8": "2308.07251v1",
        "9": "2210.08066v2",
        "10": "2203.08421v1",
        "11": "2307.12239v2",
        "12": "2403.07542v1",
        "13": "2206.01741v2",
        "14": "2110.10403v1",
        "15": "2104.12533v5",
        "16": "2211.10043v1",
        "17": "2207.04044v5",
        "18": "2310.05664v2",
        "19": "2109.01316v1",
        "20": "2312.10529v1",
        "21": "2111.06091v4",
        "22": "2205.07056v1",
        "23": "2208.09592v2",
        "24": "2206.14409v3",
        "25": "2111.04734v2",
        "26": "2109.08417v1",
        "27": "2208.02034v1",
        "28": "2201.09873v1",
        "29": "2305.18948v2",
        "30": "2404.00122v1",
        "31": "2211.06004v1",
        "32": "2207.10866v1",
        "33": "2403.13167v1",
        "34": "2104.12753v3",
        "35": "2107.00781v2",
        "36": "2209.14378v2",
        "37": "2310.12296v1",
        "38": "2109.03201v6",
        "39": "2301.03505v3",
        "40": "2102.10662v2",
        "41": "2310.09998v3",
        "42": "2107.13967v3",
        "43": "2012.12556v6",
        "44": "2304.04225v1",
        "45": "2206.00566v2",
        "46": "2103.00112v3",
        "47": "2201.12785v3",
        "48": "2204.05525v1",
        "49": "2307.09120v1",
        "50": "2309.05674v1",
        "51": "2304.04614v1",
        "52": "2308.16271v1",
        "53": "2208.04309v1",
        "54": "2306.05688v1",
        "55": "2204.04627v2",
        "56": "2312.01740v1",
        "57": "2306.03373v2",
        "58": "2204.08680v3",
        "59": "2212.11677v1",
        "60": "2401.13082v1",
        "61": "2310.15025v1",
        "62": "2211.14255v1",
        "63": "2206.01136v3",
        "64": "2301.03831v1",
        "65": "2106.04108v3",
        "66": "2108.01684v1",
        "67": "2110.10957v1",
        "68": "2201.10737v5",
        "69": "2310.19898v1",
        "70": "2210.05313v1",
        "71": "2112.11325v6",
        "72": "2308.10729v1",
        "73": "2307.08473v1",
        "74": "2312.17030v1",
        "75": "2209.05700v1",
        "76": "2108.09174v1",
        "77": "2211.09108v1",
        "78": "2103.10455v3",
        "79": "2108.02266v1",
        "80": "2106.13797v7",
        "81": "2212.14397v1",
        "82": "2206.00902v1",
        "83": "2101.01097v2",
        "84": "2307.00711v2",
        "85": "2209.00383v3",
        "86": "2310.14416v1",
        "87": "2203.10435v1",
        "88": "2212.09263v1",
        "89": "2106.02689v3",
        "90": "2103.16302v2",
        "91": "2311.08141v2",
        "92": "2307.01146v4",
        "93": "2312.00634v2",
        "94": "2203.04050v3",
        "95": "2106.06716v1",
        "96": "2108.11993v2",
        "97": "2202.06014v2",
        "98": "2012.09958v1",
        "99": "2312.06272v1",
        "100": "2204.02839v1",
        "101": "2112.11010v2",
        "102": "2301.10847v1",
        "103": "2111.01236v2",
        "104": "2306.08045v2",
        "105": "2306.04086v3",
        "106": "2303.16450v1",
        "107": "2311.01310v2",
        "108": "2303.14806v2",
        "109": "2207.09339v3",
        "110": "2402.17863v1",
        "111": "2312.07128v1",
        "112": "2304.01401v1",
        "113": "2208.11572v1",
        "114": "2209.14156v2",
        "115": "2106.14385v1",
        "116": "2110.03921v2",
        "117": "2202.12295v3",
        "118": "2206.06488v2",
        "119": "2112.02244v2",
        "120": "2403.17937v1",
        "121": "2309.02617v1",
        "122": "2310.07781v1",
        "123": "2206.14413v2",
        "124": "2206.12925v2",
        "125": "2309.04902v1",
        "126": "2307.09050v1",
        "127": "2207.08518v2",
        "128": "2304.12615v1",
        "129": "2110.04035v1",
        "130": "2402.12138v1",
        "131": "2112.05080v2",
        "132": "2303.15105v1",
        "133": "2211.13928v1",
        "134": "2105.04553v2",
        "135": "2201.00462v2",
        "136": "2203.09795v1",
        "137": "2301.13156v4",
        "138": "2204.07962v1",
        "139": "2109.02974v1",
        "140": "2107.00641v1",
        "141": "2206.04636v3",
        "142": "2210.03546v1",
        "143": "2303.12068v1",
        "144": "2210.14007v1",
        "145": "2404.15244v1",
        "146": "2103.16469v1",
        "147": "2203.13253v1",
        "148": "2212.10724v1",
        "149": "2204.08412v1",
        "150": "2302.10484v1",
        "151": "2101.08461v3",
        "152": "2111.10017v1",
        "153": "2208.00713v1",
        "154": "2011.00931v2",
        "155": "2212.06795v2",
        "156": "2203.00960v1",
        "157": "2404.04924v1",
        "158": "2105.14217v4",
        "159": "2207.01527v1",
        "160": "2107.00652v3",
        "161": "2401.02931v1",
        "162": "2402.14327v2",
        "163": "2107.14467v1",
        "164": "2105.05537v1",
        "165": "2112.09300v1",
        "166": "2205.11239v2",
        "167": "2105.09511v3",
        "168": "2204.01254v1",
        "169": "2304.14571v1",
        "170": "2101.08833v2",
        "171": "2210.05151v1",
        "172": "2109.08937v4",
        "173": "2305.11365v2",
        "174": "2404.07473v1",
        "175": "2207.11553v1",
        "176": "2404.13434v1",
        "177": "2310.12755v1",
        "178": "2302.04303v1",
        "179": "2207.13415v1",
        "180": "2211.04963v1",
        "181": "2206.12571v2",
        "182": "2111.14725v1",
        "183": "2204.00631v2",
        "184": "2309.02783v1",
        "185": "2211.14425v2",
        "186": "2202.11094v5",
        "187": "2403.18361v2",
        "188": "2307.08994v3",
        "189": "2212.02871v1",
        "190": "2401.11671v1",
        "191": "2210.15943v2",
        "192": "2309.04825v1",
        "193": "2311.01197v1",
        "194": "2206.08948v1",
        "195": "2106.03746v2",
        "196": "2206.00771v2",
        "197": "2211.14449v2",
        "198": "2304.06446v2",
        "199": "2304.02186v1",
        "200": "2311.01475v2",
        "201": "2103.01209v4",
        "202": "2203.12944v1",
        "203": "2210.07124v1",
        "204": "2107.03172v2",
        "205": "2106.00588v2",
        "206": "2212.00776v2",
        "207": "2311.01429v1",
        "208": "2210.13570v1",
        "209": "2201.09792v1",
        "210": "2404.06135v1",
        "211": "2207.03450v1",
        "212": "2304.13991v1",
        "213": "2403.09157v1",
        "214": "2203.09830v1",
        "215": "2103.06104v2",
        "216": "2112.13085v1",
        "217": "2112.04981v1",
        "218": "2304.00287v2",
        "219": "2401.12666v1",
        "220": "2402.11301v1",
        "221": "2401.04746v1",
        "222": "2305.00678v1",
        "223": "2303.09975v4",
        "224": "2203.04568v3",
        "225": "2401.08868v2",
        "226": "2211.11679v3",
        "227": "2106.03650v1",
        "228": "2402.02491v1",
        "229": "1911.11390v2",
        "230": "2210.14618v1",
        "231": "2306.06842v2",
        "232": "2307.14332v1",
        "233": "2304.11450v1",
        "234": "2307.13897v1",
        "235": "2303.13731v1",
        "236": "2309.13353v1",
        "237": "2208.01159v4",
        "238": "2201.03545v2",
        "239": "2203.01932v2",
        "240": "2306.01340v2",
        "241": "2211.09533v1",
        "242": "2104.14702v3",
        "243": "2309.13196v3",
        "244": "2102.07074v4",
        "245": "2404.11732v1",
        "246": "2212.13764v1",
        "247": "2107.04735v1",
        "248": "2207.14134v2",
        "249": "2107.08623v1",
        "250": "2206.06619v1",
        "251": "2207.03620v3",
        "252": "2311.09653v1",
        "253": "2207.13298v3",
        "254": "2311.17975v1",
        "255": "2204.07722v1",
        "256": "2106.13230v1",
        "257": "2112.11685v1",
        "258": "2110.14944v1",
        "259": "2303.16892v1",
        "260": "2211.06726v2",
        "261": "2308.13680v1",
        "262": "2201.09450v3",
        "263": "2301.00989v1",
        "264": "2207.02059v2",
        "265": "2303.09514v4",
        "266": "2106.12620v2",
        "267": "2203.13444v1",
        "268": "2109.08409v1",
        "269": "2401.15307v1",
        "270": "2103.14899v2",
        "271": "2207.02250v1",
        "272": "2206.08356v2",
        "273": "2205.10663v2",
        "274": "2305.06115v1",
        "275": "2403.17701v3",
        "276": "2210.04393v1",
        "277": "2105.10189v3",
        "278": "2106.03720v2",
        "279": "2210.12381v3",
        "280": "2308.03006v1",
        "281": "2403.18637v1",
        "282": "2402.08473v1",
        "283": "2111.07918v1",
        "284": "2403.01407v1",
        "285": "2206.06829v4",
        "286": "2210.07072v1",
        "287": "2205.03806v1",
        "288": "2203.08566v1",
        "289": "2306.07303v1",
        "290": "2312.03568v1",
        "291": "2305.01280v1",
        "292": "2204.13791v3",
        "293": "2209.05588v1",
        "294": "2310.05446v5",
        "295": "2308.10707v1",
        "296": "2108.05305v2",
        "297": "2210.04135v3",
        "298": "2304.03650v2",
        "299": "2206.05375v1",
        "300": "2108.13015v1",
        "301": "2205.13535v3",
        "302": "2307.00536v2",
        "303": "2205.13425v2",
        "304": "2201.00978v1",
        "305": "2211.14705v1",
        "306": "2204.07780v1",
        "307": "2206.11073v1",
        "308": "2112.10809v1",
        "309": "2302.11325v2",
        "310": "2204.07154v1",
        "311": "2107.02192v3",
        "312": "2403.02308v2",
        "313": "2103.04430v2",
        "314": "2203.07239v1",
        "315": "2107.12292v1",
        "316": "2304.05316v1",
        "317": "2210.03105v2",
        "318": "2210.02693v1",
        "319": "2205.08534v4",
        "320": "2106.04560v2",
        "321": "2203.03937v4",
        "322": "2203.02430v1",
        "323": "2303.16293v1",
        "324": "2202.11539v2",
        "325": "2106.03348v4",
        "326": "2404.08281v1",
        "327": "2201.05991v3",
        "328": "2104.00921v2",
        "329": "2301.09416v1",
        "330": "2310.05026v1",
        "331": "2312.04557v1",
        "332": "2206.09325v2",
        "333": "2010.11929v2",
        "334": "2305.01279v1",
        "335": "2211.11943v1",
        "336": "2103.07976v5",
        "337": "2309.09492v1",
        "338": "2104.06468v1",
        "339": "2112.02507v4",
        "340": "2309.01430v1",
        "341": "2304.08756v2",
        "342": "2302.09108v1",
        "343": "2112.14000v1",
        "344": "2203.11987v2",
        "345": "2210.14139v1",
        "346": "2307.01985v2",
        "347": "2106.09681v2",
        "348": "2205.10342v1",
        "349": "2106.06847v3",
        "350": "2201.08683v1",
        "351": "2403.17839v1",
        "352": "2109.12271v2",
        "353": "2404.10940v1",
        "354": "2201.12903v1",
        "355": "2202.10108v2",
        "356": "2304.03012v1",
        "357": "2201.08050v2",
        "358": "2201.01615v4",
        "359": "2305.16340v3",
        "360": "2308.06377v3",
        "361": "2404.07705v1",
        "362": "2404.04140v1",
        "363": "2204.08721v2",
        "364": "2401.11856v1",
        "365": "2205.02833v1",
        "366": "2209.01763v1",
        "367": "2112.09747v3",
        "368": "2210.07240v1",
        "369": "2107.00651v1",
        "370": "2305.11403v5",
        "371": "2211.00937v1",
        "372": "2301.04648v1",
        "373": "2210.01035v1",
        "374": "2212.05677v5",
        "375": "2308.05022v2",
        "376": "2106.12102v2",
        "377": "2201.10953v2",
        "378": "1907.03576v1",
        "379": "2311.15157v1",
        "380": "2106.04263v5",
        "381": "2104.13636v2",
        "382": "2308.01944v1",
        "383": "2111.06707v1",
        "384": "2310.01843v1",
        "385": "2106.01401v2",
        "386": "2304.05821v2",
        "387": "2111.13300v2",
        "388": "2210.11006v3",
        "389": "2109.13857v1",
        "390": "2110.13107v3",
        "391": "2310.12570v2",
        "392": "2403.16350v1",
        "393": "2302.13987v2",
        "394": "2209.01206v1",
        "395": "2203.06318v1",
        "396": "2109.13086v1",
        "397": "2303.11331v2",
        "398": "2305.03273v1",
        "399": "2308.03005v1",
        "400": "2210.00314v3",
        "401": "2305.16316v2",
        "402": "2308.03364v2",
        "403": "2202.07925v2",
        "404": "2401.17050v1",
        "405": "2112.00965v1",
        "406": "2102.05644v1",
        "407": "2109.08141v1",
        "408": "2107.06263v3",
        "409": "2204.08043v1",
        "410": "2307.11988v1",
        "411": "2402.00033v1",
        "412": "2211.01785v2",
        "413": "2212.03035v1",
        "414": "2111.14821v2",
        "415": "2112.04894v2",
        "416": "2311.11205v2",
        "417": "2201.05047v4",
        "418": "2103.03024v1",
        "419": "2203.15269v1",
        "420": "2105.14173v3",
        "421": "2207.14284v3",
        "422": "2111.14791v2",
        "423": "2203.10314v1",
        "424": "2201.02001v4",
        "425": "2110.05092v2",
        "426": "2210.12599v2",
        "427": "2401.10831v3",
        "428": "2203.03682v2",
        "429": "2205.07417v2",
        "430": "2207.05557v1",
        "431": "2211.15107v2",
        "432": "2307.09402v1",
        "433": "2212.11115v1",
        "434": "2302.14611v1",
        "435": "2308.10561v2",
        "436": "2111.13152v3",
        "437": "2203.02891v1",
        "438": "2208.03486v3",
        "439": "2107.02960v3",
        "440": "2403.07392v3",
        "441": "2303.09233v2",
        "442": "2308.01045v2",
        "443": "2305.04276v2",
        "444": "2201.01293v7",
        "445": "2202.06268v2",
        "446": "2101.11986v3",
        "447": "2207.05501v4",
        "448": "2403.13677v1",
        "449": "2402.07245v2",
        "450": "2207.13259v1",
        "451": "2305.09880v3",
        "452": "2103.05103v1",
        "453": "2103.15691v2",
        "454": "2309.14065v7",
        "455": "2110.13083v1",
        "456": "2304.14508v1",
        "457": "2107.02655v1",
        "458": "2305.13031v1",
        "459": "2107.05475v3",
        "460": "2302.01791v1",
        "461": "2305.07848v3",
        "462": "2202.12165v3",
        "463": "2312.01897v2",
        "464": "2210.03168v1",
        "465": "2108.02432v1",
        "466": "2203.15221v2",
        "467": "2201.04019v4",
        "468": "2101.03848v3",
        "469": "2203.16194v4",
        "470": "2401.10153v1",
        "471": "2301.11553v1",
        "472": "2207.04535v2",
        "473": "2308.06693v1",
        "474": "2105.12723v4",
        "475": "2206.10552v2",
        "476": "2308.13969v1",
        "477": "2107.05274v2",
        "478": "2301.08739v3",
        "479": "2203.10247v2",
        "480": "2307.02321v2",
        "481": "2203.15380v4",
        "482": "2110.13385v1",
        "483": "2303.15198v2",
        "484": "2403.18063v1",
        "485": "2310.04099v2",
        "486": "2403.04200v1",
        "487": "2108.09322v2",
        "488": "2201.06251v2",
        "489": "2203.02916v2",
        "490": "2103.13413v1",
        "491": "2210.09220v1",
        "492": "2305.19365v1",
        "493": "2404.11273v1",
        "494": "2308.02161v1",
        "495": "2109.04611v1",
        "496": "2203.05922v1",
        "497": "2311.17626v1",
        "498": "2311.07263v1",
        "499": "2106.01548v3",
        "500": "2305.04236v2",
        "501": "2107.08192v1",
        "502": "2201.07384v2",
        "503": "2103.15808v1",
        "504": "2303.06908v2",
        "505": "2309.12717v1",
        "506": "2105.14432v2",
        "507": "2308.08724v1",
        "508": "2304.01184v2",
        "509": "2204.07118v1",
        "510": "2206.09959v5",
        "511": "2308.09369v1",
        "512": "2403.17177v1",
        "513": "2209.15001v3",
        "514": "2111.13156v1",
        "515": "2302.08052v1",
        "516": "2210.05844v2",
        "517": "2107.02191v1",
        "518": "2308.14036v2",
        "519": "2307.02280v1",
        "520": "2307.13236v1",
        "521": "2307.08579v2",
        "522": "2208.08900v2",
        "523": "2210.16897v1",
        "524": "2102.04306v1",
        "525": "2210.15871v1",
        "526": "2211.11167v2",
        "527": "2304.01054v1",
        "528": "2311.05707v1",
        "529": "2111.12994v2",
        "530": "2404.00358v1",
        "531": "2209.09545v1",
        "532": "2307.02100v2",
        "533": "2401.11718v1",
        "534": "2207.04978v1",
        "535": "2309.01017v1",
        "536": "2109.02497v2",
        "537": "2108.06076v4",
        "538": "2212.12552v1",
        "539": "2203.09581v3",
        "540": "2403.13642v1",
        "541": "2103.10619v2",
        "542": "2212.13766v2",
        "543": "2301.06429v3",
        "544": "2002.12585v2",
        "545": "2110.01655v1",
        "546": "2105.03889v1",
        "547": "2112.00582v1",
        "548": "2203.09887v2",
        "549": "2107.00451v2",
        "550": "2302.11802v1",
        "551": "2210.12755v2",
        "552": "2211.14764v1",
        "553": "2402.15578v1",
        "554": "2206.08645v2",
        "555": "2307.07240v1",
        "556": "2204.00423v1",
        "557": "2207.02796v2",
        "558": "2110.08568v1",
        "559": "2210.01391v1",
        "560": "2110.07160v1",
        "561": "2307.06666v2",
        "562": "2401.00912v1",
        "563": "2211.04188v2",
        "564": "2206.00923v1",
        "565": "2112.10762v2",
        "566": "2111.14887v2",
        "567": "2103.10504v3",
        "568": "2309.12424v1",
        "569": "2102.00719v3",
        "570": "2303.13111v3",
        "571": "2309.11523v5",
        "572": "2208.08352v1",
        "573": "2302.11481v1",
        "574": "2112.13492v1",
        "575": "2403.17921v1",
        "576": "2309.08035v1",
        "577": "2403.04968v1",
        "578": "2305.07270v4",
        "579": "2003.08077v4",
        "580": "2402.05964v2",
        "581": "2011.09763v2",
        "582": "2207.10228v1",
        "583": "2106.10270v2",
        "584": "2302.09365v1",
        "585": "2110.09408v3",
        "586": "2305.04961v1",
        "587": "2308.14160v1",
        "588": "2206.00182v2",
        "589": "2206.00481v2",
        "590": "2308.03359v1",
        "591": "2303.10689v1",
        "592": "2212.02791v1",
        "593": "2207.02027v1",
        "594": "2112.02841v2",
        "595": "2209.09004v3",
        "596": "2110.15156v1",
        "597": "2305.15773v1",
        "598": "2310.17742v1",
        "599": "2210.15769v1",
        "600": "2109.13925v2",
        "601": "2308.11421v1",
        "602": "2305.10727v1",
        "603": "2206.10845v1",
        "604": "2210.06908v1",
        "605": "2110.00966v2",
        "606": "2401.11243v1",
        "607": "2105.07926v4",
        "608": "2106.00666v3",
        "609": "2204.03408v1",
        "610": "2112.01527v3",
        "611": "2206.13294v2",
        "612": "2305.16318v2",
        "613": "2103.15358v2",
        "614": "2311.04157v1",
        "615": "2105.14110v2",
        "616": "2403.14552v1",
        "617": "2303.14189v2",
        "618": "2112.01838v2",
        "619": "2401.00722v1",
        "620": "2205.12602v1",
        "621": "2209.13222v1",
        "622": "2206.12634v1",
        "623": "2308.13331v1",
        "624": "2404.16371v1",
        "625": "2210.04020v2",
        "626": "2310.11725v2",
        "627": "2306.03377v2",
        "628": "2309.00928v1",
        "629": "2112.00336v1",
        "630": "2105.10920v1",
        "631": "2202.12587v1",
        "632": "2006.03677v4",
        "633": "2110.04009v1",
        "634": "2209.08956v1",
        "635": "2404.05196v1",
        "636": "2309.16210v1",
        "637": "2310.17683v1",
        "638": "2108.04938v1",
        "639": "2305.15302v1",
        "640": "2307.01115v1",
        "641": "2103.12957v1",
        "642": "2201.04676v3",
        "643": "2303.11340v2",
        "644": "2311.11378v1",
        "645": "2307.02010v2",
        "646": "2307.02092v1",
        "647": "2404.05102v1",
        "648": "2108.10059v1",
        "649": "2310.13135v3",
        "650": "2101.01909v2",
        "651": "2010.13294v2",
        "652": "1912.08226v2",
        "653": "2207.02206v2",
        "654": "2111.11802v4",
        "655": "2310.13120v1",
        "656": "2306.06656v1",
        "657": "2404.15817v1",
        "658": "2307.08504v2",
        "659": "2309.06721v2",
        "660": "2304.11906v3",
        "661": "2402.07545v1",
        "662": "2303.14358v1",
        "663": "2205.14319v1",
        "664": "2306.04670v3",
        "665": "2404.14945v1",
        "666": "2208.01753v1",
        "667": "2302.01027v1",
        "668": "2307.07313v1",
        "669": "2307.08263v1",
        "670": "2306.01988v1",
        "671": "2201.01090v1",
        "672": "2304.06710v1",
        "673": "2211.05187v1",
        "674": "2205.09579v3",
        "675": "2206.09731v2",
        "676": "2105.04281v3",
        "677": "2308.16145v2",
        "678": "2303.07034v2",
        "679": "2208.04939v2",
        "680": "2302.11867v3",
        "681": "2301.01208v1",
        "682": "2208.05114v1",
        "683": "2302.07387v2",
        "684": "2012.09841v3",
        "685": "2205.15448v3",
        "686": "2312.14606v1",
        "687": "2204.03957v1",
        "688": "2203.01587v3",
        "689": "2302.11184v2",
        "690": "2107.09240v1",
        "691": "2401.11644v1",
        "692": "2208.12259v3",
        "693": "2205.04437v3",
        "694": "2201.08741v2",
        "695": "2206.00806v1",
        "696": "2210.14319v1",
        "697": "2112.05814v3",
        "698": "2304.10891v1",
        "699": "2305.04722v1",
        "700": "2404.10700v1",
        "701": "2107.02239v4",
        "702": "2108.03227v3",
        "703": "2312.02725v3",
        "704": "2203.12861v3",
        "705": "2012.09838v2",
        "706": "1902.06729v2",
        "707": "2209.08194v1",
        "708": "2112.05425v1",
        "709": "2305.09211v3",
        "710": "2111.15637v2",
        "711": "2304.06391v1",
        "712": "2210.15722v1",
        "713": "2304.01715v2",
        "714": "2401.05481v1",
        "715": "2206.01191v5",
        "716": "2401.10536v1",
        "717": "2205.09256v3",
        "718": "2310.13605v1",
        "719": "2110.00335v1",
        "720": "2309.13245v1",
        "721": "1909.06273v1",
        "722": "2306.01257v2",
        "723": "2201.11403v5",
        "724": "2310.12031v1",
        "725": "2205.12956v2",
        "726": "2205.06944v1",
        "727": "2203.03821v5",
        "728": "2112.13983v1",
        "729": "2204.07098v1",
        "730": "2308.06904v1",
        "731": "2304.07434v1",
        "732": "2112.11435v2",
        "733": "2106.12378v1",
        "734": "2203.14043v1",
        "735": "2207.03782v1",
        "736": "2103.09712v2",
        "737": "2108.11575v5",
        "738": "2307.03854v4",
        "739": "2305.07223v2",
        "740": "2210.11909v1",
        "741": "2403.00396v1",
        "742": "2402.00534v1",
        "743": "2205.08303v1",
        "744": "2404.15451v1",
        "745": "2302.08641v1",
        "746": "2205.01580v1",
        "747": "2201.10728v1",
        "748": "2209.09657v1",
        "749": "2201.08582v4",
        "750": "2204.00746v2",
        "751": "2106.04520v2",
        "752": "2311.03427v1",
        "753": "2311.01308v1",
        "754": "2303.01237v1",
        "755": "2307.13640v1",
        "756": "2402.04563v1",
        "757": "2204.11449v1",
        "758": "2307.08051v1",
        "759": "2304.03481v1",
        "760": "2111.02387v3",
        "761": "2306.10959v1",
        "762": "2104.11227v1",
        "763": "2206.04584v1",
        "764": "2106.02320v4",
        "765": "2401.03694v1",
        "766": "2309.02031v2",
        "767": "2309.05015v1",
        "768": "2110.02453v2",
        "769": "2112.02624v2",
        "770": "2110.08037v1",
        "771": "2110.03864v1",
        "772": "2208.06049v3",
        "773": "2206.08883v1",
        "774": "2212.14678v1",
        "775": "2303.10333v1",
        "776": "2401.03836v4",
        "777": "2111.11418v3",
        "778": "2308.10839v1",
        "779": "2208.11484v2",
        "780": "2104.01745v1",
        "781": "2106.02852v2",
        "782": "2106.03146v1",
        "783": "2403.04562v1",
        "784": "2309.11933v1",
        "785": "2309.16108v4",
        "786": "2307.01486v1",
        "787": "2108.02759v2",
        "788": "2102.12122v2",
        "789": "2004.06193v2",
        "790": "2204.11024v1",
        "791": "2304.04554v2",
        "792": "2212.13504v3",
        "793": "2303.01939v1",
        "794": "2301.04944v3",
        "795": "2310.19001v1",
        "796": "2403.06577v1",
        "797": "2402.06423v1",
        "798": "2311.03873v1",
        "799": "2111.10480v6",
        "800": "2108.07851v6",
        "801": "2207.10026v1",
        "802": "2208.13113v1",
        "803": "2203.09173v1",
        "804": "2205.00823v1",
        "805": "2207.10666v1",
        "806": "2306.07265v2",
        "807": "2309.16588v2",
        "808": "2203.12848v1",
        "809": "2208.10861v1",
        "810": "2108.11084v3",
        "811": "2307.03254v1",
        "812": "2303.06440v2",
        "813": "2402.08793v1",
        "814": "2211.08717v1",
        "815": "2306.00396v1",
        "816": "2306.06289v2",
        "817": "2104.12099v2",
        "818": "2111.08314v1",
        "819": "2211.07157v3",
        "820": "2205.03892v2",
        "821": "2310.04779v1",
        "822": "2207.04403v1",
        "823": "2110.04869v2",
        "824": "2106.06112v3",
        "825": "2203.09773v2",
        "826": "2311.08236v1",
        "827": "2306.13776v1",
        "828": "2306.16103v2",
        "829": "2306.12298v1",
        "830": "2308.03475v2",
        "831": "2209.12152v4",
        "832": "2111.10493v2",
        "833": "2209.05777v1",
        "834": "2306.15350v2",
        "835": "2102.08005v2",
        "836": "2310.18550v1",
        "837": "2110.05270v1",
        "838": "2311.02506v1",
        "839": "2302.08474v1",
        "840": "2309.05224v1",
        "841": "2205.10873v2",
        "842": "2106.05786v1",
        "843": "2403.15069v1",
        "844": "2305.13035v5",
        "845": "2212.03029v3",
        "846": "2301.06869v1",
        "847": "2103.11816v2",
        "848": "2306.12243v3",
        "849": "2106.12011v6",
        "850": "2207.10774v4",
        "851": "2201.03178v2",
        "852": "2211.13654v2",
        "853": "2212.06595v1",
        "854": "2204.08446v2",
        "855": "2305.19129v1",
        "856": "2206.07990v3",
        "857": "2305.09566v2",
        "858": "2211.09552v1",
        "859": "2310.13604v1",
        "860": "2402.02634v1",
        "861": "2211.05776v3",
        "862": "2306.02901v1",
        "863": "2106.11539v2",
        "864": "2311.17428v1",
        "865": "2404.05207v1",
        "866": "2402.10887v1",
        "867": "2402.16033v1",
        "868": "2202.05054v1",
        "869": "2308.13442v2",
        "870": "2402.03317v1",
        "871": "2301.00973v1",
        "872": "2306.06635v1",
        "873": "2303.09998v2",
        "874": "2210.15933v1",
        "875": "2111.05297v3",
        "876": "2108.13341v2",
        "877": "2312.14502v1",
        "878": "2311.17893v1",
        "879": "2311.12418v1",
        "880": "2306.06446v4",
        "881": "2110.05722v3",
        "882": "2212.03690v1",
        "883": "2309.09276v1",
        "884": "2107.09011v4",
        "885": "2203.06649v3",
        "886": "2404.06075v1",
        "887": "2209.02178v2",
        "888": "2404.00279v1",
        "889": "2005.13117v4",
        "890": "2205.10650v2",
        "891": "2210.09969v1",
        "892": "2201.11037v1",
        "893": "2111.11067v2",
        "894": "2109.09920v1",
        "895": "2303.03800v1",
        "896": "2310.18651v4",
        "897": "1512.09049v1",
        "898": "2207.08569v3",
        "899": "2209.07704v1",
        "900": "2204.02547v1",
        "901": "2307.07982v1",
        "902": "2403.05246v2",
        "903": "2303.12194v2",
        "904": "2311.04434v1",
        "905": "2401.09630v3",
        "906": "2401.00740v1",
        "907": "2304.09630v4",
        "908": "2404.15956v2",
        "909": "2111.13587v2",
        "910": "2107.05790v2",
        "911": "2403.09394v1",
        "912": "2302.00290v3",
        "913": "2109.12932v3",
        "914": "2110.02178v2",
        "915": "2210.09871v2",
        "916": "2204.13575v1",
        "917": "2210.03861v1",
        "918": "2208.10431v2",
        "919": "2106.10587v2",
        "920": "2312.08568v2",
        "921": "2309.04105v1",
        "922": "2310.18969v1",
        "923": "2303.13509v1",
        "924": "2307.12591v1",
        "925": "2309.04752v1",
        "926": "2207.05420v2",
        "927": "2103.16553v1",
        "928": "2304.07314v1",
        "929": "2106.02277v1",
        "930": "2304.02942v2",
        "931": "2403.02863v1",
        "932": "2108.03032v3",
        "933": "2209.10126v1",
        "934": "2012.14214v5",
        "935": "2310.01292v1",
        "936": "2108.05988v2",
        "937": "2210.15808v2",
        "938": "2203.15251v2",
        "939": "2204.07733v2",
        "940": "2209.14024v1",
        "941": "2205.04222v1",
        "942": "2312.09251v1",
        "943": "2110.06915v3",
        "944": "2202.10240v7",
        "945": "2201.10147v2",
        "946": "2312.01435v2",
        "947": "2311.12028v2",
        "948": "2203.14557v3",
        "949": "2212.11548v1",
        "950": "2312.01324v2",
        "951": "2308.15004v1",
        "952": "2309.16889v2",
        "953": "2201.00520v3",
        "954": "2201.00814v2",
        "955": "2204.05453v4",
        "956": "2007.11888v1",
        "957": "2306.07470v1",
        "958": "2309.01692v1",
        "959": "2203.10537v2",
        "960": "2404.10407v1",
        "961": "2207.11103v1",
        "962": "2201.03230v2",
        "963": "2310.01209v1",
        "964": "2310.20057v1",
        "965": "2202.04942v2",
        "966": "2311.12589v2",
        "967": "1812.02707v2",
        "968": "2211.05781v2",
        "969": "2109.09536v1",
        "970": "2310.03108v1",
        "971": "2303.17472v1",
        "972": "2108.05887v1",
        "973": "2209.10158v1",
        "974": "2309.01408v1",
        "975": "2208.11108v1",
        "976": "2207.03041v1",
        "977": "2209.13959v2",
        "978": "2112.05112v2",
        "979": "2310.10353v1",
        "980": "2211.05109v1",
        "981": "2308.06009v1",
        "982": "2109.00642v1",
        "983": "2401.06960v1",
        "984": "2402.05861v1",
        "985": "2309.01365v3",
        "986": "2404.02905v1",
        "987": "2203.13655v2",
        "988": "2207.01172v1",
        "989": "2106.14156v1",
        "990": "2301.11022v1",
        "991": "1812.01397v3",
        "992": "2404.14657v1",
        "993": "2012.12877v2",
        "994": "2403.19111v1",
        "995": "2207.09666v1",
        "996": "2106.13488v4",
        "997": "2311.01283v1",
        "998": "2106.03106v2",
        "999": "2207.02126v1",
        "1000": "2304.12406v2"
    }
}