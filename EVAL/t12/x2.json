{
  "survey": "Transformer models have significantly advanced the field of visual segmentation within computer vision, addressing limitations of traditional convolutional neural networks (CNNs) through innovative self-attention mechanisms that enhance spatial context capturing and recognition accuracy. This survey paper reviews the evolution, methodologies, and performance improvements associated with transformer-based segmentation techniques. Key advancements include the integration of transformers with CNNs to optimize segmentation outcomes, particularly in complex scenarios like Depth-aware Video Panoptic Segmentation (DVPS) and few-shot segmentation tasks. The survey highlights the transformative impact of transformers in achieving notable performance metrics, such as 80.4\n\nIntroduction Significance of Transformer-Based Visual Segmentation Transformer models have significantly transformed visual segmentation by overcoming the limitations of traditional convolutional neural networks (CNNs) in spatial context capture and visual recognition enhancement [1,2]. Their self-attention mechanisms integrate global information, enhancing image recognition and segmentation accuracy. For instance, the Panoptic SegFormer model has demonstrated remarkable efficiency and accuracy improvements in panoptic segmentation tasks [3]. The integration of transformers with CNNs has been pivotal in optimizing the performance of Vision Transformers (ViTs), leveraging the strengths of both architectures [4]. This combination is particularly beneficial in complex scenarios like Depth-aware Video Panoptic Segmentation (DVPS), where transformers excel in simultaneous predictions of panoptic segmentation and depth [5]. Moreover, transformers have facilitated universal architectures that manage diverse segmentation tasks, streamlining research efforts and reducing the need for multiple models [6]. In few-shot segmentation, where rapid adaptation to novel classes is crucial, transformers enhance the interaction between visual and textual data, exemplified by systems like MDETR, thereby increasing model flexibility and efficacy [7,8]. Their impact is further underscored in scene parsing tasks, achieving impressive metrics such as 80.4\\ The importance of image segmentation in computer vision is evident in critical applications like brain tumor segmentation, vital for early diagnosis and treatment improvements [9]. Transformers also address computational inefficiencies in end-to-end object detection, as highlighted by the DAB-DETR method, which mitigates slow training convergence in DETR models [10,11]. The necessity for a unified model capable of handling various segmentation tasks emphasizes the transformative potential of transformer models [12]. Motivation for Using Transformer Models The motivation for adopting transformer models in visual segmentation stems from their ability to enhance the discriminative power of instance embeddings, essential for accurate instance association in video instance segmentation [13]. Unified approaches like TransTrack leverage previous object features as queries to streamline detection and association processes in current frames, benefiting tasks such as referring expression comprehension (REC) and segmentation (RES) [14,15]. Modules like the Cycle-Consistent TRansformer (CyCTR) exemplify this motivation by aggregating pixel-wise support features into query features, thereby improving segmentation accuracy [7]. The Masked-attention Mask Transformer (Mask2Former) addresses the need for a unified architecture that generalizes across different segmentation tasks, overcoming inefficiencies in existing methods [6]. In medical imaging, particularly for brain tumor diagnosis, transformers are driven by the need for efficient analysis that can significantly impact patient outcomes [9]. The inefficiency of using multiple models for different segmentation tasks complicates workflows and raises computational costs, motivating the adoption of transformers to streamline processes [12]. The significant influence of sequence length on the performance of masked autoencoders in computer vision tasks further underscores the utility of transformers, which efficiently manage varying scales and high resolutions [16]. Additionally, the effectiveness of transformer architectures challenges the notion that attention-based token mixers solely drive performance, highlighting their broader applicability beyond attention mechanisms [17]. Evolution of Transformer-Based Segmentation Techniques The evolution of transformer-based segmentation techniques marks a significant shift from traditional convolutional methods to advanced transformer architectures, enhancing global context modeling and semantic relationships. Initially dominated by fully-convolutional networks, segmentation tasks experienced a pivotal transition with the introduction of the SEgmentation TRansformer (SETR), which eliminated reliance on convolutions for superior context modeling [18]. The Panoptic SegFormer exemplifies this evolution by integrating a deeply-supervised mask decoder and a query decoupling strategy, improving efficiency and accuracy in panoptic segmentation tasks [3]. The introduction of Mask2Former further signifies progress by using masked attention to consolidate multiple segmentation tasks within a single framework, outperforming specialized architectures [6]. This trend towards integrated models capable of handling multiple tasks concurrently is evident in the OMG-Seg model, designed to encompass over ten distinct segmentation tasks within a single transformer-based encoder-decoder architecture [12]. Moreover, the insights from MetaFormer suggest that the general architecture of transformers may be more critical for performance than specific token mixer modules, indicating a shift towards architectural innovations [17]. These advancements collectively highlight the transformative role of transformers in reshaping visual segmentation, offering models that are versatile and powerful, adept at addressing a variety of vision tasks. The adoption of transformers has led to significant improvements in learning long-range dependencies, evident in their success across 3D vision tasks such as classification, segmentation, and pose estimation. Integration of advanced network-level and block-level architectures has yielded consistent performance gains across various tasks, with transformers demonstrating superiority over traditional 3D ConvNets in video processing by effectively managing long-term temporal dynamics with reduced computational complexity. Despite challenges such as the absence of inductive biases and quadratic scaling with input length, transformers continue to excel, bolstered by their strong representation capabilities and self-attention mechanisms. Hybrid approaches that combine hierarchical transformers with ConvNet priors have further expanded their applicability as generic vision backbones, showcasing remarkable performance across diverse tasks [19,20,21,22,23]. Structure of the Survey This survey systematically explores the transformative role of transformer models in visual segmentation, beginning with an introduction that establishes their significance and motivation in this domain. Subsequent sections delve into the background of computer vision and traditional segmentation methods, highlighting limitations that prompted the adoption of transformer-based approaches. The survey examines architectural innovations and integration strategies of transformer models within computer vision, emphasizing self-attention mechanisms and their implications for segmentation tasks. The methodologies section provides an in-depth analysis of various techniques employed in transformer-based visual segmentation, including encoder-decoder architectures, integration with convolutional networks, and approaches for point cloud and video segmentation. Performance improvements and challenges associated with these models are discussed, focusing on metrics, comparative analyses, computational complexity, and data requirements. The survey concludes with a discussion on current applications and future directions, exploring optimization of transformer architectures, expansion of applications, and potential for cross-domain and multimodal applications. Each section builds upon the previous, offering a comprehensive view of advancements and ongoing research in transformer-based visual segmentation.The following sections are organized as shown in . Background Overview of Computer Vision and Visual Segmentation Computer vision, a pivotal area of artificial intelligence, enables machines to interpret visual data, encompassing tasks like object recognition, image classification, and scene understanding [24]. Visual segmentation, a critical component, involves dividing images into meaningful segments to identify and classify distinct objects or regions, essential in applications from autonomous vehicles to medical imaging. The integration of multimodal learning frameworks, particularly those employing transformer models, has advanced visual segmentation by processing diverse data types simultaneously, enhancing accuracy and robustness [25]. Datasets enriched with depth information, semantic labels, and instance annotations are crucial for evaluating segmentation models, especially in 3D point cloud restoration [26]. As computer vision progresses, visual segmentation remains central, driving innovations that improve image analysis precision and efficiency, underscored by standardized benchmarks like the ImageNet Large Scale Visual Recognition Challenge [24]. Traditional Visual Segmentation Methods Traditional segmentation methods, primarily using convolutional neural networks (CNNs) and fully-convolutional networks, rely on encoder-decoder architectures for pixel-wise predictions. These methods, while foundational, struggle with capturing non-local contexts and handling varying object scales, crucial for effective segmentation [1]. Two-stage methods like Mask R-CNN excel in mask average precision but face complexity in region of interest operations [27]. One-stage methods, though potentially more efficient, encounter challenges in compact mask representation [27], often depending on bounding box predictions and non-maximum suppression, which introduce inefficiencies [28]. In 3D segmentation, traditional approaches like 3D voxelization and convolution networks face difficulties with the sparsity and varying density of outdoor point clouds [29]. These limitations are evident in tasks like multiple object tracking, where traditional methods involve complex processes prone to errors [14,30]. Traditional methods often require specialized architectures for each task, increasing research efforts and inefficiencies [6]. In few-shot segmentation, reliance on semantic-level prototypes neglects crucial pixel-wise support information [7]. Manual segmentation methods are time-consuming, highlighting the need for automation [9]. The limited performance of Vision Transformers compared to CNNs further emphasizes the constraints of conventional methods [4]. These challenges, along with inefficiencies in traditional architectures, have spurred the emergence of innovative approaches like Panoptic SegFormer [3]. Limitations of Traditional Methods Traditional segmentation methods face significant limitations in complex visual environments. The reliance on fixed region of interest operations restricts adaptability and precision in instance segmentation [31]. Intricate segmentation modules complicate learning processes, reducing effectiveness [32]. Techniques like atrous convolutions and feature pyramid fusion are computationally intensive or fall short of performance expectations [33]. In query-based instance segmentation, effective separation and robustness against geometric transformations are often lacking [34]. One-stage methods struggle with compact mask representation, limiting performance [27]. Inefficiency in leveraging masked data hinders performance and training speed of large models [35]. Traditional methods also struggle with multi-scale context capture and adapting to varying object sizes without detail loss [36]. The dependence on anchor boxes complicates detection and introduces sensitive hyper-parameters [37]. Video processing complexities in current methods lead to inefficiencies and errors [14]. Traditional methods often use semantic-level prototypes, resulting in suboptimal query predictions [7]. Convolutional approaches fail to fully exploit relationships between image elements [2]. The lack of a universal architecture for segmentation tasks necessitates innovations like Mask2Former [6]. In medical imaging, manual segmentation of brain tumors is challenging and time-consuming [9]. Traditional methods struggle with multiscale feature management and achieving high accuracy without excessive training, underscoring the need for transformer-based models [3]. Traditional methods also struggle with temporal variations in video sequences, relying on motion and appearance-based heuristics [30]. The Transformer attention mechanism's difficulty in processing image feature maps results in suboptimal performance for small object detection [38]. Challenges in accurately predicting and reconstructing masked patches affect representation quality [39]. The significant increase in encoder tokens for effective object detection creates computational bottlenecks [10]. The inability to decouple mask size from patch size restricts sequence utilization during pre-training [16]. Inefficiencies in DETR models and slow convergence rates necessitate advanced techniques like DAB-DETR [11]. High computational costs with increased tokens in multi-scale features affect efficiency in object detection tasks [40]. In recent years, transformer models have revolutionized the field of computer vision, leading to significant advancements in performance and efficiency. As illustrated in , the hierarchical structure of these models is marked by several architectural innovations that enhance their functionality. This figure highlights the integration of transformer models with traditional methodologies, showcasing how self-attention mechanisms play a pivotal role in this synergy. Furthermore, the image emphasizes the segmentation enhancements and efficiency improvements that transformers offer, underlining their transformative potential and adaptability across a range of visual tasks. By examining these elements, we gain a deeper understanding of the evolving landscape of computer vision and the critical advancements brought forth by transformer architectures. Transformer Models in Computer Vision Architectural Innovations in Transformer Models Transformer models have revolutionized visual tasks through architectural innovations that enhance context modeling, scalability, and computational efficiency. As illustrated in , these innovations can be categorized into segmentation enhancements, object detection advances, and training efficiency improvements. Each category highlights key models and methodologies that contribute to the evolving landscape of transformer-based visual tasks. For instance, Mask2Former utilizes masked attention for localized feature extraction in segmentation tasks [6], while few-shot segmentation models employ cycle-consistent attention to focus on informative pixels [7]. Discriminative Query Embedding Learning (DQEL) refines instance segmentation by creating unique, transformation-equivariant query embeddings [34]. The Multi-task Collaborative Network (MCN) enhances visual task architectures with Consistency Energy Maximization (CEM) and Adaptive Soft Non-Located Suppression (ASNLS) [15]. CLOVIS improves video instance segmentation using contrastive learning for discriminative embeddings [13]. Large models benefit from asymmetric encoder-decoder architectures and effective masking strategies, achieving faster training and improved accuracy [35]. Attention mechanisms in transformers enhance feature representation and tracking efficiency, as seen in TransTrack's iterative query updates for object tracking. Deformable DETR introduces flexible attention modules, optimizing key point sampling and performance [38]. FCOS simplifies object detection by eliminating anchor boxes [37]. Sparse DETR reduces computational costs through selective token updating while maintaining detection performance [10]. LS-MAE decouples mask and patch sizes, enabling longer sequences and improved task performance [16]. DAB-DETR uses dynamic anchor boxes to enhance training efficiency [11], while OMG-Seg employs task-specific queries for effective segmentation management [12]. MetaFormer abstracts transformer design, emphasizing architectural frameworks [17]. These advancements underscore transformers' impact on visual segmentation, integrating spatial feature aggregation and global context modeling, as demonstrated by models like Segmenter, which outperform traditional methods on benchmarks like ADE20K and Pascal Context [41,19,22,23]. The ability to integrate local and global features, handle multimodal tasks, and enhance representation learning highlights transformers' adaptability and power in computer vision. Integration with Traditional Models Integrating transformer models with traditional architectures has significantly improved visual segmentation performance. ConvNeXt models modernize ResNet by combining convolutional strengths with transformer innovations for enhanced scalability and performance [20]. Group DETR employs a group-wise approach to object queries, enhancing efficiency and accuracy in complex tasks [42]. CompFeat refines features using a novel attention mechanism, merging transformer and traditional extraction techniques for superior outcomes [43]. MinVIS demonstrates synergy between transformers and traditional methods by processing video frames independently, using query-based approaches for effective tracking with minimal labeled data [44]. These strategies highlight the transformative potential of combining traditional models with advanced transformer capabilities, resulting in robust and efficient visual segmentation processes. Self-Attention Mechanisms Self-attention mechanisms are central to transformer models, offering a sophisticated framework for modeling complex dependencies within image data. Unlike convolutional networks, which struggle with long-range dependencies, self-attention enables dynamic focus on input elements, crucial for aggregating global contextual information and improving segmentation accuracy [2]. Masked autoencoders use self-attention to learn representations by concentrating on relevant input regions [45]. TimeSformer captures temporal relationships across frames, enhancing dynamic scene understanding [46]. Vision-Language Transformer employs multi-head attention for nuanced cross-modal understanding [47]. Hierarchical modules in expression-decoupling frameworks refine temporal comprehension by processing static and motion cues separately [48]. MatteFormer uses self-attention with prior-tokens for precision in image matting [49]. VideoMAE with tube masking illustrates self-attention's role in learning robust video representations [50]. Contextual Autoencoder (CAE) enhances representation quality through self-attention by leveraging relationships between visible and masked patches [39]. Emerging research shows Vision Transformer (ViT) features reveal explicit semantic information, a capacity less pronounced in supervised ViTs and convolutional networks, highlighting self-attention's potential in enhancing semantic understanding [51]. These advancements emphasize self-attention's pivotal role in transformer models, enabling efficient processing and integration of information across contexts and modalities, redefining visual segmentation capabilities. Methodologies of Transformer-Based Visual Segmentation Encoder-Decoder Architectures and Masking Techniques Encoder-decoder architectures are pivotal in transformer-based visual segmentation, effectively modeling spatial and temporal dependencies to enhance segmentation precision. DAB-DETR uses dynamic anchor boxes within Transformer decoders, boosting detection performance and convergence [11]. OMG-Seg employs a transformer encoder-decoder framework to facilitate task interplay during co-training, demonstrating versatility across segmentation tasks [12]. Advanced masking techniques further augment encoder-decoder models. Masked Autoencoders (MAE) utilize asymmetric architectures to reconstruct masked image patches, ensuring feature integrity and enhancing representation quality [16]. Contextual Autoencoder (CAE) employs an encoder-regressor-decoder setup to predict and reconstruct masked representations, stressing the importance of masking in maintaining contextual information [39]. Deformable DETR optimizes segmentation efficiency and accuracy through deformable attention modules focusing on key sampling points [38]. Sparse DETR enhances computational efficiency by selectively updating encoder tokens without sacrificing performance [10]. Lite DETR streamlines multi-scale feature processing by reducing detection head computational demands [40]. These methodologies illustrate the transformative potential of advanced masking techniques in refining segmentation processes. The MOTR method employs track queries to model instances across video sequences, showcasing encoder-decoder architectures' power in dynamic environments [30]. FCOS offers a straightforward framework for object detection by predicting locations directly from feature maps, highlighting encoder-decoder structures' versatility in segmentation methodologies [37]. PoolFormer innovates by replacing transformers' attention module with basic spatial pooling operators, emphasizing fundamental token mixing [17]. Table provides a comprehensive overview of encoder-decoder architectures and masking techniques, illustrating their significant role in advancing visual segmentation methodologies. These methodologies highlight encoder-decoder architectures and masking techniques' significant impact on advancing visual segmentation. Models like Mask2Former and Segmenter leverage masked attention and transformer designs to excel in panoptic, instance, and semantic segmentation tasks. By integrating global context modeling and innovative spatial feature aggregation, these architectures surpass traditional convolutional methods, setting new benchmarks on datasets such as COCO, ADE20K, and Pascal Context, underscoring their vital role in computer vision's evolving landscape [6,19,41,18]. By effectively combining local and global features, these techniques continue to redefine segmentation models' capabilities across diverse visual tasks. Integration with Convolutional Networks Integrating convolutional networks with transformers marks a significant advancement in visual segmentation methodologies, merging convolutional networks' spatial feature aggregation strengths with transformers' dynamic attention and global context modeling capabilities. This synergy enhances spatial feature extraction while leveraging transformers' ability to model global context from the first layer, as demonstrated by architectures like Convolutional vision Transformer (CvT) and Segmenter. These models achieve state-of-the-art results in semantic segmentation tasks across datasets such as ADE20K and Pascal Context, maintaining the advantages of both designs without relying on traditional positional encoding, and demonstrating competitive results with fewer parameters and lower computational costs [19,18,41,4,22]. This hybrid approach addresses traditional convolutional architectures' limitations by incorporating transformer-based innovations, thereby improving segmentation accuracy and efficiency. ConvNeXt exemplifies this integration by adapting standard ConvNet architectures to incorporate transformer-associated features, such as layer normalization and attention mechanisms, leading to enhanced scalability and performance [20]. HTC innovatively combines cascaded refinement of detection and segmentation tasks into a joint processing framework, effectively integrating convolutional networks' strengths with transformer architectures to optimize segmentation outcomes [52]. Benchmark testing of a simple dual-encoder architecture against state-of-the-art models highlights the benefits of integrating traditional approaches for improved performance. This dual-encoder setup combines convolutional networks' robust feature extraction capabilities with transformers' contextual modeling strengths, resulting in superior segmentation accuracy and efficiency [53]. These integration strategies collectively underscore merging convolutional networks with transformers' transformative potential, facilitating more robust and efficient visual segmentation processes. By leveraging both deep learning and cascade architectures' complementary strengths, this hybrid approach revolutionizes segmentation models across diverse visual tasks, including video segmentation, instance segmentation, and semantic segmentation. Recent advancements, such as the Hybrid Task Cascade (HTC) framework, interweave detection and segmentation for improved performance, alongside decoupled body and edge supervision to enhance object consistency and boundary refinement. Additionally, unified models like TarViS demonstrate flexibility across various segmentation tasks by employing abstract queries for target definition, achieving state-of-the-art results without task-specific retraining. These innovations broaden segmentation models' capabilities and applications in fields ranging from autonomous driving to video conferencing [54,52,55,56]. Point Cloud and 3D Segmentation Techniques Segmenting point clouds and 3D data is challenging due to point cloud structures' irregular and sparse nature, which traditional convolutional methods struggle to manage effectively [57]. Transformer models have emerged as powerful tools in this domain, adept at modeling local and global interactions, essential for accurate 3D segmentation. Table presents a detailed comparison of point cloud and 3D segmentation techniques, illustrating their modeling approaches, the challenges they address, and their practical applications in diverse fields. In the pursuit of enhancing 3D segmentation capabilities, Table presents a detailed examination of prominent techniques, showcasing their innovative modeling strategies and addressing critical challenges in the domain. The Point Transformer method exemplifies transformers' application to unordered and unstructured point sets, employing a local-global attention mechanism to relate features from point sets, effectively learning representations from 3D point clouds [58]. This approach addresses existing methods' limitations that fail to generalize well across tasks and domains [59]. Similarly, the Point Cloud Transformer (PCT) utilizes transformer principles to achieve permutation invariance, enhancing point cloud learning by capturing intricate spatial relationships [60]. Pointformer is designed for 3D point clouds, learning features by modeling interactions among points and capturing local and global contexts. This capability marks a significant advancement over traditional methods requiring complex post-processing steps that hinder computational efficiency [61]. The Sparse Cross-Scale Attention Network (SCAN) exemplifies transformers' innovative use by aligning multi-scale sparse features with global voxel-encoded attention to capture long-range relationships in point cloud segmentation [62]. The Generative Shape Proposal Network (GSPN) offers a unique method by generating 3D object proposals through an analysis-by-synthesis approach, reconstructing shapes from point cloud data [63]. This generative strategy complements discriminative techniques employed by other transformer-based methods, broadening 3D segmentation techniques' scope. Panoptic-PolarNet integrates semantic segmentation and instance clustering within a single inference network, demonstrating unified frameworks' potential to enhance 3D segmentation outcomes [64]. V-Net, a fully convolutional neural network, is designed for end-to-end training on 3D MRI volumes, predicting segmentation maps for the entire volume, illustrating these models' versatility in medical imaging applications [65]. These advancements collectively highlight transformer-based techniques' transformative impact in 3D point cloud segmentation, providing more efficient and accurate solutions for complex 3D environments. By effectively integrating local and global features, these techniques continue to redefine segmentation models' capabilities across diverse 3D vision tasks [22]. Video Segmentation and Temporal Context Transformers have revolutionized video segmentation by leveraging their ability to model temporal context and synthesize frame-level information into cohesive video-level understanding. The Video K-Net framework exemplifies this advancement by utilizing learnable kernels to effectively model object appearances and facilitate instance association across frames, providing an end-to-end solution for video panoptic segmentation [66]. This approach contrasts with traditional methods, often requiring separate processing steps for individual frames, underscoring transformers' transformative impact in streamlining video segmentation tasks. The VisTR framework further illustrates temporal and spatial context integration by employing an end-to-end approach to directly predict instance masks from video clips, enhancing efficiency and accuracy [67]. By incorporating temporal dynamics, VisTR improves feature discriminability across frames, providing a robust framework for handling complex video data. Similarly, the SeqFormer model utilizes sequential transformer architectures to aggregate temporal information, learning representations of video-level instances, emphasizing temporal context's significance in achieving accurate segmentation outcomes in dynamic video environments [68]. VideoMAE employs a self-supervised video pre-training method with masked autoencoders, utilizing a high masking ratio to learn robust video representations, demonstrating transformers' effectiveness in capturing temporal dynamics [50]. The Temporally Efficient Vision Transformer (TeViT) focuses on efficiently fusing temporal context in video instance segmentation while minimizing computational overhead, showcasing transformers' potential to optimize temporal processing in video tasks [69]. Additionally, Mask2Former addresses the need for a benchmark that effectively evaluates segmentation architectures' performance in video contexts, emphasizing standardized evaluation metrics' importance [70]. The STEP method highlights video panoptic segmentation's complexities, involving assigning semantic classes and tracking identities to every pixel in a video, demonstrating temporal context's critical role in achieving comprehensive segmentation outcomes [71]. These methodologies illustrate transformer models' transformative impact in advancing video segmentation techniques. They emphasize addressing transformers' lack of inductive biases and computational challenges associated with scaling to handle video data's high dimensionality. By integrating strategies like temporally efficient designs and spatiotemporal query interactions, these approaches enhance capturing long-term temporal dynamics and improve segmentation accuracy and efficiency. This is particularly evident in dynamic video environments, where effectively utilizing temporal context is crucial for superior performance in tasks like video instance segmentation and semantic segmentation, as demonstrated by state-of-the-art results on various benchmarks [41,55,69,23]. Performance Improvements and Challenges Performance Metrics and Evaluation Transformer-based visual segmentation models are evaluated using a variety of metrics across tasks such as 3D vision, semantic segmentation, and video processing. These metrics, including Mean Intersection over Union (mIoU) and Average Precision (AP), highlight the models' ability to capture long-range dependencies and model global context, surpassing traditional convolution-based methods [41,21,22,23]. Video segmentation performance is assessed with metrics like Multiple Object Tracking Accuracy (MOTA) and Higher Order Tracking Accuracy (HOTA), showcasing improvements in dynamic environments [30,66]. Training efficiency metrics, such as detection accuracy and convergence speed, are also crucial, as demonstrated by Deformable DETR's evaluations [38]. OMG-Seg provides a comprehensive framework for assessing transformer models across various tasks [12]. Table provides a detailed overview of representative benchmarks used for evaluating transformer-based visual segmentation models, highlighting the diversity in task formats and performance metrics across different domains. These metrics collectively offer insights into the accuracy, efficiency, and adaptability of transformer-based models. Comparative Analysis of Transformer Models Comparative analyses reveal significant performance enhancements of transformer-based models over traditional approaches in terms of accuracy, adaptability, and computational efficiency. These improvements are attributed to transformers' ability to learn long-range dependencies and manage diverse data representations, as seen in 3D vision, video processing, and image segmentation tasks [21,19,22,23]. Mask2Former achieves state-of-the-art results across multiple tasks, outperforming specialized architectures [6]. In video processing, transformers surpass traditional 3D ConvNets in action classification tasks with lower computational complexity [23]. FCOS competes effectively with traditional object detection frameworks, emphasizing the potential of integrating newer architectures [37]. Contextual Autoencoders (CAE) further demonstrate transformers' adaptability in diverse environments [39]. DINO experiments highlight significant performance improvements in Vision Transformers, achieving top-1 accuracy of 80.1\\ Efficiency and Computational Complexity Efficiency and computational complexity are crucial factors in the adoption and performance of transformer-based visual segmentation models. While these models excel in context modeling and segmentation accuracy, they often demand more computational resources than traditional convolutional methods due to intricate attention mechanisms [21,19,72]. Sparse DETR reduces computation costs by 38 Challenges in Data Requirements Transformer models face significant challenges regarding data requirements, impacting performance across tasks. Their data-intensive nature necessitates large volumes of high-quality data, hindering scalability and real-world application integration [11]. Lite DETR encounters challenges in high-resolution feature processing scenarios, emphasizing data quality's importance [40]. ConvMAE's original masking strategy introduces computational costs and performance gaps between pretraining and fine-tuning phases [73]. Traditional object detection frameworks face slow training convergence and high memory costs [74]. Transformers' inherent limitations, including inductive biases and computational demands, complicate data handling for high-dimensional video inputs [23]. Specific architectural choices and training techniques may not generalize across all datasets or tasks, presenting additional complexity [51]. Future research could enhance models like MEInst in challenging tasks and adapt frameworks for instance-level recognition applications [27]. Mask2Former simplifies segmentation tasks and improves computation focus [6], but managing complex visual tasks remains a concern [17]. Addressing these challenges requires ensuring high data quality, effective preprocessing, and adaptive learning mechanisms to enhance performance, paving the way for resilient and efficient models capable of handling diverse visual data [25,72,22]. Applications and Future Directions Current Applications in Various Domains Transformer-based segmentation models have shown exceptional versatility and efficacy across diverse domains, significantly advancing visual tasks. In urban scene parsing, models like Panoptic SegFormer effectively distinguish between 'things' and 'stuff', crucial for autonomous driving and urban environment analysis [3]. Their adaptability extends to LiDAR panoptic segmentation and 3D detection, highlighting their utility in complex real-world scenarios [29]. In video processing, Video K-Net serves as a unified model for segmentation and tracking, providing an effective baseline for video segmentation tasks [66]. PolyphonicFormer achieves state-of-the-art results on Depth-aware Video Panoptic Segmentation (DVPS) datasets, securing first place in the ICCV-2021 BMTT Challenge for the video + depth track, validating its effectiveness in video object segmentation [5]. TransTrack exhibits competitive results in multiple object tracking, achieving 74.5\\ In 3D segmentation, the Swin Transformer has improved processing of high-resolution images and effectively models various scales, suitable for a wide range of tasks [75]. The CyCTR method enhances segmentation accuracy by leveraging pixel-wise relationships, illustrating the current applications of transformer-based segmentation across different fields [7]. In medical imaging, transformer models significantly enhance brain tumor segmentation accuracy and efficiency, showcasing potential improvements in diagnostic processes [9]. Deformable DETR surpasses the original DETR in detecting small objects with fewer training epochs, broadening applicability in various detection tasks [38]. Furthermore, the Contextual Autoencoder (CAE) demonstrates robustness across semantic segmentation, object detection, instance segmentation, and classification tasks [39]. The OMG-Seg model exemplifies the versatility of transformer-based models by addressing semantic, instance, panoptic, and video segmentation tasks [12]. These applications highlight the transformative impact of transformer-based segmentation models across diverse domains, providing robust solutions for complex visual tasks. By leveraging self-attention mechanisms, transformers excel in learning long-range dependencies, enhancing performance in 3D vision, image classification, and video processing. They surpass traditional convolutional networks' capabilities and reduce reliance on vision-specific inductive biases, driving innovations in computer vision and paving the way for future research directions to tackle existing challenges [76,21,22]. Optimizing Transformer Architectures Optimizing transformer architectures is crucial for improving efficiency, scalability, and applicability across computer vision tasks. Future research should focus on refining mask and patch configurations, as demonstrated by LS-MAE, extending utility beyond computer vision [16]. Optimization of DINO through diverse dataset testing and exploration of additional self-supervised techniques could significantly enhance transformer performance [51]. Adapting dynamic anchor boxes in DAB-DETR to other detection frameworks represents a promising avenue for optimizing transformer architectures, potentially increasing detection accuracy and training efficiency across tasks [11]. OMG-Seg's design principles, prioritizing reduced computational and parameter overhead, exemplify effective strategies for managing multiple segmentation tasks efficiently [12]. Lite DETR offers insights into optimizing DETR-based models, suggesting enhancements in feature handling applicable to other models within the DETR family [40]. The MetaFormer architecture, focusing on simple token mixing strategies, lays the groundwork for future research aimed at improving model performance through architectural innovations [17]. Collectively, these optimization strategies highlight the transformative potential of transformer models in visual analysis, demonstrating their capacity to integrate advanced network and block-level architectures for improved spatial feature aggregation. By leveraging self-attention mechanisms and long-range dependency learning, transformers provide robust, efficient, and versatile solutions adaptable to complex visual analysis tasks across domains, including 3D vision, multimodal learning, and video processing. These models consistently outperform traditional convolutional networks, underscoring their ability to handle varied data representations and processing challenges while paving the way for advancements in overcoming limitations such as inductive biases and computational scaling [25,19,21,22,23]. As research progresses, these models are set to redefine performance and efficiency standards in computer vision. Expanding Applications and Datasets Expanding applications and developing new datasets for transformer-based visual segmentation models are vital for enhancing capabilities and scalability across domains. Future research should optimize existing frameworks and explore novel methodologies to tackle complex challenges in visual segmentation. For example, advancements in TimeSformer's attention mechanism could enhance video-related tasks beyond classification, broadening applications in dynamic video environments [46]. Emerging trends in unsupervised learning techniques and more efficient models highlight potential for cross-domain applications, fostering advancements that improve model adaptability and performance across diverse visual tasks [55]. Further optimizations of matching schemes in Hybrid DETR and testing on additional visual tasks could enhance the utility and scope of transformer-based models [77]. In 3D segmentation, optimizing the clustering process in CMT-DeepLab and adapting the method to other tasks or datasets could extend applicability while addressing computational challenges and maintaining accuracy [78]. Enhancements to the point-wise refinement module in Cylindrical models and adaptations for diverse point cloud scenarios emphasize developing more efficient transformer architectures for 3D data. Exploring improvements in tracking algorithms and extending benchmarks like YouTube-VIS to cover additional categories could facilitate broader applications, enhancing model robustness in varied video contexts [67]. Enhancements in CLOVIS's contrastive learning framework and applicability to complex video scenarios indicate promising directions for expanding applications [13]. Improving Video K-Net's robustness in dynamic environments and extending its applicability to other video segmentation tasks could refine transformer models for video processing [66]. In medical imaging, enhancing model robustness against diverse MRI conditions and expanding applicability to other imaging types could significantly impact diagnostic processes and patient outcomes [9]. These insights emphasize the transformative potential of expanding transformer-based visual segmentation applications and datasets, particularly in improving model efficiency, scalability, and adaptability across diverse visual tasks. These advancements are driven by transformers' ability to learn long-range dependencies, demonstrated in domains like 3D vision, multimodal learning, and image recognition. By replacing traditional convolutional methods, transformers have shown promising results in classification, segmentation, and detection tasks, often outperforming conventional models while requiring fewer computational resources. Ongoing research and development continue to address challenges and explore new directions, solidifying transformers' role in advancing computer vision technologies [76,25,21,22]. Cross-Domain and Multimodal Applications Cross-domain and multimodal applications of transformer models hold substantial potential, driven by their ability to integrate diverse data types and efficiently process complex visual tasks. Expanding datasets to encompass varied scenarios enhances transformer models' robustness and adaptability across domains [79]. Refining evaluation metrics to capture a broader range of performance aspects allows better assessment of these models' capabilities in addressing cross-domain challenges. The integration of video and text processing, exemplified by the MTTR framework, highlights transformers' transformative potential in multimodal applications. This integration emphasizes future research directions aimed at leveraging synergy between data modalities to enhance model performance and versatility [80]. Additionally, the Temporally Efficient Vision Transformer (TeViT) showcases versatility, suggesting potential applications in video analysis tasks beyond video instance segmentation [69]. This versatility indicates broader applicability of transformers in handling complex video contexts and synthesizing information from multiple sources. These advancements illustrate the transformative impact of transformer models, revolutionizing cross-domain and multimodal applications by leveraging strong representation capabilities and learning long-range dependencies. This has led to integrated and efficient solutions across visual tasks, including 3D vision, video processing, and multimodal learning, surpassing traditional convolutional and recurrent networks in performance. As transformers evolve, they pave the way for addressing challenges such as managing high-dimensional data and incorporating inductive biases, enhancing AI's effectiveness and applicability in complex visual domains [25,19,21,22,23]. As research progresses, transformers are poised to redefine benchmarks and establish new standards for performance and adaptability in computer vision. Conclusion This survey delves into the profound influence of transformer models on visual segmentation, focusing on their architectural advancements and integration strategies. The Boundary Squeeze method exemplifies notable progress in both segmentation accuracy and processing efficiency, surpassing previous leading techniques. A novel instance segmentation approach has demonstrated considerable enhancements over Mask R-CNN, maintaining swift processing even on high-resolution images. TarViS emerges as a robust unified framework for video segmentation, achieving leading performance across various benchmarks. The MeViS benchmark addresses the complexities of motion expression-guided video segmentation, providing a critical resource for ongoing research. The introduction of GRES and the gRefCOCO dataset significantly bolsters Referring Expression Segmentation, enabling more thorough evaluations. X-Decoder showcases exceptional adaptability, achieving top-tier results across a spectrum of segmentation tasks, underscoring its versatility. The STEP benchmark is pivotal in advancing video panoptic segmentation research, shaping model evaluation and development. Finally, the unified framework of QueryInst demonstrates superior performance across multiple instance segmentation tasks, highlighting the advantages of integrated methodologies. Collectively, these developments underscore the transformative role of transformer-based visual segmentation, setting the stage for more cohesive and scalable solutions in computer vision.",
  "reference": {
    "1": "2007.09451v1",
    "2": "2004.13621v1",
    "3": "2109.03814v4",
    "4": "2103.15808v1",
    "5": "2112.02582v4",
    "6": "2112.01527v3",
    "7": "2106.02320v4",
    "8": "2104.12763v2",
    "9": "2002.10120v3",
    "10": "2001.05566v5",
    "11": "2111.14330v2",
    "12": "2201.12329v4",
    "13": "2401.10229v2",
    "14": "2207.10661v1",
    "15": "2012.15460v2",
    "16": "2003.08813v1",
    "17": "2210.07224v1",
    "18": "2111.11418v3",
    "19": "2012.15840v3",
    "20": "2211.05781v4",
    "21": "2201.03545v2",
    "22": "2405.08463v1",
    "23": "2208.04309v1",
    "24": "2201.05991v3",
    "25": "1409.0575v3",
    "26": "2206.06488v2",
    "27": "2012.05258v1",
    "28": "2003.11712v2",
    "29": "2203.12827v1",
    "30": "2011.10033v1",
    "31": "2105.03247v4",
    "32": "2003.05664v4",
    "33": "2301.01208v1",
    "34": "2210.00911v2",
    "35": "2111.06377v3",
    "36": "1706.05587v3",
    "37": "2006.09214v3",
    "38": "2010.04159v4",
    "39": "2202.03026v3",
    "40": "2303.07335v1",
    "41": "2105.05633v3",
    "42": "2207.13085v3",
    "43": "2012.03400v1",
    "44": "2208.02245v1",
    "45": "2209.07522v1",
    "46": "2102.05095v4",
    "47": "2108.05565v1",
    "48": "2404.03645v1",
    "49": "2203.15662v1",
    "50": "2203.12602v3",
    "51": "2104.14294v2",
    "52": "1901.07518v2",
    "53": "2102.05918v2",
    "54": "2007.10035v2",
    "55": "2107.01153v4",
    "56": "2301.02657v2",
    "57": "1908.04512v1",
    "58": "2012.09164v2",
    "59": "2111.14819v2",
    "60": "2012.09688v4",
    "61": "2012.11409v3",
    "62": "2201.05972v1",
    "63": "1812.03320v1",
    "64": "2103.14962v1",
    "65": "1606.04797v1",
    "66": "2204.04656v2",
    "67": "1905.04804v4",
    "68": "2112.08275v2",
    "69": "2204.08412v1",
    "70": "2112.10764v1",
    "71": "2102.11859v2",
    "72": "2203.09507v3",
    "73": "2203.06883v1",
    "74": "2211.12860v6",
    "75": "2102.04530v1",
    "76": "2012.00720v2",
    "77": "2301.01156v3",
    "78": "2205.03892v2",
    "79": "2207.08914v1",
    "80": "2103.14030v2",
    "81": "2010.11929v2",
    "82": "2207.13080v3",
    "83": "2206.08948v1",
    "84": "1704.00675v3",
    "85": "2111.14821v2"
  },
  "chooseref": {
    "1": "2102.04530v1",
    "2": "2012.11409v3",
    "3": "2208.04309v1",
    "4": "2201.03545v2",
    "5": "2002.05709v3",
    "6": "2107.01153v4",
    "7": "2405.08463v1",
    "8": "2203.06883v1",
    "9": "2203.16507v2",
    "10": "2010.11929v2",
    "11": "2106.02638v3",
    "12": "1706.03762v7",
    "13": "2003.07853v2",
    "14": "2208.01159v4",
    "15": "1808.00897v1",
    "16": "1909.00179v1",
    "17": "2105.11668v3",
    "18": "2206.08948v1",
    "19": "2107.06263v3",
    "20": "2111.15174v2",
    "21": "1912.04573v4",
    "22": "1608.03609v1",
    "23": "2104.06399v2",
    "24": "2012.03400v1",
    "25": "2003.05664v4",
    "26": "2108.06152v3",
    "27": "2207.08914v1",
    "28": "2202.03026v3",
    "29": "2205.03892v2",
    "30": "2103.15808v1",
    "31": "2107.10224v4",
    "32": "2107.10224v4",
    "33": "2011.10033v1",
    "34": "2201.12329v4",
    "35": "2207.13080v3",
    "36": "2203.03605v4",
    "37": "2203.01305v3",
    "38": "2404.03645v1",
    "39": "1611.07715v2",
    "40": "1512.03385v1",
    "41": "1606.00915v2",
    "42": "2010.04159v4",
    "43": "1811.11168v2",
    "44": "2211.05781v4",
    "45": "2301.03580v2",
    "46": "2006.02334v2",
    "47": "2211.12860v6",
    "48": "1809.02983v4",
    "49": "1909.06121v3",
    "50": "2104.14294v2",
    "51": "2005.12872v3",
    "52": "2111.14821v2",
    "53": "2011.14503v5",
    "54": "2103.15734v2",
    "55": "2210.07224v1",
    "56": "2004.13621v1",
    "57": "2006.09214v3",
    "58": "2204.04654v2",
    "59": "2101.07448v1",
    "60": "1612.03144v2",
    "61": "2007.09451v1",
    "62": "2106.02320v4",
    "63": "1708.02002v2",
    "64": "2012.00720v2",
    "65": "1605.06211v1",
    "66": "2306.00968v1",
    "67": "1812.03320v1",
    "68": "1904.01803v2",
    "69": "2212.11270v1",
    "70": "2107.13154v1",
    "71": "2207.13085v3",
    "72": "2108.13341v2",
    "73": "1901.07518v2",
    "74": "2001.05566v5",
    "75": "1409.0575v3",
    "76": "2007.10035v2",
    "77": "2107.13155v2",
    "78": "2207.10661v1",
    "79": "1906.11109v2",
    "80": "2204.12109v1",
    "81": "2105.01928v3",
    "82": "1908.04512v1",
    "83": "2203.07997v3",
    "84": "2102.05095v4",
    "85": "2106.14855v2",
    "86": "2005.14165v4",
    "87": "1804.09337v1",
    "88": "2210.00911v2",
    "89": "1906.01140v2",
    "90": "2103.00020v1",
    "91": "2011.11964v2",
    "92": "2303.07335v1",
    "93": "1904.11491v1",
    "94": "2104.12763v2",
    "95": "2302.01872v1",
    "96": "2105.03247v4",
    "97": "1902.03604v2",
    "98": "2303.07336v2",
    "99": "2112.11010v2",
    "100": "2112.01526v2",
    "101": "2012.00759v3",
    "102": "2206.02777v3",
    "103": "2003.11712v2",
    "104": "2301.01208v1",
    "105": "2112.10764v1",
    "106": "2111.06377v3",
    "107": "2205.09113v2",
    "108": "2112.09133v2",
    "109": "2112.01527v3",
    "110": "2203.15662v1",
    "111": "2308.08544v1",
    "112": "2111.11418v3",
    "113": "1405.0312v3",
    "114": "2208.02245v1",
    "115": "1608.00272v3",
    "116": "1911.05722v3",
    "117": "2003.08813v1",
    "118": "2205.14354v4",
    "119": "2206.06488v2",
    "120": "2104.11227v1",
    "121": "1904.07392v1",
    "122": "1711.07971v3",
    "123": "1909.11065v6",
    "124": "2102.01558v6",
    "125": "2401.10229v2",
    "126": "2106.04263v5",
    "127": "2012.09688v4",
    "128": "2109.03814v4",
    "129": "1911.10194v3",
    "130": "2103.14962v1",
    "131": "2206.00468v1",
    "132": "2301.00954v4",
    "133": "2107.06278v2",
    "134": "2012.09164v2",
    "135": "2111.14819v2",
    "136": "2004.01658v1",
    "137": "1612.00593v2",
    "138": "1706.02413v1",
    "139": "1912.08193v2",
    "140": "2112.02582v4",
    "141": "2012.00364v4",
    "142": "1612.01105v2",
    "143": "2102.12122v2",
    "144": "2301.01156v3",
    "145": "1706.05587v3",
    "146": "2012.15840v3",
    "147": "1711.08588v2",
    "148": "2112.11037v2",
    "149": "2106.02351v3",
    "150": "2102.11859v2",
    "151": "2102.05918v2",
    "152": "2105.15203v3",
    "153": "2209.08575v1",
    "154": "2105.05633v3",
    "155": "1909.02651v1",
    "156": "2002.10120v3",
    "157": "1708.02551v1",
    "158": "1608.05442v2",
    "159": "1708.03088v1",
    "160": "2305.13173v1",
    "161": "2112.08275v2",
    "162": "2201.05972v1",
    "163": "2111.14330v2",
    "164": "2203.12827v1",
    "165": "2203.12612v6",
    "166": "2111.09883v2",
    "167": "2103.14030v2",
    "168": "2301.02657v2",
    "169": "2204.08412v1",
    "170": "1903.12174v2",
    "171": "2209.07522v1",
    "172": "1704.00675v3",
    "173": "1604.01685v2",
    "174": "2203.09507v3",
    "175": "2209.09554v2",
    "176": "2012.12877v2",
    "177": "2012.15460v2",
    "178": "2205.15361v2",
    "179": "2104.13840v4",
    "180": "1901.03784v2",
    "181": "2211.09808v1",
    "182": "2312.15715v1",
    "183": "1606.04797v1",
    "184": "2206.04403v2",
    "185": "2210.15871v1",
    "186": "1409.1556v6",
    "187": "2106.03348v4",
    "188": "2204.04656v2",
    "189": "2106.03299v1",
    "190": "1905.04804v4",
    "191": "2006.11339v1",
    "192": "2106.13230v1",
    "193": "2201.05991v3",
    "194": "2304.04694v1",
    "195": "2203.12602v3",
    "196": "2012.05258v1",
    "197": "2108.05565v1",
    "198": "1904.02689v2",
    "199": "1809.03327v1",
    "200": "2102.12472v2"
  }
}