{"paper_id": 282652460, "title": "Full-parameter fine-tuning vs. LoRA fine-tuning on PULI models", "author_names": ["Kristóf Varga", "Péter Hatvani", "Zijian Gy˝oz˝o Yang"], "venue": "Proceedings of the International Conference on Formal Methods and Foundations of Artificial Intelligence", "abstract": "In this study, we compare full-parameter fine-tuning and parameter- efficient LoRA on various Hungarian PULI large language models, evaluating their performance across six Hungarian language understanding benchmarks. While full-parameter fine-tuning updates all model weights and requires substantial computational resources, LoRA adapts a smaller subset of parameters, enabling more efficient training. Our experiments on the monolingual PULI 3SX and the multilingual LlumiX and LlumiX-Llama-3.1 models reveal that LoRA consistently matches or surpasses full fine-tuning on most tasks, particularly when applied to larger models. Notably, LlumiXLlama- 3.1 with LoRA achieves state-of-the-art results on five out of six benchmarks while significantly reducing resource demands. These findings highlight LoRA’s potential as a scalable and effective fine-tuning method for Hungarian large language models.", "year": 2025, "publicationdate": null, "externalids": {"DOI": "10.17048/fmfai.2025.226"}, "doi_lower": "10.17048/fmfai.2025.226"}
{"paper_id": 281500962, "title": "SEC-Prompt:SEmantic Complementary Prompting for Few-Shot", "author_names": ["A. D. Details"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 257663528, "title": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation", "author_names": ["Fengji Zhang", "B. Chen", "Yue Zhang", "Jin Liu", "Daoguang Zan", "Yi Mao", "Jian-Guang Lou", "Weizhu Chen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder", "year": 2023, "publicationdate": "2023-03-22", "externalids": {"DOI": "10.48550/arXiv.2303.12570"}, "doi_lower": "10.48550/arxiv.2303.12570"}
{"paper_id": 60617176, "title": "Harmonic coding at 8 kbits/sec", "author_names": ["J. Rodrigues", "L. Almeida"], "venue": "ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing", "abstract": null, "year": 1987, "publicationdate": "1987-04-01", "externalids": {"DOI": "10.1109/ICASSP.1987.1169441"}, "doi_lower": "10.1109/icassp.1987.1169441"}
{"paper_id": 278909620, "title": "OpenDevin: An Open Platform for AI Software Developers as Generalist Agents", "author_names": ["Xingyao Wang", "Boxuan Li", "Yufan Song", "Frank F. Xu", "Xiangru Tang", "Mingchen Zhuge", "Jiayi Pan", "Yueqi Song", "Bowen Li", "Jaskirat Singh", "Hoang H. Tran", "Fuqiang Li", "Ren Ma", "Mingzhang Zheng", "Bill Qian", "Yanjun Shao", "Niklas Muennighoff", "Yizhe Zhang", "Binyuan Hui", "Junyang Lin", "Robert Brennan", "Hao Peng", "Heng Ji", "Graham Neubig"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2407.16741"}, "doi_lower": "10.48550/arxiv.2407.16741"}
{"paper_id": 96524237, "title": "Kristall- und Molekülstruktur von 5.10-Dihydro-5.10-diniethylphenaziniiimtniodid und 5.10-Dihydro-5.10-diethylphenaziniumtriiodid / Molecular and Crystal Structure of 5,10-Dihydro-5.10-dimethylphenaziniumtriiodide and 5.10-Dihydro-5.10-diethylphenazinium triiodide", "author_names": ["H. Keller", "W. Moroni", "D. Nöthe", "M. Scherz", "J. Weiss"], "venue": "", "abstract": "Oxidation of 5,10-dihydro-5,10-dimethylphenazine and 5,10-dihydro-5,10-diethyl-phenazine under different reaction conditions leads to several iodine containing solids. The preparation and X-ray structure of two of them, 5,10-dihydro-5,10-dimethyl-phcnaziniumtriiodide (3) and 5,10-dihydro-5,10-diethylphenaziniumtriiodido (4) are reported here. Compound 3 crystallizes in space group P21/n with lattice parameters a = 8.552(6) Å, b= 16.953(2) Å, c- 12.157(9) Å and β= 103.46(2)° with four formula units in the unit cell. The structure was refined to an R-value of 0.046 using 2387 independent reflections. The lattice constains distinct, slightly distorted triiodide ions and bent 5,10-dihydro-5,10-dimethylphenazinium radical cations. Compound 4 crystallizes in the same space group P21/n with lattice parameters a = 8.531(6) Å, b = 8.332(21) Å, c = 13.320(15) Å and β= 94.44(19)° with two formula units in the unit cell. The structure was refined to an R-value of 0.076 using 1195 independent reflections. The lattice contains strictly linear symmetrical triiodide ions and planar centrosymmetrical 5,10-dihydro-5,10-diethyl- phenazinium radical cations.", "year": 1978, "publicationdate": "1978-08-01", "externalids": {"DOI": "10.1515/znb-1978-0805"}, "doi_lower": "10.1515/znb-1978-0805"}
{"paper_id": 273501653, "title": "MVVM: Deploy Your AI Agents-Securely, Efficiently, Everywhere", "author_names": ["Yiwei Yang", "Aibo Hu", "Yusheng Zheng", "Brian Zhao", "Xinqi Zhang", "Dawei Xiang", "Kexin Chu", "Wei Zhang", "Andi Quinn"], "venue": "", "abstract": "The rise of AI agents powered by Large Language Models (LLMs) presents critical challenges: how to securely execute and migrate these agents across heterogeneous environments while protecting sensitive user data, maintaining availability during network failures, minimizing response latency for time-critical decisions, and ensuring output safety in mission-critical applications. We present MVVM, a WebAssembly-based secure container framework that enables transparent live migration of LLM agent workspaces between edge devices and cloud servers with end-to-end privacy guarantees, resilient multi-tier replication, speculative execution for latency optimization, and integrated validation for safety assurance. MVVM introduces two key innovations: (1) a two-way sandboxing framework leveraging hardware enclaves and accelerator extensions that protects both the agent from malicious hosts and the host from compromised agents; (2) an efficient cross platform migration mechanism using WebAssembly and WASI's platform-agnostic design, enabling seamless movement across ARM phones, RISC-V MCUs, x86 servers, and heterogeneous accelerators; and three astonishing use cases: (1) privacy-aware daemon that automatically determines whether to execute locally or remotely based on data sensitivity and resource availability; (2) multi-tier replication with intelligent quality degradation that maintains service availability despite network failures or resource constraints; (3) a comprehensive execution framework combining speculative execution for 10x latency reduction with parallel validation that ensures output safety without compromising responsiveness. Our evaluation demonstrates that MVVM is validated on three separate devices across 18 workloads.", "year": 2024, "publicationdate": "2024-10-21", "externalids": {}, "doi_lower": null}
{"paper_id": 166760670, "title": "ACCESSIBLE TRANSIT IS PROFITABLE TO EVERYONE.", "author_names": ["Paul Boulinier"], "venue": "", "abstract": null, "year": 1999, "publicationdate": "1999-06-01", "externalids": {}, "doi_lower": null}
{"paper_id": 269293048, "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "author_names": ["Marah Abdin", "Sam Ade Jacobs", "A. A. Awan", "J. Aneja", "Ahmed Awadallah", "H. Awadalla", "Nguyen Bach", "Amit Bahree", "Arash Bakhtiari", "Harkirat Singh Behl", "Alon Benhaim", "Misha Bilenko", "Johan Bjorck", "Sébastien Bubeck", "Martin Cai", "C. C. T. Mendes", "Weizhu Chen", "Vishrav Chaudhary", "Parul Chopra", "Allison Del Giorno", "Gustavo de Rosa", "Matthew Dixon", "Ronen Eldan", "Victor Fragoso", "Dan Iter", "Abhishek Goswami", "S. Gunasekar", "Emman Haider", "Junheng Hao", "Russell J. Hewett", "Jamie Huynh", "Mojan Javaheripi", "Xin Jin", "Piero Kauffmann", "Nikos Karampatziakis", "Dongwoo Kim", "Young Jin Kim", "Mahoud Khademi", "Lev Kurilenko", "James R. Lee", "Yin Tat Lee", "Yuanzhi Li", "Chen Liang", "Weishung Liu", "Eric Lin", "Zeqi Lin", "Piyush Madan", "Arindam Mitra", "Hardik Modi", "Anh Nguyen", "Brandon Norick", "Barun Patra", "D. Perez-Becker", "Thomas Portet", "Reid Pryzant", "Heyang Qin", "Marko Radmilac", "Liliang Ren", "Corby Rosset", "Sambudha Roy", "Olli Saarikivi", "Amin Saied", "Adil Salim", "Michael Santacroce", "Shital Shah", "Ning Shang", "Hiteshi Sharma", "Xianmin Song", "Olatunji Ruwase", "Praneetha Vaddamanu", "Xin Wang", "Rachel Ward", "Guanhua Wang", "P. Witte", "Michael Wyatt", "Can Xu", "Jiahang Xu", "Sonali Yadav", "Fan Yang", "Ziyi Yang", "Donghan Yu", "Cheng-Yuan Zhang", "Cyril Zhang", "Jianwen Zhang", "L. Zhang", "Yi Zhang", "Yunan Zhang", "Xiren Zhou", "Yifan Yang"], "venue": "arXiv.org", "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.", "year": 2024, "publicationdate": "2024-04-22", "externalids": {"DOI": "10.48550/arXiv.2404.14219"}, "doi_lower": "10.48550/arxiv.2404.14219"}
{"paper_id": 257532815, "title": "GPT-4 Technical Report", "author_names": ["OpenAI Josh Achiam", "Steven Adler", "Sandhini Agarwal", "Lama Ahmad", "Ilge Akkaya", "Florencia Leoni Aleman", "Diogo Almeida", "Janko Altenschmidt", "Sam Altman", "Shyamal Anadkat", "Red Avila", "Igor Babuschkin", "S. Balaji", "Valerie Balcom", "Paul Baltescu", "Haim-ing Bao", "Mo Bavarian", "J. Belgum", "Irwan Bello", "Jake Berdine", "Gabriel Bernadett-Shapiro", "Christopher Berner", "Lenny Bogdonoff", "Oleg Boiko", "Made-laine Boyd", "Anna-Luisa Brakman", "Greg Brockman", "Tim Brooks", "Miles Brundage", "Kevin Button", "Trevor Cai", "Rosie Campbell", "Andrew Cann", "Brittany Carey", "Chelsea Carlson", "Rory Carmichael", "Brooke Chan", "Che Chang", "Fotis Chantzis", "Derek Chen", "Sully Chen", "Ruby Chen", "Jason Chen", "Mark Chen", "Benjamin Chess", "Chester Cho", "Casey Chu", "Hyung Won Chung", "Dave Cummings", "Jeremiah Currier", "Yunxing Dai", "Cory Decareaux", "Thomas Degry", "Noah Deutsch", "Damien Deville", "Arka Dhar", "David Dohan", "Steve Dowling", "Sheila Dunning", "Adrien Ecoffet", "Atty Eleti", "Tyna Eloundou", "David Farhi", "L. Fedus", "Niko Felix", "Sim'on Posada Fishman", "Juston Forte", "Is-abella Fulford", "Leo Gao", "Elie Georges", "C. Gibson", "Vik Goel", "Tarun Gogineni", "Gabriel Goh", "Raphael Gontijo-Lopes", "Jonathan Gordon", "Morgan Grafstein", "Scott Gray", "Ryan Greene", "Joshua Gross", "S. Gu", "Yufei Guo", "Chris Hallacy", "Jesse Han", "Jeff Harris", "Yuchen He", "Mike Heaton", "Johannes Heidecke", "Chris Hesse", "Alan Hickey", "W. Hickey", "Peter Hoeschele", "Brandon Houghton", "Kenny Hsu", "Shengli Hu", "Xin Hu", "Joost Huizinga", "Shantanu Jain", "Shawn Jain", "Joanne Jang", "Angela Jiang", "Roger Jiang", "Haozhun Jin", "Denny Jin", "Shino Jomoto", "B. Jonn", "Heewoo Jun", "Tomer Kaftan", "Lukasz Kaiser", "Ali Kamali", "I. Kanitscheider", "N. Keskar", "Tabarak Khan", "Logan Kilpatrick", "Jong Wook Kim", "Christina Kim", "Yongjik Kim", "Hendrik Kirchner", "J. Kiros", "Matthew Knight", "Daniel Kokotajlo", "Lukasz Kondraciuk", "Andrew Kondrich", "Aris Konstantinidis", "Kyle Kosic", "Gretchen Krueger", "Vishal Kuo", "Michael Lampe", "Ikai Lan", "Teddy Lee", "Jan Leike", "Jade Leung", "Daniel Levy", "Chak Li", "Rachel Lim", "Molly Lin", "Stephanie Lin", "Ma-teusz Litwin", "Theresa Lopez", "Ryan Lowe", "Patricia Lue", "A. Makanju", "Kim Malfacini", "Sam Manning", "Todor Markov", "Yaniv Markovski", "Bianca Martin", "Katie Mayer", "Andrew Mayne", "Bob McGrew", "S. McKinney", "Christine McLeavey", "Paul McMillan", "Jake McNeil", "David Medina", "Aalok Mehta", "Jacob Menick", "Luke Metz", "An-drey Mishchenko", "Pamela Mishkin", "Vinnie Monaco", "Evan Morikawa", "Daniel P. Mossing", "Tong Mu", "Mira Murati", "O. Murk", "David M'ely", "Ashvin Nair", "Reiichiro Nakano", "Rajeev Nayak", "Arvind Neelakantan", "Richard Ngo", "Hyeonwoo Noh", "Ouyang Long", "Cullen O'Keefe", "J. Pachocki", "A. Paino", "Joe Palermo", "Ashley Pantuliano", "Giambattista Parascandolo", "J. Parish", "Emy Parparita", "Alexandre Passos", "Mikhail Pavlov", "Andrew Peng", "Adam Perelman", "Filipe de Avila Belbute Peres", "Michael Petrov", "Henrique Pondé de Oliveira Pinto", "Michael Pokorny", "Michelle Pokrass", "Vitchyr H. Pong", "Tolly Powell", "Alethea Power", "Boris Power", "Elizabeth Proehl", "Raul Puri", "Alec Radford", "Jack W. Rae", "Aditya Ramesh", "Cameron Raymond", "Francis Real", "Kendra Rimbach", "Carl Ross", "Bob Rotsted", "Henri Roussez", "N. Ryder", "M. Saltarelli", "Ted Sanders", "Shibani Santurkar", "Girish Sastry", "Heather Schmidt", "David Schnurr", "John Schulman", "Daniel Selsam", "Kyla Sheppard", "Toki Sherbakov", "Jessica Shieh", "Sarah Shoker", "Pranav Shyam", "Szymon Sidor", "Eric Sigler", "Maddie Simens", "Jordan Sitkin", "Katarina Slama", "Ian Sohl", "Benjamin Sokolowsky", "Yang Song", "Natalie Staudacher", "F. Such", "Natalie Summers", "I. Sutskever", "Jie Tang", "N. Tezak", "Madeleine Thompson", "P. Tillet", "Amin Tootoonchian", "Elizabeth Tseng", "Preston Tuggle", "Nick Turley", "Jerry Tworek", "Juan Felipe Cer'on Uribe", "Andrea Vallone", "Arun Vijayvergiya", "Chelsea Voss", "Carroll L. Wainwright", "Justin Jay Wang", "Alvin Wang", "Ben Wang", "Jonathan Ward", "Jason Wei", "CJ Weinmann", "Akila Welihinda", "Peter Welinder", "Jiayi Weng", "Lilian Weng", "Matt Wiethoff", "Dave Willner", "Clemens Winter", "Samuel Wolrich", "Hannah Wong", "Lauren Workman", "Sherwin Wu", "Jeff Wu", "Michael Wu", "Kai Xiao", "Tao Xu", "Sarah Yoo", "Kevin Yu", "Qim-ing Yuan", "Wojciech Zaremba", "Rowan Zellers", "Chong Zhang", "Marvin Zhang", "Shengjia Zhao", "Tianhao Zheng", "Juntang Zhuang", "William Zhuk", "Barret Zoph"], "venue": "", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {}, "doi_lower": null}
{"paper_id": 218486987, "title": "A Transformer-based Approach for Source Code Summarization", "author_names": ["Wasi Uddin Ahmad", "Saikat Chakraborty", "Baishakhi Ray", "Kai-Wei Chang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens’ position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research.", "year": 2020, "publicationdate": "2020-05-01", "externalids": {"DOI": "10.18653/v1/2020.acl-main.449"}, "doi_lower": "10.18653/v1/2020.acl-main.449"}
{"paper_id": 232185260, "title": "Unified Pre-training for Program Understanding and Generation", "author_names": ["Wasi Uddin Ahmad", "Saikat Chakraborty", "Baishakhi Ray", "Kai-Wei Chang"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Code summarization and generation empower conversion between programming language (PL) and natural language (NL), while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks. PLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding. Experiments on code summarization in the English language, code generation, and code translation in seven programming languages show that PLBART outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program repair, clone detection, and vulnerable code detection, demonstrate PLBART’s effectiveness in program understanding. Furthermore, analysis reveals that PLBART learns program syntax, style (e.g., identifier naming convention), logical flow (e.g., “if“ block inside an “else“ block is equivalent to “else if“ block) that are crucial to program semantics and thus excels even with limited annotations.", "year": 2021, "publicationdate": "2021-03-10", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.211"}, "doi_lower": "10.18653/v1/2021.naacl-main.211"}
{"paper_id": 266362863, "title": "Traces of Memorisation in Large Language Models for Code", "author_names": ["Ali Al-Kaswan", "M. Izadi", "Arie van Deursen"], "venue": "International Conference on Software Engineering", "abstract": "Large language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as Software Engineering. Large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. The content of these datasets is memorised and can be extracted by attackers with data extraction attacks. In this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. We adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. We run both benchmarks against a variety of models, and perform a data extraction attack. We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. From the training data that was identified to be potentially extractable we were able to extract 47% from a CodeGen-Mono-16B code completion model. We also observe that models memorise more, as their parameter count grows, and that their pretraining data are also vulnerable to attack. We also find that data carriers are memorised at a higher rate than regular code or documentation and that different model architectures memorise different samples. Data leakage has severe outcomes, so we urge the research community to further investigate the extent of this phenomenon using a wider range of models and extraction techniques in order to build safeguards to mitigate this issue.", "year": 2023, "publicationdate": "2023-12-18", "externalids": {"DOI": "10.1145/3597503.3639133"}, "doi_lower": "10.1145/3597503.3639133"}
{"paper_id": 255570209, "title": "SantaCoder: don't reach for the stars!", "author_names": ["Loubna Ben Allal", "Raymond Li", "Denis Kocetkov", "Chenghao Mou", "Christopher Akiki", "Carlos Muñoz Ferrandis", "Niklas Muennighoff", "Mayank Mishra", "A. Gu", "Manan Dey", "Logesh Kumar Umapathi", "Carolyn Jane Anderson", "Yangtian Zi", "J. Poirier", "Hailey Schoelkopf", "S. Troshin", "Dmitry Abulkhanov", "M. Romero", "M. Lappert", "F. Toni", "Bernardo Garc'ia del R'io", "Qian Liu", "Shamik Bose", "Urvashi Bhattacharyya", "Terry Yue Zhuo", "I. Yu", "Paulo Villegas", "Marco Zocca", "Sourab Mangrulkar", "D. Lansky", "Huu Nguyen", "Danish Contractor", "Luisa Villa", "Jia Li", "Dzmitry Bahdanau", "Yacine Jernite", "S. Hughes", "Daniel Fried", "Arjun Guha", "H. D. Vries", "L. V. Werra"], "venue": "arXiv.org", "abstract": "The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.", "year": 2023, "publicationdate": "2023-01-09", "externalids": {"DOI": "10.48550/arXiv.2301.03988"}, "doi_lower": "10.48550/arxiv.2301.03988"}
{"paper_id": 2923536, "title": "Mining idioms from source code", "author_names": ["Miltiadis Allamanis", "Charles Sutton"], "venue": "SIGSOFT FSE", "abstract": "We present the first method for automatically mining code idioms from a corpus of previously written, idiomatic software projects. We take the view that a code idiom is a syntactic fragment that recurs across projects and has a single semantic purpose. Idioms may have metavariables, such as the body of a for loop. Modern IDEs commonly provide facilities for manually defining idioms and inserting them on demand, but this does not help programmers to write idiomatic code in languages or using libraries with which they are unfamiliar. We present Haggis, a system for mining code idioms that builds on recent advanced techniques from statistical natural language processing, namely, nonparametric Bayesian probabilistic tree substitution grammars. We apply Haggis to several of the most popular open source projects from GitHub. We present a wide range of evidence that the resulting idioms are semantically meaningful, demonstrating that they do indeed recur across software projects and that they occur more frequently in illustrative code examples collected from a Q&A site. Manual examination of the most common idioms indicate that they describe important program concepts, including object creation, exception handling, and resource management.", "year": 2014, "publicationdate": "2014-04-01", "externalids": {"DOI": "10.1145/2635868.2635901"}, "doi_lower": "10.1145/2635868.2635901"}
{"paper_id": 269034476, "title": "AMAZON CODEWHISPERER: EARLY ADOPTION BY INFOSYS", "author_names": [], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 173188048, "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms", "author_names": ["Aida Amini", "Saadia Gabriel", "Shanchuan Lin", "Rik Koncel-Kedziorski", "Yejin Choi", "Hannaneh Hajishirzi"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver by learning to map problems to their operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, we significantly enhance the AQUA-RAT dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model with automatic problem categorization. Our experiments show improvements over competitive baselines in our dataset as well as the AQUA-RAT dataset. The results are still lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at https://math-qa.github.io/math-QA/", "year": 2019, "publicationdate": "2019-05-30", "externalids": {"DOI": "10.18653/v1/N19-1245"}, "doi_lower": "10.18653/v1/n19-1245"}
{"paper_id": 202643539, "title": "Vol. 37, Issue 4, August 2010 / Band 37, Heft 4, August 2010", "author_names": [], "venue": "Transfusion Medicine and Hemotherapy", "abstract": null, "year": 2010, "publicationdate": "2010-07-27", "externalids": {"DOI": "10.1159/000319681"}, "doi_lower": "10.1159/000319681"}
{"paper_id": 268232499, "title": "The Claude 3 Model Family: Opus, Sonnet, Haiku", "author_names": [], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 221543812, "title": "A Tool-Based Perspective on Software Code Maintainability Metrics: A Systematic Literature Review", "author_names": ["Luca Ardito", "Riccardo Coppola", "L. Barbato", "Diego Verga"], "venue": "Scientific Programming", "abstract": "Software maintainability is a crucial property of software projects. It can be defined as the ease with which a software system or component can be modified to be corrected, improved, or adapted to its environment. The software engineering literature proposes many models and metrics to predict the maintainability of a software project statically. However, there is no common accordance with the most dependable metrics or metric suites to evaluate such nonfunctional property. The goals of the present manuscript are as follows: (i) providing an overview of the most popular maintainability metrics according to the related literature; (ii) finding what tools are available to evaluate software maintainability; and (iii) linking the most popular metrics with the available tools and the most common programming languages. To this end, we performed a systematic literature review, following Kitchenham’s SLR guidelines, on the most relevant scientific digital libraries. The SLR outcome provided us with 174 software metrics, among which we identified a set of 15 most commonly mentioned ones, and 19 metric computation tools available to practitioners. We found optimal sets of at most five tools to cover all the most commonly mentioned metrics. The results also highlight missing tool coverage for some metrics on commonly used programming languages and minimal coverage of metrics for newer or less popular programming languages. We consider these results valuable for researchers and practitioners who want to find the best selection of tools to evaluate the maintainability of their projects or to bridge the discussed coverage gaps for newer programming languages.", "year": 2020, "publicationdate": "2020-08-04", "externalids": {"DOI": "10.1155/2020/8840389"}, "doi_lower": "10.1155/2020/8840389"}
{"paper_id": 253116642, "title": "Multi-lingual Evaluation of Code Generation Models", "author_names": ["Ben Athiwaratkun", "Sanjay Krishna Gouda", "Zijian Wang", "Xiaopeng Li", "Yuchen Tian", "Ming Tan", "Wasi Uddin Ahmad", "Shiqi Wang", "Qing Sun", "Mingyue Shang", "Sujan Kumar Gonugondla", "Hantian Ding", "Varun Kumar", "Nathan Fulton", "A. Farahani", "Siddhartha Jain", "Robert Giaquinto", "Haifeng Qian", "M. Ramanathan", "Ramesh Nallapati", "Baishakhi Ray", "Parminder Bhatia", "Sudipta Sengupta", "D. Roth", "Bing Xiang"], "venue": "International Conference on Learning Representations", "abstract": "We present new benchmarks on evaluation code generation models: MBXP and Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming languages and are generated using a scalable conversion framework that transpiles prompts and test cases from the original Python datasets into the corresponding data in the target language. Using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of few-shot prompting to teach the model new languages, and zero-shot translation abilities even on mono-lingual settings. Furthermore, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks. Overall, our benchmarks represents a significant step towards a deeper understanding of language models' code generation abilities. We publicly release our code and datasets at https://github.com/amazon-research/mxeval.", "year": 2022, "publicationdate": "2022-10-27", "externalids": {"DOI": "10.48550/arXiv.2210.14868"}, "doi_lower": "10.48550/arxiv.2210.14868"}
{"paper_id": 274488320, "title": "On Program Synthesis and Large Language Models", "author_names": ["Hans Hüttel"], "venue": "Communications of the ACM", "abstract": "Why it is unlikely new developments in machine intelligence will eventually make programming obsolete.", "year": 2024, "publicationdate": "2024-12-03", "externalids": {"DOI": "10.1145/3680410"}, "doi_lower": "10.1145/3680410"}
{"paper_id": 8236317, "title": "Layer Normalization", "author_names": ["Jimmy Ba", "J. Kiros", "Geoffrey E. Hinton"], "venue": "arXiv.org", "abstract": "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.", "year": 2016, "publicationdate": "2016-07-21", "externalids": {}, "doi_lower": null}
{"paper_id": 259095478, "title": "StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code", "author_names": ["Hannah McLean Babe", "S. Nguyen", "Yangtian Zi", "Arjun Guha", "Molly Q. Feldman", "Carolyn Jane Anderson"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Code LLMs are being rapidly deployed and there is evidence that they can make professional programmers more productive. Current benchmarks for code generation measure whether models generate correct programs given an expert prompt. In this paper, we present a new benchmark containing multiple prompts per problem, written by a specific population of non-expert prompters: beginning programmers. StudentEval contains 1,749 prompts for 48 problems, written by 80 students who have only completed one semester of Python programming. Our students wrote these prompts while working interactively with a Code LLM, and we observed very mixed success rates. We use StudentEval to evaluate 5 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. We analyze the prompts and find significant variation in students' prompting techniques. We also find that nondeterministic LLM sampling could mislead students into thinking that their prompts are more (or less) effective than they actually are, which has implications for how to teach with Code LLMs.", "year": 2023, "publicationdate": "2023-06-07", "externalids": {"DOI": "10.48550/arXiv.2306.04556"}, "doi_lower": "10.48550/arxiv.2306.04556"}
{"paper_id": 263134555, "title": "Qwen Technical Report", "author_names": ["Jinze Bai", "Shuai Bai", "Yunfei Chu", "Zeyu Cui", "Kai Dang", "Xiaodong Deng", "Yang Fan", "Wenhang Ge", "Yu Han", "Fei Huang", "Binyuan Hui", "Luo Ji", "Mei Li", "Junyang Lin", "Runji Lin", "Dayiheng Liu", "Gao Liu", "Chengqiang Lu", "K. Lu", "Jianxin Ma", "Rui Men", "Xingzhang Ren", "Xuancheng Ren", "Chuanqi Tan", "Sinan Tan", "Jianhong Tu", "Peng Wang", "Shijie Wang", "Wei Wang", "Shengguang Wu", "Benfeng Xu", "Jin Xu", "An Yang", "Hao Yang", "Jian Yang", "Jian Yang", "Shusheng Yang", "Yang Yao", "Bowen Yu", "Yu Bowen", "Hongyi Yuan", "Zheng Yuan", "Jianwei Zhang", "Xing Zhang", "Yichang Zhang", "Zhenru Zhang", "Chang Zhou", "Jingren Zhou", "Xiaohuan Zhou", "Tianhang Zhu"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.", "year": 2023, "publicationdate": "2023-09-28", "externalids": {"DOI": "10.48550/arXiv.2309.16609"}, "doi_lower": "10.48550/arxiv.2309.16609"}
{"paper_id": 254823489, "title": "Constitutional AI: Harmlessness from AI Feedback", "author_names": ["Yuntao Bai", "Saurav Kadavath", "Sandipan Kundu", "Amanda Askell", "John Kernion", "Andy Jones", "A. Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "Carol Chen", "Catherine Olsson", "Chris Olah", "Danny Hernandez", "Dawn Drain", "Deep Ganguli", "Dustin Li", "Eli Tran-Johnson", "E. Perez", "Jamie Kerr", "J. Mueller", "Jeffrey Ladish", "J. Landau", "Kamal Ndousse", "Kamilė Lukošiūtė", "Liane Lovitt", "M. Sellitto", "Nelson Elhage", "Nicholas Schiefer", "Noem'i Mercado", "Nova Dassarma", "R. Lasenby", "Robin Larson", "Sam Ringer", "Scott Johnston", "Shauna Kravec", "S. E. Showk", "Stanislav Fort", "Tamera Lanham", "Timothy Telleen-Lawton", "Tom Conerly", "T. Henighan", "Tristan Hume", "Sam Bowman", "Zac Hatfield-Dodds", "Benjamin Mann", "Dario Amodei", "Nicholas Joseph", "Sam McCandlish", "Tom B. Brown", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.", "year": 2022, "publicationdate": "2022-12-15", "externalids": {"DOI": "10.48550/arXiv.2212.08073"}, "doi_lower": "10.48550/arxiv.2212.08073"}
{"paper_id": 262217135, "title": "CodePlan: Repository-Level Coding using LLMs and Planning", "author_names": ["Ramakrishna Bairi", "Atharv Sonwane", "Aditya Kanade", "C. VageeshD", "Arun Shankar Iyer", "Suresh Parthasarathy", "S. Rajamani", "B. Ashok", "Shashank Shet"], "venue": "Proc. ACM Softw. Eng.", "abstract": "Software engineering activities such as package migration, fixing error reports from static analysis or testing, and adding type annotations or other specifications to a codebase, involve pervasively editing the entire repository of code. We formulate these activities as repository-level coding tasks. Recent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in offering high-quality solutions to localized coding problems. Repository-level coding tasks are more involved and cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire repository may be too large to fit into the prompt. We frame repository-level coding as a planning problem and present a task-agnostic, neuro-symbolic framework called CodePlan to solve it. CodePlan synthesizes a multi-step chain-of-edits (plan), where each step results in a call to an LLM on a code location with context derived from the entire repository, previous code changes and task-specific instructions. CodePlan is based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm (symbolic components) with the neural LLMs. We evaluate the effectiveness of CodePlan on two repository-level tasks: package migration (C#) and temporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires inter-dependent changes to many files (between 2–97 files). Coding tasks of this level of complexity have not been automated using LLMs before. Our results show that CodePlan has better match with the ground truth compared to baselines. CodePlan is able to get 5/7 repositories to pass the validity checks (i.e., to build without errors and make correct code edits) whereas the baselines (without planning but with the same type of contextual information as CodePlan) cannot get any of the repositories to pass them. We provide our (non-proprietary) data, evaluation scripts and supplementary material at https://github.com/microsoft/codeplan.", "year": 2023, "publicationdate": "2023-09-21", "externalids": {"DOI": "10.1145/3643757"}, "doi_lower": "10.1145/3643757"}
{"paper_id": 7164502, "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments", "author_names": ["Satanjeev Banerjee", "A. Lavie"], "venue": "IEEvaluation@ACL", "abstract": null, "year": 2005, "publicationdate": "2005-06-01", "externalids": {}, "doi_lower": null}
{"paper_id": 248624529, "title": "A Methodology for Controlling Bias and Fairness in Synthetic Data Generation", "author_names": ["Enrico Barbierato", "M. L. D. Vedova", "D. Tessera", "Daniele Toti", "Nicola Vanoli"], "venue": "Applied Sciences", "abstract": "The development of algorithms, based on machine learning techniques, supporting (or even replacing) human judgment must take into account concepts such as data bias and fairness. Though scientific literature proposes numerous techniques to detect and evaluate these problems, less attention has been dedicated to methods generating intentionally biased datasets, which could be used by data scientists to develop and validate unbiased and fair decision-making algorithms. To this end, this paper presents a novel method to generate a synthetic dataset, where bias can be modeled by using a probabilistic network exploiting structural equation modeling. The proposed methodology has been validated on a simple dataset to highlight the impact of tuning parameters on bias and fairness, as well as on a more realistic example based on a loan approval status dataset. In particular, this methodology requires a limited number of parameters compared to other techniques for generating datasets with a controlled amount of bias and fairness.", "year": 2022, "publicationdate": "2022-05-04", "externalids": {"DOI": "10.3390/app12094619"}, "doi_lower": "10.3390/app12094619"}
{"paper_id": 250144196, "title": "Grounded Copilot: How Programmers Interact with Code-Generating Models", "author_names": ["Shraddha Barke", "M. James", "N. Polikarpova"], "venue": "Proc. ACM Program. Lang.", "abstract": "Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants—with a range of prior experience using the assistant—as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.", "year": 2022, "publicationdate": "2022-06-30", "externalids": {"DOI": "10.1145/3586030"}, "doi_lower": "10.1145/3586030"}
{"paper_id": 266818336, "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism", "author_names": ["DeepSeek-AI Xiao Bi", "Deli Chen", "Guanting Chen", "Shanhuang Chen", "Damai Dai", "C. Deng", "Honghui Ding", "Kai Dong", "Qiushi Du", "Zhe Fu", "Huazuo Gao", "Kaige Gao", "Wenjun Gao", "Ruiqi Ge", "Kang Guan", "Daya Guo", "Jianzhong Guo", "Guangbo Hao", "Zhewen Hao", "Ying He", "Wen-Hui Hu", "Panpan Huang", "Erhang Li", "Guowei Li", "Jiashi Li", "Yao Li", "Y. K. Li", "W. Liang", "Fangyun Lin", "A. Liu", "Bo Liu (Benjamin Liu)", "Wen Liu", "Xiaodong Liu", "Xin Liu", "Yiyuan Liu", "Haoyu Lu", "Shanghao Lu", "Fuli Luo", "Shirong Ma", "X. Nie", "Tian Pei", "Yishi Piao", "Junjie Qiu", "Hui Qu", "Tongzheng Ren", "Z. Ren", "C. Ruan", "Zhangli Sha", "Zhihong Shao", "Jun-Mei Song", "Xuecheng Su", "Jingxiang Sun", "Yaofeng Sun", "Min Tang", "Bing-Li Wang", "Peiyi Wang", "Shiyu Wang", "Yaohui Wang", "Yongji Wang", "Tong Wu", "Yu Wu", "Xin Xie", "Zhenda Xie", "Ziwei Xie", "Yi Xiong", "Hanwei Xu", "R. X. Xu", "Yanhong Xu", "Dejian Yang", "Yu-mei You", "Shuiping Yu", "Xin-yuan Yu", "Bo Zhang", "Haowei Zhang", "Lecong Zhang", "Liyue Zhang", "Mingchuan Zhang", "Minghu Zhang", "Wentao Zhang", "Yichao Zhang", "Chenggang Zhao", "Yao Zhao", "Shangyan Zhou", "Shunfeng Zhou", "Qihao Zhu", "Yuheng Zou"], "venue": "arXiv.org", "abstract": "The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.", "year": 2024, "publicationdate": "2024-01-05", "externalids": {}, "doi_lower": null}
{"paper_id": 268680448, "title": "Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback", "author_names": ["Zhangqian Bi", "Yao Wan", "Zheng Wang", "Hongyu Zhang", "Batu Guan", "Fangxin Lu", "Zili Zhang", "Yulei Sui", "Xuanhua Shi", "Hai Jin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project's context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.", "year": 2024, "publicationdate": "2024-03-25", "externalids": {"DOI": "10.48550/arXiv.2403.16792"}, "doi_lower": "10.48550/arxiv.2403.16792"}
{"paper_id": 268075577, "title": "Taking Flight with Copilot", "author_names": ["Christian Bird", "Denae Ford", "Thomas Zimmermann", "Nicole Forsgren", "Eirini Kalliamvakou", "Travis Lowdermilk", "Idan Gazit"], "venue": "Communications of the ACM", "abstract": "Early insights and opportunities of AI-powered pair-programming tools.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.1145/3589996"}, "doi_lower": "10.1145/3589996"}
{"paper_id": 248177957, "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model", "author_names": ["Sid Black", "Stella Biderman", "Eric Hallahan", "Quentin Anthony", "Leo Gao", "Laurence Golding", "Horace He", "Connor Leahy", "Kyle McDonell", "Jason Phang", "M. Pieler", "USVSN Sai Prashanth", "Shivanshu Purohit", "Laria Reynolds", "J. Tow", "Benqi Wang", "Samuel Weinbach"], "venue": "BIGSCIENCE", "abstract": "We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe GPT-NeoX-20B’s architecture and training, and evaluate its performance. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.", "year": 2022, "publicationdate": "2022-04-14", "externalids": {"DOI": "10.48550/arXiv.2204.06745"}, "doi_lower": "10.48550/arxiv.2204.06745"}
{"paper_id": 245758737, "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow", "author_names": ["Sid Black", "Leo Gao", "Phil Wang", "Connor Leahy", "Stella Biderman"], "venue": "", "abstract": null, "year": 2021, "publicationdate": "2021-03-21", "externalids": {"DOI": "10.5281/ZENODO.5297715"}, "doi_lower": "10.5281/zenodo.5297715"}
{"paper_id": 235554555, "title": "Educating Software and AI Stakeholders About Algorithmic Fairness, Accountability, Transparency and Ethics", "author_names": ["Veronika Bogina", "Alan Hartman", "T. Kuflik", "Avital Shulner-Tal"], "venue": "International Journal of Artificial Intelligence in Education", "abstract": "This paper discusses educating stakeholders of algorithmic systems (systems that apply Artificial Intelligence/Machine learning algorithms) in the areas of algorithmic fairness, accountability, transparency and ethics (FATE). We begin by establishing the need for such education and identifying the intended consumers of educational materials on the topic. We discuss the topics of greatest concern and in need of educational resources; we also survey the existing materials and past experiences in such education, noting the scarcity of suitable material on aspects of fairness in particular. We use an example of a college admission platform to illustrate our ideas. We conclude with recommendations for further work in the area and report on the first steps taken towards achieving this goal in the framework of an academic graduate seminar course, a graduate summer school, an embedded lecture in a software engineering course, and a workshop for high school teachers.", "year": 2021, "publicationdate": "2021-04-21", "externalids": {"DOI": "10.1007/s40593-021-00248-0"}, "doi_lower": "10.1007/s40593-021-00248-0"}
{"paper_id": 237091588, "title": "On the Opportunities and Risks of Foundation Models", "author_names": ["Rishi Bommasani", "Drew A. Hudson", "E. Adeli", "R. Altman", "Simran Arora", "Sydney von Arx", "Michael S. Bernstein", "Jeannette Bohg", "Antoine Bosselut", "E. Brunskill", "Erik Brynjolfsson", "S. Buch", "Dallas Card", "Rodrigo Castellon", "Niladri S. Chatterji", "Annie S. Chen", "Kathleen A. Creel", "Jared Davis", "Dora Demszky", "Chris Donahue", "M. Doumbouya", "Esin Durmus", "Stefano Ermon", "J. Etchemendy", "Kawin Ethayarajh", "L. Fei-Fei", "Chelsea Finn", "Trevor Gale", "Lauren Gillespie", "Karan Goel", "Noah D. Goodman", "S. Grossman", "Neel Guha", "Tatsunori Hashimoto", "Peter Henderson", "John Hewitt", "Daniel E. Ho", "Jenny Hong", "Kyle Hsu", "Jing Huang", "Thomas F. Icard", "Saahil Jain", "Dan Jurafsky", "Pratyusha Kalluri", "Siddharth Karamcheti", "G. Keeling", "Fereshte Khani", "O. Khattab", "Pang Wei Koh", "M. Krass", "Ranjay Krishna", "Rohith Kuditipudi", "Ananya Kumar", "Faisal Ladhak", "Mina Lee", "Tony Lee", "J. Leskovec", "Isabelle Levent", "Xiang Lisa Li", "Xuechen Li", "Tengyu Ma", "Ali Malik", "Christopher D. Manning", "Suvir Mirchandani", "E. Mitchell", "Zanele Munyikwa", "Suraj Nair", "A. Narayan", "D. Narayanan", "Benjamin Newman", "Allen Nie", "Juan Carlos Niebles", "H. Nilforoshan", "Julian Nyarko", "Giray Ogut", "Laurel J. Orr", "Isabel Papadimitriou", "J. Park", "C. Piech", "Eva Portelance", "Christopher Potts", "Aditi Raghunathan", "Robert Reich", "Hongyu Ren", "Frieda Rong", "Yusuf H. Roohani", "Camilo Ruiz", "Jack Ryan", "Christopher R'e", "Dorsa Sadigh", "Shiori Sagawa", "Keshav Santhanam", "Andy Shih", "K. Srinivasan", "Alex Tamkin", "Rohan Taori", "A. Thomas", "Florian Tramèr", "Rose E. Wang", "William Wang", "Bohan Wu", "Jiajun Wu", "Yuhuai Wu", "Sang Michael Xie", "Michihiro Yasunaga", "Jiaxuan You", "M. Zaharia", "Michael Zhang", "Tianyi Zhang", "Xikun Zhang", "Yuhui Zhang", "Lucia Zheng", "Kaitlyn Zhou", "Percy Liang"], "venue": "arXiv.org", "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.", "year": 2021, "publicationdate": "2021-08-16", "externalids": {}, "doi_lower": null}
{"paper_id": 650129, "title": "Learning a Metric for Code Readability", "author_names": ["Raymond P. L. Buse", "Westley Weimer"], "venue": "IEEE Transactions on Software Engineering", "abstract": null, "year": 2010, "publicationdate": "2010-07-01", "externalids": {"DOI": "10.1109/TSE.2009.70"}, "doi_lower": "10.1109/tse.2009.70"}
{"paper_id": 269004450, "title": "Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts", "author_names": ["Weilin Cai", "Juyong Jiang", "Le Qin", "Junwei Cui", "Sunghun Kim", "Jiayi Huang"], "venue": "arXiv.org", "abstract": "Expert parallelism has emerged as a key strategy for distributing the computational workload of sparsely-gated mixture-of-experts (MoE) models across multiple devices, enabling the processing of increasingly large-scale models. However, the All-to-All communication inherent to expert parallelism poses a significant bottleneck, limiting the efficiency of MoE models. Although existing optimization methods partially mitigate this issue, they remain constrained by the sequential dependency between communication and computation operations. To address this challenge, we propose ScMoE, a novel shortcut-connected MoE architecture integrated with an overlapping parallelization strategy. ScMoE decouples communication from its conventional sequential ordering, enabling up to 100% overlap with computation. Compared to the prevalent top-2 MoE baseline, ScMoE achieves speedups of 1.49 times in training and 1.82 times in inference. Moreover, our experiments and analyses indicate that ScMoE not only achieves comparable but in some instances surpasses the model quality of existing approaches.", "year": 2024, "publicationdate": "2024-04-07", "externalids": {"DOI": "10.48550/arXiv.2404.05019"}, "doi_lower": "10.48550/arxiv.2404.05019"}
{"paper_id": 279586232, "title": "A Survey on Mixture of Experts", "author_names": ["Weilin Cai", "Juyong Jiang", "Fan Wang", "Jing Tang", "Sunghun Kim", "Jiayi Huang"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2407.06204"}, "doi_lower": "10.48550/arxiv.2407.06204"}
{"paper_id": 229156229, "title": "Extracting Training Data from Large Language Models", "author_names": ["Nicholas Carlini", "Florian Tramèr", "Eric Wallace", "Matthew Jagielski", "Ariel Herbert-Voss", "Katherine Lee", "Adam Roberts", "Tom B. Brown", "D. Song", "Ú. Erlingsson", "Alina Oprea", "Colin Raffel"], "venue": "USENIX Security Symposium", "abstract": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.", "year": 2020, "publicationdate": "2020-12-14", "externalids": {}, "doi_lower": null}
{"paper_id": 261048815, "title": "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs", "author_names": ["Federico Cassano", "John Gouwar", "F. Lucchetti", "Claire Schlesinger", "Carolyn Jane Anderson", "Michael Greenberg", "Abhinav Jangda", "Arjun Guha"], "venue": "Proc. ACM Program. Lang.", "abstract": "Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as building blocks for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming language. Code LLMs produce impressive results on high-resource programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages that have limited training data available (e.g., OCaml, Racket, and several others). This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach, called MultiPL-T, generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. MultiPL-T translates training data from high-resource languages into training data for low-resource languages in the following way. 1) We use a Code LLM to synthesize unit tests for commented code from a high-resource source language, filtering out faulty tests and code with low test coverage. 2) We use a Code LLM to translate the code from the high-resource source language to a target low-resource language. This gives us a corpus of candidate training data in the target language, but many of these translations are wrong. 3) We use a lightweight compiler to compile the test cases generated in (1) from the source language to the target language, which allows us to filter our obviously wrong translations. The result is a training corpus in the target low-resource language where all items have been validated with test cases. We apply this approach to generate tens of thousands of new, validated training items for five low-resource languages: Julia, Lua, OCaml, R, and Racket, using Python as the source high-resource language. Furthermore, we use an open Code LLM (StarCoderBase) with open training data (The Stack), which allows us to decontaminate benchmarks, train models without violating licenses, and run experiments that could not otherwise be done. Using datasets generated with MultiPL-T, we present fine-tuned versions of StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket that outperform other fine-tunes of these base models on the natural language to code task. We also present Racket fine-tunes for two very recent models, DeepSeek Coder and StarCoder2, to show that MultiPL-T continues to outperform other fine-tuning approaches for low-resource languages. The MultiPL-T approach is easy to apply to new languages, and is significantly more efficient and effective than alternatives such as training longer.", "year": 2023, "publicationdate": "2023-08-19", "externalids": {"DOI": "10.1145/3689735"}, "doi_lower": "10.1145/3689735"}
{"paper_id": 251622387, "title": "A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages", "author_names": ["Federico Cassano", "John Gouwar", "Daniel Nguyen", "S. Nguyen", "Luna Phipps-Costin", "Donald Pinckney", "Ming-Ho Yee", "Yangtian Zi", "Carolyn Jane Anderson", "Molly Q. Feldman", "Arjun Guha", "M. Greenberg", "Abhinav Jangda"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2208.08227"}, "doi_lower": "10.48550/arxiv.2208.08227"}
{"paper_id": 254591305, "title": "ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages", "author_names": ["Yekun Chai", "Shuohuan Wang", "Chao Pang", "Yu Sun", "Hao Tian", "Hua Wu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Software engineers working with the same programming language (PL) may speak different natural languages (NLs) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric. In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs). We release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs. We employ two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translation language modeling that relies on parallel data of many NLs and PLs. Extensive results show that ERNIE-Code outperforms previous multilingual LLMs for PL or NL across a wide range of end tasks of code intelligence, including multilingual code-to-text, text-to-code, code-to-code, and text-to-text generation. We further show its advantage of zero-shot prompting on multilingual code summarization and text-to-text translation. We release our code and pre-trained checkpoints.", "year": 2022, "publicationdate": "2022-12-13", "externalids": {"DOI": "10.48550/arXiv.2212.06742"}, "doi_lower": "10.48550/arxiv.2212.06742"}
{"paper_id": 246430316, "title": "Training and Evaluating a Jupyter Notebook Data Science Assistant", "author_names": ["Shubham Chandel", "Colin B. Clement", "Guillermo Serrato", "Neel Sundaresan"], "venue": "arXiv.org", "abstract": "We study the feasibility of a Data Science assistant powered by a sequence-to-sequence transformer by training a new model JuPyT5 on all publicly available Jupyter Notebook GitHub repositories and developing a new metric: Data Science Problems (DSP). DSP is a collection of 1119 problems curated from 306 pedagogical notebooks with 92 dataset dependencies, natural language and Markdown problem descriptions, and assert-based unit tests. These notebooks were designed to test university students' mastery of various Python implementations of Math and Data Science, and we now leverage them to study the ability of JuPyT5 to understand and pass the tests. We analyze the content of DSP, validate its quality, and we find that given 100 sampling attempts JuPyT5 is able to solve 77.5\\% of the DSP problems. We further present various ablation and statistical analyses and compare DSP to other recent natural language to code benchmarks.", "year": 2022, "publicationdate": "2022-01-30", "externalids": {}, "doi_lower": null}
{"paper_id": 259360395, "title": "A Survey on Evaluation of Large Language Models", "author_names": ["Yu-Chu Chang", "Xu Wang", "Jindong Wang", "Yuan Wu", "Kaijie Zhu", "Hao Chen", "Linyi Yang", "Xiaoyuan Yi", "Cunxiang Wang", "Yidong Wang", "Weirong Ye", "Yue Zhang", "Yi Chang", "Philip S. Yu", "Qian Yang", "Xingxu Xie"], "venue": "ACM Transactions on Intelligent Systems and Technology", "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey", "year": 2023, "publicationdate": "2023-07-06", "externalids": {"DOI": "10.1145/3641289"}, "doi_lower": "10.1145/3641289"}
{"paper_id": 271908889, "title": "Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback", "author_names": ["Zhaofeng Liu", "Jing Su", "Jia Cai", "Jingzhi Yang", "Chenfan Wu"], "venue": "International Conference on Intelligent Computing", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.1007/978-981-97-5669-8_11"}, "doi_lower": "10.1007/978-981-97-5669-8_11"}
{"paper_id": 250920542, "title": "CodeT: Code Generation with Generated Tests", "author_names": ["Bei Chen", "Fengji Zhang", "A. Nguyen", "Daoguang Zan", "Zeqi Lin", "Jian-Guang Lou", "Weizhu Chen"], "venue": "International Conference on Learning Representations", "abstract": "The task of generating code solutions for a given programming problem can benefit from the use of pre-trained language models such as Codex, which can produce multiple diverse samples. However, a major challenge for this task is to select the most appropriate solution from the multiple samples generated by the pre-trained language models. A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases, but the manual creation of such test cases is often costly and time-consuming. In this paper, we propose a novel method, CodeT, that leverages the same pre-trained language models to automatically generate test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios. CodeT then executes the code samples using the generated test cases, and performs a dual execution agreement, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples. We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS and CodeContests, using five different pre-trained language models with varying sizes and capabilities. Our results show that CodeT can significantly improve the performance of code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks. For instance, CodeT improves the pass@1 metric on HumanEval to 65.8%, which represents an absolute improvement of 18.8% over the code-davinci-002 model, and an absolute improvement of more than 20% over the previous state-of-the-art results.", "year": 2022, "publicationdate": "2022-07-21", "externalids": {"DOI": "10.48550/arXiv.2207.10397"}, "doi_lower": "10.48550/arxiv.2207.10397"}
{"paper_id": 248266381, "title": "On the Transferability of Pre-trained Language Models for Low-Resource Programming Languages", "author_names": ["Fuxiang Chen", "F. Fard", "David Lo", "T. Bryksin"], "venue": "IEEE International Conference on Program Comprehension", "abstract": "A recent study by Ahmed and Devanbu reported that using a corpus of code written in multilingual datasets to fine-tune multilingual Pre-trained Language Models (PLMs) achieves higher performance as opposed to using a corpus of code written in just one programming language. However, no analysis was made with respect to fine-tuning monolingual PLMs. Furthermore, some programming languages are inherently different and code written in one language usually cannot be interchanged with the others, i.e., Ruby and Java code possess very different structure. To better understand how monolingual and multilingual PLMs affect different programming languages, we investigate 1) the performance of PLMs on Ruby for two popular Software Engineering tasks: Code Summarization and Code Search, 2) the strategy (to select programming languages) that works well on fine-tuning multilingual PLMs for Ruby, and 3) the performance of the fine-tuned PLMs on Ruby given different code lengths. In this work, we analyze over a hundred of pre-trained and fine-tuned models. Our results show that 1) multilingual PLMs have a lower Performance-to-Time Ratio (the BLEU, METEOR, or MRR scores over the fine-tuning duration) as compared to monolingual PLMs, 2) our proposed strategy to select target programming languages to fine-tune multilingual PLMs is effective — it reduces the time to fine-tune yet achieves higher performance in Code Summarization and Code Search tasks, and 3) our proposed strategy consistently shows good performance on different code lengths.", "year": 2022, "publicationdate": "2022-04-05", "externalids": {"DOI": "10.1145/3524610.3527917"}, "doi_lower": "10.1145/3524610.3527917"}
{"paper_id": 261530434, "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "author_names": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.", "year": 2023, "publicationdate": "2023-09-04", "externalids": {"DOI": "10.48550/arXiv.2309.01431"}, "doi_lower": "10.48550/arxiv.2309.01431"}
{"paper_id": 235755472, "title": "Evaluating Large Language Models Trained on Code", "author_names": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Pondé", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "Scott Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mo Bavarian", "Clemens Winter", "P. Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "Jan Leike", "Josh Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "I. Sutskever", "Wojciech Zaremba"], "venue": "arXiv.org", "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.", "year": 2021, "publicationdate": "2021-07-07", "externalids": {}, "doi_lower": null}
{"paper_id": 253801709, "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks", "author_names": ["Wenhu Chen", "Xueguang Ma", "Xinyi Wang", "William W. Cohen"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github https://github.com/wenhuchen/Program-of-Thoughts", "year": 2022, "publicationdate": "2022-11-22", "externalids": {}, "doi_lower": null}
{"paper_id": 600040, "title": "Tree-to-tree Neural Networks for Program Translation", "author_names": ["Xinyun Chen", "Chang Liu", "D. Song"], "venue": "Neural Information Processing Systems", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.", "year": 2018, "publicationdate": "2018-02-11", "externalids": {}, "doi_lower": null}
{"paper_id": 260380531, "title": "Reducing the Carbon Impact of Generative AI Inference (today and in 2035)", "author_names": ["A. Chien", "Liuzixuan Lin", "H. Nguyen", "V. Rao", "Tristan Sharma", "Rajini Wijayawardana"], "venue": "HotCarbon", "abstract": "Generative AI, exemplified in ChatGPT, Dall-E 2, and Stable Diffusion, are exciting new applications consuming growing quantities of computing. We study the compute, energy, and carbon impacts of generative AI inference. Using ChatGPT as an exemplar, we create a workload model and compare request direction approaches (Local, Balance, CarbonMin), assessing their power use and carbon impacts. Our workload model shows that for ChatGPT-like services, inference dominates emissions, in one year producing 25x the carbon-emissions of training GPT-3. The workload model characterizes user experience, and experiments show that carbon emissions-aware algorithms (CarbonMin) can both maintain user experience and reduce carbon emissions dramatically (35%). We also consider a future scenario (2035 workload and power grids), and show that CarbonMin can reduce emissions by 56%. In both cases, the key is intelligent direction of requests to locations with low-carbon power. Combined with hardware technology advances, CarbonMin can keep emissions increase to only 20% compared to 2022 levels for 55x greater workload. Finally we consider datacenter headroom to increase effectiveness of shifting. With headroom, CarbonMin reduces 2035 emissions by 71%.", "year": 2023, "publicationdate": "2023-07-09", "externalids": {"DOI": "10.1145/3604930.3605705"}, "doi_lower": "10.1145/3604930.3605705"}
{"paper_id": 247951931, "title": "PaLM: Scaling Language Modeling with Pathways", "author_names": ["A. Chowdhery", "Sharan Narang", "Jacob Devlin", "Maarten Bosma", "Gaurav Mishra", "Adam Roberts", "P. Barham", "Hyung Won Chung", "Charles Sutton", "Sebastian Gehrmann", "Parker Schuh", "Kensen Shi", "Sasha Tsvyashchenko", "Joshua Maynez", "Abhishek Rao", "Parker Barnes", "Yi Tay", "Noam Shazeer", "Vinodkumar Prabhakaran", "Emily Reif", "Nan Du", "Ben Hutchinson", "Reiner Pope", "James Bradbury", "Jacob Austin", "M. Isard", "Guy Gur-Ari", "Pengcheng Yin", "Toju Duke", "Anselm Levskaya", "S. Ghemawat", "Sunipa Dev", "H. Michalewski", "Xavier García", "Vedant Misra", "Kevin Robinson", "L. Fedus", "Denny Zhou", "Daphne Ippolito", "D. Luan", "Hyeontaek Lim", "Barret Zoph", "A. Spiridonov", "Ryan Sepassi", "David Dohan", "Shivani Agrawal", "Mark Omernick", "Andrew M. Dai", "Thanumalayan Sankaranarayana Pillai", "Marie Pellat", "Aitor Lewkowycz", "Erica Moreira", "R. Child", "Oleksandr Polozov", "Katherine Lee", "Zongwei Zhou", "Xuezhi Wang", "Brennan Saeta", "Mark Díaz", "Orhan Firat", "Michele Catasta", "Jason Wei", "K. Meier-Hellstern", "D. Eck", "J. Dean", "Slav Petrov", "Noah Fiedel"], "venue": "Journal of machine learning research", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "year": 2022, "publicationdate": "2022-04-05", "externalids": {}, "doi_lower": null}
{"paper_id": 253018554, "title": "Scaling Instruction-Finetuned Language Models", "author_names": ["Hyung Won Chung", "Le Hou", "S. Longpre", "Barret Zoph", "Yi Tay", "W. Fedus", "Eric Li", "Xuezhi Wang", "Mostafa Dehghani", "Siddhartha Brahma", "Albert Webson", "S. Gu", "Zhuyun Dai", "Mirac Suzgun", "Xinyun Chen", "A. Chowdhery", "Dasha Valter", "Sharan Narang", "Gaurav Mishra", "Adams Wei Yu", "Vincent Zhao", "Yanping Huang", "Andrew M. Dai", "Hongkun Yu", "Slav Petrov", "Ed H. Chi", "J. Dean", "Jacob Devlin", "Adam Roberts", "Denny Zhou", "Quoc V. Le", "Jason Wei"], "venue": "Journal of machine learning research", "abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.", "year": 2022, "publicationdate": "2022-10-20", "externalids": {"DOI": "10.48550/arXiv.2210.11416"}, "doi_lower": "10.48550/arxiv.2210.11416"}
{"paper_id": 239998651, "title": "Training Verifiers to Solve Math Word Problems", "author_names": ["K. Cobbe", "Vineet Kosaraju", "Mo Bavarian", "Mark Chen", "Heewoo Jun", "Lukasz Kaiser", "Matthias Plappert", "Jerry Tworek", "Jacob Hilton", "Reiichiro Nakano", "Christopher Hesse", "John Schulman"], "venue": "arXiv.org", "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.", "year": 2021, "publicationdate": "2021-10-27", "externalids": {}, "doi_lower": null}
{"paper_id": 202643539, "title": "Vol. 37, Issue 4, August 2010 / Band 37, Heft 4, August 2010", "author_names": [], "venue": "Transfusion Medicine and Hemotherapy", "abstract": null, "year": 2010, "publicationdate": "2010-07-27", "externalids": {"DOI": "10.1159/000319681"}, "doi_lower": "10.1159/000319681"}
{"paper_id": 270560319, "title": "CodeGemma: Open Code Models Based on Gemma", "author_names": ["CodeGemma Team Heri Zhao", "Jef-frey Hui", "Joshua Howland", "Nam Nguyen", "Siqi Zuo", "A. Hu", "Christopher A. Choquette-Choo", "Jingyue Shen", "Joe Kelley", "Kshi-tij Bansal", "Luke Vilnis", "Mateo Wirth", "Paul Michel", "Peter Choy", "Pratik Joshi", "Ravin Kumar", "Sarmad Hashmi", "Shubham Agrawal", "Zhitao Gong", "Jane Fine", "Tris Warkentin", "A. Hartman", "Bin Ni", "Kathy Korevec", "Kelly Schaefer", "Scott Huffman"], "venue": "arXiv.org", "abstract": "This paper introduces CodeGemma, a collection of specialized open code models built on top of Gemma, capable of a variety of code and natural language generation tasks. We release three model variants. CodeGemma 7B pretrained (PT) and instruction-tuned (IT) variants have remarkably resilient natural language understanding, excel in mathematical reasoning, and match code capabilities of other open models. CodeGemma 2B is a state-of-the-art code completion model designed for fast code infilling and open-ended generation in latency-sensitive settings.", "year": 2024, "publicationdate": "2024-06-17", "externalids": {"DOI": "10.48550/arXiv.2406.11409"}, "doi_lower": "10.48550/arxiv.2406.11409"}
{"paper_id": 282738888, "title": "VisionCAD: An Integration-Free Radiology Copilot Framework", "author_names": ["Jiaming Li", "Junlei Wu", "Sheng Wang", "Honglin Xiong", "Jiangdong Cai", "Zihao Zhao", "Yitao Zhu", "Yuan Yin", "Dinggang Shen", "Qian Wang"], "venue": "arXiv.org", "abstract": "Widespread clinical deployment of computer-aided diagnosis (CAD) systems is hindered by the challenge of integrating with existing hospital IT infrastructure. Here, we introduce VisionCAD, a vision-based radiological assistance framework that circumvents this barrier by capturing medical images directly from displays using a camera system. The framework operates through an automated pipeline that detects, restores, and analyzes on-screen medical images, transforming camera-captured visual data into diagnostic-quality images suitable for automated analysis and report generation. We validated VisionCAD across diverse medical imaging datasets, demonstrating that our modular architecture can flexibly utilize state-of-the-art diagnostic models for specific tasks. The system achieves diagnostic performance comparable to conventional CAD systems operating on original digital images, with an F1-score degradation typically less than 2\\% across classification tasks, while natural language generation metrics for automated reports remain within 1\\% of those derived from original images. By requiring only a camera device and standard computing resources, VisionCAD offers an accessible approach for AI-assisted diagnosis, enabling the deployment of diagnostic capabilities in diverse clinical settings without modifications to existing infrastructure.", "year": 2025, "publicationdate": "2025-11-01", "externalids": {"DOI": "10.48550/arXiv.2511.00381"}, "doi_lower": "10.48550/arxiv.2511.00381"}
{"paper_id": 268958653, "title": "Should I Be Scared Of First AI Software Engineer Or Not ?", "author_names": ["Basit Durrani", "Daivik Jain", "Er.Basit Durrani", "Er. Daivik Jain"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 17181517, "title": "Inducing Tree-Substitution Grammars", "author_names": ["Trevor Cohn", "Phil Blunsom", "S. Goldwater"], "venue": "Journal of machine learning research", "abstract": null, "year": 2010, "publicationdate": "2010-03-01", "externalids": {"DOI": "10.5555/1756006.1953031"}, "doi_lower": "10.5555/1756006.1953031"}
{"paper_id": 15912959, "title": "Z3: An Efficient SMT Solver", "author_names": ["L. D. Moura", "Nikolaj S. Bjørner"], "venue": "International Conference on Tools and Algorithms for Construction and Analysis of Systems", "abstract": null, "year": 2008, "publicationdate": "2008-03-29", "externalids": {"DOI": "10.1007/978-3-540-78800-3_24"}, "doi_lower": "10.1007/978-3-540-78800-3_24"}
{"paper_id": 258841328, "title": "QLoRA: Efficient Finetuning of Quantized LLMs", "author_names": ["Tim Dettmers", "Artidoro Pagnoni", "Ari Holtzman", "Luke Zettlemoyer"], "venue": "Neural Information Processing Systems", "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14314"}, "doi_lower": "10.48550/arxiv.2305.14314"}
{"paper_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "author_names": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/N19-1423"}, "doi_lower": "10.18653/v1/n19-1423"}
{"paper_id": 264172238, "title": "CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion", "author_names": ["Yangruibo Ding", "Zijian Wang", "Wasi Uddin Ahmad", "Hantian Ding", "Ming Tan", "Nihal Jain", "M. K. Ramanathan", "Ramesh Nallapati", "Parminder Bhatia", "Dan Roth", "Bing Xiang"], "venue": "Neural Information Processing Systems", "abstract": "Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly. To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file. Extensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt. However, despite such improvements, the pinnacle of performance remains notably unattained even with the highest-performing model, indicating that CrossCodeEval is also capable of assessing model's capability in leveraging extensive context to make better code completion. Finally, we benchmarked various methods in retrieving cross-file context, and show that CrossCodeEval can also be used to measure the capability of code retrievers.", "year": 2023, "publicationdate": "2023-10-17", "externalids": {"DOI": "10.48550/arXiv.2310.11248"}, "doi_lower": "10.48550/arxiv.2310.11248"}
{"paper_id": 254877371, "title": "CoCoMIC: Code Completion by Jointly Modeling In-file and Cross-file Context", "author_names": ["Yangruibo Ding", "Zijian Wang", "Wasi Uddin Ahmad", "M. Ramanathan", "Ramesh Nallapati", "Parminder Bhatia", "D. Roth", "Bing Xiang"], "venue": "International Conference on Language Resources and Evaluation", "abstract": "While pre-trained language models (LM) for code have achieved great success in code completion, they generate code conditioned only on the contents within the file, i.e., in-file context, but ignore the rich semantics in other files within the same project, i.e., project-level cross-file context, a critical source of information that is especially useful in modern modular software development. Such overlooking constrains code LMs’ capacity in code completion, leading to unexpected behaviors such as generating hallucinated class member functions or function calls with unexpected arguments. In this work, we propose CoCoMIC, a novel framework that jointly learns the in-file and cross-file context on top of code LMs. To empower CoCoMIC, we develop CCFinder, a static-analysis-based tool that locates and retrieves the most relevant project-level cross-file context for code completion. CoCoMIC successfully improves the existing code LM with a 33.94% relative increase in exact match and 28.69% in identifier matching for code completion when the cross-file context is provided. Finally, we perform a series of ablation studies and share valuable insights for future research on integrating cross-file context into code LMs.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10007"}, "doi_lower": "10.48550/arxiv.2212.10007"}
{"paper_id": 263886074, "title": "A Survey for In-context Learning", "author_names": ["Qingxiu Dong", "Lei Li", "Damai Dai", "Ce Zheng", "Zhiyong Wu", "Baobao Chang", "Xu Sun", "Jingjing Xu", "Lei Li", "Zhifang Sui"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 269128474, "title": "Evaluating Large Language Models in Class-Level Code Generation", "author_names": ["Xueying Du", "Mingwei Liu", "Kaixin Wang", "Hanlin Wang", "Junwei Liu", "Yixuan Chen", "Jiayi Feng", "Chaofeng Sha", "Xin Peng", "Yiling Lou"], "venue": "International Conference on Software Engineering", "abstract": "Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a sim-ple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios. To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the method-level. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers' expertise to build more LLM benchmarks based on practical and complicated software development scenarios.", "year": 2024, "publicationdate": "2024-04-12", "externalids": {"DOI": "10.1145/3597503.3639219"}, "doi_lower": "10.1145/3597503.3639219"}
{"paper_id": 107563353, "title": "Training from Scratch.", "author_names": ["N. Kuhn"], "venue": "", "abstract": null, "year": 1998, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 263671720, "title": "Large Language Models for Software Engineering: Survey and Open Problems", "author_names": ["Angela Fan", "Beliz Gokkaya", "Mark Harman", "Mitya Lyubarskiy", "Shubho Sengupta", "Shin Yoo", "Jie M. Zhang"], "venue": "2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE)", "abstract": "This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.", "year": 2023, "publicationdate": "2023-05-14", "externalids": {"DOI": "10.1109/ICSE-FoSE59343.2023.00008"}, "doi_lower": "10.1109/icse-fose59343.2023.00008"}
{"paper_id": 211171605, "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "author_names": ["Zhangyin Feng", "Daya Guo", "Duyu Tang", "Nan Duan", "Xiaocheng Feng", "Ming Gong", "Linjun Shou", "Bing Qin", "Ting Liu", "Daxin Jiang", "Ming Zhou"], "venue": "Findings", "abstract": "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both “bimodal” data of NL-PL pairs and “unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.", "year": 2020, "publicationdate": "2020-02-19", "externalids": {"DOI": "10.18653/v1/2020.findings-emnlp.139"}, "doi_lower": "10.18653/v1/2020.findings-emnlp.139"}
{"paper_id": 248157108, "title": "InCoder: A Generative Model for Code Infilling and Synthesis", "author_names": ["Daniel Fried", "Armen Aghajanyan", "Jessy Lin", "Sida I. Wang", "Eric Wallace", "Freda Shi", "Ruiqi Zhong", "Wen-tau Yih", "Luke Zettlemoyer", "M. Lewis"], "venue": "International Conference on Learning Representations", "abstract": "Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. https://sites.google.com/view/incoder-code-models", "year": 2022, "publicationdate": "2022-04-12", "externalids": {"DOI": "10.48550/arXiv.2204.05999"}, "doi_lower": "10.48550/arxiv.2204.05999"}
{"paper_id": 253708270, "title": "PAL: Program-aided Language Models", "author_names": ["Luyu Gao", "Aman Madaan", "Shuyan Zhou", "Uri Alon", "Pengfei Liu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "venue": "International Conference on Machine Learning", "abstract": "Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as\"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .", "year": 2022, "publicationdate": "2022-11-18", "externalids": {"DOI": "10.48550/arXiv.2211.10435"}, "doi_lower": "10.48550/arxiv.2211.10435"}
{"paper_id": 266359151, "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "author_names": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Qianyu Guo", "Meng Wang", "Haofen Wang"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.", "year": 2023, "publicationdate": "2023-12-18", "externalids": {}, "doi_lower": null}
{"paper_id": 266844690, "title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution", "author_names": ["Alex Gu", "Baptiste Rozière", "Hugh Leather", "Armando Solar-Lezama", "Gabriel Synnaeve", "Sida Wang"], "venue": "International Conference on Machine Learning", "abstract": "We present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a generic recipe for generating our execution benchmark which can be used to create future variation of the benchmark. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval do not show the same improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction, highlighting the gap between open and closed source models. As no model is close to acing CRUXEval, we provide examples of consistent GPT-4 failures on simple programs as a lens into its code reasoning capabilities and areas for improvement.", "year": 2024, "publicationdate": "2024-01-05", "externalids": {"DOI": "10.48550/arXiv.2401.03065"}, "doi_lower": "10.48550/arxiv.2401.03065"}
{"paper_id": 3015769, "title": "Dimensions in program synthesis", "author_names": ["M. Gyssens", "J. Paredaens", "D. V. Gucht", "G. Fletcher"], "venue": "Formal Methods in Computer-Aided Design", "abstract": null, "year": 2010, "publicationdate": "2010-07-26", "externalids": {"DOI": "10.1145/1836089.1836091"}, "doi_lower": "10.1145/1836089.1836091"}
{"paper_id": 202643539, "title": "Vol. 37, Issue 4, August 2010 / Band 37, Heft 4, August 2010", "author_names": [], "venue": "Transfusion Medicine and Hemotherapy", "abstract": null, "year": 2010, "publicationdate": "2010-07-27", "externalids": {"DOI": "10.1159/000319681"}, "doi_lower": "10.1159/000319681"}
{"paper_id": 259203998, "title": "Textbooks Are All You Need", "author_names": ["Suriya Gunasekar", "Yi Zhang", "J. Aneja", "C. C. T. Mendes", "Allison Del Giorno", "Sivakanth Gopi", "Mojan Javaheripi", "Piero Kauffmann", "Gustavo de Rosa", "Olli Saarikivi", "A. Salim", "S. Shah", "Harkirat Singh Behl", "Xin Wang", "Sébastien Bubeck", "Ronen Eldan", "A. Kalai", "Y. Lee", "Yuan-Fang Li"], "venue": "arXiv.org", "abstract": "We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\"data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.", "year": 2023, "publicationdate": "2023-06-20", "externalids": {}, "doi_lower": null}
{"paper_id": 247315559, "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation", "author_names": ["Daya Guo", "Shuai Lu", "Nan Duan", "Yanlin Wang", "Ming Zhou", "Jian Yin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference. In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language. The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation. To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree. Furthermore, we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning, and then align representations among programming languages using a cross-modal generation task. We evaluate UniXcoder on five code-related tasks over nine datasets. To further evaluate the performance of code fragment representation, we also construct a dataset for a new task, called zero-shot code-to-code search. Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.", "year": 2022, "publicationdate": "2022-03-08", "externalids": {"DOI": "10.48550/arXiv.2203.03850"}, "doi_lower": "10.48550/arxiv.2203.03850"}
{"paper_id": 221761146, "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "author_names": ["Daya Guo", "Shuo Ren", "Shuai Lu", "Zhangyin Feng", "Duyu Tang", "Shujie Liu", "Long Zhou", "Nan Duan", "Jian Yin", "Daxin Jiang", "M. Zhou"], "venue": "International Conference on Learning Representations", "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.", "year": 2020, "publicationdate": "2020-09-17", "externalids": {}, "doi_lower": null}
{"paper_id": 259262301, "title": "LongCoder: A Long-Range Pre-trained Language Model for Code Completion", "author_names": ["Daya Guo", "Canwen Xu", "Nan Duan", "Jian Yin", "Julian McAuley"], "venue": "International Conference on Machine Learning", "abstract": "In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens - bridge tokens and memory tokens - to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference. All the codes and data are available at https://github.com/microsoft/CodeBERT.", "year": 2023, "publicationdate": "2023-06-26", "externalids": {"DOI": "10.48550/arXiv.2306.14893"}, "doi_lower": "10.48550/arxiv.2306.14893"}
{"paper_id": 267211867, "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence", "author_names": ["Daya Guo", "Qihao Zhu", "Dejian Yang", "Zhenda Xie", "Kai Dong", "Wentao Zhang", "Guanting Chen", "Xiao Bi", "Yu Wu", "Y. K. Li", "Fuli Luo", "Yingfei Xiong", "W. Liang"], "venue": "arXiv.org", "abstract": "The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.", "year": 2024, "publicationdate": "2024-01-25", "externalids": {"DOI": "10.48550/arXiv.2401.14196"}, "doi_lower": "10.48550/arxiv.2401.14196"}
{"paper_id": 234336067, "title": "Transitioning from Real to Synthetic data: Quantifying the bias in model", "author_names": ["Aman Gupta", "Deepak L. Bhatt", "Anubha Pandey"], "venue": "arXiv.org", "abstract": "With the advent of generative modeling techniques, synthetic data and its use has penetrated across various domains from unstructured data such as image, text to structured dataset modeling healthcare outcome, risk decisioning in financial domain, and many more. It overcomes various challenges such as limited training data, class imbalance, restricted access to dataset owing to privacy issues. To ensure the trained model used for automated decisioning purposes makes a fair decision there exist prior work to quantify and mitigate those issues. This study aims to establish a trade-off between bias and fairness in the models trained using synthetic data. Variants of synthetic data generation techniques were studied to understand bias amplification including differentially private generation schemes. Through experiments on a tabular dataset, we demonstrate there exist a varying levels of bias impact on models trained using synthetic data. Techniques generating less correlated feature performs well as evident through fairness metrics with 94\\%, 82\\%, and 88\\% relative drop in DPD (demographic parity difference), EoD (equality of odds) and EoP (equality of opportunity) respectively, and 24\\% relative improvement in DRP (demographic parity ratio) with respect to the real dataset. We believe the outcome of our research study will help data science practitioners understand the bias in the use of synthetic data.", "year": 2021, "publicationdate": "2021-05-10", "externalids": {}, "doi_lower": null}
{"paper_id": 267027552, "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture", "author_names": ["M. A. D. L. Balaguer", "Vinamra Benara", "Renato Luiz de Freitas Cunha", "Roberto de M. Estevao Filho", "Todd Hendry", "Daniel Holstein", "Jennifer Marsman", "Nick Mecklenburg", "S. Malvar", "Leonardo Nunes", "Rafael Padilha", "Morris Sharp", "B. Silva", "Swati Sharma", "Vijay Aski", "Ranveer Chandra"], "venue": "arXiv.org", "abstract": "There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.", "year": 2024, "publicationdate": "2024-01-16", "externalids": {"DOI": "10.48550/arXiv.2401.08406"}, "doi_lower": "10.48550/arxiv.2401.08406"}
{"paper_id": 256662452, "title": "CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models", "author_names": ["Hossein Hajipour", "Thorsten Holz", "Lea Schonherr", "Mario Fritz"], "venue": "2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)", "abstract": "Large language models (LLMs) for automatic code generation have recently achieved breakthroughs in several programming tasks. Their advances in competition-level programming problems have made them an essential pillar of AI-assisted pair programming, and tools such as GitHub Copilot have emerged as part of the daily programming workflow used by millions of developers. Training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure. While these models have been extensively evaluated for their ability to produce functionally correct programs, there remains a lack of comprehensive investigations and benchmarks addressing the security aspects of these models.In this work, we propose a method to systematically study the security issues of code language models to assess their susceptibility to generating vulnerable code. To this end, we introduce the first approach to automatically find generated code that contains vulnerabilities in black-box code generation models. This involves proposing a novel few-shot prompting approach. We evaluate the effectiveness of our approach by examining code language models in generating high-risk security weaknesses. Furthermore, we use our method to create a collection of diverse non-secure prompts for various vulnerability scenarios. This dataset serves as a benchmark to evaluate and compare the security weaknesses of code language models.", "year": 2023, "publicationdate": "2023-02-08", "externalids": {"DOI": "10.1109/SaTML59370.2024.00040"}, "doi_lower": "10.1109/satml59370.2024.00040"}
{"paper_id": 258218228, "title": "Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study", "author_names": ["Perttu Hämäläinen", "Mikke Tavast", "Anton Kunnari"], "venue": "International Conference on Human Factors in Computing Systems", "abstract": "Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) research. Motivated by this, we explore the potential of large language models (LLMs) in generating synthetic user research data. We use OpenAI’s GPT-3 model to generate open-ended questionnaire responses about experiencing video games as art, a topic not tractable with traditional computational user models. We test whether synthetic responses can be distinguished from real responses, analyze errors of synthetic data, and investigate content similarities between synthetic and real data. We conclude that GPT-3 can, in this context, yield believable accounts of HCI experiences. Given the low cost and high speed of LLM data generation, synthetic data should be useful in ideating and piloting new experiments, although any findings must obviously always be validated with real data. The results also raise concerns: if employed by malicious users of crowdsourcing services, LLMs may make crowdsourcing of self-report data fundamentally unreliable.", "year": 2023, "publicationdate": "2023-04-19", "externalids": {"DOI": "10.1145/3544548.3580688"}, "doi_lower": "10.1145/3544548.3580688"}
{"paper_id": 258865812, "title": "Reasoning with Language Model is Planning with World Model", "author_names": ["Shibo Hao", "Yi Gu", "Haodi Ma", "Joshua Jiahua Hong", "Zhen Wang", "D. Wang", "Zhiting Hu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.14992"}, "doi_lower": "10.48550/arxiv.2305.14992"}
{"paper_id": 206594692, "title": "Deep Residual Learning for Image Recognition", "author_names": ["Kaiming He", "X. Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "year": 2015, "publicationdate": "2015-12-10", "externalids": {"DOI": "10.1109/cvpr.2016.90"}, "doi_lower": "10.1109/cvpr.2016.90"}
{"paper_id": 232134851, "title": "Measuring Mathematical Problem Solving With the MATH Dataset", "author_names": ["Dan Hendrycks", "Collin Burns", "Saurav Kadavath", "Akul Arora", "Steven Basart", "Eric Tang", "D. Song", "J. Steinhardt"], "venue": "NeurIPS Datasets and Benchmarks", "abstract": "Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.", "year": 2021, "publicationdate": "2021-03-05", "externalids": {}, "doi_lower": null}
{"paper_id": 233354284, "title": "Code of Conduct Conversations in Open Source Software Projects on Github", "author_names": ["Renee Li", "Pavitthra Pandurangan", "Hana Frluckaj", "Laura A. Dabbish"], "venue": "Proc. ACM Hum. Comput. Interact.", "abstract": "The rapid growth of open source software necessitates a deeper understanding of moderation and governance methods currently used within these projects. The code of conduct, a set of rules articulating standard behavior and responsibilities for participation within a community, is becoming an increasingly common policy document in open source software projects for setting project norms of behavior and discouraging negative or harassing comments and conversation. This study describes the conversations around adopting and crafting a code of conduct as well as those utilizing code of conduct for community governance. We conduct a qualitative analysis of a random sample of GitHub issues that involve the code of conduct. We find that codes of conduct are used both proactively and reactively to govern community behavior in project issues. Oftentimes, the initial addition of a code of conduct does not involve much community participation and input. However, a controversial moderation act is capable of inciting mass community feedback and backlash. Project maintainers balance the tension between disciplining potentially offensive forms of speech and encouraging broad and inclusive participation. These results have implications for the design of inclusive and effective governance practices for open source software communities.", "year": 2021, "publicationdate": "2021-04-22", "externalids": {"DOI": "10.1145/3449093"}, "doi_lower": "10.1145/3449093"}
{"paper_id": 247778764, "title": "Training Compute-Optimal Large Language Models", "author_names": ["Jordan Hoffmann", "Sebastian Borgeaud", "A. Mensch", "Elena Buchatskaya", "Trevor Cai", "Eliza Rutherford", "Diego de Las Casas", "Lisa Anne Hendricks", "Johannes Welbl", "Aidan Clark", "Tom Hennigan", "Eric Noland", "Katie Millican", "George van den Driessche", "Bogdan Damoc", "Aurelia Guy", "Simon Osindero", "K. Simonyan", "Erich Elsen", "Jack W. Rae", "O. Vinyals", "L. Sifre"], "venue": "arXiv.org", "abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.", "year": 2022, "publicationdate": "2022-03-29", "externalids": {}, "doi_lower": null}
{"paper_id": 263609227, "title": "L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation", "author_names": ["Samuel Holt", "Max Ruiz Luyten", "M. Schaar"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2310.02003"}, "doi_lower": "10.48550/arxiv.2310.02003"}
{"paper_id": 127986954, "title": "The Curious Case of Neural Text Degeneration", "author_names": ["Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi"], "venue": "International Conference on Learning Representations", "abstract": "Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. \nIn this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.", "year": 2019, "publicationdate": "2019-04-22", "externalids": {}, "doi_lower": null}
{"paper_id": 265301950, "title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework", "author_names": ["Sirui Hong", "Mingchen Zhuge", "Jonathan Chen", "Xiawu Zheng", "Yuheng Cheng", "Ceyao Zhang", "Jinlin Wang", "Zili Wang", "Steven Ka Shing Yau", "Z. Lin", "Liyang Zhou", "Chenyu Ran", "Lingfeng Xiao", "Chenglin Wu", "Jürgen Schmidhuber"], "venue": "International Conference on Learning Representations", "abstract": "Remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Existing LLM-based multi-agent systems can already solve simple dialogue tasks. Solutions to more complex tasks, however, are complicated through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors. MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together. On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems. Our project can be found at https://github.com/geekan/MetaGPT", "year": 2023, "publicationdate": "2023-08-01", "externalids": {}, "doi_lower": null}
{"paper_id": 261048648, "title": "Large Language Models for Software Engineering: A Systematic Literature Review", "author_names": ["Xinying Hou", "Yanjie Zhao", "Yue Liu", "Zhou Yang", "Kailong Wang", "Li Li", "Xiapu Luo", "David Lo", "John C. Grundy", "Haoyu Wang"], "venue": "ACM Transactions on Software Engineering and Methodology", "abstract": "Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a Systematic Literature Review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We selected and analyzed 395 research articles from January 2017 to January 2024 to answer four key Research Questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, pre-processing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at https://github.com/security-pride/LLM4SE_SLR.", "year": 2023, "publicationdate": "2023-08-21", "externalids": {"DOI": "10.1145/3695988"}, "doi_lower": "10.1145/3695988"}
{"paper_id": 59599816, "title": "Parameter-Efficient Transfer Learning for NLP", "author_names": ["N. Houlsby", "A. Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin de Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "S. Gelly"], "venue": "International Conference on Machine Learning", "abstract": "Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.", "year": 2019, "publicationdate": "2019-02-02", "externalids": {}, "doi_lower": null}
{"paper_id": 235458009, "title": "LoRA: Low-Rank Adaptation of Large Language Models", "author_names": ["J. E. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Weizhu Chen"], "venue": "International Conference on Learning Representations", "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.", "year": 2021, "publicationdate": "2021-06-17", "externalids": {}, "doi_lower": null}
{"paper_id": 266374622, "title": "AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation", "author_names": ["Dong Huang", "Qingwen Bu", "Jie M. Zhang", "Michael Luck", "Heming Cui"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2312.13010"}, "doi_lower": "10.48550/arxiv.2312.13010"}
{"paper_id": 275570199, "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models", "author_names": ["Fengli Xu", "Qianyue Hao", "Zefang Zong", "Jingwei Wang", "Yunke Zhang", "Jingyi Wang", "Xiaochong Lan", "Jiahui Gong", "Tianjian Ouyang", "Fanjin Meng", "Chenyang Shao", "Yuwei Yan", "Qinglong Yang", "Yiwen Song", "Sijian Ren", "Xinyuan Hu", "Yu Li", "J. Feng", "Chen Gao", "Yong Li"], "venue": "arXiv.org", "abstract": "Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of\"thought\"-- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to\"think\"with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI's o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.", "year": 2025, "publicationdate": "2025-01-16", "externalids": {"DOI": "10.48550/arXiv.2501.09686"}, "doi_lower": "10.48550/arxiv.2501.09686"}
{"paper_id": 275570199, "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models", "author_names": ["Fengli Xu", "Qianyue Hao", "Zefang Zong", "Jingwei Wang", "Yunke Zhang", "Jingyi Wang", "Xiaochong Lan", "Jiahui Gong", "Tianjian Ouyang", "Fanjin Meng", "Chenyang Shao", "Yuwei Yan", "Qinglong Yang", "Yiwen Song", "Sijian Ren", "Xinyuan Hu", "Yu Li", "J. Feng", "Chen Gao", "Yong Li"], "venue": "arXiv.org", "abstract": "Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of\"thought\"-- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to\"think\"with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI's o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.", "year": 2025, "publicationdate": "2025-01-16", "externalids": {"DOI": "10.48550/arXiv.2501.09686"}, "doi_lower": "10.48550/arxiv.2501.09686"}
{"paper_id": 253581341, "title": "Execution-based Evaluation for Data Science Code Generation Models", "author_names": ["Junjie Huang", "Chenglong Wang", "Jipeng Zhang", "Cong Yan", "Haotian Cui", "J. Inala", "Colin B. Clement", "Nan Duan", "Jianfeng Gao"], "venue": "DASH", "abstract": "Code generation models can benefit data scientists’ productivity by automatically generating code from context and text descriptions. An important measure of the modeling progress is whether a model can generate code that can correctly execute to solve the task. However, due to the lack of an evaluation dataset that directly supports execution-based model evaluation, existing work relies on code surface form similarity metrics (e.g., BLEU, CodeBLEU) for model selection, which can be inaccurate. To remedy this, we introduce ExeDS, an evaluation dataset for execution evaluation for data science code generation tasks. ExeDS contains a set of 534 problems from Jupyter Notebooks, each consisting of code context, task description, reference program, and the desired execution output. With ExeDS, we evaluate the execution performance of five state-of-the-art code generation models that have achieved high surface-form evaluation scores. Our experiments show that models with high surface-form scores do not necessarily perform well on execution metrics, and execution-based metrics can better capture model code generation errors. All the code and data will be released upon acceptance.", "year": 2022, "publicationdate": "2022-11-17", "externalids": {"DOI": "10.48550/arXiv.2211.09374"}, "doi_lower": "10.48550/arxiv.2211.09374"}
{"paper_id": 259216042, "title": "Keynote: The Elastic AI Ecosystem — Towards A Holistic Pervasive System for Adaptive Artificial Intelligence", "author_names": ["Gregor Schiele"], "venue": "2023 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)", "abstract": "The structure of pervasive application systems that use artificial intelligence (AI) is getting more complicated. On the one hand, AI is moving closer to its data sources, often sensors embedded in the environment or worn by users. Originally, sensors would stream their data to remote Cloud centers, where it would be processed and stored. In recent years, Edge systems have emerged, moving the AI-based processing of sensor data into the local environment of a pervasive system. Now, data is processed directly on the embedded sensor devices, allowing for tight control of data privacy and latency but at the same time forcing the AI to work with minimal compute power, memory, and energy. On the other hand, not everything can be done locally, either because the necessary AI models are too resource hungry or because they integrate data from many sources and locations. Therefore, it is often necessary and beneficial to connect local AI with Edge and Cloud AI and to let the system determine dynamically which part of the overall system should be executed where.", "year": 2023, "publicationdate": "2023-03-13", "externalids": {"DOI": "10.1109/PerComWorkshops56833.2023.10150372"}, "doi_lower": "10.1109/percomworkshops56833.2023.10150372"}
{"paper_id": 271212307, "title": "Qwen2 Technical Report", "author_names": ["An Yang", "Baosong Yang", "Binyuan Hui", "Bo Zheng", "Bowen Yu", "Chang Zhou", "Chengpeng Li", "Chengyuan Li", "Dayiheng Liu", "Fei Huang", "Guanting Dong", "Haoran Wei", "Huan Lin", "Jialong Tang", "Jialin Wang", "Jian Yang", "Jianhong Tu", "Jianwei Zhang", "Jianxin Ma", "Jin Xu", "Jingren Zhou", "Jinze Bai", "Jinzheng He", "Junyang Lin", "Kai Dang", "Keming Lu", "Ke-Yang Chen", "Kexin Yang", "Mei Li", "Min Xue", "Na Ni", "Pei Zhang", "Peng Wang", "Ru Peng", "Rui Men", "Ruize Gao", "Runji Lin", "Shijie Wang", "Shuai Bai", "Sinan Tan", "Tianhang Zhu", "Tianhao Li", "Tianyu Liu", "Wenbin Ge", "Xiaodong Deng", "Xiaohuan Zhou", "Xingzhang Ren", "Xinyu Zhang", "Xipin Wei", "Xuancheng Ren", "Yang Fan", "Yang Yao", "Yichang Zhang", "Yunyang Wan", "Yunfei Chu", "Zeyu Cui", "Zhenru Zhang", "Zhi-Wei Fan"], "venue": "arXiv.org", "abstract": "This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.", "year": 2024, "publicationdate": "2024-07-15", "externalids": {"DOI": "10.48550/arXiv.2407.10671"}, "doi_lower": "10.48550/arxiv.2407.10671"}
{"paper_id": 202712680, "title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search", "author_names": ["Hamel Husain", "Hongqiu Wu", "Tiferet Gazit", "Miltiadis Allamanis", "Marc Brockschmidt"], "venue": "arXiv.org", "abstract": "Semantic code search is the task of retrieving relevant code given a natural language query. While related to other information retrieval tasks, it requires bridging the gap between the language used in code (often abbreviated and highly technical) and natural language more suitable to describe vague concepts and ideas. \nTo enable evaluation of progress on code search, we are releasing the CodeSearchNet Corpus and are presenting the CodeSearchNet Challenge, which consists of 99 natural language queries with about 4k expert relevance annotations of likely results from CodeSearchNet Corpus. The corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation. In this article, we describe the methodology used to obtain the corpus and expert labels, as well as a number of simple baseline solutions for the task. \nWe hope that CodeSearchNet Challenge encourages researchers and practitioners to study this interesting task further and will host a competition and leaderboard to track the progress on the challenge. We are also keen on extending CodeSearchNet Challenge to more queries and programming languages in the future.", "year": 2019, "publicationdate": "2019-09-20", "externalids": {}, "doi_lower": null}
{"paper_id": 268875891, "title": "Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization", "author_names": ["Yoichi Ishibashi", "Yoshimasa Nishimura"], "venue": "arXiv.org", "abstract": "Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy.", "year": 2024, "publicationdate": "2024-04-02", "externalids": {"DOI": "10.48550/arXiv.2404.02183"}, "doi_lower": "10.48550/arxiv.2404.02183"}
{"paper_id": 277820398, "title": "LangProp: A code optimization framework using Language Models applied to driving", "author_names": ["Shu Ishida", "Gianluca Corrado", "George Fedoseev", "Hudson Yeo", "Lloyd Russell", "Jamie Shotton", "João F. Henriques", "Anthony Hu"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2401.10314"}, "doi_lower": "10.48550/arxiv.2401.10314"}
{"paper_id": 263797641, "title": "Mapping Language to Code in Programmatic Context", "author_names": [], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Source code is rarely written in isolation. It depends significantly on the programmatic context, such as the class that the code would reside in. To study this phenomenon, we introduce the task of generating class member functions given English documentation and the programmatic context provided by the rest of the class. This task is challenging because the desired code can vary greatly depending on the functionality the class provides (e.g., a sort function may or may not be available when we are asked to “return the smallest element” in a particular member variable list). We introduce CONCODE, a new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment. We also present a detailed error analysis suggesting that there is significant room for future work on this task.", "year": 2018, "publicationdate": "2018-08-29", "externalids": {"DOI": "10.18653/v1/D18-1192"}, "doi_lower": "10.18653/v1/d18-1192"}
{"paper_id": 255096269, "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization", "author_names": ["S. Iyer", "Xi Victoria Lin", "Ramakanth Pasunuru", "Todor Mihaylov", "Daniel Simig", "Ping Yu", "Kurt Shuster", "Tianlu Wang", "Qing Liu", "Punit Singh Koura", "Xian Li", "Brian O'Horo", "Gabriel Pereyra", "Jeff Wang", "Christopher Dewan", "Asli Celikyilmaz", "Luke S. Zettlemoyer", "Veselin Stoyanov"], "venue": "arXiv.org", "abstract": "Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.", "year": 2022, "publicationdate": "2022-12-22", "externalids": {}, "doi_lower": null}
{"paper_id": 238419458, "title": "Towards Continual Knowledge Learning of Language Models", "author_names": ["Joel Jang", "Seonghyeon Ye", "Sohee Yang", "Joongbo Shin", "Janghoon Han", "Gyeonghun Kim", "Stanley Jungkyu Choi", "Minjoon Seo"], "venue": "International Conference on Learning Representations", "abstract": "Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs. The benchmark datasets, evaluation script, and baseline code to reproduce our results are available at https://github.com/joeljang/continual-knowledge-learning.", "year": 2021, "publicationdate": "2021-10-07", "externalids": {}, "doi_lower": null}
{"paper_id": 121680873, "title": "Perplexity—a measure of the difficulty of speech recognition tasks", "author_names": ["F. Jelinek", "R. Mercer", "L. Bahl", "J. Baker"], "venue": "", "abstract": null, "year": 1977, "publicationdate": "1977-12-01", "externalids": {"DOI": "10.1121/1.2016299"}, "doi_lower": "10.1121/1.2016299"}
{"paper_id": 6344783, "title": "Oracle-guided component-based program synthesis", "author_names": ["Susmit Jha", "Sumit Gulwani", "S. Seshia", "A. Tiwari"], "venue": "2010 ACM/IEEE 32nd International Conference on Software Engineering", "abstract": null, "year": 2010, "publicationdate": "2010-05-01", "externalids": {"DOI": "10.1145/1806799.1806833"}, "doi_lower": "10.1145/1806799.1806833"}
{"paper_id": 264743032, "title": "AI Alignment: A Comprehensive Survey", "author_names": ["Jiaming Ji", "Tianyi Qiu", "Boyuan Chen", "Borong Zhang", "Hantao Lou", "Kaile Wang", "Yawen Duan", "Zhonghao He", "Jiayi Zhou", "Zhaowei Zhang", "Fanzhi Zeng", "Kwan Yee Ng", "Juntao Dai", "Xuehai Pan", "Aidan O'Gara", "Yingshan Lei", "Hua Xu", "Brian Tse", "Jie Fu", "S. McAleer", "Yaodong Yang", "Yizhou Wang", "Song-Chun Zhu", "Yike Guo", "Wen Gao"], "venue": "arXiv.org", "abstract": "AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, so do risks from misalignment. To provide a comprehensive and up-to-date overview of the alignment field, in this survey, we delve into the core concepts, methodology, and practice of alignment. First, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss techniques for learning from feedback and learning under distribution shift. On backward alignment, we discuss assurance techniques and governance practices. We also release and continually update the website (www.alignmentsurvey.com) which features tutorials, collections of papers, blog posts, and other resources.", "year": 2023, "publicationdate": "2023-10-30", "externalids": {}, "doi_lower": null}
{"paper_id": 216034102, "title": "Question selection for interactive program synthesis", "author_names": ["Ruyi Ji", "Jingjing Liang", "Yingfei Xiong", "Lu Zhang", "Zhenjiang Hu"], "venue": "ACM-SIGPLAN Symposium on Programming Language Design and Implementation", "abstract": "Interactive program synthesis aims to solve the ambiguity in specifications, and selecting the proper question to minimize the rounds of interactions is critical to the performance of interactive program synthesis. In this paper we address this question selection problem and propose two algorithms. SampleSy approximates a state-of-the-art strategy proposed for optimal decision tree and has a short response time to enable interaction. EpsSy further reduces the rounds of interactions by approximating SampleSy with a bounded error rate. To implement the two algorithms, we further propose VSampler, an approach to sampling programs from a probabilistic context-free grammar based on version space algebra. The evaluation shows the effectiveness of both algorithms.", "year": 2020, "publicationdate": "2020-06-06", "externalids": {"DOI": "10.1145/3385412.3386025"}, "doi_lower": "10.1145/3385412.3386025"}
{"paper_id": 263828962, "title": "Benchmarking and Explaining Large Language Model-based Code Generation: A Causality-Centric Approach", "author_names": ["Zhenlan Ji", "Pingchuan Ma", "Zongjie Li", "Shuai Wang"], "venue": "arXiv.org", "abstract": "While code generation has been widely used in various software development scenarios, the quality of the generated code is not guaranteed. This has been a particular concern in the era of large language models (LLMs)- based code generation, where LLMs, deemed a complex and powerful black-box model, is instructed by a high-level natural language specification, namely a prompt, to generate code. Nevertheless, effectively evaluating and explaining the code generation capability of LLMs is inherently challenging, given the complexity of LLMs and the lack of transparency. Inspired by the recent progress in causality analysis and its application in software engineering, this paper launches a causality analysis-based approach to systematically analyze the causal relations between the LLM input prompts and the generated code. To handle various technical challenges in this study, we first propose a novel causal graph-based representation of the prompt and the generated code, which is established over the fine-grained, human-understandable concepts in the input prompts. The formed causal graph is then used to identify the causal relations between the prompt and the derived code. We illustrate the insights that our framework can provide by studying over 3 popular LLMs with over 12 prompt adjustment strategies. The results of these studies illustrate the potential of our technique to provide insights into LLM effectiveness, and aid end-users in understanding predictions. Additionally, we demonstrate that our approach provides actionable insights to improve the quality of the LLM-generated code by properly calibrating the prompt.", "year": 2023, "publicationdate": "2023-10-10", "externalids": {"DOI": "10.48550/arXiv.2310.06680"}, "doi_lower": "10.48550/arxiv.2310.06680"}
{"paper_id": 266693763, "title": "Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models", "author_names": ["Terry Yue Zhuo", "A. Zebaze", "Nitchakarn Suppattarachai", "L. V. Werra", "H. D. Vries", "Qian Liu", "Niklas Muennighoff"], "venue": "arXiv.org", "abstract": "The high cost of full-parameter fine-tuning (FFT) of Large Language Models (LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods. However, it remains unclear which methods provide the best cost-performance trade-off at different model scales. We introduce Astraios, a suite of 28 instruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up to 16 billion parameters. Through investigations across 5 tasks and 8 different datasets encompassing both code comprehension and code generation tasks, we find that FFT generally leads to the best downstream performance across all scales, and PEFT methods differ significantly in their efficacy based on the model scale. LoRA usually offers the most favorable trade-off between cost and performance. Further investigation into the effects of these methods on both model robustness and code security reveals that larger models tend to demonstrate reduced robustness and less security. At last, we explore the relationships among updated parameters, cross-entropy loss, and task performance. We find that the tuning effectiveness observed in small models generalizes well to larger models, and the validation loss in instruction tuning can be a reliable indicator of overall downstream performance.", "year": 2024, "publicationdate": "2024-01-01", "externalids": {"DOI": "10.48550/arXiv.2401.00788"}, "doi_lower": "10.48550/arxiv.2401.00788"}
{"paper_id": 259076266, "title": "SelfEvolve: A Code Evolution Framework via Large Language Models", "author_names": ["Shuyang Jiang", "Yuhao Wang", "Yu Wang"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called \\autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, \\autoknow~obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, \\autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate \\autoknow~on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that \\autoknow~outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of \\autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that \\autoknow~can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.", "year": 2023, "publicationdate": "2023-06-05", "externalids": {"DOI": "10.48550/arXiv.2306.02907"}, "doi_lower": "10.48550/arxiv.2306.02907"}
{"paper_id": 263829697, "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?", "author_names": ["Carlos E. Jimenez", "John Yang", "Alexander Wettig", "Shunyu Yao", "Kexin Pei", "Ofir Press", "Karthik Narasimhan"], "venue": "International Conference on Learning Representations", "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.", "year": 2023, "publicationdate": "2023-10-10", "externalids": {"DOI": "10.48550/arXiv.2310.06770"}, "doi_lower": "10.48550/arxiv.2310.06770"}
{"paper_id": 282769116, "title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering", "author_names": ["Carlos Jimenez", "K. Lieret", "Karthik R. Narasimhan", "Ofir Press", "Alexander Wettig", "John Yang", "Shunyu Yao"], "venue": "Advances in Neural Information Processing Systems 37", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.52202/079017-1601"}, "doi_lower": "10.52202/079017-1601"}
{"paper_id": 15424967, "title": "A Formalism for Dependency Grammar Based on Tree Adjoining Grammar", "author_names": ["A. Joshi", "Owen Rambow"], "venue": "", "abstract": null, "year": 2003, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 251765058, "title": "Repair Is Nearly Generation: Multilingual Program Repair with LLMs", "author_names": ["Harshit Joshi", "J. Cambronero", "Sumit Gulwani", "Vu Le", "Ivan Radicek", "Gust Verbruggen"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Most programmers make mistakes when writing code. Some of these mistakes are small and require few edits to the original program – a class of errors recently termed last mile mistakes. These errors break the flow for experienced developers and can stump novice programmers. Existing automated repair techniques targeting this class of errors are language-specific and do not easily carry over to new languages. Transferring symbolic approaches requires substantial engineering and neural approaches require data and retraining. We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex. Such a multilingual engine enables a flipped model for programming assistance, one where the programmer writes code and the AI assistance suggests fixes, compared to traditional code suggestion technology. Taking inspiration from the way programmers manually fix bugs, we show that a prompt-based strategy that conceptualizes repair as localization, transformation, and candidate ranking, can successfully repair programs in multiple languages with minimal effort. We present the first results for such a multilingual repair engine by evaluating on 6 different languages and comparing performance to language-specific repair engines. We show that RING can outperform language-specific repair engines for three of these languages.", "year": 2022, "publicationdate": "2022-08-24", "externalids": {"DOI": "10.48550/arXiv.2208.11640"}, "doi_lower": "10.48550/arxiv.2208.11640"}
{"paper_id": 210861095, "title": "Scaling Laws for Neural Language Models", "author_names": ["J. Kaplan", "Sam McCandlish", "T. Henighan", "Tom B. Brown", "Benjamin Chess", "R. Child", "Scott Gray", "Alec Radford", "Jeff Wu", "Dario Amodei"], "venue": "arXiv.org", "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.", "year": 2020, "publicationdate": "2020-01-23", "externalids": {}, "doi_lower": null}
{"paper_id": 257365592, "title": "xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval", "author_names": ["Mohammad Abdullah Matin Khan", "M Saiful Bari", "Do Xuan Long", "Weishi Wang", "Md. Rizwan Parvez", "Shafiq R. Joty"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2303.03004"}, "doi_lower": "10.48550/arxiv.2303.03004"}
{"paper_id": 202643539, "title": "Vol. 37, Issue 4, August 2010 / Band 37, Heft 4, August 2010", "author_names": [], "venue": "Transfusion Medicine and Hemotherapy", "abstract": null, "year": 2010, "publicationdate": "2010-07-27", "externalids": {"DOI": "10.1159/000319681"}, "doi_lower": "10.1159/000319681"}
{"paper_id": 268732857, "title": "sDPO: Don't Use Your Data All at Once", "author_names": ["Dahyun Kim", "Yungi Kim", "Wonho Song", "Hyeonwoo Kim", "Yunsu Kim", "Sanghoon Kim", "Chanjun Park"], "venue": "International Conference on Computational Linguistics", "abstract": "As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important. We propose stepwise DPO (sDPO), an extension of the recently popularized direct preference optimization (DPO) for alignment tuning. This approach involves dividing the available preference datasets and utilizing them in a stepwise manner, rather than employing it all at once. We demonstrate that this method facilitates the use of more precisely aligned reference models within the DPO training framework. Furthermore, sDPO trains the final model to be more performant, even outperforming other popular LLMs with more parameters.", "year": 2024, "publicationdate": "2024-03-28", "externalids": {"DOI": "10.48550/arXiv.2403.19270"}, "doi_lower": "10.48550/arxiv.2403.19270"}
{"paper_id": 266550918, "title": "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling", "author_names": ["Dahyun Kim", "Chanjun Park", "Sanghoon Kim", "Wonsung Lee", "Wonho Song", "Yunsu Kim", "Hyeonwoo Kim", "Yungi Kim", "Hyeonju Lee", "Jihoo Kim", "Changbae Ahn", "Seonghoon Yang", "Sukyung Lee", "Hyunbyung Park", "Gyoungjin Gim", "Mikyoung Cha", "Hwalsuk Lee", "Sunghun Kim"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. We show experimentally that DUS is simple yet effective in scaling up high-performance LLMs from small ones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field.", "year": 2023, "publicationdate": "2023-12-23", "externalids": {"DOI": "10.48550/arXiv.2312.15166"}, "doi_lower": "10.48550/arxiv.2312.15166"}
{"paper_id": 3918101, "title": "Systematic literature reviews in software engineering - A systematic literature review", "author_names": ["B. Kitchenham", "P. Brereton", "D. Budgen", "M. Turner", "J. Bailey", "S. Linkman"], "venue": "Information and Software Technology", "abstract": null, "year": 2009, "publicationdate": null, "externalids": {"DOI": "10.1016/J.INFSOF.2008.09.009"}, "doi_lower": "10.1016/j.infsof.2008.09.009"}
{"paper_id": 254044610, "title": "The Stack: 3 TB of permissively licensed source code", "author_names": ["Denis Kocetkov", "Raymond Li", "Loubna Ben Allal", "Jia Li", "Chenghao Mou", "Carlos Muñoz Ferrandis", "Yacine Jernite", "Margaret Mitchell", "Sean Hughes", "Thomas Wolf", "Dzmitry Bahdanau", "L. V. Werra", "H. D. Vries"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called\"Am I in The Stack\"(https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.", "year": 2022, "publicationdate": "2022-11-20", "externalids": {"DOI": "10.48550/arXiv.2211.15533"}, "doi_lower": "10.48550/arxiv.2211.15533"}
{"paper_id": 258179434, "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment", "author_names": ["Andreas Kopf", "Yannic Kilcher", "Dimitri von Rutte", "Sotiris Anagnostidis", "Zhi Rui Tam", "K. Stevens", "A. Barhoum", "Nguyen Minh Duc", "Oliver Stanley", "Rich'ard Nagyfi", "ES Shahul", "Sameer Suri", "David Glushkov", "A. Dantuluri", "Andrew Maguire", "Christoph Schuhmann", "Huu Nguyen", "A. Mattick"], "venue": "Neural Information Processing Systems", "abstract": "Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models. We release our code and data under a fully permissive licence.", "year": 2023, "publicationdate": "2023-04-14", "externalids": {"DOI": "10.48550/arXiv.2304.07327"}, "doi_lower": "10.48550/arxiv.2304.07327"}
{"paper_id": 280034819, "title": "Is Model Attention Aligned with Human Attention? An Empirical Study on Large Language Models for Code Generation", "author_names": ["Bonan Kou", "Shengmai Chen", "Zhijie Wang", "Lei Ma", "Tianyi Zhang"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.01220"}, "doi_lower": "10.48550/arxiv.2306.01220"}
{"paper_id": 219401607, "title": "Unsupervised Translation of Programming Languages", "author_names": ["M. Lachaux", "Baptiste Rozière", "Lowik Chanussot", "Guillaume Lample"], "venue": "Neural Information Processing Systems", "abstract": "A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.", "year": 2020, "publicationdate": "2020-06-05", "externalids": {}, "doi_lower": null}
{"paper_id": 253734939, "title": "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation", "author_names": ["Yuhang Lai", "Chengxi Li", "Yiming Wang", "Tianyi Zhang", "Ruiqi Zhong", "Luke Zettlemoyer", "S. Yih", "Daniel Fried", "Si-yi Wang", "Tao Yu"], "venue": "International Conference on Machine Learning", "abstract": "We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.", "year": 2022, "publicationdate": "2022-11-18", "externalids": {"DOI": "10.48550/arXiv.2211.11501"}, "doi_lower": "10.48550/arxiv.2211.11501"}
{"paper_id": 257378329, "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset", "author_names": ["Hugo Laurenccon", "Lucile Saulnier", "Thomas Wang", "Christopher Akiki", "Albert Villanova del Moral", "Teven Le Scao", "L. V. Werra", "Chenghao Mou", "E. G. Ponferrada", "Huu Nguyen", "Jorg Frohberg", "Mario vSavsko", "Quentin Lhoest", "Angelina McMillan-Major", "Gérard Dupont", "Stella Biderman", "Anna Rogers", "Loubna Ben Allal", "F. Toni", "Giada Pistilli", "Olivier Nguyen", "Somaieh Nikpoor", "Maraim Masoud", "Pierre Colombo", "Javier de la Rosa", "Paulo Villegas", "Tristan Thrush", "S. Longpre", "Sebastian Nagel", "Leon Weber", "M. Muñoz", "Jian Zhu", "Daniel Alexander van Strien", "Zaid Alyafeai", "Khalid Almubarak", "Minh Chien Vu", "Itziar Gonzalez-Dios", "Aitor Soroa Etxabe", "Kyle Lo", "Manan Dey", "Pedro Ortiz Suarez", "Aaron Gokaslan", "Shamik Bose", "David Ifeoluwa Adelani", "Long Phan", "H. Tran", "I. Yu", "S. Pai", "Jenny Chim", "Violette Lepercq", "Suzana Ilic", "Margaret Mitchell", "Sasha Luccioni", "Yacine Jernite"], "venue": "Neural Information Processing Systems", "abstract": "As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.", "year": 2023, "publicationdate": "2023-03-07", "externalids": {"DOI": "10.48550/arXiv.2303.03915"}, "doi_lower": "10.48550/arxiv.2303.03915"}
{"paper_id": 250280117, "title": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning", "author_names": ["Hung Le", "Yue Wang", "Akhilesh Deepak Gotmare", "S. Savarese", "S. Hoi"], "venue": "Neural Information Processing Systems", "abstract": "Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose\"CodeRL\", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.", "year": 2022, "publicationdate": "2022-07-05", "externalids": {"DOI": "10.48550/arXiv.2207.01780"}, "doi_lower": "10.48550/arxiv.2207.01780"}
{"paper_id": 253420279, "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "author_names": ["Teven Le Scao", "Angela Fan", "Christopher Akiki", "Ellie Pavlick", "Suzana Ili'c", "Daniel Hesslow", "Roman Castagn'e", "A. Luccioni", "François Yvon", "Matthias Gallé", "J. Tow", "Alexander M. Rush", "Stella Biderman", "Albert Webson", "Pawan Sasanka Ammanamanchi", "Thomas Wang", "Benoît Sagot", "Niklas Muennighoff", "Albert Villanova del Moral", "Olatunji Ruwase", "Rachel Bawden", "Stas Bekman", "Angelina McMillan-Major", "Iz Beltagy", "Huu Nguyen", "Lucile Saulnier", "Samson Tan", "Pedro Ortiz Suarez", "Victor Sanh", "Hugo Laurenccon", "Yacine Jernite", "Julien Launay", "Margaret Mitchell", "Colin Raffel", "Aaron Gokaslan", "Adi Simhi", "Aitor Soroa Etxabe", "Alham Fikri Aji", "Amit Alfassy", "Anna Rogers", "Ariel Kreisberg Nitzav", "Canwen Xu", "Chenghao Mou", "Chris C. Emezue", "Christopher Klamm", "Colin Leong", "Daniel Alexander van Strien", "David Ifeoluwa Adelani", "Dragomir R. Radev", "E. G. Ponferrada", "Efrat Levkovizh", "Ethan Kim", "Eyal Natan", "F. Toni", "Gérard Dupont", "Germán Kruszewski", "Giada Pistilli", "Hady ElSahar", "Hamza Benyamina", "H. Tran", "Ian Yu", "Idris Abdulmumin", "Isaac Johnson", "Itziar Gonzalez-Dios", "Javier de la Rosa", "Jenny Chim", "Jesse Dodge", "Jian Zhu", "Jonathan Chang", "Jorg Frohberg", "Josephine Tobing", "J. Bhattacharjee", "Khalid Almubarak", "Kimbo Chen", "Kyle Lo", "L. V. Werra", "Leon Weber", "Long Phan", "Loubna Ben Allal", "Ludovic Tanguy", "Manan Dey", "M. Muñoz", "Maraim Masoud", "María Grandury", "Mario vSavsko", "Max Huang", "Maximin Coavoux", "Mayank Singh", "Mike Tian-Jian Jiang", "Minh Chien Vu", "M. A. Jauhar", "Mustafa Ghaleb", "Nishant Subramani", "Nora Kassner", "Nurulaqilla Khamis", "Olivier Nguyen", "Omar Espejel", "Ona de Gibert", "Paulo Villegas", "Peter Henderson", "Pierre Colombo", "Priscilla Amuok", "Quentin Lhoest", "Rheza Harliman", "Rishi Bommasani", "R. L'opez", "Rui Ribeiro", "Salomey Osei", "Sampo Pyysalo", "Sebastian Nagel", "Shamik Bose", "Shamsuddeen Hassan Muhammad", "S. Sharma", "S. Longpre", "Somaieh Nikpoor", "S. Silberberg", "S. Pai", "S. Zink", "Tiago Timponi Torrent", "Timo Schick", "Tristan Thrush", "V. Danchev", "Vassilina Nikoulina", "Veronika Laippala", "Violette Lepercq", "V. Prabhu", "Zaid Alyafeai", "Zeerak Talat", "Arun Raja", "Benjamin Heinzerling", "Chenglei Si", "Elizabeth Salesky", "Sabrina J. Mielke", "Wilson Y. Lee", "Abheesht Sharma", "Andrea Santilli", "Antoine Chaffin", "Arnaud Stiegler", "Debajyoti Datta", "Eliza Szczechla", "Gunjan Chhablani", "Han Wang", "Harshit Pandey", "Hendrik Strobelt", "Jason Alan Fries", "Jos Rozen", "Leo Gao", "Lintang Sutawika", "M Saiful Bari", "Maged Saeed Al-shaibani", "Matteo Manica", "Nihal V. Nayak", "R. Teehan", "Samuel Albanie", "Sheng Shen", "Srulik Ben-David", "Stephen H. Bach", "Taewoon Kim", "T. Bers", "Thibault Févry", "Trishala Neeraj", "Urmish Thakker", "Vikas Raunak", "Xiang Tang", "Zheng-Xin Yong", "Zhiqing Sun", "Shaked Brody", "Y. Uri", "Hadar Tojarieh", "Adam Roberts", "Hyung Won Chung", "Jaesung Tae", "Jason Phang", "Ofir Press", "Conglong Li", "D. Narayanan", "Hatim Bourfoune", "J. Casper", "Jeff Rasley", "Max Ryabinin", "Mayank Mishra", "Minjia Zhang", "M. Shoeybi", "Myriam Peyrounette", "N. Patry", "Nouamane Tazi", "Omar Sanseviero", "Patrick von Platen", "Pierre Cornette", "Pierre Franccois Lavall'ee", "R. Lacroix", "Samyam Rajbhandari", "Sanchit Gandhi", "Shaden Smith", "S. Requena", "Suraj Patil", "Tim Dettmers", "Ahmed Baruwa", "Amanpreet Singh", "Anastasia Cheveleva", "Anne-Laure Ligozat", "Arjun Subramonian", "Aur'elie N'ev'eol", "Charles Lovering", "Dan Garrette", "D. Tunuguntla", "Ehud Reiter", "Ekaterina Taktasheva", "E. Voloshina", "Eli Bogdanov", "Genta Indra Winata", "Hailey Schoelkopf", "Jan-Christoph Kalo", "Jekaterina Novikova", "J. Forde", "Xiangru Tang", "Jungo Kasai", "Ken Kawamura", "Liam Hazan", "Marine Carpuat", "Miruna Clinciu", "Najoung Kim", "Newton Cheng", "Oleg Serikov", "Omer Antverg", "Oskar van der Wal", "Rui Zhang", "Ruochen Zhang", "Sebastian Gehrmann", "Shachar Mirkin", "S. Pais", "Tatiana Shavrina", "Thomas Scialom", "Tian Yun", "Tomasz Limisiewicz", "Verena Rieser", "Vitaly Protasov", "V. Mikhailov", "Yada Pruksachatkun", "Yonatan Belinkov", "Zachary Bamberger", "Zdenˇek Kasner", "Zdeněk Kasner", "A. Pestana", "A. Feizpour", "Ammar Khan", "Amy Faranak", "A. Santos", "Anthony Hevia", "Antigona Unldreaj", "Arash Aghagol", "Arezoo Abdollahi", "A. Tammour", "A. HajiHosseini", "Bahareh Behroozi", "Benjamin Ayoade Ajibade", "B. Saxena", "Carlos Muñoz Ferrandis", "Danish Contractor", "D. Lansky", "Davis David", "Douwe Kiela", "D. A. Nguyen", "Edward Tan", "Emi Baylor", "Ez-inwanne Ozoani", "F. Mirza", "Frankline Ononiwu", "Habib Rezanejad", "H.A. Jones", "Indrani Bhattacharya", "Irene Solaiman", "Irina Sedenko", "Isar Nejadgholi", "J. Passmore", "Joshua Seltzer", "Julio Bonis Sanz", "Karen Fort", "Lívia Dutra", "Mairon Samagaio", "Maraim Elbadri", "Margot Mieskes", "M. Gerchick", "Martha Akinlolu", "Michael McKenna", "Mike Qiu", "M. Ghauri", "Mykola Burynok", "Nafis Abrar", "Nazneen Rajani", "Nour Elkott", "N. Fahmy", "Olanrewaju Samuel", "Ran An", "R. Kromann", "Ryan Hao", "S. Alizadeh", "Sarmad Shubber", "Silas L. Wang", "Sourav Roy", "S. Viguier", "Thanh-Cong Le", "Tobi Oyebade", "T. Le", "Yoyo Yang", "Zach Nguyen", "Abhinav Ramesh Kashyap", "A. Palasciano", "A. Callahan", "Anima Shukla", "Antonio Miranda-Escalada", "Ayush Singh", "Benjamin Beilharz", "Bo Wang", "C. Brito", "Chenxi Zhou", "Chirag Jain", "Chuxin Xu", "Clémentine Fourrier", "Daniel Le'on Perin'an", "Daniel Molano", "Dian Yu", "Enrique Manjavacas", "Fabio Barth", "Florian Fuhrimann", "Gabriel Altay", "Giyaseddin Bayrak", "Gully Burns", "Helena U. Vrabec", "I. Bello", "Isha Dash", "J. Kang", "John Giorgi", "Jonas Golde", "J. Posada", "Karthi Sivaraman", "Lokesh Bulchandani", "Lu Liu", "Luisa Shinzato", "Madeleine Hahn de Bykhovetz", "Maiko Takeuchi", "Marc Pàmies", "M. A. Castillo", "Marianna Nezhurina", "Mario Sanger", "M. Samwald", "Michael Cullan", "Michael Weinberg", "M. Wolf", "Mina Mihaljcic", "Minna Liu", "M. Freidank", "Myungsun Kang", "Natasha Seelam", "N. Dahlberg", "N. Broad", "N. Muellner", "Pascale Fung", "Patricia Haller", "Patrick Haller", "R. Eisenberg", "Robert Martin", "Rodrigo Canalli", "Rosaline Su", "Ruisi Su", "Samuel Cahyawijaya", "Samuele Garda", "Shlok S Deshmukh", "Shubhanshu Mishra", "Sid Kiblawi", "Simon Ott", "Sinee Sang-aroonsiri", "Srishti Kumar", "Stefan Schweter", "S. Bharati", "Tanmay Laud", "Théo Gigant", "Tomoya Kainuma", "Wojciech Kusa", "Yanis Labrak", "Yashasvi Bajaj", "Y. Venkatraman", "Yifan Xu", "Ying Xu", "Yu Xu", "Z. Tan", "Zhongli Xie", "Zifan Ye", "M. Bras", "Younes Belkada", "Thomas Wolf"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.", "year": 2022, "publicationdate": "2022-11-09", "externalids": {"DOI": "10.48550/arXiv.2211.05100"}, "doi_lower": "10.48550/arxiv.2211.05100"}
{"paper_id": 261493811, "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback", "author_names": ["Harrison Lee", "Samrat Phatale", "Hassan Mansoor", "Kellie Lu", "Thomas Mesnard", "Colton Bishop", "Victor Carbune", "Abhinav Rastogi"], "venue": "International Conference on Machine Learning", "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards\"self-improvement\"by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.", "year": 2023, "publicationdate": "2023-09-01", "externalids": {}, "doi_lower": null}
{"paper_id": 233296808, "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "author_names": ["Brian Lester", "Rami Al-Rfou", "Noah Constant"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3’s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” We release code and model checkpoints to reproduce our experiments.", "year": 2021, "publicationdate": "2021-04-18", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.243"}, "doi_lower": "10.18653/v1/2021.emnlp-main.243"}
{"paper_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "author_names": ["Patrick Lewis", "Ethan Perez", "Aleksandara Piktus", "F. Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Kuttler", "M. Lewis", "Wen-tau Yih", "Tim Rocktäschel", "Sebastian Riedel", "Douwe Kiela"], "venue": "Neural Information Processing Systems", "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.", "year": 2020, "publicationdate": "2020-05-22", "externalids": {}, "doi_lower": null}
{"paper_id": 263897368, "title": "Towards Enhancing In-Context Learning for Code Generation", "author_names": ["Jia Li", "Yunfei Zhao", "Yongming Li", "Ge Li", "Zhi Jin"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2303.17780"}, "doi_lower": "10.48550/arxiv.2303.17780"}
{"paper_id": 4798744, "title": "Static analysis of android apps: A systematic literature review", "author_names": ["Li Li", "Tégawendé F. Bissyandé", "Mike Papadakis", "Siegfried Rasthofer", "Alexandre Bartel", "Damien Octeau", "Jacques Klein", "Y. L. Traon"], "venue": "Information and Software Technology", "abstract": null, "year": 2017, "publicationdate": "2017-08-01", "externalids": {"DOI": "10.1016/J.INFSOF.2017.04.001"}, "doi_lower": "10.1016/j.infsof.2017.04.001"}
{"paper_id": 258588247, "title": "StarCoder: may the source be with you!", "author_names": ["Raymond Li", "Loubna Ben Allal", "Yangtian Zi", "Niklas Muennighoff", "Denis Kocetkov", "Chenghao Mou", "Marc Marone", "Christopher Akiki", "Jia Li", "Jenny Chim", "Qian Liu", "Evgenii Zheltonozhskii", "Terry Yue Zhuo", "Thomas Wang", "Olivier Dehaene", "Mishig Davaadorj", "J. Lamy-Poirier", "João Monteiro", "Oleh Shliazhko", "Nicolas Gontier", "Nicholas Meade", "A. Zebaze", "Ming-Ho Yee", "Logesh Kumar Umapathi", "Jian Zhu", "Benjamin Lipkin", "Muhtasham Oblokulov", "Zhiruo Wang", "Rudra Murthy", "J. Stillerman", "Siva Sankalp Patel", "Dmitry Abulkhanov", "Marco Zocca", "Manan Dey", "Zhihan Zhang", "N. Fahmy", "Urvashi Bhattacharyya", "W. Yu", "Swayam Singh", "Sasha Luccioni", "Paulo Villegas", "M. Kunakov", "Fedor Zhdanov", "Manuel Romero", "Tony Lee", "Nadav Timor", "Jennifer Ding", "Claire Schlesinger", "Hailey Schoelkopf", "Jana Ebert", "Tri Dao", "Mayank Mishra", "A. Gu", "Jennifer Robinson", "Carolyn Jane Anderson", "Brendan Dolan-Gavitt", "Danish Contractor", "Siva Reddy", "Daniel Fried", "Dzmitry Bahdanau", "Yacine Jernite", "Carlos Muñoz Ferrandis", "Sean M. Hughes", "Thomas Wolf", "Arjun Guha", "L. V. Werra", "H. D. Vries"], "venue": "Trans. Mach. Learn. Res.", "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.", "year": 2023, "publicationdate": "2023-05-09", "externalids": {}, "doi_lower": null}
{"paper_id": 230433941, "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "author_names": ["Xiang Lisa Li", "Percy Liang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.", "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2021.acl-long.353"}, "doi_lower": "10.18653/v1/2021.acl-long.353"}
{"paper_id": 202643539, "title": "Vol. 37, Issue 4, August 2010 / Band 37, Heft 4, August 2010", "author_names": [], "venue": "Transfusion Medicine and Hemotherapy", "abstract": null, "year": 2010, "publicationdate": "2010-07-27", "externalids": {"DOI": "10.1159/000319681"}, "doi_lower": "10.1159/000319681"}
{"paper_id": 246527904, "title": "Competition-level code generation with AlphaCode", "author_names": ["Yujia Li", "David Choi", "Junyoung Chung", "Nate Kushman", "Julian Schrittwieser", "Rémi Leblond", "Tom", "Eccles", "James Keeling", "Felix Gimeno", "A. D. Lago", "T. Hubert", "Peter Choy", "Cyprien de", "Masson d’Autume", "Igor Babuschkin", "Xinyun Chen", "Po-Sen Huang", "Johannes Welbl", "Sven Gowal", "Alexey", "Cherepanov", "James Molloy", "D. Mankowitz", "Esme Sutherland Robson", "Pushmeet Kohli", "Nando de", "Freitas", "K. Kavukcuoglu", "O. Vinyals"], "venue": "Science", "abstract": "Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Description Machine learning systems can program too Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers’ productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. —YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests.", "year": 2022, "publicationdate": "2022-02-08", "externalids": {"DOI": "10.1126/science.abq1158"}, "doi_lower": "10.1126/science.abq1158"}
{"paper_id": 248266620, "title": "Unleashing the Power of Compiler Intermediate Representation to Enhance Neural Program Embeddings", "author_names": ["Zongjie Li", "Pingchuan Ma", "Huaijin Wang", "Shuai Wang", "Qiyi Tang", "Sen Nie", "Shi Wu"], "venue": "International Conference on Software Engineering", "abstract": "Neural program embeddings have demonstrated considerable promise in a range of program analysis tasks, including clone identification, program repair, code completion, and program synthesis. However, most existing methods generate neural program embeddings di-rectly from the program source codes, by learning from features such as tokens, abstract syntax trees, and control flow graphs. This paper takes a fresh look at how to improve program embed-dings by leveraging compiler intermediate representation (IR). We first demonstrate simple yet highly effective methods for enhancing embedding quality by training embedding models alongside source code and LLVM IR generated by default optimization levels (e.g., -02). We then introduce IRGEN, a framework based on genetic algorithms (GA), to identify (near-)optimal sequences of optimization flags that can significantly improve embedding quality. We use IRGEN to find optimal sequences of LLVM optimization flags by performing GA on source code datasets. We then extend a popular code embedding model, CodeCMR, by adding a new objective based on triplet loss to enable a joint learning over source code and LLVM IR. We benchmark the quality of embedding using a rep-resentative downstream application, code clone detection. When CodeCMR was trained with source code and LLVM IRs optimized by findings of IRGEN, the embedding quality was significantly im-proved, outperforming the state-of-the-art model, CodeBERT, which was trained only with source code. Our augmented CodeCMR also outperformed CodeCMR trained over source code and IR optimized with default optimization levels. We investigate the properties of optimization flags that increase embedding quality, demonstrate IRGEN's generalization in boosting other embedding models, and establish IRGEN's use in settings with extremely limited training data. Our research and findings demonstrate that a straightforward addition to modern neural code embedding models can provide a highly effective enhancement.", "year": 2022, "publicationdate": "2022-04-20", "externalids": {"DOI": "10.1145/3510003.3510217"}, "doi_lower": "10.1145/3510003.3510217"}
{"paper_id": 257771591, "title": "Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning", "author_names": ["Vladislav Lialin", "Vijeta Deshpande", "Anna Rumshisky"], "venue": "arXiv.org", "abstract": "This paper presents a systematic overview of parameter-efficient fine-tuning methods, covering over 50 papers published between early 2019 and mid-2024. These methods aim to address the challenges of fine-tuning large language models by training only a small subset of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a specific focus on real-life efficiency in fine-tuning multibillion-scale language models. We also conduct an extensive head-to-head experimental comparison of 15 diverse PEFT methods, evaluating their performance and efficiency on models up to 11B parameters. Our findings reveal that methods previously shown to surpass a strong LoRA baseline face difficulties in resource-constrained settings, where hyperparameter optimization is limited and the network is fine-tuned only for a few epochs. Finally, we provide a set of practical recommendations for using PEFT methods and outline potential future research directions.", "year": 2023, "publicationdate": "2023-03-28", "externalids": {}, "doi_lower": null}
{"paper_id": 263423935, "title": "Holistic Evaluation of Language Models", "author_names": ["Percy Liang", "Rishi Bommasani", "Tony Lee", "Dimitris Tsipras", "Dilara Soylu", "Michihiro Yasunaga", "Yian Zhang", "Deepak Narayanan", "Yuhuai Wu", "Ananya Kumar", "Benjamin Newman", "Binhang Yuan", "Bobby Yan", "Ce Zhang", "Christian Cosgrove", "Christopher D. Manning", "Christopher Ré", "Diana Acosta-Navas", "Drew A. Hudson", "E. Zelikman", "Esin Durmus", "Faisal Ladhak", "Frieda Rong", "Hongyu Ren", "Huaxiu Yao", "Jue Wang", "Keshav Santhanam", "Laurel J. Orr", "Lucia Zheng", "Mert Yüksekgönül", "Mirac Suzgun", "Nathan Kim", "Neel Guha", "Niladri S. Chatterji", "O. Khattab", "Peter Henderson", "Qian Huang", "Ryan Chi", "Sang Michael Xie", "Shibani Santurkar", "Surya Ganguli", "Tatsunori Hashimoto", "Thomas Icard", "Tianyi Zhang", "Vishrav Chaudhary", "William Wang", "Xuechen Li", "Yifan Mai", "Yuhui Zhang", "Yuta Koreeda"], "venue": "arXiv.org", "abstract": "Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.", "year": 2022, "publicationdate": "2022-11-16", "externalids": {"DOI": "10.48550/arXiv.2211.09110"}, "doi_lower": "10.48550/arxiv.2211.09110"}
{"paper_id": 964287, "title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author_names": ["Chin-Yew Lin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": null, "year": 2004, "publicationdate": "2004-07-25", "externalids": {}, "doi_lower": null}
{"paper_id": 1742535, "title": "Mining Internet-Scale Software Repositories", "author_names": ["Erik J. Linstead", "Paul Rigor", "S. Bajracharya", "C. Lopes", "P. Baldi"], "venue": "Neural Information Processing Systems", "abstract": null, "year": 2007, "publicationdate": "2007-12-03", "externalids": {}, "doi_lower": null}
{"paper_id": 269613809, "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model", "author_names": ["Zhihong Shao", "Damai Dai", "Daya Guo", "Bo Liu (Benjamin Liu)", "Zihan Wang", "Huajian Xin"], "venue": "arXiv.org", "abstract": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.", "year": 2024, "publicationdate": "2024-05-07", "externalids": {"DOI": "10.48550/arXiv.2405.04434"}, "doi_lower": "10.48550/arxiv.2405.04434"}
{"paper_id": 271961606, "title": "MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning", "author_names": ["Bingchang Liu", "Chaoyu Chen", "Zi Gong", "Cong Liao", "Huan Wang", "Zhichao Lei", "Ming Liang", "Dajun Chen", "Min Shen", "Hailian Zhou", "Wei Jiang", "Hang Yu", "Jianguo Li"], "venue": "Knowledge Discovery and Data Mining", "abstract": "Code LLMs have emerged as a specialized research field, with remarkable studies dedicated to enhancing model's coding capabilities through fine-tuning on pre-trained models. Previous fine-tuning approaches were typically tailored to specific downstream tasks or scenarios, which meant separate fine-tuning for each task, requiring extensive training resources and posing challenges in terms of deployment and maintenance. Furthermore, these approaches failed to leverage the inherent interconnectedness among different code-related tasks. To overcome these limitations, we present a multi-task fine-tuning framework, MFTCoder, that enables simultaneous and parallel fine-tuning on multiple tasks. By incorporating various loss functions, we effectively address common challenges in multi-task learning, such as data imbalance, varying difficulty levels, and inconsistent convergence speeds. Extensive experiments have conclusively demonstrated that our multi-task fine-tuning approach outperforms both individual fine-tuning on single tasks and fine-tuning on a mixed ensemble of tasks. Moreover, MFTCoder offers efficient training capabilities, including efficient data tokenization modes and parameter efficient fine-tuning (PEFT) techniques, resulting in significantly improved speed compared to traditional fine-tuning methods. MFTCoder seamlessly integrates with several mainstream open-source LLMs, such as CodeLLama and Qwen. Our MFTCoder fine-tuned CodeFuse-DeepSeek-33B claimed the top spot on the Big Code Models Leaderboard ranked by WinRate as of January 30, 2024. MFTCoder is open-sourced at https://github.com/codefuse-ai/MFTCOder", "year": 2024, "publicationdate": "2024-08-24", "externalids": {"DOI": "10.1145/3637528.3671609"}, "doi_lower": "10.1145/3637528.3671609"}
{"paper_id": 258437095, "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation", "author_names": ["Jiawei Liu", "Chun Xia", "Yuyao Wang", "Lingming Zhang"], "venue": "Neural Information Processing Systems", "abstract": "Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.", "year": 2023, "publicationdate": "2023-05-02", "externalids": {}, "doi_lower": null}
{"paper_id": 259501019, "title": "RLTF: Reinforcement Learning from Unit Test Feedback", "author_names": ["Jiate Liu", "Yiqin Zhu", "Kaiwen Xiao", "Qiang Fu", "Xiao Han", "Wei Yang", "Deheng Ye"], "venue": "Trans. Mach. Learn. Res.", "abstract": "The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. However, current representative works either rely solely on offline frameworks, limiting the exploration of new sample spaces, or fall short in the utilization of unit test signals, not accounting for specific error locations within the code. To address these issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code. Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our code is available at: https://github.com/Zyq-scut/RLTF.", "year": 2023, "publicationdate": "2023-07-10", "externalids": {"DOI": "10.48550/arXiv.2307.04349"}, "doi_lower": "10.48550/arxiv.2307.04349"}
{"paper_id": 236493269, "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing", "author_names": ["Pengfei Liu", "Weizhe Yuan", "Jinlan Fu", "Zhengbao Jiang", "Hiroaki Hayashi", "Graham Neubig"], "venue": "ACM Computing Surveys", "abstract": "This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia–Pretrain including constantly updated survey and paperlist.", "year": 2021, "publicationdate": "2021-07-28", "externalids": {"DOI": "10.1145/3560815"}, "doi_lower": "10.1145/3560815"}
{"paper_id": 264961016, "title": "Retrieval-Augmented Generation for Code Summarization via Hybrid GNN", "author_names": ["Shangqing Liu", "Yu Chen", "Xiaofei Xie", "J. Siow", "Yang Liu"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 259075246, "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems", "author_names": ["Tianyang Liu", "Canwen Xu", "Julian McAuley"], "venue": "International Conference on Learning Representations", "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is publicly available at https://github.com/Leolty/repobench.", "year": 2023, "publicationdate": "2023-06-05", "externalids": {"DOI": "10.48550/arXiv.2306.03091"}, "doi_lower": "10.48550/arxiv.2306.03091"}
{"paper_id": 258866038, "title": "Uncovering and Quantifying Social Biases in Code Generation", "author_names": ["Y. Liu", "Xiaokang Chen", "Yan Gao", "Zhe Su", "Fengji Zhang", "Daoguang Zan", "Jian-Guang Lou", "Pin-Yu Chen", "Tsung-Yi Ho"], "venue": "Neural Information Processing Systems", "abstract": "With the popularity of automatic code generation tools, such as Copilot, the study of the potential hazards of these tools is gaining importance. In this work, we explore the social bias problem in pre-trained code generation models. We propose a new paradigm to construct code prompts and successfully uncover social biases in code generation models. To quantify the severity of social biases in generated code, we develop a dataset along with three metrics to evaluate the overall social bias and fine-grained unfairness across different demographics. Experimental results on three pre-trained code generation models (Codex, InCoder, and CodeGen) with varying sizes, reveal severe social biases. Moreover, we conduct analysis to provide useful insights for further choice of code generation models with low social bias. (This work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.)", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.15377"}, "doi_lower": "10.48550/arxiv.2305.15377"}
{"paper_id": 268063676, "title": "StarCoder 2 and The Stack v2: The Next Generation", "author_names": ["Anton Lozhkov", "Raymond Li", "Loubna Ben Allal", "Federico Cassano", "J. Lamy-Poirier", "Nouamane Tazi", "Ao Tang", "Dmytro Pykhtar", "Jiawei Liu", "Yuxiang Wei", "Tianyang Liu", "Max Tian", "Denis Kocetkov", "Arthur Zucker", "Younes Belkada", "Zijian Wang", "Qian Liu", "Dmitry Abulkhanov", "Indraneil Paul", "Zhuang Li", "Wen-Ding Li", "Megan L. Risdal", "Jia Li", "Jian Zhu", "Terry Yue Zhuo", "Evgenii Zheltonozhskii", "Nii Osae Osae Dade", "W. Yu", "Lucas Krauss", "Naman Jain", "Yixuan Su", "Xuanli He", "Manan Dey", "Edoardo Abati", "Yekun Chai", "Niklas Muennighoff", "Xiangru Tang", "Muhtasham Oblokulov", "C. Akiki", "Marc Marone", "Chenghao Mou", "Mayank Mishra", "A. Gu", "Binyuan Hui", "Tri Dao", "A. Zebaze", "Olivier Dehaene", "N. Patry", "Canwen Xu", "Julian J. McAuley", "Han Hu", "Torsten Scholak", "Sébastien Paquet", "Jennifer Robinson", "C. Anderson", "Nicolas Chapados", "M. Patwary", "Nima Tajbakhsh", "Yacine Jernite", "Carlos Muñoz Ferrandis", "Lingming Zhang", "Sean Hughes", "Thomas Wolf", "Arjun Guha", "L. V. Werra", "H. D. Vries"], "venue": "arXiv.org", "abstract": "The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.", "year": 2024, "publicationdate": "2024-02-29", "externalids": {"DOI": "10.48550/arXiv.2402.19173"}, "doi_lower": "10.48550/arxiv.2402.19173"}
{"paper_id": 247450969, "title": "ReACC: A Retrieval-Augmented Code Completion Framework", "author_names": ["Shuai Lu", "Nan Duan", "Hojae Han", "Daya Guo", "Seung-won Hwang", "Alexey Svyatkovskiy"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing ”external” context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark.", "year": 2022, "publicationdate": "2022-03-15", "externalids": {"DOI": "10.48550/arXiv.2203.07722"}, "doi_lower": "10.48550/arxiv.2203.07722"}
{"paper_id": 231855531, "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation", "author_names": ["Shuai Lu", "Daya Guo", "Shuo Ren", "Junjie Huang", "Alexey Svyatkovskiy", "Ambrosio Blanco", "Colin B. Clement", "Dawn Drain", "Daxin Jiang", "Duyu Tang", "Ge Li", "Lidong Zhou", "Linjun Shou", "Long Zhou", "Michele Tufano", "Ming Gong", "Ming Zhou", "Nan Duan", "Neel Sundaresan", "Shao Kun Deng", "Shengyu Fu", "Shujie Liu"], "venue": "NeurIPS Datasets and Benchmarks", "abstract": "Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.", "year": 2021, "publicationdate": "2021-02-09", "externalids": {}, "doi_lower": null}
{"paper_id": 259164815, "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "author_names": ["Ziyang Luo", "Can Xu", "Pu Zhao", "Qingfeng Sun", "Xiubo Geng", "Wenxiang Hu", "Chongyang Tao", "Jing Ma", "Qingwei Lin", "Daxin Jiang"], "venue": "International Conference on Learning Representations", "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM", "year": 2023, "publicationdate": "2023-06-14", "externalids": {}, "doi_lower": null}
{"paper_id": 269587748, "title": "Automatic Programming: Large Language Models and Beyond", "author_names": ["Michael R. Lyu", "Baishakhi Ray", "Abhik Roychoudhury", "Shin Hwei Tan", "Patanamon Thongtanunam"], "venue": "ACM Transactions on Software Engineering and Methodology", "abstract": "Automatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large Language Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns around quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality, security, and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of automatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable automatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future, where programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of automatically generated programs from LLMs can help produce higher assurance code from LLMs, along with evidence of assurance.", "year": 2024, "publicationdate": "2024-05-03", "externalids": {"DOI": "10.1145/3708519"}, "doi_lower": "10.1145/3708519"}
{"paper_id": 254877330, "title": "Is Self-Attention Powerful to Learn Code Syntax and Semantics?", "author_names": ["Wei Ma", "Mengjie Zhao", "Xiaofei Xie", "Q. Hu", "Shangqing Liu", "Jie Zhang", "Wenhan Wang", "Yang Liu"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2212.10017"}, "doi_lower": "10.48550/arxiv.2212.10017"}
{"paper_id": 257900871, "title": "Self-Refine: Iterative Refinement with Self-Feedback", "author_names": ["Aman Madaan", "Niket Tandon", "Prakhar Gupta", "Skyler Hallinan", "Luyu Gao", "Sarah Wiegreffe", "Uri Alon", "Nouha Dziri", "Shrimai Prabhumoye", "Yiming Yang", "S. Welleck", "Bodhisattwa Prasad Majumder", "Shashank Gupta", "A. Yazdanbakhsh", "Peter Clark"], "venue": "Neural Information Processing Systems", "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {"DOI": "10.48550/arXiv.2303.17651"}, "doi_lower": "10.48550/arxiv.2303.17651"}
{"paper_id": 276458393, "title": "A review on enhancing education with AI: exploring the potential of ChatGPT, Bard, and generative AI", "author_names": ["Anduamlak Abebe Fenta"], "venue": "Discover Education", "abstract": "Generative AI systems, such as ChatGPT and Bard, can significantly enhance education by providing personalized learning experiences. They can tailor content, quizzes, and explanations to individual students; act as virtual tutors, and offer instant feedback. However, privacy concerns have arisen because of the need for personal data. Ensuring ethical AI use, transparency, and student privacy is essential. Although AI can automate tasks and analyze student performance, there are concerns about job displacement and the devaluation of human teaching. The rapid advancement of AI has raised questions about the relevance of traditional curricula and the need for educators’ upskilling. Integrating AI into education requires careful consideration of ethical, privacy, and pedagogical factors to maximize benefits and mitigate challenges.", "year": 2025, "publicationdate": "2025-02-18", "externalids": {"DOI": "10.1007/s44217-025-00426-5"}, "doi_lower": "10.1007/s44217-025-00426-5"}
{"paper_id": 246680398, "title": "Generating Training Data with Language Models: Towards Zero-Shot Language Understanding", "author_names": ["Yu Meng", "Jiaxin Huang", "Yu Zhang", "Jiawei Han"], "venue": "Neural Information Processing Systems", "abstract": "Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class.", "year": 2022, "publicationdate": "2022-02-09", "externalids": {}, "doi_lower": null}
{"paper_id": 278545687, "title": "Text to SQL Using LLaMa-3 LLM", "author_names": ["Deepali Gohil", "Kunal Patel", "Devanand Kangane", "Jeet Maradia", "Sarvesh Ramesh Zende", "Saloni Jain"], "venue": "International Journal of Scientific Research in Computer Science Engineering and Information Technology", "abstract": null, "year": 2024, "publicationdate": "2024-12-12", "externalids": {"DOI": "10.32628/cseit2410755"}, "doi_lower": "10.32628/cseit2410755"}
{"paper_id": 125697426, "title": "Surprising Power of Small Data", "author_names": ["George Rajna"], "venue": "", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 85467156, "title": "TBCNN: A Tree-Based Convolutional Neural Network for Programming Language Processing", "author_names": ["Lili Mou", "Ge Li", "Zhi Jin", "Lu Zhang", "Tao Wang"], "venue": "arXiv.org", "abstract": "Deep neural networks have made significant breakthroughs in many fields of artificial intelligence. However, it has not been applied in the field of programming language processing. In this paper, we propose the tree-based convolutional neural network (TBCNN) to model programming languages, which contain rich and explicit tree structural information. In our model, program vector representations are learned by the \"coding\" pretraining criterion based on abstract syntax trees (ASTs); the convolutional layer explicitly captures neighboring features on the tree; with the \"binary continuous tree\" and \"3-way pooling,\" our model can deal with ASTs of different shapes and sizes.We evaluate the program vector representations empirically, showing that the coding criterion successfully captures underlying features of AST nodes, and that program vector representations significantly speed up supervised learning. We also compare TBCNN to baseline methods; our model achieves better accuracy in the task of program classification. To our best knowledge, this paper is the first to analyze programs with deep neural networks; we extend the scope of deep learning to the field of programming language processing. The experimental results validate its feasibility; they also show a promising future of this new research area.", "year": 2014, "publicationdate": "2014-09-17", "externalids": {}, "doi_lower": null}
{"paper_id": 268987616, "title": "An Investigation into Misuse of Java Security APIs by Large Language Models", "author_names": ["Zahra Mousavi", "Chadni Islam", "Kristen Moore", "A. Abuadbba", "M. A. Babar"], "venue": "ACM Asia Conference on Computer and Communications Security", "abstract": "The increasing trend of using Large Language Models (LLMs) for code generation raises the question of their capability to generate trustworthy code. While many researchers are exploring the utility of code generation for uncovering software vulnerabilities, one crucial but often overlooked aspect is the security Application Programming Interfaces (APIs). APIs play an integral role in upholding software security, yet effectively integrating security APIs presents substantial challenges. This leads to inadvertent misuse by developers, thereby exposing software to vulnerabilities. To overcome these challenges, developers may seek assistance from LLMs. In this paper, we systematically assess ChatGPT's trustworthiness in code generation for security API use cases in Java. To conduct a thorough evaluation, we compile an extensive collection of 48 programming tasks for 5 widely used security APIs. We employ both automated and manual approaches to effectively detect security API misuse in the code generated by ChatGPT for these tasks. Our findings are concerning: around 70% of the code instances across 30 attempts per task contain security API misuse, with 20 distinct misuse types identified. Moreover, for roughly half of the tasks, this rate reaches 100%, indicating that there is a long way to go before developers can rely on ChatGPT to securely implement security API code.", "year": 2024, "publicationdate": "2024-04-04", "externalids": {"DOI": "10.1145/3634737.3661134"}, "doi_lower": "10.1145/3634737.3661134"}
{"paper_id": 253244575, "title": "A Simple, Yet Effective Approach to Finding Biases in Code Generation", "author_names": ["Spyridon Mouselinos", "Mateusz Malinowski", "H. Michalewski"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Recently, high-performing code generation systems based on large language models have surfaced. They are trained on massive corpora containing much more natural text than actual executable computer code. This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances. To investigate the effect, we propose the\"block of influence\"concept, which enables a modular decomposition and analysis of the coding challenges. We introduce an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test. Finally, we demonstrate how our framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.", "year": 2022, "publicationdate": "2022-10-31", "externalids": {"DOI": "10.48550/arXiv.2211.00609"}, "doi_lower": "10.48550/arxiv.2211.00609"}
{"paper_id": 253117056, "title": "Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming", "author_names": ["Hussein Mozannar", "Gagan Bansal", "Adam Fourney", "E. Horvitz"], "venue": "arXiv.org", "abstract": "Code-recommendation systems, such as Copilot and CodeWhisperer, have the potential to improve programmer productivity by suggesting and auto-completing code. However, to fully realize their potential, we must understand how programmers interact with these systems and identify ways to improve that interaction. To make progress, we studied GitHub Copilot, a code-recommendation system used by millions of programmers daily. We developed CUPS, a taxonomy of common programmer activities when interacting with Copilot. Our study of 21 programmers, who completed coding tasks and retrospectively labeled their sessions with CUPS, showed that CUPS can help us understand how programmers interact with code-recommendation systems, revealing inefficiencies and time costs. Our insights reveal how programmers interact with Copilot and motivate new interface designs and metrics.", "year": 2022, "publicationdate": "2022-10-25", "externalids": {"DOI": "10.48550/arXiv.2210.14306"}, "doi_lower": "10.48550/arxiv.2210.14306"}
{"paper_id": 260886874, "title": "OctoPack: Instruction Tuning Code Large Language Models", "author_names": ["Niklas Muennighoff", "Qian Liu", "Qi Liu", "A. Zebaze", "Qinkai Zheng", "Binyuan Hui", "Terry Yue Zhuo", "Swayam Singh", "Xiangru Tang", "L. V. Werra", "S. Longpre"], "venue": "International Conference on Learning Representations", "abstract": "Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "year": 2023, "publicationdate": "2023-08-14", "externalids": {}, "doi_lower": null}
{"paper_id": 10849890, "title": "13 Thin Middleware for Ubiquitous Computing Koushik Sen", "author_names": ["G. Agha"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 15715668, "title": "The Attack of the Clones: A Study of the Impact of Shared Code on Vulnerability Patching", "author_names": ["Antonio Nappa", "Richard B. Johnson", "Leyla Bilge", "Juan Caballero", "Tudor Dumitras"], "venue": "IEEE Symposium on Security and Privacy", "abstract": null, "year": 2015, "publicationdate": "2015-05-17", "externalids": {"DOI": "10.1109/SP.2015.48"}, "doi_lower": "10.1109/sp.2015.48"}
{"paper_id": 256900680, "title": "LEVER: Learning to Verify Language-to-Code Generation with Execution", "author_names": ["Ansong Ni", "Srini Iyer", "Dragomir R. Radev", "Ves Stoyanov", "Wen-tau Yih", "Sida I. Wang", "Xi Victoria Lin"], "venue": "International Conference on Machine Learning", "abstract": "The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generation probability, and marginalizing over programs with the same execution results. On four datasets across the domains of table QA, math QA and basic Python programming, LEVER consistently improves over the base code LLMs(4.6% to 10.9% with code-davinci-002) and achieves new state-of-the-art results on all of them.", "year": 2023, "publicationdate": "2023-02-16", "externalids": {"DOI": "10.48550/arXiv.2302.08468"}, "doi_lower": "10.48550/arxiv.2302.08468"}
{"paper_id": 263310373, "title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models", "author_names": ["Ansong Ni", "Pengcheng Yin", "Yilun Zhao", "Martin Riddell", "Troy Feng", "Rui Shen", "Stephen Yin", "Ye Liu", "Semih Yavuz", "Caiming Xiong", "Shafiq R. Joty", "Yingbo Zhou", "Dragomir R. Radev", "Arman Cohan"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Abstract Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs. Despite promising results, there is a notable lack of a comprehensive evaluation of these models’ language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning, and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition, we assess confidence calibration, and conduct human evaluations to identify typical failures across different tasks and models. L2CEval offers a comprehensive understanding of the capabilities and limitations of LLMs in language-to-code generation. We release the evaluation framework1 and all model outputs, hoping to lay the groundwork for further future research. All future evaluations (e.g., LLaMA-3, StarCoder2, etc) will be updated on the project website: https://l2c-eval.github.io/.", "year": 2023, "publicationdate": "2023-09-29", "externalids": {"DOI": "10.1162/tacl_a_00705"}, "doi_lower": "10.1162/tacl_a_00705"}
{"paper_id": 252668917, "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis", "author_names": ["Erik Nijkamp", "Bo Pang", "Hiroaki Hayashi", "Lifu Tu", "Haiquan Wang", "Yingbo Zhou", "S. Savarese", "Caiming Xiong"], "venue": "International Conference on Learning Representations", "abstract": "Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.", "year": 2022, "publicationdate": "2022-03-25", "externalids": {}, "doi_lower": null}
{"paper_id": 249017897, "title": "Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source Code", "author_names": ["Changan Niu", "Chuanyi Li", "Bin Luo", "Vincent Ng"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Recent years have seen the successful application of deep learning to software engineering (SE). In particular, the development and use of pre-trained models of source code has enabled state-of-the-art results to be achieved on a wide variety of SE tasks. This paper provides an overview of this rapidly advancing field of research and reflects on future research directions.", "year": 2022, "publicationdate": "2022-05-24", "externalids": {"DOI": "10.48550/arXiv.2205.11739"}, "doi_lower": "10.48550/arxiv.2205.11739"}
{"paper_id": 202643539, "title": "Vol. 37, Issue 4, August 2010 / Band 37, Heft 4, August 2010", "author_names": [], "venue": "Transfusion Medicine and Hemotherapy", "abstract": null, "year": 2010, "publicationdate": "2010-07-27", "externalids": {"DOI": "10.1159/000319681"}, "doi_lower": "10.1159/000319681"}
{"paper_id": 259187989, "title": "Is Self-Repair a Silver Bullet for Code Generation?", "author_names": ["Theo X. Olausson", "J. Inala", "Chenglong Wang", "Jianfeng Gao", "Armando Solar-Lezama"], "venue": "International Conference on Learning Representations", "abstract": "Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair -- in which the model debugs and repairs its own code -- has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.", "year": 2023, "publicationdate": "2023-06-16", "externalids": {}, "doi_lower": null}
{"paper_id": 258989607, "title": "ChatGPT and large language models in gastroenterology", "author_names": ["Prateek Sharma", "S. Parasa"], "venue": "Nature reviews: Gastroenterology & hepatology", "abstract": null, "year": 2023, "publicationdate": "2023-05-30", "externalids": {"DOI": "10.1038/s41575-023-00799-8"}, "doi_lower": "10.1038/s41575-023-00799-8"}
{"paper_id": 144855737, "title": "New Online Sales Models Announced for 2006", "author_names": ["Ashleigh Bell"], "venue": "", "abstract": null, "year": 2005, "publicationdate": "2005-07-01", "externalids": {"DOI": "10.1080/17521740701695970"}, "doi_lower": "10.1080/17521740701695970"}
{"paper_id": 281374866, "title": "Code Less to Code More", "author_names": ["Federico Bruzzone", "Walter Cazzola", "L. Favalli"], "venue": "Journal of Systems and Software", "abstract": null, "year": 2025, "publicationdate": "2025-09-01", "externalids": {"DOI": "10.1016/j.jss.2025.112554"}, "doi_lower": "10.1016/j.jss.2025.112554"}
{"paper_id": 246426909, "title": "Training language models to follow instructions with human feedback", "author_names": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "John Schulman", "Jacob Hilton", "Fraser Kelton", "Luke E. Miller", "Maddie Simens", "Amanda Askell", "Peter Welinder", "P. Christiano", "Jan Leike", "Ryan J. Lowe"], "venue": "Neural Information Processing Systems", "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.", "year": 2022, "publicationdate": "2022-03-04", "externalids": {}, "doi_lower": null}
{"paper_id": 266162497, "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "author_names": ["O. Ovadia", "Meni Brief", "Moshik Mishaeli", "Oren Elisha"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.", "year": 2023, "publicationdate": "2023-12-10", "externalids": {"DOI": "10.48550/arXiv.2312.05934"}, "doi_lower": "10.48550/arxiv.2312.05934"}
{"paper_id": 260704086, "title": "Evaluating and Explaining Large Language Models for Code Using Syntactic Structures", "author_names": ["David N. Palacio", "Alejandro Velasco", "Daniel Rodríguez-Cárdenas", "Kevin Moran", "D. Poshyvanyk"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) for code are a family of high-parameter, transformer-based neural networks pre-trained on massive datasets of both natural and programming languages. These models are rapidly being employed in commercial AI-based developer tools, such as GitHub CoPilot. However, measuring and explaining their effectiveness on programming tasks is a challenging proposition, given their size and complexity. The methods for evaluating and explaining LLMs for code are inextricably linked. That is, in order to explain a model's predictions, they must be reliably mapped to fine-grained, understandable concepts. Once this mapping is achieved, new methods for detailed model evaluations are possible. However, most current explainability techniques and evaluation benchmarks focus on model robustness or individual task performance, as opposed to interpreting model predictions. To this end, this paper introduces ASTxplainer, an explainability method specific to LLMs for code that enables both new methods for LLM evaluation and visualizations of LLM predictions that aid end-users in understanding model predictions. At its core, ASTxplainer provides an automated method for aligning token predictions with AST nodes, by extracting and aggregating normalized model logits within AST structures. To demonstrate the practical benefit of ASTxplainer, we illustrate the insights that our framework can provide by performing an empirical evaluation on 12 popular LLMs for code using a curated dataset of the most popular GitHub projects. Additionally, we perform a user study examining the usefulness of an ASTxplainer-derived visualization of model predictions aimed at enabling model users to explain predictions. The results of these studies illustrate the potential for ASTxplainer to provide insights into LLM effectiveness, and aid end-users in understanding predictions.", "year": 2023, "publicationdate": "2023-08-07", "externalids": {"DOI": "10.48550/arXiv.2308.03873"}, "doi_lower": "10.48550/arxiv.2308.03873"}
{"paper_id": 11080756, "title": "Bleu: a Method for Automatic Evaluation of Machine Translation", "author_names": ["Kishore Papineni", "Salim Roukos", "T. Ward", "Wei-Jing Zhu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.", "year": 2002, "publicationdate": "2002-07-06", "externalids": {"DOI": "10.3115/1073083.1073135"}, "doi_lower": "10.3115/1073083.1073135"}
{"paper_id": 269005612, "title": "The Fact Selection Problem in LLM-Based Program Repair", "author_names": ["Nikhil Parasaram", "Huijie Yan", "Boyu Yang", "Zineb Flahy", "Abriele Qudsi", "Damian Ziaber", "Earl T. Barr", "Sergey Mechtaev"], "venue": "International Conference on Software Engineering", "abstract": "Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs). Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs? To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open -source Python projects within the BugsInPy benchmark. Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial. Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it. Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes. These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance. We found that there is no one-size-fits-all set of facts for bug repair. Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt. This model significantly surpasses the performance of the best generic fact set. To underscore the significance of the fact selection problem, we benchmarked Maniple against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods. On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17% above the best configuration.", "year": 2024, "publicationdate": "2024-04-08", "externalids": {"DOI": "10.1109/ICSE55347.2025.00162"}, "doi_lower": "10.1109/icse55347.2025.00162"}
{"paper_id": 237304122, "title": "Retrieval Augmented Code Generation and Summarization", "author_names": ["Md. Rizwan Parvez", "W. Ahmad", "Saikat Chakraborty", "Baishakhi Ray", "Kai-Wei Chang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Software developers write a lot of source code and documentation during software development. Intrinsically, developers often recall parts of source code or code summaries that they had written in the past while implementing software or documenting them. To mimic developers' code or summary generation behavior, we propose a retrieval augmented framework, REDCODER, that retrieves relevant code or summaries from a retrieval database and provides them as a supplement to code generation or summarization models. REDCODER has a couple of uniqueness. First, it extends the state-of-the-art dense retrieval technique to search for relevant code or summaries. Second, it can work with retrieval databases that include unimodal (only code or natural language description) or bimodal instances (code-description pairs). We conduct experiments and extensive analysis on two benchmark datasets of code generation and summarization in Java and Python, and the promising results endorse the effectiveness of our proposed retrieval augmented framework.", "year": 2021, "publicationdate": "2021-08-26", "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.232"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.232"}
{"paper_id": 271520201, "title": "Evaluating In-Context Learning of Libraries for Code Generation", "author_names": ["Arkil Patel", "Siva Reddy", "Dzmitry Bahdanau", "Pradeep Dasigi"], "venue": "arXiv.org", "abstract": "Contemporary Large Language Models (LLMs) exhibit a high degree of code generation and comprehension capability. A particularly promising area is their ability to interpret code modules from unfamiliar libraries for solving user-instructed tasks. Recent work has shown that large proprietary LLMs can learn novel library usage in-context from demonstrations. These results raise several open questions: whether demonstrations of library usage is required, whether smaller (and more open) models also possess such capabilities, etc. In this work, we take a broader approach by systematically evaluating a diverse array of LLMs across three scenarios reflecting varying levels of domain specialization to understand their abilities and limitations in generating code based on libraries defined in-context. Our results show that even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context. Our findings further reveal that LLMs exhibit a surprisingly high proficiency in learning novel library modules even when provided with just natural language descriptions or raw code implementations of the functions, which are often cheaper to obtain than demonstrations. Overall, our results pave the way for harnessing LLMs in more adaptable and dynamic coding environments.", "year": 2023, "publicationdate": "2023-11-16", "externalids": {"DOI": "10.48550/arXiv.2311.09635"}, "doi_lower": "10.48550/arxiv.2311.09635"}
{"paper_id": 256059103, "title": "Preference-based Representation Learning for Collections / Aissatou Diallo ; Iryna Gurevych, Johannes Fürnkranz, Fabio Aiolli", "author_names": ["Aïssatou Diallo"], "venue": "", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 231941039, "title": "Program Comprehension and Code Complexity Metrics: An fMRI Study", "author_names": ["Norman Peitek", "S. Apel", "Chris Parnin", "A. Brechmann", "J. Siegmund"], "venue": "International Conference on Software Engineering", "abstract": "Background: Researchers and practitioners have been using code complexity metrics for decades to predict how developers comprehend a program. While it is plausible and tempting to use code metrics for this purpose, their validity is debated, since they rely on simple code properties and rarely consider particularities of human cognition. Aims: We investigate whether and how code complexity metrics reflect difficulty of program comprehension. Method: We have conducted a functional magnetic resonance imaging (fMRI) study with 19 participants observing program comprehension of short code snippets at varying complexity levels. We dissected four classes of code complexity metrics and their relationship to neuronal, behavioral, and subjective correlates of program comprehension, overall analyzing more than 41 metrics. Results: While our data corroborate that complexity metrics can-to a limited degree-explain programmers' cognition in program comprehension, fMRI allowed us to gain insights into why some code properties are difficult to process. In particular, a code's textual size drives programmers' attention, and vocabulary size burdens programmers' working memory. Conclusion: Our results provide neuro-scientific evidence supporting warnings of prior research questioning the validity of code complexity metrics and pin down factors relevant to program comprehension. Future Work: We outline several follow-up experiments investigating fine-grained effects of code complexity and describe possible refinements to code complexity metrics.", "year": 2021, "publicationdate": "2021-05-01", "externalids": {"DOI": "10.1109/ICSE43902.2021.00056"}, "doi_lower": "10.1109/icse43902.2021.00056"}
{"paper_id": 268359001, "title": "RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion", "author_names": ["H. N. Phan", "H. N. Phan", "Tien N. Nguyen", "Nghi D. Q. Bui"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2403.06095"}, "doi_lower": "10.48550/arxiv.2403.06095"}
{"paper_id": 268819785, "title": "Stable Code Technical Report", "author_names": ["Nikhil Pinnaparaju", "Reshinth Adithyan", "Duy Phung", "J. Tow", "James Baicoianu", "Ashish Datta", "Maksym Zhuravinskyi", "Dakota Mahan", "Marco Bellagente", "Carlos Riquelme", "Nathan Cooper"], "venue": "arXiv.org", "abstract": "We introduce Stable Code, the first in our new-generation of code language models series, which serves as a general-purpose base code language model targeting code completion, reasoning, math, and other software engineering-based tasks. Additionally, we introduce an instruction variant named Stable Code Instruct that allows conversing with the model in a natural chat interface for performing question-answering and instruction-based tasks. In this technical report, we detail the data and training procedure leading to both models. Their weights are available via Hugging Face for anyone to download and use at https://huggingface.co/stabilityai/stable-code-3b and https://huggingface.co/stabilityai/stable-code-instruct-3b. This report contains thorough evaluations of the models, including multilingual programming benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of its release, Stable Code is the state-of-the-art open model under 3B parameters and even performs comparably to larger models of sizes 7 billion and 15 billion parameters on the popular Multi-PL benchmark. Stable Code Instruct also exhibits state-of-the-art performance on the MT-Bench coding tasks and on Multi-PL completion compared to other instruction tuned models. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.", "year": 2024, "publicationdate": "2024-04-01", "externalids": {"DOI": "10.48550/arXiv.2404.01226"}, "doi_lower": "10.48550/arxiv.2404.01226"}
{"paper_id": 237347130, "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation", "author_names": ["Ofir Press", "Noah A. Smith", "M. Lewis"], "venue": "International Conference on Learning Representations", "abstract": "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.", "year": 2021, "publicationdate": "2021-08-27", "externalids": {}, "doi_lower": null}
{"paper_id": 263671523, "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!", "author_names": ["Xiangyu Qi", "Yi Zeng", "Tinghao Xie", "Pin-Yu Chen", "Ruoxi Jia", "Prateek Mittal", "Peter Henderson"], "venue": "International Conference on Learning Representations", "abstract": "Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.", "year": 2023, "publicationdate": "2023-10-05", "externalids": {"DOI": "10.48550/arXiv.2310.03693"}, "doi_lower": "10.48550/arxiv.2310.03693"}
{"paper_id": 49313245, "title": "Improving Language Understanding by Generative Pre-Training", "author_names": ["Alec Radford", "Karthik Narasimhan"], "venue": "", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 160025533, "title": "Language Models are Unsupervised Multitask Learners", "author_names": ["Alec Radford", "Jeff Wu", "R. Child", "D. Luan", "Dario Amodei", "I. Sutskever"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 258959321, "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "author_names": ["Rafael Rafailov", "Archit Sharma", "E. Mitchell", "Stefano Ermon", "Christopher D. Manning", "Chelsea Finn"], "venue": "Neural Information Processing Systems", "abstract": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {}, "doi_lower": null}
{"paper_id": 247922681, "title": "Evaluating the Text-to-SQL Capabilities of Large Language Models", "author_names": ["Nitarshan Rajkumar", "Raymond Li", "Dzmitry Bahdanau"], "venue": "arXiv.org", "abstract": "We perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.", "year": 2022, "publicationdate": "2022-03-15", "externalids": {"DOI": "10.48550/arXiv.2204.00498"}, "doi_lower": "10.48550/arxiv.2204.00498"}
{"paper_id": 67724178, "title": "A Systematic Review of Interaction in Search-Based Software Engineering", "author_names": ["Aurora Ramírez", "J. Romero", "C. Simons"], "venue": "IEEE Transactions on Software Engineering", "abstract": "Search-Based Software Engineering (SBSE) has been successfully applied to automate a wide range of software development activities. Nevertheless, in those software engineering problems where human evaluation and preference are crucial, such insights have proved difficult to characterize in search, and solutions might not look natural when that is the expectation. In an attempt to address this, an increasing number of researchers have reported the incorporation of the ’human-in-the-loop’ during search and interactive SBSE has attracted significant attention recently. However, reported results are fragmented over different development phases, and a great variety of novel interactive approaches and algorithmic techniques have emerged. To better integrate these results, we have performed a systematic literature review of interactive SBSE. From a total of 669 papers, 26 primary studies were identified. To enable their analysis, we formulated a classification scheme focused on four crucial aspects of interactive search, i.e., the problem formulation, search technique, interactive approach, and the empirical framework. Our intention is that the classification scheme affords a methodological approach for interactive SBSE. Lastly, as well as providing a detailed cross analysis, we identify and discuss some open issues and potential future trends for the research community.", "year": 2019, "publicationdate": "2019-08-01", "externalids": {"DOI": "10.1109/TSE.2018.2803055"}, "doi_lower": "10.1109/tse.2018.2803055"}
{"paper_id": 202643539, "title": "Vol. 37, Issue 4, August 2010 / Band 37, Heft 4, August 2010", "author_names": [], "venue": "Transfusion Medicine and Hemotherapy", "abstract": null, "year": 2010, "publicationdate": "2010-07-27", "externalids": {"DOI": "10.1159/000319681"}, "doi_lower": "10.1159/000319681"}
{"paper_id": 221836101, "title": "CodeBLEU: a Method for Automatic Evaluation of Code Synthesis", "author_names": ["Shuo Ren", "Daya Guo", "Shuai Lu", "Long Zhou", "Shujie Liu", "Duyu Tang", "M. Zhou", "Ambrosio Blanco", "Shuai Ma"], "venue": "arXiv.org", "abstract": "Evaluation metrics play a vital role in the growth of an area as it defines the standard of distinguishing between good and bad models. In the area of code synthesis, the commonly used evaluation metric is BLEU or perfect accuracy, but they are not suitable enough to evaluate codes, because BLEU is originally designed to evaluate the natural language, neglecting important syntactic and semantic features of codes, and perfect accuracy is too strict thus it underestimates different outputs with the same semantic logic. To remedy this, we introduce a new automatic evaluation metric, dubbed CodeBLEU. It absorbs the strength of BLEU in the n-gram match and further injects code syntax via abstract syntax trees (AST) and code semantics via data-flow. We conduct experiments by evaluating the correlation coefficient between CodeBLEU and quality scores assigned by the programmers on three code synthesis tasks, i.e., text-to-code, code translation, and code refinement. Experimental results show that our proposed CodeBLEU can achieve a better correlation with programmer assigned scores compared with BLEU and accuracy.", "year": 2020, "publicationdate": "2020-09-22", "externalids": {}, "doi_lower": null}
{"paper_id": 111446439, "title": "The idea of software radar", "author_names": ["Wan-xuan Fu"], "venue": "", "abstract": null, "year": 2010, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 272841311, "title": "AI Image Generator Utilizing OpenAI and Replit", "author_names": ["D. Garg", "Mr. Adeen Shaikh"], "venue": "INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT", "abstract": "In this research, we leverage OpenAI's DALL- E 3 API in the assistance of generating images from a web interface which has been built on Replit. DALL-E 3 is a very advanced generative AI model due to its ability of generating complex and creative images from textual inputs. We intend to make use of the capabilities of DALL-E 3 and have it available to users in the generation of high-quality, custom images. Replit-the popular cooperative programming platform-secures the setup of the development environment, API request and response management, and user interfaces which allow users to interact with these AI models. The following apparently consider these factors at the implementation of AI in graphic design: better creativity, effective usage of time, and chance of creating unique aesthetic contents. Key Words: DALL-E 3 API, Generative AI, Web interface, Replit, Custom images, Graphic design.", "year": 2024, "publicationdate": "2024-09-23", "externalids": {"DOI": "10.55041/ijsrem37566"}, "doi_lower": "10.55041/ijsrem37566"}
{"paper_id": 267028402, "title": "Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering", "author_names": ["T. Ridnik", "Dedy Kredo", "Itamar Friedman"], "venue": "arXiv.org", "abstract": "Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium", "year": 2024, "publicationdate": "2024-01-16", "externalids": {"DOI": "10.48550/arXiv.2401.08500"}, "doi_lower": "10.48550/arxiv.2401.08500"}
{"paper_id": 259164815, "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "author_names": ["Ziyang Luo", "Can Xu", "Pu Zhao", "Qingfeng Sun", "Xiubo Geng", "Wenxiang Hu", "Chongyang Tao", "Jing Ma", "Qingwei Lin", "Daxin Jiang"], "venue": "International Conference on Learning Representations", "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM", "year": 2023, "publicationdate": "2023-06-14", "externalids": {}, "doi_lower": null}
{"paper_id": 256846588, "title": "The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development", "author_names": ["Steven I. Ross", "Fernando Martinez", "Stephanie Houde", "Michael J. Muller", "Justin D. Weisz"], "venue": "International Conference on Intelligent User Interfaces", "abstract": "Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model’s responses. We developed a prototype system – the Programmer’s Assistant – in order to explore the utility of conversational interactions grounded in code, as well as software engineers’ receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant’s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.", "year": 2023, "publicationdate": "2023-02-14", "externalids": {"DOI": "10.1145/3581641.3584037"}, "doi_lower": "10.1145/3581641.3584037"}
{"paper_id": 261100919, "title": "Code Llama: Open Foundation Models for Code", "author_names": ["Baptiste Rozière", "Jonas Gehring", "Fabian Gloeckle", "Sten Sootla", "Itai Gat", "Xiaoqing Tan", "Yossi Adi", "Jingyu Liu", "Tal Remez", "J. Rapin", "Artyom Kozhevnikov", "I. Evtimov", "Joanna Bitton", "Manish P Bhatt", "Cris-tian Cantón Ferrer", "Aaron Grattafiori", "Wenhan Xiong", "Alexandre D'efossez", "Jade Copet", "Faisal Azhar", "Hugo Touvron", "Louis Martin", "Nicolas Usunier", "Thomas Scialom", "Gabriel Synnaeve"], "venue": "arXiv.org", "abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.", "year": 2023, "publicationdate": "2023-08-24", "externalids": {"DOI": "10.48550/arXiv.2308.12950"}, "doi_lower": "10.48550/arxiv.2308.12950"}
{"paper_id": 239009562, "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "author_names": ["Victor Sanh", "Albert Webson", "Colin Raffel", "Stephen H. Bach", "Lintang Sutawika", "Zaid Alyafeai", "Antoine Chaffin", "Arnaud Stiegler", "Teven Le Scao", "Arun Raja", "Manan Dey", "M Saiful Bari", "Canwen Xu", "Urmish Thakker", "S. Sharma", "Eliza Szczechla", "Taewoon Kim", "Gunjan Chhablani", "Nihal V. Nayak", "Debajyoti Datta", "Jonathan D. Chang", "Mike Tian-Jian Jiang", "Han Wang", "Matteo Manica", "Sheng Shen", "Zheng-Xin Yong", "Harshit Pandey", "Rachel Bawden", "Thomas Wang", "Trishala Neeraj", "Jos Rozen", "Abheesht Sharma", "Andrea Santilli", "Thibault Févry", "Jason Alan Fries", "R. Teehan", "Stella Biderman", "Leo Gao", "T. Bers", "Thomas Wolf", "Alexander M. Rush"], "venue": "arXiv.org", "abstract": "Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.", "year": 2021, "publicationdate": "2021-10-15", "externalids": {}, "doi_lower": null}
{"paper_id": 220363858, "title": "You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion", "author_names": ["R. Schuster", "Congzheng Song", "Eran Tromer", "Vitaly Shmatikov"], "venue": "USENIX Security Symposium", "abstract": "Code autocompletion is an integral feature of modern code editors and IDEs. The latest generation of autocompleters uses neural language models, trained on public open-source code repositories, to suggest likely (not just statically feasible) completions given the current context. \nWe demonstrate that neural code autocompleters are vulnerable to data- and model-poisoning attacks. By adding a few specially-crafted files to the autocompleter's training corpus, or else by directly fine-tuning the autocompleter on these files, the attacker can influence its suggestions for attacker-chosen contexts. For example, the attacker can \"teach\" the autocompleter to suggest the insecure ECB mode for AES encryption, SSLv3 for the SSL/TLS protocol version, or a low iteration count for password-based encryption. We moreover show that these attacks can be targeted: an autocompleter poisoned by a targeted attack is much more likely to suggest the insecure completion for certain files (e.g., those from a specific repo). \nWe quantify the efficacy of targeted and untargeted data- and model-poisoning attacks against state-of-the-art autocompleters based on Pythia and GPT-2. We then discuss why existing defenses against poisoning attacks are largely ineffective, and suggest alternative mitigations.", "year": 2020, "publicationdate": "2020-07-05", "externalids": {}, "doi_lower": null}
{"paper_id": 3725815, "title": "Self-Attention with Relative Position Representations", "author_names": ["Peter Shaw", "Jakob Uszkoreit", "Ashish Vaswani"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.", "year": 2018, "publicationdate": "2018-03-06", "externalids": {"DOI": "10.18653/v1/N18-2074"}, "doi_lower": "10.18653/v1/n18-2074"}
{"paper_id": 12462234, "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "author_names": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc V. Le", "Geoffrey E. Hinton", "J. Dean"], "venue": "International Conference on Learning Representations", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "year": 2017, "publicationdate": "2017-01-23", "externalids": {}, "doi_lower": null}
{"paper_id": 260202985, "title": "PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback", "author_names": ["Bo Shen", "Jiaxin Zhang", "Taihong Chen", "Daoguang Zan", "Bing Geng", "An Fu", "Muhan Zeng", "Ailun Yu", "Jichuan Ji", "Jingyang Zhao", "Yuenan Guo", "Qianxiang Wang"], "venue": "arXiv.org", "abstract": "Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.", "year": 2023, "publicationdate": "2023-07-27", "externalids": {"DOI": "10.48550/arXiv.2307.14936"}, "doi_lower": "10.48550/arxiv.2307.14936"}
{"paper_id": 261660562, "title": "Greening Large Language Models of Code", "author_names": ["Jieke Shi", "Zhou Yang", "Hong Jin Kang", "Bowen Xu", "Junda He", "D. Lo"], "venue": "2024 IEEE/ACM 46th International Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS)", "abstract": "Large language models of code have shown remarkable effectiveness across various software engineering tasks. Despite the availability of many cloud services built upon these powerful models, there remain several scenarios where developers cannot take full advantage of them, stemming from factors such as restricted or unreliable internet access, institutional privacy policies that prohibit external transmission of code to third-party vendors, and more. Therefore, developing a compact, efficient, and yet energy-saving model for deployment on developers’ devices becomes essential.To this aim, we propose Avatar, a novel approach that crafts a deployable model from a large language model of code by optimizing it in terms of model size, inference latency, energy consumption, and carbon footprint while maintaining a comparable level of effectiveness (e.g., prediction accuracy on downstream tasks). The key idea of Avatar is to formulate the optimization of language models as a multi-objective configuration tuning problem and solve it with the help of a Satisfiability Modulo Theories (SMT) solver and a tailored optimization algorithm. The SMT solver is used to form an appropriate configuration space, while the optimization algorithm identifies the Pareto-optimal set of configurations for training the optimized models using knowledge distillation. We evaluate Avatar with two popular language models of code, i.e., CodeBERT and GraphCodeBERT, on two popular tasks, i.e., vulnerability prediction and clone detection. We use Avatar to produce optimized models with a small size (3 MB), which is 160× smaller than the original large models. On the two tasks, the optimized models significantly reduce the energy consumption (up to 184× less), carbon footprint (up to 157× less), and inference latency (up to 76× faster), with only a negligible loss in effectiveness (1.67%).Lay AbstractLarge language models of code have proven to be highly effective for various software engineering tasks, such as spotting program defects and helping developers write code. While many cloud services built on these models (e.g., GitHub Copilot) are now accessible, several factors, such as unreliable internet access (e.g., over 20% of GitHub Copilot’s issues are related to network connectivity [22]) and privacy concerns (e.g., Apple has banned the internal use of external AI tools to protect confidential data [53]), hinder developers from fully utilizing these services. Therefore, deploying language models of code on developers’ devices like laptops appears promising. However, local deployment faces challenges: (1) Consumer-grade personal devices typically lack sufficient memory and the high-performance CPUs/GPUs required for efficient model execution; (2) Even if the hardware requirements are met, deploying the models on many devices can result in considerable energy consumption and carbon emissions, negatively impacting environmental sustainability.To address these challenges, we present Avatar, an innovative approach that optimizes large language models of code and enables their deployment on consumer-grade devices. Avatar can optimize two popular models from a large size of 481 MB to a compact size of 3 MB, resulting in significant reductions in inference time, energy consumption, and carbon emissions by hundreds of times. Our technique effectively lowers the entry barrier for leveraging large language models of code, making them available to ordinary developers without the need for high-performance computing equipment. Furthermore, it also contributes to a more sustainable and user-friendly software development environment.", "year": 2023, "publicationdate": "2023-09-08", "externalids": {"DOI": "10.1145/3639475.3640097"}, "doi_lower": "10.1145/3639475.3640097"}
{"paper_id": 265295529, "title": "Refactoring Programs Using Large Language Models with Few-Shot Examples", "author_names": ["Atsushi Shirafuji", "Yusuke Oda", "Jun Suzuki", "Makoto Morishita", "Yutaka Watanobe"], "venue": "Asia-Pacific Software Engineering Conference", "abstract": "A less complex and more straightforward program is a crucial factor that enhances its maintainability and makes writing secure and bug-free programs easier. However, due to its heavy workload and the risks of breaking the working programs, programmers are reluctant to do code refactoring, and thus, it also causes the loss of potential learning experiences. To mitigate this, we demonstrate the application of using a large language model (LLM), GPT-3.5, to suggest less complex versions of the user-written Python program, aiming to encourage users to learn how to write better programs. We propose a method to leverage the prompting with few-shot examples of the LLM by selecting the best-suited code refactoring examples for each target programming problem based on the prior evaluation of prompting with the one-shot example. The quantitative evaluation shows that 95.68% of programs can be refactored by generating 10 candidates each, resulting in a 17.35% reduction in the average cyclomatic complexity and a 25.84% decrease in the average number of lines after filtering only generated programs that are semantically correct. Further-more, the qualitative evaluation shows outstanding capability in code formatting, while unnecessary behaviors such as deleting or translating comments are also observed.", "year": 2023, "publicationdate": "2023-11-20", "externalids": {"DOI": "10.1109/APSEC60848.2023.00025"}, "doi_lower": "10.1109/apsec60848.2023.00025"}
{"paper_id": 256416258, "title": "Execution-based Code Generation using Deep Reinforcement Learning", "author_names": ["Parshin Shojaee", "Aneesh Jain", "Sindhu Tipirneni", "Chandan K. Reddy"], "venue": "Trans. Mach. Learn. Res.", "abstract": "The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important to note that PPOCoder is a task-agnostic and model-agnostic framework that can be used across different code generation tasks and PLs. Extensive experiments on three code generation tasks demonstrate the effectiveness of our proposed approach compared to SOTA methods, achieving significant improvements in compilation success rates and functional correctness across different PLs.", "year": 2023, "publicationdate": "2023-01-31", "externalids": {"DOI": "10.48550/arXiv.2301.13816"}, "doi_lower": "10.48550/arxiv.2301.13816"}
{"paper_id": 261201492, "title": "’De Vries-De Vries’", "author_names": ["mr. E.H.M. Harbers", "prof. mr. G.M.F. Snijders"], "venue": "Tijdschrift voor Agrarisch Recht", "abstract": null, "year": 2008, "publicationdate": "2008-05-01", "externalids": {"DOI": "10.5117/tvar2008.5.007"}, "doi_lower": "10.5117/tvar2008.5.007"}
{"paper_id": 250072448, "title": "Repository-Level Prompt Generation for Large Language Models of Code", "author_names": ["Disha Shrivastava", "H. Larochelle", "Daniel Tarlow"], "venue": "International Conference on Machine Learning", "abstract": "With the success of large language models (LLMs) of code and their use as code assistants (e.g. Codex used in GitHub Copilot), techniques for introducing domain-specific knowledge in the prompt design process become important. In this work, we propose a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using prompt proposals. The prompt proposals take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files (e.g. imports, parent class files). Our technique doesn't require any access to the weights of the LLM, making it applicable in cases where we only have black-box access to the LLM. We conduct experiments on the task of single-line code-autocompletion using code repositories taken from Google Code archives. We demonstrate that an oracle constructed from our prompt proposals gives a remarkably high relative improvement of 36% over Codex, showing the quality of these proposals. Further, we show that when we train a model to predict a prompt proposal, we can achieve significant performance gains over Codex and other baselines. We release our code, data, and trained checkpoints at: \\url{https://github.com/shrivastavadisha/repo_level_prompt_generation}.", "year": 2022, "publicationdate": "2022-06-26", "externalids": {"DOI": "10.48550/arXiv.2206.12839"}, "doi_lower": "10.48550/arxiv.2206.12839"}
{"paper_id": 264555385, "title": "CodeFusion: A Pre-trained Diffusion Model for Code Generation", "author_names": ["Mukul Singh", "J. Cambronero", "Sumit Gulwani", "Vu Le", "Carina Negreanu", "Gust Verbruggen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.", "year": 2023, "publicationdate": "2023-10-26", "externalids": {"DOI": "10.48550/arXiv.2310.17680"}, "doi_lower": "10.48550/arxiv.2310.17680"}
{"paper_id": 280975614, "title": "Analysing Code-Based Retrieval Augmented Generation Methods for Knowledge Retention", "author_names": ["Aswin Dev S P", "Jerit Joshy", "Mohammed Farhan T M", "Praneeth C F", "Dimple Elizabeth Baby"], "venue": "Access", "abstract": "Traditional Retrieval Augemented Systems (RAG) fails to capture the intricate and complex contextual information within a code repository. This is mainly due to the presence of multiple files, functions, classes and various dependencies existing among themselves. This calls for an efficient pipeline to extract additional context for the LLM agent in a RAG system. In this paper, we try to investigate two methodologies for efficiently retrieving relevant information from source code repositories, namely CodeSplitter in LlamaIndex and CodexGraph. CodeSplitter uses AST parsing to form semantically meaningful code chunks that are efficiently and precisely retrieved within structured boundaries, whereas CodexGraph employs static analysis to comprehensively create graph databases encapsulating inter-file and inter-module relationships and perform holistic reasoning. We analyse the performance of both of these pipelines by implementing a RAG system using Large Language Models like GPT-4.0 and LLaMA 3.2 to evaluate their effectiveness in improving code analysis, retrieval, and project knowledge retention.", "year": 2025, "publicationdate": "2025-06-11", "externalids": {"DOI": "10.1109/ACCESS65134.2025.11135582"}, "doi_lower": "10.1109/access65134.2025.11135582"}
{"paper_id": 233307138, "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "author_names": ["Jianlin Su", "Yu Lu", "Shengfeng Pan", "Bo Wen", "Yunfeng Liu"], "venue": "Neurocomputing", "abstract": null, "year": 2021, "publicationdate": "2021-04-20", "externalids": {"DOI": "10.1016/j.neucom.2023.127063"}, "doi_lower": "10.1016/j.neucom.2023.127063"}
{"paper_id": 202643539, "title": "Vol. 37, Issue 4, August 2010 / Band 37, Heft 4, August 2010", "author_names": [], "venue": "Transfusion Medicine and Hemotherapy", "abstract": null, "year": 2010, "publicationdate": "2010-07-27", "externalids": {"DOI": "10.1159/000319681"}, "doi_lower": "10.1159/000319681"}
{"paper_id": 273662679, "title": "Generation Probabilities Are Not Enough: Uncertainty Highlighting in AI Code Completions", "author_names": ["Helena Vasconcelos", "Gagan Bansal", "Adam Fourney", "Q. V. Liao", "Jennifer Wortman Vaughan", "Jennifer Wortman Vaughan"], "venue": "ACM Trans. Comput. Hum. Interact.", "abstract": "Large-scale generative models have enabled the development of AI-powered code completion tools to assist programmers in writing code. Like all AI-powered tools, these code completion tools are not always accurate and can introduce bugs or even security vulnerabilities into code if not properly detected and corrected by a human programmer. One technique that has been proposed and implemented to help programmers locate potential errors is to highlight uncertain tokens. However, little is known about the effectiveness of this technique. Through a mixed-methods study with 30 programmers, we compare three conditions: providing the AI system’s code completion alone, highlighting tokens with the lowest likelihood of being generated by the underlying generative model, and highlighting tokens with the highest predicted likelihood of being edited by a programmer. We find that highlighting tokens with the highest predicted likelihood of being edited leads to faster task completion and more targeted edits and is subjectively preferred by study participants. In contrast, highlighting tokens according to their probability of being generated does not provide any benefit over the baseline with no highlighting. We further explore the design space of how to convey uncertainty in AI-powered code completion tools and find that programmers prefer highlights that are granular, informative, interpretable, and not overwhelming. This work contributes to building an understanding of what uncertainty means for generative models and how to convey it effectively.", "year": 2023, "publicationdate": "2023-02-14", "externalids": {"DOI": "10.1145/3702320"}, "doi_lower": "10.1145/3702320"}
{"paper_id": 267338108, "title": "Olive: An Instruction Following LLaMA Model For Odia Language", "author_names": ["Shantipriya Parida", "Sambit Sekhar", "Subhadarshi Panda", "Swateek Jena", "Abhijeet Parida", "Soumendra Kumar Sahoo", "S. Dash"], "venue": "2023 IEEE Silchar Subsection Conference (SILCON)", "abstract": "The AI community is experiencing a profound impact from Large Language Models (LLMs), and the introduction of ChatGPT and GPT-4 is prompting a reconsideration of the potential of artificial general intelligence(AGI). However, most of the LLMs are trained in English and other high-resource languages, resulting in the unavailability of LLM and its related technologies and services for many low-resource languages. In India, where only 10% of the population is proficient in English, the need for LLM models adapted to regional languages becomes crucial.In this paper, we emphasized the need for LLM for the low-resource Odia language by evaluating the available LLM-supporting Odia language. We describe the development process of the instruction-tuning LLM model for Odia. The developed instruction tuning Odia LLM is available freely for research and non-commercial purposes.", "year": 2023, "publicationdate": "2023-11-03", "externalids": {"DOI": "10.1109/SILCON59133.2023.10404195"}, "doi_lower": "10.1109/silcon59133.2023.10404195"}
{"paper_id": 268379206, "title": "Gemma: Open Models Based on Gemini Research and Technology", "author_names": ["Gemma Team Thomas Mesnard", "Cassidy Hardin", "Robert Dadashi", "Surya Bhupatiraju", "Shreya Pathak", "L. Sifre", "Morgane Rivière", "Mihir Kale", "J Christopher Love", "P. Tafti", "L'eonard Hussenot", "A. Chowdhery", "Adam Roberts", "Aditya Barua", "Alex Botev", "Alex Castro-Ros", "Ambrose Slone", "Am'elie H'eliou", "Andrea Tacchetti", "Anna Bulanova", "Antonia Paterson", "Beth Tsai", "Bobak Shahriari", "Charline Le Lan", "Christopher A. Choquette-Choo", "Clé-ment Crepy", "Daniel Cer", "Daphne Ippolito", "David Reid", "Elena Buchatskaya", "Eric Ni", "Eric Noland", "Geng Yan", "George Tucker", "George-Christian Muraru", "Grigory Rozhdestvenskiy", "H. Michalewski", "Ian Tenney", "Ivan Grishchenko", "Jacob Austin", "James Keeling", "Jane Labanowski", "Jean-Baptiste Lespiau", "J. Stanway", "Jenny Brennan", "Jeremy Chen", "Johan Ferret", "Justin Chiu", "J. Mao-Jones", "Kather-ine Lee", "Kathy Yu", "Katie Millican", "Lars Lowe Sjoesund", "Lisa Lee", "Lucas Dixon", "Machel Reid", "Maciej Mikuła", "Mateo Wirth", "Michael Sharman", "Nikolai Chinaev", "Nithum Thain", "Olivier Bachem", "Os-car Chang", "Oscar Wahltinez", "Paige Bailey", "Paul Michel", "Petko Yotov", "Pier Giuseppe Sessa", "R. Chaabouni", "R. Comanescu", "Reena Jana", "Rohan Anil", "Ross Mcilroy", "Ruibo Liu", "Ryan Mullins", "Samuel L. Smith", "Sebastian Borgeaud", "Sertan Girgin", "Sholto Douglas", "Shree Pandya", "Siamak Shakeri", "Soham De", "Ted Klimenko", "Tom Hennigan", "Vladimir Feinberg", "Wojciech Stokowiec", "Yu-hui Chen", "Zafarali Ahmed", "Zhitao Gong", "Tris Warkentin", "Ludovic Peran", "Minh Giang", "Clément Farabet", "O. Vinyals", "Jeffrey Dean", "K. Kavukcuoglu", "D. Hassabis", "Z. Ghahramani", "Douglas Eck", "Joelle Barral", "Fernando Pereira", "Eli Collins", "Armand Joulin", "Noah Fiedel", "Evan Senter", "Alek Andreev", "Kathleen Kenealy"], "venue": "arXiv.org", "abstract": "This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.", "year": 2024, "publicationdate": "2024-03-13", "externalids": {"DOI": "10.48550/arXiv.2403.08295"}, "doi_lower": "10.48550/arxiv.2403.08295"}
{"paper_id": 254926675, "title": "Benchmarking Large Language Models for Automated Verilog RTL Code Generation", "author_names": ["Shailja Thakur", "Baleegh Ahmad", "Zhenxing Fan", "H. Pearce", "Benjamin Tan", "R. Karri", "Brendan Dolan-Gavitt", "S. Garg"], "venue": "Design, Automation and Test in Europe", "abstract": "Automating hardware design could obviate a signif-icant amount of human error from the engineering process and lead to fewer errors. Verilog is a popular hardware description language to model and design digital systems, thus generating Verilog code is a critical first step. Emerging large language models (LLMs) are able to write high-quality code in other programming languages. In this paper, we characterize the ability of LLMs to generate useful Verilog. For this, we fine-tune pre-trained LLMs on Verilog datasets collected from GitHub and Verilog textbooks. We construct an evaluation framework comprising test-benches for functional analysis and a flow to test the syntax of Verilog code generated in response to problems of varying difficulty. Our findings show that across our problem scenarios, the fine-tuning results in LLMs more capable of producing syntactically correct code (25.9% overall). Further, when analyzing functional correctness, a fine-tuned open-source CodeGen LLM can outperform the state-of-the-art commercial Codex LLM (6.5% overall). We release our training/evaluation scripts and LLM checkpoints as open source contributions.", "year": 2022, "publicationdate": "2022-12-13", "externalids": {"DOI": "10.23919/DATE56975.2023.10137086"}, "doi_lower": "10.23919/date56975.2023.10137086"}
{"paper_id": 226749075, "title": "Processing Scripts of the ALPACA Dataset", "author_names": ["Felix M. Riese"], "venue": "", "abstract": null, "year": 2020, "publicationdate": null, "externalids": {"DOI": "10.5281/ZENODO.3871459"}, "doi_lower": "10.5281/zenodo.3871459"}
{"paper_id": 259950998, "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "author_names": ["Hugo Touvron", "Louis Martin", "Kevin R. Stone", "Peter Albert", "Amjad Almahairi", "Yasmine Babaei", "Niko-lay Bashlykov", "Soumya Batra", "Prajjwal Bhargava", "Shruti Bhosale", "D. Bikel", "Lukas Blecher", "Cris-tian Cantón Ferrer", "Moya Chen", "Guillem Cucurull", "David Esiobu", "Jude Fernandes", "J. Fu", "Wenyin Fu", "Brian Fuller", "Cynthia Gao", "Vedanuj Goswami", "Naman Goyal", "A. Hartshorn", "Saghar Hosseini", "Rui Hou", "Hakan Inan", "Marcin Kardas", "Viktor Kerkez", "Madian Khabsa", "Isabel M. Kloumann", "A. Korenev", "Punit Singh Koura", "M. Lachaux", "Thibaut Lavril", "Jenya Lee", "Diana Liskovich", "Yinghai Lu", "Yuning Mao", "Xavier Martinet", "Todor Mihaylov", "Pushkar Mishra", "Igor Molybog", "Yixin Nie", "Andrew Poulton", "J. Reizenstein", "Rashi Rungta", "Kalyan Saladi", "A. Schelten", "Ruan Silva", "Eric Michael Smith", "R. Subramanian", "Xia Tan", "Binh Tang", "Ross Taylor", "Adina Williams", "Jian Xiang Kuan", "Puxin Xu", "Zhengxu Yan", "Iliyan Zarov", "Yuchen Zhang", "Angela Fan", "M. Kambadur", "Sharan Narang", "Aur'elien Rodriguez", "Robert Stojnic", "Sergey Edunov", "Thomas Scialom"], "venue": "arXiv.org", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "year": 2023, "publicationdate": "2023-07-18", "externalids": {}, "doi_lower": null}
{"paper_id": 258564113, "title": "Transformers in Natural Language Processing", "author_names": ["François Yvon"], "venue": "International Conference on Advances in Computing and Artificial Intelligence", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.1007/978-3-031-24349-3_6"}, "doi_lower": "10.1007/978-3-031-24349-3_6"}
{"paper_id": 258714748, "title": "Synthetic data, real errors: how (not) to publish and use synthetic data", "author_names": ["B. V. Breugel", "Zhaozhi Qian", "M. Schaar"], "venue": "International Conference on Machine Learning", "abstract": "Generating synthetic data through generative models is gaining interest in the ML community and beyond, promising a future where datasets can be tailored to individual needs. Unfortunately, synthetic data is usually not perfect, resulting in potential errors in downstream tasks. In this work we explore how the generative process affects the downstream ML task. We show that the naive synthetic data approach -- using synthetic data as if it is real -- leads to downstream models and analyses that do not generalize well to real data. As a first step towards better ML in the synthetic data regime, we introduce Deep Generative Ensemble (DGE) -- a framework inspired by Deep Ensembles that aims to implicitly approximate the posterior distribution over the generative process model parameters. DGE improves downstream model training, evaluation, and uncertainty quantification, vastly outperforming the naive approach on average. The largest improvements are achieved for minority classes and low-density regions of the original data, for which the generative uncertainty is largest.", "year": 2023, "publicationdate": "2023-05-16", "externalids": {"DOI": "10.48550/arXiv.2305.09235"}, "doi_lower": "10.48550/arxiv.2305.09235"}
{"paper_id": 233210249, "title": "Not All Attention Is All You Need", "author_names": ["Hongqiu Wu", "Hai Zhao", "Min Zhang"], "venue": "arXiv.org", "abstract": "Beyond the success story of pre-trained language models (PrLMs) in recent natural language processing, they are susceptible to over-fitting due to unusual large model size. To this end, dropout serves as a therapy. However, existing methods like random-based, knowledge-based and search-based dropout are more general but less effective onto self-attention based models, which are broadly chosen as the fundamental architecture of PrLMs. In this paper, we propose a novel dropout method named AttendOut to let self-attention empowered PrLMs capable of more robust task-specific tuning. We demonstrate that state-of-the-art models with elaborate training design may achieve much stronger results. We verify the universality of our approach on extensive natural language processing tasks.", "year": 2021, "publicationdate": "2021-04-10", "externalids": {}, "doi_lower": null}
{"paper_id": 258832435, "title": "GPT-SW3: An Autoregressive Language Model for the Nordic Languages", "author_names": ["Ariel Ekgren", "Amaru Cuba Gyllensten", "F. Stollenwerk", "Joey Öhman", "T. Isbister", "Evangelia Gogoulou", "F. Carlsson", "Alice Heiman", "Judit Casademont", "Magnus Sahlgren"], "venue": "arXiv.org", "abstract": "This paper details the process of developing the first native large generative language model for the Nordic languages, GPT-SW3. We cover all parts of the development process, from data collection and processing, training configuration and instruction finetuning, to evaluation and considerations for release strategies. We hope that this paper can serve as a guide and reference for other researchers that undertake the development of large generative models for smaller languages.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.48550/arXiv.2305.12987"}, "doi_lower": "10.48550/arxiv.2305.12987"}
{"paper_id": 266977002, "title": "Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code Generation", "author_names": ["Chong Wang", "Jian Zhang", "Yebo Feng", "Tianlin Li", "Weisong Sun", "Yang Liu", "Xin Peng"], "venue": "ACM Transactions on Software Engineering and Methodology", "abstract": "Recent code large language models (LLMs) have shown promising performance in generating standalone functions. However, they face limitations in repository-level code generation due to their lack of awareness of repository-level dependencies (e.g., user-defined attributes), resulting in dependency errors such as undefined-variable and no-member errors. In this work, we introduce ToolGen, an approach that integrates autocompletion tools into the code LLM generation process to address these dependencies. ToolGen comprises two main phases: Trigger Insertion and Model Fine-tuning (Offline), and Tool-integrated Code Generation (Online). During the offline phase, ToolGen augments functions within a given code corpus with a special mark token, indicating positions to trigger autocompletion tools. These augmented functions, along with their corresponding descriptions, are then used to fine-tune a selected code LLM. In the online phase, ToolGen iteratively generates functions by predicting tokens step-by-step using the fine-tuned LLM. Whenever a mark token is encountered, ToolGen invokes the autocompletion tool to suggest code completions and selects the most appropriate one through constrained greedy search. We conduct comprehensive experiments to evaluate ToolGen’s effectiveness in repository-level code generation across three distinct code LLMs: CodeGPT, CodeT5, and CodeLlama. To facilitate this evaluation, we create a benchmark comprising 671 real-world code repositories and introduce two new dependency-based metrics: Dependency Coverage and Static Validity Rate. The results demonstrate that ToolGen significantly improves Dependency Coverage by 31.4% to 39.1% and Static Validity Rate by 44.9% to 57.7% across the three LLMs, while maintaining competitive or improved performance in widely recognized similarity metrics such as BLEU-4, CodeBLEU, Edit Similarity, and Exact Match. On the CoderEval dataset, ToolGen achieves improvements of 40.0% and 25.0% in test pass rate (Pass@1) for CodeT5 and CodeLlama, respectively, while maintaining the same pass rate for CodeGPT. ToolGen also demonstrates high efficiency in repository-level code generation, with latency ranging from 0.63 to 2.34 seconds for generating each function. Furthermore, our generalizability evaluation confirms ToolGen’s consistent performance when applied to diverse code LLMs, encompassing various model architectures and scales.", "year": 2024, "publicationdate": "2024-01-12", "externalids": {"DOI": "10.1145/3714462"}, "doi_lower": "10.1145/3714462"}
{"paper_id": 259924919, "title": "Software Testing With Large Language Models: Survey, Landscape, and Vision", "author_names": ["Junjie Wang", "Yuchao Huang", "Chunyang Chen", "Zhe Liu", "Song Wang", "Qing Wang"], "venue": "IEEE Transactions on Software Engineering", "abstract": "Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.", "year": 2023, "publicationdate": "2023-07-14", "externalids": {"DOI": "10.1109/TSE.2024.3368208"}, "doi_lower": "10.1109/tse.2024.3368208"}
{"paper_id": 248708304, "title": "Machine/Deep Learning for Software Engineering: A Systematic Literature Review", "author_names": ["Simin Wang", "LiGuo Huang", "Amiao Gao", "Jidong Ge", "Tengfei Zhang", "Haitao Feng", "Ishna Satyarth", "Ming Li", "He Zhang", "Vincent Ng"], "venue": "IEEE Transactions on Software Engineering", "abstract": "Since 2009, the deep learning revolution, which was triggered by the introduction of ImageNet, has stimulated the synergy between Software Engineering (SE) and Machine Learning (ML)/Deep Learning (DL). Meanwhile, critical reviews have emerged that suggest that ML/DL should be used cautiously. To improve the applicability and generalizability of ML/DL-related SE studies, we conducted a 12-year Systematic Literature Review (SLR) on 1,428 ML/DL-related SE papers published between 2009 and 2020. Our trend analysis demonstrated the impacts that ML/DL brought to SE. We examined the complexity of applying ML/DL solutions to SE problems and how such complexity led to issues concerning the reproducibility and replicability of ML/DL studies in SE. Specifically, we investigated how ML and DL differ in data preprocessing, model training, and evaluation when applied to SE tasks, and what details need to be provided to ensure that a study can be reproduced or replicated. By categorizing the rationales behind the selection of ML/DL techniques into five themes, we analyzed how model performance, robustness, interpretability, complexity, and data simplicity affected the choices of ML/DL models.", "year": 2023, "publicationdate": "2023-03-01", "externalids": {"DOI": "10.1109/TSE.2022.3173346"}, "doi_lower": "10.1109/tse.2022.3173346"}
{"paper_id": 254877229, "title": "ReCode: Robustness Evaluation of Code Generation Models", "author_names": ["Shiqi Wang", "Zheng Li", "Haifeng Qian", "Cheng Yang", "Zijian Wang", "Mingyue Shang", "Varun Kumar", "Samson Tan", "Baishakhi Ray", "Parminder Bhatia", "Ramesh Nallapati", "M. Ramanathan", "D. Roth", "Bing Xiang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in code generation. In this paper, we propose ReCode, a comprehensive robustness evaluation benchmark for code generation models. We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They are carefully designed to be natural in real-life coding practice, preserve the original semantic meaning, and thus provide multifaceted assessments of a model’s robustness performance. With human annotators, we verified that over 90% of the perturbed prompts do not alter the semantic meaning of the original prompt. In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking advantage of the fact that executing the generated code can serve as objective evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well as function completion tasks derived from them. Interesting observations include: better robustness for CodeGen over InCoder and GPT-J; models are most sensitive to syntax perturbations; more challenging robustness evaluation on MBPP over HumanEval.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10264"}, "doi_lower": "10.48550/arxiv.2212.10264"}
{"paper_id": 264487359, "title": "Knowledge Editing for Large Language Models: A Survey", "author_names": ["Song Wang", "Yaochen Zhu", "Haochen Liu", "Zaiyi Zheng", "Chen Chen", "Jundong Li"], "venue": "ACM Computing Surveys", "abstract": "Large Language Models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME), also known as Knowledge Editing or Model Editing, has attracted increasing attention, which aims at precisely modifying the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim at providing a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.", "year": 2023, "publicationdate": "2023-10-24", "externalids": {"DOI": "10.1145/3698590"}, "doi_lower": "10.1145/3698590"}
{"paper_id": 267406155, "title": "Executable Code Actions Elicit Better LLM Agents", "author_names": ["Xingyao Wang", "Yangyi Chen", "Lifan Yuan", "Yizhe Zhang", "Yunzhu Li", "Hao Peng", "Heng Ji"], "venue": "International Conference on Machine Learning", "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.", "year": 2024, "publicationdate": "2024-02-01", "externalids": {"DOI": "10.48550/arXiv.2402.01030"}, "doi_lower": "10.48550/arxiv.2402.01030"}
{"paper_id": 247362946, "title": "Compilable Neural Code Generation with Compiler Feedback", "author_names": ["Xin Wang", "Yasheng Wang", "Yao Wan", "Fei Mi", "Yitong Li", "Pingyi Zhou", "Jin Liu", "Hao Wu", "Xin Jiang", "Qun Liu"], "venue": "Findings", "abstract": "Automatically generating compilable programs with (or without) natural language descriptions has always been a touchstone problem for computational linguistics and automated software engineering. Existing deep-learning approaches model code generation as text generation, either constrained by grammar structures in decoder, or driven by pre-trained language models on large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of them account for compilability of the generated programs. To improve compilability of the generated programs, this paper proposes COMPCODER, a three-stage pipeline utilizing compiler feedback for compilable code generation, including language model fine-tuning, compilability reinforcement, and compilability discrimination. Comprehensive experiments on two code generation tasks demonstrate the effectiveness of our proposed approach, improving the success rate of compilation from 44.18 to 89.18 in code completion on average and from 70.3 to 96.2 in text-to-code generation, respectively, when comparing with the state-of-the-art CodeGPT.", "year": 2022, "publicationdate": "2022-03-10", "externalids": {"DOI": "10.48550/arXiv.2203.05132"}, "doi_lower": "10.48550/arxiv.2203.05132"}
{"paper_id": 202643539, "title": "Vol. 37, Issue 4, August 2010 / Band 37, Heft 4, August 2010", "author_names": [], "venue": "Transfusion Medicine and Hemotherapy", "abstract": null, "year": 2010, "publicationdate": "2010-07-27", "externalids": {"DOI": "10.1159/000319681"}, "doi_lower": "10.1159/000319681"}
{"paper_id": 254877310, "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "author_names": ["Yizhong Wang", "Yeganeh Kordi", "Swaroop Mishra", "Alisa Liu", "Noah A. Smith", "Daniel Khashabi", "Hannaneh Hajishirzi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10560"}, "doi_lower": "10.48550/arxiv.2212.10560"}
{"paper_id": 258685677, "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation", "author_names": ["Yue Wang", "Hung Le", "Akhilesh Deepak Gotmare", "Nghi D. Q. Bui", "Junnan Li", "Steven C. H. Hoi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.", "year": 2023, "publicationdate": "2023-05-13", "externalids": {"DOI": "10.48550/arXiv.2305.07922"}, "doi_lower": "10.48550/arxiv.2305.07922"}
{"paper_id": 232257721, "title": "Code Completion by Modeling Flattened Abstract Syntax Trees as Graphs", "author_names": ["Yanlin Wang", "Hui Li"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Code completion has become an essential component of integrated development environments. Contemporary code completion methods rely on the abstract syntax tree (AST) to generate syntactically correct code. However, they cannot fully capture the sequential and repetitive patterns of writing code and the structural information of the AST. To alleviate these problems, we propose a new code completion approach named CCAG, which models the flattened sequence of a partial AST as an AST graph. CCAG uses our proposed AST Graph Attention Block to capture different dependencies in the AST graph for representation learning in code completion. The sub-tasks of code completion are optimized via multi-task learning in CCAG, and the task balance is automatically achieved using uncertainty without the need to tune task weights. The experimental results show that CCAG has superior performance than state-of-the-art approaches and it is able to provide intelligent code completion.", "year": 2021, "publicationdate": "2021-03-17", "externalids": {"DOI": "10.1609/aaai.v35i16.17650"}, "doi_lower": "10.1609/aaai.v35i16.17650"}
{"paper_id": 237386541, "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation", "author_names": ["Yue Wang", "Weishi Wang", "Shafiq R. Joty", "S. Hoi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.", "year": 2021, "publicationdate": "2021-09-02", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.685"}, "doi_lower": "10.18653/v1/2021.emnlp-main.685"}
{"paper_id": 260356605, "title": "Aligning Large Language Models with Human: A Survey", "author_names": ["Yufei Wang", "Wanjun Zhong", "Liangyou Li", "Fei Mi", "Xingshan Zeng", "Wenyong Huang", "Lifeng Shang", "Xin Jiang", "Qun Liu"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2307.12966"}, "doi_lower": "10.48550/arxiv.2307.12966"}
{"paper_id": 254877069, "title": "Execution-Based Evaluation for Open-Domain Code Generation", "author_names": ["Zhiruo Wang", "Shuyan Zhou", "Daniel Fried", "Graham Neubig"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "To extend the scope of coding queries to more realistic settings, we propose ODEX, the first Open-Domain EXecution-based natural language (NL) to Python code generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse libraries, along with 1,707 human-written test cases for execution. Our NL-Code pairs are harvested from StackOverflow forums to encourage natural and practical coding queries. Moreover, ODEX supports four natural languages as intents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing behavioral differences among top-performing code language models (LM). While CODEX achieves better overall results, CODEGEN improves effectively via scaling -- CODEGEN 6.1B performs comparably with CODEX 12B. Both models show substantial gaps between open and closed domains, but CODEGEN gaps tend to decrease with model size while CODEX gaps increase. We release ODEX to facilitate research into open-domain problems for the code generation community.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10481"}, "doi_lower": "10.48550/arxiv.2212.10481"}
{"paper_id": 249674500, "title": "Emergent Abilities of Large Language Models", "author_names": ["Jason Wei", "Yi Tay", "Rishi Bommasani", "Colin Raffel", "Barret Zoph", "Sebastian Borgeaud", "Dani Yogatama", "Maarten Bosma", "Denny Zhou", "Donald Metzler", "Ed H. Chi", "Tatsunori Hashimoto", "O. Vinyals", "P. Liang", "J. Dean", "W. Fedus"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.", "year": 2022, "publicationdate": "2022-06-15", "externalids": {"DOI": "10.48550/arXiv.2206.07682"}, "doi_lower": "10.48550/arxiv.2206.07682"}
{"paper_id": 262618463, "title": "Towards Greener Yet Powerful Code Generation via Quantization: An Empirical Study", "author_names": ["Xiaokai Wei", "Sujan Kumar", "Gonugondla", "Shiqi Wang", "W. Ahmad", "Baishakhi Ray", "Haifeng Qian", "Xiaopeng Li", "Varun Kumar", "Zijian Wang", "Yuchen Tian", "Qing Sun", "Ben Athiwaratkun Mingyue", "Shang Murali", "Krishnan Ramanathan", "Parminder Bhatia", "Bing Xiang"], "venue": "ESEC/SIGSOFT FSE", "abstract": "ML-powered code generation aims to assist developers to write code in a more productive manner by intelligently generating code blocks based on natural language prompts. Recently, large pretrained deep learning models have pushed the boundary of code generation and achieved impressive performance. However, the huge number of model parameters poses a significant challenge to their adoption in a typical software development environment, where a developer might use a standard laptop or mid-size server to develop code. Such large models cost significant resources in terms of memory, latency, dollars, as well as carbon footprint. Model compression is a promising approach to address these challenges. We have identified quantization as one of the most promising compression techniques for code-generation as it avoids expensive retraining costs. As quantization represents model parameters with lower-bit integer (e.g., int8), the model size and runtime latency would both benefit. We empirically evaluate quantized models on code generation tasks across different dimensions: (i) resource usage and carbon footprint, (ii) accuracy, and (iii) robustness. Through systematic experiments we find a code-aware quantization recipe that could run even a 6-billion-parameter model in a regular laptop without significant accuracy or robustness degradation. We find that the recipe is readily applicable to code summarization task as well.", "year": 2023, "publicationdate": "2023-11-30", "externalids": {"DOI": "10.1145/3611643.3616302"}, "doi_lower": "10.1145/3611643.3616302"}
{"paper_id": 265609970, "title": "Magicoder: Source Code Is All You Need", "author_names": ["Yuxiang Wei", "Zhe Wang", "Jiawei Liu", "Yifeng Ding", "Lingming Zhang"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2312.02120"}, "doi_lower": "10.48550/arxiv.2312.02120"}
{"paper_id": 277983858, "title": "Conceptual Foundations of LLM-Powered Agents: From Language Processing to Autonomous Reasoning", "author_names": ["Yifei Jian"], "venue": "Scientific Journal of Intelligent Systems Research", "abstract": "Large language models (LLMs) have transformed artificial intelligence by enabling advanced natural language processing and generation. However, their evolution toward autonomous agents require additional capabilities, including structured reasoning, adaptive learning, and interaction with external environments. This article explores the progression from early NLP models to modern LLM-based agents, detailing their core mechanisms, key capabilities, and applications. Additionally, we discuss the challenges in developing fully autonomous AI systems, such as alignment issues, computational efficiency, and decision-making constraints. Finally, we examine future trends, including reinforcement learning integration, enhanced memory architectures, and humanAI collaboration, which will shape the next generation of intelligent agents.", "year": 2025, "publicationdate": "2025-04-20", "externalids": {"DOI": "10.54691/atmxvb43"}, "doi_lower": "10.54691/atmxvb43"}
{"paper_id": 267681974, "title": "QuRating: Selecting High-Quality Data for Training Language Models", "author_names": ["Alexander Wettig", "Aatmik Gupta", "Saumya Malik", "Danqi Chen"], "venue": "International Conference on Machine Learning", "abstract": "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.", "year": 2024, "publicationdate": "2024-02-15", "externalids": {"DOI": "10.48550/arXiv.2402.09739"}, "doi_lower": "10.48550/arxiv.2402.09739"}
{"paper_id": 238226716, "title": "Fake it till you make it: face analysis in the wild using synthetic data alone", "author_names": ["Erroll Wood", "Tadas Baltruvsaitis", "Charlie Hewitt", "Sebastian Dziadzio", "Matthew Johnson", "V. Estellers", "T. Cashman", "J. Shotton"], "venue": "IEEE International Conference on Computer Vision", "abstract": "We demonstrate that it is possible to perform face-related computer vision in the wild using synthetic data alone. The community has long enjoyed the benefits of synthesizing training data with graphics, but the domain gap between real and synthetic data has remained a problem, especially for human faces. Researchers have tried to bridge this gap with data mixing, domain adaptation, and domain-adversarial training, but we show that it is possible to synthesize data with minimal domain gap, so that models trained on synthetic data generalize to real in-the-wild datasets. We describe how to combine a procedurally-generated parametric 3D face model with a comprehensive library of hand-crafted assets to render training images with unprecedented realism and diversity. We train machine learning systems for face-related tasks such as landmark localization and face parsing, showing that synthetic data can both match real data in accuracy as well as open up new approaches where manual labeling would be impossible.", "year": 2021, "publicationdate": "2021-09-30", "externalids": {"DOI": "10.1109/ICCV48922.2021.00366"}, "doi_lower": "10.1109/iccv48922.2021.00366"}
{"paper_id": 268509840, "title": "Repoformer: Selective Retrieval for Repository-Level Code Completion", "author_names": ["Di Wu", "W. Ahmad", "Dejiao Zhang", "M. K. Ramanathan", "Xiaofei Ma"], "venue": "International Conference on Machine Learning", "abstract": "Recent advances in retrieval-augmented generation (RAG) have initiated a new era in repository-level code completion. However, the invariable use of retrieval in existing methods exposes issues in both efficiency and robustness, with a large proportion of the retrieved contexts proving unhelpful or harmful to code language models (code LMs). In this paper, we propose a selective RAG framework to avoid retrieval when unnecessary. To power this framework, we design a self-supervised learning approach to enable a code LM to accurately self-evaluate whether retrieval can improve its output quality and robustly leverage the potentially noisy retrieved contexts. Using this LM as both the selective RAG policy and the generation model, our framework achieves state-of-the-art repository-level code completion performance on diverse benchmarks including RepoEval, CrossCodeEval, and CrossCodeLongEval, a new long-form code completion benchmark. Meanwhile, our analyses show that selectively retrieving brings as much as 70% inference speedup in the online serving setting without harming the performance. We further demonstrate that our framework is able to accommodate different generation models, retrievers, and programming languages. These advancements position our framework as an important step towards more accurate and efficient repository-level code completion.", "year": 2024, "publicationdate": "2024-03-15", "externalids": {"DOI": "10.48550/arXiv.2403.10059"}, "doi_lower": "10.48550/arxiv.2403.10059"}
{"paper_id": 260925901, "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework", "author_names": ["Qingyun Wu", "Gagan Bansal", "Jieyu Zhang", "Yiran Wu", "Shaokun Zhang", "Erkang Zhu", "Beibin Li", "Li Jiang", "Xiaoyun Zhang", "Chi Wang"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 275901979, "title": "The rise and potential of large language model based agents: a survey", "author_names": ["Zhiheng Xi", "Wenxiang Chen", "Xin Guo", "Wei He", "Yiwen Ding", "Boyang Hong", "Ming Zhang", "Junzhe Wang", "Senjie Jin", "Enyu Zhou", "Rui Zheng", "Xiaoran Fan", "Xiao Wang", "Limao Xiong", "Yuhao Zhou", "Weiran Wang", "Changhao Jiang", "Yicheng Zou", "Xiangyang Liu", "Zhangyue Yin", "Shihan Dou", "Rongxiang Weng", "Wenjuan Qin", "Yongyan Zheng", "Xipeng Qiu", "Xuanjing Huang", "Qi Zhang", "Tao Gui"], "venue": "Science China Information Sciences", "abstract": null, "year": 2025, "publicationdate": "2025-01-17", "externalids": {"DOI": "10.1007/s11432-024-4222-0"}, "doi_lower": "10.1007/s11432-024-4222-0"}
{"paper_id": 256627727, "title": "Data Selection for Language Models via Importance Resampling", "author_names": ["Sang Michael Xie", "Shibani Santurkar", "Tengyu Ma", "Percy Liang"], "venue": "Neural Information Processing Systems", "abstract": "Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given unlabeled target samples. Due to the scale and dimensionality of the raw text data, existing methods use simple heuristics or require human experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. We instantiate the DSIR framework with hashed n-gram features for efficiency, enabling the selection of 100M documents from the full Pile dataset in 4.5 hours. To measure whether hashed n-gram features preserve the aspects of the data that are relevant to the target, we define KL reduction, a data metric that measures the proximity between the selected pretraining data and the target on some feature space. Across 8 data selection methods (including expert selection), KL reduction on hashed n-gram features highly correlates with average downstream accuracy (r=0.82). When selecting data for continued pretraining on a specific domain, DSIR performs comparably to expert curation across 8 target distributions. When pretraining general-domain models (target is Wikipedia and books), DSIR improves over random selection and heuristic filtering baselines by 2-2.5% on the GLUE benchmark. Code is available at https://github.com/p-lambda/dsir.", "year": 2023, "publicationdate": "2023-02-06", "externalids": {"DOI": "10.48550/arXiv.2302.03169"}, "doi_lower": "10.48550/arxiv.2302.03169"}
{"paper_id": 86724704, "title": "New anticancer agents. state of the art and perspectives", "author_names": ["M. D’Incalci"], "venue": "", "abstract": null, "year": 1992, "publicationdate": "1992-09-01", "externalids": {"DOI": "10.1016/1043-6618(92)91156-B"}, "doi_lower": "10.1016/1043-6618(92)91156-b"}
{"paper_id": 3155193, "title": "Precise Condition Synthesis for Program Repair", "author_names": ["Yingfei Xiong", "Jie Wang", "Runfa Yan", "Jiachen Zhang", "Shi Han", "Gang Huang", "Lu Zhang"], "venue": "International Conference on Software Engineering", "abstract": "Due to the difficulty of repairing defect, many research efforts have been devoted into automatic defect repair. Given a buggy program that fails some test cases, a typical automatic repair technique tries to modify the program to make all tests pass. However, since the test suites in real world projects are usually insufficient, aiming at passing the test suites often leads to incorrect patches. This problem is known as weak test suites or overfitting. In this paper we aim to produce precise patches, that is, any patch we produce has a relatively high probability to be correct. More concretely, we focus on condition synthesis, which was shown to be able to repair more than half of the defects in existing approaches. Our key insight is threefold. First, it is important to know what variables in a local context should be used in an \"if\" condition, and we propose a sorting method based on the dependency relations between variables. Second, we observe that the API document can be used to guide the repair process, and propose document analysis technique to further filter the variables. Third, it is important to know what predicates should be performed on the set of variables, and we propose to mine a set of frequently used predicates in similar contexts from existing projects. Based on the insight, we develop a novel program repair system, ACS, that could generate precise conditions at faulty locations. Furthermore, given the generated conditions are very precise, we can perform a repair operation that is previously deemed to be too overfitting: directly returning the test oracle to repair the defect. Using our approach, we successfully repaired 18 defects on four projects of Defects4J, which is the largest number of fully automatically repaired defects reported on the dataset so far. More importantly, the precision of our approach in the evaluation is 78.3%, which is significantly higher than previous approaches, which are usually less than 40%.", "year": 2016, "publicationdate": "2016-08-28", "externalids": {"DOI": "10.1109/ICSE.2017.45"}, "doi_lower": "10.1109/icse.2017.45"}
{"paper_id": 258298159, "title": "WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions", "author_names": ["Can Xu", "Qingfeng Sun", "Kai Zheng", "Xiubo Geng", "Pu Zhao", "Jiazhan Feng", "Chongyang Tao", "Daxin Jiang"], "venue": "International Conference on Learning Representations", "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM", "year": 2023, "publicationdate": "2023-04-24", "externalids": {}, "doi_lower": null}
{"paper_id": 251765058, "title": "Repair Is Nearly Generation: Multilingual Program Repair with LLMs", "author_names": ["Harshit Joshi", "J. Cambronero", "Sumit Gulwani", "Vu Le", "Ivan Radicek", "Gust Verbruggen"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Most programmers make mistakes when writing code. Some of these mistakes are small and require few edits to the original program – a class of errors recently termed last mile mistakes. These errors break the flow for experienced developers and can stump novice programmers. Existing automated repair techniques targeting this class of errors are language-specific and do not easily carry over to new languages. Transferring symbolic approaches requires substantial engineering and neural approaches require data and retraining. We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex. Such a multilingual engine enables a flipped model for programming assistance, one where the programmer writes code and the AI assistance suggests fixes, compared to traditional code suggestion technology. Taking inspiration from the way programmers manually fix bugs, we show that a prompt-based strategy that conceptualizes repair as localization, transformation, and candidate ranking, can successfully repair programs in multiple languages with minimal effort. We present the first results for such a multilingual repair engine by evaluating on 6 different languages and comparing performance to language-specific repair engines. We show that RING can outperform language-specific repair engines for three of these languages.", "year": 2022, "publicationdate": "2022-08-24", "externalids": {"DOI": "10.48550/arXiv.2208.11640"}, "doi_lower": "10.48550/arxiv.2208.11640"}
{"paper_id": 281333659, "title": "A First Look at License Compliance Capability of LLMs in Code Generation", "author_names": ["Weiwei Xu", "Kai Gao", "Hao He", "Minghui Zhou"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2408.02487"}, "doi_lower": "10.48550/arxiv.2408.02487"}
{"paper_id": 268364103, "title": "Robustness, Security, Privacy, Explainability, Efficiency, and Usability of Large Language Models for Code", "author_names": ["Zhou Yang", "Zhensu Sun", "Terry Yue Zhuo", "Prem Devanbu", "David Lo"], "venue": "arXiv.org", "abstract": "Large language models for code (LLM4Code), which demonstrate strong performance (e.g., high accuracy) in processing source code, have significantly transformed software engineering. Many studies separately investigate the non-functional properties of LM4Code, but there is no systematic review of how these properties are evaluated and enhanced. This paper fills this gap by thoroughly examining 146 relevant studies, thereby presenting the first systematic literature review to identify seven important properties beyond accuracy, including robustness, security, privacy, explainability, efficiency, and usability. We discuss the current state-of-the-art methods and trends, identify gaps in existing research, and present promising directions for future study.", "year": 2024, "publicationdate": "2024-03-12", "externalids": {"DOI": "10.48550/arXiv.2403.07506"}, "doi_lower": "10.48550/arxiv.2403.07506"}
{"paper_id": 261048934, "title": "Unveiling Memorization in Code Models", "author_names": ["Zhou Yang", "Zhipeng Zhao", "Chenyu Wang", "Jieke Shi", "Dongsun Kim", "Donggyun Han", "David Lo"], "venue": "International Conference on Software Engineering", "abstract": "The availability of large-scale datasets, advanced architectures, and powerful computational resources have led to effective code models that automate diverse software engineering activities. The datasets usually consist of billions of lines of code from both open-source and private repositories. A code model memorizes and produces source code verbatim, which potentially contains vulnerabilities, sensitive information, or code with strict licenses, leading to potential security and privacy issues. This paper investigates an important problem: to what extent do code models memorize their training data? We conduct an empirical study to explore memorization in large pre-trained code models. Our study highlights that simply extracting 20,000 outputs (each having 512 tokens) from a code model can produce over 40,125 code snippets that are memorized from the training data. To provide a better understanding, we build a taxonomy of memorized contents with 3 categories and 14 subcategories. The results show that the prompts sent to the code models affect the distribution of memorized contents. We identify several key factors of memorization. Specifically, given the same architecture, larger models suffer more from memorization problem. A code model produces more memorization when it is allowed to generate longer outputs. We also find a strong positive correlation between the number of an output's occurrences in the training data and that in the generated outputs, which indicates that a potential way to reduce memorization is to remove duplicates in the training data. We then identify effective metrics that infer whether an output contains memorization accurately. We also make suggestions to deal with memorization.", "year": 2023, "publicationdate": "2023-08-19", "externalids": {"DOI": "10.1145/3597503.3639074"}, "doi_lower": "10.1145/3597503.3639074"}
{"paper_id": 258762525, "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "author_names": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "T. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "venue": "Neural Information Processing Systems", "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {"DOI": "10.48550/arXiv.2305.10601"}, "doi_lower": "10.48550/arxiv.2305.10601"}
{"paper_id": 252762395, "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "author_names": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "venue": "International Conference on Learning Representations", "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io", "year": 2022, "publicationdate": "2022-10-06", "externalids": {}, "doi_lower": null}
{"paper_id": 43922261, "title": "Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow", "author_names": ["Pengcheng Yin", "Bowen Deng", "Edgar Chen", "Bogdan Vasilescu", "Graham Neubig"], "venue": "IEEE Working Conference on Mining Software Repositories", "abstract": "For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. StackOverflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high-quality code snippets. However, existing heuristic methods (e.g. pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.", "year": 2018, "publicationdate": "2018-05-23", "externalids": {"DOI": "10.1145/3196398.3196408"}, "doi_lower": "10.1145/3196398.3196408"}
{"paper_id": 268856712, "title": "HyperCLOVA X Technical Report", "author_names": ["Kang Min Yoo", "Jaegeun Han", "Sookyo In", "Heewon Jeon", "Jisu Jeong", "Jaewook Kang", "Hyunwook Kim", "Kyung-min Kim", "Munhyong Kim", "Sungju Kim", "Donghyun Kwak", "Hanock Kwak", "S. Kwon", "Bado Lee", "Dongsoo Lee", "Gichang Lee", "Jooho Lee", "Baeseong Park", "Seongjin Shin", "Joonsang Yu", "Seolki Baek", "Sumin Byeon", "Eungsup Cho", "Dooseok Choe", "Jeesung Han", "Youngkyun Jin", "Hyein Jun", "Jaeseung Jung", "Chanwoong Kim", "Jinhong Kim", "Jinuk Kim", "Dokyeong Lee", "Dongwook Park", "Jeong Min Sohn", "Sujung Han", "J. Heo", "S. Hong", "Mina Jeon", "Hyunhoon Jung", "Jungeun Jung", "Wangkyo Jung", "Chungjoon Kim", "Hyeri Kim", "Jonghyun Kim", "Min Young Kim", "Soeun Lee", "Joonhee Park", "Jieun Shin", "Sojin Yang", "Jung-Ki Yoon", "Hwaran Lee", "Sanghwan Bae", "Jeehwan Cha", "Dong-Ui Ham", "Youngki Hong", "Yunki Hong", "Myunggeun Ji", "Yeguk Jin", "Chansong Jo", "Shinyoung Joo", "S. Jung", "Hyomin Kim", "Jungwhan Kim", "Minkyoung Kim", "Minseung Kim", "Sungdong Kim", "Yonghee Kim", "Youngjun Kim", "Donghyeon Ko", "Dughyun Lee", "Jaehong Lee", "Jieun Lee", "Jongjin Lee", "M. Lee", "Yehbin Lee", "Taehong Min", "Kiyoon Moon", "Jaesun Park", "Kyuyon Park", "Seunghyun Seo", "Gyubin Son", "Wonjoon Yoo", "Myungin You", "Doheon Ahn", "Homin Ahn", "Joohee Ahn", "Seongmin Ahn", "Chanwoo An", "Hyeryun An", "Junho An", "Sang-Min An", "Boram Byun", "Jongho Cha", "M. Chang", "Seunggyu Chang", "Haesong Cho", "Youngdo Cho", "Dalnim Choi", "Daseul Choi", "Hyoseok Choi", "M. Choi", "Sangho Choi", "Seongjae Choi", "Wooyong Choi", "Sewhan Chun", "Dong Young Go", "Chiheon Ham", "Danbi Han", "Jaemin Han", "Mihak Hong", "Moonyoung Hong", "S. Hong", "S. Hwang", "Eunbin Hyun", "Jinbae Im", "Jaehyung Jang", "Jaeni Jang", "Si-Hyong Jang", "Sung-Kook Jang", "Joonha Jeon", "Yujin Jeon", "Daun Jeong", "Joonhyun Jeong", "Kyeongseok Jeong", "Mini Jeong", "Yeji Jeong", "Sol Jin", "Hanbyeol Jo", "Hanju Jo", "Minjung Jo", "Lee Jonghyun", "Chaeyoon Jung", "Hyungsik Jung", "Jaeuk Jung", "Juyoung Jung", "Kwangsun Jung", "Seungjae Jung", "Soonwon Ka", "Donghan Kang", "Soyoung Kang", "Taeho Kil", "Areum Kim", "Beomyoung Kim", "Byeongwook Kim", "Daehee Kim", "Donggook Kim", "Donggook Kim", "Donghyun Kim", "Euna Kim", "Eunchul Kim", "Geewook Kim", "Gyu Ri Kim", "Hanbyul Kim", "Heesu Kim", "Isaac Kim", "Jeonghoon Kim", "Jihye Kim", "Joonghoon Kim", "Minjae Kim", "Minsub Kim", "Pil Hwan Kim", "Sammy Kim", "Seokhun Kim", "Seonghyeon Kim", "Soojin Kim", "Soong Kim", "Soyoon Kim", "Sunyoung Kim", "Taeho Kim", "Wonho Kim", "Yoonsik Kim", "You Jin Kim", "Yuri Kim", "Beomseok Kwon", "Ohsung Kwon", "Yoo-Hwan Kwon", "Anna Lee", "Byungwook Lee", "Changho Lee", "Daun Lee", "DongJae Lee", "Ha-Ram Lee", "Hodong Lee", "Hwiyeong Lee", "Hyunmi Lee", "Injae Lee", "Jaeung Lee", "Jeongsang Lee", "Jisoo Lee", "Joongjae Lee", "Juhan Lee", "Jung Hyun Lee", "Junghoon Lee", "Junwoo Lee", "Se Yun Lee", "Sujin Lee", "Sungjae Lee", "Sungwoo Lee", "Wonjae Lee", "Zoo Hyun Lee", "Jong Kun Lim", "Kun Lim", "Taemin Lim", "Yuri Min", "Nuri Na", "JeongYeon Nam", "Kyeong-Min Nam", "Yeonseog Noh", "Biro Oh", "Hyangnam Oh", "Jungsik Oh", "Solgil Oh", "Yeontaek Oh", "Boyoun Park", "Cheonbok Park", "Dongju Park", "Hyeon-ju Park", "Hyunjung Park", "Hyunjung Park", "Jihye Park", "Jooseok Park", "Junghwan Park", "Jungsoo Park", "Miru Park", "S. Park", "Seunghyun Park", "Taerim Park", "Wonkyeong Park", "Hyunjoon Ryu", "Jeonghun Ryu", "Nahyeon Ryu", "Soonshin Seo", "Suk Min Seo", "Yoonjeong Shim", "Kyuyong Shin", "Wonkwang Shin", "Hyun Sim", "Mihyun Sim", "W. Sim", "Hyejin Soh", "Bokyung Son", "Hyunjun Son", "Seulah Son", "Chiyoung Song", "Chiyoung Song", "Ka Yeon Song", "Minchul Song", "Seungmin Song", "Jisung Wang", "Matt Yeo", "Y. Yeo", "Myeong Yeon Yi", "Moonbin Yim", "Taehwan Yoo", "Y. Yoo", "S. Yoon", "Young Jin Yoon", "Hangyeol Yu", "Ui Seon Yu", "Xingdong Zuo", "Jeongin Bae", "Joungeun Bae", "Hyun-Woong Cho", "Seonghyun Cho", "Yongjin Cho", "Taekyoon Choi", "Yera Choi", "Jiwan Chung", "Zhenghui Han", "Byeongho Heo", "Euisuk Hong", "T. Hwang", "Seonyeol Im", "Sumin Jegal", "Sumin Jeon", "Yelim Jeong", "Yonghyun Jeong", "Can Jiang", "Juyong Jiang", "Jiho Jin", "Ara Jo", "Younghyun Jo", "Hoyoun Jung", "Juyoung Jung", "Daehee Kim", "Ginam Kim", "Hangyeol Kim", "Heeseung Kim", "Hyojin Kim", "Hyojun Kim", "Hyun-Ah Kim", "Jeehye Kim", "Jinhong Kim", "Jiseon Kim", "Jonghak Kim", "Jung Yoon Kim", "Rak Yeong Kim", "Seoyoon Kim", "Sewon Kim", "Sooyoung Kim", "Sukyoung Kim", "Taeyong Kim", "Naeun Ko", "Bonseung Koo", "Heeyoung Kwak", "Haena Kwon", "Youngjin Kwon", "Boram Lee", "Bruce W. Lee", "Dagyeong Lee", "Erin Lee", "Euijin Lee", "Ha Gyeong Lee", "Hyojin Lee", "Hyunjeong Lee", "Jeeyoon Lee", "Jeonghyun Lee", "Jongheok Lee", "Joonhyung Lee", "Junhyuk Lee", "M. Lee", "Nayeon Lee", "Sangkyu Lee", "Se Young Lee", "Seulgi Lee", "Seung Jin Lee", "Suhyeon Lee", "Yeonjae Lee", "Yesol Lee", "Youngbeom Lee", "Yujin Lee", "Shaodong Li", "Tianyu Liu", "Seong-Eun Moon", "Taehong Moon", "Max-Lasse Nihlenramstroem", "Wonseok Oh", "Yuri Oh", "Hongbeen Park", "Hyekyung Park", "Nohil Park", "S. Park", "Jiwon Ryu", "Miru Ryu", "Simo Ryu", "Ahreum Seo", "H. Seo", "Kangdeok Seo", "Jamin Shin", "Seungyoun Shin", "Heetae Sin", "Jiangping Wang", "Lei Wang", "Ning Xiang", "Longxiang Xiao", "Jing Xu", "Seonyeong Yi", "Haanju Yoo", "Haneul Yoo", "Hwanhee Yoo", "Liang Yu", "Youngjae Yu", "Weijie Yuan", "Bo Zeng", "Qian Zhou", "Kyunghyun Cho", "J. Ha", "Joonsuk Park", "J. Hwang", "H. Kwon", "Soonyong Kwon", "Jungyeon Lee", "Seungho Lee", "Se-Eun Choi", "Sang-Woo Lee", "Jung Hwa Lim", "Nako Sung"], "venue": "arXiv.org", "abstract": "We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model's cross-lingual proficiency and strong generalization ability to untargeted languages, including machine translation between several language pairs and cross-lingual inference tasks. We believe that HyperCLOVA X can provide helpful guidance for regions or countries in developing their sovereign LLMs.", "year": 2024, "publicationdate": "2024-04-02", "externalids": {"DOI": "10.48550/arXiv.2404.01954"}, "doi_lower": "10.48550/arxiv.2404.01954"}
{"paper_id": 268196085, "title": "Leveraging pre-trained language models for code generation", "author_names": ["Ahmed Soliman", "Samir Shaheen", "M. Hadhoud"], "venue": "Complex & Intelligent Systems", "abstract": "Code assistance refers to the utilization of various tools, techniques, and models to help developers in the process of software development. As coding tasks become increasingly complex, code assistant plays a pivotal role in enhancing developer productivity, reducing errors, and facilitating a more efficient coding workflow. This assistance can manifest in various forms, including code autocompletion, error detection and correction, code generation, documentation support, and context-aware suggestions. Language models have emerged as integral components of code assistance, offering developers the capability to receive intelligent suggestions, generate code snippets, and enhance overall coding proficiency. In this paper, we propose new hybrid models for code generation by leveraging pre-trained language models BERT, RoBERTa, ELECTRA, and LUKE with the Marian Causal Language Model. Selecting these models based on their strong performance in various natural language processing tasks. We evaluate the performance of these models on two datasets CoNaLa and DJANGO and compare them to existing state-of-the-art models. We aim to investigate the potential of pre-trained transformer language models to revolutionize code generation, offering improved precision and efficiency in navigating complex coding scenarios. Additionally, conducting error analysis and refining the generated code. Our results show that these models, when combined with the Marian Decoder, significantly improve code generation accuracy and efficiency. Notably, the RoBERTaMarian model achieved a maximum BLEU score of 35.74 and an exact match accuracy of 13.8% on CoNaLa, while LUKE-Marian attained a BLEU score of 89.34 and an exact match accuracy of 78.50% on DJANGO. Implementation of this work is available at https://github.com/AhmedSSoliman/Leveraging-Pretrained-Language-Models-for-Code-Generation.", "year": 2024, "publicationdate": "2024-02-29", "externalids": {"DOI": "10.1007/s40747-024-01373-8"}, "doi_lower": "10.1007/s40747-024-01373-8"}
{"paper_id": 52815560, "title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task", "author_names": ["Tao Yu", "Rui Zhang", "Kai-Chou Yang", "Michihiro Yasunaga", "Dongxu Wang", "Zifan Li", "James Ma", "Irene Z Li", "Qingning Yao", "Shanelle Roman", "Zilin Zhang", "Dragomir R. Radev"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "We present Spider, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at https://yale-lily.github.io/seq2sql/spider.", "year": 2018, "publicationdate": "2018-09-24", "externalids": {"DOI": "10.18653/v1/D18-1425"}, "doi_lower": "10.18653/v1/d18-1425"}
{"paper_id": 266521384, "title": "WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation", "author_names": ["Zhaojian Yu", "Xin Zhang", "Ning Shang", "Yangyu Huang", "Can Xu", "Yishujie Zhao", "Wenxiang Hu", "Qiufeng Yin"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2312.14187"}, "doi_lower": "10.48550/arxiv.2312.14187"}
{"paper_id": 260887189, "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher", "author_names": ["Youliang Yuan", "Wenxiang Jiao", "Wenxuan Wang", "Jen-Tse Huang", "Pinjia He", "Shuming Shi", "Zhaopeng Tu"], "venue": "International Conference on Learning Representations", "abstract": "Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. Notably, we identify that LLMs seem to have a ''secret cipher'', and propose a novel SelfCipher that uses only role play and several demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases. Our code and data will be released at https://github.com/RobustNLP/CipherChat.", "year": 2023, "publicationdate": "2023-08-12", "externalids": {"DOI": "10.48550/arXiv.2308.06463"}, "doi_lower": "10.48550/arxiv.2308.06463"}
{"paper_id": 258059818, "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears", "author_names": ["Zheng Yuan", "Hongyi Yuan", "Chuanqi Tan", "Wei Wang", "Songfang Huang", "Feiran Huang"], "venue": "Neural Information Processing Systems", "abstract": "Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). However, PPO is sensitive to hyperparameters and requires multiple models in its standard implementation, making it hard to train and scale up to larger parameter counts. In contrast, we propose a novel learning paradigm called RRHF, which scores sampled responses from different sources via a logarithm of conditional probabilities and learns to align these probabilities with human preferences through ranking loss. RRHF can leverage sampled responses from various sources including the model responses from itself, other large language model responses, and human expert responses to learn to rank them. RRHF only needs 1 to 2 models during tuning and can efficiently align language models with human preferences robustly without complex hyperparameter tuning. Additionally, RRHF can be considered an extension of SFT and reward model training while being simpler than PPO in terms of coding, model counts, and hyperparameters. We evaluate RRHF on the Helpful and Harmless dataset, demonstrating comparable alignment performance with PPO by reward model score and human labeling. Extensive experiments show that the performance of RRHF is highly related to sampling quality which suggests RRHF is a best-of-n learner. Codes available at https://github.com/GanjinZero/RRHF.", "year": 2023, "publicationdate": "2023-04-11", "externalids": {"DOI": "10.48550/arXiv.2304.05302"}, "doi_lower": "10.48550/arxiv.2304.05302"}
{"paper_id": 14556905, "title": "Research Statement Arjun Guha", "author_names": [], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 231672601, "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models", "author_names": ["Elad Ben-Zaken", "Shauli Ravfogel", "Yoav Goldberg"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods.Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.", "year": 2021, "publicationdate": "2021-06-18", "externalids": {"DOI": "10.18653/v1/2022.acl-short.1"}, "doi_lower": "10.18653/v1/2022.acl-short.1"}
{"paper_id": 249642442, "title": "CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation", "author_names": ["Daoguang Zan", "Bei Chen", "Dejian Yang", "Zeqi Lin", "Minsu Kim", "Bei Guan", "Yongji Wang", "Weizhu Chen", "Jian-Guang Lou"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Code generation is a longstanding challenge, aiming to generate a code snippet based on a natural language description. Usually, expensive text-code paired data is essential for training a code generation model. Recently, thanks to the success of pre-training techniques, large language models are trained on large unlabelled code corpora and perform well in generating code. In this paper, we investigate how to leverage an unlabelled code corpus to train a model for library-oriented code generation. Since it is a common practice for programmers to reuse third-party libraries, in which case the text-code paired data are harder to obtain due to the huge number of libraries. We observe that library-oriented code snippets are more likely to share similar code sketches. Hence, we present CERT with two steps: a sketcher generates the sketch, then a generator fills the details in the sketch. Both the sketcher and generator are continually pre-trained upon a base model using unlabelled data. Also, we carefully craft two benchmarks to evaluate library-oriented code generation named PandasEval and NumpyEval. Experimental results have shown the impressive performance of CERT. For example, it surpasses the base model by an absolute 15.67% improvement in terms of pass@1 on PandasEval. Our work is available at https://github.com/microsoft/PyCodeGPT.", "year": 2022, "publicationdate": "2022-06-14", "externalids": {"DOI": "10.48550/arXiv.2206.06888"}, "doi_lower": "10.48550/arxiv.2206.06888"}
{"paper_id": 258557362, "title": "Large Language Models Meet NL2Code: A Survey", "author_names": ["Daoguang Zan", "B. Chen", "Fengji Zhang", "Di Lu", "Bingchao Wu", "Bei Guan", "Yongji Wang", "Jian-Guang Lou"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are “Large Size, Premium Data, Expert Tuning”. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.", "year": 2022, "publicationdate": "2022-12-19", "externalids": {"DOI": "10.18653/v1/2023.acl-long.411"}, "doi_lower": "10.18653/v1/2023.acl-long.411"}
{"paper_id": 268681570, "title": "CodeS: Natural Language to Code Repository via Multi-Layer Sketch", "author_names": ["Daoguang Zan", "Ailun Yu", "Wei Liu", "Dong Chen", "Bo Shen", "Yafen Yao", "Wei Li", "Xiaolin Chen", "Yongshun Gong", "Bei Guan", "Zhiguang Yang", "Yongji Wang", "Li-zhen Cui", "Qianxiang Wang"], "venue": "ACM Transactions on Software Engineering and Methodology", "abstract": "The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development. In light of this, we introduce a new software engineering task, namely Natural Language to code Repository (NL2Repo). This task aims to generate an entire code repository from its natural language requirements. To address this task, we propose a simple yet effective framework CodeS, which decomposes NL2Repo into multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first generates a repository’s directory structure for given requirements; FileSketcher then generates a file sketch for each file in the generated structure; SketchFiller finally fills in the details for each function in the generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry out evaluations through both automated benchmarking and manual feedback analysis. For benchmark-based evaluation, we craft a repository-oriented benchmark, SketchEval, and design an evaluation metric, SketchBLEU. For feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30 participants in conducting empirical studies. Extensive experiments prove the effectiveness and practicality of CodeS on the NL2Repo task.", "year": 2024, "publicationdate": "2024-03-25", "externalids": {"DOI": "10.1145/3768577"}, "doi_lower": "10.1145/3768577"}
{"paper_id": 269717748, "title": "RepoCoder Repository-Level Code Completion Through Iterative Retrieval and Generation", "author_names": ["Fengji Zhang", "Bei Chen", "Yue Zhang", "J. Keung", "Jin Liu", "Daoguang Zan", "Yi Mao", "Jian-Guang Lou", "Weizhu Chen", "Kong", "Microsoft Corporation"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 249395268, "title": "Automated Feedback Generation for Competition-Level Code", "author_names": ["Jialu Zhang", "De Li", "John C. Kolesar", "Hanyuan Shi", "R. Piskac"], "venue": "International Conference on Automated Software Engineering", "abstract": "Competitive programming has become a popular way for programmers to test their skills. Competition-level programming problems are challenging in nature, and participants often fail to solve the problem on their first attempt. Some online platforms for competitive programming allow programmers to practice on competition-level problems, and the standard feedback for an incorrect practice submission is the first test case that the submission fails. Often, the failed test case does not provide programmers with enough information to resolve the errors in their code, and they abandon the problem after making several more unsuccessful attempts. We present Clef, the first data-driven tool that can generate feedback on competition-level code automatically by repairing programmers’ incorrect submissions. The key development is that Clef can learn how to generate repairs for incorrect submissions by examining the repairs that other programmers made to their own submissions over time. Since the differences between an incorrect program and a correct program for the same task may be significant, we introduce a new data structure, merge trees, to capture the changes between submissions. Merge trees are versatile: they can encode both large algorithm-level redesigns and small statement-level alterations. We evaluated Clef on six real-world problems from Codeforces, the world’s largest platform for competitive programming. Clef achieves accuracy in repairing programmers’ incorrect submissions. When given incorrect submissions from programmers who never found the solution to a problem on their own, Clef repairs the users’ programs of the time.", "year": 2022, "publicationdate": "2022-06-03", "externalids": {"DOI": "10.1145/3551349.3560425"}, "doi_lower": "10.1145/3551349.3560425"}
{"paper_id": 257631760, "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning", "author_names": ["Qingru Zhang", "Minshuo Chen", "Alexander Bukharin", "Pengcheng He", "Yu Cheng", "Weizhu Chen", "Tuo Zhao"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2303.10512"}, "doi_lower": "10.48550/arxiv.2303.10512"}
{"paper_id": 269613953, "title": "NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts", "author_names": ["Shudan Zhang", "Hanlin Zhao", "Xiao Liu", "Qinkai Zheng", "Zehan Qi", "Xiaotao Gu", "Xiaohan Zhang", "Yuxiao Dong", "Jie Tang"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have manifested strong ability to generate codes for productive activities. However, current benchmarks for code synthesis, such as HumanEval, MBPP, and DS-1000, are predominantly oriented towards introductory tasks on algorithm and data science, insufficiently satisfying challenging requirements prevalent in real-world coding. To fill this gap, we propose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks. NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains. Noting the extraordinary difficulty in creating testing cases for real-world queries, we also introduce a semi-automated pipeline to enhance the efficiency of test case construction. Comparing with manual solutions, it achieves an efficiency increase of more than 4 times. Our systematic experiments on 39 LLMs find that performance gaps on NCB between models with close HumanEval scores could still be significant, indicating a lack of focus on practical code synthesis scenarios or over-specified optimization on HumanEval. On the other hand, even the best-performing GPT-4 is still far from satisfying on NCB. The evaluation toolkit and development set are available at https://github.com/THUDM/NaturalCodeBench.", "year": 2024, "publicationdate": "2024-05-07", "externalids": {"DOI": "10.48550/arXiv.2405.04520"}, "doi_lower": "10.48550/arxiv.2405.04520"}
{"paper_id": 261530162, "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models", "author_names": ["Yue Zhang", "Yafu Li", "Leyang Cui", "Deng Cai", "Lemao Liu", "Tingchen Fu", "Xinting Huang", "Enbo Zhao", "Yu Zhang", "Yulong Chen", "Longyue Wang", "A. Luu", "Wei Bi", "Freda Shi", "Shuming Shi"], "venue": "Computational Linguistics", "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.", "year": 2023, "publicationdate": "2023-09-03", "externalids": {"DOI": "10.1162/coli.a.16"}, "doi_lower": "10.1162/coli.a.16"}
{"paper_id": 279398583, "title": "Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code", "author_names": ["Ziyin Zhang", "Chaoyu Chen", "Bingchang Liu", "Cong Liao", "Zi Gong", "Hang Yu", "Jianguo Li", "Rui Wang"], "venue": "Trans. Mach. Learn. Res.", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 199897699, "title": "Feng Xiao-Ting", "author_names": ["台灣文化研究中心"], "venue": "", "abstract": null, "year": 2011, "publicationdate": "2011-08-26", "externalids": {}, "doi_lower": null}
{"paper_id": 257900969, "title": "A Survey of Large Language Models", "author_names": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Z. Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "J. Nie", "Ji-rong Wen"], "venue": "arXiv.org", "abstract": "Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.", "year": 2023, "publicationdate": "2023-03-31", "externalids": {}, "doi_lower": null}
{"paper_id": 257834177, "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X", "author_names": ["Qinkai Zheng", "Xiao Xia", "Xu Zou", "Yuxiao Dong", "Shanshan Wang", "Yufei Xue", "Zihan Wang", "Lei Shen", "Andi Wang", "Yang Li", "Teng Su", "Zhilin Yang", "Jie Tang"], "venue": "Knowledge Discovery and Data Mining", "abstract": "Large pre-trained code generation models, such as OpenAI Codex, can generate syntax-and function-correct code, making the coding of programmers more productive. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 8 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible since Sep. 2022, we open-sourced its code, model weights, API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {"DOI": "10.1145/3580305.3599790"}, "doi_lower": "10.1145/3580305.3599790"}
{"paper_id": 267782452, "title": "OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement", "author_names": ["Tianyu Zheng", "Ge Zhang", "Tianhao Shen", "Xueling Liu", "Bill Yuchen Lin", "Jie Fu", "Wenhu Chen", "Xiang Yue"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter.", "year": 2024, "publicationdate": "2024-02-22", "externalids": {"DOI": "10.48550/arXiv.2402.14658"}, "doi_lower": "10.48550/arxiv.2402.14658"}
{"paper_id": 258426713, "title": "Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation", "author_names": ["Wenqing Zheng", "S. Sharan", "Ajay Jaiswal", "Kevin Wang", "Yihan Xi", "Dejia Xu", "Zhangyang Wang"], "venue": "International Conference on Machine Learning", "abstract": "For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of\"outline-then-detail\". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to jointly encode the natural language descriptions and syntactically aligned I/O data samples. Extensive evaluations show that ChainCoder outperforms state-of-the-arts, demonstrating that our progressive generation eases the reasoning procedure and guides the language model to generate higher-quality solutions. Our codes are available at: https://github.com/VITA-Group/ChainCoder.", "year": 2023, "publicationdate": "2023-04-28", "externalids": {"DOI": "10.48550/arXiv.2305.00909"}, "doi_lower": "10.48550/arxiv.2305.00909"}
{"paper_id": 265281389, "title": "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends", "author_names": ["Zibin Zheng", "Kaiwen Ning", "Yanlin Wang", "Jingwen Zhang", "Dewu Zheng", "Mingxi Ye", "Jiachi Chen"], "venue": "arXiv.org", "abstract": "General large language models (LLMs), represented by ChatGPT, have demonstrated significant potential in tasks such as code generation in software engineering. This has led to the development of specialized LLMs for software engineering, known as Code LLMs. A considerable portion of Code LLMs is derived from general LLMs through model fine-tuning. As a result, Code LLMs are often updated frequently and their performance can be influenced by the base LLMs. However, there is currently a lack of systematic investigation into Code LLMs and their performance. In this study, we conduct a comprehensive survey and analysis of the types of Code LLMs and their differences in performance compared to general LLMs. We aim to address three questions: (1) What LLMs are specifically designed for software engineering tasks, and what is the relationship between these Code LLMs? (2) Do Code LLMs really outperform general LLMs in software engineering tasks? (3) Which LLMs are more proficient in different software engineering tasks? To answer these questions, we first collect relevant literature and work from five major databases and open-source communities, resulting in 134 works for analysis. Next, we categorize the Code LLMs based on their publishers and examine their relationships with general LLMs and among themselves. Furthermore, we investigate the performance differences between general LLMs and Code LLMs in various software engineering tasks to demonstrate the impact of base models and Code LLMs. Finally, we comprehensively maintained the performance of LLMs across multiple mainstream benchmarks to identify the best-performing LLMs for each software engineering task. Our research not only assists developers of Code LLMs in choosing base models for the development of more advanced LLMs but also provides insights for practitioners to better understand key improvement directions for Code LLMs.", "year": 2023, "publicationdate": "2023-11-17", "externalids": {"DOI": "10.48550/arXiv.2311.10372"}, "doi_lower": "10.48550/arxiv.2311.10372"}
{"paper_id": 270226918, "title": "LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step", "author_names": ["Li Zhong", "Zilong Wang", "Jingbo Shang"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2402.16906"}, "doi_lower": "10.48550/arxiv.2402.16906"}
{"paper_id": 260900008, "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification", "author_names": ["Aojun Zhou", "Ke Wang", "Zimu Lu", "Weikang Shi", "Sichun Luo", "Zipeng Qin", "Shaoqing Lu", "Anya Jia", "Linqi Song", "Mingjie Zhan", "Hongsheng Li"], "venue": "International Conference on Learning Representations", "abstract": "Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \\textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \\uline{c}ode-based \\uline{s}elf-\\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset \\textbf{(53.9\\% $\\to$ 84.3\\%)}.", "year": 2023, "publicationdate": "2023-08-15", "externalids": {"DOI": "10.48550/arXiv.2308.07921"}, "doi_lower": "10.48550/arxiv.2308.07921"}
{"paper_id": 263829963, "title": "Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models", "author_names": ["Andy Zhou", "Kai Yan", "Michal Shlapentokh-Rothman", "Haohan Wang", "Yu-Xiong Wang"], "venue": "International Conference on Machine Learning", "abstract": "While language models (LMs) have shown potential across a range of decision-making tasks, their reliance on simple acting processes limits their broad deployment as autonomous agents. In this paper, we introduce Language Agent Tree Search (LATS) -- the first general framework that synergizes the capabilities of LMs in reasoning, acting, and planning. By leveraging the in-context learning ability of LMs, we integrate Monte Carlo Tree Search into LATS to enable LMs as agents, along with LM-powered value functions and self-reflections for proficient exploration and enhanced decision-making. A key feature of our approach is the incorporation of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that surpasses the constraints of existing techniques. Our experimental evaluation across diverse domains, including programming, interactive question-answering (QA), web navigation, and math, validates the effectiveness and generality of LATS in decision-making while maintaining competitive or improved reasoning performance. Notably, LATS achieves state-of-the-art pass@1 accuracy (92.7%) for programming on HumanEval with GPT-4 and demonstrates gradient-free performance (average score of 75.9) comparable to gradient-based fine-tuning for web navigation on WebShop with GPT-3.5. Code can be found at https://github.com/lapisrocks/LanguageAgentTreeSearch", "year": 2023, "publicationdate": "2023-10-06", "externalids": {"DOI": "10.48550/arXiv.2310.04406"}, "doi_lower": "10.48550/arxiv.2310.04406"}
{"paper_id": 258822910, "title": "LIMA: Less Is More for Alignment", "author_names": ["Chunting Zhou", "Pengfei Liu", "Puxin Xu", "Srini Iyer", "Jiao Sun", "Yuning Mao", "Xuezhe Ma", "Avia Efrat", "Ping Yu", "L. Yu", "Susan Zhang", "Gargi Ghosh", "M. Lewis", "Luke Zettlemoyer", "Omer Levy"], "venue": "Neural Information Processing Systems", "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.", "year": 2023, "publicationdate": "2023-05-18", "externalids": {}, "doi_lower": null}
{"paper_id": 252734952, "title": "DocPrompting: Generating Code by Retrieving the Docs", "author_names": ["Shuyan Zhou", "Uri Alon", "Frank F. Xu", "Zhiruo Wang", "Zhengbao Jiang", "Graham Neubig"], "venue": "International Conference on Learning Representations", "abstract": "Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of code to keep current with all available APIs by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages documentation by (1) retrieving the relevant documentation pieces given an NL intent, and (2) generating code based on the NL intent and the retrieved documentation. DocPrompting is general: it can be applied to any programming language and is agnostic to the underlying neural model. We demonstrate that DocPrompting consistently improves NL-to-code models: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.", "year": 2022, "publicationdate": "2022-07-13", "externalids": {}, "doi_lower": null}
{"paper_id": 270562723, "title": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence", "author_names": ["DeepSeek-AI", "Qihao Zhu", "Daya Guo", "Zhihong Shao", "Dejian Yang", "Peiyi Wang", "Runxin Xu", "Y. Wu", "Yukun Li", "Huazuo Gao", "Shirong Ma", "Wangding Zeng", "Xiao Bi", "Zihui Gu", "Hanwei Xu", "Damai Dai", "Kai Dong", "Liyue Zhang", "Yishi Piao", "Zhibin Gou", "Zhenda Xie", "Zhewen Hao", "Bing-Li Wang", "Jun-Mei Song", "Deli Chen", "Xin Xie", "Kang Guan", "Yu-mei You", "A. Liu", "Qiushi Du", "Wenjun Gao", "Xuan Lu", "Qinyu Chen", "Yaohui Wang", "C. Deng", "Jiashi Li", "Chenggang Zhao", "C. Ruan", "Fuli Luo", "W. Liang"], "venue": "arXiv.org", "abstract": "We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.", "year": 2024, "publicationdate": "2024-06-17", "externalids": {"DOI": "10.48550/arXiv.2406.11931"}, "doi_lower": "10.48550/arxiv.2406.11931"}
{"paper_id": 258352761, "title": "ICE-Score: Instructing Large Language Models to Evaluate Code", "author_names": ["Terry Yue Zhuo"], "venue": "Findings", "abstract": "Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text. Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code intelligence tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code intelligence tasks. Moreover, utilizing human-written test suites to evaluate functional correctness can be challenging in domains with low resources. To overcome these obstacles, we propose ICE-Score, a new evaluation metric via instructing large language models (LLMs) for code assessments. Our metric addresses the limitations of existing approaches by achieving superior correlations with functional correctness and human preferences, without the need for test oracles or references. We evaluate the efficacy of our metric on two different aspects (human preference and execution success) and four programming languages. Our results demonstrate that our metric surpasses state-of-the-art metrics for code generation, delivering high levels of accuracy and consistency across various programming languages and tasks. We also make our evaluation metric and datasets available to the public, encouraging further research in evaluating code intelligence tasks.", "year": 2023, "publicationdate": "2023-04-27", "externalids": {}, "doi_lower": null}
{"paper_id": 270702705, "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions", "author_names": ["Terry Yue Zhuo", "Minh Chien Vu", "Jenny Chim", "Han Hu", "Wenhao Yu", "Ratnadira Widyasari", "Imam Nur Bani Yusuf", "Haolan Zhan", "Junda He", "Indraneil Paul", "Simon Brunner", "Chen Gong", "Thong Hoang", "A. Zebaze", "Xiao-ke Hong", "Wen-Ding Li", "Jean Kaddour", "Minglian Xu", "Zhihan Zhang", "Prateek Yadav", "Naman Jain", "Alex Gu", "Zhoujun Cheng", "Jiawei Liu", "Qian Liu", "Zijian Wang", "David Lo", "Binyuan Hui", "Niklas Muennighoff", "Daniel Fried", "Xiao-Nan Du", "H. D. Vries", "L. V. Werra"], "venue": "International Conference on Learning Representations", "abstract": "Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks ranging from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks or standalone function calls. Solving challenging and practical tasks requires the capability of utilizing diverse function calls as tools to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding complex instructions. Fulfilling both of these characteristics can pose a great challenge for LLMs.To assess how well LLMs can solve challenging and practical tasks via programs, we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that automatically transforms the original docstrings into short instructions only with essential information. Our extensive evaluation of 60 LLMs shows that LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%. The results underscore the need for further advancements in this area.", "year": 2024, "publicationdate": "2024-06-22", "externalids": {"DOI": "10.48550/arXiv.2406.15877"}, "doi_lower": "10.48550/arxiv.2406.15877"}
{"paper_id": 202643539, "title": "Vol. 37, Issue 4, August 2010 / Band 37, Heft 4, August 2010", "author_names": [], "venue": "Transfusion Medicine and Hemotherapy", "abstract": null, "year": 2010, "publicationdate": "2010-07-27", "externalids": {"DOI": "10.1159/000319681"}, "doi_lower": "10.1159/000319681"}
{"paper_id": 160400641, "title": "Die Mundart von Barchfeld an der Werra", "author_names": ["Heinrich Weldner"], "venue": "", "abstract": null, "year": 1991, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 202643539, "title": "Vol. 37, Issue 4, August 2010 / Band 37, Heft 4, August 2010", "author_names": [], "venue": "Transfusion Medicine and Hemotherapy", "abstract": null, "year": 2010, "publicationdate": "2010-07-27", "externalids": {"DOI": "10.1159/000319681"}, "doi_lower": "10.1159/000319681"}
