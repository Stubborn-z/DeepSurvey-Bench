{
  "survey": "Large Language Models (LLMs) have emerged as pivotal tools in AI-driven code generation, revolutionizing software development through their advanced capabilities in code synthesis and comprehension. This survey paper provides a comprehensive review of LLMs, focusing on their methodologies, applications, and the challenges they face. It highlights the transformative impact of LLMs in enhancing programmer productivity, facilitating AI-driven code creation, and integrating natural language processing techniques into automated programming tasks. Key findings reveal LLMs' adaptability across diverse programming environments, their proficiency in generating syntactically and semantically correct code, and their performance on established benchmarks. Despite their potential, challenges such as model bias, security vulnerabilities, and high computational resource demands persist, necessitating further research to optimize their deployment and address ethical considerations. Future directions include expanding benchmarks and datasets, refining training and evaluation methodologies, and ensuring responsible use of LLMs in real-world applications. As LLMs continue to evolve, they promise to further transform software development practices, offering innovative solutions to complex programming challenges while addressing societal implications.\n\nIntroduction Significance of LLMs in AI-Driven Code Creation Large Language Models (LLMs) are revolutionizing AI-driven code creation, significantly influencing software development through enhanced code generation and comprehension capabilities. They excel in complex reasoning tasks, closely mimicking human-like reasoning essential for modern software engineering [1]. The advent of Constitutional AI, which integrates self-critique mechanisms, exemplifies the transformative potential of LLMs in this domain [2]. In code completion, LLMs effectively tackle challenges posed by lengthy code inputs, thereby enhancing the efficiency and accuracy of generation tasks [3]. Instruction tuning, particularly through the structure of Git commits, aligns LLM performance with human instructions, which is critical since larger models do not inherently guarantee better alignment and may produce misleading outputs [4,5]. LLMs also overcome limitations of existing encoder-decoder frameworks, particularly in tasks like code completion that demand efficient inference [6]. Their adaptability to various programming tasks is further supported by methodologies optimizing memory usage during fine-tuning, allowing for effective deployment on constrained hardware [7]. Despite their transformative capabilities, the integration of LLMs into software development faces challenges, including high computational, memory, and storage costs linked to few-shot in-context learning (ICL) [8]. Understanding the scaling laws of neural language models—such as model size, dataset size, and compute resources—is vital for optimizing training and ensuring effective application in AI-driven code creation [9]. The significance of LLMs extends to enhancing programmer productivity and refining code generation processes. Techniques like Nucleus Sampling, which balances diversity and reliability, play a crucial role in AI-driven code creation [10]. The interaction between programmers and AI tools, such as GitHub Copilot, is poised to alter programming practices significantly [11]. As these models evolve, they promise to reshape software development landscapes, underscoring their importance in contemporary programming. Motivation for the Survey This survey on large language models (LLMs) for code generation is motivated by the need to address critical deficiencies in existing research and applications. A primary concern is the high memory consumption associated with finetuning large models, which restricts their accessibility on standard hardware [7]. The survey aims to explore methodologies that enhance LLM efficiency, making them more usable in resource-constrained environments. Current program understanding and generation methods often lack versatility across various programming benchmarks and tasks. This work emphasizes the necessity for innovative approaches that incorporate additional signals, such as unit tests, to enhance performance in complex coding tasks [12]. By investigating these innovative strategies, the survey seeks to bridge existing gaps, particularly in generating code from natural language. Additionally, there is a notable lack of comprehensive evaluations of language-to-code generation capabilities of LLMs, revealing significant gaps in current research [13]. By focusing on these evaluation challenges, the survey contributes to developing more robust, efficient, and contextually aware AI programming tools. Understanding the interactions between programmers and AI coding assistants is essential for optimizing LLM integration into development environments [11]. This survey delves into these interactions to enhance productivity and improve AI-driven code creation efficacy. By addressing these challenges, the survey significantly advances the field of LLMs for code generation, offering insights into optimizing their deployment in real-world programming. It highlights the potential of LLMs, including smaller open-source models like Llama-2 and StarCoder, to interpret and generate code using novel libraries based on in-context specifications. The survey also underscores the broader application of LLMs across software engineering tasks, from coding and design to testing and program repair, while identifying the need for hybrid techniques to mitigate issues such as hallucinations. Furthermore, it explores systematic evaluations of LLMs, the importance of well-curated datasets, and the role of prompt engineering, paving the way for more adaptable coding environments and setting a roadmap for future research in optimizing LLM performance in software engineering [14,15,16,17]. Objectives of the Paper This paper aims to enhance the understanding and application of large language models (LLMs) in code generation by improving their capabilities and addressing inherent limitations. A significant objective is to propose frameworks such as CodeRL, which integrates pretrained language models with deep reinforcement learning to enhance program synthesis [12]. This approach seeks to leverage LLM strengths in generating code from natural language inputs, thereby broadening their utility across diverse programming tasks. Additionally, the paper establishes a systematic evaluation framework, L2CEval, to assess LLMs' capabilities in generating code from natural language. This initiative addresses the need for comprehensive metrics that capture the nuances of language-to-code generation [13]. By focusing on these evaluation challenges, the paper contributes to developing robust and contextually aware AI programming tools. Another objective is to explore the dynamics of interactions between programmers and AI coding assistants, categorizing these interactions into modes of acceleration and exploration. This understanding is crucial for optimizing LLM integration into development environments, enhancing productivity and improving the efficacy of AI-driven code creation [11]. Through these objectives, the paper seeks to advance innovation and research in LLMs for code generation, evaluating their capabilities in interpreting unfamiliar code libraries, optimizing software engineering processes, and addressing non-functional properties such as robustness and security. Ultimately, this effort aims to enhance LLM practical applications in real-world programming scenarios while identifying and mitigating potential challenges like security API misuse and generating trustworthy code [14,18,19,16,15]. Structure of the Survey The survey is systematically organized to cover multifaceted aspects of large language models (LLMs) for code generation, providing comprehensive insights into their capabilities, methodologies, advancements, applications, challenges, and future directions. The introduction highlights the transformative role of LLMs in AI-driven code creation, emphasizing their potential to emulate human-like decision-making through extensive Web knowledge acquisition. It outlines the motivation for the survey, detailing the objectives of systematically reviewing LLM-based autonomous agents, exploring their diverse applications, and evaluating their performance within software engineering and reasoning tasks [20,15,21,8,17]. The background and core concepts section discusses the evolution of LLMs, principles of code generation, and the integration of natural language processing techniques, culminating in automated programming and AI code generation. This foundational knowledge is crucial for understanding LLM capabilities, which are explored in the subsequent section. The focus is on handling diverse programming tasks, generating syntactically and semantically correct code, and performance evaluation using benchmarks. The methodologies and techniques section examines training and fine-tuning approaches, including supervised learning, reinforcement learning, and transfer learning, which are vital for optimizing LLM performance. This is followed by advancements and innovations, highlighting recent developments in pre-training, dropout techniques, architectural frameworks, code reasoning, and multilingual capabilities. Applications of LLMs in AI-driven code creation are discussed, emphasizing their integration into development environments, enhancement of programmer productivity, and utilization in real-world programming tasks. The survey addresses challenges and limitations of LLMs, highlighting issues such as model bias, security vulnerabilities, and significant computational demands. It underscores the importance of evaluating LLMs at both task and societal levels to understand potential risks, reviewing various evaluation methods and benchmarks to optimize LLM applications across diverse domains, including software engineering. Despite their transformative capabilities, LLMs present technical challenges like hallucinations, necessitating hybrid techniques for reliable application. The survey identifies research gaps and suggests promising directions for future studies to enhance LLM robustness, security, privacy, explainability, efficiency, and usability across applications [19,20,16,15,17]. Finally, the future directions section speculates on potential research and development paths, including expanding benchmarks and datasets, optimizing training techniques, and addressing ethical and societal implications. This organizational structure categorizes evaluation methods into fields such as natural language processing tasks, reasoning, medical applications, and ethical considerations, using criteria like task relevance and societal impact for a holistic exploration of LLMs in code generation [20].The following sections are organized as shown in . Background and Core Concepts Evolution of Large Language Models The evolution of large language models (LLMs) has been marked by significant advancements in architecture and functionality, particularly in code generation tasks. Early models like BERT, utilizing transformer architectures, established a foundation for contextual language representations, leading to enhanced innovations [1]. Scaling laws indicate that larger models yield better performance and improved sample efficiency, reinforcing the notion that increased model size correlates with enhanced capabilities in code generation [22]. Recent efforts have concentrated on effectively scaling model capacity. Techniques such as the Mixture of Experts (MoE) enhance scalability and efficiency, enabling LLMs to function in constrained computational environments [8]. Models like DeepSeek-V2 exemplify this trend by supporting extended context lengths, demonstrating advancements in handling larger contexts [23]. The introduction of diverse datasets has significantly improved LLM versatility and performance in code generation. Benchmarks like DS-1000 feature realistic problems from platforms such as StackOverflow, employing multi-criteria evaluations to comprehensively assess LLM capabilities [24]. Additionally, Octopack leverages extensive code and human instructions from Git commits to further enhance performance [4]. Architectural innovations have addressed traditional transformer limitations, particularly regarding long input sequences during inference. Techniques such as relative position representations in self-attention mechanisms enhance the models' ability to process longer sequences [8]. Moreover, approaches optimizing task-specific vectors while preserving pretrained parameters tackle the inefficiencies of fine-tuning large models for new NLP tasks [8]. Comprehensive evaluation frameworks have emerged to assess LLM capabilities in code generation. Benchmarks like GPT-NeoX-20B address evaluation challenges in large autoregressive models, yielding critical insights into performance metrics and research accessibility [25]. Research organization into stages—pre-training, adaptation tuning, utilization, and capacity evaluation—provides a structured framework for advancing LLM capabilities [8]. Despite these advancements, challenges persist, such as the need for continual learning methodologies to address knowledge retention and acquisition. As LLMs evolve, overcoming limitations like naive greedy decoding in chain-of-thought prompting is essential for capturing diverse reasoning paths in code generation tasks. Conversational interactions enhance context and continuity in software development, increasing the utility of LLMs. Ensuring the safe application of LLMs in automated programming necessitates addressing challenges related to code quality, security, and programmer responsibility. The growing use of tools like GitHub Copilot underscores the importance of mitigating security API misuse, which can introduce vulnerabilities in automatically generated code. By leveraging advances in software engineering, such as program repair and analysis, developers can enhance the reliability and security of AI-generated code, paving the way for a future where programmers adapt their roles to harness automatic programming's full potential [26,18]. Core Concepts in Code Generation Core principles of code generation using LLMs involve transforming natural language inputs into executable code, leveraging advanced machine learning architectures. Transformer-based models like UniXcoder utilize semantic relationships between code elements, integrating abstract syntax trees (AST) and code comments for improved understanding and generation [6]. This approach mitigates the limitations of relying solely on syntactic structures, which often overlook nuanced semantic relationships critical for efficient code comprehension. Accurate code generation relies on effective representation of code tokens and their interrelationships. Attention mechanisms enhance translation accuracy by focusing on corresponding sub-trees within the code, improving the coherence of generated outputs [10]. However, challenges in modeling long-range dependencies between code tokens remain crucial for generating coherent summaries. The DS-1000 benchmark specifically evaluates performance in generating solutions for data science tasks using Python libraries, contributing to a comprehensive understanding of code generation capabilities [24]. Integrating contextual information is vital in code generation. Retrieval-augmented generation (RAG) methods inject factual information into LLMs, enhancing adaptability and performance in dynamic coding environments [12]. This is particularly beneficial in modular software development, where cross-file context is essential for accurate code completion. The L2CEval benchmark evaluates language-to-code generation across various tasks, including semantic parsing and Python programming, offering insights into models' capabilities [13]. Evaluating code generation requires benchmarks reflecting real-world programming scenarios. Holistic evaluation frameworks focus on assessing language models across diverse scenarios and metrics, highlighting their capabilities and limitations [9]. These benchmarks provide insights into model performance, particularly for individuals with limited programming experience, underscoring the need for varied evaluation methods. Instruction tuning, especially supervised instruction tuning, is crucial for refining LLM methodologies in code generation. This process aligns model outputs with human instructions, enhancing the quality and accuracy of generated code. Techniques like Chain of Thought prompting improve reasoning capabilities by demonstrating intermediate steps. This adaptability is essential for overcoming challenges related to injecting new information and optimizing model performance across domains, particularly in light of vulnerabilities associated with training data extraction and the limitations of unsupervised fine-tuning [27,28,29,30]. Continued research into these core concepts will significantly advance the efficiency and reliability of programming tools, enhancing LLM applications in real-world scenarios. Natural Language Processing Techniques Natural Language Processing (NLP) techniques are foundational to the functionality of LLMs, enabling them to process and generate human-like text, which is critical for code generation applications. Transfer learning is a pivotal NLP technique that enhances LLM performance by leveraging pre-trained knowledge from extensive datasets across various tasks. This approach involves training a model on a data-rich task before fine-tuning it for specific downstream tasks, such as summarization, question answering, and text classification. Utilizing architectures like unified text-to-text transformers and methods such as adapter modules for parameter efficiency, transfer learning enables LLMs to achieve state-of-the-art results on numerous benchmarks. Despite challenges in incorporating new factual information, transfer learning remains essential for enhancing LLM capabilities across diverse language understanding and generation tasks [29,31,32,33,30]. Benchmarks are vital for evaluating the functional correctness of code generated from natural language descriptions. The DSP metric offers a comprehensive evaluation of a model's ability to understand and solve complex data science tasks [34], assessing LLMs' capability to accurately translate natural language into executable code, particularly in complex environments. Integrating RAG techniques into NLP models enhances performance by improving noise robustness, negative rejection, information integration, and counterfactual robustness. These capabilities—spanning robustness, security, privacy, explainability, efficiency, and usability—are crucial for generating accurate and contextually relevant code, especially in scenarios where traditional models may struggle, such as interpreting unfamiliar libraries or adapting to dynamic coding environments. Contemporary LLMs exhibit proficiency in learning novel library modules from minimal input, significantly transforming software engineering practices [14,19]. Evaluating LLMs in multilingual contexts is another critical aspect of NLP techniques. Benchmarks designed to assess performance across various programming languages ensure that LLMs can generate code in a linguistically diverse manner, essential for global software development. The use of diverse datasets, such as the Pile, consisting of extensive text data from various domains, underscores the importance of NLP techniques in enhancing model generalization and few-shot learning effectiveness [25]. Adapting existing supervised datasets into prompted formats enhances the diversity of task representations, enabling LLMs to adeptly handle a broad spectrum of programming scenarios. This approach capitalizes on LLMs' inherent code generation and comprehension capabilities, facilitating their ability to interpret novel library modules and optimize software engineering processes. By leveraging diverse datasets and prompt engineering, LLMs can effectively address complex software testing tasks and improve performance across various programming languages, paving the way for more dynamic and adaptable coding environments [14,31,15,17,30]. This adaptability is crucial for tackling the complexities of real-world coding tasks, which current benchmarks often inadequately cover. As NLP techniques advance, their integration into LLMs is set to significantly enhance code generation capabilities. This evolution will enable models to produce more accurate, efficient, and contextually aware code across diverse programming tasks and languages. Recent research highlights LLMs' proficiency in interpreting novel code libraries and modules from natural language descriptions or raw code, even without explicit demonstrations. Furthermore, integrating automated program repair and analysis tools can address concerns about code quality and security, ensuring that automatically generated code adheres to high assurance standards. The development of conversational interactions with LLMs, exemplified by systems like the Programmer’s Assistant, shows promise in facilitating deeper engagement and co-creative processes in software development. This holistic approach optimizes code generation while addressing critical aspects such as robustness, security, privacy, and usability, paving the way for more dynamic and adaptable coding environments [14,26,19,35,15]. Automated Programming and AI Code Generation Automated programming, driven by AI and LLMs, signifies a substantial evolution in software development, enhancing efficiency and reliability in code generation. This advancement is exemplified by Automated Program Repair (APR) techniques, which improve the reliability of LLM-generated code by automatically correcting errors [36], crucial for maintaining high code quality and reducing manual debugging efforts. Benchmarks like DeepSeek-C54 introduce novel evaluation frameworks focusing on fill-in-the-blank tasks and utilizing high-quality project-level code corpora to assess open-source models [37]. This approach emphasizes contextual understanding and synthesis, thereby enhancing the accuracy of generated code compared to traditional benchmarks. The introduction of multi-step reasoning tasks and multilingual capabilities in benchmarks, such as those described in [38], provides a comprehensive assessment of LLMs, ensuring robustness across complex reasoning and diverse linguistic contexts. These benchmarks are pivotal in testing LLM adaptability to real-world coding challenges. NaturalCodeBench (NCB) aims to bridge the gap between synthetic benchmarks and actual coding tasks, providing a more challenging and realistic evaluation environment [39]. This benchmark tests LLMs' capabilities in handling authentic programming scenarios, offering insights into their practical applicability and effectiveness. Categorizing research into stages such as local AI processing, Edge AI, and Cloud AI highlights the diverse applications of AI in automated programming, from embedded systems to complex cloud-based tasks [40]. This categorization underscores LLMs' adaptability across various computational environments, ensuring utility in a wide range of applications. Frameworks addressing LLM implications in automatic programming, such as those discussed in [26], categorize research concerns about code quality and security. These frameworks are essential for understanding the broader impact of LLMs on software development, particularly in ensuring generated code meets necessary quality and security standards. Self-Debugging methods enable LLMs to enhance code generation accuracy by analyzing execution results and providing natural language explanations [41]. This capability allows models to autonomously identify and correct errors, significantly improving the reliability of generated code. The LEVER framework enhances the verification process by reranking programs based on learned verification scores, adding assurance to the correctness of generated code [42]. This verification is critical for ensuring LLMs produce not only syntactically correct but also functionally accurate code. Techniques such as the (IA)$^3$ method, which involves scaling activations by learned vectors, enable effective adaptation of pre-trained models to new tasks with minimal additional parameters, optimizing LLM performance in diverse programming environments [43]. Stable Code introduces a unique combination of general-purpose and instruction-tuned models, offering a comprehensive approach to code generation that balances generality with task-specific tuning [44]. This ensures LLMs are versatile yet precise in their code generation capabilities, adapting effectively to various programming challenges. Through these advancements, AI and LLMs continue to transform automated programming by enhancing efficiency, security, and contextual understanding. Tools like GitHub Copilot exemplify the growing popularity of automatic programming; however, challenges remain, particularly regarding code quality and trust during deployment. While LLMs can assist in uncovering software vulnerabilities and improving code assurance through automated repair, they also highlight significant issues, such as the misuse of security APIs. Research indicates that LLM-generated code often contains security API misuses, with studies revealing that up to 70\\ In recent years, the evolution of Large Language Models (LLMs) has significantly transformed the landscape of software engineering, particularly in the realm of code generation. As illustrated in , this figure categorizes the capabilities of LLMs, showcasing their adaptability and efficiency in addressing a variety of programming tasks. The hierarchical structure presented emphasizes key concepts and relationships that underline the methodologies for generating syntactically and semantically correct code. Furthermore, it highlights the performance of these models on various benchmarks and evaluations, thereby underscoring the significant advancements and applications of LLMs in the field. This comprehensive overview not only enhances our understanding of LLMs but also sets the stage for future research and development in automated programming solutions. Capabilities of Large Language Models in Code Generation Handling Diverse Programming Tasks Large language models (LLMs) exhibit remarkable adaptability in diverse programming tasks, leveraging advanced methodologies to enhance performance across various coding environments. A significant challenge is the high adaptation costs of fine-tuning large-scale pre-trained language models (PLMs), which can be impractical for task-specific instances [45]. Addressing these inefficiencies is crucial for optimizing LLM deployment in varied programming contexts. illustrates the categorization of handling diverse programming tasks using LLMs, highlighting key areas such as LLM adaptability, optimization challenges, and non-functional properties. This figure showcases the roles these models play in enhancing coding practices and software engineering processes, underscoring the importance of their versatility. AlphaCode exemplifies LLM versatility by generating millions of code variations through transformer-based networks, showcasing their potential to produce diverse and contextually relevant solutions [46]. This capability is essential for complex programming environments. LLM proficiency is further demonstrated by their ability to learn and utilize novel library modules effectively, with smaller models often performing comparably to larger ones [14]. This finding indicates that resource-constrained models can also exhibit substantial adaptability, making them viable for environments with limited computational resources. The Programmer's Assistant, evaluated by 42 participants, illustrates the capacity of LLMs for engaging in deeper, contextual conversations, leading to more relevant responses and improved task performance [35]. Such capabilities are pivotal for effective interactions between programmers and AI models, ensuring generated code aligns closely with user intent and task requirements. Moreover, Code Completion with Attention and Graphs (CCAG) effectively captures complex dependencies and patterns in code, addressing limitations of traditional Abstract Syntax Tree (AST)-based methods that often overlook intricate relationships [47]. By improving adaptability, CCAG ensures LLMs generate code that accurately reflects programming complexities. The diversity of deep learning applications, such as malware detection, further illustrates LLM versatility in adapting to various coding challenges [48]. This adaptability is vital for maintaining LLM effectiveness across a wide range of programming tasks, contributing to their growing significance in AI-driven code generation. Through these advancements, LLMs continue to demonstrate impressive capabilities in managing diverse programming tasks, transforming software engineering by enhancing efficiency, reliability, and adaptability in code generation processes. Additionally, LLMs address non-functional properties such as robustness, security, privacy, and usability, crucial for deploying automatically generated code. Their application in software testing, including test case preparation and program repair, ensures software quality and reliability. The integration of LLMs into software engineering processes paves the way for innovative and efficient coding practices while highlighting areas for future research and development [14,26,19,15,17]. Generating Syntactically and Semantically Correct Code LLMs possess a robust capacity for generating code that is both syntactically and semantically accurate, employing advanced methodologies to ensure high-quality outputs. The integration of Abstract Syntax Trees (AST) and comments in models like UniXcoder enhances code representation, achieving state-of-the-art performance across various tasks [6]. This approach addresses limitations of relying solely on syntactic structures, capturing nuanced semantic relationships crucial for efficient code understanding. Innovative techniques such as prompt tuning significantly outperform traditional few-shot learning methods, matching the performance of larger models [5]. This adaptability is crucial for optimizing LLM performance across diverse coding tasks, ensuring generated code aligns closely with user intent. Experiments indicate that fine-tuning language models with human feedback improves alignment with user intent, enhancing truthfulness and reducing toxicity [5]. Incorporating Automated Program Repair (APR) techniques into LLMs enhances their ability to rectify common programming mistakes in both auto-generated and human-crafted solutions, improving the reliability and correctness of generated code. This integration minimizes the need for manual debugging and ensures high code quality by leveraging retrieval-augmented frameworks like REDCODER, which enhance code generation and summarization through dense retrieval techniques [26,49,50,28,36]. The LEVER framework improves the generation of syntactically and semantically correct code by implementing a verification process that evaluates program correctness based on execution results. It trains verifiers to assess generated code accuracy by considering natural language input, the program itself, and execution outcomes. By integrating verification scores with language model generation probabilities, LEVER consistently enhances code generation performance, achieving new state-of-the-art results across diverse datasets, including table QA, math QA, and basic Python programming tasks [15,19,42]. Self-Instruct significantly improves performance on instructional tasks, aligning language models with instructions while reducing the need for human annotations. This alignment enhances LLM reasoning processes, ensuring generated code is both syntactically correct and semantically coherent. LLMs can interpret and learn novel library usage from in-context demonstrations, natural language descriptions, or raw code implementations, allowing them to adapt to dynamic coding environments. Methods like ASTxplainer align token predictions with abstract syntax tree nodes for improved explainability, while fine-grained natural language and code alignments from sources like Stack Overflow refine models' ability to synthesize, retrieve, and summarize code accurately across diverse programming languages [14,15,51,52]. Through these advancements, LLMs continue to generate syntactically and semantically correct code by leveraging innovative methodologies that enhance accuracy and reliability across diverse programming tasks. Notably, LLMs exhibit proficiency in understanding and utilizing novel code libraries, even with minimal context such as natural language descriptions or raw code implementations. This adaptability, coupled with their impact on software engineering, optimizes processes and outcomes across various tasks, although challenges in ensuring code quality and security, particularly regarding the misuse of security APIs, remain. Ongoing evaluations of non-functional properties like robustness and explainability, along with automated program repair techniques, are crucial for advancing the trustworthiness and effectiveness of LLM-generated code [14,26,18,19,15]. Performance on Benchmarks and Evaluation Evaluating LLM performance in code generation is critical for understanding capabilities and identifying areas for enhancement. Benchmarks and metrics provide standardized performance measures, facilitating comprehensive assessments across diverse coding tasks. Table provides a detailed overview of the benchmarks employed to assess the performance of large language models in code generation and other tasks, highlighting the metrics and domains critical for evaluating model efficiency and effectiveness. The JuPyT5 model solved 77.5\\ Stable Code exhibited state-of-the-art performance on various benchmarks, underscoring the value of innovative approaches in enhancing LLM capabilities [44]. Despite advancements, challenges persist, as evidenced by Codex-002's 43.3\\ Evaluation of Gemma models revealed strong performance, surpassing similarly sized open models on 11 out of 18 tasks, underscoring the importance of incorporating sophisticated reasoning capabilities into LLMs for enhanced code generation [23]. Experiments on benchmarks such as HumanEvalPack demonstrated state-of-the-art performance of models like OctoCoder and OctoGeeX, providing insights into their adaptability and efficiency [4]. QLoRA shows efficient fine-tuning of LLMs is feasible on limited hardware, achieving state-of-the-art results while significantly reducing memory requirements [7]. This efficiency is crucial for optimizing LLM deployment in resource-constrained environments, ensuring utility across diverse programming tasks. CodeRL achieved state-of-the-art results on the APPS benchmark and exhibited strong zero-shot transfer capabilities on the MBPP benchmark, highlighting the potential of reinforcement learning approaches in enhancing LLM performance [12]. Evaluation revealed 25 top-level findings and improved coverage from 17.9\\ Experiments demonstrated that GPT-NeoX-20B outperformed baseline models in terms of perplexity, indicating capabilities in generating code [25]. The L2CEval framework employs metrics such as Accuracy and F1-score to measure LLM effectiveness in generating accurate code, providing a robust basis for evaluating model performance [13]. Through these comprehensive evaluations, benchmarks play a crucial role in advancing understanding and development of LLMs by systematically assessing capabilities in various domains, such as code generation and comprehension. Evaluations guide future innovations by highlighting LLMs' potential to interpret and generate code from unfamiliar libraries, even with minimal demonstrations or natural language descriptions. They emphasize the importance of evaluating LLMs not only for task-specific performance but also for societal impacts, informing improvements in methodologies like automated Verilog RTL code generation. Continuous benchmarking is essential for fostering adaptable, dynamic, and proficient LLMs, ultimately enhancing both research and practical applications [14,53,20]. Methodologies and Techniques Supervised Learning and Fine-Tuning Techniques Supervised learning and fine-tuning are pivotal in refining large language models (LLMs) for code generation, ensuring outputs are precise and contextually relevant. Techniques like prefix-tuning optimize continuous prefix vectors for specific tasks, enhancing adaptability while maintaining computational efficiency [29]. Adapter modules facilitate task-specific customization without altering the original model parameters, enabling efficient personalization for code generation [43]. The LongCoder model utilizes sparse Transformer architectures, employing sliding window mechanisms for self-attention and leveraging bridge and memory tokens to boost code completion performance [3]. This approach is crucial for managing extensive code inputs, thereby enhancing efficiency and accuracy in generation tasks. InstructGPT employs a two-step fine-tuning process with supervised learning followed by reinforcement learning from human feedback, ensuring alignment with user intent and enhancing output truthfulness [5]. Similarly, MFTCoder showcases parallel fine-tuning across varied tasks, demonstrating LLM adaptability in diverse programming scenarios [54]. UniXcoder achieves state-of-the-art performance using mask attention matrices and prefix adapters, leveraging contrastive learning for representation alignment [6]. This integration enhances LLMs' ability to represent code tokens and their interrelationships, improving output coherence. QLoRA illustrates efficient fine-tuning by backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters, significantly reducing memory requirements [7]. This approach is crucial for optimizing LLM deployment in resource-constrained environments. CodeRL integrates pretrained language models with deep reinforcement learning, providing dense feedback on generated code to enhance program synthesis [12]. This reinforcement learning methodology improves reliability and correctness, reducing manual debugging needs. The L2CEval benchmark evaluates various large language models, including those pretrained on code, through different prompting methods and instruction tuning, providing insights into the effectiveness of supervised learning and fine-tuning techniques [13]. These methodologies ensure efficient, accurate, and contextually aware outputs across diverse programming tasks. LLMs have shown promise in interpreting and generating code using unfamiliar libraries in context, with smaller open-source models like Llama-2 and StarCoder demonstrating proficiency in understanding novel code libraries. While fine-tuning enhances pre-trained knowledge, retrieval-augmented generation (RAG) often surpasses it in incorporating new information. Automated programming tools like GitHub Copilot have transformed code generation, although challenges regarding code quality and trust persist. Advances in software engineering, such as program repair, can further enhance the reliability of automatically generated code. As LLMs evolve, their role in software engineering expands, optimizing processes and outcomes across various tasks [14,26,30,15,55]. illustrates the categorization of supervised learning and fine-tuning techniques for code generation in large language models. The techniques are organized into three primary categories: enhancing efficiency, improving alignment, and optimizing resources, highlighting key methodologies and innovations within each category. Reinforcement Learning and Preference Optimization Reinforcement learning and preference optimization enhance LLMs in code generation by aligning outputs with human expectations and improving code quality. The RLTF framework uses online reinforcement learning with multi-granularity unit test feedback to refine code generation processes, boosting reliability and correctness [56]. This feedback-driven approach is vital for improving adaptability and accuracy across diverse coding environments. Proximal Policy Optimization (PPO) optimizes surrogate objective functions through stochastic gradient ascent, allowing multiple updates from the same data sample, enhancing efficiency and robustness [57]. This methodology refines outputs to better match human preferences, ensuring syntactically and semantically accurate generated code. CodeRL's critic network evaluates program correctness, guiding the generation process through reinforcement learning, essential for improving reliability and reducing manual debugging [12]. Low-Rank Adaptation (LoRA) reduces the number of parameters needing training by injecting trainable low-rank matrices into frozen pre-trained model weights, optimizing memory usage and enhancing performance in resource-constrained settings [58]. Hyperparameter optimization challenges emphasize the need for efficient methodologies [59]. The Self-Instruct method generates and filters instructions and sample outputs to fine-tune the original model, aligning language models with human instructions [60]. Instruction tuning enhances adaptability and efficiency in handling diverse coding tasks [61]. This iterative refinement process ensures models can respond to feedback and improve outputs. Leveraging deep reinforcement learning and preference optimization methodologies, recent advancements in LLMs for code generation have significantly enhanced capabilities. These approaches address limitations of traditional supervised fine-tuning by integrating feedback mechanisms, such as unit tests and compiler feedback, to improve functional correctness and efficiency. Models like CodeRL and StepCoder achieve state-of-the-art results across various benchmarks by employing techniques like Proximal Policy Optimization and breaking down complex tasks into manageable subtasks. These methodologies enable LLMs to adapt to novel code libraries and execute complex reasoning tasks, ensuring accurate and contextually aware outputs across diverse programming challenges [14,12,62,8,63]. Transfer Learning and Unified Frameworks Transfer learning and unified frameworks streamline LLM adaptation to diverse tasks, enhancing capabilities in code generation. CodeS exemplifies modular processing of code repository components, optimizing model handling of complex coding environments [64]. This modularity is crucial for efficient management and generation across varied programming scenarios. Graph-guided masked attention functions in GraphCodeBERT leverage data flow during pre-training to incorporate code structure effectively [65]. This approach enhances understanding of syntactic and semantic relationships within code, vital for generating accurate outputs. ASTxplainer visualizes LLM predictions by mapping them to syntactic structures, offering insights into decision-making processes and facilitating better interpretation [51]. This capability refines predictive accuracy and ensures alignment with intended programming logic. Adapter modules, as seen in MFTCoder, enable efficient transfer learning by introducing a few additional parameters for each task while keeping most of the model unchanged. Prefix-tuning leverages pretrained knowledge while minimizing computational and storage overhead [66]. These techniques contribute to a unified framework enhancing adaptability and efficiency. ScMoE integrates overlapping communication and computation processes to reduce latency in Mixture of Experts (MoE) models [67]. This integration is pivotal for optimizing learning processes and ensuring efficient adaptation to new tasks. DeepSeek-V2 employs sparse activation mechanisms to optimize training and inference, showcasing transfer learning principles in enhancing performance [68]. This optimization maintains high efficiency and accuracy in resource-constrained settings. The Tree of Thought (ToT) methodology explores decision-making paths, allowing informed choices based on self-evaluation [69]. This capability refines reasoning processes, ensuring generated code is syntactically correct and semantically coherent. Transfer learning and unified frameworks significantly enhance LLM capabilities for code generation. Recent advancements show that smaller, open-source models like Llama-2 and StarCoder can interpret novel code libraries from context using natural language descriptions or raw code implementations. This adaptability enables LLMs to operate in dynamic environments, making them valuable tools for diverse tasks. Efforts to unify architectures and learning methods have led to efficient training processes, as seen in CodeGen2 models. These innovations improve accuracy and efficiency while addressing non-functional properties such as robustness, security, and usability. Systematic literature reviews highlight the transformative impact of LLMs in software engineering, optimizing processes and outcomes while offering promising directions for future research and application [14,19,32,15,55]. Advancements and Innovations Innovations in Pre-Training and Dropout Techniques Recent advancements in pre-training and dropout techniques have markedly improved the efficiency and accuracy of large language models (LLMs) in code generation. The LEVER framework exemplifies these improvements by integrating verification scores with generation probabilities to rerank programs, aligning outputs with logical and functional standards [42]. PLBART merges denoising autoencoding with diverse programming tasks, outperforming existing models and enabling effective navigation through complex coding environments [70]. Adapter modules further enhance performance by reducing parameter overhead without demanding extensive computational resources [29]. Innovative methods such as the AI Code Competitions benchmark highlight uncertain tokens, offering insights into LLM reliability [71]. Retrieval-augmented generation (RAG) effectively mitigates hallucinations and enhances content accuracy [72], while reinforcement learning advances refine reasoning capabilities [1]. Comprehensive evaluation frameworks like Octopack and HELM ensure thorough assessment across coding tasks [4,9]. Nucleus Sampling improves output quality by balancing diversity and reliability [10], and the L2CEval framework analyzes LLMs across tasks, considering factors like model size and instruction tuning [13]. Grounded theory analysis in user interaction studies provides insights for optimizing LLM interaction models [11]. Collectively, these advancements underscore the importance of pre-training and dropout techniques in enhancing LLM performance for complex programming tasks, including program synthesis and software testing [15,17,55]. Innovative Architectures and Frameworks Recent architectural innovations have significantly enhanced LLM performance and adaptability in code generation tasks. Sparse attention mechanisms, exemplified by the LongCoder model, optimize long sequence processing by focusing on relevant tokens, thereby enhancing efficiency and reducing computational overhead [3]. Mixture of Experts (MoE) models dynamically select specialized sub-models, optimizing resource use and scalability, particularly in resource-constrained environments [67]. GraphCodeBERT's graph-guided masked attention functions incorporate code structure during pre-training, improving understanding of syntactic and semantic relationships [65]. The ASTxplainer framework maps predictions to syntactic structures, facilitating better decision-making interpretation [51]. CodeS's modular approach organizes code repository processing, optimizing handling of complex environments [64]. These architectural advancements, alongside improved pre-training methodologies, empower LLMs to tackle tasks like code generation and software testing effectively [14,15,16,17]. The integration of LLMs in software engineering continues to evolve, offering promising avenues for future research and application. Innovations in Code Reasoning and Execution Innovations in code reasoning and execution have substantially enhanced LLM capabilities, allowing them to perform complex programming tasks with increased accuracy and efficiency. CodeAct's real-time execution of code actions boosts LLM agents' success in agent-oriented tasks [73], crucial for dynamic interactions in complex environments. Automated Program Repair (APR) techniques integrated with LLMs enable automatic error correction, improving reliability and reducing manual debugging [36]. Clef's merge trees capture submission changes, offering a structured framework for code modification understanding [74]. CodeT automates test case generation, ensuring comprehensive testing and validation of generated code [75]. The autoknow method enhances reasoning through self-reflection, allowing LLMs to autonomously identify and correct errors [76]. CRUXEval's benchmark methodology focuses on practical execution tasks with diverse Python functions [77]. Training frameworks emphasizing multi-step mathematical reasoning assess LLM capabilities, ensuring syntactically correct and semantically coherent code generation [78]. These innovations collectively optimize LLM code reasoning and execution processes across various software engineering tasks [15,16]. Multilingual and Domain-Specific Advancements Advancements in multilingual capabilities and domain-specific applications have significantly enhanced LLM versatility in code generation tasks. Diverse datasets like ERNIE-Code, featuring examples from multiple programming and natural languages, expand code intelligence task scopes [79]. Multilingual benchmarks demonstrate LLM ability to generate code in varied linguistic contexts, essential for global software development. Fine-tuning multilingual pre-trained language models (PLMs) with diverse programming datasets improves performance in tasks like code summarization and search [80,19,81,31,82], enhancing adaptability in international contexts. Domain-specific advancements tailor LLMs to specialized fields such as data science, healthcare, and finance. Fine-tuning with domain-specific datasets ensures contextually relevant and functionally accurate code generation. The CONCODE dataset aids in generating class member functions based on English documentation and programmatic context, while retrieval-augmented frameworks like REDCODER mimic developer behavior [14,28,80,50]. Frameworks incorporating domain-specific knowledge into LLM training processes further enhance performance, enabling accurate execution of complex reasoning tasks. These developments optimize software engineering processes and enhance software testing techniques [14,19,20,15,17]. Systematic literature reviews and evaluations continue to identify unique LLM features and applications in software engineering, emphasizing curated datasets and optimized performance strategies. Evaluations across domains highlight robust assessment frameworks' critical role in understanding LLM potential risks and contributions, aiding in developing more proficient models [15,20]. Applications in AI-Driven Code Creation The integration of artificial intelligence, particularly large language models (LLMs), has revolutionized code creation and software development by enhancing productivity, improving code quality, and streamlining workflows. This section examines the applications of LLMs in AI-driven code creation, emphasizing their integration into development environments and their broader implications for programming practices. Integration of LLMs in Development Environments LLMs have transformed development environments by improving code quality and developer productivity through interactive dialogues that generate contextually relevant code snippets [11]. AlphaCode exemplifies this by using transformer-based networks to generate diverse code variations, aiding developers in exploring optimal solutions [46]. The Programmer's Assistant, tested by 42 participants, demonstrates LLMs' ability to facilitate contextual conversations, enhancing task performance [35]. Frameworks like CodeRL integrate pretrained language models with deep reinforcement learning to provide feedback on generated code, thus improving program synthesis and reducing debugging efforts [12]. Code Completion with Attention and Graphs (CCAG) surpasses traditional AST-based methods by capturing complex code dependencies, enhancing adaptability across programming tasks [47]. Retrieval-augmented generation (RAG) methods further augment LLM performance by incorporating new factual data, crucial for accurate code completion in modular development [12]. These advancements underscore LLMs' transformative impact on software engineering, optimizing processes, and enhancing contextual understanding [14,15,19,20]. Enhancing Programmer Productivity LLMs have significantly boosted programmer productivity by streamlining coding tasks. IntelliCode Compose, for instance, generates complete lines of code, reducing manual effort and speeding up development [83]. DeepSeek-V2 increases generation throughput, allowing developers to focus on higher-level design [68]. The Direct Preference Optimization (DPO) framework aligns model outputs with user preferences, ensuring relevant code generation [84]. These innovations highlight LLMs' role in facilitating automatic code generation, language translation, and conversational interactions, addressing non-functional properties like robustness and security. Their ability to learn from in-context demonstrations underscores their adaptability in dynamic coding environments, promising to redefine programming roles and enhance trust in automatically generated code [14,26,19,35,15]. Real-World Programming Tasks and Dataset Utilization LLMs' application in real-world programming tasks demonstrates their versatility and the significance of diverse datasets for performance optimization. Comprehensive datasets, such as those used in the LEVER framework, enhance LLMs' ability to address various programming challenges, including table QA and Python programming [42]. PLBART, utilizing a diverse dataset of Java and Python functions, excels in code summarization and translation tasks [70]. CoCoGen's integration with LLMs like GPT-3.5-Turbo improves code generation accuracy by addressing errors through compiler feedback and static analysis, achieving over 80 CodeT5's benchmarks for defect detection and generation tasks ensure LLMs are equipped to tackle a wide array of programming challenges, from code generation to program repair [14,20,16,15,17]. Clef enhances programmers' learning by providing automated feedback on incorrect submissions, employing a data-driven approach for effective code repair [74,49]. Haggis's application to open-source projects underscores the importance of leveraging diverse datasets to capture programming patterns, enhancing LLM adaptability [14,18,19,15,17]. CodeBERT's evaluation on tasks like natural language code search highlights dataset diversity's role in optimizing LLM performance, achieving state-of-the-art results through a bimodal pre-trained model [82,85,19,86]. Utilizing diverse datasets enhances LLM capabilities in software engineering, enabling efficient code generation and comprehension across various tasks. Studies show LLMs, including smaller models like Llama-2 and StarCoder, can effectively learn and utilize novel code libraries from in-context specifications, underscoring their transformative impact in dynamic coding environments [14,15]. Challenges and Limitations The deployment of large language models (LLMs) in AI-assisted programming is shaped by challenges that affect their reliability and generalizability. This section addresses critical issues such as model bias and evaluation difficulties, which are pivotal in understanding security vulnerabilities and the computational demands associated with LLM deployment. Model Bias and Evaluation Challenges Bias and evaluation issues significantly impact LLMs' effectiveness across programming environments. The reliance on extensive datasets can introduce biases, as they may not encompass all programming paradigms, thus affecting model generalization [24,13]. Training data quality is crucial, as seen in CodeRL, where inadequate training can impair model performance [12]. Evaluation methods, such as those used in QLoRA, often struggle with reliability [7], and benchmarks like DS-1000 may not fully represent real-world scenarios [24]. Aligning AI systems with human values is essential to mitigate risks associated with increased autonomy. Synthetic data reliance and existing benchmarks can lead to an overestimation of model capabilities [11,13]. Moreover, integrating multiple knowledge sources poses challenges in ensuring the quality of retrieved information, impacting LLM reliability [10]. Addressing these challenges requires inclusive benchmarks and refined evaluation methods to ensure LLMs produce reliable outputs across diverse tasks, including specialized domains like reasoning and ethics [14,20]. Security Vulnerabilities LLMs introduce security vulnerabilities in AI-assisted programming that necessitate careful mitigation. Bias from synthetic data can lead to unfair models and security risks [87]. Training data extraction attacks pose threats to LLMs, especially those trained on private datasets, risking sensitive information confidentiality [27]. Data quality and model interpretability are crucial for security, as poor quality can yield inaccurate predictions, and lack of interpretability hinders vulnerability identification [17]. High computational demands during training can restrict security measures if models are not optimized [55]. Mitigating these vulnerabilities requires strategies to identify biases, employ automated interventions, and evaluate security in AI-driven tools. Integrating AI processing across local, Edge, and Cloud systems while managing resource constraints enhances efficiency and adaptability, improving both quality and security of generated code [88,40,89,28,90]. Computational Resource Demands LLM deployment for code generation faces significant computational resource challenges, affecting scalability and accessibility. Extensive parameter training, as seen in prefix-tuning, incurs high costs, limiting adoption [29]. Large models like GPT-NeoX-20B require substantial resources, restricting usage to well-funded groups [25]. Benchmark complexity exacerbates resource demands for effective training and evaluation. In-context learning (ICL) methods, which incorporate training examples into input without gradient-based training, face computational, memory, and storage challenges. Efficient methodologies like parameter-efficient fine-tuning (PEFT) offer alternatives that reduce costs while maintaining accuracy, optimizing resource utilization across domains such as law and medicine [14,91,43,92]. Addressing these resource challenges is crucial to ensure LLMs' accessibility and efficiency across diverse programming environments. Future Directions Expansion of Benchmarks and Datasets Advancing large language models (LLMs) in code generation requires expanding benchmarks and datasets to encompass diverse programming languages and tasks, enhancing model generalization and applicability across domains. Future research should refine reinforcement learning techniques and explore novel architectures for reasoning models [1], while optimizing token mechanisms for varied programming tasks [3]. Improved metrics for memorization detection and diverse code samples in datasets are crucial for enhancing evaluation [72]. Integrating diverse datasets boosts performance, particularly in additional languages and complex translation tasks [70], and expanding benchmarks to include varied problem types will provide holistic evaluation [34]. Studies should further explore dataset enhancements and additional programming tasks [4], aligning with the need to improve model architectures and training methods for Continual Knowledge Learning [93]. Optimizations of the (IA)$^3$ method across tasks may yield insights into optimizing outputs [43]. Expanding benchmarks and datasets is crucial for refining evaluation methodologies related to instruction tuning [44], focusing on IR generation, neural translation models, and diverse training signals [94,12]. Optimizing nucleus size and sampling processes can improve text generation quality [10]. Comprehensive benchmark expansion will enhance LLM evaluation and optimization, leading to efficient implementations in real-world coding scenarios, and future work should enhance retrieval mechanisms to support additional languages and contexts [95]. Optimization of Training and Evaluation Techniques Recent advancements in optimizing training and evaluation methodologies for LLMs emphasize refining transfer learning frameworks and novel datasets to enhance NLP capabilities. Future research should optimize bias term selection for fine-tuning, extending beyond BERT [96], and exemplar selection for chain-of-thought prompting to enhance performance in reasoning tasks [97]. Refining exploration mechanisms and applying the Tree of Thought (ToT) to diverse tasks could improve robustness and adaptability [69], while optimizing prompt tuning across tasks and architectures offers valuable insights into improving outputs [98]. Extending frameworks like CodeBLEU to additional tasks and refining components for accuracy presents opportunities for improving evaluation [41]. Enhancing debugging capabilities, self-feedback methods, and applying these to more languages and tasks can refine interaction models [41]. Optimizing models for specific tasks or developing efficient training techniques to reduce resource requirements enhances accessibility and scalability [25]. By optimizing training and evaluation techniques, LLMs can better navigate real-world programming complexities, ensuring efficient, accurate, and contextually aware outputs across applications. Addressing Ethical and Societal Implications The adoption of LLMs in code generation necessitates examining ethical and societal implications, particularly bias and unfairness from synthetic training data [2]. Addressing these biases involves improving model alignment with human expectations and exploring AI capability trends [99], alongside developing techniques for verifying AI-generated solutions with traditional methods [16]. Data privacy concerns require safeguards for sensitive information in decentralized systems [40], and defenses against poisoning attacks are essential for security and ethical AI use [100]. Questions regarding the reliability of automated code and ethical programmer responsibility remain [26], prompting research into adaptability in dynamic environments and human-agent interaction trends [21]. Transparency and accountability in training and documentation foster trust and fairness [101], with future research developing standards for openness and strategies for training efficiency [22]. Enhancing performance, reducing errors, and incorporating user feedback are critical for exploration [5], and understanding interaction modes can improve AI assistant design and usability [11]. Enhancements in multi-modal learning and UniXcoder applications in diverse tasks should be explored [6]. Addressing ethical and societal implications advances responsible LLM use in code generation, ensuring positive societal contributions while minimizing risks and biases. Conclusion This survey delves into the profound impact of large language models (LLMs) on code generation, underscoring their pivotal role in advancing AI-driven software development. It explores the progression of LLMs, their seamless integration into programming environments, and their enhancement of developer efficiency. Key insights reveal LLMs' versatility across diverse coding tasks, their proficiency in generating code that is both syntactically and semantically precise, and their robust performance against industry benchmarks, which collectively affirm their crucial contribution to modern software engineering. Furthermore, the survey identifies critical challenges such as inherent model biases, potential security risks, and substantial computational demands. Addressing these obstacles is crucial for maximizing the practical application of LLMs in real-world programming scenarios. Future investigations should focus on expanding evaluation benchmarks and datasets, refining training methodologies to enhance model capabilities, and examining ethical and societal considerations to ensure the responsible and fair utilization of LLMs in code generation.",
  "reference": {
    "1": "2212.10403v2",
    "2": "2212.08073v1",
    "3": "2306.14893v1",
    "4": "2308.07124v2",
    "5": "2203.02155v1",
    "6": "2203.03850v1",
    "7": "2305.14314v1",
    "8": "2212.10403v2",
    "9": "2211.09110v2",
    "10": "1904.09751v2",
    "11": "2206.15000v3",
    "12": "2207.01780v3",
    "13": "2309.17446v2",
    "14": "2311.09635v2",
    "15": "2308.10620v6",
    "16": "2310.03533v4",
    "17": "2307.07221v3",
    "18": "2404.03823v1",
    "19": "2403.07506v1",
    "20": "2307.03109v9",
    "21": "2308.11432v7",
    "22": "2001.08361v1",
    "23": "2403.08295v4",
    "24": "2211.11501v1",
    "25": "2204.06745v1",
    "26": "2405.02213v2",
    "27": "2012.07805v2",
    "28": "2211.00609v2",
    "29": "1902.00751v2",
    "30": "2312.05934v3",
    "31": "2204.09653v1",
    "32": "1910.10683v4",
    "33": "2303.18223v16",
    "34": "2201.12901v1",
    "35": "2302.07080v1",
    "36": "2205.10583v4",
    "37": "2401.14196v2",
    "38": "2204.02311v5",
    "39": "2405.04520v1",
    "40": "2403.00833v1",
    "41": "2304.05128v2",
    "42": "2302.08468v3",
    "43": "2205.05638v2",
    "44": "2404.01226v1",
    "45": "2203.06904v2",
    "46": "2203.07814v1",
    "47": "2103.09499v1",
    "48": "2103.05292v3",
    "49": "2210.14306v5",
    "50": "2108.11601v2",
    "51": "2308.03873v1",
    "52": "1805.08949v1",
    "53": "2212.11140v1",
    "54": "2311.02303v1",
    "55": "2305.02309v2",
    "56": "2307.04349v2",
    "57": "1707.06347v2",
    "58": "2106.09685v2",
    "59": "2303.15647v2",
    "60": "2212.10560v2",
    "61": "2109.01652v5",
    "62": "2402.01391v2",
    "63": "2301.13816v4",
    "64": "2403.16443v1",
    "65": "2009.08366v4",
    "66": "2101.00190v1",
    "67": "2404.05019v3",
    "68": "2405.04434v5",
    "69": "2305.10601v2",
    "70": "2103.06333v2",
    "71": "2302.07248v3",
    "72": "2312.10997v5",
    "73": "2402.01030v4",
    "74": "2206.01848v1",
    "75": "2207.10397v2",
    "76": "2306.02907v1",
    "77": "2401.03065v1",
    "78": "2110.14168v2",
    "79": "2212.06742v2",
    "80": "1808.09588v1",
    "81": "2204.00498v1",
    "82": "2210.14868v3",
    "83": "2005.08025v2",
    "84": "2305.18290v3",
    "85": "2403.16792v3",
    "86": "2305.07922v2",
    "87": "2107.03374v2",
    "88": "2304.05302v3",
    "89": "2212.09420v2",
    "90": "2002.08155v4",
    "91": "2105.04144v1",
    "92": "2305.15377v1",
    "93": "2108.07258v3",
    "94": "2302.04012v2",
    "95": "2312.11658v2",
    "96": "2310.03003v1",
    "97": "2310.16218v4",
    "98": "2306.08568v2",
    "99": "2010.03150v1",
    "100": "2106.10199v5",
    "101": "2201.11903v6",
    "102": "2104.08691v2",
    "103": "2310.19852v6",
    "104": "2007.02220v3",
    "105": "2307.05532v1"
  },
  "chooseref": {
    "1": "2005.00653v1",
    "2": "2211.00609v2",
    "3": "2303.18223v16",
    "4": "2307.03109v9",
    "5": "2308.11432v7",
    "6": "2202.13169v3",
    "7": "2302.07248v3",
    "8": "2401.03003v4",
    "9": "2310.19852v6",
    "10": "2404.03823v1",
    "11": "1706.03762v7",
    "12": "2206.01848v1",
    "13": "2205.10583v4",
    "14": "2405.02213v2",
    "15": "2212.11140v1",
    "16": "2309.01431v2",
    "17": "1810.04805v2",
    "18": "2106.10199v5",
    "19": "2211.05100v4",
    "20": "2307.14936v1",
    "21": "2206.06888v1",
    "22": "2201.11903v6",
    "23": "2212.10007v2",
    "24": "2401.08500v1",
    "25": "2103.09499v1",
    "26": "2308.12950v3",
    "27": "2207.03578v5",
    "28": "2406.11409v2",
    "29": "2302.04012v2",
    "30": "2403.16443v1",
    "31": "2305.07922v2",
    "32": "2109.00859v1",
    "33": "2002.08155v4",
    "34": "2009.10297v2",
    "35": "2310.17680v3",
    "36": "2303.17568v2",
    "37": "2203.13474v5",
    "38": "2302.00288v3",
    "39": "2207.01780v3",
    "40": "1909.09436v3",
    "41": "2207.10397v2",
    "42": "2102.04664v2",
    "43": "2203.07814v1",
    "44": "2203.05132v1",
    "45": "2212.08073v1",
    "46": "2310.11248v2",
    "47": "2401.03065v1",
    "48": "2211.11501v1",
    "49": "2302.03169v3",
    "50": "2103.05292v3",
    "51": "2205.11739v1",
    "52": "1512.03385v1",
    "53": "2401.14196v2",
    "54": "2401.02954v1",
    "55": "2405.04434v5",
    "56": "2203.06904v2",
    "57": "2305.18290v3",
    "58": "2212.06742v2",
    "59": "2206.07682v2",
    "60": "2311.09635v2",
    "61": "2308.03873v1",
    "62": "2107.03374v2",
    "63": "2204.00498v1",
    "64": "2404.00599v1",
    "65": "2402.01030v4",
    "66": "2301.13816v4",
    "67": "2211.09374v1",
    "68": "2212.10481v2",
    "69": "1910.10683v4",
    "70": "2012.07805v2",
    "71": "2109.15102v2",
    "72": "2205.05638v2",
    "73": "2312.05934v3",
    "74": "2109.01652v5",
    "75": "2310.03003v1",
    "76": "2403.05530v5",
    "77": "2403.08295v4",
    "78": "2202.04538v2",
    "79": "2308.06463v2",
    "80": "2204.06745v1",
    "81": "2009.08366v4",
    "82": "2206.15000v3",
    "83": "2211.09110v2",
    "84": "2404.01954v2",
    "85": "2204.05999v3",
    "86": "2308.10792v10",
    "87": "2005.08025v2",
    "88": "2306.09896v5",
    "89": "2305.01210v3",
    "90": "2403.16792v3",
    "91": "2306.05685v4",
    "92": "2310.16218v4",
    "93": "2309.17446v2",
    "94": "2005.14165v4",
    "95": "2212.09420v2",
    "96": "2308.10620v6",
    "97": "2310.03533v4",
    "98": "1607.06450v1",
    "99": "1805.08949v1",
    "100": "2305.02309v2",
    "101": "2302.08468v3",
    "102": "2306.14893v1",
    "103": "2106.09685v2",
    "104": "1808.09588v1",
    "105": "1905.13319v1",
    "106": "2103.03874v2",
    "107": "2311.02303v1",
    "108": "1404.0417v3",
    "109": "2210.14868v3",
    "110": "2110.08207v3",
    "111": "2405.04520v1",
    "112": "2308.07124v2",
    "113": "2108.07258v3",
    "114": "2204.09653v1",
    "115": "2307.09288v2",
    "116": "2402.14658v3",
    "117": "2304.07327v2",
    "118": "2307.05532v1",
    "119": "2212.12017v3",
    "120": "1701.06538v1",
    "121": "2211.10435v2",
    "122": "2204.02311v5",
    "123": "2207.11280v1",
    "124": "1902.00751v2",
    "125": "2404.14219v4",
    "126": "2403.00833v1",
    "127": "2107.13586v1",
    "128": "1608.07754v5",
    "129": "2101.00190v1",
    "130": "2211.12588v4",
    "131": "1707.06347v2",
    "132": "2010.03150v1",
    "133": "2305.14314v1",
    "134": "2402.09739v3",
    "135": "2309.16609v1",
    "136": "2401.08406v3",
    "137": "2203.07722v1",
    "138": "2210.03629v3",
    "139": "2212.10264v1",
    "140": "2210.14306v5",
    "141": "2305.14992v2",
    "142": "2311.11690v1",
    "143": "2303.11366v4",
    "144": "2208.11640v3",
    "145": "2403.10059v2",
    "146": "2206.12839v3",
    "147": "2108.11601v2",
    "148": "2005.11401v4",
    "149": "2312.10997v5",
    "150": "2307.04349v2",
    "151": "2403.07506v1",
    "152": "2304.05302v3",
    "153": "1904.00935v1",
    "154": "2310.06770v3",
    "155": "2303.15647v2",
    "156": "2210.11416v5",
    "157": "2001.08361v1",
    "158": "2212.10560v2",
    "159": "2404.02183v1",
    "160": "1803.02155v2",
    "161": "2203.11171v4",
    "162": "2303.17651v2",
    "163": "2306.02907v1",
    "164": "2404.05019v3",
    "165": "2309.01219v3",
    "166": "2307.07221v3",
    "167": "1809.08887v5",
    "168": "2404.01226v1",
    "169": "2402.19173v1",
    "170": "2402.01391v2",
    "171": "2306.04556v1",
    "172": "2305.09235v2",
    "173": "2304.05128v2",
    "174": "2306.11644v2",
    "175": "2404.05520v3",
    "176": "2211.15533v1",
    "177": "1904.09751v2",
    "178": "2101.00027v1",
    "179": "2104.08691v2",
    "180": "2302.07080v1",
    "181": "2110.03215v4",
    "182": "2212.10403v2",
    "183": "2212.10403v2",
    "184": "2312.11658v2",
    "185": "2108.12409v2",
    "186": "2201.12901v1",
    "187": "2203.15556v1",
    "188": "2203.02155v1",
    "189": "2110.14168v2",
    "190": "2105.04144v1",
    "191": "2305.10601v2",
    "192": "1802.03691v3",
    "193": "2305.15377v1",
    "194": "2203.03850v1",
    "195": "2103.06333v2",
    "196": "2006.03511v3",
    "197": "2308.09932v2",
    "198": "2306.08568v2",
    "199": "2007.02220v3",
    "200": "2403.19270v2"
  }
}