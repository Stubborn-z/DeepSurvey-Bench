{
  "authors": [
    "Juyong Jiang",
    "Fan Wang",
    "Jiasi Shen",
    "Sungju Kim",
    "Sunghun Kim"
  ],
  "literature_review_title": "A Survey on Large Language Models for Code Generation",
  "year": "2024",
  "date": "2024-06-01",
  "category": "cs.CL",
  "abstract": "Large Language Models (LLMs) have garnered remarkable advancements across\ndiverse code-related tasks, known as Code LLMs, particularly in code generation\nthat generates source code with LLM from natural language descriptions. This\nburgeoning field has captured significant interest from both academic\nresearchers and industry professionals due to its practical significance in\nsoftware development, e.g., GitHub Copilot. Despite the active exploration of\nLLMs for a variety of code tasks, either from the perspective of natural\nlanguage processing (NLP) or software engineering (SE) or both, there is a\nnoticeable absence of a comprehensive and up-to-date literature review\ndedicated to LLM for code generation. In this survey, we aim to bridge this gap\nby providing a systematic literature review that serves as a valuable reference\nfor researchers investigating the cutting-edge progress in LLMs for code\ngeneration. We introduce a taxonomy to categorize and discuss the recent\ndevelopments in LLMs for code generation, covering aspects such as data\ncuration, latest advances, performance evaluation, ethical implications,\nenvironmental impact, and real-world applications. In addition, we present a\nhistorical overview of the evolution of LLMs for code generation and offer an\nempirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks\nacross various levels of difficulty and types of programming tasks to highlight\nthe progressive enhancements in LLM capabilities for code generation. We\nidentify critical challenges and promising opportunities regarding the gap\nbetween academia and practical development. Furthermore, we have established a\ndedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey)\nto continuously document and disseminate the most recent advances in the field.",
  "structure": [
    {
      "section_title": "Introduction/Pre-section",
      "level": "1",
      "content": "\\documentclass[acmsmall,screen]{acmart} % \\providecommand\\BibTeX{{% \\normalfont B\\kern-0.5em{\\scshape i\\kern-0.25em b\\kern-0.8em\\TeX}}} acmcopyright 2018 2018 XXXXXXX.XXXXXXX JACM 37 4 1 8 hyperref % hyperlinks amsfonts % blackboard math symbols nicefrac % compact symbols for 1/2, etc. microtype % microtypography lipsum graphicx amsmath bbding pifont wasysym color tikz \\usepackage[edges]{forest} hiddendraw{RGB}{205, 44, 36} hidden-blue{RGB}{194,232,247} hidden-orange{RGB}{243,202,120} hidden-yellow{RGB}{242,244,193} mybox=[ rectangle, draw=hiddendraw, rounded corners, text opacity=1, minimum height=1.5em, minimum width=5em, inner sep=2pt, align=center, fill opacity=.5, ] \\let\\Bbbk\\relax % resolve \\Bbbk conflicts with `amssymb` and `newtxmath` amssymb% http://ctan.org/pkg/amssymb pifont% http://ctan.org/pkg/pifont amsthm amsmath bbding multirow % merge rows subfigure definition definition{Definition}[section] \\cmark{51} \\xmark{55} mathtools colortbl color tikz \\usepackage[edges]{forest} enumitem wrapfig makecell \\smallsection[1]{\\noindent {\\bf #1}.1mm} \\ea{et~al.} document A Survey on Large Language Models for Code Generation Juyong Jiang jjiang472@connect.hkust-gz.edu.cn Equally major contributors. \\institution{The Hong Kong University of Science and Technology (Guangzhou) Guangzhou China} Fan Wang fwang380@connect.hkust-gz.edu.cn \\authornotemark[1] \\institution{The Hong Kong University of Science and Technology (Guangzhou) Guangzhou China} Jiasi Shen sjs@cse.ust.hk Corresponding authors. \\institution{The Hong Kong University of Science and Technology Hong Kong China} Sungju Kim sungju.kim@navercorp.com \\authornotemark[2] \\institution{NAVER Cloud % Hyperscale AI Seoul South Korea} Sunghun Kim hunkim@cse.ust.hk \\authornotemark[2] \\institution{The Hong Kong University of Science and Technology (Guangzhou) Guangzhou China} \\sjs[1]{{black #1}} \\kath[1]{{black #1}} \\revise[1]{{black #1}} \\done[1]{{black #1}} CCSXML <ccs2012> <concept> <concept_id>10002944.10011122.10002945</concept_id> <concept_desc>General and reference~Surveys and overviews</concept_desc> <concept_significance>500</concept_significance> </concept> <concept> <concept_id>10011007.10011074.10011092</concept_id> <concept_desc>Software and its engineering~Software development techniques</concept_desc> <concept_significance>500</concept_significance> </concept> <concept> <concept_id>10010147.10010178</concept_id> <concept_desc>Computing methodologies~Artificial intelligence</concept_desc> <concept_significance>500</concept_significance> </concept> </ccs2012> CCSXML \\ccsdesc[500]{General and reference~Surveys and overviews} \\ccsdesc[500]{Software and its engineering~Software development techniques} \\ccsdesc[500]{Computing methodologies~Artificial intelligence} Large Language Models, Code Large Language Models, Code Generation \\maketitle",
      "origin_cites_number": 0
    },
    {
      "section_title": "Introduction",
      "level": "1",
      "content": "The advent of Large Language Models (LLMs) such as ChatGPT\\href{https://chat.openai.com/{https://chat.openai.com}} gpt-3.5-turbo has profoundly transformed the landscape of automated code-related tasks chen2021evaluating, including code completion wang2021code,lu2022reacc,guo2023longcoder,wu2024repoformer, code translation lachaux2020unsupervised,szafraniec2022code,chen2018tree, and code repair olausson2023self,fan2023automated,joshi2023repair,parasaram2024fact,xu2024aligning,zhang2024pydex. A particularly intriguing application of LLMs is code generation, a task that involves producing source code from natural language descriptions. Despite varying definitions across studies ren2020codebleu,chen2023teaching,shojaee2023execution,wang2023codet5+, for the main scope of this survey, we focus on the code generation task and adopt a consistent definition of code generation as the natural-language-to-code (NL2Code) task \\cite{austin2021program,athiwaratkun2022multi,zan2023large.} To enhance clarity, the differentiation between code generation and other code-related tasks, along with a more nuanced definition, is summarized in Table \\ref{tab:code_tasks.} This area has garnered substantial interest from both academia and industry, as evidenced by the development of tools like GitHub Copilot\\href{https://github.com/features/copilot{https://github.com/features/copilot}} chen2021evaluating, CodeGeeX\\href{https://codegeex.cn/en-US{https://codegeex.cn/en-US}} zheng2023codegeex, and Amazon CodeWhisperer\\href{https://aws.amazon.com/codewhisperer{https://aws.amazon.com/codewhisperer}}, which leverage groundbreaking code LLMs to facilitate software development. Initial investigations into code generation primarily utilized heuristic rules or expert systems, such as probabilistic grammar-based frameworks joshi2003formalism,cohn2010inducing,allamanis2014mining,xiong2017precise,ji2020question and specialized language models de2008z3,gulwani2010dimensions,jha2010oracle. These early techniques were typically rigid and difficult to scale. However, the introduction of Transformer-based LLMs has shifted the paradigm, establishing them as the preferred method due to their superior proficiency and versatility. One remarkable aspect of LLMs is their capability to follow instructions wei2022emergent,ouyang2022training,xu2023wizardlm,muennighoff2023octopack,chung2024scaling, enabling even novice programmers to write code by simply articulating their requirements. This emergent ability has democratized coding, making it accessible to a broader audience zan2023large. The performance of LLMs on code generation tasks has seen remarkable improvements, as illustrated by the HumanEval leaderboard\\href{https://paperswithcode.com/sota/code-generation-on-humaneval{https://paperswithcode.com/sota/code-generation-on-humaneval}}, which showcases the evolution from PaLM 8B chowdhery2023palm of 3.6\\% to LDB zhong2024ldb of 95.1\\% on Pass@1 metrics. As can be seen, the HumanEval benchmark chen2021evaluating has been established as a de facto standard for evaluating the coding proficiency of LLMs chen2021evaluating. To offer a comprehensive chronological evolution, we present an overview of the development of LLMs for code generation, as illustrated in Figure fig:timeline. The landscape of LLMs for code generation is characterized by a spectrum of models, with certain models like ChatGPT ouyang2022training, GPT4 achiam2023gpt, LLaMA touvron2023llama,touvron2023llama2, and Claude 3 claude3 serving general-purpose applications, while others such as StarCoder li2023starcoder,lozhkov2024starcoder, Code LLaMA roziere2023code, DeepSeek-Coder guo2024deepseek, and Code Gemma codegemma_2024 are tailored specifically for code-centric tasks. The convergence of code generation with the latest LLM advancements is pivotal, especially when programming languages can be considered as distinct dialects of multilingual natural language athiwaratkun2022multi,zheng2023codegeex. These models are not only tested against software engineering (SE) requirements but also propel the advancement of LLMs into practical production zhang2023unifying. While recent surveys have shed light on code LLMs from the lenses of Natural Language Processing (NLP), Software Engineering (SE), or a combination of both disciplines \\cite{zan2023large,zheng2023survey,zhang2023unifying,fan2023large,hou2024large,lyu2024automatic, they have often encompassed a broad range of code-related tasks.} There remains a dearth of literature specifically reviewing advanced topics in code generation, such as meticulous data curation, instruction tuning, alignment with feedback, prompting techniques, the development of autonomous coding agents, retrieval augmented code generation, LLM-as-a-Judge for code generation, among others. A notably pertinent study athiwaratkun2022multi,zan2023large also concentrates on LLMs for text-to-code generation (NL2Code), yet it primarily examines models released from 2020 to 2022. Consequently, this noticeable temporal gap has resulted in an absence of up-to-date literature reviews that contemplate the latest advancements, including models like CodeQwen codeqwen, WizardCoder luo2023wizardcoder, CodeFusion singh2023codefusion, and PPOCoder shojaee2023execution, as well as the comprehensive exploration of the advanced topics previously mentioned. Recognizing the need for a dedicated and up-to-date literature review, this survey endeavors to fill that void. We provide a systematic review that will serve as a foundational reference for researchers quickly exploring the latest progress in LLMs for code generation. A taxonomy is introduced to categorize and examine recent advancements, encompassing data curation wang2023self,luo2023wizardcoder,wei2023magicoder, advanced topics parvez2021retrieval,lu2022reacc,le2022coderl,muennighoff2023octopack,liu2023rltf,chen2022codet,ni2023lever,chen2023teaching,huang2023agentcoder,shrivastava2023repofusion,zhang2023repocoder, evaluation methods chen2021evaluating,hendrycks2021measuring,jimenez2023swe,zhuo2024ice, and practical applications chen2021evaluating,zheng2023codegeex. This category aligns with the comprehensive lifecycle of an LLM for code generation. Furthermore, we pinpoint critical challenges and identify promising opportunities to bridge the research-practicality divide. Therefore, this survey allows NLP and SE researchers to seamlessly equip with a thorough understanding of LLM for code generation, highlighting cutting-edge directions and current hurdles and prospects. The remainder of the survey is organized following the structure outlined in our taxonomy in Figure fig:taxonomy. In Section sec:background, we introduce the preliminaries of LLM with Transformer architecture and formulate the task of LLM for code generation. Section \\ref{sec:methodology, we detail the systematic methodologies employed in conducting literature reviews.} Then, in Section sec:taxonomy, we propose a taxonomy, categorizing the complete process of LLMs in code generation. Section sec:overview delves into the specifics of LLMs for code generation within this taxonomy framework. In Section sec:challenges, we underscore the critical challenges and promising opportunities for bridging the research-practicality gap and conclude this work in Section sec:conclusion. figure*[p!] \\centering \\includegraphics[width=0.97\\linewidth]{images/codellm_timeline_v4.pdf} A chronological overview of large language models (LLMs) for code generation in recent years. The timeline was established mainly according to the release date. The models with publicly available model checkpoints are highlighted in green color. figure*",
      "origin_cites_number": 38
    },
    {
      "section_title": "Background",
      "level": "1",
      "content": "",
      "origin_cites_number": 0
    },
    {
      "section_title": "Large Language Models",
      "level": "2",
      "content": "The effectiveness of large language models (LLMs) is fundamentally attributed to their substantial quantity of model parameters, large-scale and diversified datasets, and the immense computational power utilized during training kaplan2020scaling,hoffmann2022training. Generally, scaling up language models consistently results in enhanced performance and sample efficiency across a broad array of downstream tasks wei2022emergent,zhao2023survey. However, with the expansion of the model size to a certain extent (e.g., GPT-3 brown2020language with 175B-parameters and PaLM chowdhery2023palm with 540B), LLMs have exhibited an unpredictable phenomenon known as emergent abilitiesIt should be noted that an LLM is not necessarily superior to a smaller language model, and emergent abilities may not manifest in all LLMs \\cite{zhao2023survey.}, including instruction following ouyang2022training, in-context learning dong2022survey, and step-by-step reasoning wei2022chain,huang2022towards, which are absent in smaller models but apparent in larger ones wei2022emergent. Adhering to the same architectures of the Transformer vaswani2017attention in LLMs, code LLMs are specifically pre-trained (or continually pre-trained on general LLMs) using large-scale unlabeled code corpora with a smaller portion of text (and math) data, whereas general-purpose LLMs are pre-trained primarily on large-scale text data, incorporating a smaller amount of code and math data to enhance logical reasoning capabilities. Additionally, some code LLMs, such as Qwen2.5-Coder \\cite{hui2024qwen2, incorporate synthetic data in their training processes, a practice that is attracting increasing attention from both industry and academia.} Analogous to LLMs, Code LLMs can also be classified into three architectural categories: encoder-only models, decoder-only models, and encoder-decoder models. For encoder-only models, such as CodeBERT feng2020codebert, they are typically suitable for code comprehension tasks including type prediction, code retrieval, and clone detection. For decoder-only models, such as StarCoder brown2020language, they predominantly excel in generation tasks, such as code generation, code translation, and code summarization. Encoder-decoder models, such as CodeT5 wang2021codet5, can accommodate both code understanding and generation tasks but do not necessarily outperform encoder-only or decoder-only models. The overall architectures of the different Code LLMs for code generation are depicted in Figure fig:architecture. In the following subsection, we will delineate the key modules of the Transformer layers in Code LLMs.",
      "origin_cites_number": 14
    },
    {
      "section_title": "Multi-Head Self-Attention Modules",
      "level": "3",
      "content": "Each Transformer layer incorporates a multi-head self-attention (MHSA) mechanism to discern the inherent semantic relationships within a sequence of tokens across $h$ distinct latent representation spaces. Formally, the MHSA employed by the Transformer can be formulated as follows: equation aligned h^{(l)}=MultiHeadSelfAttn(Q,K,V) =Concat\\left\\{Head_i\\right\\}_{i=1}^hW^O, aligned equation equation aligned Head_i =Attention(\\mathbf{H^{(l-1)}W_i^Q}_{Q},\\mathbf{H^{(l-1)}W_i^K}_{K}, \\mathbf{H^{(l-1)}W_i^V}_V), aligned equation equation aligned Attention(Q, K, V)=softmax\\left(\\mathbf{QK^T}{d_{model/h}}\\right)V, aligned equation where $H^{(l-1)} \\in R^{n\\times d_{model}}$ denotes the input to the $l$-th Transformer layer, while $h^{(l)} \\in R^{n\\times d_{model}}$ represents the output of MHSA sub-layer. The quantity of distinct attention heads is represented by $h$, and $d_{model}$ refers to the model dimension. The set of projections $\\left\\{\\mathbf{W_i^Q, W_i^K, W_i^V, W_i^O\\right\\} \\in R^{d_{model} \\times d_{model}/ h}$ encompasses the affine transformation parameters for each attention head $Head_i$, transforming the Query $Q$, Key $K$, Value $V$, and the output of the attention sub-layer.} The $softmax$ function is applied in a row-wise manner. The dot-products of queries and keys are divided by a scaling factor $d_{model/h}$ to counteract the potential risk of excessive large inner products and correspondingly diminished gradients in the $softmax$ function, thus encouraging a more balanced attention landscape. In addition to multi-head self-attention, there are two other types of attention based on the source of queries and key-value pairs: itemize \\item Masked Multi-Head Self-Attention. Within the decoder layers of the Transformer, the self-attention mechanism is constrained by introducing an attention mask, ensuring that queries at each position can only attend to all key-value pairs up to and inclusive of that position. To facilitate parallel training, this is typically executed by assigning a value of 0 to the lower triangular part and setting the remaining elements to $-\\infty$. Consequently, each item attends only to its predecessors and itself. Formally, this modification in Equation eq:attention can be depicted as follows: equation aligned Attention(Q, K, V)=softmax\\left(\\mathbf{QK^T}{d_{model/h}} + M_{mask} \\right)V, aligned equation equation aligned M_{mask} = \\Big(m_{ij}\\Big)_{n\\times n} = \\Big(I(i\\ge j)\\Big)_{n\\times n} = cases 0 & for $i \\ge j$ \\\\ -\\infty & otherwise cases, aligned equation This form of self-attention is commonly denoted as autoregressive or causal attention lin2022survey. \\item Cross-Layer Multi-Head Self-Attention. The queries are derived from the outputs of the preceding (decoder) layer, while the keys and values are projected from the outputs of the encoder. itemize",
      "origin_cites_number": 2
    },
    {
      "section_title": "Position-wise Feed-Forward Networks",
      "level": "3",
      "content": "Within each Transformer layer, a Position-wise Feed-Forward Network (PFFN) is leveraged following the MHSA sub-layer to refine the sequence embeddings at each position $i$ in a separate and identical manner, thereby encoding more intricate feature representations. The PFFN is composed of a pair of linear transformations, interspersed with a ReLU activation function. Formally, equation aligned PFFN(h^{(l)})=\\left(Concat\\left\\{FFN(h^{(l)}_i)^T\\right\\}_{i=1}^{n}\\right)^T, aligned equation equation aligned FFN(h^{(l)}_i)=ReLU(h^{(l)}_iW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}, aligned equation where $h^{(l)} \\in R^{n\\times d_{model}}$ is the outputs of MHSA sub-layer in $l$-th Transformer layer, and $h^{(l)}_i \\in R^{d_{model}}$ denotes the latent representation at each sequence position. The projection matrices $\\left\\{W^{(1)}, (W^{(2)})^T \\right\\} \\in R^{d_{model} \\times 4d_{model}}$ and bias vectors $\\{b^{(1)}, b^{(2)}\\} \\in R^{d_{model}}$ are parameters learned during training. These parameters remain consistent across all positions while are individually initialized from layer to layer. In this context, $T$ represents the transpose operation on a matrix.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Residual Connection and Normalization",
      "level": "3",
      "content": "To alleviate the issue of vanishing or exploding gradients resulting from network deepening, the Transformer model incorporates a residual connection he2016deep around each of the aforementioned modules, followed by Layer Normalization ba2016layer. For the placement of Layer Normalization operation, there are two widely used approaches: 1) Post-Norm: Layer normalization is implemented subsequent to the element-wise residual addition, in accordance with the vanilla Transformer vaswani2017attention. 2) Pre-Norm: Layer normalization is applied to the input of each sub-layer, as seen in models like GPT-2 radford2019language. Formally, it can be formulated as: equation aligned Post-Norm: H^{(l)} &=LayerNorm(PFFN(h^{(l)})+h^{(l)}),\\\\ h^{(l)}&=LayerNorm(MHSA(H^{(l-1)})+H^{(l-1)}) aligned equation equation aligned Pre-Norm: H^{(l)} &=PFFN(LayerNorm(h^{(l)}))+h^{(l)},\\\\ h^{(l)}&=MHSA(LayerNorm(H^{(l-1)}))+H^{(l-1)} aligned equation",
      "origin_cites_number": 4
    },
    {
      "section_title": "Positional Encoding",
      "level": "3",
      "content": "Given that self-attention alone cannot discern the positional information of each input token, the vanilla Transformer introduces an absolute positional encoding method to supplement this positional information, known as sinusoidal position embeddings vaswani2017attention. Specifically, for a token at position $pos$, the position embedding is defined as: equation aligned p_{pos,2i}=\\sin(pos{10000^{2i/d_{model}}}), aligned equation equation aligned p_{pos,2i+1}=\\cos(pos{10000^{2i/d_{model}}}), aligned equation where $2i, 2i+1$ represent the dimensions of the position embedding, while $d_{model}$ denotes the model dimension. Subsequently, each position embedding is added to the corresponding token embedding, and the sum is fed into the Transformer. Since the inception of this method, a variety of innovative positional encoding approaches have emerged, such as learnable embeddings devlin2018bert, relative position embeddings shaw2018self, RoPE su2024roformer, and ALiBi press2021train. For more detailed descriptions of each method, please consult lin2022survey,zhao2023length.",
      "origin_cites_number": 6
    },
    {
      "section_title": "\\done{Architecture",
      "level": "3",
      "content": "} There are two types of Transformer architecture for code generation task, including encoder-decoder and decoder-only. For the encoder-decoder architecture, it consists of both an encoder and a decoder, in which the encoder processes the input data and generates a set of representations, which are then used by the decoder to produce the output. However, for decoder-only architecture, it consists only of the decoder part of the transformer, where it uses a single stack of layers to both process input data and generate output. Therefore, the encoder-decoder architecture is suited for tasks requiring mapping between different input and output domains, while the decoder-only architecture is designed for tasks focused on sequence generation and continuation. The overview of LLMs with these two architectures are illustrated in Figure \\ref{fig:architecture. }",
      "origin_cites_number": 0
    },
    {
      "section_title": "Code Generation",
      "level": "2",
      "content": "Large language models (LLMs) for code generation refer to the use of LLM to generate source code from natural language descriptions, a process also known as a natural-language-to-code task. Typically, these natural language descriptions encompass programming problem statements (or docstrings) and may optionally include some programming context (e.g., function signatures, assertions, etc.). Formally, these natural language (NL) descriptions can be represented as $x$. Given $x$, the use of an LLM with model parameters $\\theta$ to generate a code solution $y$ can be denoted as $P_{\\theta}(y\\midx)$. The advent of in-context learning abilities in LLM wei2022emergent has led to the appending of exemplars to the natural language description $x$ as demonstrations to enhance code generation performance or constrain the generation format li2023towards,patel2023evaluating. A fixed set of $M$ exemplars is denoted as $\\{(x_i, y_i)\\}_{i=1}^M$. Consequently, following ni2023lever, a more general formulation of LLMs for code generation with few-shot (or zero-shot) exemplars can be revised as: equation aligned P_\\theta(y\\midx) \\Rightarrow P_\\theta(y\\midprompt(x, \\{(x_i, y_i)\\}_{i=1}^k)), k\\in\\{0, 1, \\dots, M\\} aligned equation where $prompt(x, \\{(x_i, y_i)\\}_{i=1}^k))$ is a string representation of the overall input, and $\\{(x_i, y_i)\\}_{i=1}^k$ denotes a set of $k$ exemplars randomly selected from $\\{(x_i, y_i)\\}_{i=1}^M$. In particular, when $k=0$, this denotes zero-shot code generation, equivalent to vanilla ones without in-context learning. In the decoding process, a variety of decoding strategies can be performed for code generation, including deterministic-based strategies (e.g., greedy search and beam search) and sampling-based strategies (e.g., temperature sampling, top-k sampling, and top-p (nucleus) sampling). For more detailed descriptions of each decoding strategy, please consult holtzman2019curious. For example, the greedy search and sampling-based decoding strategies can be formulated as follows: equation aligned Greedy Search: y^* = \\mathrm{argmax}_y P_\\theta(y\\midprompt(x, \\{(x_i, y_i)\\}_{i=1}^k)), k\\in\\{0, 1, \\dots, M\\} aligned equation equation aligned Sampling: y \\sim P_\\theta(y\\midprompt(x, \\{(x_i, y_i)\\}_{i=1}^k)), k\\in\\{0, 1, \\dots, M\\} aligned equation To verify the functionality correctness of the generated code solution, $\\mathbf{y$ is subsequently executed via a compiler or interpreter, represented as $Exe(\\cdot)$, on a suit of unit tests $\\mathcal{T}$. The feedback from this execution can be denoted as $Feedback(Exe(y,\\mathcal{T}))$. If the generated code solution fails to pass all test cases, the error feedback can be iteratively utilized to refine the code by leveraging the previous attempt ($y_{pre}$) and the associated feedback. Formally, equation aligned y \\sim P_\\theta(y\\midprompt(x, \\{(x_i, y_i)\\}_{i=1}^k, y_{pre}, Feedback(Exe(y,\\mathcal{T})))), k\\in\\{0, 1, \\dots, M\\} aligned equation Further details and relevant studies on using feedback to improve code generation are comprehensively discussed in Section sec:reinforcement_learning and sec:prompting. } \\done{",
      "origin_cites_number": 4
    },
    {
      "section_title": "Methodology",
      "level": "1",
      "content": "In this section, we detail the systematic methodologies employed in conducting literature reviews. We follow the systematic literature review methodology outlined by kitchenham2009systematic, which has been widely adopted in numerous software engineering literature reviews hou2024large,li2017static,liu2022deep,ramirez2018systematic,wang2022machine. The overall process is illustrated in Figure fig:review_process, and the detailed steps in our methodology are documented below.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Research Questions",
      "level": "2",
      "content": "To deliver a comprehensive and up-to-date literature review on the latest advancements in large language models (LLMs) for code generation, this systematic literature review addresses the following research questions (RQs): RQ1: How can we categorize and evaluate the latest advances in LLMs for code generation? The recent proliferation of LLMs has resulted in many of these models being adapted for code generation task. While the adaptation of LLMs for code generation essentially follows the evolution of LLMs, this evolution encompasses a broad spectrum of research directions and advancements. For software engineering (SE) researchers, it can be challenging and time-consuming to fully grasp the comprehensive research landscape of LLMs and their adaptation to code generation. RQ1 aims to propose a taxonomy that serves as a comprehensive reference for researchers, enabling them to quickly familiarize themselves with the state-of-the-art in this dynamic field and identify specific research problems and directions of interest. RQ2: What are the key insights into LLMs for code generation? RQ2 seeks to assist researchers in establishing a comprehensive, up-to-date, and advanced understanding of LLMs for code generation. This includes discussing various aspects of this rapidly evolving domain, such as data curation, latest advancements, performance evaluation, ethical and environmental implications, and real-world applications. A historical overview of the evolution of LLMs for code generation is provided, along with an empirical comparison using the widely recognized HumanEval and MBPP benchmarks, as well as the more practical and challenging BigCodeBench benchmark, to highlight the progressive enhancements in LLM capabilities for code generation. RQ2 offers an in-depth analysis of critical insights related to LLMs for code generation. RQ3: What are the critical challenges and promising research opportunities in LLMs for code generation? Despite the revolutionary impact of LLMs on the paradigm of code generation and their remarkable performance, numerous challenges remain unaddressed. These challenges primarily stem from the gap between academic research and practical development. For instance, while the HumanEval benchmark is established as a de facto standard for evaluating the coding proficiency of LLMs in academia, it has been shown that this evaluation does not adequately reflect practical development scenarios jimenez2023swe,du2024evaluating,liu2024your,ding2024crosscodeeval. RQ3 aims to identify critical challenges and highlight promising opportunities to bridge the gap between research and practical application.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Search Process",
      "level": "2",
      "content": "",
      "origin_cites_number": 0
    },
    {
      "section_title": "Search Strings",
      "level": "3",
      "content": "To address the aforementioned three research questions (RQs), we initiate a manual review of conference proceedings and journal articles from top-tier venues in the fields of LLMs and SE, as detailed in Table tab:venues. This process allowed us to identify relevant studies and derive search strings, which are subsequently utilized for an automated search across various scientific databases. The complete set of search keywords is presented in Table tab:keywords.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Search Databases",
      "level": "3",
      "content": "Following the development of search strings, we executed an automated search using four popular scientific databases: the ACM Digital Library, IEEE Xplore Digital Library, arXiv, and DBLP. Our search focus on identifying papers whose titles contain keywords pertinent to LLMs and code generation. This approach enhances the likelihood of retrieving relevant papers since both sets of keywords must be present in the title. Although this title-based search strategy effectively retrieves a large volume of papers, it is important to note that in some instances shojaee2023execution, the scope of code generation can be broader, encompassing areas such as code completion, code translation, and program synthesis. As outlined in Section sec:introduction, this survey adopts a prevalent definition of code generation as the natural-language-to-code (NL2Code) task austin2021program,athiwaratkun2022multi,zan2023large. Consequently, we conduct further automatic filtering based on the content of the papers. Papers focusing on ``code completion'' and ``code translation'' are excluded unless they pertain to the specific topics discussed in Section sec:repository_level and Section sec:retrieval_augmented, where code completion is a primary focus. After completing the automated search, the results from each database are merged and deduplicated using scripts. This process yields 294 papers from arXiv, 73 papers from the ACM Digital Library, 36 papers from IEEE Xplore, and 261 papers from DBLP.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Inclusion and Exclusion Criteria",
      "level": "2",
      "content": "The search process conducted across various databases and venues is intentionally broad to gather a comprehensive pool of candidate papers. This approach maximizes the collection of potentially relevant studies. However, such inclusivity may lead to the inclusion of papers that do not align with the scope of this survey, as well as duplicate entries from multiple sources. To address this, we have established a clear set of inclusion and exclusion criteria, based on the guidelines from hou2024large,wang2024software. These criteria are applied to each paper to ensure alignment with our research scope and questions, and to eliminate irrelevant studies. Inclusion Criteria. A paper will be included if it meets any of the following criteria: itemize \\item It is available in full text. \\item It presents a dataset or benchmark specifically designed for code generation with LLMs. \\item It explores specific LLM techniques, such as pre-training or instruction tuning, for code generation. \\item It provides an empirical study or evaluation related to the use of LLMs for code generation. \\item It discusses the ethical considerations and environmental impact of deploying LLMs for code generation. \\item It proposes tools or applications powered by LLMs for code generation. itemize Exclusion Criteria. Conversely, papers will be excluded if they meet any of the following conditions: itemize \\item They are not written in English. \\item They are found in books, theses, monographs, keynotes, panels, or venues (excluding arXiv) that do not undergo a full peer-review process. \\item They are duplicate papers or different versions of similar studies by the same authors. \\item They focus on text generation rather than source code generation, such as generating code comments, questions, test cases, or summarization. \\item They do not address the task of code generation, for instance, focusing on code translation instead. \\item They leverage software engineering methods to enhance code generation without emphasizing LLMs. \\item They do not utilize LLMs, opting for other models like Long Short-Term Memory (LSTM) networks. \\item They use encoder-only language models, such as BERT, which are not directly applicable to code generation task. \\item LLMs are mentioned only in future work or discussions without being central to the proposed approach. itemize Papers identified through both manual and automated searches undergo a detailed manual review to ensure they meet the inclusion criteria and do not fall under the exclusion criteria. Specifically, the first two authors independently review each paper to determine its eligibility. In cases of disagreement, the third author makes the final inclusion decision.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Quality Assessment",
      "level": "2",
      "content": "To ensure the inclusion of high-quality studies, we have developed a comprehensive set of ten Quality Assessment Criteria (QAC) following hou2024large. These QAC are designed to evaluate the relevance, clarity, validity, and significance of the papers considered for our review. In accordance with hou2024large, the first three QAC assess the studyâ€™s alignment with our objectives. These criteria are rated as ``irrelevant/unmet'', ``partially relevant/met'', or ``relevant/fully met'', corresponding to scores of -1, 0, and 1, respectively. If a study receive a score of -1 across these initial three criteria, it is deemed ineligible for further consideration and subsequently excluded from our review process. The subsequent seven QAC focus on a more detailed content evaluation, employing a scoring range of -1 to 2, representing ``poor'', ``fair'', ``good'', and ``excellent''. We compute a cumulative score based on the responses to QAC4 through QAC10 for each paper. For published works, the maximum achievable score is 14 (2 points per question). We retain those with a score of 11.2 (80\\% of the total score) or higher. For unpublished papers available on arXiv, QAC4 defaults to a score of 0, making the maximum possible score for the remaining criteria 12. Accordingly, we retain papers scoring 9.6 (80\\% of the adjusted total score) or above . itemize \\item QAC1: Is the research not classified as a secondary study, such as a systematic literature review or survey? (-1, 0, 1) \\item QAC2: Does the study incorporate the use of LLMs? (-1, 0, 1) \\item QAC3: Is the study relevant to the code generation task? (-1, 0, 1) \\item QAC4: Is the research published in a prestigious venue? (-1, 0, 1, 2) \\item QAC5: Does the study present a clear research motivation? (-1, 0, 1, 2) \\item QAC6: Are the key contributions and limitations of the study discussed? (-1, 0, 1, 2) \\item QAC7: Does the study contribute to the academic or industrial community? (-1, 0, 1, 2) \\item QAC8: Are the LLM techniques employed in the study clearly described? (-1, 0, 1, 2) \\item QAC9: Are the experimental setups, including experimental environments and dataset information, thoroughly detailed? (-1, 0, 1, 2) \\item QAC10: Does the study clearly confirm its experimental findings? (-1, 0, 1, 2) itemize",
      "origin_cites_number": 2
    },
    {
      "section_title": "Snowballing Search",
      "level": "2",
      "content": "Following the quality assessment, we establish an initial set of papers for our study. To minimize the risk of excluding pertinent literature, we implement a snowballing search strategy. Snowballing search involves utilizing a paper's reference list or its citations to discover additional relevant studies, known as backward and forward snowballing, respectively. In this survey, we exclusively employed backward snowballing following wang2024software. Despite this effort, no additional studies are identified through this method. This could be attributed to the task-specific nature of the code generation (natural-language-to-code), where reference studies are typically published earlier. Consequently, our methodology, which encompassed an extensive manual and automated search, likely covered the relevant literature comprehensively, explaining the lack of additional studies through snowballing search.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Data Collection and Analysis",
      "level": "2",
      "content": "The data collection process for our study, illustrated in Figure fig:review_process, began with a manual search through conference proceedings and journal articles from leading venues in LLMs and SE. This initial step yielded 42 papers, from which we extracted relevant search strings. Following this, we performed an automated search across four academic databases using keyword-based queries, resulting in the retrieval of 664 papers. After performing automatic filtering (351 papers), applying inclusion and exclusion criteria (247 papers), conducting quality assessments (235 papers), and utilizing snowballing search (235 papers), we finalize a collection of 235 papers focusing on LLMs for code generation. To provide insights from the selected papers, we begin by presenting an overview of their distribution across publication venues each year, as illustrated at the top of Figure fig:venues_num_topic_dist. Our analysis indicates that 14\\% of the papers are published in LLM-specific venues and 7\\% in SE venues. Remarkably, 49\\% of the papers remain unpublished in peer-reviewed venues and are available on arXiv. This trend is understandable given the emerging nature of this field, with many works being recent and pending formal submission. Despite the absence of peer review on arXiv, our quality assessment process ensures that only high-quality papers are included, thereby maintaining the integrity of this survey. Furthermore, the annual trend in the number of collected papers indicates nearly exponential growth in the field. From a single paper in the period 2018 to 2020, the numbers increased to 6 in 2021, 11 in 2022, 75 in 2023, and 140 in 2024. This trend reflects growing interest and attention in this research area, with expectations for continued expansion in the future. Additionally, to capture the breadth of advancements in LLMs for code generation, we conducted a distribution analysis of the research topics covered in the included papers, as shown at the bottom of Figure fig:venues_num_topic_dist. We observe that the development of LLMs for code generation closely aligns with broader trends in general-purpose LLM research. Notably, the most prevalent research topics are Pre-training and Foundation Models (21.5\\%), Prompting (11.8\\%), and Evaluation and Benchmarks (24.1\\%). These areas hold significant promise for enhancing, refining, and evaluating LLM-driven code generation. }",
      "origin_cites_number": 0
    },
    {
      "section_title": "Taxonomy",
      "level": "1",
      "content": "The recent surge in the development of LLMs has led to a significant number of these models being repurposed for code generation task through continual pre-training or fine-tuning. This trend is particularly observable in the realm of open-source models. For instance, Meta AI initially made the LLaMA touvron2023llama model publicly available, which was followed by the release of Code Llama roziere2023code, designed specifically for code generation. Similarly, DeepSeek LLM bi2024deepseek developed and released by DeepSeek has been extended to create DeepSeek Coder guo2024deepseek, a variant tailored for code generation. The Qwen team has developed and released Code Qwen codeqwen, building on their original Qwen bai2023qwen model. Microsoft, on the other hand, has unveiled WizardLM xu2023wizardlm and is exploring its coding-oriented counterpart, WizardCoder luo2023wizardcoder. Google has joined the fray by releasing Gemma team2024gemma, subsequently followed by Code Gemma codegemma_2024. Beyond simply adapting general-purpose LLMs for code-related tasks, there has been a proliferation of models specifically engineered for code generation. Notable examples include StarCoder li2023starcoder, OctoCoder muennighoff2023octopack, and CodeGen nijkamp2022codegen. These models underscore the trend of LLMs being developed with a focus on code generation. Recognizing the importance of these developments, we conduct a thorough analysis of selected papers on LLMs for code generation, sourced from widely used scientific databases as mentioned in Section \\ref{sec:methodology. Based on this analysis, we propose a taxonomy that categorizes and evaluates the latest advancements in LLMs for code generation.} This taxonomy, depicted in Figure fig:taxonomy, serves as a comprehensive reference for researchers seeking to quickly familiarize themselves with the state-of-the-art in this dynamic field. It is important to highlight that the category of recent advances emphasizes the core techniques used in the current state-of-the-art code LLMs. In the subsequent sections, we will provide an in-depth analysis of each category related to code generation. This will encompass a definition of the problem, the challenges to be addressed, and a comparison of the most prominent models and their performance evaluation. line-color{RGB}{0, 119, 182} fill-color{RGB}{114, 200, 222} category=[ rectangle, draw=line-color, rounded corners, text opacity=1, minimum height=1.5em, minimum width=5em, inner sep=2pt, align=center, fill opacity=.5, ] leaf=[category,minimum height=1em, fill=fill-color!40, text width=20em, text=black,align=left,font=\\tiny, inner xsep=2pt, inner ysep=1pt, ] figure*[tp] \\centering 0.97{ forest forked edges, for tree={ grow=east, reversed=true,%increase counter-clockwise anchor=base west, parent anchor=east, child anchor=west, base=left, font=\\small, rectangle, draw=line-color, rounded corners,align=left, minimum width=2.5em, s sep=3pt, inner xsep=2pt, inner ysep=1pt, ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center}, }, where level=1{text width=3.2em,font=\\scriptsize,}{}, where level=2{text width=3.8em,font=\\tiny}{}, where level=3{text width=4.0em,font=\\tiny}{},%yshift=0.26pt where level=4{text width=4.2em,font=\\tiny}{},%yshift=0.26pt [LLMs for Code Generation, ver [Data \\\\Curation \\\\(Sec. sec:data_curation) [Pre-training [CodeSearchNethusain2019codesearchnet{,} Google BigQueryhoffa2016github{,} The Pilegao2020pile{,} CodeParrottunstall2022natural{,} GitHub Codetunstall2022natural\\\\ ROOTSlaurenccon2022bigscience{,} The Stackkocetkov2022stack{,} The Stack v2lozhkov2024starcoder ,leaf,text width=24.5em] ] [Instruction \\\\Tuning [CommitPackFT muennighoff2023octopack{,} Code Alpacacodealpaca{,} OA-Leetoa-leet10k{,} OSS-Instructwei2023magicoder{,} Evol-instructionevol_instruction\\\\ Self-OSS-Instruct-SC2-Exec-Filterstarcoder2instruct ,leaf,text width=24.5em] ] [Benchmarks [General [HumanEvalchen2021evaluating{,} HumanEval+liu2024your{,} HumanEvalPackmuennighoff2023octopack{,} MBPPaustin2021program\\\\ MBPP+liu2024your{,} CoNaLayin2018learning{,} Spideryu2018spider{,} CONCODEiyer2018mapping{,} ODEXwang2022execution\\\\ CoderEvalyu2024codereval{,} ReCodewang2022recode{,} StudentEvalbabe2023studenteval ,leaf,text width=19em] ] [Competitions [APPShendrycks2021measuring{,} CodeContestsli2022competition ,leaf,text width=19em] ] [Data Science [DSPchandel2022training{,} DS-1000lai2023ds{,} ExeDShuang2022execution ,leaf,text width=19em] ] [Multilingual [MBXPathiwaratkun2022multi{,} Multilingual HumanEvalathiwaratkun2022multi{,} HumanEval-Xzheng2023codegeex{,} MultiPL-Ecassano2022scalable\\\\ xCodeEvalkhan2023xcodeeval ,leaf,text width=19em] ] [Reasoning [MathQA-Xathiwaratkun2022multi{,} MathQA-Pythonaustin2021program{,} GSM8Kcobbe2021training{,} GSM-HARDgao2023pal ,leaf,text width=19em] ] [Repository [RepoEvalzhang2023repocoder{,} Stack-Reposhrivastava2023repofusion{,} Repobenchliu2023repobench{,} EvoCodeBenchli2024evocodebench\\\\ SWE-benchjimenez2023swe{,} CrossCodeEvalding2024crosscodeeval{,} SketchEvalzan2024codes ,leaf,text width=19em] ] ] ] [Recent \\\\Advances [Data \\\\Synthesis \\\\(Sec. sec:data_synthesis) [Self-Instruct wang2023self{,} Evol-Instruct xu2023wizardlm{,} Phi-1gunasekar2023textbooks{,} Code Alpacacodealpaca{,} WizardCoderluo2023wizardcoder\\\\ Magicoderwei2023magicoder{,} StarCoder2-instruct starcoder2instruct ,leaf,text width=24.5em] ] [Pre-training \\\\(Sec. sec:pre-training) [Model \\\\Architectures [Encoder-Decoder [PyMT5clement2020pymt5{,} PLBARTahmad2021unified{,} CodeT5wang2021codet5{,} JuPyT5chandel2022training\\\\ AlphaCodeli2022competition{,} CodeRLle2022coderl{,} ERNIE-Codechai2022ernie\\\\ PPOCodershojaee2023execution{,} CodeT5+wang2023codet5+{,} CodeFusionsingh2023codefusion\\\\ AST-T5gong2024ast ,leaf,text width=13em] ] [Decoder-Only [GPT-Csvyatkovskiy2020intellicode{,} GPT-Neogpt-neo{,} GPT-Jgpt-j{,} Codexchen2021evaluating\\\\ CodeGPTlu2021codexglue{,} CodeParrottunstall2022natural{,} PolyCoderxu2022systematic\\\\ CodeGennijkamp2022codegen{,} GPT-NeoXblack2022gpt{,} PaLM-Coderchowdhery2023palm\\\\ InCoderfried2022incoder{,} PanGu-Coderchristopoulou2022pangu{,} PyCodeGPTzan2022cert\\\\ CodeGeeXzheng2023codegeex{,} BLOOMle2023bloom{,} ChatGPTgpt-3.5-turbo\\\\ SantaCoderallal2023santacoder{,} LLaMAtouvron2023llama{,} GPT-4achiam2023gpt\\\\ CodeGen2nijkamp2023codegen2{,} replit-codereplit-code{,} StarCoderli2023starcoder\\\\ WizardCoderluo2023wizardcoder{,} phi-1gunasekar2023textbooks{,} ChainCoderzheng2023outline\\\\ CodeGeeX2zheng2023codegeex{,} PanGu-Coder2shen2023pangu{,} Llama 2touvron2023llama2\\\\ OctoPackmuennighoff2023octopack{,} Code Llamaroziere2023code{,} MFTCoderliu2023mftcoder\\\\ phi-1.5li2023textbooks{,} CodeShellxie2024codeshell{,} Magicoderwei2023magicoder\\\\ AlphaCode 2alphacode2{,} StableCodepinnaparaju2024stable{,} WaveCoderyu2023wavecoder\\\\ phi-2phi-2{,} DeepSeek-Coderguo2024deepseek{,} StepCoderdou2024stepcoder\\\\ OpenCodeInterpreterzheng2024opencodeinterpreter{,} StarCoder 2lozhkov2024starcoder\\\\ Claude 3claude3{,} ProCoderbi2024iterative{,} CodeGemmacodegemma_2024\\\\ CodeQwencodeqwen{,} Llama3llama3\\\\ StarCoder2-Instructstarcoder2instruct{,} Codestralcodestral ,leaf,text width=13em] ] ] [Pre-training \\\\Tasks [CLMli2023starcoder,luo2023wizardcoder,wei2023magicoder,guo2024deepseek{,} DAEahmad2021unified,wang2021codet5,wang2023codet5+{,} Auxiliarywang2021codet5,chai2022ernie,wang2023codet5+ ,leaf,text width=16.5em] ] ] [Fine-tuning [Instruction \\\\Tuning \\\\(Sec. sec:instruction_tuning) [Full Parameter\\\\ Fine-tuning [Code Alpacacodealpaca{,} CodeT5+wang2021codet5{,} WizardCoderluo2023wizardcoder\\\\ StarCoderli2023starcoder{,} Pangu-Coder2shen2023pangu{,} OctoPackmuennighoff2023octopack\\\\ CodeGeeX2zheng2023codegeex{,} Magicoderwei2023magicoder{,} CodeGemmacodegemma_2024\\\\ StarCoder2-instructstarcoder2instruct ,leaf,text width=13em] ] [Parameter \\\\Efficient \\\\Fine-tuning [CodeUpcodeup{,} ASTRAIOSzhuo2024astraios ,leaf,text width=8em] ] ] [Reinforcement \\\\Learning \\\\with Feedback \\\\(Sec. sec:reinforcement_learning) [CodeRLle2022coderl{,} CompCoderwang2022compilable{,} PPOCodershojaee2023execution{,} RLTFliu2023rltf\\\\ PanGu-Coder2shen2023pangu{,} StepCoderdou2024stepcoder ,leaf,text width=15em] ] ] [Prompting \\\\Engineering \\\\(Sec. sec:prompting) [Reflexionshinn2024reflexion{,} LATSzhou2023language{,} Self-Debuggingchen2023teaching{,} SelfEvolvejiang2023selfevolve\\\\ Theo X. et al.olausson2023self{,} CodeTchen2022codet{,} LEVERni2023lever{,} AlphaCodiumridnik2024code ,leaf,text width=17em] ] [Repository\\\\ Level \\& Long \\\\Context \\\\(Sec. sec:repository_level) [RepoCoderzhang2023repocoder{,} CoCoMICding2022cocomic{,} RepoHyperphan2024repohyper{,} RLPGshrivastava2023repository\\\\ Repoformerwu2024repoformer{,} RepoFusionshrivastava2023repofusion{,} ToolGenwang2024teaching{,} CodePlanbairi2023codeplan\\\\ CodeSzan2024codes ,leaf,text width=17em] ] [Retrieval \\\\Augmented \\\\(Sec. sec:retrieval_augmented) [HGNNliu2020retrieval{,} REDCODERparvez2021retrieval{,} ReACClu2022reacc{,} DocPromptingzhou2022docprompting\\\\ RepoCoderzhang2023repocoder{,} Su et al.su2024arks ,leaf,text width=17em] ] [Autonomous \\\\Coding Agents \\\\(Sec. sec:autonomous_agents) [AgentCoder huang2023agentcoder{,} MetaGPThong2023metagpt{,} CodeAct wang2024executable{,} AutoCodeRover zhang2024autocoderover{,} DevinDevin\\\\ OpenDevinOpenDevin{,} SWE-agentswe-agent{,} L2MACholt2023l2mac{,} OpenDevin CodeAct 1.0OpenDevin_CodeAct ,leaf,text width=22em] ] ] [Evaluation \\\\(Sec. sec:evaluation) [Metrics [Exact Match{,} BLEUpapineni2002bleu{,} ROUGElin2004rouge{,} METEORbanerjee2005meteor{,} CodeBLEUren2020codebleu{,} pass@kchen2021evaluating\\\\ n@kli2022competition{,} test case averagehendrycks2021measuring{,} execution accuracyrajkumar2022evaluating{,} pass@tolausson2023self{,} perplexityjelinek1977perplexity ,leaf,text width=22em] ] [Human \\\\Evaluation [CodePlanbairi2023codeplan{,} RepoFusionshrivastava2023repofusion{,} CodeBLEUren2020codebleu ,leaf,text width=13em] ] [LLM-as-a-Judge [AlpacaEvalalpaca_eval{,} MT-benchzheng2024judging{,} ICE-Scorezhuo2024ice ,leaf,text width=13em] ] ] [black{Code LLMs} \\\\black{Alignment} \\\\(Sec. sec:grest_llm4code) [black{Greenshi2024greening,wei2023towards{,}} black{Responsibilityliu2023uncovering,xu2024first{,}} black{Efficiencyyang2024robustness{,}} black{Safetyschuster2021you,hajipour2024codelmsec,yang2024unveiling,al2024traces,yuan2023gpt,fried2022incoder,allal2023santacoder{,}} black{Trustworthinessji2023benchmarking,palacio2023evaluating} ,leaf,text width=29.8em] ] [Application \\\\(Sec. sec:application) [GitHub Copilotchen2021evaluating{,} CodeGeeXzheng2023codegeex{,} CodeWhispererCodeWhisperer{,} CodeiumCodeium{,} CodeArts Snapshen2023pangu{,} TabNineTabNine{,} ReplitReplit ,leaf,text width=29.8em] ] ] forest } Taxonomy of \\done{LLMs for code generation.} figure*",
      "origin_cites_number": 212
    },
    {
      "section_title": "Large Langauge Models for Code Generation",
      "level": "1",
      "content": "LLMs with Transformer architecture have revolutionized a multitude of fields, and their application in code generation has been particularly impactful. These models follow a comprehensive process that starts with the curation and synthesis of code data, followed by a structured training approach that includes pre-training and fine-tuning (instruction tuning), reinforcement learning with various feedback, and the use of sophisticated prompt engineering techniques. Recent advancements have seen the integration of repository-level and retrieval-augmented code generation, as well as the development of autonomous coding agents. Furthermore, the evaluation of coding abilities of LLMs has become a critical component of this research area. Figure \\ref{fig:codellm_workflow illustrates the general training, inference, and evaluation workflow for Code LLMs and their associated databases. } In the forthcoming sections, we will explore these dimensions of LLMs in the context of code generation in detail. Section sec:data_curation will address the data curation and processing strategies employed throughout the various stages of LLM development. Section sec:data_synthesis will discuss data synthesis methods designed to mitigate the scarcity of high-quality data. Section sec:pre-training will outline the prevalent model architectures used in LLMs for code generation. Moving to Section sec:instruction_tuning, we will examine the techniques for full parameter fine-tuning and parameter-efficient fine-tuning, which are essential for tailoring LLMs to code generation task. Section sec:reinforcement_learning will shed light on enhancing code quality through reinforcement learning, utilizing the power of feedback. Section sec:prompting will delve into the strategic use of prompts to maximize the coding capabilities of LLMs. The innovative approaches of repository-level and retrieval-augmented code generation will be elaborated in Sections sec:repository_level and sec:retrieval_augmented, respectively. Additionally, Section sec:autonomous_agents will discuss the exciting field of autonomous coding agents. Section \\ref{sec:evaluation discusses various evaluation strategies and offer an empirical comparison using the widely recognized HumanEval, MBPP, and the more practical and challenging BigCodeBench benchmarks to highlight the progressive enhancements in LLM capabilities for code generation.} Furthermore, the ethical implications and the environmental impact of using LLMs for code generation are discussed in Section \\ref{sec:responsible_codeai, aiming to establish a trustworthiness, responsibility, safety, efficiency, and green of LLM for code generation.} Lastly, Section sec:application will provide insights into some of the practical applications that leverage LLMs for code generation, demonstrating the real-world impact of these sophisticated models. Through this comprehensive exploration, we aim to highlight the significance and potential of LLMs within the domain of automated code generation.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Data Curation \\& Processing",
      "level": "2",
      "content": "The exceptional performance of LLMs can be attributed to their training on large-scale and diverse datasets zan2023large. Meanwhile, the extensive parameters of these models necessitate substantial data to unlock their full potential, in alignment with established scaling law kaplan2020scaling,hoffmann2022training. For a general-purpose LLM, amassing a large-scale corpus of natural language from a variety of sources is imperative. Such sources include webpages, conversation data, books and news, scientific data, and code brown2020language,chowdhery2023palm,bai2023qwen,touvron2023llama,touvron2023llama2,yoo2024hyperclova, while these data are often crawled from the web and must undergo meticulous and aggressive pre-processing raffel2020exploring,zhang2023unifying. Fortunately, multiple platforms and websites offer large-scale, open-source, and permissively licensed code corpora, such as GitHub\\href{https://github.com{https://github.com}} and Stack Overflow\\href{https://stackoverflow.com{https://stackoverflow.com}}. Notably, the number of stars or forks of GitHub repositories has emerged as a valuable metric for filtering high-quality code datasets. In a similar vein, the quantity of votes on Stack Overflow can serve to discern the most relevant and superior answers. Nonetheless, raw datasets are frequently laden with redundant, noisy data and personal information, eliciting concerns regarding privacy leakage, which may include the names and email addresses of repository contributors carlini2021extracting,laurenccon2022bigscience,al2024traces. Consequently, it is essential to undertake rigorous data-cleaning procedures. Typically, this process encompasses exact match deduplication, code data filtering based on average line length and a defined threshold for the fraction of alphanumeric characters, the removal of auto-generated files through keyword searches, and the expunction of personal user data tunstall2022natural,kocetkov2022stack. Specifically, the standard data preprocessing workflow is depicted in Figure fig:datapipeline. The development of a proficient LLM for code generation necessitates the utilization of various types of code data at different developmental stages. Therefore, we categorize code data into three distinct classes: pre-training datasets, instruction-tuning datasets, and benchmarks for performance evaluation. The subsequent subsections will provide a detailed illustration of code data within each classification. figure*[t] \\centering \\includegraphics[width=\\linewidth]{images/DataProcessing.pdf} A diagram depicting the standard data preprocessing workflow utilized in the pre-training phase of \\done{LLMs for code generation.} figure*",
      "origin_cites_number": 6
    },
    {
      "section_title": "Pre-training",
      "level": "3",
      "content": "The remarkable success of bidirectional pre-trained language models (PLMs) such as BERT devlin2018bert and unidirectional PLMs like GPT radford2018improving has firmly established the practice of pre-training on large-scale unlabeled datasets to endow models with a broad spectrum of general knowledge. Extending this principle to the realm of code generation enables LLMs to assimilate fundamental coding principles, including the understanding of code structure dependencies, the semantics of code identifiers, and the intrinsic logic of code sequences chen2021evaluating,wang2021codet5,guo2022unixcoder,wang2023codet5+. In light of this advancement, there has been a proliferation of large-scale unlabeled code datasets proposed to serve as the foundational training ground for LLMs to develop coding proficiency. A brief introduction of these datasets is as follows, with the statistics available in Table tab:pretraining_dataset. itemize \\item CodeSearchNet husain2019codesearchnet: CodeSearchNet corpus is a comprehensive dataset, consisting of 2 million (comment, code) pairs from open-source repositories on GitHub. It includes code and documentation in several programming languages including Go, Java, PHP, Python, JavaScript, and Ruby. The dataset was primarily compiled to promote research into the problem of code retrieval using natural language. \\item Google BigQuery hoffa2016github: the Google BigQuery Public Datasets program offers a full snapshot of the content of more than 2.8 million open source GitHub repositories in BigQuery. \\item The Pile gao2020pile: the Pile is an 825 GiB diverse and open source language modeling dataset aggregating 22 smaller, high-quality datasets including GitHub, Books3, and Wikipedia (en). It aims to encompass text from as many modalities as possible, thereby facilitating the development of models with broader generalization capabilities. For code generation, the GitHub composite is specifically utilized. \\item CodeParrot tunstall2022natural: the CodeParrot dataset contains Python files used to train the code generation model in Chapter 10: Training Transformers from Scratch in the ``NLP with Transformers book'' tunstall2022natural. Created with the GitHub dataset available via Google's BigQuery, the CodeParrot dataset includes approximately 22 million Python files and is 180 GB (50 GB compressed) big. \\item GitHub Code tunstall2022natural: the GitHub Code dataset comprises 115M code files derived from GitHub, spanning 32 programming languages and 60 extensions totaling 1TB of data. The dataset was created from the public GitHub dataset on Google BiqQuery. \\item ROOTS laurenccon2022bigscience: the BigScience ROOTS Corpus is a 1.6TB dataset spanning 59 languages that was used to train the 176B BigScience Large Open-science Open-access Multilingual (BLOOM) language model. For the code generation task, the code subset of the ROOTS Corpus will be specifically utilized. \\item The Stack kocetkov2022stack: the Stack contains over 6TB of permissively licensed source code files that cover 358 programming languages. The dataset was compiled as part of the BigCode Project, an open scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs). \\item The Stack v2 lozhkov2024starcoder: The Stack v2, a dataset created as part of the BigCode Project, contains over 3B files across more than 600 programming and markup languages. The dataset is derived from the Software Heritage archive\\href{https://archive.softwareheritage.org/{https://archive.softwareheritage.org}}, the largest public archive of software source code and accompanying development history. itemize",
      "origin_cites_number": 23
    },
    {
      "section_title": "Instruction Tuning",
      "level": "3",
      "content": "Instruction tuning refers to the process of supervised fine-tuning \\done{LLMs using a collection of datasets structured as various instructions, with the purpose of following a wide range of task instructions wei2021finetuned,sanh2022multitask,ouyang2022training,chung2024scaling. } This method has demonstrated a considerable improvement in model performance and an enhanced ability to generalize to unseen tasks that the model has not previously encountered, as evidenced by recent studies ouyang2022training,chung2024scaling. Leveraging the benefits of instruction tuning, instruction tuning has been expanded into coding domains, especially for code generation, which involves the automatic generation of the intended code from a natural language description. The promise of instruction tuning in this area has led numerous researchers to develop large-scale instruction-tuning datasets tailored for code generation. Below, we provide an overview of several notable datasets tailored for instruction tuning, with their respective statistics detailed in Table tab:instruction_dataset. itemize \\item CodeAlpaca-20k codealpaca: CodeAlpaca-20k is a collection of 20K instruction-following data generated using the data synthesis techniques termed Self-Instruct outlined in wang2023self, with modifications for code generation, editing, and optimization tasks instead of general tasks. \\item CommitPackFT muennighoff2023octopack: CommitPackFT is a 2GB refined version of CommitPack. It is filtered to only include high-quality commit messages that resemble natural language instructions. \\item Evol-Instruct-Code-80k evol_instruction: Evol-Instruct-Code-80k is an open-source implementation of Evol-Instruct-Code described in the WizardCoder paper luo2023wizardcoder, which enhances the fine-tuning effect of pre-trained code large models by adding complex code instructions. \\item Magicoder-OSS-Instruct-75k wei2023magicoder: is a 75k synthetic data generated through OSS-Instruct with gpt-3.5-turbo-1106 and used to train both Magicoder and Magicoder-S series models. \\item Self-OSS-Instruct-SC2-Exec-Filter-50k starcoder2instruct: Self-OSS-Instruct-SC2-Exec-Filter-50k is generated by StarCoder2-15B using the OSS-Instruct wei2023magicoder data synthesis approach. It was subsequently used to fine-tune StarCoder-15B without any human annotations or distilled data from huge and proprietary LLMs. itemize",
      "origin_cites_number": 16
    },
    {
      "section_title": "Benchmarks",
      "level": "3",
      "content": "To rigorously assess the efficacy of LLMs for code generation, the research community has introduced a variety of high-quality benchmarks in recent years. Building on the foundational work by chen2021evaluating, numerous variations of the HumanEval dataset and additional benchmarks have emerged, aiming to evaluate a broader spectrum of code generation capabilities in LLMs. We roughly divide these benchmarks into six distinct categories based on their application contexts, including general-purpose, competitive programming, data science, multilingual, logical reasoning, and repository-level. It is important to highlight that logical reasoning encompasses math-related benchmarks, as it aims to create ``code-based solutions'' for solving complex mathematical problems \\cite{chen2022program,gao2023pal,zhou2023solving. This strategy can therefore mitigate the limitations of LLMs in performing intricate mathematical computations. } The statistics for these benchmarks are presented in Table tab:benchmark. General itemize \\item HumanEval chen2021evaluating: HumanEval comprises 164 manually scripted Python programming problems, each featuring a function signature, docstring, body, and multiple unit tests. \\item HumanEval+ liu2024your: HumanEval+ extends the original HumanEval chen2021evaluating benchmark by increasing the scale of the test cases by 80 times. As the test cases increase, HumanEval+ can catch significant amounts of previously undetected incorrect code synthesized by LLMs. \\item HumanEvalPack muennighoff2023octopack: expands HumanEval chen2021evaluating by extending it to encompass three coding tasks across six programming languages, namely code synthesis, code repair, and code explanation. \\item MBPP austin2021program: MBPP is a collection of approximately 974 Python programming problems, crowd-sourced and designed for entry-level programmers. Each problem comes with an English task description, a code solution, and three automated test cases. \\item MBPP+ liu2024your: MBPP+ enhances MBPP austin2021program by eliminating ill-formed problems and rectifying problems with incorrect implementations. The test scale of MBPP+ is also expanded by 35 times for test augmentation. \\item CoNaLa yin2018learning: CoNaLa contains almost 597K data samples for evaluating Python code generation. The curated part of CoNaLa is crawled from Stack Overflow, automatically filtered, and then curated by annotators. The mined part of CoNaLais automatically mined, with almost 600k examples. \\item Spider yu2018spider: Spider is large-scale complex text-to-SQL dataset covering 138 different domains. It has over 10K questions and 5.6K complex SQL queries on 200 databases. This dataset aims to test a model's ability to generalize to SQL queries, database schemas, and new domains. \\item CONCODE iyer2018mapping: CONCODE is a dataset with over 100K samples consisting of Java classes from public GitHub repositories. It provides near zero-shot conditions that can test the model's ability to generalize to unseen natural language tokens with unseen environments. \\item ODEX wang2022execution: ODEX is an open-domain dataset focused on the execution-based generation of Python code from natural language. It features 945 pairs of natural language queries and their corresponding Python code, all extracted from StackOverflow forums. \\item CoderEval yu2024codereval: CoderEval is a pragmatic code generation benchmark that includes 230 Python and 230 Java code generation problems. It can be used to evaluate the model performance in generating pragmatic code beyond just generating standalone functions. \\item ReCode wang2022recode: Recode serves as a comprehensive robustness evaluation benchmark. ReCode applies perturbations to docstrings, function and variable names, code syntax, and code format, thereby providing multifaceted assessments of a model's robustness performance. \\item StudentEval babe2023studenteval: StudentEval is a dataset of 1,749 prompts for 48 problems, authored by 80 students who have only completed a one-semester Python programming class. Unlike many other benchmarks, it has multiple prompts per problem and multiple attempts by the same participant, each problem is also accompanied by a set of instructor-written test cases. \\item BigCodeBench \\cite{zhuo2024bigcodebench: BigCodeBench has 1,140 complex Python programming tasks, covering 723 function calls from 139 popular libraries across 7 domains. This benchmark is specifically designed to assess LLMs' ability to call multiple functions from cross-domain libraries and follow complex instructions to solve programming tasks, helping to bridge the evaluation gap between isolated coding exercises and the real-world programming scenario.} \\item ClassEval \\cite{du2024evaluating: ClassEval is a manually-crafted benchmark consisting of 100 classes and 412 methods for evaluating LLMs in the class-level code generation scenario. Particularly, the task samples of ClassEval present higher complexities, involving long code generation and sophisticated docstring information, thereby benefiting the evaluation of the LLMs' capabilities in generating complicated code.} \\item NaturalCodeBench \\cite{zhang2024naturalcodebench: NaturalCodeBench is a comprehensive code benchmark featuring 402 high-quality problems in Python and Java. These problems are selected from natural user queries from online coding services and span 6 distinct domains, shaping an evaluation environment aligned with real-world applications.} itemize Competitions itemize \\item APPS hendrycks2021measuring: The APPS benchmark is composed of 10K Python problems, spanning three levels of difficulty: introductory, interview, and competition. Each entry in the dataset includes a programming problem described in English, corresponding ground truth Python solutions, test cases defined by their inputs and outputs or function names if provided. \\item CodeContests li2022competition: CodeContests is a competitive programming dataset consisting of samples from various sources including Aizu, AtCoder, CodeChef, Codeforces, and HackerEarth. The dataset encompasses programming problems accompanied by test cases in the form of paired inputs and outputs, along with both correct and incorrect human solutions in multiple programming languages. \\item LiveCodeBench \\cite{naman2024livecodebench: LiveCodeBench is a comprehensive and contamination-free benchmark for evaluating a wide array of code-related capabilities of LLMs, including code generation, self-repair, code execution, and test output prediction. It continuously gathers new coding problems from contests across three reputable competition platforms: LeetCode, AtCoder, and CodeForces. The latest release of the dataset includes 713 problems that were released between May 2023 and September 2024. } itemize Data Science itemize \\item DSP chandel2022training: DSP allows for model evaluation based on real data science pedagogical notebooks. It includes well-structured problems, along with unit tests to verify the correctness of solutions and a Docker environment for reproducible execution. \\item DS-1000 lai2023ds: DS-1000 has 1K science questions from seven Python libraries, namely NumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, and Matplotlib. The DS-1000 benchmark features: (1) realistic problems with diverse contexts (2) implementation of multi-criteria evaluation metrics, and (3) defense against memorization. \\item ExeDS huang2022execution: ExeDS is a data science code generation dataset specifically designed for execution evaluation. It contains 534 problems with execution outputs from Jupyter Notebooks, as well as 123K examples for training and validation. itemize Multilingual itemize \\item MBXP athiwaratkun2022multi: MBXP is a multilingual adaptation of the original MBPP austin2021program dataset. It is created using a framework that translates prompts and test cases from the original Python datasets into the corresponding data in the targeted programming language. \\item Multilingual HumanEval athiwaratkun2022multi: Multilingual HumanEval is a dataset derived from HumanEval chen2021evaluating. It is designed to assess the performance of models in a multilingual context. It helps uncover the generalization ability of the given model on languages that are out-of-domain. \\item HumanEval-X zheng2023codegeex: HumanEval-X is developed for evaluating the multilingual ability of code generation models with 820 hand-writing data samples in C++, Java, JavaScript, and Go. \\item MultiPL-E cassano2022scalable: MultiPL-E is a dataset for evaluating LLMs for code generation across 18 programming languages. It adopts the HumanEval chen2021evaluating and the MBPP austin2021program Python benchmarks and uses little compilers to translate them to other languages. \\item xCodeEval khan2023xcodeeval: xCodeEval is an executable multilingual multitask benchmark consisting of 25M examples covering 17 programming languages. Its tasks include code understanding, generation, translation, and retrieval. itemize Reasoning itemize \\item MathQA-X athiwaratkun2022multi MathQA-X is the multilingual version of MathQA amini2019mathqa. It is generated by utilizing a conversion framework that converts samples from Python datasets into the target language. \\item MathQA-Python austin2021program MathQA-Python is a Python version of the MathQA benchmarkamini2019mathqa. The benchmark, containing more than 23K problems, is designed to assess the capability of models to synthesize code from complex textual descriptions. \\item GSM8K cobbe2021training: GSM8K is a dataset of 8.5K linguistically diverse grade school math problems. The dataset is crafted to facilitate the task of question answering on basic mathematical problems that requires multi-step reasoning. \\item GSM-HARD gao2023pal: GSM-HARD is a more challenging version of the GSM8K cobbe2021training dataset. It replaces the numbers in the GSM8K questions with larger, less common numbers, thereby increasing the complexity and difficulty level of the problems. \\item CRUXEval \\cite{gu2024cruxeval: CRUXEval contains 800 Python functions, each paired with an input-output example. This benchmark supports two tasks: input prediction and output prediction, designed to evaluate the code reasoning, understanding, and execution capabilities of code LLMs.} itemize Repository itemize \\item RepoEval zhang2023repocoder: RepoEval enables the evaluation of repository-level code completion. It can offer different levels of granularity and improved evaluation accuracy through the use of unit tests. \\item Stack-Repo shrivastava2023repofusion: Stack-Repo is a dataset of 200 Java repositories from GitHub with near-deduplicated files. These files are augmented with three types of repository contexts: prompt proposal contexts, BM25 Contexts (based on BM25 similarity scores), and RandomNN Contexts (obtained using the nearest neighbors in the representation space of an embedding model). \\item Repobench liu2023repobench: Repobench is a benchmark specifically used for evaluating repository-level code auto-completion systems. Supporting both Python and Java, it consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). \\item EvoCodeBench li2024evocodebench: EvoCodeBench is an evolutionary code generation benchmark, constructed through a rigorous pipeline and aligned with real-world repositories. This benchmark also provides comprehensive annotations and robust evaluation metrics. \\item SWE-bench jimenez2023swe: SWE-bench is a dataset that tests a modelâ€™s ability to automatically solve GitHub issues. The dataset has 2,294 Issue-Pull Request pairs from 12 popular Python repositories. \\item CrossCodeEval ding2024crosscodeeval: CrossCodeEval is a diverse and multilingual scope completion dataset covering four languages: Python, Java, TypeScript, and C\\#. This benchmark tests the model's ability to understand in-depth cross-file information and accurately complete the code. \\item SketchEval zan2024codes: SketchEval is a repository-oriented benchmark that encompasses data from 19 repositories, each varying in complexity. In addition to the dataset, SketchEval introduces a metric, known as SketchBLEU, to measure the similarity between two repositories based on their structures and semantics. itemize",
      "origin_cites_number": 88
    },
    {
      "section_title": "Data Synthesis",
      "level": "2",
      "content": "figure*[t] \\centering \\includegraphics[width=\\linewidth]{images/data_synthesis_v2.pdf} \\done{The comparison among three representative data synthesis methods used for generating instruction data with LLMs. The Code Alpaca \\cite{codealpaca employs the self-instruct method, whereas WizardCoder luo2023wizardcoder and Magicoder wei2023magicoder utilize the Evol-Instruct and OSS-Instruct methods, respectively.}} figure* Numerous studies have demonstrated that high-quality datasets are integral to enhancing the performance of LLMs in various downstream tasks brown2020language,meng2022generating,xie2023data,zhou2024lima,kopf2024openassistant,wettig2024qurating. For instance, the LIMA model, a 65B parameter LLaMa language model fine-tuned with a standard supervised loss on a mere 1,000 meticulously curated prompts and responses, achieved performance on par with, or even superior to, GPT-4 in 43\\% of evaluated cases. This figure rose to 58\\% when compared to Bard and 65\\% against DaVinci003, all without the use of reinforcement learning or human preference modeling zhou2024lima. The QuRating initiative strategically selects pre-training data embodying four key textual qualities â€” writing style, facts \\& trivia, required expertise, and educational value â€” that resonate with human intuition. Training a 1.3B parameter model on such data resulted in reduced perplexity and stronger in-context learning compared to baseline models wettig2024qurating. Despite these advancements, acquiring quality data remains a significant challenge due to issues such as data scarcity, privacy concerns, and prohibitive costs wang2023self,liu2024best. Human-generated data is often labor-intensive and expensive to produce, and it may lack the necessary scope and detail to navigate complex, rare, or ambiguous scenarios. As a resolution to these challenges, synthetic data has emerged as a viable alternative. By generating artificial datasets that replicate the intricacies of real-world information, models such as GPT-3.5-turbo gpt-3.5-turbo and GPT-4 achiam2023gpt have enabled the creation of rich datasets without the need for human annotation wang2023self,hamalainen2023evaluating,liu2024best,moritzlaurer. This approach is particularly beneficial in enhancing the instruction-following capabilities of LLMs, with a focus on generating synthetic instruction-based data. A notable example of this approach is the Self-Instruct wang2023self framework, which employs an off-the-shelf language model to generate a suite of instructions, inputs, and outputs. This data is then refined by removing invalid or redundant entries before being used to fine-tune the model. The empirical evidence supports the efficacy of this synthetic data generation methodology. Building upon this concept, the Alpaca alpaca model, fine-tuned on 52k pieces of instruction-following data from a 7B parameter LLaMa touvron2023llama model, exhibits performance comparable to the text-davinci-003 model. WizardLM xu2023wizardlm introduced the Evol-Instruct technique, which incrementally transforms simple instructions into more complex variants. The fine-tuned LLaMa model using this technique has shown promising results in comparison to established proprietary LLMs such as ChatGPT gpt-3.5-turbo and GPT-4 achiam2023gpt, to some extent. Moreover, Microsoft has contributed to this field with their Phi series of models, predominantly trained on synthetic high-quality data, which includes Phi-1 (1.3B) gunasekar2023textbooks for Python coding, Phi-1.5 (1.3B) li2023textbooks for common sense reasoning and language understanding, Phi-2 (2.7B) phi-2 for advanced reasoning and language understanding, and Phi-3 (3.8B) abdin2024phi for general purposes. These models have consistently outperformed larger counterparts across various benchmarks, demonstrating the efficacy of synthetic data in model training. Drawing on the successes of data synthesis for general-purpose LLMs, researchers have expanded the application of synthetic data to the realm of code generation. The Code Alpaca model, as described in codealpaca, has been fine-tuned on a 7B and 13B LLaMA model using a dataset of 20k instruction-following examples for code generation. This dataset was created by text-davinci-003\\href{https://platform.openai.com/docs/deprecations{https://platform.openai.com}} and employed the Self-Instruct technique wang2023self. Building on this, the WizardCoder 15B luo2023wizardcoder utilizes the Evol-Instruct technique to create an enhanced dataset of 78k evolved code instruction examples. This dataset originates from the initial 20k instruction-following dataset used by Code Alpaca codealpaca, which was also generated by text-davinci-003. The WizardCoder model, fine-tuned on the StarCoder li2023starcoder base model, achieved a 57.3\\% pass@1 on the HumanEval benchmarks. This performance not only surpasses all other open-source Code LLMs by a significant margin but also outperforms leading closed LLMs such as Anthropicâ€™s Claude and Googleâ€™s Bard. In a similar vein, Magicoder wei2023magicoder introduces a novel data synthesis approach termed OSS-INSTRUCT which enlightens LLMs with open-source code snippets to generate high-quality instruction data for coding tasks. It aims to address the inherent biases often present in synthetic data produced by LLMs. Building upon CodeLlama roziere2023code, the MagicoderS-CL-7B model â€” fine-tuned with 75k synthetic instruction data using the OSS-INSTRUCT technique and with gpt-3.5-turbo-1106 as the data generator â€” has outperformed the prominent ChatGPT on the HumanEval Plus benchmark, achieving pass@1 of 66.5\\% versus 65.9\\%. In a noteworthy development, Microsoft has introduced the phi-1 model gunasekar2023textbooks, a more compact LLM of only 1.3B parameters. Despite its smaller size, phi-1 has been trained on high-quality textbook data sourced from the web (comprising 6 billion tokens) and supplemented with synthetic textbooks and exercises generated with GPT-3.5 (1 billion tokens). It has achieved pass@1 of 50.6\\% on HumanEval and 55.5\\% on MBPP, setting a new state-of-the-art for Python coding performance among existing small language models (SLMs). The latest contribution to this field is from the BigCode team, which has presented StarCoder2-15B-instruct starcoder2instruct, the first entirely self-aligned code LLM trained with a transparent and permissive pipeline. This model aligns closely with the OSS-INSTRUCT principles established by Magicoder, generating instructions based on seed functions filtered from the Stack v1 dataset kocetkov2022stack and producing responses through self-validation. Unlike Magicoder, StarCoder2-15B-instruct employs its base model, StarCoder2-15B, as the data generator, thus avoiding reliance on large and proprietary LLMs like GPT-3.5-turbo gpt-3.5-turbo. Figure \\ref{fig:data_synthesis illustrates the comparison between Self-Instruct, Evol-Instruct, and OSS-Instruct data synthesis methods.} While synthetic data has demonstrated its potential across both small- and large-scale LMs for a variety of general and specialized tasks, including code generation, it also poses several challenges that must be addressed. These challenges include a lack of data diversity wettig2024qurating, the need to ensure the factuality and fidelity of the information wood2021fake,van2023synthetic, and the potential to amplify existing biases or introduce new ones barbierato2022methodology,gupta2021transitioning.",
      "origin_cites_number": 34
    },
    {
      "section_title": "Pre-Training",
      "level": "2",
      "content": "",
      "origin_cites_number": 0
    },
    {
      "section_title": "Model Architectures",
      "level": "3",
      "content": "Since the inception of the Transformer architecture for machine translation vaswani2017attention, it has become the de facto backbone for a multitude of LLMs that address a wide range of downstream tasks. The Transformer and its derivatives owe their prominence to their exceptional ability to parallelize computation and their powerful representational capacities zhao2023survey,yoo2024hyperclova. Through innovative scaling techniques, such as Mixture-of-Experts (MoE) shazeer2017outrageously,cai2024shortcut and Depth-Up-Scaling (DUS) kim2023solar, the capacity of Transformer-based LLMs has expanded to encompass hundreds of billions or even trillions of parameters. These scaled-up models have exhibited a range of emergent abilities kaplan2020scaling,hoffmann2022training,wei2022emergent, such as instruction following ouyang2022training, in-context learning dong2022survey, and step-by-step reasoning wei2022chain,huang2022towards that were previously unforeseen. In the domain of code generation using LLMs, the architecture of contemporary models generally falls into one of two categories: encoder-decoder models, such as CodeT5 wang2021codet5, CodeT5+ wang2023codet5+, and CodeRL le2022coderl; or decoder-only models, such as Codex chen2021evaluating, StarCoder li2023starcoder, Code Llama roziere2023code, and CodeGemma codegemma_2024. These architectures are depicted in Figure fig:architecture(b) and (c), respectively. For a comprehensive overview, Table tab:encoder_decoder_models details the encoder-decoder architectures, while Table tab:decoder_only_models focuses on the decoder-only models utilized in code generation.",
      "origin_cites_number": 71
    },
    {
      "section_title": "Pre-training Tasks",
      "level": "3",
      "content": "In the initial phase, language models for code generation are typically trained from scratch using datasets consisting of manually annotated pairs of natural language descriptions and corresponding code snippets, within a supervised learning framework. However, manual annotation is not only laborious and time-consuming, but the efficacy of the resulting models is also constrained by both the volume and the quality of the available annotated data. This limitation is especially pronounced in the context of low-resource programming languages, such as Swahili and Yoruba, where annotated examples are scarce chen2022transferability,cassano2023knowledge. In light of these challenges, there has been a shift towards an alternative training strategy that involves pre-training models on extensive and unlabelled code corpora. This method is aimed at imbuing the models with a broad understanding of programming knowledge, encompassing elements like identifiers, code structure, and underlying semantics chen2021evaluating. In this regard, two pre-training tasks have gained prominence for their effectiveness, namely Causal Language Modeling (CLM), also known as unidirectional language modeling or next-token prediction, and Denoising Autoencoding (DAE). The CLM task can be applied to both decoder-only and encoder-decoder model architectures, while DAE tasks are specifically designed for encoder-decoder frameworks. It should also be noted that there is a variety of additional auxiliary pre-training tasks that can further enhance model performance. These include Masked Identifier Prediction, Identifier Tagging, Bimodal Dual Generation wang2021codet5, Text-Code Matching, and Text-Code Contrastive Learning wang2023codet5+. These tasks contribute to a more nuanced and comprehensive pre-training process, equipping the models with the capabilities necessary to handle a wide range of code generation scenarios. Causal Language Modeling. In decoder-only LLMs, given a sequence of tokens $x=\\{x_1,\\dots,x_n\\}$, the CLM task refers to autoregressively predict the target tokens $x_i$ based on the preceding tokens $x_{<i}$ in a sequence. The causal language modeling objective for training decoder LLMs is to minimize the following likelihood: equation aligned L_{CLM}^{Decoder-only}(x)=-\\log(\\prod_{i=1}^n P_{\\theta}(x_i\\midx_{<i}))=\\sum_{i=1}^n -\\log P_{\\theta}(x_i\\midx_{<i}) aligned equation where $x_{<i}$ represents the sequence of preceding tokens $\\{x_1,\\dots,x_{i-1}\\}$ before $x_{i}$ in the input, $\\theta$ denotes the model parameters. The conditional probability $P_{\\theta}(x_i|x_{<i}))$ is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block. To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to $-\\infty$, ensuring that each token $x_i$ attends only to its predecessors and itself. On the contrary, in encoder-decoder LLMs, a pivot token $x_k$ is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence $x_{in}=\\{x_1,\\dots,x_k\\}$ of the encoder and the sequence after it as the target output $x_{out}=\\{x_{k+1},\\dots,x_n\\}$ of decoder. Formally, the causal language modeling objective for training encoder-decoder LLMs is to minimize loss function as follows: equation aligned L_{CLM}^{Encoder-Decoder}(x)=-\\log(\\prod_{i=k+1}^n P_{\\theta}(x_i\\midx_{\\leq k}, x_{<i}))=\\sum_{i=k+1}^n -\\log P_{\\theta}(x_i\\midx_{\\leq k}, x_{<i}) aligned equation where $x_{\\leq k}$ is the source sequence input and $x_{<i}$ denotes the target sequence autoregressively generated so far. During the inference phase, pre-trained LLMs that have been trained on large-scale code corpus can generate code in a zero-shot manner without the need for fine-tuning. This is achieved through the technique of prompt engineering, which guides the model to produce the desired outputFor more information on prompt engineering, visit \\href{https://www.promptingguide.ai{https://www.promptingguide.ai}} radford2019language,brown2020language. Additionally, recent studies have explored the use of few-shot learning, also referred to as in-context learning, to enhance model performance further li2023towards,patel2023evaluating. Denoising Autoencoding. In addition to causal language modeling (CLM), the denoising autoencoding (DAE) task has been extensively applied in pre-training encoder-decoder architectures for code generation, such as PLBART ahmad2021unified, CodeT5 wang2021codet5, and its enhanced successor, CodeT5+ wang2023codet5+. Following T5 raffel2020exploring and CodeT5 wang2021codet5, the DAE refers to initially perturbing the source sequence by introducing randomly masked spans of varying lengths. This corrupted sequence serves as the input for the encoder. Subsequently, the decoder employs an autoregressive strategy to reconstruct the masked spans, integrating sentinel tokens to facilitate the generation process. This method has proven effective in improving the model's ability to generate semantically and syntactically accurate code by learning robust contextual representations wang2021codet5,wang2023codet5+. Formally, the denoising autoencoding objective for training encoder-decoder LLMs is to minimize the following likelihood: equation L_{DAE}^{Encoder-Decoder}(x)= \\sum_{i=1}^k -\\log P_\\theta(x_{i}^{masked\\_spans}\\midx^{\\backslash masked\\_spans}, x_{<i}^{masked\\_spans}) equation where $\\theta$ denotes the model parameters, $x^{\\backslash masked\\_spans}$ is the noisy input with masked spans, $x^{masked\\_spans}$ is the masked spans to predict from the decoder with $k$ denoting the number of tokens in $x^{masked\\_spans}$, and $x_{<i}^{masked\\_spans}$ is the span sequence autoregressively generated so far. Compared with CLM, the DAE task presents a more challenging scenario, as it necessitates a deeper understanding and capture of the intrinsic semantic relationships among token sequences by LLMs raffel2020exploring. figure*[t] \\centering \\includegraphics[width=\\linewidth]{images/various_ft_v6.pdf} \\done{Comparison of instruction tuning with various fine-tuning strategies and prompting for code tasks, adapted from \\cite{wei2021finetuned. For (A), which involves training a Transformer from scratch, please refer to ahmad2020transformer for its use in source code summarization task. In the case of (E), we utilize a representative RLHF ouyang2022training as an example. Additional reinforcement learning methods, such as DPO rafailov2024direct, are also applicable at this stage. }} figure*",
      "origin_cites_number": 17
    },
    {
      "section_title": "Instruction Tuning",
      "level": "2",
      "content": "figure*[t] \\centering \\includegraphics[width=\\linewidth]{images/Instruction_Exemplar_v2.pdf} Two exemplars of instruction data sampled from Code Alpaca \\cite{codealpaca used to instruction-tune pre-trained code LLM to enhance their alignment with natural language instructions. The instruction corpus encompasses a variety of tasks, each accompanied by distinct instructions, such as prime numbers generation and URLs extraction.} figure* After pre-training LLMs on large-scale datasets, the next phase typically involves augmenting the model's ability to process and follow various instructions, known as instruction tuning. Instruction tuning generally refers to the supervised fine-tuning of pre-trained LLMs using datasets comprised of structured examples framed as various natural language instructions \\cite{wei2021finetuned,ouyang2022training,iyer2022opt,zhang2023instruction. The comparison of instruction tuning with various fine-tuning strategies and prompting for code tasks is depicted in Figure fig:various_ft. } Two exemplars of instruction data sampled from Code Alpaca codealpaca are demonstrated in Figure fig:instruction. It capitalizes on the heterogeneity of instruction types, positioning instruction tuning as a form of multi-task prompted training that significantly enhances the model's generalization to unseen tasks wei2021finetuned,sanh2022multitask,ouyang2022training,chung2024scaling. In the realm of code generation, natural language descriptions serve as the instructions guiding the model to generate corresponding code snippets. Consequently, a line of research on instruction tuning LLMs for code generation has garnered substantial interest across academia and industry. To perform instruction tuning, instruction data are typically compiled from source code with permissive licenses husain2019codesearchnet,kocetkov2022stack,lozhkov2024starcoder (refer to Section sec:instruction_data) or are constructed from synthetic code data luo2023wizardcoder,wei2023magicoder,starcoder2instruct (refer to Section sec:data_synthesis). These datasets are then utilized to fine-tune LLMs through a supervised learning paradigm. However, the substantial computational resources required for full parameter fine-tuning (FFT) LLM pose a notable challenge, particularly in scenarios with constrained resources ding2022delta,lialin2023scaling. To mitigate this issue, parameter-efficient fine-tuning (PEFT) has emerged as a compelling alternative strategy, gaining increasing attention for its potential to reduce resource consumption ding2022delta. In the following subsection, we categorize existing works based on their instruction-tuning strategies to provide a comprehensive and systematic review.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Full Parameter Fine-tuning",
      "level": "3",
      "content": "Full parameter fine-tuning (FFT) involves updating all parameters within a pre-trained model, as shown in Figure fig:finetune(a). This approach is often preferred when ample computational resources and substantial training data are available, as it typically leads to better performance. wang2021codet5 introduces an encoder-decoder pre-trained language model for code generation, named CodeT5+. They instruction-tune this model on a dataset comprising 20k instruction samples from Code Alpaca codealpaca, resulting in an instruction-following model called InstructCodeT5+, which exhibited improved capabilities in code generation. luo2023wizardcoder leverages the Evol-Instruct data synthesis technique from WizardLM xu2023wizardlm to evolve 20K code Alpaca codealpaca instruction samples into a 78K code instruction dataset. This enriched dataset is then used to fine-tune the StarCoder base model, resulting in WizardCoder, which showcases notable advancements in code generation. In a similar vein, inspired by the successes of WizardCoder luo2023wizardcoder and RRHF yuan2023rrhf, Pangu-Coder 2 shen2023pangu applies the Evol-Instruct method to generate 68k high-quality instruction samples from the initial 20k Code Alpaca codealpaca instruction samples. Additionally, they introduces a novel reinforcement learning via Rank Responses to align Test \\& Teacher Feedback (RRTF), which further enhances the performance of Pangu-Coder 2 in code generation. Diverging from synthetic instruction data generation methods, OctoPack muennighoff2023octopack utilizes real-world data by curating CommitPack from the natural structure of Git commits, which inherently pair code changes with human-written instructions. This dataset, consisting of 4 terabytes of Git commits across 350 programming languages, is employed to fine-tune StarCoder li2023starcoder and CodeGeeX2 zheng2023codegeex, leading to the instruction-following code models of OctoCoder and OctoGeeX for code generation, respectively. The most recent innovation comes from Magicoder wei2023magicoder, who proposes OSS-INSTRUCT, a novel data synthesis method that leverages open-source code snippets to generate high-quality instruction data for code generation. This approach seeks to reduce the bias often present in synthetic data generated by LLM. In line with OSS-INSTRUCT, the BigCode team introduces StarCoder2-15B-instruct starcoder2instruct, which they claim to be the first entirely self-aligned LLM for code generation, trained with a fully permissive and transparent pipeline. Moreover, codegemma_2024 harnesses open-source mathematics datasets, such as MATH hendrycks2021measuring and GSM8k cobbe2021training, along with synthetically generated code following the OSS-INSTRUCT wei2023magicoder paradigm, to instruction-tune CodeGemma 7B, yielding exceptional results in mathematical reasoning and code generation tasks.",
      "origin_cites_number": 18
    },
    {
      "section_title": "Parameter-Efficient Fine-tuning",
      "level": "3",
      "content": "To mitigate the extensive computational and resource demands inherent in fine-tuning LLMs, the concept of parameter-efficient fine-tuning (PEFT) has emerged to focus on updating a minimal subset of parameters, which may either be a selection of the model's parameters or an array of additional parameters specifically introduced for the tuning process ding2022delta,lialin2023scaling. The categorization of these methods is depicted in Figure fig:finetune(b), (c), and (d). A plethora of innovative PEFT approaches have been developed, among which BitFit zaken2021bitfit, Adapter houlsby2019parameter, Prompt tuning lester2021power, Prefix-tuning li2021prefix, LoRA hu2021lora, IA$^3$ liu2022few, QLoRA dettmers2024qlora, and AdaLoRA zhang2023adaptive are particularly noteworthy. A seminal study in this field, LoRA hu2021lora, proposes a parameter update mechanism for a pre-trained weight matrix â€” such as those found in the key or value projection matrices of a Transformer block's multi-head self-attention layer â€” by factorizing the update into two low-rank matrices. Crucially, all original model parameters remain frozen, with only the pair of low-rank matrices being trainable. After fine-tuning, the product of these low-rank matrices can be seamlessly incorporated into the existing weight matrix through an element-wise addition. This process can be formally described as: equation aligned (W_0 + \\DeltaW)x = W_0x + \\DeltaWx = W_0^{frozen}x + \\frac{\\alpha{r}B^{trainable}_{up}A^{trainable}_{down}}_{\\DeltaW}x aligned equation where $W_0 \\in R^{d\\times k}$ denotes a pre-trained weight matrix, $B^{trainable}_{up} \\in R^{d\\times r}$ and $A^{trainable}_{down} \\in R^{r\\times k}$ are two trainable low-rank matrixes and initialized by a zero matrix and a random Gaussian distribution $N(0,\\sigma^{2})$ respectively, to ensure $\\DeltaW=0$ at the beginning of training. The rank $r \\ll \\min(d, k)$, the $\\alpha{r}$ is a scaling coefficient to balance the importance of the LoRA module, like a learning rate. Despite the advancements in PEFT methods, their application in code generation remains limited. For instance, codeup pioneered the use of parameter-efficient instruction-tuning on a Llama 2 touvron2023llama2 model with a single RTX 3090 GPU, leading to the development of a multilingual code generation model called CodeUp. More recently, ASTRAIOS zhuo2024astraios conducted a thorough empirical examination of parameter-efficient instruction tuning for code comprehension and generation tasks. This study yielded several perceptive observations and conclusions, contributing valuable insights to the domain. figure*[t] \\centering \\includegraphics[width=\\linewidth]{images/PEFT.pdf} An illustration of full parameter fine-tuning (FFT) and parameter-efficient fine-tuning (PEFT) methods. (a) refers to the Full Fine-tuning method, which updates all parameters of the base model during fine-tuning. (b) stands for the Specification-based PEFT method that conditionally fine-tunes a small subset of the model parameters while freezing the rest of the model, e.g. BitFit \\cite{zaken2021bitfit. (c) represents the Addition-based PEFT method that fine-tunes the incremental parameters introduced into the base model or input, e.g. Adapter houlsby2019parameter, Prefix-tuning li2021prefix, and Prompt-tuning lester2021power. (d) symbolizes the Reparameterization-based method which reparameterizes existing model parameters by low-rank transformation, e.g. LoRA hu2021lora, QLoRA dettmers2024qlora, and AdaLoRA zhang2023adaptive. } figure*",
      "origin_cites_number": 20
    },
    {
      "section_title": "Reinforcement Learning with Feedback",
      "level": "2",
      "content": "LLMs have exhibited remarkable instruction-following capabilities through instruction tuning. However, they often produce outputs that are unexpected, toxic, biased, or hallucinated outputs that do not align with users' intentions or preferences ouyang2022training,wang2023aligning,ji2023ai. Consequently, aligning LLMs with human preference has emerged as a pivotal area of research. A notable work is InstructGPT ouyang2022training, which further fine-tunes an instruction-tuned model utilizing reinforcement learning with human feedback (RLHF) on a dataset where labelers have ranked model outputs in order of quality, from best to worst. This method has been instrumental in the development of advanced conversational language models, such as ChatGPT gpt-3.5-turbo and Bard manyika2023overview. Despite its success, acquiring high-quality human preference ranking data is a resource-intensive process lee2023rlaif. To address this, Reinforcement Learning from AI Feedback (RLAIF) bai2022constitutional,lee2023rlaif has been proposed to leverage powerful off-the-shelf LLMs (e.g., ChatGPT gpt-3.5-turbo and GPT-4 achiam2023gpt) to simulate human annotators by generating preference data. Building on RLHF's success, researchers have explored reinforcement learning with feedback to enhance code generation in LLMs. Unlike RLHF, which relies on human feedback, this approach employs compilers or interpreters to automatically provide feedback on code samples through code execution on unit test cases, catalyzing the advancement of this research domain. CodeRL le2022coderl introduced an actor-critic reinforcement learning framework for code generation. In this setup, the language model serves as the actor-network, while a token-level functional correctness reward predictor acts as the critic. Generated code is assessed through unit test signals from a compiler, which can indicate compiler errors, runtime errors, unit test failures, or passes. CompCoder wang2022compilable enhances code compilability by employing compiler feedback, including language model fine-tuning, compilability reinforcement, and compilability discrimination strategies. Subsequently, PPOCoder shojaee2023execution integrates pre-trained code model CodeT5 wang2021codet5 with Proximal Policy Optimization (PPO) schulman2017proximal. This integration not only utilizes execution (i.e., compilers or interpreters) feedback to assess syntactic and functional correctness but also incorporates a reward function that evaluates the syntactic and semantic congruence between abstract syntax tree (AST) sub-trees and data flow graph (DFG) edges in the generated code against the ground truth. Additionally, the framework applies a KL-divergence penalty to maintain fidelity between the actively learned policy and the referenced pre-trained model, enhancing the optimization process. More recently, RLTF liu2023rltf has proposed an online reinforcement learning framework that provides fine-grained feedback based on compiler error information and location, along with adaptive feedback that considers the ratio of passed test cases. Despite these successes, reinforcement learning algorithms face inherent limitations such as inefficiency, instability, extensive resource requirements, and complex hyperparameter tuning, which can impede the performance and scalability of LLMs. To overcome these challenges, recent studies have introduced various variants of RL methods that do not rely on PPO, including DPO rafailov2024direct, RRHF yuan2023rrhf, and sDPO kim2024sdpo. In essence, these methods aim to maximize the likelihood between the logarithm of conditional probabilities of preferred and rejected responses, which may be produced by LLMs with varying capabilities. Inspired by RRHF yuan2023rrhf, PanGu-Coder 2 shen2023pangu leverages a novel framework, Reinforcement Learning via Rank Responses to align Test \\& Teacher Feedback (RRTF), significantly enhancing code generation capabilities, as evidenced by pass@1 of 62.20\\% on the HumanEval benchmark. Taking a step forward, the integration of more non-differentiable code features, such as coding style markovtsev2019style,chen2023duetcs and readability buse2009learning, into the reinforcement learning feedback for LLM-based code generation, presents an exciting avenue for future research.",
      "origin_cites_number": 21
    },
    {
      "section_title": "Prompting Engineering",
      "level": "2",
      "content": "Large-scale language models (LLMs) such as GPT-3 and its successors have been trained on large-scale data corpora, endowing them with substantial world knowledge brown2020language,wei2021finetuned,ouyang2022training. Despite this, crafting an effective prompting as a means of communicating with LLMs to harness their full potential remains a long-standing challenge \\cite{liu2023pre. Recent advancements in prompting engineering have expanded the capabilities of LLMs, enabling more sophisticated task completion and enhancing both reliability and performance.} Notable techniques include Chain-of-Thought (CoT) wei2022chain, Self-Consistency wang2022self, Tree-of-Thought (ToT) yao2024tree, Program of Thoughts (PoT) chen2022program, Reasoning via Planning (RAP) hao2023reasoning, ReAct yao2023react, Self-Refine madaan2024self, Reflexion shinn2024reflexion, and LATS zhou2023language. For instance, CoT significantly improves the LLMs' ability to perform complex reasoning by providing a few chain-of-thought demonstrations as exemplars in prompting. Prompting engineering is particularly advantageous as it bypasses the need for additional training and can significantly elevate performance. Consequently, numerous studies have leveraged this technique for iterative and self-improving (refining) code generation within proprietary LLMs such as ChatGPT and GPT-4. Figure fig:reflection illustrates the general pipeline for self-improving code generation with LLMs. For instance, Self-Debugging chen2023teaching involves prompting an LLM to iteratively refine a predicted program by utilizing feedback composed of code explanations combined with execution results, which assists in identifying and rectifying errors. When unit tests are unavailable, this feedback can rely solely on code explanations. Similarly, LDB \\cite{zhong2024ldb prompts LLMs to refine generated code by incorporating debugging feedback, which consists of the evaluation of the correctness of variable values throughout runtime execution, as assessed by the LLMs. } In parallel, SelfEvolve jiang2023selfevolve employs a two-stage process where LLMs first generate domain-specific knowledge for a problem, followed by a trial code. This code is then iteratively refined through interactive prompting and execution feedback. An empirical investigation by olausson2023self provides a comprehensive analysis of the self-repairing capabilities for code generation in models like Code Llama, GPT-3.5, and GPT-4, using problem sets from HumanEval and APPS. This study yields a series of insightful observations and findings, shedding light on the self-refinement effectiveness of these LLMs. Moreover, Reflexion shinn2024reflexion introduces a general approach for code generation wherein LLM-powered agents engage in verbal self-reflection on task feedback signals, storing these reflections in an episodic memory buffer to inform and improve decision-making in subsequent interactions. LATS zhou2023language adopts a novel strategy, utilizing LLMs as agents, value functions, and optimizers. It enhances decision-making by meticulously constructing trajectories through Monte Carlo Tree Search (MCTS) algorithms, integrating external feedback, and learning from experience. This approach has demonstrated remarkable results in code generation, achieving a pass@1 of 94.4\\% on the HumanEval benchmark with GPT-4. figure*[t] \\centering \\includegraphics[width=0.95\\linewidth]{images/Self-Refine_v6.pdf} An illustration of the self-improving code generation pipeline using prompts for \\done{LLMs. This process incorporates iterative self-refinement by integrating execution outcomes and includes an optional self-reflection mechanism to enhance generation quality. } figure* Distinct from the aforementioned methods, CodeT chen2022codet and LEVER ni2023lever prompt LLMs to generate numerous code samples, which are then re-ranked based on execution outcomes to select the optimal solution. Notably, these approaches do not incorporate a self-refinement step to further improve code generation.",
      "origin_cites_number": 19
    },
    {
      "section_title": "Repository Level \\& Long Context",
      "level": "2",
      "content": "In contemporary software engineering practices, modifications to a code repository are widespread and encompass a range of activities, including package migration, temporary code edits, and the resolution of GitHub issues. While LLMs showcase impressive prowess in function-level code generation, they often falter when grappling with the broader context inherent to a repository, such as import dependencies, parent classes, and files bearing similar names. These deficiencies result in suboptimal performance in repository-level code generation, as identified in recent studies shrivastava2023repository,shrivastava2023repofusion. The challenges faced by LLMs in this domain are primarily due to the following factors: itemize \\item Code repositories typically contain intricate interdependencies scattered across various files, including shared utilities, configurations, and cross-API invocations, which arise from modular design principles zhang2023repocoder,bairi2023codeplan. \\item Repositories are characterized by their unique structures, naming conventions, and coding styles, which are essential for maintaining clarity and facilitating ongoing maintenance chen2023duetcs. \\item The vast context of an entire repository often exceeds the context length limitations of LLMs, thus hindering their ability to integrate comprehensive contextual information bairi2023codeplan. \\item LLMs may not have been adequately trained on extensive sets of repository data, such as proprietary software or projects that are still in development shrivastava2023repofusion. itemize Given that the scope of a typical software repository encompasses hundreds of thousands of tokens, it is imperative to enhance the capacity of LLMs to handle extensive contexts when they are employed for repository-level code generation. Fortunately, recent advancements in positional encoding techniques, such as ALiBi press2021train and RoPE su2024roformer, have shown promise in improving the Transformer's ability to generalize from shorter training sequences to longer inference sequences zhao2023length. This progress addresses the third challenge mentioned above to a certain degree, thereby enabling better contextualization of coding activities within full repositories. To further refine LLMs for repository-level code completion, several innovative approaches have been introduced. RepoCoder zhang2023repocoder leverages a similarity-based retrieval system within an iterative retrieval-generation paradigm to enrich the context and enhance code completion quality. In a similar vein, CoCoMIC ding2022cocomic employs a cross-file context finder named CCFINDER to pinpoint and retrieve the most relevant cross-file contexts within a repository. RepoHyper phan2024repohyper introduces a semantic graph structure, termed RSG, to encapsulate the expansive context of code repositories and uses an ``Expand and Refine'' retrieval method to obtain relevant code snippets. Moreover, a framework known as RLPG shrivastava2023repository has been proposed to generate repository-level prompts that integrate the repository's structure with the relevant context across all files. However, the constant reliance on retrieval mechanisms has raised concerns regarding efficiency and robustness, as some retrieved contexts may prove unhelpful or harmful. In response, Repoformer wu2024repoformer introduces a selective Retrieval-Augmented Generation (RAG) framework that judiciously bypasses retrieval when it is deemed redundant. This approach incorporates a self-supervised learning strategy that equips a code LLM with the ability to perform a self-assessment on the utility of retrieval for enhancing the quality of its output, thereby effectively utilizing potentially noisy retrieved contexts. Additionally, RepoFusion shrivastava2023repofusion has been developed to train models to combine multiple relevant contexts from a repository, aiming to produce more precise and context-aware code completions. In a novel approach, Microsoft's CodePlan bairi2023codeplan frames repository-level coding tasks as a planning problem, generating a multi-step chain of edits (plan) where each step involves invoking an LLM on a specific code location, considering context from the entire repository, preceding code modifications, and task-specific instructions. Advancing the state-of-the-art, zan2024codes tackles the formidable challenge of NL2Repo, an endeavor that seeks to create a complete code repository from natural language requirements. To address this complex task, they introduce the CodeS framework, which strategically breaks down NL2Repo into a series of manageable sub-tasks using a multi-layer sketch approach. The CodeS framework comprises three distinct modules: 1) RepoSketcher, for creating a directory structure of the repository based on given requirements; 2) FileSketcher, for sketching out each file within that structure; and 3) SketchFiller, for fleshing out the specifics of each function within the file sketches zan2024codes. Accordingly, a surge of benchmarks tailored for repository-level code generation has emerged, such as RepoEval zhang2023repocoder, Stack-Repo shrivastava2023repofusion, Repobench liu2023repobench, EvoCodeBench li2024evocodebench, SWE-bench jimenez2023swe, CrossCodeEval ding2024crosscodeeval, and SketchEval zan2024codes. The detailed statistics and comparisons of these benchmarks are presented in Table tab:benchmark. Despite the progress made by these methods in repository-level code generation, significant challenges remain to be addressed. Programming developers are often required to invest considerable time in editing and debugging vaithilingam2022expectation,mozannar2022reading,shrivastava2023repofusion,barke2023grounded,bird2022taking. However, the advent of LLM-powered coding agents, such as AutoCodeRover zhang2024autocoderover, SWE-Agent swe-agent, and OpenDevin OpenDevin, has demonstrated their potential to tackle complex problems, paving the way for future exploration in this field (for more details, see Section sec:autonomous_agents).",
      "origin_cites_number": 28
    },
    {
      "section_title": "Retrieval Augmented",
      "level": "2",
      "content": "figure*[t] \\centering \\includegraphics[width=\\linewidth]{images/RAG_Code_v4.pdf} A workflow illustration of the Retrieval-Augmented Code Generation (RACG). Upon receiving a query (instruction), the retriever selects the relevant contexts from a large-scale vector database. Subsequently, the retrieved contexts are merged with the query, and this combined input is fed into the generator (LLM) to produce the target code solution. figure* LLMs have exhibited impressive capabilities but are hindered by several critical issues such as hallucination liang2023holistic,zhang2023siren, obsolescence of knowledge jang2022towards, and non-transparent bommasani2021opportunities, untraceable reasoning processes zhou2022least,wei2022chain,huang2023towards,gao2023retrieval. While techniques like instruction-tuning (see Section sec:instruction_tuning) and reinforcement learning with feedback (see Section sec:reinforcement_learning) mitigate these issues, they also introduce new challenges, such as catastrophic forgetting and the requirement for substantial computational resources during training ovadia2023fine,gupta2024rag. Recently, Retrieval-Augmented Generation (RAG) has emerged as an innovative approach to overcoming these limitations by integrating knowledge from external databases. Formally defined, RAG denotes a model that, in response to queries, initially sources relevant information from an extensive corpus of documents, and then leverages this retrieved information in conjunction with the original query to enhance the response's quality and accuracy, especially for knowledge-intensive tasks. The RAG framework typically consists of a vector database, a retriever, a re-ranker, and a generator. It is commonly implemented using tools such as LangChainLangChain facilitates the development of LLM-powered applications. \\href{https://www.langchain.com{https://www.langchain.com}} and LLamaIndexLLamaIndex is a leading data framework for building LLM applications. \\href{https://www.llamaindex.ai/{https://www.llamaindex.ai}}. By performing continuous knowledge updates of the database and the incorporation of domain-specific data, RAG circumvents the need for re-training LLMs from scratch gao2023retrieval. Consequently, RAG has substantially advanced LLM performance across a variety of tasks lewis2020retrieval,chen2024benchmarking. Due to the nature of code, code LLMs are also susceptible to the aforementioned issues that affect general-purpose LLMs. For instance, they may exhibit a hallucination phenomenon when instructions fall outside the scope of their training data or necessitate the latest programming packages. Given the dynamic nature of publicly available source-code libraries like PyTorch, which undergo frequent expansion and updates, deprecated calling methods can become a significant challenge. If Code LLMs are not updated in tandem with the latest functions and APIs, this can introduce potential errors and safety risks. Retrieval-Augmented Code Generation (RACG) stands as a promising solution to these concerns. A workflow illustration of the RACG is depicted in Figure fig:rag. Despite its potential, the adoption of RAG for code generation remains limited. Drawing inspiration from the common practice among programmers of referencing related code snippets, liu2020retrieval introduced a novel retrieval-augmented mechanism with graph neural networks (GNNs), termed HGNN, which unites the advantages of similar examples retrieval with the generalization capabilities of generative models for code summarization, which is the reverse process of code generation. parvez2021retrieval pioneered a retrieval augmented framework named REDCODER for code generation by retrieving and integrating relevant code snippets from a source-code database, thereby providing supplementary context for the generation process. Subsequently, a retrieval-augmented code completion framework termed ReACC lu2022reacc is proposed to leverage both lexical copying and semantic referencing of related code, achieving state-of-the-art performance on the CodeXGLUE benchmark lu2021codexglue. In the spirit of how programmers often consult textual resources such as code manuals and documentation to comprehend functionalities, DocPrompting zhou2022docprompting explicitly utilizes code documentation by retrieving the relevant documentation pieces based on a natural language query and then generating the target code by blending the query with the retrieved information. More recently, RepoCoder zhang2023repocoder, an iterative retrieval-generation framework, is proposed for enhancing repository-level code completion by effectively utilizing code analogies across different files within a repository to inform and improve code suggestions. Furthermore, breaking away from reliance on a singular source of retrieval, su2024arks developed a multi-faceted ``knowledge soup'' that integrates web searches, documentation, execution feedback, and evolved code snippets. Then, it incorporates an active retrieval strategy that iteratively refines the query and enriches the knowledge soup, expanding the scope of information available for code generation. Despite these advancements, several limitations in retrieval-augmented code generation warrant further exploration: 1) the quality of the retrieved information significantly impacts overall performance; 2) the effective integration of retrieved code information with the query needs optimization; 3) an over-reliance on retrieved information may lead to inadequate responses that fail to address the query's intent; 4) additional retrieved information necessitates larger context windows for the LLM, resulting in increased computational demands.",
      "origin_cites_number": 14
    },
    {
      "section_title": "Autonomous Coding Agents",
      "level": "2",
      "content": "The advent of LLMs has marked the beginning of a new era of potential pathways toward artificial general intelligence (AGI), capturing significant attention in both academia and industry xi2023rise,weng2023agent,wang2024survey,huang2024position. A rapidly expanding array of applications for LLM-based autonomous agents, including AutoGPT autogpt, AgentGPT agentgpt, BabyAGI babyagi, and AutoGen wu2023autogen, underlines the promise of this technology. LLM-powered autonomous agents are systems endowed with sophisticated reasoning abilities, leveraging an LLM as a central computational engine or controller. This allows them to formulate and execute problem-solving plans through a series of tool-enabled functions or API calls. Moreover, these agents are designed to function within a shared environment where they can communicate and engage in cooperative, competitive, or negotiating interactions huang2023agentcoder,wu2023autogen,wang2024survey. The typical architecture of such an agent encompasses an LLM-based Agent, a memory module, a planning component, and a tool utilization module, as depicted in Figure fig:agent. figure*[t] \\centering \\includegraphics[width=\\linewidth]{images/Agent_v4.pdf} The general architecture of an LLM-powered autonomous agent system, adapted from \\cite{weng2023agent. Planning: The agent decomposes large tasks into smaller, manageable sub-goals or engages in self-criticism and self-reflection on past actions to learn from mistakes and improve future performance. Memory: This component enables the agent to store and retrieve past information. Tools: The agent is trained to invoke external functions or APIs. Action: The agent executes actions, with or without the use of tools, to interact with the environment. The gray dashed lines represent the data flow within the system. } figure* figure*[t] blue \\centering \\includegraphics[width=0.95\\linewidth]{images/metagpt.png} \\done{MetaGPT integrates human workflow efficiencies into LLM-based multi-agent collaboration to break down complex code-related tasks into specific, actionable procedures. These procedures are then assigned to various roles, such as Product Manager, Architect, and Engineer played by LLM. The image is sourced from the original paper \\cite{hong2023metagpt. }} figure* In the realm of automated code generation, LLM-powered autonomous agents have demonstrated remarkable proficiency. For instance, AgentCoder huang2023agentcoder achieved a groundbreaking pass@1 of 96.3\\% on the HumanEval benchmark, forwarding a step closer to the future of automated software development ishibashi2024self. The innovative meta-programming framework termed MetaGPT \\cite{hong2023metagpt integrates human workflow efficiencies into LLM-based multi-agent collaboration, as shown in Figure fig:matagpt.} Furthermore, huang2023agentcoder introduces AgentCoder, a multi-agent framework composed of three specialized agents, each with distinct roles and capabilities. These roles include a programmer agent responsible for code generation, a test designer agent tasked with generating unit test cases, and a test executor agent that executes the code and provides feedback. This division of labor within AgentCoder promotes more efficient and effective code generation. CodeAct wang2024executable distinguishes itself by utilizing executable Python code to consolidate LLM agent actions within a unified action space, in contrast to the generation of JSON or textual formats. Additionally, AutoCodeRover zhang2024autocoderover is proposed to autonomously resolve GitHub issues for program enhancement. To address the complexity of tasks within software engineering, two innovative autonomous AI software engineers Devin\\href{https://www.cognition.ai/introducing-devin{https://www.cognition.ai/introducing-devin}}Devin and OpenDevin\\href{https://github.com/OpenDevin/OpenDevin{https://github.com/OpenDevin/OpenDevin}}OpenDevin, have been released and rapidly garnered considerable interest within the software engineering (SE) and artificial general intelligence (AGI) community. Subsequently, an autonomous system, SWE-agent swe-agent, leverages a language model to interact with a computer to address software engineering tasks, successfully resolving 12.5\\% of issues on the SWE-bench benchmark jimenez2023swe. L2MAC holt2023l2mac has been introduced as the first practical, LLM-based, multi-agent, general-purpose stored-program automatic computer that utilizes a von Neumann architecture, designed specifically for the generation of long and consistent outputs. At the time of writing this survey, OpenDevin has enhanced CodeAct with bash command-based tools, leading to the release of OpenDevin CodeAct 1.0 OpenDevin_CodeAct, which sets a new state-of-the-art performance on the SWE-Bench Lite benchmark jimenez2023swe. Despite these remarkable advancements, the journey toward fully realized AI software engineers employing LLM-powered autonomous agents is far from complete xi2023rise,wang2024survey. Critical aspects such as prompt design, context length, agent count, and toolsets call for further refinement and optimization, especially as problem complexities escalate ishibashi2024self.",
      "origin_cites_number": 23
    },
    {
      "section_title": "Evaluation",
      "level": "2",
      "content": "Despite the impressive capabilities of LLMs, they exhibit a range of behaviors that are both beneficial and potentially risky. These behaviors can enhance performance across various downstream tasks but may also introduce reliability and trustworthiness concerns in LLM deployment chen2021evaluating, xu2022systematic, chang2024survey. Consequently, it is imperative to develop precise evaluation approaches to discern the qualitative and quantitive differences between models, thereby encouraging further advancements in LLM capabilities. Evaluation strategies for LLMs in code generation mirror those for general-purpose LLMs and can be divided into three principal categories: metrics-based, human-centered, and LLM-based approaches. Detailed benchmarks for these evaluation strategies are presented in Section sec:benchmark and summarized in Table tab:benchmark. Subsequent subsections will provide a thorough analysis of each approach.",
      "origin_cites_number": 57
    },
    {
      "section_title": "Metrics",
      "level": "3",
      "content": "The pursuit of effective and reliable automatic evaluation metrics for generated content is a long-standing challenge within the field of natural language processing (NLP) chen1998evaluation, papineni2002bleu, lin2004rouge. At the early stage, most works directly leverage token-matching-based metrics, such as Exact Match, BLEU papineni2002bleu, ROUGE lin2004rouge, and METEOR banerjee2005meteor, which are prevalent in text generation of NLP, to assess the quality of code generation. While these metrics offer a rapid and cost-effective approach for assessing the quality of generated code, they often fall short of capturing the syntactical and functional correctness, as well as the semantic features of the code. To eliminate this limitation, CodeBLEU ren2020codebleu was introduced, enhancing the traditional BLEU metric papineni2002bleu by incorporating syntactic information through abstract syntax trees (AST) and semantic understanding via data-flow graph (DFG). Despite these improvements, the metric does not fully resolve issues pertaining to execution errors or discrepancies in the execution results of the generated code. In light of these challenges, execution-based metrics have gained prominence for evaluating code generation, including pass@k chen2021evaluating, n@k li2022competition, test case average hendrycks2021measuring, execution accuracy rajkumar2022evaluating, and pass@t olausson2023self. In particular, the pass@k, serving as a principal evaluation metric, assesses the probability that at least one out of $k$ code samples generated by a model will pass all unit tests. An unbiased estimator for pass@k introduced by chen2021evaluating is defined as: equation aligned pass@k \\coloneqq E_task \\left[1-\\binom{n-c{k}}{n{k}}\\right] aligned equation where $n$ is the total number of sampled candidate code solutions, $k$ is the number of randomly selected code solutions from these candidates for each programming problem, with $n\\ge k$, and $c$ is the count of correct samples within the $k$ selected. Nevertheless, these execution-based methods are heavily dependent on the quality of unit tests and are limited to evaluating executable code zan2023large. Consequently, when unit tests are unavailable, token-matching-based metrics are often employed as an alternative for evaluation. Furthermore, in scenarios lacking a ground truth label, unsupervised metrics such as perplexity (PPL) jelinek1977perplexity can serve as evaluative tools. Perplexity quantifies an LLM's uncertainty in predicting new content, thus providing an indirect measure of the model's generalization capabilities and the quality of the generated code. Taken together, while the aforementioned methods primarily focus on the functional correctness of code, they do not provide a holistic evaluation that encompasses other critical dimensions such as code vulnerability nappa2015attack, maintainability ardito2020tool, readability buse2009learning, complexity and efficiency peitek2021program, stylistic consistency markovtsev2019style, and execution stability raemaekers2012measuring. A comprehensive evaluation framework that integrates these aspects remains an open area for future research and development in the field of code generation assessment.",
      "origin_cites_number": 20
    },
    {
      "section_title": "Human Evaluation",
      "level": "3",
      "content": "Given the intrinsic characteristics of code, the aforementioned automatic evaluation metrics are inherently limited in their capacity to fully assess code quality. For instance, metrics specifically designed to measure code style consistency are challenging to develop and often fail to capture this aspect adequately chen2023duetcs. When it comes to repository-level code generation, the evaluation of overall code quality is substantially complicated due to the larger scale of the task, which involves cross-file designs and intricate internal as well as external dependencies, as discussed by bairi2023codeplan,shrivastava2023repofusion. To overcome these challenges, conducting human evaluations becomes necessary, as it yields relatively robust and reliable results. Human assessments also offer greater adaptability across various tasks, enabling the simplification of complex and multi-step evaluations. Moreover, human evaluations are essential for demonstrating the effectiveness of certain token-matching-based metrics, such as CodeBLEU ren2020codebleu. These studies typically conduct experiments to evaluate the correlation coefficient between proposed metrics and quality scores assigned by actual users, demonstrating their superiority over existing metrics. Moreover, in an effort to better align LLMs with human preferences and intentions, InstructGPT ouyang2022training employs human-written prompts and demonstrations, and model output ranking in the fine-tuning of LLMs using reinforcement learning from human feedback (RLHF). Although similar alignment learning techniques have been applied to code generation, the feedback in this domain typically comes from a compiler or interpreter, which offers execution feedback, rather than from human evaluators. Notable examples include CodeRL le2022coderl, PPOCoder shojaee2023execution, RLTF liu2023rltf, and PanGu-Coder2 shen2023pangu. Further information on this topic is available in Section sec:reinforcement_learning. Nonetheless, human evaluations are not without drawbacks, as they can be prone to certain issues that may compromise their accuracy and consistency. For instance, 1) personalized tastes and varying levels of expertise among human evaluators can introduce biases and inconsistencies into the evaluation process; 2) conducting comprehensive and reliable human evaluations often necessitates a substantial number of evaluators, leading to significant expenses and time-consuming; 3) the reproducibility of human evaluations is often limited, which presents challenges in extending previous evaluation outcomes or monitoring the progress of LLMs, as highlighted by zhao2023survey.",
      "origin_cites_number": 9
    },
    {
      "section_title": "LLM-as-a-Judge",
      "level": "3",
      "content": "figure*[t] \\centering \\includegraphics[width=0.95\\linewidth]{images/llm-as-judge_v4.pdf} \\done{The pipeline of (Code) LLM-as-a-judge for evaluating generated code by Code LLMs. There are primarily two types of approaches: pairwise comparison and single answer grading. } figure* The powerful instruction-following capabilities of LLMs have stimulated researchers to innovatively investigate the potential of LLM-based evaluations. The LLM-as-a-Judge zheng2024judging refers to the application of advanced proprietary LLMs (e.g., GPT4, Gemini, and Claud 3) as proxies for human evaluators. This involves designing prompts with specific requirements to guide LLMs in conducting evaluations, as demonstrated by AlpacaEval alpaca_eval and MT-bench zheng2024judging. This method reduces reliance on human participation, thereby facilitating more efficient and scalable evaluations. Moreover, LLMs can offer insightful explanations for the assigned rating scores, thereby augmenting the interpretability of evaluations zhao2023survey. Nevertheless, the use of LLM-based evaluation for code generation remains relatively underexplored compared with general-purpose LLM. The pipeline of (Code) LLM-as-a-judge for evaluating generated code by Code LLMs is depicted in Figure \\ref{fig:llm-as-judge.} A recent work zhuo2024ice introduces the ICE-Score evaluation metric, which instructs LLM for code assessments. This approach attains superior correlations with functional correctness and human preferences, thereby eliminating the requirement for test oracles or references. As the capabilities of LLM continue to improve, we anticipate seeing more research in this direction. Despite their scalability and explainability, the effectiveness of LLM-based evaluation is constrained by the inherent limitations of the chosen LLM. Several studies have shown that most LLMs, including GPT-4, suffer from several issues, including position, verbosity, and self-enhancement biases, as well as restricted reasoning ability zheng2024judging. Specifically, position bias refers to the tendency of LLMs to disproportionately favor responses that are presented in certain positions, which can skew the perceived quality of answers based on their order of presentation. Meanwhile, verbosity bias describes the inclination of LLMs to prefer lengthier responses, even when these are not necessarily of higher quality compared to more concise ones. Self-enhancement bias, on the other hand, is observed when LLMs consistently overvalue the quality of the text they generate zheng2024judging,zhao2023survey. Moreover, due to their inherent limitations in tackling complex reasoning challenges, LLMs may not be entirely reliable as evaluators for tasks that require intensive reasoning, such as those involving mathematical problem-solving. However, these shortcomings can be partially addressed through the application of deliberate prompt engineering and fine-tuning techniques, as suggested by zheng2024judging. \\done{",
      "origin_cites_number": 8
    },
    {
      "section_title": "Empirical Comparison",
      "level": "3",
      "content": "figure*[t] \\centering \\includegraphics[width=\\linewidth]{images/mbpp_scatter.pdf} \\done{The performance comparison of LLMs for code generation on the MBPP \\cite{austin2021program benchmark, measured by Pass@1. For models with various sizes, we report only the largest size version of each model with a magnitude of B parameters.} } figure* figure*[t] \\centering \\includegraphics[width=\\linewidth]{images/bigcodebench_complete_bar.pdf} \\done{The performance comparison of LLMs for code generation on the BigCodeBench \\cite{zhuo2024bigcodebench benchmark, measured by Pass@1. For models with various sizes, we report only the largest size version of each model with a magnitude of B parameters.} } figure* In this section, we present a performance comparison of LLMs for code generation using the well-regarded HumanEval, MBPP, and the more practical and challenging BigCodeBench benchmarks. This empirical comparison aims to highlight the progressive enhancements in LLM capabilities for code generation. These benchmarks assess an LLM's ability to generate source code across various levels of difficulty and types of programming tasks. Specifically, HumanEval focuses on complex code generation, MBPP targets basic programming tasks, and BigCodeBench emphasizes practical and challenging programming tasks. Due to the limitations in computational resources we faced, we have cited experimental results from original papers or widely recognized open-source leaderboards within the research community, such as the HumanEval Leaderboard \\href{https://paperswithcode.com/sota/code-generation-on-humaneval{https://paperswithcode.com/sota/code-generation-on-humaneval}}, EvalPlus Leaderboard \\href{https://evalplus.github.io/leaderboard.html{https://evalplus.github.io/leaderboard.html}}, Big Code Models Leaderboard \\href{https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard{https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard}}, and BigCodeBench Leaderboard \\href{https://bigcode-bench.github.io/{https://bigcode-bench.github.io/}}. We report performance on HumanEval using the pass@1 metric, as shown in Table tab:performance_humaneval, while MBPP and BigCodeBench results are presented with pass@1 in Figures fig:mbpp_performance and fig:bigcodebench_performance, respectively. We offer the following insights: itemize \\item The performance gap between open-source and closed-source models across the three benchmarks is gradually narrowing. For instance, on the HumanEval benchmark, DeepSeek-Coder-V2-Instruct with 21B activation parameters and Qwen2.5-Coder-Instruct 7B achieve 90\\% and 88.4\\% pass@1, respectively. These results are comparable to the much larger closed-source LLMs, such as Claude-3.5-Sonnet, which achieves 92.0\\% pass@1. On the MBPP benchmark, Qwen2.5-Coder-Instruct 7B with 83.5\\% pass@1 significantly outperforms GPT-3.5-Turbo with 52.2\\% pass@1 and closely rivals the closed-source Claude-3-Opus with 86.4\\% pass@1. On the BigCodeBench, DeepSeek-Coder-V2-Instruct achieves 59.7\\%, surpassing all compared closed-source and open-source LLMs except for slightly falling behind GPT-4o-0513, which achieves 61.1\\%. \\item Generally, as the number of model parameters increases, the performance of code LLMs improves. However, Qwen2.5-Coder-Instruct 7B achieves 88.4\\% pass@1, outperforming larger models like StarCoder2-Instruct 15.5B with 72.6\\% pass@1, DeepSeek-Coder-Instruct 33B with 79.3\\% pass@1, and Code Llama-Instruct 70B with 67.8\\% pass@1 on the HumanEval benchmark. Similar trends are observed across the other two benchmarks, suggesting that code LLMs with 7B parameters may be sufficiently capable for code generation task. \\item Instruction-tuned models consistently outperform their base (pretrained) counterparts across the HumanEval and MBPP benchmarks. For instance, Qwen2.5-Coder-Instruct surpasses Qwen2.5-Coder by an average of 26.04\\%, StarCoder2-Instruct improves upon StarCoder 2 by an average of 35.20\\%, and CodeGemma-Instruct enhances CodeGemma by an average of 11.26\\%. Additionally, DeepSeek-Coder-Instruct outperforms DeepSeek-Coder by an average of 23.71\\%, while Code Llama-Instruct shows a 13.80\\% improvement over Code Llama. Detailed results can be found in Table tab:instruct_improve. These findings underscore the effectiveness of instruction tuning, although the quality of the instruction tuning dataset plays a critical role in determining model performance luo2023wizardcoder,zhou2024lima. \\item Performance on the HumanEval benchmark is nearly saturated. However, MBPP, which involves basic programming tasks, and BigCodeBench, which involves more practical and challenging programming tasks, demand more capable code LLMs. Additionally, while these benchmarks primarily evaluate the functional correctness of code, they do not provide a comprehensive assessment across other critical dimensions. Developing a more holistic evaluation framework that integrates various aspects remains an open area for future research and development in LLMs for code generation evaluation. itemize Discussion: We discuss certain code LLMs in Table tab:performance_humaneval for clarity: (1) General LLMs accessed via API are not specifically trained on large code corpora but achieve state-of-the-art performance in code generation, such as Claude-3.5-Sonnet with 92.0\\% pass@1 on HumanEval benchmark. (2) AlphaCode targets code generation for more complex and unseen problems that require a deep understanding of algorithms and intricate natural language, such as those encountered in competitive programming. The authors of AlphaCode found that large-scale model sampling to navigate the search space, such as 1M samples per problem for CodeContests, followed by filtering based on program behavior to produce a smaller set of submissions, is crucial for achieving good and reliable performance on problems that necessitate advanced reasoning. (3) Phi-1 1.3B is a specialized LLM for code, trained on ``textbook quality'' data from the web (6B tokens) and synthetically generated textbooks and exercises using GPT-3.5 (1B tokens). (4) Code Llama 70B is initialized with Llama 2 model weights and continually pre-trained on 1T tokens from a code-heavy dataset and long-context fine-tuned with approximately 20B tokens. However, Code Llama-Instruct 70B is fine-tuned from Code Llama-Python 70B without long-context fine-tuning, using an additional 260M tokens to better follow human instructions. Surprisingly, these models underperform compared to smaller parameter Code LLMs like Qwen2.5-Coder-Instruct 7B, DeepSeek-Coder-V2-Instruct 21B, and Codestral 22B across all three benchmarks. The underlying reasons for this discrepancy remain unclear and warrant further exploration. (5) Unlike other open-source Code LLMs, DeepSeek-Coder-V2-Instruct is further pre-trained on DeepSeek-V2 liu2024deepseek, which employs a Mixture-of-Experts (MoE) architecture with only 21B activation parameters out of 236B parameters, using an additional 6 trillion tokens composed of 60\\% source code, 10\\% math corpus, and 30\\% natural language corpus. For a comprehensive understanding of MoE in LLMs, please refer to cai2024survey. } \\done{",
      "origin_cites_number": 5
    },
    {
      "section_title": "Code LLMs Alignment",
      "level": "2",
      "content": "The pre-training of LLMs for next-token prediction, aimed at maximizing conditional generation likelihood across vast textual corpora, equips these models with extensive world knowledge and emergent capabilities brown2020language. This training approach enables the generation of coherent and fluent text in response to diverse instructions. Nonetheless, LLMs can sometimes misinterpret human instructions, produce biased content, or generate factually incorrect information (commonly referred to as hallucinations), which may limit their practical utility wang2023aligning,zhao2023survey,ji2023ai. Aligning LLMs with human intentions and values, known as LLM alignment, has consequently become a critical research focus ji2023ai,wang2023aligning. Key objectives frequently discussed in the context of LLM alignment include robustness, interpretability, controllability, ethicality, trustworthiness, security, privacy, fairness, and safety. In recent years, significant efforts have been made by researchers to achieve this alignment, employing techniques such as Reinforcement Learning with Human Feedback (RLHF) ouyang2022training. However, the alignment of Code LLMs has not been extensively explored. Compared to text generation, aligning code generation with human intentions and values is even more crucial. For instance, users without programming expertise might prompt Code LLM to generate source code and subsequently execute it on their computers, potentially causing catastrophic damage. Some potential risks include: itemize \\item Malware Infection: The code could contain viruses, worms, or trojans that compromise our system's security. \\item Data Loss: It might delete or corrupt important files and data. \\item Unauthorized Access: It can create backdoors, allowing attackers to access our system remotely. \\item Performance Issues: The code might consume excessive resources, slowing down our system. \\item Privacy Breaches: Sensitive information, such as passwords or personal data, might be stolen. \\item System Damage: It may alter system settings or damage hardware components. \\item Network Spread: It could propagate across networks, affecting other devices. \\item Financial Loss: If the code is ransomware, it might encrypt data and demand payment for decryption. \\item Legal Consequences: Running certain types of malicious code can lead to legal repercussions. itemize As illustrated, aligning Code LLMs to produce source code consistent with human preferences and values is of paramount importance in software development. A recent study yang2024robustness provides the first systematic literature review identifying seven critical non-functional properties of LLMs for code, beyond accuracy, including robustness, security, privacy, explainability, efficiency, and usability. This study is highly pertinent to the alignment of Code LLMs. We recommend readers refer to this survey for more detailed insights. In this survey, we identify five core principles that serve as the key objectives for aligning Code LLMs: Green, Responsibility, Efficiency, Safety, and Trustworthiness (collectively referred to as GREST). These principles are examined from a broader perspective. Each category encompasses various concepts and properties, which are summarized in Table tab:codellm_alignment. In the following, we define each principle and briefly introduce a few notable works to enhance understanding. Green: The Green principle underscores the importance of environmental sustainability in the development and deployment of LLMs for code generation. This involves optimizing energy consumption and reducing both the carbon footprint and financial costs associated with training and inference processes. Currently, training, inference, and deployment of Code LLMs are notably resource-intensive. For example, training GPT-3, with its 175 billion parameters, required the equivalent of 355 years of single-processor computing time and consumed 284,000 kWh of energy, resulting in an estimated 552.1 tons of CO$_2$ emissions samsi2023words. Furthermore, a ChatGPT-like application, with an estimated usage of 11 million requests per hour, can produce emissions of 12.8k metric tons of CO$_2$ per year, which is 25 times the carbon emissions associated with training GPT-3 chien2023reducing. To mitigate these costs, several techniques are often employed, such as the development of specialized hardware (e.g., Tensor Processing Units (TPUs) and Neural Processing Units (NPUs)), model compression methods (e.g., quantization and knowledge distillation), parameter-efficient fine-tuning (PEFT), and the use of renewable energy sources. For instance, Shi et al. shi2024greening applied knowledge distillation to reduce the size of CodeBERT feng2020codebert and GraphCodeBERT guo2020graphcodebert, resulting in optimized models of just 3MB. These models are 160 times smaller than the original large models and significantly reduce energy consumption by up to 184 times and carbon footprint by up to 157 times. Similarly, Wei et al. wei2023towards utilized quantization techniques for Code LLMs such as CodeGen nijkamp2022codegen and Incoder fried2022incoder by employing lower-bit integers (e.g., int8). This approach reduced storage requirements by 67.3\\% to 70.8\\%, carbon footprint by 28.8\\% to 55.0\\%, and pricing costs by 28.9\\% to 55.0\\%. Responsibility: The Responsibility principle in the context of Code LLMs underscores the importance of ethical considerations, fairness, and accountability throughout their lifecycle. This involves addressing biases in training data, ensuring fairness and transparency in model decision-making, maintaining accountability for outputs, adhering to applicable laws (e.g., copyright), implementing safeguards against misuse, and providing clear communication about the model's capabilities and limitations. Specifically, itemize \\item Bias Mitigation. Biases in code generation can lead to flawed software and reinforce stereotypes, potentially causing significant societal impacts. For example, an Code LLM that inherits biases from its training data may produce source code/software that inadvertently discriminates against certain user groups. This can result in applications that fail to meet the diverse needs of users, promoting exclusionary practices and reinforcing existing stereotypes mouselinos2022simple,liu2023uncovering. \\item Fairness and Transparency. A lack of fairness and transparency in Code LLM decision-making can result in biased or suboptimal code solutions. If the model's decision-making process is opaque, developers might unknowingly introduce code that favors specific frameworks or libraries, thereby limiting innovation and diversity in software development. This opacity can create unfair advantages and hinder collaborative efforts within tech communities bogina2022educating. \\item Legal Compliance. Compliance with relevant laws, such as licensing and copyright, is crucial when using Code LLMs for code generation to avoid legal complications. If an Code LLM generates code snippets that inadvertently infringe on existing copyrights, it can lead to legal disputes and financial liabilities for developers and organizations xu2024first. Such risks may discourage the use of advanced AI tools, thus stifling innovation and affecting growth and collaboration within the tech community. \\item Accountability. Without accountability for code generated by Code LLMs, addressing bugs or security vulnerabilities becomes challenging. If a model generates faulty code leading to a security breach, the absence of clear accountability can result in significant financial and reputational damage for companies. This uncertainty can delay critical issue resolution and impede trust in AI-assisted development liesenfeld2023opening. \\item Misuse Prevention. Failing to implement mechanisms to prevent the misuse of Code LLMs can enable the creation of harmful software. For example, models could be exploited to generate malware or unauthorized scripts, posing cybersecurity risks. Without proper safeguards, these models can facilitate malicious activities, threatening both individual and organizational security mousavi2024investigation. \\item Clear Communication. Without clear communication about a model's capabilities and limitations, developers may misuse the model or overestimate its abilities. Relying on the model to generate complex, mission-critical code without human oversight can lead to significant software failures. Misunderstanding its limitations can result in faulty implementations and lost productivity ross2023programmer. itemize To adhere to this principle, potential mitigation methods include bias detection and mitigation, quantification and evaluation, and adherence to ethical guidelines. Liu et al. liu2023uncovering propose a new paradigm for constructing code prompts, successfully uncovering social biases in code generation models, and developing a dataset along with three metrics to evaluate overall social bias. Recently, Xu et al. xu2024first introduced LiCoEval, an evaluation benchmark for assessing the license compliance capabilities of LLMs. Additionally, incorporating diverse perspectives in development teams and engaging with stakeholders from various communities can further align Code LLM outputs with ethical standards and societal values. Efficiency: The Efficiency principle emphasizes optimizing the performance and speed of Code LLMs for code generation while minimizing the computational resources required for training and inference. For instance, training the GPT-3 model, which consists of 175 billion parameters, demands substantial resources. It requires approximately 1,024 NVIDIA V100 GPUs, costing around $4.6$ million and taking approximately 34 days to complete the training process. To address these challenges, various techniques are employed, including model compression methods such as pruning, quantization, and knowledge distillation. Additionally, optimized algorithms like AdamW, parallel strategies such as tensor, pipeline, and data parallelism, and parameter-efficient fine-tuning (PEFT) (see Section sec:peft) are often utilized. For a comprehensive and detailed discussion on methods to enhance the efficiency of Code LLMs for code generation, please refer to Section 4.5.2, ``Efficiency Enhancement'', in yang2024robustness. Safety: The Safety principle of Code LLMs is of utmost importance due to their potential to introduce vulnerabilities, errors, or privacy breaches into software systems. Ensuring safety involves comprehensive testing and validation processes to detect and mitigate these risks. For instance, attackers might compromise the training process of LLMs by injecting malicious examples into the training data, a method known as data poisoning attacks schuster2021you. Even when attackers lack access to the training process, they may employ techniques like the black-box inversion approach introduced by Hajipour et al. hajipour2024codelmsec. This method uses few-shot prompting to identify prompts that coax black-box code generation models into producing vulnerable code. Furthermore, Yang et al. yang2024unveiling and Al-Kaswan et al. al2024traces reveals that Code LLMs, such as CodeParrot codeparrot, can memorize training data, potentially outputting personally identifiable information like emails, names, and IP addresses, thereby posing significant privacy risks. Additionally, Yuan et al. yuan2023gpt demonstrate that engaging with ChatGPT and GPT-4 in non-natural languages can circumvent safety alignment measures, leading to unsafe outcomes, such as ``The steps involved in stealing money from a bank.''. To bolster the safety of LLMs in code generation, it is crucial to detect and eliminate privacy-related information from training datasets. For example, approaches outlined in fried2022incoder and allal2023santacoder utilize carefully crafted regular expressions to identify and remove private information from training data. To counteract black-box inversion, implementing prompt filtering mechanisms is recommended to identify and block prompts that might result in insecure code generation. Moreover, adversarial training can enhance the model's resilience to malicious prompts. Employing reinforcement learning methods can further align Code LLMs with human preferences, thereby reducing the likelihood of producing harmful outputs. Trustworthiness: The Trustworthiness principle focuses on developing Code LLMs that users can depend on for accurate and reliable code generation, which is crucial for their acceptance and widespread adoption. Achieving this requires ensuring model transparency, providing explanations for decisions, and maintaining consistent performance across various scenarios. For instance, Ji et al. ji2023benchmarking propose a causal graph-based representation of prompts and generated code to identify the causal relationships between them. This approach offers insights into the effectiveness of Code LLMs and assists end-users in understanding the generation. Similarly, Palacio et al. palacio2023evaluating introduce ASTxplainer, a tool that extracts and aggregates normalized model logits within Abstract Syntax Tree (AST) structures. This alignment of token predictions with AST nodes provides visualizations that enhance end-user understanding of Code LLM predictions. Therefore, by prioritizing trustworthiness, we can bolster user confidence and facilitate the integration of Code LLMs into diverse coding environments. By adhering to the aforementioned principles as key objectives for aligning Code LLMs, researchers and developers can create LLMs for code generation that are not only capable but also ethical, sustainable, and user-centric. } figure*[t] \\centering \\includegraphics[width=0.95\\linewidth]{images/Copilot.pdf} \\includegraphics[width=0.95\\linewidth]{images/leetcode.jpg} \\done{An exemplar of GitHub Copilot to demonstrate how to use development tools powered by LLMs, including powerful GPT 4o, o1-preview (Preview), and o1-mini (Preview). To illustrate its capabilities, we input the description of the ``5. Longest Palindromic Substring'' problem from LeetCode into Copilot's chat box. The code generated by Copilot is then submitted to the online judge platform, where it is successfully accepted. } figure*",
      "origin_cites_number": 40
    },
    {
      "section_title": "Applications",
      "level": "2",
      "content": "Code LLMs have been integrated with development tools and platforms, such as integrated development environments (IDEs) and version control systems, improving programming efficiency substantially. In this section, we will briefly introduce several widely used applications as coding assistants. The statistics of these applications are provided in Table tab:products. GitHub Copilot. GitHub Copilot, powered by OpenAI's Codex, is an AI pair programmer that helps you write better code faster. Copilot suggests whole lines or blocks of code as you type, based on the context provided by your existing code and comments. It's trained on a dataset that includes a significant portion of the public code available on GitHub, which enables it to understand a wide range of programming languages and coding styles. Copilot not only improves productivity but also serves as a learning tool by providing programmers with examples of how certain functions can be implemented or how specific problems can be solved. CodeGeeX. CodeGeeX stands out as a multifaceted programming assistant, proficient in code completion, comment generation, code translation, and developer interactions. Its underlying code generation LLM has been refined with extensive training on vast amounts of code data, exhibiting superior performance on benchmarks like HumanEval, HumanEval-X, and DS1000. Renowned for supporting multilingual code generation, CodeGeeX plays a pivotal role in enhancing the efficiency of code development. CodeWhisperer. Amazon's CodeWhisperer is a versatile, machine learning-driven code generator that offers on-the-fly code recommendations. Tailored to your coding patterns and comments, CodeWhisperer provides personalized suggestions that range from succinct comments to complex functions, all aimed at streamlining your coding workflow. Codeium. Codeium is an AI-accelerated coding toolkit that offers a suite of functions, including code completion, explanation, translation, search, and user chatting. Compatible with over 70 programming languages, Codeium delivers fast and cutting-edge solutions to coding challenges, simplifying the development process for its users. CodeArts Snap. Huawei's CodeArts Snap is capable of generating comprehensive function-level code from both Chinese and English descriptions. This tool not only reduces the monotony of manual coding but also efficiently generates test code, in addition to providing automatic code analysis and repair services. Tabnine. Tabnine is an AI coding assistant that empowers development teams to leverage AI for streamlining the software development lifecycle while maintaining strict standards for privacy, security, and compliance. With a focus on enhancing coding efficiency, code quality, and developer satisfaction, Tabnine offers AI-driven automation that is tailored to the needs of your team. Supporting over one million developers worldwide, Tabnine is applicable across various industries. Replit. Replit is a multifunctional platform that caters to a diverse array of software development needs. As a complimentary online IDE, it facilitates code collaboration, and cloud services, and fosters a thriving developer community. Replit also enables users to compile and execute code in more than 50 programming languages directly within a web browser, eliminating the need for local software installations. To illustrate the use of development tools powered by LLMs, we employ GitHub Copilot within Visual Studio Code (VS Code) as our example. Note that \\begin{itemize \\item [1] For details on using the GitHub Copilot extension in VS Code, please refer to the useful document at https://code.visualstudio.com/docs/copilot/overview{https://code.visualstudio.com/docs/copilot/overview}. \\item [2] If you would like to get free access to Copilot as a student, teacher, or open-source maintainer, please refer to this tutorial at https://docs.github.com/en/copilot/managing-copilot/managing-copilot-as-an-individual-subscriber/managing-your-copilot-subscription/getting-free-access-to-copilot-as-a-student-teacher-or-maintainer{https://docs.github.com/en/copilot/managing-copilot/managing-copilot-as-an-individual-subscriber/managing-your-copilot-subscription/getting-free-access-to-copilot-as-a-student-teacher-or-maintainer} and GitHub education application portal at \\\\ https://education.github.com/discount_requests/application{https://education.github.com/discount\\_requests/application}. itemize As depicted in the upper section of Figure fig:copilot, users can interact with Copilot through the chat box in the lower left corner, where they can inquire about various coding-related tasks. This feature is now supported by the advanced capabilities of GPT-4o, o1-preview (Preview), and o1-mini (Preview). From the generated content, Copilot demonstrates the ability to plan solutions to coding problems. It can write code and subsequently explain the generated code to enhance user comprehension. Within the right-side workspace, users can engage in inline chat conversations to generate or refactor source code, conduct code explanations, fix coding errors, resolve issues encountered during terminal command executions, produce documentation comments, and generate unit tests. To illustrate its capabilities, we input the description of the ``5. Longest Palindromic Substring'' problem from LeetCode into Copilot's chat box. The code generated by Copilot is then submitted to the online judge platform, where it is successfully accepted, as shown at the lower section of Figure fig:copilot. }",
      "origin_cites_number": 0
    },
    {
      "section_title": "Challenges \\& Opportunities",
      "level": "1",
      "content": "According to our investigations, the LLMs have revolutionized the paradigm of code generation and achieved remarkable performance. Despite this promising progress, there are still numerous challenges that need to be addressed. These challenges are mainly caused by the gap between academia and practical development. For example, in academia, the HumanEval benchmark has been established as a de facto standard for evaluating the coding proficiency of LLMs. However, many works have illustrated the evaluation of HumanEval can't reflect the scenario of practical development jimenez2023swe,du2024evaluating,liu2024your,ding2024crosscodeeval. In contrast, these serious challenges offer substantial opportunities for further research and applications. In this section, we pinpoint critical challenges and identify promising opportunities, aiming to bridge the research-practicality divide. Enhancing complex code generation at repository and software scale. In practical development scenarios, it often involves a large number of complex programming problems of varying difficulty levels zhang2022automated,li2022competition. While LLMs have shown proficiency in generating function-level code snippets, these models often struggle with more complex, unseen programming problems, repository- and software-level problems that are commonplace in real-world software development. To this end, it requires strong problem-solving skills in LLM beyond simply functional-level code generation. For example, AlphaCode li2022competition achieved an average ranking in the top 54.3\\% in programming competitions where an understanding of algorithms and complex natural language is required to solve competitive programming problems. jimenez2023swe argues that existing LLMs can't resolve real-world GitHub issues well since the best-performing model, Claude 2, is able to solve a mere 1.96\\% of the issues. The reason for poor performance is mainly attributed to the weak reasoning capabilities huang2022towards, complex internal- and external- dependencies bairi2023codeplan, and context length limitation of LLMs bairi2023codeplan. Therefore, the pursuit of models that can handle more complex, repository- and software-level code generation opens up new avenues for automation in software development and makes programming more productive and accessible. Innovating model architectures tuned to code structures. Due to their scalability and effectiveness, Transformer-based LLM architectures have become dominant in solving code generation task. Nevertheless, they might not be optimally designed to capture the inherent structure and syntax of programming languages (PLs) guo2020graphcodebert,guo2022unixcoder,ma2022code,kou2023model. Code has a highly structured nature, with a syntax that is more rigid than natural language. This presents a unique challenge for LLMs, which are often derived from models that were originally designed for natural language processing (NLP). The development of novel model architectures that inherently understand and integrate the structural properties of code represents a significant opportunity to improve code generation and comprehension. Innovations such as tree-based neural networks mou2014tbcnn, which mirror the abstract syntax tree (AST) representation of code, can offer a more natural way for models to learn and generate programming languages. Additionally, leveraging techniques from the compiler theory, such as intermediate representations (IR) li2022unleashing, could enable models to operate on a more abstract and generalizable level, making them effective across multiple programming languages paul2024ircoder. By exploring architectures beyond the traditional sequential models, researchers can unlock new potentials in code generation. Curating high-quality code data for pre-training and fine-tuning of LLMs. The efficacy of LLMs largely depends on the quality and diversity of code datasets used during pre-training and fine-tuning phases zhou2024lima,kopf2024openassistant,wettig2024qurating. Currently, there is a scarcity of large, high-quality datasets that encompass a wide range of programming tasks, styles, and languages. This limitation constrains the ability of LLMs to generalize across unseen programming tasks, different coding environments, and real-world software development scenarios. The development of more sophisticated data acquisition techniques, such as automated code repositories mining linstead2007mining, advanced filtering algorithms, and code data synthesis liu2024best (see Section sec:data_synthesis), can lead to the creation of richer datasets. Collaborations with industry partners (e.g., GitHub) could also facilitate access to proprietary codebases, thereby enhancing the practical relevance of the training material. Furthermore, the adoption of open-source models for dataset sharing can accelerate the collective effort to improve the breadth and depth of code data available for LLM research. Developing comprehensive benchmarks and metrics for coding proficiency evaluation in LLMs. Current benchmarks like HumanEval may not capture the full spectrum of coding skills required for practical software development ni2023l2ceval. Additionally, metrics often focus on syntactic correctness or functional accuracy, neglecting aspects such as code efficiency peitek2021program, style chen2023duetcs, readability buse2009learning, or maintainability ardito2020tool. The design of comprehensive benchmarks that simulate real-world software development challenges could provide a more accurate assessment of LLMs' coding capabilities. These benchmarks should include diverse programming tasks of varying difficulty levels, such as debugging zhong2024ldb, refactoring shirafuji2023refactoring, and optimization ishida2024langprop, and should be complemented by metrics that evaluate qualitative aspects of code. The establishment of community-driven benchmarking platforms could facilitate continuous evaluation and comparison of LLMs for code generation across the industry and academia. Support for low-resource, low-level, and domain-specific programming languages. LLMs are predominantly trained in popular high-level programming languages, leaving low-resource, low-level, and domain-specific languages underrepresented. This lack of focus restricts the applicability of LLMs in certain specialized fields and systems programming thakur2023benchmarking. Intensifying research on transfer learning and meta-learning approaches may enable LLMs to leverage knowledge from high-resource languages to enhance their performance on less common ones chen2022transferability,cassano2023knowledge. Additionally, partnerships with domain experts can guide the creation of targeted datasets and fine-tuning strategies to better serve niche markets. The development of LLMs with a capacity for multilingual code generation also presents a significant opportunity for broadening the scope of applications. Continuous learning for LLMs to keep pace with evolving coding knowledge. The software development landscape is continuously evolving, with new languages, frameworks, and best practices emerging regularly. LLMs risk becoming outdated if they cannot adapt to these changes and incorporate the latest programming knowledge jang2022towards,wang2023knowledge. While retrieval augmented code generation mitigates these issues, the performance is limited by the quality of the retrieval context While retrieval-augmented code generation offers a partial solution to these issues, its effectiveness is inherently constrained by the quality of retrieved context. lu2022reacc,zhou2022docprompting,zhang2023repocoder. Therefore, establishing mechanisms for continuous learning and updating of LLMs can help maintain their relevance over time. This could involve real-time monitoring of code repositories to identify trends and innovations, as well as the creation of incremental learning systems that can assimilate new information without forgetting previously acquired knowledge. Engaging the LLMs in active learning scenarios where they interact with human developers may also foster ongoing knowledge acquisition. Ensuring code safety and aligning LLM outputs with human coding preferences. Ensuring the safety and security of code generated by LLMs is a paramount concern, as is their ability to align with human preferences and ethical standards. Current models may inadvertently introduce vulnerabilities or generate code that does not adhere to desired norms chen2021evaluating,yang2024robustness. Research into the integration of formal verification tools within the LLM pipeline can enhance the safety of the produced code. Additionally, developing frameworks for alignment learning that capture and reflect human ethical preferences can ensure that the code generation process aligns with societal values ouyang2022training,qi2023fine. Transparent and explainable AI methodologies can also contribute to building trust in the LLM-generated code by making the decision-making process more accessible to developers.",
      "origin_cites_number": 28
    },
    {
      "section_title": "Conclusion",
      "level": "1",
      "content": "In this survey, we provide a systematic literature review, serving as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. A thorough introduction and analysis for data curation, the latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications are illustrated. In addition, we present a historical overview of the evolution of LLMs for code generation in recent years and offer an empirical comparison using the widely recognized HumanEval, MBPP, and the more practical and challenging BigCodeBench benchmarks to highlight the progressive enhancements in LLM capabilities for code generation. Critical challenges and promising opportunities regarding the gap between academia and practical development are also identified for future investigation. Furthermore, we have established a dedicated resource website to continuously document and disseminate the most recent advances in the field. We hope this survey can contribute to a comprehensive and systematic overview of LLM for code generation and promote its thriving evolution. We optimistically believe that LLM will ultimately change all aspects of coding and automatically write safe, helpful, accurate, trustworthy, and controllable code, like professional programmers, and even solve coding problems that currently cannot be solved by humans. ACM-Reference-Format",
      "origin_cites_number": 0
    }
  ],
  "literature_review_id": 270214176,
  "meta_info": {
    "cite_counts": 350,
    "Conference_journal_name": "ACM Transactions on Software Engineering and Methodology",
    "influentialcitationcount": 21,
    "Author_info": {
      "Publicationsh": 6,
      "h_index": 5,
      "Citations": 687,
      "Highly Influential Citations": 0
    },
    "all_cites_title": [
      "Full Parameter Fine-tuning Code Alpaca",
      "Prompting Engineering (Sec. 5.6) Reflexion[236], LATS[327], Self-Debugging[51], SelfEvolve[122]",
      "Theo X. et al.[195], CodeT[45], LEVER[190], AlphaCodium[224] Repository Level & Long Context (Sec. 5.7) RepoCoder[309], CoCoMIC[69], RepoHyper[209], RLPG[240] Repoformer[282], RepoFusion[239], ToolGen[259], CodePlan[22] CodeS[308] Retrieval Augmented (Sec. 5.8) HGNN[166], REDCODER[205], ReACC[171], DocPrompting[330] RepoCoder[309], Su et al.[242]",
      "Autonomous Coding Agents (Sec. 5.",
      "AgentCoder [104], MetaGPT[100], CodeAct [265], AutoCodeRover [316], Devin[61] OpenDevin[199], SWE-agent[124], L2MAC[98], OpenDevin CodeAct 1.0[287]",
      "Evaluation (Sec. 5.10)",
      "AgentGPT: Assemble, configure, and deploy autonomous AI Agents in your browser",
      "AutoGPT is the vision of accessible AI for everyone",
      "Phi-3 technical report: A highly capable language model locally on your phone",
      "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "A Transformer-based Approach for Source Code Summarization",
      "Unified pre-training for program understanding and generation",
      "Traces of memorisation in large language models for code",
      "SantaCoder: don't reach for the stars! arXiv preprint",
      "Mining idioms from source code",
      "What is CodeWhisperer?",
      "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
      "J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.",
      "The Claude 3 Model Family: Opus, Sonnet, Haiku",
      "A tool-based perspective on software code maintainability metrics: a systematic literature review",
      "Multi-lingual evaluation of code generation models",
      "Program synthesis with large language models",
      "Layer normalization",
      "StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code",
      "Qwen technical report",
      "Constitutional ai: Harmlessness from ai feedback",
      "Shashank Shet, et al. 2023. Codeplan: Repository-level coding using llms and planning",
      "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "A methodology for controlling bias and fairness in synthetic data generation",
      "Grounded copilot: How programmers interact with code-generating models",
      "Deepseek llm: Scaling open-source language models with longtermism",
      "Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback",
      "Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools",
      "Gpt-neox-20b: An open-source autoregressive language model",
      "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow",
      "Educating software and AI stakeholders about algorithmic fairness, accountability, transparency and ethics",
      "On the opportunities and risks of foundation models",
      "Language models are few-shot learners",
      "Learning a metric for code readability",
      "Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts",
      "A survey on mixture of experts",
      "J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.",
      "Extracting training data from large language models",
      "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs",
      "A scalable and extensible approach to benchmarking nl2code for 18 programming languages",
      "ERNIE-Code: Beyond english-centric cross-lingual pretraining for programming languages",
      "Training and evaluating a jupyter notebook data science assistant",
      "A survey on evaluation of large language models",
      "Code Alpaca: An Instruction-following LLaMA model for code generation",
      "DUETCS: Code Style Transfer through Generation and Retrieval",
      "Codet: Code generation with generated tests",
      "On the transferability of pre-trained language models for low-resource programming languages",
      "Benchmarking large language models in retrieval-augmented generation",
      "Evaluating large language models trained on code",
      "Evaluation metrics for language models",
      "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
      "Teaching large language models to self-debug",
      "Tree-to-tree neural networks for program translation",
      "Reducing the Carbon Impact of Generative AI Inference (today and in 2035)",
      "Palm: Scaling language modeling with pathways",
      "Pangu-coder: Program synthesis with function-level language modeling",
      "Scaling instruction-finetuned language models",
      "PyMT5: multi-mode translation of natural language and Python code with transformers",
      "Training verifiers to solve math word problems",
      "CodeGemma Team, Ale Jakse Hartman, Andrea Hu, Christopher A. Choquette-Choo, Heri Zhao, Jane Fine, Jeffrey Hui, Jingyue Shen, Joe Kelley, Joshua Howland, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Nam Nguyen, Paul Michel, J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.",
      "CodeGemma: Open Code Models Based on Gemma",
      "Free, ultrafast Copilot alternative for Vim and Neovim",
      "Introducing Devin, the first AI software engineer",
      "Inducing tree-substitution grammars",
      "Cognitive Computations. 2023. oa_leet10k",
      "An efficient SMT solver",
      "Qlora: Efficient finetuning of quantized llms",
      "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models",
      "Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion",
      "Cocomic: Code completion by jointly modeling in-file and cross-file context",
      "A survey on in-context learning",
      "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback",
      "Evaluating large language models in class-level code generation",
      "Training CodeParrot from Scratch",
      "Large language models for software engineering: Survey and open problems",
      "Automated repair of programs from large language models",
      "Codebert: A pre-trained model for programming and natural languages",
      "Incoder: A generative model for code infilling and synthesis",
      "The pile: An 800gb dataset of diverse text for language modeling",
      "Pal: Program-aided language models",
      "Retrieval-augmented generation for large language models: A survey",
      "AST-T5: Structure-Aware Pretraining for Code Generation and Understanding",
      "Cruxeval: A benchmark for code reasoning, understanding and execution",
      "Dimensions in program synthesis",
      "J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.",
      "Textbooks are all you need",
      "UniXcoder: Unified Cross-Modal Pre-training for Code Representation",
      "Graphcodebert: Pre-training code representations with data flow",
      "Longcoder: A long-range pre-trained language model for code completion",
      "DeepSeek-Coder: When the Large Language Model Meets Programming-The Rise of Code Intelligence",
      "Transitioning from Real to Synthetic data: Quantifying the bias in model",
      "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",
      "CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models",
      "Evaluating large language models in generating synthetic hci research data: a case study",
      "Reasoning with language model is planning with world model",
      "Deep residual learning for image recognition",
      "Measuring mathematical problem solving with the math dataset",
      "GitHub on BigQuery: Analyze all the open source code",
      "Training compute-optimal large language models",
      "L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation",
      "The curious case of neural text degeneration",
      "Metagpt: Meta programming for multi-agent collaborative framework",
      "Large Language Models for Software Engineering: A Systematic Literature Review",
      "Parameter-efficient transfer learning for NLP",
      "Lora: Low-rank adaptation of large language models",
      "AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation",
      "Towards reasoning in large language models: A survey",
      "Towards Reasoning in Large Language Models: A Survey",
      "Execution-based evaluation for data science code generation models",
      "Position Paper: Agent AI Towards a Holistic Intelligence",
      "Qwen2. 5-coder technical report",
      "Codesearchnet challenge: Evaluating the state of semantic code search",
      "Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization",
      "LangProp: A code optimization framework using Language Models applied to driving",
      "Mapping Language to Code in Programmatic Context",
      "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "Towards Continual Knowledge Learning of Language Models",
      "Perplexity-a measure of the difficulty of speech recognition tasks",
      "Oracle-guided component-based program synthesis",
      "Ai alignment: A comprehensive survey",
      "Question selection for interactive program synthesis",
      "Benchmarking and explaining large language modelbased code generation: A causality-centric approach",
      "CodeUp: A Multilingual Code Generation Llama2 Model with Parameter-Efficient Instruction-Tuning",
      "Selfevolve: A code evolution framework via large language models",
      "SWE-bench: Can Language Models Resolve Real-world Github Issues?",
      "SWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING",
      "A formalism for dependency grammar based on tree adjoining grammar",
      "Repair is nearly generation: Multilingual program repair with llms",
      "Scaling laws for neural language models",
      "Md Rizwan Parvez, and Shafiq Joty. 2023. xcodeeval: A large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval",
      "J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.",
      "sDPO: Don't Use Your Data All at Once",
      "Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling",
      "Systematic literature reviews in software engineering-a systematic literature review",
      "The Stack: 3 TB of permissively licensed source code",
      "Openassistant conversations-democratizing large language model alignment",
      "Is model attention aligned with human attention? an empirical study on large language models for code generation",
      "Unsupervised translation of programming languages",
      "DS-1000: A natural and reliable benchmark for data science code generation",
      "The bigscience roots corpus: A 1.6 tb composite multilingual dataset",
      "Synthetic data: save money, time and carbon with open source",
      "Coderl: Mastering code generation through pretrained models and deep reinforcement learning",
      "Bloom: A 176b-parameter open-access multilingual language model",
      "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
      "The power of scale for parameter-efficient prompt tuning",
      "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories",
      "Towards enhancing in-context learning for code generation",
      "Static analysis of android apps: A systematic literature review",
      "Starcoder: may the source be with you! arXiv preprint",
      "AlpacaEval: An Automatic Evaluator of Instruction-following Models",
      "Prefix-tuning: Optimizing continuous prompts for generation",
      "J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.",
      "Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee",
      "Competition-level code generation with alphacode",
      "Unleashing the power of compiler intermediate representation to enhance neural program embeddings",
      "Scaling down to scale up: A guide to parameter-efficient fine-tuning",
      "Holistic Evaluation of Language Models",
      "Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators",
      "Rouge: A package for automatic evaluation of summaries",
      "Xiangyang Liu, and Xipeng Qiu. 2022. A survey of transformers",
      "Mining internet-scale software repositories",
      "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model",
      "Mftcoder: Boosting code llms with multitask fine-tuning",
      "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
      "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation",
      "Rltf: Reinforcement learning from unit test feedback",
      "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "Best Practices and Lessons Learned on Synthetic Data for Language Models",
      "Retrieval-Augmented Generation for Code Summarization via Hybrid GNN",
      "Repobench: Benchmarking repository-level code autocompletion systems",
      "Uncovering and quantifying social biases in code generation",
      "Deep learning for android malware defenses: a systematic literature review",
      "StarCoder 2 and The Stack v2: The Next Generation",
      "ReACC: A Retrieval-Augmented Code Completion Framework",
      "J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.",
      "Codexglue: A machine learning benchmark dataset for code understanding and generation",
      "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
      "Automatic Programming: Large Language Models and Beyond",
      "Are Code Pre-trained Models Powerful to Learn Code Syntax and Semantics? arXiv preprint",
      "Self-refine: Iterative refinement with self-feedback",
      "An overview of Bard: an early experiment with generative AI",
      "STYLE-ANALYZER: fixing code style inconsistencies with interpretable unsupervised algorithms",
      "Generating training data with language models: Towards zero-shot language understanding",
      "Introducing Meta Llama 3: The most capable openly available LLM to date",
      "Phi-2: The surprising power of small language models",
      "TBCNN: A tree-based convolutional neural network for programming language processing",
      "An investigation into misuse of java security apis by large language models",
      "A simple, yet effective approach to finding biases in code generation",
      "Reading between the lines: Modeling user behavior and costs in AI-assisted programming",
      "Octopack: Instruction tuning code large language models",
      "Armando Solar-Lezama, Koushik Sen, and Ion Stoica",
      "The attack of the clones: A study of the impact of shared code on vulnerability patching",
      "Lever: Learning to verify language-to-code generation with execution",
      "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models",
      "Lessons for training llms on programming and natural languages",
      "Codegen: An open large language model for code with multi-turn program synthesis",
      "Deep learning meets software engineering: A survey on pre-trained models of source code",
      "J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.",
      "Is Self-Repair a Silver Bullet for Code Generation?",
      "Chatgpt: Optimizing language models for dialogue",
      "OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/.",
      "New models and developer products announced at DevDay",
      "OpenDevin: Code Less, Make More",
      "Training language models to follow instructions with human feedback",
      "Fine-tuning or retrieval? comparing knowledge injection in llms",
      "Evaluating and explaining large language models for code using syntactic structures",
      "Bleu: a method for automatic evaluation of machine translation",
      "The Fact Selection Problem in LLM-Based Program Repair",
      "Retrieval Augmented Code Generation and Summarization",
      "Evaluating In-Context Learning of Libraries for Code Generation",
      "Goran GlavaÅ¡, and Iryna Gurevych",
      "Program comprehension and code complexity metrics: An fmri study",
      "RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion",
      "Stable Code Technical Report",
      "Train short, test long: Attention with linear biases enables input length extrapolation",
      "Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint",
      "Improving language understanding by generative pre-training",
      "Language models are unsupervised multitask learners",
      "Measuring software library stability through historical version analysis",
      "Direct preference optimization: Your language model is secretly a reward model",
      "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "Evaluating the text-to-sql capabilities of large language models",
      "A systematic review of interaction in search-based software engineering",
      "J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.",
      "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "Codebleu: a method for automatic evaluation of code synthesis",
      "Idea to software, fast",
      "Replit. 2023. replit-code-v1-3b",
      "Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering",
      "Evol-Instruct-Code-80k",
      "The programmer's assistant: Conversational interaction with a large language model for software development",
      "Code llama: Open foundation models for code",
      "From words to watts: Benchmarking the energy costs of large language model inference",
      "Multitask Prompted Training Enables Zero-Shot Task Generalization",
      "Proximal policy optimization algorithms",
      "You autocomplete me: Poisoning vulnerabilities in neural code completion",
      "Self-attention with relative position representations",
      "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "Boosting large language models for code with ranking feedback",
      "Greening large language models of code",
      "Reflexion: Language agents with verbal reinforcement learning",
      "Refactoring Programs Using Large Language Models with Few-Shot Examples",
      "Execution-based code generation using deep reinforcement learning",
      "Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak",
      "Repository-level prompt generation for large language models of code",
      "Codefusion: A pre-trained diffusion model for code generation",
      "ARKS: Active Retrieval in Knowledge Soup for Code Generation",
      "Roformer: Enhanced transformer with rotary position embedding",
      "Intellicode compose: Code generation using transformer",
      "J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.",
      "Code translation with compiler representations",
      "AI Code Completions",
      "Stanford Alpaca: An Instruction-following LLaMA model",
      "Gemma: Open models based on gemini research and technology",
      "Code with CodeQwen1",
      "Benchmarking large language models for automated verilog rtl code generation",
      "The evolved code alpaca dataset",
      "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "Open foundation and fine-tuned chat models",
      "Natural language processing with transformers",
      "Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models",
      "Synthetic data, real errors: how (not) to publish and use synthetic data",
      "Attention is all you need",
      "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
      "Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code Generation",
      "Software testing with large language models: Survey, landscape, and vision",
      "A survey on large language model based autonomous agents",
      "Machine/deep learning for software engineering: A systematic literature review",
      "ReCode: Robustness Evaluation of Code Generation Models",
      "Knowledge editing for large language models: A survey",
      "Executable code actions elicit better llm agents",
      "Compilable Neural Code Generation with Compiler Feedback",
      "Self-consistency improves chain of thought reasoning in language models",
      "J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.",
      "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
      "Code completion by modeling flattened abstract syntax trees as graphs",
      "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
      "Aligning large language models with human: A survey",
      "Execution-based evaluation for open-domain code generation",
      "Finetuned language models are zero-shot learners",
      "Emergent Abilities of Large Language Models",
      "Chain-of-thought prompting elicits reasoning in large language models",
      "Towards greener yet powerful code generation via quantization: An empirical study",
      "Magicoder: Source code is all you need",
      "LLM-powered Autonomous Agents. lilianweng.github.io",
      "QuRating: Selecting High-Quality Data for Training Language Models",
      "Fake it till you make it: face analysis in the wild using synthetic data alone",
      "Repoformer: Selective Retrieval for Repository-Level Code Completion",
      "Autogen: Enabling next-gen llm applications via multi-agent conversation framework",
      "The rise and potential of large language model based agents: A survey",
      "Rui Xie, Zhengran Zeng, Zhuohao Yu, Chang Gao, Shikun Zhang, and Wei Ye. 2024. CodeShell Technical Report. arXiv preprint arXiv:2403.15747 (2024).",
      "Data selection for language models via importance resampling",
      "Introducing OpenDevin CodeAct 1.0, a new State-of-the-art in Coding Agents",
      "Precise condition synthesis for program repair",
      "Wizardlm: Empowering large language models to follow complex instructions",
      "A systematic evaluation of large language models of code",
      "Shin Hwei Tan, and Pinjia He. 2024. Aligning LLMs for FL-free Program Repair",
      "A First Look at License Compliance Capability of LLMs in Code Generation",
      "Robustness, security, privacy, explainability, efficiency, and usability of large language models for code",
      "Unveiling memorization in code models",
      "Tree of thoughts: Deliberate problem solving with large language models",
      "ReAct: Synergizing Reasoning and Acting in Language Models",
      "Learning to mine aligned code and natural language pairs from stack overflow",
      "HyperCLOVA X Technical Report",
      "Codereval: A benchmark of pragmatic code generation with generative pre-trained models",
      "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task",
      "Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation",
      "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
      "Rrhf: Rank responses to align language models with human feedback without tears",
      "Ding Naman Jain Harm de Vries Leandro von Werra Arjun Guha Lingming Zhang Yuxiang Wei, Federico Cassano",
      "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
      "CERT: continual pre-training on sketches for library-oriented code generation",
      "Large Language Models Meet NL2Code: A Survey",
      "CodeS: Natural Language to Code Repository via Multi-Layer Sketch",
      "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation",
      "Pydex: Repairing bugs in introductory python assignments using llms",
      "Automated feedback generation for competition-level code",
      "Adaptive budget allocation for parameter-efficient fine-tuning",
      "Instruction tuning for large language models: A survey",
      "NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts",
      "Siren's song in the AI ocean: a survey on hallucination in large language models",
      "AutoCodeRover: Autonomous Program Improvement",
      "Unifying the perspectives of nlp and software engineering: A survey on language models for code",
      "Xiachong Feng, Bin Qin, and Ting Liu",
      "A survey of large language models",
      "Judging llm-as-a-judge with mt-bench and chatbot arena",
      "Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x",
      "OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement",
      "Outline, then details: Syntactically guided coarse-to-fine code generation",
      "A survey of large language models for code: Evolution, benchmarking, and future trends",
      "LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step",
      "Solving challenging math word problems using gpt-4 code interpreter with code-based selfverification",
      "Language agent tree search unifies reasoning acting and planning in language models",
      "Lima: Less is more for alignment",
      "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
      "DocPrompting: Generating Code by Retrieving the Docs",
      "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
      "ICE-Score: Instructing Large Language Models to Evaluate Code",
      "Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions",
      "J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.",
      "Leandro von Werra, Harm de Vries",
      "J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018."
    ]
  }
}