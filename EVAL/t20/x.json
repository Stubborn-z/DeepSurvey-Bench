{
  "survey": "Graph retrieval-augmented generation (RAG) represents a significant advancement in information processing by integrating graph neural networks and knowledge graphs to enhance information retrieval and content generation. This survey explores the transformative impact of RAG systems, focusing on their ability to leverage structured semantic information for improved reasoning and prediction capabilities in complex tasks such as Knowledge Base Question Answering (KBQA). The integration of graph structures enhances the reasoning faculties of large language models (LLMs), addressing bottlenecks in domains like pharmaceutical regulation and industrial knowledge management. Techniques such as multi-hop reasoning, hybrid retrieval models, and advanced neural network methodologies are examined for their role in optimizing retrieval processes and enhancing model performance. Despite challenges related to scalability, data quality, and ethical considerations, RAG systems offer promising directions for future research, emphasizing the need for robust frameworks that effectively synthesize multifaceted information. The survey concludes by highlighting the potential of RAG to revolutionize information retrieval and content generation, paving the way for scalable solutions across diverse applications.\n\nIntroduction Motivation and Relevance to Current Research Trends Graph retrieval-augmented generation (RAG) signifies a transformative advancement in information processing, dynamically integrating contextually relevant data from interconnected graph structures to enhance the predictive and reasoning capabilities of advanced models [1]. This integration is crucial for improving Knowledge Base Question Answering (KBQA) systems, addressing previous limitations in coverage and generalization, and effectively answering complex queries by leveraging the synergy of pre-trained language models and knowledge graphs. RAG's sophistication lies in its ability to enrich semantic interpretations, particularly in knowledge-intensive tasks where interconnected symbolic representations are essential [2]. The strategic incorporation of graph structures significantly enhances the reasoning abilities of large language models (LLMs), which often struggle with intricate queries requiring external knowledge [3]. This enhancement is particularly vital in fields such as pharmaceutical regulation and industrial knowledge management, where traditional systems encounter substantial challenges due to the complexity of navigating extensive datasets [4]. Furthermore, GRAG, a specific variant of RAG, markedly improves information retrieval from networked documents, refining conventional retrieval-augmented methodologies [5]. Significance of Graph Retrieval-Augmented Generation Graph retrieval-augmented generation (RAG) represents a significant leap in artificial intelligence-generated content by integrating advanced retrieval techniques that overcome the static limitations of traditional LLMs. By incorporating up-to-date external information, RAG enhances the accuracy and reliability of outputs, particularly in knowledge-intensive tasks such as KBQA, which increasingly require intricate inference over multiple knowledge base triples. Recent advancements, including models like GRAFT-Net, showcase substantial improvements by merging knowledge bases with entity-linked text, effectively addressing the challenges posed by incomplete KBs through both Information Retrieval-based and Neural Semantic Parsing-based approaches [6,7,8]. The innovative GRAG framework exemplifies this by utilizing a divide-and-conquer strategy for optimal subgraph retrieval, seamlessly integrating textual and graph information into LLMs, thereby bolstering their reasoning capabilities [5]. Similarly, the RAG4DyG method demonstrates enhanced predictive accuracy and adaptability for dynamic graph modeling, further underscoring RAG's superiority over traditional methods [1]. The introduction of GRBench signifies a crucial advancement in evaluating LLM reasoning capabilities through graph structures, offering a more dynamic and interconnected approach compared to traditional benchmarks [9]. This fosters models' abilities to engage in multi-hop reasoning and effectively utilize interconnected knowledge bases. The Think-on-Graph (ToG) method further illustrates the potential of treating LLMs as agents that interactively explore knowledge graphs, enhancing reasoning capabilities based on retrieved knowledge [10]. Incorporating passage retrieval has significantly improved performance in open-domain question answering, indicating promising future research directions [11]. The History Semantic Graph Enhanced KBQA model (HSGE) showcases the efficacy of graph-based approaches in modeling long-range semantic dependencies in conversation history while maintaining computational efficiency [12]. Additionally, the KALMV method systematically verifies and corrects errors in language model outputs, enhancing the factual accuracy of generated texts [13]. The integration of LLMs with Graph Machine Learning (Graph ML) substantially boosts model performance, particularly in generalization and efficiency with fewer labeled examples [14]. This integration fosters more robust and scalable information systems capable of navigating complex datasets and improving decision-making processes across various domains. The BEQUE framework further enhances long-tail query rewriting performance, demonstrating its positive impact on e-commerce metrics [15]. RAG's significance extends to revolutionizing educational practices by enhancing learning experiences for both students and teachers [16]. The alignment of model outputs with user intent, as exemplified by the InstructGPT model, leads to improved user satisfaction and reduced undesirable outputs [17]. The transformative impact of graph retrieval-augmented generation lies in its potential to enhance LLM performance, applicability, and efficiency across diverse domains, paving the way for future research and development [18]. Structure of the Survey This survey is systematically organized to provide an in-depth exploration of graph retrieval-augmented generation (RAG), commencing with an introduction that articulates the motivation and significance of this emerging field. The paper is divided into several key sections that collectively deepen the understanding of the topic. The Introduction establishes the importance of RAG within current research trends, followed by an examination of its significance and potential impact across various domains. This foundational overview is essential for contextualizing subsequent discussions. The Background and Definitions section delves into core concepts integral to RAG, including Graph Neural Networks (GNNs), Knowledge Graphs (KGs), and Information Retrieval (IR). It elucidates the definitions and interrelations of these components, providing a basis for understanding their collective enhancement of retrieval-augmented generation processes. The role of Graph Neural Networks in RAG is explored in detail, emphasizing advancements in architectures and frameworks that leverage these networks to improve content generation, underscoring the technological innovations driving RAG's effectiveness. Following this, the Knowledge Graphs and Their Integration section analyzes how KGs are utilized to enhance the accuracy and relevance of generated content. Techniques for effective integration and reasoning over knowledge graphs are discussed, illustrating their pivotal role in the generation process. The survey further investigates advanced Information Retrieval Techniques, focusing on methods for retrieving relevant subgraphs and triplets from knowledge graphs, along with innovative ranking and reranking strategies. These techniques aim to enhance content generation by surfacing long-tail knowledge, mitigating information overload, and improving the accuracy and robustness of AI-generated content through RAG frameworks. Additionally, it highlights the integration of graph neural networks and hybrid models to optimize retrieval performance in complex question-answering and knowledge graph link prediction tasks [19,20,21,22]. The exploration of hybrid and dual-encoder retrieval models further illustrates the sophisticated methodologies employed in RAG. Applications of RAG in Information Retrieval are showcased, demonstrating its impact on enhancing question-answering systems and its applicability in domain-specific contexts, providing practical insights into RAG's real-world utility. The survey concludes with a discussion on Challenges and Future Directions, identifying current limitations and proposing potential avenues for future research. This exploration encompasses scalability, efficiency, data quality, interpretability, and ethical considerations, offering a comprehensive roadmap for advancing the field by addressing the integration of large language models with knowledge graphs, optimizing generative models for open-domain question answering, enhancing complex question answering over knowledge bases, improving patent phrase similarity matching, and understanding the impact of generative AI in education [11,23,24,25,8]. Through this structured approach, the survey aims to provide a nuanced understanding of graph retrieval-augmented generation (GRAG), highlighting its ability to enhance query-focused summarization, capture long-tail biomedical knowledge, and integrate textual and topological information for improved AI-generated content. By examining the components, applications, and future potential of GRAG, the survey facilitates a deeper exploration into how these techniques address the limitations of traditional retrieval-augmented generation methods, offering valuable insights for researchers and practitioners across fields ranging from biomedical research to AI content generation [20,5,22,26].The following sections are organized as shown in . Background and Definitions Graph Neural Networks (GNNs) Graph Neural Networks (GNNs) are essential for analyzing graph-structured data by leveraging graph connectivity to improve data processing and model performance [27]. GNNs employ neural network methodologies to dynamically integrate information from interconnected nodes, crucial for modeling complex interactions in fields like financial analysis and industrial knowledge management [28]. Notable architectures such as Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs) have advanced GNN capabilities. GCNs, exemplified by the PullNet framework, enhance retrieval and reasoning through neighborhood data aggregation, despite challenges like overfitting and over-smoothing [29]. GATs improve upon this by using self-attention mechanisms to evaluate neighboring features' importance, capturing intricate semantic relationships [27]. Integrating transformer architectures with graph data underscores GNNs' role in enhancing model understanding and predictive abilities [5]. In zero-shot entity linking tasks, GNNs effectively process graph-structured data to connect entities across contexts without prior examples [27]. They also play a crucial role in knowledge graph link prediction, using node neighborhoods to infer new facts, enriching information beyond mere query data [28]. Challenges remain, particularly regarding large language models (LLMs) and their domain-specific applications. The hallucination phenomenon in LLMs, where models generate plausible yet factually incorrect content, necessitates further investigation [30]. Ongoing research into GNNs promises to revolutionize graph data processing, especially in specialized domains like medical LLM training, where tailored training paradigms are essential [1]. Knowledge Graphs (KGs) Knowledge Graphs (KGs) are sophisticated data structures encapsulating complex relationships among entities, enhancing reasoning and inference in various natural language processing (NLP) tasks. They synthesize interconnected entities and attributes for a holistic understanding, crucial in Knowledge Base Question Answering (KBQA) by navigating extensive knowledge bases to provide factual information [31]. Integrating KGs into large language models (LLMs) addresses traditional knowledge base approaches' limitations, which often struggle with restrictive and sparse data representations [2]. KGs bolster LLM reasoning capabilities, particularly in zero-shot question answering tasks, where explicit knowledge representations are vital for accuracy [3]. Frameworks utilizing KGs to identify answer entities multiple hops away from topic entities enhance long-form answers in specialized domains [6]. Despite their potential, KGs face challenges related to non-overlapping vocabularies of entities and relations, complicating representation transferability across graphs [2]. Nevertheless, KGs remain central to improving information retrieval accuracy and the quality of generated answers by LLMs, as evidenced by frameworks optimizing reasoning paths and knowledge selection [31]. KGs significantly contribute to link prediction, inferring missing relationships within graphs and enriching datasets with new connections [6]. Their construction to represent relationships between document passages further emphasizes their role in enhancing information retrieval accuracy [3]. Through these capabilities, KGs facilitate nuanced data interpretations, advancing AI systems' abilities to navigate complex information landscapes and ensuring robust solutions across diverse domains. Information Retrieval (IR) Information retrieval (IR) is fundamental in graph retrieval-augmented generation, extracting relevant information from extensive datasets to support complex reasoning tasks [6]. Traditionally, IR models like TF-IDF and BM25 have been effective for selecting candidate contexts in open-domain question answering. However, these methods often fall short of retrieval-augmented generation systems' nuanced demands, requiring advanced techniques to integrate external evidence into generative models [15]. Integrating IR with large language models (LLMs) enhances performance by aligning model outputs with user intent, optimizing long-tail queries in domains like e-commerce, where retrieving specific information significantly influences search outcomes [15]. Moreover, dynamically updating knowledge and retrieving subgraphs are vital for KBQA, enabling effective reasoning and accurate answer generation in complex information environments [6]. In graph retrieval-augmented generation, challenges of traditional IR methods, such as generating inaccurate or irrelevant information, highlight the necessity for innovative retrieval techniques leveraging external knowledge sources. These techniques enhance language model outputs' accuracy and relevance, facilitate pertinent document retrieval, and improve compliance and efficacy in specialized domains like pharmaceutical regulation [6]. IR plays a critical role in graph retrieval-augmented generation by providing a robust framework for retrieving and integrating external evidence, enhancing generative models' precision. This is evident in biomedical research, where traditional large language models may overlook rare but significant information. Advanced retrieval methods like knowledge graphs and hybrid models capture long-tail knowledge and mitigate information overload. Frameworks such as retrieve-and-read illustrate improved link prediction in knowledge graphs by emphasizing salient context information. Integrating passage retrieval with generative models in open-domain question answering demonstrates how retrieving multiple evidence sources enhances model performance, underscoring IR's indispensable nature in refining generative tasks [21,11,22]. This highlights the ongoing need for advancements in IR techniques to meet evolving demands of complex information systems, ensuring robust solutions across diverse applications. Interrelation of Core Concepts The interrelation of Graph Neural Networks (GNNs), Knowledge Graphs (KGs), and Information Retrieval (IR) establishes a comprehensive framework enhancing retrieval-augmented generation systems. This interaction overcomes traditional methodologies' limitations by enabling the synthesis and processing of complex, structured information. GNNs utilize KGs' structural intricacies, augmenting large language models' (LLMs) reasoning capabilities and facilitating more accurate content generation [2]. This synergy addresses reasoning biases in Knowledge Base Question Answering (KBQA), where existing methods often retrieve incomplete subgraphs without intermediate supervision, leading to biased reasoning paths [32]. Knowledge Graphs serve as repositories of interconnected entities and relationships, providing semantic context necessary for effective reasoning and inference. Integrating KGs with GNNs and IR mechanisms enables extraction of both direct and indirect semantic relationships, mitigating pre-trained language models' (PLMs) limitations in handling structured knowledge bases. This integration addresses complex questions involving multiple subjects, compound relations, or numerical operations, overcoming existing methods' inadequacies in synthesizing multifaceted information [33]. Information Retrieval aligns model outputs with user intent, supplying authoritative and up-to-date knowledge enriching LLM capabilities. The interrelation of IR with GNNs and KGs is exemplified by frameworks like GraphRAG, utilizing entity knowledge graphs and community summaries to tackle complex queries [5]. Integrating jargon identification and clarification processes enhances retrieval-augmented generation frameworks' effectiveness [34]. The PullNet framework illustrates iterative retrieval processes' robustness, linking reasoning and retrieval to improve multi-hop question answering [29]. Despite these advancements, challenges persist, such as limited question variety and multilingual support in existing benchmarks, restricting applicability in real-world scenarios [14]. The GRBench benchmark underscores GNNs, KGs, and IR interrelation, utilizing graph structures to enhance reasoning performance in knowledge-intensive tasks [35]. Additionally, the relationship between knowledge graphs and information retrieval is emphasized by methods leveraging KGs to mitigate information overload, ensuring focused and relevant retrieval processes [32]. In recent years, the field of retrieval-augmented generation has witnessed significant advancements, particularly through the incorporation of Graph Neural Networks (GNNs). These developments not only enhance the generative capabilities of models but also offer a robust framework for future research. illustrates the hierarchical structure of these advancements, categorizing the integration and optimization of GNN architectures alongside innovative methodologies and model enhancements. The figure emphasizes the transformative potential of GNNs across various domains, underscoring their pivotal role in refining generative models and providing a flexible foundation for ongoing research endeavors. By visualizing these relationships, we gain a clearer understanding of how GNNs can be leveraged to advance the field further. Graph Neural Networks in Retrieval-Augmented Generation Advancements in Graph Neural Network Architectures Recent advancements in Graph Neural Network (GNN) architectures have significantly improved retrieval-augmented generation systems, optimizing the extraction and utilization of structured graph data for complex reasoning tasks. illustrates these advancements, focusing on retrieval mechanisms, knowledge integration, and efficiency improvements. Each category highlighted in the figure corresponds to key frameworks and methodologies that contribute to the evolution of retrieval-augmented generation systems. Frameworks like PullNet exemplify the integration of GNNs with retrieval mechanisms, enhancing multi-hop question answering by leveraging graph structures for intricate queries [29]. The GRAG framework employs a divide-and-conquer strategy, optimizing subgraph retrieval to enhance content understanding and generation [5], showcasing GNNs' efficiency in managing large volumes of graph-structured data. Innovative methodologies such as Think-on-Graph (ToG) facilitate interactive exploration of Knowledge Graphs (KGs) via beam search, discovering optimal reasoning paths and improving content generation [10]. The TIARA model further advances the field by retrieving multiple relevant components from knowledge bases, essential for Knowledge Base Question Answering (KBQA) [36]. Large Knowledge Models (LKMs) integrate symbolic knowledge with large language models (LLMs), enhancing cognitive alignment and reasoning capabilities [2], vital for interpretability and reliability in knowledge-intensive tasks. The Sentence-BERT (SBERT) architecture significantly reduces the time required to identify similar sentence pairs while maintaining accuracy [35], crucial for real-time applications. InstructGPT incorporates human feedback to refine language model performance, aligning outputs with user needs [17]. These advancements continue to drive the evolution of retrieval-augmented generation systems, enhancing generalization, transferability, and few-shot learning capabilities across diverse domains such as education, open-domain question answering, and graph machine learning [24,11,14]. Frameworks and Methodologies The integration of Graph Neural Networks (GNNs) into retrieval-augmented generation has spurred the development of various frameworks and methodologies designed to enhance language models' reasoning and generative capabilities. The Knowledge-Driven Chain-of-Thought (KD-CoT) framework leverages structured interactions between large language models (LLMs) and a question-answering system to improve reasoning accuracy in knowledge-intensive tasks [30]. Golden-Retriever augments questions by identifying domain-specific jargon, enhancing document retrieval from industrial knowledge bases [37]. The RAG4DyG framework incorporates contextually and temporally relevant examples from broader graph structures to enhance dynamic graph predictions, utilizing a contrastive learning module to optimize information retrieval [1]. Research often categorizes into semantic parsing-based methods and information retrieval-based methods, providing insights into procedural differences in addressing the Knowledge Base Question Answering (KBQA) task. The \"Retrieve-Rewrite-Answer\" framework transforms knowledge graph (KG) data into textualized statements to enhance LLM performance in KGQA, while the \"ChatKBQA\" framework adopts a generate-then-retrieve approach, leveraging fine-tuned LLMs for efficient knowledge retrieval and semantic parsing [38,39]. These frameworks illustrate the transformative potential of integrating GNNs into retrieval-augmented generation systems, enhancing generative models' capabilities in domains such as open-domain question answering, social network analysis, and molecular discovery. The synergy between GNNs and Large Language Models (LLMs) provides a flexible framework for aggregating evidence, enhancing reasoning, and reducing reliance on labeled data, paving the way for promising future research directions [40,14,41,11]. Knowledge Graphs and Their Integration The integration of knowledge graphs (KGs) within retrieval-augmented generation (RAG) systems is pivotal for enhancing the reasoning capabilities and semantic understanding of large language models (LLMs). This section delves into the roles of KGs, highlighting their contributions to structured knowledge representation and their impact on generative system performance. An exploration of KGs within RAG frameworks underscores their importance in facilitating accurate information retrieval and nuanced content generation. Function of Knowledge Graphs Knowledge graphs (KGs) are crucial in RAG systems, providing structured knowledge representations that enhance the reasoning and factual accuracy of LLMs. They assist in linking relevant entity mentions to unseen entities, supporting zero-shot learning scenarios and improving response precision [32]. By organizing complex data relationships, KGs enable nuanced reasoning over structured semantic information, essential for effective content generation. Frameworks like TIARA and GRAG utilize KGs to identify key entities and logical forms, integrating both textual and topological data to enhance answer accuracy. The KG-Agent framework exemplifies the dynamic role of KGs in RAG, where iterative decision-making and reasoning over KGs facilitate continuous knowledge memory updates [31]. This structured navigation refines information retrieval, ensuring the accuracy and relevance of generated outputs. In question-answering systems, KGs streamline the retrieval of pertinent information, guiding LLMs through logical reasoning processes to resolve complex queries [3]. The GRAG method integrates textual and topological information, enhancing the retrieval and generation of contextually accurate content [5]. This integration is vital for RAG, where layered reasoning and structured knowledge synthesis are pivotal for comprehensive question answering. Techniques like GraphRAG leverage KGs to build entity indexes and generate community summaries, improving the comprehensiveness and diversity of answers for global sensemaking questions. HybridRAG combines KGs with vector retrieval methods to enhance information extraction and question-answering accuracy, particularly in intricate domains such as finance and biomedical research, where capturing long-tail knowledge is essential [42,22,26]. As illustrated in , the multifaceted role of knowledge graphs in enhancing LLMs is depicted, showcasing their utilization in various frameworks and their impact on information retrieval processes. The integration of KGs within large-scale generative models significantly enhances the interpretation and generation of contextually nuanced and semantically rich content across various applications. Knowledge Integration and Reasoning Integrating KGs into RAG systems is crucial for enhancing LLMs' reasoning capabilities. This integration provides a structured framework for synthesizing complex relationships and high-level patterns within data. ConvE, a convolutional neural network-based model, captures intricate relationships in KGs through deep learning techniques, enabling the extraction of high-level semantic patterns that enhance generative model interpretability [43]. Path-based reasoning methods with GNNs improve the interpretability and scalability of knowledge integration processes [28]. These methods leverage KGs' structural properties to facilitate efficient reasoning over interconnected entities, allowing LLMs to generate contextually relevant and semantically rich content. The path-based approach enables dynamic exploration of logical reasoning paths, enhancing LLMs' ability to navigate complex information landscapes and resolve intricate queries. Advanced frameworks employing iterative reasoning processes exemplify the integration of GNNs and KGs. These frameworks utilize a retrieve-and-read approach, dynamically selecting relevant subgraph contexts to enhance reasoning and response generation. By continuously updating knowledge memory, these systems improve the retrieval and generation of accurate responses, addressing challenges such as superfluous computation and over-smoothing in traditional GNNs. The collaboration between KGs and LLMs fosters reliable knowledge-based reasoning and transparency in reasoning processes, significantly advancing performance across various datasets [44,21]. By integrating textual and topological information, these frameworks leverage KGs' strengths to provide comprehensive and nuanced data interpretations, thereby enhancing the accuracy and relevance of generated outputs. The integration of KGs and reasoning techniques within RAG systems is vital for advancing LLM interpretability and effectiveness. Employing advanced methods such as passage retrieval with generative models, semantic similarity matching using pre-trained language models, and leveraging KGs for enhanced reasoning offers promising directions for future research and development. They ensure robust and scalable solutions across diverse applications, from improving open-domain question answering and patent similarity assessments to enhancing biomedical knowledge retrieval and reasoning over complex datasets. These methodologies not only achieve state-of-the-art results but also tackle challenges like information overload and knowledge constraints, paving the way for more accurate and interpretable outcomes in various fields [23,11,22,45]. Techniques for Effective Integration Effective integration techniques for KGs in RAG systems are vital for enhancing the semantic understanding and reasoning capabilities of LLMs. Multi-hop reasoning frameworks facilitate the exploration of complex relationships within KGs by iteratively retrieving and integrating relevant subgraphs [32]. This approach allows for dynamic synthesis of information across multiple entities and relationships, improving the accuracy and relevance of generated content. Applying GNNs enhances the interpretability and scalability of knowledge integration processes [28]. GNNs utilize KGs' structural properties to facilitate efficient reasoning over interconnected entities, enabling the generation of contextually relevant and semantically rich content. The PullNet framework exemplifies this integration by combining iterative retrieval with reasoning processes to improve multi-hop question answering capabilities [29]. Utilizing transformer architectures alongside KGs further underscores the potential for effective integration, as seen in the GRAG framework, which optimizes subgraph retrieval to enhance content understanding and generation [5]. This approach highlights the importance of advanced neural network methodologies in managing and processing large volumes of graph-structured data efficiently. The ConvE model illustrates the integration of convolutional neural networks with KGs, capturing intricate relationships through deep learning techniques [43]. By modeling these relationships, ConvE enables the extraction of high-level semantic patterns, thereby enhancing generative models' interpretability and reasoning capabilities. The effective integration of KGs in RAG systems is essential for advancing LLMs' semantic understanding and reasoning capabilities, addressing challenges such as hallucinations, inadequate knowledge updating, and limited transparency in reasoning. By enabling LLMs to iteratively explore and retrieve task-relevant knowledge subgraphs, these systems enhance the models' ability to perform reliable, knowledge-based reasoning and improve proficiency in solving complex issues. This integration also aids in capturing long-tail knowledge, particularly in fields like biomedical research, mitigating information overload, and enhancing retrieval precision and recall. Additionally, it empowers LLMs to search for domain-specific knowledge in a zero-shot manner, increasing the explainability of their reasoning processes and significantly improving performance across various datasets [44,46,22]. By leveraging multi-hop reasoning frameworks, GNNs, and advanced neural network methodologies, these techniques present promising directions for future research and development, ensuring robust and scalable solutions across diverse applications. Information Retrieval Techniques Advanced information retrieval techniques are pivotal in enhancing graph retrieval-augmented generation systems, addressing challenges like information overload and domain-specific complexities. Table presents a comprehensive comparison of different information retrieval methods, elucidating their strategies, efficiency, and application domains in the context of graph retrieval-augmented generation systems. By integrating methodologies such as Graph-Based Retrieval, Retrieval-Augmented Generation (RAG), HybridRAG, and Graph Retrieval-Augmented Generation (GRAG), these systems leverage textual and topological data to improve precision, recall, and contextual relevance across applications, including biomedical research and financial document analysis [42,20,5,22]. Key to their effectiveness are methodologies for retrieving relevant data from knowledge graphs (KGs), particularly through subgraph and triplet retrieval methods, foundational for extracting essential information and supporting complex reasoning tasks. Subgraph and Triplet Retrieval Methods Subgraph and triplet retrieval methods are crucial for extracting critical information from KGs, enhancing the precision and relevance of generated content. These methods address complex reasoning tasks by leveraging structured interactions within KGs. For example, the PullNet framework retrieves relevant subgraphs from both the corpus and knowledge base, essential for answering multi-hop questions [29]. Similarly, QA-GNN uses relevance scoring to identify pertinent subgraphs, improving accuracy in question-answering systems [32]. Path-based reasoning combined with graph neural networks (GNNs) enhances scalable and interpretable reasoning in question answering tasks [28]. The KILT benchmark highlights the importance of integrating knowledge from extensive textual resources, directly relating to methods for retrieving relevant subgraphs and triplets from KGs [47]. This integration supports the reasoning generation capabilities of large language models (LLMs), as seen in the KD-CoT framework, which retrieves external knowledge through a QA system [30]. Innovative approaches like Think-on-Graph (ToG) show improved reasoning depth and accuracy across datasets compared to baseline LLM methods [10]. Additionally, research on decomposing questions and retrieving relevant entities from KGs is pivotal in content generation [3]. The Sentence-BERT (SBERT) architecture contributes by generating semantically meaningful sentence embeddings, enabling rapid similarity comparisons crucial for efficient retrieval processes [35]. These methods advance the semantic understanding and reasoning capabilities of LLMs within retrieval-augmented generation systems, offering promising directions for future research and development. Ranking and Reranking Techniques Ranking and reranking techniques optimize information retrieval processes within graph retrieval-augmented generation systems, supporting accurate content generation by aligning model outputs with user preferences. Frameworks like InstructGPT prioritize user-centric design to enhance satisfaction and minimize undesirable outputs [17]. By focusing on user preferences, these methods improve the relevance and applicability of generated content. Ranking and reranking techniques, alongside KGs, enable dynamic evaluation of retrieved information, significantly enhancing precision and relevance. For instance, KG-Rank uses medical KGs to retrieve and reorder triplets, improving long-form answer accuracy by over 18 Integrating ranking and reranking techniques within retrieval-augmented generation systems optimizes information selection and ordering, facilitating the generation of contextually accurate and semantically rich content. Despite challenges like knowledge updates and high training costs, retrieval-augmented generation enhances robustness and accuracy by sourcing relevant data from external repositories. Evaluating these systems involves unique challenges due to their hybrid structure, necessitating comprehensive benchmarks and metrics to assess relevance, accuracy, and faithfulness, critical for future advancements in AI-generated content [48,20]. Hybrid and Dual-Encoder Retrieval Models Hybrid and dual-encoder retrieval models enhance the efficiency and accuracy of information retrieval within graph retrieval-augmented generation systems by exploiting both traditional and advanced neural network methodologies. The survey by ASurveyonL122 emphasizes the efficiency of fine-tuning existing models over training LLMs from scratch, highlighting the importance of pre-trained architectures for streamlining information retrieval tasks [49]. The U-Retrieval technique within MedGraphRAG exemplifies hybrid models by combining Top-down Precise Retrieval with Bottom-up Response Refinement, enhancing information retrieval in medical contexts [50]. This approach underscores the significance of integrating multiple retrieval strategies to navigate complex information landscapes effectively. DiFaR's framework represents another innovative hybrid model, integrating the retrieval of facts directly from KGs without intermediate steps, thus improving information retrieval efficiency [51]. This direct retrieval approach facilitates the rapid extraction of pertinent information, enabling generative models to synthesize and utilize structured knowledge effectively. The ENGINE model combines LLMs with GNNs through a tunable side structure, enhancing efficiency in training and inference [27]. This integration is particularly relevant to hybrid and dual-encoder models, allowing dynamic adaptation of retrieval strategies based on specific task requirements. Hybrid and dual-encoder retrieval models are integral to advancing graph retrieval-augmented generation systems. By harnessing pre-trained architectures and incorporating diverse retrieval strategies, these models pave the way for promising advancements in future research and development. They ensure robust and scalable solutions across various applications by integrating retrieval-augmented techniques that enhance evidence aggregation, improve coherence and contextual relevance, and bridge language models with external knowledge sources, including structured knowledge bases. These approaches achieve state-of-the-art results in tasks like open-domain question answering and scientific document classification, addressing challenges related to completeness, timeliness, and adaptability [11,52,53,54]. Applications in Information Retrieval The integration of graph structures with advanced language models is pivotal in information retrieval, particularly enhancing question answering systems. This combination deepens the understanding of complex queries and optimizes the relevance and accuracy of generated responses. The following subsection explores the enhancements brought by graph retrieval-augmented generation in question answering, illustrating its significance through frameworks like ArcaneQA and its applications in culturally nuanced scenarios. Enhancing Question Answering Graph retrieval-augmented generation significantly enhances question answering systems by merging structured knowledge from knowledge graphs (KGs) with the reasoning capabilities of large language models (LLMs). This integration permits precise and contextually relevant answers to complex queries. The ArcaneQA framework exemplifies this approach, demonstrating competitive performance in effectiveness and efficiency while addressing challenges in Knowledge Base Question Answering (KBQA) [55]. By leveraging graph structures, ArcaneQA improves the ability of question answering systems to navigate and synthesize information from extensive knowledge bases, thereby enhancing the accuracy and relevance of responses. Incorporating cultural knowledge into graph retrieval-augmented generation showcases its potential to enrich question answering systems. This integration allows systems to provide more human-like and contextually aware responses, as evidenced by its application in natural language processing (NLP) tasks [56]. Such cultural integration enhances the semantic understanding of LLMs, enabling them to deliver comprehensive answers to culturally specific inquiries. Graph retrieval-augmented generation addresses the limitations of LLMs in capturing long-tail and complex information, improving their ability to synthesize diverse data. This is particularly relevant in fields like biomedical research and customer service, where it mitigates information overload and enhances retrieval accuracy. By transforming graph-based knowledge into well-textualized statements, it bridges the gap between knowledge graph representations and natural language, resulting in more accurate and informative answers. Additionally, this method facilitates transparent reasoning processes, reduces hallucinations, and scales effectively with larger datasets, yielding substantial improvements in retrieval and generation performance across various applications [44,57,39,22,58]. This underscores the transformative potential of this approach in enhancing question answering systems across diverse applications. Applications in Domain-Specific Contexts Graph retrieval-augmented generation (RAG) demonstrates significant potential across various domain-specific contexts by leveraging the integration of graph structures with advanced language models to enhance information retrieval and content generation. In healthcare, the MedGraphRAG framework optimizes medical information retrieval processes through precise retrieval and response refinement techniques, improving the accuracy and relevance of medical question answering systems [50]. This approach highlights RAG's transformative impact in navigating complex medical knowledge bases to deliver precise, contextually relevant medical information. In legal information retrieval, integrating graph-based methods with LLMs facilitates the extraction and synthesis of legal precedents and statutes, enhancing the accuracy and efficiency of legal research processes. The application of RAG in this context improves systems' ability to manage intricate legal queries by dynamically retrieving and integrating relevant legal knowledge from diverse sources. This approach supports informed decision-making and employs techniques similar to QA-RAG and HybridRAG frameworks, which have shown improved accuracy and efficiency in regulatory compliance and information extraction in other domains. By utilizing dual-track retrieval mechanisms and integrating knowledge graphs, RAG optimizes the handling of complex legal information, ensuring precise and contextually relevant outcomes [42,4]. E-commerce benefits from graph retrieval-augmented generation, as demonstrated by the BEQUE framework, which improves long-tail query rewriting through graph-based techniques to refine search results and enhance user satisfaction [15]. This integration of graph structures with retrieval systems allows for more personalized and accurate product recommendations, optimizing user experience and driving engagement. In educational technology, RAG aids in developing adaptive learning systems that dynamically retrieve and integrate relevant educational content, thereby enhancing the learning experience for students and educators alike. By incorporating structured knowledge from educational knowledge graphs, these systems improve the precision and effectiveness of learning resources. They utilize dense and structured factual information to supplement pre-trained language models, grounding them in external knowledge sources. This approach supports personalized learning pathways by retrieving relevant subgraphs, facilitating targeted learning experiences and improving educational outcomes. Frameworks like KnowledgeNavigator and Knowledge Solver empower LLMs to perform accurate and interpretable multi-hop reasoning, while retrieval methods address information overload by capturing long-tail knowledge, ensuring comprehensive and domain-specific insights for learners [46,21,59,22,45]. The application of graph retrieval-augmented generation across these domain-specific contexts underscores its transformative potential in enhancing information retrieval and content generation processes. By incorporating graph structures with advanced language models, RAG offers promising avenues for future research and development, ensuring robust and scalable solutions tailored to the unique demands of diverse applications. Specifically, GraphRAG enhances query-focused summarization by utilizing an entity knowledge graph and community summaries to improve the comprehensiveness and diversity of answers to global questions over large datasets. Meanwhile, MedGraphRAG adapts this framework for the medical domain, employing Triple Graph Construction and U-Retrieval techniques to generate evidence-based medical responses, thus improving safety and reliability when handling private medical data. These innovations illustrate the potential of graph-based RAG systems to advance the capabilities of large language models across various fields [50,26]. Challenges and Future Directions Challenges in Current Methods Graph retrieval-augmented generation systems encounter several challenges that limit their effectiveness and scalability. Integrating Graph Neural Networks (GNNs) with large language models (LLMs) requires extensive engineering and task-specific knowledge sources, complicating system development [5]. The hallucination phenomenon in LLMs, where models generate plausible but incorrect responses, undermines reasoning reliability [2]. Additionally, the KG-Agent framework highlights limitations in LLMs' decision-making capabilities when interacting with Knowledge Graphs (KGs) [31]. Misalignment between language model outputs and user intent further affects contextual accuracy, particularly in complex information retrieval tasks [17]. Systems like Keqing emphasize the dependency on the quality and completeness of knowledge graphs for effective retrieval, underscoring challenges in thorough knowledge integration [3]. The GRAG framework also illustrates limitations in retrieving and integrating textual subgraphs into LLMs [5]. Disentangling knowledge bases from language models while maintaining cognitive alignment with human understanding remains a significant hurdle [2]. Current research often inadequately addresses natural language subtleties, impacting scalability and generalization across diverse knowledge bases [6]. These challenges necessitate ongoing research to enhance the robustness and applicability of retrieval-augmented generation systems in complex information retrieval scenarios. Scalability and Efficiency Issues Scalability and efficiency are crucial challenges for graph retrieval-augmented generation systems, largely due to their reliance on knowledge base quality and structure. The UniKGQA method reveals scaling issues with larger knowledge graphs and complex queries, indicating limitations in managing extensive datasets [33]. Similarly, the DiFaR framework's reliance on initial embedding quality poses scalability challenges in intricate contexts, necessitating robust embedding strategies for efficient processing [51]. Manual dataset construction, like GRBench, introduces biases and restricts knowledge diversity, adversely affecting scalability and efficiency [9]. The dynamic nature of regulatory updates presents challenges for QA-RAG systems, requiring adaptable language models to keep pace with evolving frameworks [4]. The KG-Agent framework underscores the need for improved scalability and adaptability to various knowledge graphs and reasoning tasks, suggesting avenues for future research [31]. While effective in e-commerce, the BEQUE framework's scalability issues when generalizing to other domains highlight the need for adaptable systems [15]. Bridging scalability gaps necessitates enhanced detection methods and robust information retrieval systems to address challenges such as hallucinations [60]. Addressing these scalability and efficiency issues is vital for advancing retrieval-augmented generation systems, ensuring their capability to process and synthesize complex information across varied applications. Future research should focus on improving the scalability of existing frameworks and investigating enhancements to reinforcement learning models and integration with diverse data sources [61]. Data Quality and Diversity Concerns Data quality and diversity are critical for graph retrieval-augmented generation systems, directly influencing output reliability and effectiveness. The robustness of reasoning methods, such as RoG, depends on the quality and comprehensiveness of underlying knowledge graphs (KGs), emphasizing high data standards [62]. Noise and ambiguity in questions can adversely impact question answering system performance, highlighting the importance of clear data inputs [63]. Test set leakage complicates link prediction model evaluation, potentially leading to inaccurate performance assessments [43]. Methods like GNN-RAG are limited by KG quality and structure, necessitating well-structured knowledge bases for enhanced reasoning capabilities [64]. Additionally, systems like GraphRAG rely on the quality of entity knowledge graphs and community summaries, affecting their ability to synthesize accurate information [26]. Future research should enhance model adaptability to diverse knowledge graphs and improve knowledge base completeness, as noted in open-domain question answering studies. Expanding benchmarks to include more complex questions and reasoning tasks could also enhance data quality and diversity evaluation frameworks [65]. The accuracy of model outputs remains a significant concern, as evidenced by simple errors from systems like InstructGPT, underscoring the need for improved data quality and diversity to enhance reliability [17]. Addressing these issues is essential for advancing graph retrieval-augmented generation systems, enabling them to navigate complex information landscapes and deliver accurate, contextually relevant content across various applications. Enhancing Model Interpretability and Robustness Enhancing model interpretability and robustness is crucial for advancing retrieval-augmented generation systems, especially in integrating external knowledge sources with large language models (LLMs). The complexity of queries often results in incomplete or inaccurate answers, necessitating models that effectively interpret and utilize structured knowledge [66]. Future research should explore integrating neural semantic parsing techniques with information retrieval methods to address the increasing complexity of questions, thereby improving generative model interpretability and robustness [8]. Interpretability is underscored by frameworks like Golden-Retriever, which depend on comprehensive jargon dictionaries for document retrieval. Incompleteness in these resources can significantly affect model performance, emphasizing the need for transparent and interpretable models [37]. The HSGE model also highlights the importance of effective context modeling in conversational Knowledge Base Question Answering (KBQA) systems, demonstrating how interpretability can refine LLM reasoning processes [12]. Robustness is equally vital, as shown by the KD-CoT framework, which utilizes external knowledge to enhance reasoning accuracy. This approach highlights the necessity for robust models that can seamlessly integrate and utilize external information [30]. Exploring new methodologies for knowledge integration is essential for overcoming current limitations, paving the way for more efficient retrieval-augmented generation systems [20]. Future research should prioritize developing innovative frameworks that enhance the integration of LLMs with graph technologies, facilitating better representation and analysis of complex relationships in areas like social networks and biological data. This includes leveraging LLM linguistic capabilities to advance Graph Representation Learning (GRL) and addressing methodological gaps by proposing new taxonomies and dissecting model components and operational techniques. Additionally, research should focus on bridging explicit and parametric knowledge representations, broadening the potential of these technologies in real-world applications [67,25,41]. By enhancing model interpretability and robustness, retrieval-augmented generation systems can more effectively navigate complex information landscapes, ensuring accurate and contextually relevant outputs across diverse applications. Ethical and Practical Considerations Deploying graph retrieval-augmented generation (RAG) systems requires careful consideration of ethical and practical aspects to ensure responsible use and effective performance across domains. Ethical concerns focus on the reliability and interpretability of LLM outputs, particularly in sensitive areas like healthcare, where inaccuracies can have serious implications [3]. The hallucination phenomenon in LLMs, where models generate plausible but incorrect content, underscores the need for robust knowledge bases to support accurate system responses [2]. Practical challenges in integrating LLMs with GNNs include substantial computational resource requirements and the need for optimized storage solutions for large-scale data [27]. Reliance on specific architectures, such as those in frameworks like BEQUE, raises concerns about model adaptability and versatility across diverse applications, especially when high-quality datasets are unavailable [15]. Ethical considerations regarding dependence on singular knowledge sources, as highlighted by the KILT benchmark, emphasize the importance of comprehensive knowledge representation to avoid biases [47]. Frameworks like Think-on-Graph (ToG) offer flexibility and traceability, demonstrating the potential for integrating various LLMs and knowledge graphs without incurring additional training costs. However, this also necessitates robust evaluation frameworks to ensure ethical implementation and address potential biases in knowledge synthesis [10]. The scalability issues identified in models like UniKGQA further highlight the need for ongoing research to enhance performance and applicability across diverse question answering tasks [33]. Future research should focus on optimizing retrieval processes and exploring hybrid approaches that combine semantic parsing and information retrieval to tackle natural language ambiguity, thereby enhancing the robustness and reliability of RAG systems [6]. Ensuring the quality of knowledge graphs and language models used in RAG systems is crucial, as it directly impacts both performance and ethical deployment. By addressing these ethical and practical considerations, future developments in graph retrieval-augmented generation can align with responsible AI practices and meet practical demands, paving the way for robust and scalable solutions across diverse domains. Conclusion The exploration of graph retrieval-augmented generation (RAG) highlights its significant impact on enhancing information retrieval and content generation by integrating graph structures with sophisticated neural networks. Leveraging the structured nature of knowledge graphs (KGs), RAG systems facilitate advanced reasoning and synthesis of complex data relationships, thereby increasing the precision and relevance of generated content. The History Semantic Graph Enhanced KBQA model (HSGE) exemplifies the importance of improving context modeling in Knowledge Base Question Answering (KBQA), pointing towards more effective and scalable solutions. Moreover, the effectiveness of integrating graph-based techniques in scalable question answering with external knowledge underscores advancements in reasoning and inference capabilities. Recent frameworks demonstrate substantial progress in processing both structured and unstructured data, enhancing reasoning tasks on knowledge graphs and delivering competitive performance across various applications. These developments underscore the necessity to address challenges related to scalability, efficiency, and data quality to strengthen the robustness of retrieval-augmented generation systems. Future research should focus on optimizing retrieval processes and exploring hybrid approaches that merge semantic parsing with information retrieval, thereby improving the interpretability and reliability of generative models. The survey underscores the pivotal role of graph retrieval-augmented generation in propelling artificial intelligence forward, presenting promising opportunities for future research and development across multiple domains. By addressing existing challenges and exploring innovative methodologies, RAG systems have the potential to transform information retrieval and content generation, paving the way for robust and scalable solutions tailored to the specific needs of various applications.",
  "reference": {
    "1": "2408.14523v2",
    "2": "2312.02706v2",
    "3": "2401.00426v1",
    "4": "2402.01717v1",
    "5": "2405.16506v3",
    "6": "2105.11644v1",
    "7": "1809.00782v1",
    "8": "2007.13069v1",
    "9": "2404.07103v3",
    "10": "2307.07697v6",
    "11": "2007.01282v2",
    "12": "2306.06872v1",
    "13": "2310.12836v1",
    "14": "2404.14928v2",
    "15": "2311.03758v3",
    "16": "2101.00376v2",
    "17": "2203.02155v1",
    "18": "2210.00063v2",
    "19": "2305.18742v1",
    "20": "2402.19473v6",
    "21": "2212.09724v3",
    "22": "2402.12352v1",
    "23": "2403.16265v1",
    "24": "2403.15586v1",
    "25": "2308.06374v1",
    "26": "2404.16130v2",
    "27": "2401.15569v2",
    "28": "2005.00646v2",
    "29": "1904.09537v1",
    "30": "2308.13259v2",
    "31": "2402.11163v1",
    "32": "2104.06378v5",
    "33": "2212.00959v2",
    "34": "2404.10981v2",
    "35": "1908.10084v1",
    "36": "2210.12925v1",
    "37": "2408.00798v1",
    "38": "2310.08975v3",
    "39": "2309.11206v2",
    "40": "2312.02783v4",
    "41": "2311.12399v4",
    "42": "2408.04948v1",
    "43": "1707.01476v6",
    "44": "2402.04978v2",
    "45": "2312.15880v2",
    "46": "2309.03118v1",
    "47": "2009.02252v4",
    "48": "2306.02871v1",
    "49": "2111.10541v4",
    "50": "2403.05881v3",
    "51": "2405.07437v2",
    "52": "2406.10303v2",
    "53": "2408.04187v2",
    "54": "2305.12416v1",
    "55": "2308.11761v1",
    "56": "2311.12289v1",
    "57": "1907.11692v1",
    "58": "2204.08109v3",
    "59": "1811.00146v3",
    "60": "2404.17723v2",
    "61": "2402.07630v3",
    "62": "2308.14436v1",
    "63": "1711.05851v2",
    "64": "2310.01061v2",
    "65": "1709.04071v5",
    "66": "2405.20139v1",
    "67": "1506.02075v1",
    "68": "2108.06688v5",
    "69": "2402.05952v1"
  },
  "chooseref": {
    "1": "2205.01841v1",
    "2": "2303.10395v1",
    "3": "2212.09724v3",
    "4": "2404.00579v2",
    "5": "2311.12399v4",
    "6": "2406.11903v1",
    "7": "2105.11644v1",
    "8": "2007.13069v1",
    "9": "2311.05232v2",
    "10": "2406.10303v2",
    "11": "2406.03712v2",
    "12": "2405.06211v3",
    "13": "2404.10981v2",
    "14": "2311.12289v1",
    "15": "1811.00146v3",
    "16": "2402.05952v1",
    "17": "2402.04978v2",
    "18": "2204.08109v3",
    "19": "2010.05953v2",
    "20": "1706.03762v7",
    "21": "1810.04805v2",
    "22": "2406.12608v2",
    "23": "2308.14436v1",
    "24": "2406.04744v2",
    "25": "2109.01653v1",
    "26": "2305.10037v3",
    "27": "1809.02789v1",
    "28": "2310.08975v3",
    "29": "1811.00937v2",
    "30": "2108.06688v5",
    "31": "2305.01157v3",
    "32": "2403.16265v1",
    "33": "1707.01476v6",
    "34": "2405.04819v4",
    "35": "2406.07080v1",
    "36": "2210.00063v2",
    "37": "2004.04906v3",
    "38": "2305.12416v1",
    "39": "1907.10903v4",
    "40": "2401.15569v2",
    "41": "2405.07437v2",
    "42": "1910.10683v4",
    "43": "2310.13848v2",
    "44": "2305.06590v2",
    "45": "2308.10173v1",
    "46": "2404.16130v2",
    "47": "2402.01717v1",
    "48": "2402.07630v3",
    "49": "2405.20139v1",
    "50": "2305.15066v2",
    "51": "2405.16506v3",
    "52": "2407.13193v3",
    "53": "2403.15586v1",
    "54": "1711.05851v2",
    "55": "2408.00798v1",
    "56": "2303.12320v2",
    "57": "1710.10903v3",
    "58": "2404.07103v3",
    "59": "2404.14928v2",
    "60": "2305.18742v1",
    "61": "2407.09777v2",
    "62": "2402.12352v1",
    "63": "2111.10541v4",
    "64": "2310.01089v1",
    "65": "2402.07197v4",
    "66": "2201.08860v1",
    "67": "2405.14831v3",
    "68": "2306.06872v1",
    "69": "1809.09600v1",
    "70": "2312.15883v2",
    "71": "2408.04948v1",
    "72": "2101.03737v2",
    "73": "1706.02216v4",
    "74": "2402.11163v1",
    "75": "2110.04330v2",
    "76": "2310.11220v1",
    "77": "2403.05881v3",
    "78": "2009.02252v4",
    "79": "1909.02151v1",
    "80": "1606.03126v2",
    "81": "2312.06185v5",
    "82": "2308.11761v1",
    "83": "2308.11730v3",
    "84": "2406.13862v1",
    "85": "2309.03118v1",
    "86": "2306.04136v1",
    "87": "2310.12836v1",
    "88": "2308.13259v2",
    "89": "2312.15880v2",
    "90": "2402.08170v3",
    "91": "2308.07134v5",
    "92": "2005.14165v4",
    "93": "2312.02706v2",
    "94": "2311.03758v3",
    "95": "2308.06374v1",
    "96": "2403.18105v2",
    "97": "2405.13055v1",
    "98": "2311.10723v2",
    "99": "2312.02783v4",
    "100": "1506.02075v1",
    "101": "2007.01282v2",
    "102": "2307.09288v2",
    "103": "2307.03172v3",
    "104": "2408.04187v2",
    "105": "2308.09729v5",
    "106": "2210.01613v1",
    "107": "2211.10991v1",
    "108": "2404.19234v1",
    "109": "2404.00492v1",
    "110": "2404.07677v2",
    "111": "1809.00782v1",
    "112": "1911.11641v1",
    "113": "2402.02216v3",
    "114": "2101.00190v1",
    "115": "1904.09537v1",
    "116": "2104.06378v5",
    "117": "2202.00120v2",
    "118": "2404.19543v2",
    "119": "2202.06129v1",
    "120": "2405.14431v1",
    "121": "2210.13650v1",
    "122": "2404.10384v1",
    "123": "2310.01061v2",
    "124": "2408.14523v2",
    "125": "2402.19473v6",
    "126": "2312.10997v5",
    "127": "2404.17723v2",
    "128": "2309.11206v2",
    "129": "2101.00376v2",
    "130": "2109.08678v2",
    "131": "1907.11692v1",
    "132": "2404.13207v3",
    "133": "2005.00646v2",
    "134": "1609.02907v4",
    "135": "1908.10084v1",
    "136": "2305.09645v2",
    "137": "2202.13296v2",
    "138": "2210.12925v1",
    "139": "2310.04560v1",
    "140": "2306.02871v1",
    "141": "2104.08691v2",
    "142": "1803.06643v1",
    "143": "2307.07697v6",
    "144": "2011.07743v6",
    "145": "2310.04562v2",
    "146": "2203.02155v1",
    "147": "1705.03551v2",
    "148": "2402.16568v2",
    "149": "2212.00959v2",
    "150": "2406.02110v1",
    "151": "2306.08302v3",
    "152": "1709.04071v5",
    "153": "2009.13081v1",
    "154": "1906.07348v1",
    "155": "2401.00426v1"
  }
}