{
  "survey": "Diffusion models have emerged as a transformative force in image editing, offering novel methodologies for generating and customizing visual content through iterative refinement processes. This survey provides a comprehensive overview of the advancements and applications of diffusion models in image manipulation, emphasizing their efficacy in text-to-image synthesis and video generation. Despite challenges such as overfitting and limited controllability, diffusion models have demonstrated significant improvements in fidelity and user-directed transformations. Recent innovations, including dual diffusion architectures and mask-and-replace strategies, have enhanced precision and efficiency in image editing tasks. The survey also explores the foundational principles of diffusion models, contrasting them with traditional generative models like GANs, and highlights their superiority in handling complex dependencies and conditional generation tasks. Moreover, the survey examines the diverse applications of diffusion models, from virtual try-on scenarios to fine-grained editing and image restoration, underscoring their ability to achieve realistic effects while preserving intricate details. Challenges related to computational complexity, semantic misalignment, and input dependencies are addressed, alongside future directions for enhancing model efficiency, broader applications, and improved user control. By integrating cutting-edge methodologies and architectural innovations, diffusion models continue to redefine possibilities within generative modeling, paving the way for future developments and applications in image editing. This survey aims to illuminate a structured design space for diffusion-based models, facilitating targeted improvements in sampling efficiency and training processes, thus broadening their applicability and efficacy in diverse image editing frameworks.\n\nIntroduction Significance of Diffusion Models in Image Editing Diffusion models have emerged as a transformative force in image editing, revolutionizing the generation and customization of visual content. Their impact is particularly pronounced in text-to-image synthesis, where they effectively address inefficiencies and quality limitations of traditional methods [1]. By generating high-quality synthetic images with text-conditional guidance, diffusion models enhance both fidelity and controllability of the output [2]. Moreover, these models extend their capabilities beyond static images to dynamic media, such as video generation, demonstrating their broad applicability and potential to enhance fidelity across various formats [3]. However, challenges such as overfitting and the preservation of non-text-related image content remain critical areas requiring attention [4]. Addressing these limitations is essential for realizing the full potential of diffusion models in image editing. Limited controllability in image generation with text guidance has garnered scholarly focus, highlighting the need for improved user-directed transformations [5]. By enabling the integration of multiple concepts within a single image, diffusion models streamline the editing process and facilitate precise control over image attributes. Collectively, these advancements illustrate the significant impact of diffusion models in reshaping the image editing landscape, paving the way for future innovations across diverse domains. Recent Advancements in Diffusion Models Recent advancements in diffusion models have significantly refined their capabilities in image editing, introducing innovative techniques that enhance precision and efficiency. The dual diffusion processing architecture of DiffStyler exemplifies this progress, improving the quality and relevance of stylized images through sophisticated diffusion integration [6]. The VQ-Diffusion model employs a mask-and-replace strategy, effectively managing complex scenes and reducing error accumulation [1]. Innovative two-stage sampling solutions have been proposed to address occlusion and identity preservation more effectively than existing methods, marking a substantial improvement in the robustness of diffusion models [7]. Additionally, advancements in video generation models, particularly through the integration of diffusion models for high-definition production, have broadened their applicability beyond static images [3]. Enhancements in user experience for image generation and editing have been achieved through automatic dataset construction techniques that utilize weak supervision and segmentation masks for localized editing, thereby improving fidelity and alignment in generated images [5]. The benchmark introduced by GLIDE evaluates various guidance techniques, further enhancing photorealism and caption similarity in generated outputs [2]. Innovations in separating high-level attributes from stochastic variations within generator architectures represent a significant advancement in generative adversarial networks, providing refined control over image generation processes [8]. Collectively, these developments signify a dynamic growth period for diffusion models, expanding their applicability and effectiveness in image editing. Scope and Objectives This survey offers a comprehensive examination of diffusion models, emphasizing their transformative influences in image editing and generative tasks. It explores core principles, mathematical frameworks, and architectural dynamics of diffusion models, specifically focusing on applications involving denoising diffusion probabilistic models and noise-conditioned score networks, while excluding non-diffusion generative models [9]. A primary objective is to introduce methodologies to mitigate instability in DDIM inversion and enhance image reconstruction fidelity [10]. The survey also addresses existing challenges and solutions in image generation models, particularly diffusion-based approaches [11]. It proposes a text-to-image editing model that integrates an Image Information Removal module (IIR) to enhance editability while preserving essential visual attributes [4]. Additionally, it highlights innovative frameworks like HyperDreamBooth, which enables rapid personalization of text-to-image models using a single reference image, thus improving adaptability and efficiency in customized editing scenarios [12]. Furthermore, methods for maintaining image consistency during local editing are examined, showcasing techniques like negative-prompt inversion to address artifacts. The survey explores the extension of diffusion models for video generation, emphasizing frameworks like LaVie, which utilize cascaded video latent diffusion models for high-quality text-to-video conversion without paired datasets. LaVie integrates a pre-trained text-to-image model, a base text-to-video model, a temporal interpolation model, and a video super-resolution model, employing temporal self-attentions with rotary positional encoding to capture correlations and facilitate joint image-video fine-tuning. Supported by the Vimeo25M dataset, this framework demonstrates state-of-the-art performance in generating visually realistic and temporally coherent videos, showcasing versatility in long video generation and personalized synthesis applications [13,14]. The importance of constructing a semantic latent space within diffusion models to enhance control over the generative process is also identified as crucial for improving user engagement and flexibility in image editing [15]. Advanced methodologies such as Focus on Your Instruction are explored to ensure precise and coherent editing in multi-instruction contexts, highlighting the progressive capabilities of diffusion models in sophisticated editing tasks [16]. The survey contributes by proposing benchmarks like DrawBench for evaluating text-to-image models, ensuring alignment with provided textual prompts and enhancing the quality and accuracy of generated imagery [17]. The overarching goal is to illuminate a structured design space for diffusion-based models, facilitating targeted improvements in sampling efficiency and training processes, thereby broadening their applicability and efficacy in diverse image editing and generative frameworks. Structure of the Survey The survey is systematically organized to provide a comprehensive exploration of diffusion model-based image editing, beginning with an introduction that establishes the significance, recent advancements, and objectives of the survey. This is followed by a detailed background section elucidating the fundamentals of diffusion models, contrasting them with traditional generative models and explaining their iterative refinement processes. The subsequent section delves into specific techniques employed in diffusion model-based image editing, such as mask guidance, attention mechanisms, and innovative approaches like TryOnDiffusion and InstructEdit, which are pivotal in advancing the field. The survey then transitions into an examination of diverse applications of diffusion models within image editing, highlighting their utility in virtual try-on applications, fine-grained editing techniques, image inpainting, restoration, and cross-domain integrations. Each application underscores the models' capability to produce high-quality, realistic effects while effectively preserving intricate image details, despite challenges such as overfitting, information leakage, and computational costs associated with diffusion models. Recent advancements have improved the editability-fidelity trade-off and efficiency, enabling faster and more expressive image manipulations even on user devices [18,14,19,4]. Challenges and limitations inherent in diffusion model-based image editing are addressed next, focusing on computational complexity, semantic misalignment, and dependence on high-quality input data and pre-trained models. The survey provides an in-depth analysis of difficulties in managing complex edits within video diffusion models and highlights challenges involved in evaluating and benchmarking these models. It delves into core principles and mathematical formulations of diffusion models, explores architectural choices for maintaining temporal consistency, and discusses advancements in text-to-video generation. Additionally, the survey reviews recent developments in training and evaluation practices, including the use of diverse datasets and various evaluation metrics. Ongoing challenges such as generating longer videos and managing computational costs are also examined, offering insights into potential future directions for the field. This comprehensive overview serves as a valuable resource for researchers and practitioners by consolidating the latest research and developments in video diffusion models [14,20,11]. Finally, the survey outlines future directions by proposing enhancements in model efficiency, broader applications, improved user control, and integration with other techniques. It suggests refining architectural designs and temporal dynamics to boost efficiency, expanding applications to include diverse data modalities such as text, images, and audio signals, and enhancing user control over video content generation. Additionally, it advocates for integrating diffusion models with other methodologies to tackle challenges like generating longer videos and managing computational costs, thereby advancing the field of video diffusion models and structured data modeling [21,14,20]. The need for standardization and benchmarking to facilitate the evaluation and comparison of diffusion models is also emphasized. The conclusion synthesizes key insights and reflects on the transformative potential of diffusion models in image editing.The following sections are organized as shown in . Background Overview of Diffusion Models Diffusion models represent an advanced approach in generative modeling, employing a thermodynamics-inspired iterative process for image editing [2]. This involves a forward diffusion phase, where noise is incrementally added to data samples, followed by a reverse process that progressively denoises these samples to reconstruct images with high fidelity [2]. This systematic methodology enhances precision in image synthesis and facilitates advanced control mechanisms, particularly beneficial in text-to-image generation, where textual descriptions guide the refinement [2]. The architecture of diffusion models enables them to surpass traditional generative methods like GANs, offering refined control over complex imagery and effectively managing conditional generation tasks. This is achieved through a balance of mode coverage and sample fidelity, accommodating multifaceted visual narratives within a single frame [2]. By aligning with textual prompts, diffusion models overcome limitations inherent in prior generative paradigms, particularly in text-conditional image synthesis [2]. Beyond static images, diffusion models demonstrate versatility in video generation, employing analogous iterative processes to enhance spatial and temporal coherence [2]. Their ability to integrate various modalities and adapt to diverse contexts underscores their transformative potential across creative and industrial applications, paving the way for innovations in both image and video synthesis. Diffusion Models vs. Traditional Generative Models Diffusion models have emerged as a formidable alternative to traditional generative models, such as GANs and CGANs, offering significant advantages in sampling efficiency and output fidelity. Traditional models often struggle with disentangling high-level attributes from stochastic variations, a challenge effectively addressed by diffusion models through their systematic noise addition and removal process [8]. Unlike GANs, which are prone to mode collapse and require adversarial training, diffusion models employ a probabilistic framework to generate high-resolution images without these limitations [22]. The integration of CNNs with Diffusion Probabilistic Models (DPMs), as exemplified by ResDiff, enhances single image super-resolution, significantly improving image quality beyond traditional techniques [23]. Additionally, diffusion models provide a robust framework for capturing complex data dependencies, akin to the capabilities demonstrated in Masked Autoregressive Flow, offering a more flexible approach to generative tasks [24]. In video generation, diffusion models bridge the training-inference gap seen in traditional sequential generation methods, highlighted by the innovative Nuwa-XL approach [25]. This capability is further emphasized by their advantages in overcoming the high computational costs associated with traditional pixel-based models and the alignment challenges faced by latent-based models in text-video tasks [26]. Diffusion models redefine benchmarks traditionally centered on ODEs by incorporating SDEs, which better capture the intricacies of image editing, leading to enhanced performance across various tasks [27]. The classifier-free guidance mechanism simplifies the training process by eliminating reliance on external classifiers, enhancing the adaptability of diffusion models in diverse applications [22]. The pix2pix-zero method showcases the zero-shot editing capabilities of diffusion models, preserving original content during edits, distinguishing them from traditional generative models that often require task-specific training [28]. Despite these advancements, challenges such as performance limitations persist, necessitating ongoing research to fully exploit the potential of diffusion models in generating high-quality images [11]. Diffusion models signify a substantial advancement in generative modeling, providing enhanced control, fidelity, and versatility across a wide array of applications. Iterative Refinement Process The iterative refinement process is fundamental to diffusion models, facilitating the progressive enhancement of image quality through successive applications of denoising techniques. This process begins with the introduction of Gaussian noise, followed by a reverse diffusion phase that incrementally boosts image fidelity at each step [29]. The DDRM method exemplifies this by efficiently reconstructing images from noisy measurements, utilizing a pre-trained generative model and a sampling process based on variational inference to improve reconstruction quality [30]. In super-resolution, SR3 illustrates the effectiveness of iterative refinement by starting with Gaussian noise and progressively refining the image based on low-resolution input, showcasing diffusion models' ability to enhance image details [31]. The ELITE method further demonstrates this process through a dual mapping approach that enables rapid encoding and improved editability, facilitating the generation of customized images [32]. EF-DDPM introduces a structured noise space that simplifies image editing, enabling transformations that yield meaningful manipulations, underscoring the iterative refinement capabilities of diffusion models [33]. UniControl enhances this process by augmenting pretrained text-to-image diffusion models with a task-aware HyperNet, training on multiple condition-to-image tasks to generate images based on visual conditions and language prompts [34]. The Manifold Constraint Diffusion (MCD) method incorporates a correction term based on manifold constraints into the diffusion model's sampling process, ensuring iterations remain close to the data manifold, thus improving fidelity and precision [35]. Uni-paint offers a unified framework for multimodal inpainting, supporting various guidance modes and enabling greater user customization, further exemplifying the iterative refinement process [36]. DDIMs accelerate the reverse diffusion process through a non-Markovian approach, enhancing the efficiency of image generation and iterative refinement capabilities of diffusion models [37]. DPS combines diffusion sampling with manifold constrained gradient techniques to tackle noisy inverse problems, highlighting the importance of iterative refinement in achieving high-quality outputs [38]. WaveletDiff refines this process by adaptively managing frequency components derived from wavelet decomposition for improved speed and quality in image generation [39]. The Blended Latent Diffusion Model (BLDM) accelerates local text-driven editing by blending latents at each diffusion step, enhancing the efficiency of iterative refinement [40]. DiffIR employs a two-stage training process that utilizes compact prior representations for efficient image restoration, showcasing the adaptability of the iterative refinement process across various applications [41]. AlignYourLatents incorporates a temporal dimension into the latent diffusion approach, refining video generation through a structured iterative method [42]. The DPM-Solver method simplifies the solution of diffusion ODEs to an exponentially weighted integral of the neural network, facilitating fast sampling and further exemplifying the efficiency of the iterative refinement process [43]. Collectively, these methodologies underscore the pivotal role of iterative refinement in diffusion models, enabling precise and high-quality image enhancements across diverse applications. Techniques in Diffusion Model-Based Image Editing Advancements in diffusion model-based image editing are driven by mask guidance and attention mechanisms, which enhance edit precision and facilitate targeted manipulations. As illustrated in , the hierarchical structure of these techniques is categorized into mask guidance and attention mechanisms, pixel-wise guidance and iterative processes, and innovative techniques. Each category further delineates specific methodologies and their contributions to enhancing image editing precision, control, and creativity. Table provides a structured comparison of various methodologies in diffusion model-based image editing, emphasizing the distinct features and advancements within each approach. Techniques such as DiffEdit and InstructEdit utilize text-conditioned models to generate and refine masks for semantic editing, while Paint by Example employs exemplar-guided editing with arbitrary shape masks for precise control. Pixel-wise guidance supports fine-grained edits, illustrating the transformative potential of diffusion models in achieving high-quality, controllable image edits [44,45,46,47]. Mask Guidance and Attention Mechanisms Mask guidance and attention mechanisms significantly advance image editing precision by enabling targeted manipulation of specific regions. The CoSeR framework combines cognitive embeddings with image processing to enhance edit accuracy [48]. Attention mechanisms optimize diffusion models, as evidenced by Forgedit's vision-language joint optimization framework, which enhances editing capabilities [49]. Classifier-free guidance improves accuracy and quality in image edits [22]. A framework categorizing diffusion model performance highlights the impact of these mechanisms in enhancing control, precision, and quality across diverse applications [11]. Pixel-wise Guidance and Iterative Processes Pixel-wise guidance and iterative processes are essential for detailed image edits, enhancing control and accuracy. Table provides a comprehensive comparison of key image editing methodologies, emphasizing their guidance techniques, iterative processes, and multimodal frameworks to enhance editing precision and control. The Regenerati method employs cross-attention guidance to preserve original image attributes during editing [50]. EMILIE's iterative methodology allows precise control over edit scale [51]. DialogPaint supports iterative, multi-round editing, enabling users to refine edits through dialogue [52]. Uni-paint offers a multimodal inpainting framework utilizing pretrained diffusion models for diverse inpainting guidance [36]. Progressive Feature Blending enhances editing quality through multi-level feature blending [53]. MLLM-Guided Image Editing provides explicit guidance for nuanced modifications [54]. Moecontrol demonstrates iterative refinement in image generation processes [55]. iEdit facilitates precise pixel-wise modifications using segmentation masks and textual prompts [5]. These methodologies underscore the critical role of pixel-wise guidance and iterative processes in advancing image editing precision. Innovative Techniques in Diffusion Models Innovative techniques in diffusion models enhance image editing capabilities, offering improved precision and efficiency. The VQ-Diffusion model demonstrates effectiveness in generating high-quality images [1]. DiffStyler enhances image stylization through cross-modal style information. Imagen Video expands diffusion model applicability to dynamic media formats [3]. DPM-Solver simplifies solutions, showcasing efficiency gains [43]. Null-text guidance introduces disturbance methods, transforming outputs creatively [56]. Omg:Occlus90 emphasizes noise blending for preserving identity during complex edits [7]. The Image Information Removal module enhances text-to-image editing capabilities [4]. Techniques such as CLIP guidance demonstrate improvements in photorealism [2]. These innovations represent advancements in diffusion models, broadening their applicability across various domains, including generative modeling for text, speech, and healthcare. As diffusion models evolve, they are positioned at the forefront of Artificial Intelligence for General Creativity (AIGC), unlocking new possibilities for high-quality content creation [21,14,57,20,19]. Applications of Diffusion Models in Image Editing Virtual Try-On Applications Diffusion models have advanced virtual try-on applications, enabling realistic simulations of clothing and accessories on images. The GP-VTON framework, featuring a Local-Flow Global-Parsing warping module and Dynamic Gradient Truncation training strategy, addresses challenges in preserving garment details and achieving realistic integration, enhancing user satisfaction in digital environments [58]. StableVITON further utilizes pre-trained models for customized image generation, focusing on detail preservation and generative performance [59]. The Parser-Free VTON framework streamlines the virtual try-on process by eliminating complex parsing techniques, accommodating various garment styles and body shapes [60]. AnyDoor demonstrates diffusion models' adaptability in seamlessly integrating clothing and accessories into images [61]. Innovations like CP-VTON and LaDI-VTON ensure realistic garment integration and texture preservation, expanding virtual try-on systems' applicability in e-commerce and the metaverse [62,63]. Fine-Grained Editing Techniques Diffusion models have transformed fine-grained editing techniques, enabling precise modifications tailored to user specifications. illustrates the categorization of these techniques, focusing on user-directed modifications, versatile image generation, and virtual try-on methods. The InstructEdit framework processes user instructions for high-quality image alterations, enhancing user-directed transformations [46]. StyleDrop allows users to synthesize images in specific styles with minimal input, highlighting diffusion models' adaptability in replicating intricate stylistic elements [64]. The Energy-based method's versatility across image generation tasks underscores diffusion models' capability for detailed modifications [65]. DragDiffusion enhances precision across scenarios, while the Palette framework showcases diverse applications in tasks like colorization and JPEG restoration [66,67]. In virtual try-on contexts, GP-VTON improves semantic preservation during garment transfer, reducing texture distortion [58]. The Parser-Free VTON method corrects artifacts through dual knowledge distillation, producing high-quality images [60]. The Emu method enhances aesthetics, demonstrating diffusion models' effectiveness in fine-grained editing [68]. Image Inpainting and Restoration Diffusion models have revolutionized image inpainting and restoration, offering advanced techniques for filling missing parts and restoring damaged images with high fidelity. The Segment-Anything Model (SAM) enhances mask selection and hole filling precision, ensuring seamless integration of filled regions [69]. PowerPaint exemplifies diffusion models' versatility by using learnable task prompts for context-aware filling, maintaining visual coherence [70]. These methodologies highlight diffusion models' role in providing pixel-level control and customization, outperforming traditional models like GANs. By adopting unified frameworks and standardized evaluation protocols, diffusion models pave the way for innovations across diverse fields, including video generation and interdisciplinary scientific applications [71,67,14,11,19]. Cross-Domain and Multimodal Applications Diffusion models significantly contribute to cross-domain and multimodal applications in image editing, integrating diverse data sources for sophisticated manipulations. InstructDiffusion interprets user instructions within pixel space, facilitating flexible image manipulation with multiple input modalities [72]. This capability enhances diffusion models' versatility in complex editing tasks, allowing for coherent and contextually relevant edits by processing information from various domains. Advanced guidance techniques enable seamless integration of visual, textual, and contextual data, producing highly customized outcomes. Recent advancements, such as structure and content-guided video diffusion and retrieval-augmented models, allow precise edits without extensive retraining, enriching multimedia content creation [73,74,14]. These developments underscore diffusion models' transformative impact, paving the way for innovations in image editing across text-to-image and image-to-text conversions, granular pixel-level control, and video content creation [71,75,14,11,18]. Challenges and Limitations Diffusion model-based image editing faces challenges that extend beyond technical constraints, encompassing high computational costs and the demand for real-time efficiency. Despite the capabilities of models like Google's Imagen and OpenAI's DALL-E 2, unresolved issues remain, particularly in real-world applications on user devices. While advancements have enabled powerful text-driven image manipulations, computational demands limit practical applications. Compact, unconditional diffusion models emerge as viable alternatives to text-conditional models, highlighting challenges in computational efficiency, semantic alignment, and input dependencies [18,11]. Understanding these factors is crucial for guiding future research and development. Computational Complexity and Resource Intensity The inherent computational complexity and resource intensity of diffusion model-based image editing pose significant challenges, especially for high-definition outputs and detailed manipulations. The reliance on separate classifiers complicates guidance processes, necessitating streamlined approaches [22]. Black-box ODE solvers introduce inefficiencies in sample generation, increasing computational demands [43]. DiffStyler's dual diffusion architecture enhances image quality but presents challenges in processing speed and resource requirements, limiting scalability [6]. Extensive datasets are required for optimal performance, complicating training stability and efficiency [8]. Generating high-definition videos demands substantial computational resources, restricting access to advanced synthesis capabilities [3]. The VQ-Diffusion model offers promising directions with faster performance while maintaining quality [1]. However, precise text prompts remain a computational hurdle in real image editing, complicating the process and demanding significant resources [28]. These challenges underscore the need for ongoing research to optimize performance and reduce computational demands, enhancing the accessibility of diffusion models. Semantic Misalignment and Mask Quality Semantic misalignment and reliance on high-quality masks are critical challenges in diffusion model-based image editing, affecting transformation precision. ShadowDiffusion's dependence on degradation model quality can lead to semantic issues [76], while Imagic highlights the impact of pre-trained model quality on outputs [77]. KV Inversion addresses challenges in reflecting intended actions while maintaining image characteristics [78]. Multi-concept images struggle with foreground and background harmony [7]. Null-text guidance faces semantic misalignment in artistic styles [56]. Methods risk losing essential attributes like colors during editing [4]. CoSeR attempts to integrate global semantic information [48]. Pseudo-target image quality significantly impacts editing [5]. Benchmarking efforts reveal trade-offs between diversity and fidelity, addressing semantic issues [2]. Continued research is needed to enhance mask quality, semantic alignment, and evaluation methodologies. Dependence on Input and Pre-trained Models Dependence on input data and pre-trained models limits diffusion-based editing processes, affecting accuracy and quality. Capturing and aligning semantic information from text prompts with generated images remains challenging [65]. This misalignment results in images that inadequately reflect intended semantic content, highlighting the need for improved integration between input data and model outputs. The quality of initial parser-based images can introduce artifacts into final outputs [60]. This reliance on high-quality input data underscores diffusion models' vulnerability to errors in pre-processing stages, compromising fidelity and coherence. Ongoing research is essential to enhance robustness and adaptability, ensuring effective utilization of diverse input data while maintaining high-quality outputs. Challenges in Handling Complex Edits and Instructions Managing complex edits and instructions within diffusion models presents significant challenges due to limited comprehension of diverse tasks and user intent [79]. Reliance on large datasets, like ImageNet, restricts adaptability in intricate scenarios [80]. Extensive datasets required for conditional adversarial networks complicate processes, demanding substantial computational resources [81]. The inherent complexity necessitates iterative processing through thousands of timesteps for accurate inference, rendering the process computationally expensive [82]. This approach is crucial for maintaining fidelity but impedes efficiency in handling complex edits requiring rapid transformations. Proposed methods illustrate limitations in complex modifications needing nuanced judgment [50]. Methods like [83] struggle to integrate contextual information beyond immediate features. Existing methods often fail to interpret succinct instructions, leading to suboptimal outcomes [54]. Addressing these challenges requires ongoing research to advance interpretative and computational capabilities, enabling efficient handling of complex edits. Evaluation and Benchmarking Challenges Evaluation and benchmarking of diffusion models in image editing present critical challenges for assessing performance and ensuring reliability. Establishing comprehensive metrics that capture diverse aspects of image quality, including fidelity, diversity, and alignment with input prompts, remains difficult [2]. Lack of standardized evaluation protocols complicates comparisons between diffusion models and traditional approaches [11]. Subjective assessments introduce variability, as demonstrated by Emu's high win rate against its pre-trained counterpart [68]. This reliance underscores the necessity for objective metrics that consistently evaluate aesthetic aspects. Balancing computational efficiency with evaluation accuracy is crucial, given resource-intensive models [43]. Efficient benchmarking methods that maintain depth and comprehensiveness are essential for adoption in practical scenarios. Innovative techniques, like classifier-free guidance [22], necessitate new criteria to assess impact on performance. Complexity in integrating multimodal inputs and managing edits further complicates benchmarking, requiring frameworks addressing these challenges. Addressing evaluation and benchmarking challenges is vital for advancing diffusion model capabilities. Recent surveys highlight revolutionized content creation, yet issues like computational costs persist. Leveraging open-source frameworks and diverse metrics allows rigorous testing and improvement, ensuring models meet application demands. Innovations in pixel-wise guidance, sketch-based editing, and real-time algorithms promise enhanced efficiency and adaptability, paving the way for practical applications [14,84,11,18,47]. Continued research is essential for establishing standardized protocols to assess diffusion models' capabilities and limitations in generating high-quality images. Table provides a detailed examination of the BigGAN benchmark, illustrating its role in the evaluation of image synthesis within the broader context of diffusion model benchmarking challenges. Future Directions Enhancements in Model Efficiency Improving the efficiency of diffusion models is crucial for reducing computational costs and increasing accessibility. Recent advancements focus on optimizing sampling speed and quality, such as DPM-Solver, which enhances generative task efficiency [43]. Analytic-DPM offers analytic estimates for optimal reverse variance and KL divergence, minimizing iteration needs [82]. WaveletDiff generates high-fidelity images rapidly, suitable for real-time applications [39]. Future research should explore encoder and decoder optimizations, scalability, and classifier-free guidance enhancements [22]. DiffStyler's computational intensity necessitates efficiency improvements [6]. The Blended Latent Diffusion Model (BLDM) proposes latent space optimizations [40]. Implicit Diffusion Models (IDM) could benefit from efficiency-focused research, particularly in image super-resolution tasks. Enhancing Nuwa-XL's adaptability across video genres and local diffusion processes could boost video generation capabilities [25]. These efforts underscore the need for optimizing diffusion models for faster training and innovative applications. Broader Applications and Domains Diffusion models hold transformative potential across various fields. Enhancing video diffusion models' adaptability to larger datasets and optimizing synthesis processes can improve quality and efficiency [85]. The CatVTON framework promises advancements in virtual try-on scenarios, revolutionizing personalized virtual fitting experiences [86]. Beyond image synthesis, diffusion models can tackle complex data integration and manipulation [87]. Enhancing the VITON framework's performance with diverse clothing types and poses points to significant advancements in image editing techniques [88]. The PD method's broad applicability in image processing tasks highlights its utility in addressing diverse degradation types [89]. Future research should improve model robustness against degradation types and enhance restoration pipeline efficiency [90]. The BigGAN benchmark's application to other generative models suggests further opportunities [80]. Improving image generation techniques could enhance video quality for real-time applications, fostering multimedia content creation innovation [91]. Optimizing CGANs for complex datasets could extend diffusion models into new data processing areas [92]. These directions emphasize diffusion models' potential to revolutionize multiple domains, paving the way for advancements in video synthesis, virtual try-on, and image restoration [21,11,14,57,19]. Improved Control and Customization Advancements in diffusion models focus on enhancing user control and customization in image editing, enabling precise transformations tailored to diverse needs. The TryOnDiffusion framework aims to improve garment detail extraction and generalization across datasets, enhancing user satisfaction in virtual try-on scenarios [1]. Future research should refine user interaction mechanisms and enhance model capabilities for complex edits [28]. Null-text guidance methods aim to enhance control over artistic styles, emphasizing user-centric design [56]. CoSeR framework research may explore improved control through cognitive embeddings for better image restoration [48]. Enhancements in locking mechanisms and T2I personalization optimizations could provide users greater control, fostering creative and flexible editing [93]. DiffStyler's dual diffusion approach incorporates cross-modal style information, offering intuitive control during synthesis [8]. Future research may focus on optimizing generator architecture for diverse text input-based edits [1]. Enhancing adaptability to broader model ranges and improving performance in complex occlusion situations ensures precise control over image attributes [7]. These advancements underscore the necessity of developing diffusion models prioritizing user control and customization, paving the way for personalized image editing innovations. Integration with Other Techniques Integrating diffusion models with advanced techniques can enhance functionality and versatility across domains. Hybridizing diffusion models with GANs can leverage strengths of both architectures, improving sampling efficiency and image quality [8]. Incorporating attention mechanisms and transformer architectures can refine complex image data processing, enabling precise and context-aware editing [49]. Multi-scale architectures and hierarchical processing techniques can improve scalability, enabling effective handling of larger datasets and complex image structures [3]. Integrating diffusion models with semantic understanding frameworks, like cognitive embeddings [48], enhances contextually relevant and semantically accurate output generation. Novel optimization techniques, including variational inference and manifold learning, can enhance performance, improving training efficiency and convergence rates [30]. Advanced sampling techniques, improved likelihood estimation, and specialized data handling can expand capabilities in image synthesis, video generation, and interdisciplinary fields. Combining diffusion models with other generative frameworks can offer versatile solutions for image editing and synthesis tasks, fostering innovative developments in computer vision, natural language processing, and universal AI research [75,11,19,14]. Addressing Current Limitations Addressing diffusion models' limitations in image editing requires enhancing stability, efficiency, and adaptability across tasks. Improving dataset construction and refining loss functions are pivotal for augmenting localized editing applications [5]. Additional guidance strategies could overcome text-conditional image synthesis limitations, enhancing fidelity and alignment [2]. Enhancing initial latent representation quality can improve accuracy and fidelity by addressing object-centric instruction challenges, underspecification, grounding, and faithfulness, while preserving unrelated image content. Improvements in paired data quality and techniques like Image Information Removal, prompt-mixing, and KV Inversion facilitate fine-grained edits, retain essential attributes, and achieve better editability-fidelity trade-offs [94,4,74,78,95]. Enhancements in mask extraction processes could significantly improve performance by ensuring contextually relevant edits. Refining bias correction methods and analytic estimates could further improve robustness and accuracy. Future work should explore CNN architecture refinements or integrate additional techniques to enhance high-frequency detail prediction, addressing image super-resolution task limitations. Recent advancements in diffusion-based image generation, such as Google's Imagen and OpenAI's DALL-E 2, have yielded impressive results but face unresolved challenges. Open-source stable diffusion models allow for analysis and improvement. Text-driven image manipulation innovations have led to efficient algorithms, reducing computational costs and enabling real-time applications. Compact, unconditional diffusion models could serve as viable alternatives to text-conditional models, broadening scope and applicability [18,11]. These directions emphasize continuous innovation in diffusion model development, paving the way for robust and versatile applications in image editing and beyond. Future research should focus on enhancing stability and integrating additional techniques to improve image quality, addressing limitations like temporal consistency in video generation, computational costs, and diverse dataset use. This exploration could leverage open-source stable diffusion frameworks and mathematical principles, drawing insights from advancements in text-to-video generation and architectural design to refine capabilities in models like Google's Imagen, OpenAI's DALL-E 2, and other stable diffusion models [11,14]. Standardization and Benchmarking Establishing standards and benchmarks for diffusion models is crucial for advancing image editing, facilitating evaluation and comparison to ensure consistent and reliable performance. Benchmarks like DrawBench emphasize aligning generated images with textual prompts, enhancing accuracy and quality [17]. Developing standardized protocols for assessing fidelity and diversity is essential for overcoming semantic misalignment and mask quality challenges [2]. Comprehensive metrics capturing various aspects of image quality allow for evaluating diversity-fidelity trade-offs, improving overall performance [11]. Subjective assessments introduce variability in benchmarking results, necessitating objective metrics consistently evaluating aesthetic aspects across models and tasks [68]. Integrating multimodal inputs and handling complex edits complicates benchmarking, requiring comprehensive frameworks to address multifaceted challenges. Addressing evaluation and benchmarking challenges is crucial for advancing diffusion model-based image editing, ensuring rigorous testing to meet diverse applications and user requirements. Continued research and development are essential for establishing standardized protocols effectively assessing diffusion models' capabilities and limitations, which show promise in generating high-quality, contextually relevant images. Models like Google's Imagen, OpenAI's DALL-E 2, and stable diffusion models have demonstrated impressive advancements in visual computing, yet challenges like computational costs and maintaining temporal consistency remain. Leveraging open-source frameworks and insights from text-driven image manipulation and video diffusion models can refine evaluation metrics and datasets, enhancing performance and applicability across domains [21,11,14,18,57]. Conclusion Diffusion models have fundamentally reshaped the landscape of image editing by offering advanced capabilities in generating high-quality, diverse, and semantically coherent edits. Their recent developments have markedly enhanced computational efficiency and effectiveness, especially in text-driven image editing, revolutionizing conventional image manipulation techniques. The integration of novel strategies such as transformer-based autoregressive methods has expanded the scope for zero-shot generation, fostering innovation in the field. Models like WaveDM exemplify the efficiency improvements, achieving traditional performance benchmarks at accelerated speeds. The achievements of GLIGEN in open-world grounded text-to-image generation highlight the models' proficiency in creating varied and coherent image transformations. Furthermore, Direct Inversion showcases the detailed precision achievable with diffusion models, underscoring their transformative potential in precise image modifications. As diffusion models continue to emerge as a pivotal element in generative modeling, the survey underscores the importance of enhancing data quality to propel image editing practices forward. Classifier-free guidance offers a streamlined yet potent approach to generating samples, delivering results on par with established methods. Collectively, these advancements reflect the substantial impact of diffusion models in redefining image editing, setting the stage for ongoing progress and broader applications across multiple domains.",
  "reference": {
    "1": "2111.14822v3",
    "2": "2112.10741v3",
    "3": "2210.02303v1",
    "4": "2305.17489v2",
    "5": "2305.05947v1",
    "6": "2211.10682v2",
    "7": "2403.10983v2",
    "8": "1812.04948v3",
    "9": "2209.04747v6",
    "10": "2211.12446v2",
    "11": "2308.13142v1",
    "12": "2307.06949v2",
    "13": "2309.15103v2",
    "14": "2310.10647v2",
    "15": "2210.10960v2",
    "16": "2312.10113v1",
    "17": "2205.11487v1",
    "18": "2304.04344v1",
    "19": "2209.00796v15",
    "20": "2306.04139v2",
    "21": "2306.04139v2",
    "22": "2207.12598v1",
    "23": "2303.08714v3",
    "24": "1907.05600v3",
    "25": "2303.12346v1",
    "26": "2309.15818v3",
    "27": "2311.01410v2",
    "28": "2302.03027v1",
    "29": "2006.11239v2",
    "30": "2201.11793v3",
    "31": "2104.07636v2",
    "32": "2302.13848v2",
    "33": "2304.06140v3",
    "34": "2305.11147v3",
    "35": "2206.00941v3",
    "36": "2310.07222v1",
    "37": "2010.02502v4",
    "38": "2209.14687v4",
    "39": "2211.16152v2",
    "40": "2206.02779v2",
    "41": "2303.09472v3",
    "42": "2304.08818v2",
    "43": "2206.00927v3",
    "44": "2210.11427v1",
    "45": "2211.13227v1",
    "46": "2305.18047v1",
    "47": "2212.02024v3",
    "48": "2311.16512v4",
    "49": "2309.10556v2",
    "50": "2305.04651v1",
    "51": "2309.00613v2",
    "52": "2303.10073v2",
    "53": "2306.16894v2",
    "54": "2309.17102v2",
    "55": "2309.04372v2",
    "56": "2305.06710v4",
    "57": "2310.07204v1",
    "58": "2303.13756v1",
    "59": "2312.01725v1",
    "60": "2103.04559v2",
    "61": "2307.09481v2",
    "62": "1807.07688v3",
    "63": "2305.13501v3",
    "64": "2306.00983v1",
    "65": "2306.09869v3",
    "66": "2306.14435v6",
    "67": "2111.05826v2",
    "68": "2309.15807v1",
    "69": "2304.06790v1",
    "70": "2312.03594v4",
    "71": "2306.00950v2",
    "72": "2309.03895v1",
    "73": "2302.03011v1",
    "74": "2207.13038v1",
    "75": "2211.08332v4",
    "76": "2212.04711v2",
    "77": "2210.09276v3",
    "78": "2309.16608v1",
    "79": "2311.10089v1",
    "80": "1809.11096v2",
    "81": "1611.07004v3",
    "82": "2201.06503v3",
    "83": "2307.02421v2",
    "84": "2304.09748v1",
    "85": "2305.10474v3",
    "86": "2407.15886v2",
    "87": "2105.05233v4",
    "88": "1711.08447v4",
    "89": "2211.13524v1",
    "90": "2308.15070v3",
    "91": "2311.10709v2",
    "92": "1411.1784v1",
    "93": "2305.01644v2",
    "94": "2310.19145v1",
    "95": "2303.11306v2"
  },
  "chooseref": {
    "1": "2306.04139v2",
    "2": "1812.04948v3",
    "3": "2308.13142v1",
    "4": "2306.04139v2",
    "5": "2310.10647v2",
    "6": "2312.03594v4",
    "7": "2302.05543v3",
    "8": "2304.08818v2",
    "9": "2304.06140v3",
    "10": "2201.06503v3",
    "11": "2307.09481v2",
    "12": "2206.02779v2",
    "13": "2309.14709v3",
    "14": "2106.15282v3",
    "15": "2407.15886v2",
    "16": "2207.12598v1",
    "17": "2208.09392v1",
    "18": "2302.09778v2",
    "19": "1411.1784v1",
    "20": "2111.13606v1",
    "21": "2303.05125v1",
    "22": "2311.18608v2",
    "23": "2311.16512v4",
    "24": "2310.13165v2",
    "25": "1806.06137v3",
    "26": "1503.03585v8",
    "27": "2304.07090v1",
    "28": "2010.02502v4",
    "29": "2006.11239v2",
    "30": "2201.11793v3",
    "31": "2203.14206v1",
    "32": "2303.10073v2",
    "33": "2212.06512v4",
    "34": "2308.15070v3",
    "35": "2210.11427v1",
    "36": "2306.00950v2",
    "37": "2303.09472v3",
    "38": "2306.14685v4",
    "39": "2211.10682v2",
    "40": "2111.15640v3",
    "41": "2306.00219v2",
    "42": "2210.10960v2",
    "43": "2105.05233v4",
    "44": "2209.04747v6",
    "45": "2401.00736v3",
    "46": "2209.00796v15",
    "47": "2209.14687v4",
    "48": "2306.00986v3",
    "49": "2209.15264v2",
    "50": "2110.02711v6",
    "51": "2211.07825v1",
    "52": "2206.00386v1",
    "53": "2206.00927v3",
    "54": "2306.14435v6",
    "55": "2307.02421v2",
    "56": "2312.03771v1",
    "57": "2309.15664v1",
    "58": "2211.12446v2",
    "59": "2309.04907v1",
    "60": "2210.09292v3",
    "61": "2207.06635v5",
    "62": "2302.13848v2",
    "63": "2206.00364v2",
    "64": "2311.10089v1",
    "65": "2311.10709v2",
    "66": "2309.15807v1",
    "67": "2306.09869v3",
    "68": "1609.03126v4",
    "69": "2305.07015v4",
    "70": "2304.06720v4",
    "71": "2309.11321v1",
    "72": "2309.14934v1",
    "73": "2212.02024v3",
    "74": "2312.10113v1",
    "75": "2309.10556v2",
    "76": "2211.13524v1",
    "77": "1902.05687v4",
    "78": "1907.05600v3",
    "79": "2112.10741v3",
    "80": "2301.07093v2",
    "81": "2303.13756v1",
    "82": "2309.17102v2",
    "83": "2304.11829v2",
    "84": "2305.12966v4",
    "85": "2204.06125v1",
    "86": "2303.09618v2",
    "87": "2307.06949v2",
    "88": "2108.02938v2",
    "89": "2301.11699v3",
    "90": "2104.07636v2",
    "91": "1611.07004v3",
    "92": "2308.00906v1",
    "93": "2212.06909v2",
    "94": "2210.02303v1",
    "95": "2210.09276v3",
    "96": "2303.16491v2",
    "97": "2102.09672v1",
    "98": "2006.09011v2",
    "99": "2206.00941v3",
    "100": "2304.06790v1",
    "101": "2304.03411v1",
    "102": "2312.06738v4",
    "103": "2309.03895v1",
    "104": "2305.18047v1",
    "105": "2211.09800v2",
    "106": "2211.13203v3",
    "107": "2308.06721v1",
    "108": "2302.04841v3",
    "109": "2309.00613v2",
    "110": "2305.01644v2",
    "111": "2309.16608v1",
    "112": "2305.13501v3",
    "113": "1809.11096v2",
    "114": "2309.15103v2",
    "115": "2305.18676v1",
    "116": "2310.19145v1",
    "117": "2311.16711v2",
    "118": "2303.11306v2",
    "119": "2310.02848v1",
    "120": "2211.11018v2",
    "121": "2203.13131v1",
    "122": "2209.14792v1",
    "123": "2304.08465v1",
    "124": "1705.07057v4",
    "125": "2308.06571v1",
    "126": "2309.04372v2",
    "127": "2112.05744v4",
    "128": "2305.16807v2",
    "129": "1711.00937v2",
    "130": "2305.06710v4",
    "131": "2303.12346v1",
    "132": "2403.10983v2",
    "133": "2210.03142v3",
    "134": "2211.13227v1",
    "135": "2111.05826v2",
    "136": "2103.04559v2",
    "137": "2306.16894v2",
    "138": "2205.11487v1",
    "139": "2305.18286v1",
    "140": "2310.00426v3",
    "141": "1601.06759v3",
    "142": "2211.12572v1",
    "143": "2302.07979v2",
    "144": "2305.10474v3",
    "145": "2305.04441v1",
    "146": "2208.01626v1",
    "147": "2509.17367v1",
    "148": "2304.09748v1",
    "149": "2305.04651v1",
    "150": "2303.08714v3",
    "151": "2207.14626v2",
    "152": "2401.13627v2",
    "153": "2011.13456v2",
    "154": "2307.01952v1",
    "155": "2212.04711v2",
    "156": "2309.15818v3",
    "157": "2212.04489v2",
    "158": "2212.05034v1",
    "159": "2312.05039v1",
    "160": "2211.14305v2",
    "161": "2311.15127v1",
    "162": "2312.01725v1",
    "163": "2310.07204v1",
    "164": "2302.03011v1",
    "165": "2308.07863v1",
    "166": "2303.15649v3",
    "167": "2306.00983v1",
    "168": "2304.00186v5",
    "169": "2311.16432v2",
    "170": "2212.07603v2",
    "171": "2207.13038v1",
    "172": "2305.17489v2",
    "173": "2311.01410v2",
    "174": "1807.07688v3",
    "175": "2304.04344v1",
    "176": "2303.15403v2",
    "177": "2306.08276v1",
    "178": "2312.14611v1",
    "179": "2212.08698v1",
    "180": "2305.16322v3",
    "181": "2310.07222v1",
    "182": "2305.11147v3",
    "183": "2104.05358v1",
    "184": "1505.05770v6",
    "185": "2111.14822v3",
    "186": "2211.11319v1",
    "187": "2211.08332v4",
    "188": "2204.03458v2",
    "189": "2309.00398v2",
    "190": "1711.08447v4",
    "191": "2312.03667v1",
    "192": "2308.08947v1",
    "193": "2305.13819v2",
    "194": "2211.16152v2",
    "195": "2208.05003v1",
    "196": "2212.00490v2",
    "197": "2302.03027v1",
    "198": "2102.12092v2",
    "199": "2312.16794v2",
    "200": "2305.05947v1"
  }
}