{
  "survey": "Diffusion models have emerged as a transformative force in image editing, offering robust frameworks for generating high-quality, diverse, and controllable visual content. This survey provides a comprehensive overview of recent advancements in diffusion model-based image editing, highlighting their superiority over traditional generative adversarial networks (GANs) in overcoming semantic inconsistencies and training complexities. The introduction of instruction-based editing methods, such as InstructPix2Pix, enhances user interaction through natural language instructions, addressing limitations in zero-shot capabilities. Diffusion models have also advanced text-conditioned image editing, action editing, and image super-resolution, improving fidelity and efficiency. Despite challenges such as slow sampling times and computational demands, innovations like classifier-free guidance demonstrate the versatility of diffusion models in generating stylized images. Recent advancements include novel methodologies that enhance flexibility and user control, such as multimodal large language models and diffusion autoencoders. The survey aims to provide insights into the rapid advancements and applications of diffusion models, addressing existing knowledge gaps and facilitating future research and development. By examining the scope and challenges of diffusion models, the survey underscores their potential to revolutionize image editing, paving the way for continued innovation and expansion across diverse domains.\n\nIntroduction Significance of Diffusion Model-Based Image Editing Diffusion models have transformed image editing by enabling high-quality, diverse visual content generation with capabilities such as real-time text-driven manipulation, pixel-level editing, and controlled modifications. They facilitate efficient semantic adjustments based on text descriptions, customizable intensities for individual pixels or regions, and seamless integration of edited subjects into new backgrounds. Recent innovations have enhanced their efficiency, making them a viable alternative to traditional methods and expanding user-driven creativity across applications [1,2,3,4]. Unlike generative adversarial networks (GANs), which often face semantic inconsistencies and adversarial training challenges, diffusion models offer a robust framework for image synthesis and manipulation, particularly in text-to-image (T2I) and text-to-video (T2V) generation, addressing computational costs and text-video alignment limitations. The advent of instruction-based editing methods, such as InstructPix2Pix (IP2P), enhances user interaction through natural language instructions [5], addressing the zero-shot capabilities of existing models for global and local image editing. Additionally, diffusion models excel in text-conditioned image editing, particularly in action editing, where existing methods struggle with action semantics while preserving original content. Significant advancements have also been made in image super-resolution (SR), a domain historically challenged by over-smoothing and artifacts, necessitating breakthroughs for enhanced fidelity. The flexibility of diffusion models in personalizing text-to-image models further underscores their impact [6]. They enable text-driven image stylization, allowing users to transform natural images based on textual descriptions, thus expanding creative possibilities in image editing [7]. Despite their strengths, diffusion models face challenges, including slow sampling times due to numerous function evaluations, impacting their practicality in real-time applications [8]. The instability of deterministic diffusion inversion models (DDIM) for real images highlights the need for further improvements in diffusion models for effective editing [9]. Nevertheless, advancements such as classifier-free guidance have proven effective in generating stylized images, including cartoon-style visuals, showcasing the versatility of diffusion models. These advancements illustrate the integral role of diffusion models in advancing image editing techniques, offering improved efficiency, accuracy, and fidelity in image generation and manipulation [10]. Thus, diffusion models signify a paradigm shift in image editing, promising ongoing innovation in high-quality, diverse, and controllable visual content creation. Recent Advancements Recent advancements in diffusion model-based image editing have significantly broadened their capabilities and applications, introducing innovative methodologies that enhance flexibility and user control. A notable development is a guidance method that combines joint video and image training, enabling explicit control over temporal consistency in video outputs [11], thus demonstrating the potential of diffusion models to produce coherent and temporally consistent video content. The integration of multimodal large language models (MLLMs) further enhances the understanding and execution of editing instructions, facilitating intuitive user interactions and precise, context-aware editing processes [12]. The introduction of Diffusion Autoencoders marks a significant advancement in representation learning, enabling applications previously challenging for GANs [13]. Incorporating mixture-of-expert (MOE) controllers with task-specific adaptation training enhances the adaptability and precision of diffusion models for diverse image manipulation tasks [14]. Additionally, a mask-free approach to image retouching simplifies the editing process, representing a substantial advancement [15]. Innovations such as the asymmetric reverse process (Asyrp), which introduces a semantic latent space named h-space, further enhance the generative capabilities of diffusion models [11]. The Blended Latent Diffusion method accelerates local text-driven image editing by operating in a lower-dimensional latent space, improving efficiency and responsiveness [16]. Imagic showcases recent advancements by enabling complex text-based edits to a single real image using a pre-trained text-to-image diffusion model, highlighting the versatility of diffusion models in facilitating intricate edits [17]. Furthermore, Perfusion enhances text-to-image personalization while maintaining visual fidelity and efficiency [6]. The pix2pix-zero method automates the discovery of editing directions in the text embedding space, offering a more intuitive approach to image editing [18]. The VQ-Diffusion model addresses limitations of traditional text-to-image generation methods, representing a recent advancement in diffusion model-based editing [10]. Recent findings indicate that null-text guidance can be leveraged to create cartoons, reflecting ongoing advancements in diffusion model-based image editing [19]. The development of DiffStyler, a dual diffusion processing architecture, balances content and style during image stylization [7]. Collectively, these advancements illustrate the dynamic evolution of diffusion models in image editing, paving the way for future innovations that promise to further enhance the quality, efficiency, and diversity of visual content creation. Proposed ideas include an exact formulation of the solution to diffusion ODEs, simplifying the computation of the linear solution [8]. A benchmark comparison between CLIP guidance and classifier-free guidance highlights the latter's superiority in generating photorealistic images [20]. Purpose of the Survey This survey aims to provide a comprehensive insight into diffusion models, which have become pivotal in Artificial Intelligence for General Creativity (AIGC) [4]. By offering a thorough overview, the survey addresses rapid advancements and applications across various domains, identifying key areas of focus and research gaps [21]. It reviews denoising diffusion models in vision, addressing existing knowledge gaps in generative modeling and applications [22]. Additionally, it examines diffusion generative models for video generation, focusing on critical components such as applications, architectural design, and temporal dynamics modeling [23]. The survey scrutinizes existing issues and solutions related to image generation models, particularly those based on diffusion processes [24]. It also addresses the challenge of slow sampling speeds in diffusion probabilistic models, which limits practical applications [8]. Collectively, the survey endeavors to provide a comprehensive understanding of diffusion models, facilitating future research and development in this evolving field. Scope of the Survey This survey offers an in-depth examination of diffusion models, focusing on their applications in image synthesis, video generation, and molecule design, while excluding generative models that do not utilize diffusion processes [24]. It encompasses theoretical and practical advancements related to denoising diffusion models in computer vision, underscoring their role in enhancing visual content creation [22]. The scope extends to diffusion models specifically tailored for video generation, exploring applications across diverse input modalities such as text prompts, images, videos, and audio [23]. Furthermore, it addresses inefficiencies and misalignment issues in existing diffusion models, particularly in realistic image deblurring [25]. The survey examines recent advances in diffusion models for vision, concentrating on design elements impacting computational efficiency [26]. It delves into diffusion models for image super-resolution, discussing theoretical foundations, methodologies, and current research directions while excluding unrelated generative approaches [27]. The challenges of text-driven image manipulation methods based on unconditional diffusion models, which are currently too slow for practical use, are also investigated [1]. Moreover, the limitations of existing methods in providing flexible control for visual editing through multiple modalities, including audio, images, and text, are explored [28]. The survey encompasses the fundamental formulation of diffusion models, algorithmic advancements, and their applications across various domains such as imagery, text, speech, biology, and healthcare [4]. However, it excludes generative diffusion models applied to structured data, including tabular and time series data, due to the extensive existing literature on visual and textual data [29]. Lastly, the survey investigates diffusion models in generating and editing 2D images, videos, 3D objects, locomotion, and 4D scenes, focusing specifically on diffusion-based tools due to their rapid advancement and relevance in recent literature [30]. Collectively, this survey aims to provide a comprehensive understanding of diffusion models, facilitating future research and development in this evolving field. Structure of the Survey The survey is systematically organized to explore diffusion model-based image editing comprehensively, beginning with an introductory section that addresses the significance, recent advancements, purpose, and scope of the survey. This foundational section establishes the context and objectives, guiding readers through the motivations and intended contributions. Following the introduction, the survey delves into the background and core concepts, offering a detailed examination of the fundamental principles of diffusion models, generative models, and neural networks. This section elucidates the diffusion process and its pivotal role in image synthesis and editing, providing a comparative analysis with traditional models to highlight the distinct advantages of diffusion approaches. The subsequent section on advancements in diffusion model-based image editing presents a thorough survey of the latest techniques and methodologies, emphasizing innovations that enhance image quality and fidelity. This exploration sets the stage for the applications section, showcasing diverse uses of diffusion models in image editing, including synthesis, style transfer, inpainting, restoration, virtual try-on, object manipulation, and text-guided editing. Real-world examples and case studies illustrate practical implementations and successes. The survey then addresses challenges and limitations of diffusion model-based image editing, highlighting issues such as high computational complexity, significant data requirements, and input image quality. It examines the limitations of current models, including reliance on high-end GPUs for practical applications, and discusses evaluation challenges. Recent advancements aimed at improving efficiency, such as novel algorithms that significantly reduce computation time and innovative approaches incorporating sketches for enhanced editability and fine-grained generation, are also explored [1,31]. This critical analysis identifies areas needing improvement and informs discussions on future directions. Finally, the survey concludes with a forward-looking perspective, suggesting potential research directions and opportunities for innovation. This section emphasizes enhancing model robustness, optimizing computational efficiency, expanding applications across disciplines, improving user control, exploring novel techniques, and addressing social and ethical considerations. The conclusion reinforces the survey's key points, underscoring the importance of diffusion models in advancing image editing capabilities.The following sections are organized as shown in . Background and Core Concepts Fundamental Concepts of Diffusion Models Diffusion models represent an advanced class of generative models that transform complex datasets into simpler distributions through iterative noise addition and removal. This bidirectional mechanism consists of a forward process introducing Gaussian noise and a reverse process reconstructing the original data distribution [32]. These models are particularly adept at generating coherent, high-resolution visual content from textual descriptions, addressing limitations of traditional models [10]. A pivotal feature is the use of semantic latent spaces, enabling nuanced semantic edits on real images via text descriptions [33]. The integration of vector quantization enhances text-to-image generation, as demonstrated by the Vector Quantized Diffusion Model (VQ-Diffusion) [10]. Diffusion models excel in personalized image generation, employing dynamic rank-1 updates to incorporate user-specific visual concepts [33]. Their hierarchical refinement process enhances image quality through progressive denoising, making them ideal for high-fidelity applications like image super-resolution. The adaptability of these models allows handling of complex generative tasks through coarse-to-fine processes, using global diffusion models for keyframe generation and local models for content filling [20]. The style-based generator architecture (SGA) further demonstrates their versatility in integrating cross-modal style information while maintaining content structure [32]. Collectively, these concepts underscore the transformative impact of diffusion models on generative modeling, driving advancements in computational paradigms and algorithmic strategies within visual computing contexts. Generative Models and Neural Networks Generative models and neural networks are integral to diffusion models, enabling the transformation of noise into structured, high-fidelity visual content. Techniques employing cross-attention mechanisms, such as cross-attention Keys and gated strategies, modulate concept influence during inference, enhancing precision and control in diffusion-based image editing [6]. This synergy is further highlighted by their capacity for localized edits based on textual prompts, showcasing adaptability in complex generative tasks [33]. Neural networks enhance generative processes by integrating various forms of prior knowledge, as seen in methods combining image and degradation priors within a unified framework. Generative adversarial networks (GANs) exemplify this integration, where a generative model and a discriminative model are trained adversarially to refine data distribution. Advanced architectures like style-based generators improve attribute separation and synthesis control, while conditional GANs allow for generation conditioned on specific data, such as class labels. Additionally, generative models using score matching and Langevin dynamics provide flexible architectures achieving high-quality sample generation without adversarial training, demonstrating their capability to produce state-of-the-art results across diverse datasets [34,35,36,37]. This integration enhances diffusion models' performance and precision, showcasing sophisticated algorithmic strategies employed by neural architectures to manage intricate generative challenges. The integration of generative models and neural networks within diffusion frameworks is crucial for advancing image synthesis and editing capabilities. These models have revolutionized human creativity by generalizing patterns across diverse domains. Notably, diffusion models transform human ideation into tangible outcomes and are enhanced by novel architectures like retrieval-augmented diffusion models (RDMs), which leverage external databases to refine visual styles during training and inference. This approach, alongside multimodal models like CLIP, has significantly elevated the field of AI-Art, enabling sophisticated prompt engineering and superior visual quality in synthesized images [38,4]. These models leverage advanced neural architectures and algorithmic strategies to produce high-quality, diverse, and controllable visual content, driving innovation and expanding the boundaries of visual content creation. Diffusion Process in Image Synthesis The diffusion process is fundamental to image synthesis, employing a transformative mechanism that iteratively refines random noise into structured, high-resolution images. It encompasses a forward diffusion step, where noise is incrementally introduced to the data, and a reverse step that reconstructs the image from this noise, refining samples through a series of learned diffusion steps [39]. These methodologies utilize stochastic differential equations and score-based techniques to facilitate seamless transitions between data and noise distributions, enhancing sample generation [40]. An application of the diffusion process is the Instant-Booth framework, which utilizes pre-trained text-to-image models for personalized image generation without requiring fine-tuning at test time, transforming input images into global embeddings [11]. The prompt-mixing method further exemplifies diffusion models' versatility by generating variations of a specific object's shape in images, detailing the denoising process involved [16]. These techniques underscore the adaptability of diffusion models in managing complex generative tasks. The ILVR method refines the generative process of Denoising Diffusion Probabilistic Models (DDPM) by producing high-quality images based on reference images, enhancing the model's controllability [40]. In video synthesis, the MagicVideo framework improves the diffusion process by utilizing a pre-trained Variational Autoencoder (VAE) to map video clips into a latent space, allowing for efficient learning of video distributions [41]. This integration of latent space conditioning illustrates the potential of diffusion models in generating coherent and temporally consistent video content. The Emu method leverages a latent diffusion process, combining extensive pre-training on large datasets with targeted fine-tuning using high-quality images to optimize image generation for aesthetic appeal [42]. Furthermore, PFB-Diff integrates text-guided generated content into target images through a progressive blending approach that operates at multiple feature levels, showcasing diffusion models' versatility in facilitating effective image restoration and manipulation [7]. In video editing, the structured latent noise space for DDPM supports various editing operations by extracting edit-friendly noise maps, demonstrating diffusion models' flexibility in producing high-quality visual content across different media formats [43]. The AlignYourL method involves pre-training a Latent Diffusion Model (LDM) on images and then fine-tuning it on video sequences to achieve temporally consistent video generation [42]. This exemplifies the relevance of diffusion processes in both image and video synthesis. The diffusion process in image synthesis is a powerful mechanism that enables the creation of intricate and high-quality visual content. By exploiting time-aware encoders and controllable feature wrapping modules, diffusion models enhance image resolution and synthesis outcomes, paving the way for continued innovation in generating diverse and controllable visual content [44]. Comparative Analysis with Traditional Models Diffusion models significantly advance traditional image editing frameworks, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), by offering enhanced capabilities for complex generative tasks. Traditional models often struggle with measurement noise and nonlinearity, while diffusion models, as exemplified by EDICT, provide stable inversion processes that improve image reconstruction fidelity [9]. This capability is particularly advantageous in scenarios requiring high fidelity and robust performance, where traditional models may falter due to instability and low resolution during training [45]. A notable strength of diffusion models is their flexibility and applicability, often limited in existing methods due to reliance on multiple images or specific types of edits [17]. Innovations like the Diffusion over Diffusion architecture enable parallel generation of long videos, contrasting with traditional methods' sequential generation approach [46]. Moreover, diffusion models excel in preserving original image content while allowing for specific edits, addressing challenges highlighted by the pix2pix-zero method [18]. This capability is crucial for ensuring high-quality outputs, which traditional models often struggle to achieve across varying degradation types [47]. Despite these advantages, diffusion models face challenges related to computational costs and processing times, as the extensive inference iterations required can lead to misalignment with target results [25]. Additionally, existing methods struggle with occlusions while maintaining identity integrity and balance between foreground and background elements [48]. Traditional models may offer more streamlined solutions, as seen in the limitations of classifier guidance methods that depend on separately trained image classifiers [49]. Furthermore, the lack of semantic meaning in latent variables within existing diffusion models poses obstacles for representation learning, a domain where GANs have traditionally excelled [13]. Existing benchmarks often focus on ordinary differential equations (ODEs), which do not fully capture the benefits of stochastic differential equations (SDEs) utilized by diffusion models, leading to suboptimal performance in image editing tasks [50]. The high number of sampling steps required by diffusion models results in slower processing times compared to traditional models like GANs, which are often more efficient in generating rapid outputs [51]. Additionally, existing methods do not effectively utilize null-text guidance for cartoonization, limiting their capabilities in generating cartoon-like images [19]. These comparisons highlight diffusion models' transformative impact on image editing capabilities by enabling efficient, high-quality text-driven manipulations and layered controlled editing. This allows for semantic attribute modifications and seamless subject integration into new backgrounds while maintaining visual coherence and expressiveness with significantly reduced computational costs [1,3]. They provide robust frameworks for generating high-quality, diverse, and controllable visual content across multiple applications and media formats, underscoring their potential to overcome traditional generative models' limitations. In recent years, the field of image editing has witnessed significant advancements, particularly through the development of diffusion model-based techniques. These innovations have not only transformed the landscape of image synthesis but also enhanced the quality and fidelity of edited images. As illustrated in , this figure categorizes the innovative techniques and methodologies employed in this domain, highlighting key approaches such as VQ-Diffusion and iEdit. Furthermore, it underscores the diverse applications of these techniques across various fields, emphasizing the resultant improvements in image quality and fidelity. The impact of these advancements on a range of synthesis and editing tasks is profound, marking a pivotal shift in how images are manipulated and produced. Advancements in Diffusion Model-Based Image Editing Innovative Techniques and Methodologies Recent developments in diffusion model-based image editing have introduced techniques that significantly enhance precision, efficiency, and adaptability in image manipulation. The VQ-Diffusion model employs a mask-and-replace strategy, improving text-to-image generation through precise edits [10], showcasing the adaptability of diffusion models in efficient visual content creation. iEdit constructs pseudo-target images and utilizes segmentation masks for localized editing [33], transforming image generators into versatile tools for nuanced editing. The Glide-Towa benchmark highlights the impact of guidance techniques on image quality in diffusion models [20], marking advancements in optimizing content quality. Blended Latent Diffusion blends latents at each diffusion step, enhancing speed and accuracy of local edits [16]. The FoI method introduces effective mask extraction and cross-attention modulation, emphasizing user control in editing [5]. These methodologies reflect the dynamic evolution of diffusion models, promising future innovations in visual content creation across diverse domains, including imagery, text, speech, biology, healthcare, and structured data. Recent developments broaden their applicability to structured data, addressing previous research gaps, while integration with other generative models enhances results. Novel techniques in video diffusion models set benchmarks in temporally coherent high-fidelity video generation [52,21,4,29]. Table provides a comprehensive summary of recent advancements in diffusion model-based image editing, showcasing the methodological innovations and application domains of prominent techniques. illustrates the recent advancements in diffusion model-based image editing, highlighting key methodologies such as VQ-Diffusion, iEdit, and GLIDE for image generation, as well as Blended Latent and Focus on You for local image editing, alongside novel video generation techniques. This visual representation complements the discussion by providing a concise overview of the techniques that are shaping the future of image manipulation. Enhancements in Image Quality and Fidelity Advancements in diffusion model-based image editing have notably improved image quality and fidelity, often surpassing traditional models like GANs. These improvements arise from diffusion models' ability to disentangle style attributes without altering semantic content, excelling in both unconditional and conditional synthesis. Techniques such as classifier guidance and pixel-wise manipulation offer controllable and efficient editing while maintaining better distribution coverage compared to GANs [53,54,1,55]. Lower Fr√©chet Inception Distance (FID) scores reflect superior fidelity, with cascaded models demonstrating impressive scores at various resolutions. Innovations like KV Inversion achieve satisfactory reconstruction and action editing without extensive dataset training, enhancing quality and fidelity [56]. DiffStyler preserves structural information during stylization, applying desired styles with high fidelity [7]. The AnyDoor framework excels in zero-shot customization, outperforming existing methods in applications like virtual try-on and object swapping [57], underscoring diffusion models' adaptability in generating coherent content across diverse contexts. Collectively, these innovations reflect diffusion models' dynamic evolution, promising future enhancements in visual content creation. Table provides a comparative analysis of various diffusion model-based image editing methods, emphasizing their impact on image quality, editing techniques, and model adaptability. These advancements underscore diffusion models' pivotal role in advancing image quality and fidelity, offering robust frameworks for diverse synthesis and editing tasks across varied domains. Recognized as a leading force in generative modeling, diffusion models are poised to enhance content creation by materializing human ideation into diverse tangible instances, expanding boundaries across imagery, text, speech, biology, and healthcare. Recent advancements introduce algorithmic enhancements and new applications, such as temporally coherent video generation and structured data modeling, promising to drive innovation and redefine Artificial Intelligence for General Creativity (AIGC) [52,4,29,21]. Applications of Diffusion Models in Image Editing Diffusion models have revolutionized image editing, offering diverse applications and innovative techniques. This section examines their role in enhancing image synthesis, style transfer, inpainting, restoration, virtual try-on, and object manipulation, highlighting their capabilities and field advancements. The following subsection delves into image synthesis and style transfer, focusing on key methodologies and their implications for creative visual transformations. Image Synthesis and Style Transfer Diffusion models are pivotal in image synthesis and style transfer, facilitating creative transformations previously challenging to achieve. Through iterative noise refinement, these models generate high-quality, diverse visuals while preserving content integrity during style application [58]. Stylediffusion allows for controllable disentangled style transfer, extracting content and learning style attributes without compromising underlying content [59]. WarpDiffusion enhances virtual try-on images by combining warping and diffusion techniques, focusing on realistic garment transformations [60]. CP-VTON adapts in-shop clothes to the target person's body shape while preserving garment identity, showcasing diffusion models' transformative potential in virtual try-on applications [61]. Advancements in diffusion-based image synthesis and style transfer signify a dynamic evolution in generative modeling, impacting imagery, text, and speech fields. Innovations like retrieval-augmented diffusion models (RDMs) and text-driven image manipulation have improved visual quality and efficiency, enabling real-time, high-quality image editing. Diffusion models' disentanglement capabilities facilitate seamless style modifications without altering semantic content, enhancing visual content diversity and precision. These developments lay the groundwork for future innovations aimed at elevating visual content creation's quality, efficiency, and diversity, establishing diffusion models as indispensable tools in the creative and technological landscape [53,38,4,1]. Inpainting and Restoration Diffusion models have advanced image inpainting and restoration, providing sophisticated techniques ensuring high fidelity and semantic coherence in reconstructed images. The Inpaint Anything (IA) method enables interactive selection and filling of image areas without traditional masking, addressing mask selection and effective hole filling challenges [62]. PowerPaint employs learnable task prompts to guide model focus on various inpainting targets, effectively handling multiple tasks simultaneously, demonstrating diffusion models' adaptability in complex scenarios [63]. Recent diffusion-based inpainting advancements illustrate a dynamic evolution marked by innovations in controllability and efficiency. Differential diffusion models enable pixel-level customization for nuanced editing, while retrieval-augmented diffusion models enhance text-guided synthesis by conditioning on external databases, surpassing traditional prompt-engineering methods. These innovations promise future breakthroughs to enhance visual content creation's quality and diversity across various domains [38,2,4,1]. Virtual Try-On and Object Manipulation Diffusion models have significantly improved virtual try-on and object manipulation applications, ensuring realism and coherence in synthesized images. The CatVTON model utilizes a concatenation approach for garment transfer onto target individuals, enhancing realism and adaptability in virtual try-on processes [64]. VITON enables clothing item superimposition onto images without relying on 3D data, maintaining visual coherence and realism [65]. LaDI-VTON integrates latent diffusion with textual inversion mechanisms, generating realistic images of models wearing garments from online shops [66]. WarpDiffusion focuses on realistic garment synthesis around garment-skin boundaries, crucial for achieving realistic virtual try-on outcomes [60]. CP-VTON and GP-VTON address garment identity preservation and detail retention in complex poses [61,67]. StableVITON enhances virtual try-on by learning semantic correspondence between clothing and the human body, generating high-quality images [68]. Parser-Free Virtual Try-On improves image quality through a 'teacher-tutor-student' knowledge distillation approach, correcting artifacts [69]. The AnyDoor framework exemplifies diffusion models' versatility in object manipulation, enabling seamless integration of objects into diverse environments while maintaining visual coherence [57]. These advancements signify a transformative progression in generative modeling techniques, enhancing visual content creation's quality and efficiency while broadening diversity through multimodal approaches. Exploring generative diffusion models for structured data is poised to expand applications beyond traditional domains, promising future innovations to revolutionize artificial intelligence-driven creativity [1,52,29,38,4]. Text-Guided Image Editing Text-guided image editing marks a significant advancement in diffusion model applications, enabling visual content transformation based on textual descriptions. This approach generates edits aligning with input text prompts while maintaining consistency with the original image [70]. A prominent method involves injecting guidance image features into the generation process, integrating textual cues with visual elements [71]. Innovative methodologies like Prompt-to-Prompt editing retain core image attributes during modifications [72], showcasing diffusion models' adaptability for nuanced, context-aware edits. LayerDiffusion introduces semantic-based layered control, enhancing precision in subject integration into new backgrounds [3]. Region-based diffusion allows local style control and explicit token reweighting, improving text-guided editing capabilities [73]. Forgedit employs vision-language joint optimization and novel vector projection mechanisms to enhance editing precision and speed [74]. These advancements reflect diffusion models' dynamic evolution in text-guided image editing, paving the way for future innovations to enhance visual content creation's quality, efficiency, and diversity [75]. Applications and Case Studies Diffusion models demonstrate significant potential across various real-world applications, particularly in virtual try-on and object manipulation. VITON excels in image-based virtual try-on tasks, enabling seamless clothing integration onto individual images without 3D data, maintaining visual coherence [65]. SmartMask illustrates superior object insertion quality and background preservation, enabling effective mask-free insertion and detailed layout generation [76]. StableVITON exemplifies diffusion models' effectiveness in virtual try-on, outperforming baseline methods in qualitative and quantitative evaluations, generating high-quality images that preserve garment integrity during transformations [68]. These case studies highlight diffusion models' transformative evolution in practical applications, showcasing record-breaking performance in fields like image synthesis, video generation, and molecule design, while integrating into diverse domains like computer vision, natural language processing, and healthcare. By focusing on efficient sampling and improved likelihood estimation, these studies pave the way for future innovations enhancing visual content creation's quality, efficiency, and diversity, exploring the potential for combining diffusion models with other generative approaches for greater impact [21,4]. Challenges and Limitations Computational Complexity and Resource Intensity Diffusion models, despite their transformative impact on image editing, face significant challenges related to computational complexity and resource demands, which impede scalability. Their iterative nature requires numerous steps to achieve high-quality transformations, imposing substantial computational loads, especially in video generation where maintaining temporal coherence exacerbates these constraints [10,42,8]. Traditional diffusion methods prove inefficient in image restoration tasks due to extensive iterations needed to estimate entire images or feature maps [33,77]. Complex instructions in techniques like MOE control further complicate resource management [32]. The quality of pre-trained models is crucial for effective manipulation; suboptimal models can degrade output quality [19]. Variability from subjective user studies complicates computational assessments [20]. Performance limitations in methods such as VQ-Diffusion underscore the need for advancements in the mathematical foundations of these models [10,42]. While DPM-Solver provides efficient inference without extensive training, its reliance on initial predictions can compromise performance if low-frequency components are poorly captured [8]. Balancing high fidelity with creative control, avoiding overfitting, and maintaining compact model sizes remain substantial challenges [77,19]. Addressing these computational demands is vital for the evolution and applicability of diffusion models, which excel in image synthesis, video generation, and molecular design. Optimizing computational efficiency, sampling methods, and likelihood estimation, alongside exploring integration with other generative models, is essential for real-world adaptability. Additionally, addressing energy consumption and environmental concerns has spurred research into more efficient diffusion models [26,21,4,29]. Data Requirements and Generalizability Diffusion models' performance and versatility in image editing are heavily influenced by data requirements and generalizability. Extensive, high-quality training datasets are typically necessary for optimal performance across diverse scenarios. Limitations arise from reliance on specific models, such as those in the OMG framework, which can restrict versatility [48]. This underscores the importance of quality data sources for robust model performance. The dependency on training dataset quality and diversity affects performance in less varied contexts, limiting generalizability [35]. In text-guided image editing, the precision of input text descriptions is crucial for stylization fidelity, as evidenced by the DiffStyler method [7]. This reliance on textual quality complicates achieving consistent edits across diverse applications. Innovative data handling strategies and model design improvements are essential to enhance diffusion models' robustness and applicability. Addressing these challenges is critical for improving performance in domains such as computer vision, natural language processing, and healthcare, where diffusion models have demonstrated remarkable capabilities [1,26,21,29,4]. Quality of Input and Dependency on Pre-trained Models The efficacy of diffusion model-based image editing is closely linked to input quality and reliance on pre-trained models, posing significant challenges for consistent performance. Input data quality is paramount, particularly in text-guided scenarios with complex prompts [33]. This sensitivity highlights challenges in achieving coherent edits when input data lacks precision. Pre-trained models, such as Denoising Diffusion Implicit Models (DDIM), are foundational to many diffusion methodologies, with their robustness directly impacting output fidelity. Poorly trained models can lead to inconsistent outcomes, underscoring the critical role of pre-trained models in applications like image super-resolution and molecular design. Leveraging the generative prior in these models allows superior results across varying resolution requirements without altering core synthesis [21,78]. Variability in input quality and prompt precision complicates diffusion models' generalizability. Advanced text-to-image generation benefits from incorporating human feedback and scene-based controls to enhance fidelity and relevance. Approaches like Make-a-Scene and HIVE emphasize domain-specific knowledge and computational efficiency in managing out-of-distribution prompts [1,79,80]. Improving input quality and optimizing pre-trained models are essential for robust diffusion model applications in image editing. Addressing these issues is crucial for advancing diffusion models' capabilities in transforming human creativity into tangible outputs across diverse fields, including imagery, text, speech, biology, and healthcare. Algorithmic enhancements and structured data challenges will promote further developments in diffusion models [21,4,29]. Model Limitations and Scalability Diffusion models, despite their transformative potential in image editing, encounter notable limitations and scalability challenges. Accurate text guidance is crucial for outputs; unclear instructions can lead to suboptimal results, highlighting the need for precise inputs [81]. Methods like VideoGen depend heavily on reference image quality; poor-quality references can compromise efficacy, affecting scalability across complex tasks [82]. Computational inefficiencies inherent to diffusion processes impact usability, particularly in applications requiring rapid processing [22,16]. Scaling diffusion models to complex datasets or tasks requiring nuanced representations is challenging, particularly for methods reliant on neural discretization techniques, which may struggle with increased complexity and variability [83]. This underscores the need for advancements in model design and algorithmic strategies to enhance scalability and adaptability. Continued innovation in diffusion methodologies is essential for improving computational efficiency, input quality handling, and adaptability across diverse applications. Addressing these issues will enhance diffusion models' capabilities, critical in translating creativity into practical applications across fields like imagery, text, speech, biology, and healthcare. Emphasizing foundational principles and algorithmic improvements will ensure effective deployment in creative and underexplored areas, such as structured data modeling [4,29]. Evaluation and Benchmarking Challenges Evaluating and benchmarking diffusion models in image editing presents challenges affecting assessment reliability. A significant concern is reliance on human evaluations, which may introduce subjectivity, impacting objectivity and reproducibility [70]. Standardized, objective evaluation metrics are necessary to consistently measure performance across applications. To address the evaluation challenges faced by diffusion models in image editing applications, Table delineates a representative benchmark framework, serving as a necessary tool to gauge performance objectively and consistently. Diffusion models' complexity, stemming from iterative noise refinement processes, necessitates comprehensive evaluation frameworks that capture their capabilities in generating high-quality visual content while addressing computational overhead and energy consumption [26,29]. The absence of universally accepted benchmarks complicates evaluation, as existing methods may not fully account for diffusion-based approaches' strengths. Variability in input quality and precision of textual and visual prompts also affects benchmarking outcomes, underscoring the need for comprehensive protocols that accommodate diverse conditions. Such protocols are essential to advance the field by addressing limitations, enhancing efficiency, and improving generative diffusion models' quality [84,52,85,29]. These challenges underscore the need for ongoing innovation in evaluation and benchmarking methodologies to effectively assess diffusion models as they achieve groundbreaking performance and expand into areas like structured data modeling. Developing objective, standardized, and comprehensive frameworks is crucial for advancing diffusion models' capabilities and facilitating deployment across varied creative and technical domains [4,29]. Future Directions Enhancing Model Robustness and Adaptability Enhancing the robustness and adaptability of diffusion models is crucial for advancing their capabilities in image editing and expanding their applicability across diverse domains. Future research should focus on refining methodologies like the Blended Latent Diffusion method for faster performance in local image editing tasks, thereby increasing adaptability to dynamic scenarios [16]. Innovations in the inversion process, such as those proposed in EDICT, offer further opportunities for advancements in image editing [9]. Optimizing training processes for joint models, as discussed in Classifier methods, aligns with efforts to bolster model robustness and adaptability [49]. To improve adaptability to dynamic models and generative tasks, optimizing underlying architectures is essential, particularly in exploring diffusion models' adaptability to diverse video styles and complex narratives. Future research on Perfusion could focus on enhancing the locking mechanism for more robust generalization across varied user inputs [6]. Strengthening DiffStyler's robustness against text description variations and integrating user feedback further represent avenues for enhancing adaptability [7]. Research should also investigate enhancements in noise blending techniques and expand frameworks like OMG to accommodate a broader variety of models [48]. Optimizing solver applicability to a wider range of diffusion models, as suggested in DPM-Solver, could significantly enhance performance [8]. Further, refining generator architecture, as indicated in the Astyle-based approach, can assess versatility beyond human faces [35]. Moreover, enhancing the cognitive embedding process in CoSeR and exploring its application in other image processing tasks could bolster robustness [32]. The adaptability of methods like pix2pix-zero to various image types and editing requirements represents another promising area for future exploration [18]. Improving the IIR module's capability to manage diverse attributes and reduce overfitting in different contexts offers potential enhancements in model adaptability [77]. Future directions should also focus on enhancing the robustness of iEdit to handle more complex editing tasks and improving the quality of pseudo-target images [33]. These research directions underscore the transformative potential of diffusion models in enhancing image editing capabilities, enabling real-time, text-driven manipulations, fine-grained pixel-level control, and layered editing strategies. Such advancements facilitate efficient modifications of semantic attributes, seamless integration of edited elements, and precise customization of changes at both global and granular levels. The development of novel algorithms and frameworks, such as LayerDiffusion and differential diffusion, illustrates significant improvements in editing speed, coherence, and user control, while maintaining original image integrity [1,2,53,55,3]. By enhancing robustness and adaptability, diffusion models can overcome current limitations and pave the way for future advancements in generative modeling. Optimizing Computational Efficiency Optimizing computational efficiency is crucial for enhancing the practicality of diffusion models, particularly in real-time applications. Emerging trends emphasize the necessity for sustainable and efficient diffusion models that minimize computational overhead while maintaining performance [23]. Future research should focus on optimizing the computational efficiency of EMILIE, thereby improving the performance of diffusion models for real-time applications [86]. The wavelet-based diffusion scheme significantly reduces processing time, addressing the computational complexity of traditional diffusion models [51]. Similarly, optimizing the computational efficiency of the prompt-mixing method and exploring its applications beyond object shape variations could enhance its practicality [87]. Enhancing models for faster training and inference, as suggested in denoising diffusion models, represents another promising area for improving computational efficiency [43]. Future work could also optimize methods for efficiency, expand the range of styles and actions they can generate, and improve user interfaces for easier text input [42]. Additionally, optimizing the computational efficiency of the DPS method suggests directions for enhancing its practical applicability [88]. Improving computational efficiency in diffusion models can significantly broaden their applicability in real-time scenarios. Collectively, these research directions highlight the critical need to enhance computational efficiency in diffusion models, which are pivotal in advancing generative AI applications across various domains, including visual computing, video content creation, and multimodal tasks. This optimization is essential for enabling versatile and scalable implementations, as seen in models like Versatile Diffusion and InstructDiffusion, which integrate multiple tasks and modalities, offering fine-grained control over image synthesis. Such advancements pave the way for broader adoption and innovation in fields ranging from computer vision to artificial general intelligence [89,30,90,91,23]. By reducing computational demands, diffusion models can become more accessible and practical, facilitating their integration into real-time and resource-constrained environments. Expanding Applications and Interdisciplinary Integration The future trajectory of diffusion models in image editing is marked by the potential for significant expansion across various applications and interdisciplinary domains. Recognizing the versatility of diffusion models, there is considerable scope for integrating them with other generative models to enhance outcomes. Future research should explore optimizations of the VQ-Diffusion model and expand its applications across different domains [10], leading to more robust and adaptable models capable of addressing complex generative tasks. In guidance techniques, future research could investigate additional strategies and expand datasets to enhance benchmark applicability and robustness [20]. This could be complemented by advancements in retrieval systems and adaptable frameworks that facilitate style transfer and enable broader use in augmented and virtual realities. The integration of various conditional image generation tasks within advanced frameworks highlights significant potential for enhancing image editing applications and fostering interdisciplinary collaboration. Recent advancements, such as incorporating sketches and reference images for fine-grained control, along with efficient text-driven manipulation algorithms, demonstrate the diverse possibilities for user-driven modifications and real-time editing. Innovations like composable conditions and learnable regions offer flexible control over spatial layout and content creation while maintaining high synthesis quality [1,92,31,93,72]. Future research could explore enhancements to the multi-speed diffusion framework and its application in other generative modeling tasks, suggesting potential interdisciplinary integration. Additionally, enhancing models with more complex conditioning data and investigating their application in domains such as image tagging and intricate multi-modal data generation could further expand the applicability of diffusion models. Expanding the dataset of high-quality images for fine-tuning and applying quality-tuning to other model architectures could significantly enhance image generation capabilities by leveraging retrieval-augmented diffusion models (RDMs) and latent video diffusion models. RDMs improve generative image synthesis by conditioning the model on informative samples retrieved from an external database, allowing for more precise specification of visual styles during inference. Meanwhile, latent video diffusion models, adapted for high-resolution text-to-video and image-to-video generation, benefit from systematic curation and high-quality video finetuning. This approach enhances visual quality and enables the development of powerful motion representations for downstream tasks and adaptability to specific visual styles or camera motions [38,94]. Future work could explore enhancements to mechanisms like attention masking and investigate additional editing tasks or applications that could benefit from this approach. Building on recent advancements in diffusion models, these research directions highlight their transformative potential in revolutionizing image editing capabilities. Notably, diffusion models facilitate real-time, text-driven image manipulation, allowing for semantic edits based on textual descriptions while significantly reducing computational costs. LayerDiffusion enables layered controlled editing, maintaining subject-background consistency and opening new possibilities for controllable image editing. Moreover, frameworks like Differential Diffusion offer granular control over pixel-level changes, enabling nuanced edits and seamless integration of new elements. Collectively, these innovations promise to elevate the quality, efficiency, and diversity of visual content creation across various domains, heralding an era of Artificial Intelligence for General Creativity (AIGC) [1,2,3,4]. Improving User Control and Customization Enhancing user control and customization in diffusion model applications is pivotal for advancing image editing capabilities, allowing users to achieve precise and personalized outcomes. Future research should focus on simplifying user interfaces and integrating additional features to enhance user experience and image generation quality, as suggested in the exploration of rich-text interfaces [73]. This approach emphasizes intuitive design and functionality in facilitating effective user interaction with diffusion models. Improving model adaptability to diverse clothing styles and body shapes, as indicated in the CP-VTON framework, represents another avenue for enhancing user control and customization in virtual try-on applications [61]. This advancement could lead to more personalized and realistic outcomes, broadening the applicability of diffusion models in fashion and retail industries. In image inpainting tasks, the Inpaint Anything (IA) method exemplifies efforts to improve user control by enabling users to interactively remove objects and fill holes seamlessly, enhancing the customization process in image restoration applications [62]. This capability underscores the potential of diffusion models to facilitate precise and context-aware modifications driven by sophisticated algorithmic strategies. Future research could explore automated evaluation methods or expand benchmarks to include more diverse image types and editing scenarios, as suggested in the context of image editing [70]. These advancements would contribute to refining user control mechanisms, ensuring robust and adaptable diffusion model applications across varied creative and technical domains. The research directions outlined in these references underscore the transformative potential of diffusion models in revolutionizing user control and customization across various domains. By enabling granular control over image modifications, facilitating real-time text-driven manipulations, and integrating with structured data applications, diffusion models pave the way for future innovations that promise to significantly enhance the quality, efficiency, and diversity of visual content creation. These advancements highlight the models' ability to materialize human creativity into tangible forms, offering profound insights into their developmental trajectory and expanding their applicability beyond traditional boundaries [1,21,2,29,4]. Exploring Novel Generative Techniques and Frameworks Exploring novel generative techniques and frameworks is essential for advancing diffusion models in image editing, facilitating the development of innovative methodologies that enhance image generation quality. Future research should address unanswered questions regarding the optimization of diffusion models, as highlighted in existing literature, to improve their efficiency and effectiveness across applications [24]. This exploration could refine underlying algorithms and integrate new techniques that enhance the fidelity and diversity of generated visual content. Additionally, refining disturbance methods and exploring their applications in styles of image generation beyond cartoons, as suggested in recent studies, represents a promising avenue for expanding the utility of diffusion models [19]. This approach emphasizes developing versatile frameworks that can adapt to diverse generative tasks, broadening the scope of diffusion models across creative and technical domains. Building on recent advancements in diffusion models, these research directions highlight their transformative potential in revolutionizing image editing capabilities. Notably, diffusion models facilitate real-time, text-driven image manipulation, allowing for semantic edits based on textual descriptions while significantly reducing computational costs. LayerDiffusion enables layered controlled editing, maintaining subject-background consistency and opening new possibilities for controllable image editing. Moreover, frameworks like Differential Diffusion offer granular control over pixel-level changes, enabling nuanced edits and seamless integration of new elements. Collectively, these innovations promise to elevate the quality, efficiency, and diversity of visual content creation across various domains, heralding an era of Artificial Intelligence for General Creativity (AIGC) [1,2,3,4]. By encouraging the exploration of novel generative techniques and frameworks, diffusion models can continue to drive innovation and expand the boundaries of generative modeling. Addressing Social Implications and Ethical Considerations The application of diffusion models in image editing and generative AI technologies presents significant social implications and ethical considerations that warrant careful examination. As these models evolve and integrate into various domains, addressing potential impacts on privacy, security, and societal norms is crucial. Future research should explore emerging trends in diffusion models, address identified gaps, and investigate the social implications of generative AI technologies [30]. One primary concern is the potential misuse of diffusion models for generating deceptive or harmful content, exacerbating issues related to misinformation and digital manipulation. The advent of advanced diffusion models and text-to-image generation technologies enables the creation of highly realistic images and videos, raising significant concerns about the authenticity of digital media. This development necessitates robust verification mechanisms to ensure content integrity, as these models can manipulate images with high fidelity and efficiency, often surpassing human perceptual preferences. While these technologies offer new capabilities in image editing and scene generation, they also introduce challenges such as high computational demands and the need for improved explainability, underscoring the urgent need for innovative solutions to authenticate and verify digital content in real-time [1,27,80]. Additionally, ethical considerations surrounding the use of diffusion models in sensitive applications, such as healthcare and surveillance, require careful scrutiny. Deploying these advanced generative diffusion models in scenarios involving personal data or vulnerable populations necessitates adherence to ethical principles prioritizing user consent, robust data protection, and transparency. This is particularly important given the models' capabilities in transforming human creativity across diverse domains like imagery, text, speech, biology, and healthcare. Although diffusion models have demonstrated groundbreaking performance, their application to structured data such as tabular and time series data remains underexplored, emphasizing the need for careful consideration of ethical guidelines to ensure responsible use across all data modalities [4,29]. Moreover, the societal impact of diffusion models on creative industries and employment must be considered. While generative models like diffusion models provide groundbreaking tools for content creation across various domains, they also present challenges to traditional creative professions. These models, capable of translating human ideation into tangible outputs and offering unprecedented control over artistic production, are reshaping the landscape of artistic creation and consumption. Innovations like retrieval-augmented diffusion models and composable conditions enable detailed customization and prompt-engineering, allowing artists to specify visual styles and manipulate images with precision. Despite high computational demands, advancements in algorithmic efficiency are making these tools more accessible, potentially transforming how art is created and experienced, and raising important questions about the future of traditional artistic roles [38,4,1,92]. Collectively, addressing these social implications and ethical considerations is essential for ensuring the responsible development and deployment of diffusion models. By fostering interdisciplinary collaboration and establishing ethical guidelines, researchers and practitioners can effectively navigate the complexities of generative AI technologies, such as diffusion models, which are instrumental in transforming human creativity across various domains. These models, especially retrieval-augmented diffusion models, enhance generative image synthesis through innovative techniques like prompt-engineering, allowing for precise visual style specification and promoting positive impacts while mitigating potential risks [38,4]. Conclusion The survey underscores the significant advancements diffusion models have brought to image editing, positioning them as superior alternatives to traditional techniques. These models have demonstrated remarkable proficiency in zero-shot text-to-image generation, effectively challenging and often surpassing existing domain-specific methods. Notably, diffusion models have achieved substantial improvements in image quality, particularly in tasks like super-resolution, which calls for continued exploration of unresolved challenges. Experimental evidence supports the integration of diffusion models as a means to enhance reconstruction accuracy and computational efficiency, thereby cementing their pivotal role in the evolution of image editing. Innovative methods have notably advanced user-driven image manipulation, improving both editability and user experience. Techniques such as the InST method have excelled in capturing and transferring artistic styles with unprecedented quality and efficiency, setting new standards in the field. The application of diffusion probabilistic models has resulted in state-of-the-art achievements in image synthesis, underscoring their robust generative capabilities. Further developments, like EMILIE, have facilitated iterative refinements and multi-granular control, thereby expanding the functional scope of image editing. The efficacy of expressive instructions in instruction-based editing is highlighted by enhancements in both automated metrics and human evaluations, as demonstrated by methods like MGIE. Moreover, the ability to generate high-quality images from spoken language accentuates the importance of innovations in image retouching. The survey also highlights classifier-free guidance as a promising alternative to traditional classifier-based methods, emphasizing its contribution to advancing image editing capabilities. Collectively, these advancements illustrate the dynamic progression of diffusion models, indicating a trajectory of future innovations poised to enhance the quality, efficiency, and diversity of visual content creation across multiple domains.",
  "reference": {
    "1": "2304.04344v1",
    "2": "2306.00950v2",
    "3": "2305.18676v1",
    "4": "2306.04139v2",
    "5": "2312.10113v1",
    "6": "2305.01644v2",
    "7": "2211.10682v2",
    "8": "2206.00927v3",
    "9": "2211.12446v2",
    "10": "2111.14822v3",
    "11": "2210.10960v2",
    "12": "2309.17102v2",
    "13": "2111.15640v3",
    "14": "2309.04372v2",
    "15": "2212.07603v2",
    "16": "2206.02779v2",
    "17": "2210.09276v3",
    "18": "2302.03027v1",
    "19": "2305.06710v4",
    "20": "2112.10741v3",
    "21": "2209.00796v15",
    "22": "2209.04747v6",
    "23": "2310.10647v2",
    "24": "2308.13142v1",
    "25": "2305.12966v4",
    "26": "2210.09292v3",
    "27": "2401.00736v3",
    "28": "2312.06738v4",
    "29": "2306.04139v2",
    "30": "2310.07204v1",
    "31": "2304.09748v1",
    "32": "2311.16512v4",
    "33": "2305.05947v1",
    "34": "1902.05687v4",
    "35": "1812.04948v3",
    "36": "1907.05600v3",
    "37": "1411.1784v1",
    "38": "2207.13038v1",
    "39": "2201.06503v3",
    "40": "2305.16807v2",
    "41": "2309.15103v2",
    "42": "2210.02303v1",
    "43": "2303.16491v2",
    "44": "2303.08714v3",
    "45": "2006.09011v2",
    "46": "2303.12346v1",
    "47": "2212.00490v2",
    "48": "2403.10983v2",
    "49": "2207.12598v1",
    "50": "2311.01410v2",
    "51": "2211.16152v2",
    "52": "2204.03458v2",
    "53": "2212.08698v1",
    "54": "2105.05233v4",
    "55": "2212.02024v3",
    "56": "2309.16608v1",
    "57": "2307.09481v2",
    "58": "2306.14685v4",
    "59": "2308.07863v1",
    "60": "2312.03667v1",
    "61": "1807.07688v3",
    "62": "2304.06790v1",
    "63": "2312.03594v4",
    "64": "2407.15886v2",
    "65": "1711.08447v4",
    "66": "2305.13501v3",
    "67": "2303.13756v1",
    "68": "2312.01725v1",
    "69": "2103.04559v2",
    "70": "2212.06909v2",
    "71": "2211.12572v1",
    "72": "2208.01626v1",
    "73": "2304.06720v4",
    "74": "2309.10556v2",
    "75": "2302.07979v2",
    "76": "2312.05039v1",
    "77": "2305.17489v2",
    "78": "2305.07015v4",
    "79": "2303.09618v2",
    "80": "2203.13131v1",
    "81": "2310.02848v1",
    "82": "2309.00398v2",
    "83": "1711.00937v2",
    "84": "2302.04841v3",
    "85": "2206.00364v2",
    "86": "2309.00613v2",
    "87": "2303.11306v2",
    "88": "2209.14687v4",
    "89": "2211.08332v4",
    "90": "2309.03895v1",
    "91": "2112.05744v4",
    "92": "2302.09778v2",
    "93": "2311.16432v2",
    "94": "2311.15127v1"
  },
  "chooseref": {
    "1": "2306.04139v2",
    "2": "1812.04948v3",
    "3": "2308.13142v1",
    "4": "2306.04139v2",
    "5": "2310.10647v2",
    "6": "2312.03594v4",
    "7": "2302.05543v3",
    "8": "2304.08818v2",
    "9": "2304.06140v3",
    "10": "2201.06503v3",
    "11": "2307.09481v2",
    "12": "2206.02779v2",
    "13": "2309.14709v3",
    "14": "2106.15282v3",
    "15": "2407.15886v2",
    "16": "2207.12598v1",
    "17": "2208.09392v1",
    "18": "2302.09778v2",
    "19": "1411.1784v1",
    "20": "2111.13606v1",
    "21": "2303.05125v1",
    "22": "2311.18608v2",
    "23": "2311.16512v4",
    "24": "2310.13165v2",
    "25": "1806.06137v3",
    "26": "1503.03585v8",
    "27": "2304.07090v1",
    "28": "2010.02502v4",
    "29": "2006.11239v2",
    "30": "2201.11793v3",
    "31": "2203.14206v1",
    "32": "2303.10073v2",
    "33": "2212.06512v4",
    "34": "2308.15070v3",
    "35": "2210.11427v1",
    "36": "2306.00950v2",
    "37": "2303.09472v3",
    "38": "2306.14685v4",
    "39": "2211.10682v2",
    "40": "2111.15640v3",
    "41": "2306.00219v2",
    "42": "2210.10960v2",
    "43": "2105.05233v4",
    "44": "2209.04747v6",
    "45": "2401.00736v3",
    "46": "2209.00796v15",
    "47": "2209.14687v4",
    "48": "2306.00986v3",
    "49": "2209.15264v2",
    "50": "2110.02711v6",
    "51": "2211.07825v1",
    "52": "2206.00386v1",
    "53": "2206.00927v3",
    "54": "2306.14435v6",
    "55": "2307.02421v2",
    "56": "2312.03771v1",
    "57": "2309.15664v1",
    "58": "2211.12446v2",
    "59": "2309.04907v1",
    "60": "2210.09292v3",
    "61": "2207.06635v5",
    "62": "2302.13848v2",
    "63": "2206.00364v2",
    "64": "2311.10089v1",
    "65": "2311.10709v2",
    "66": "2309.15807v1",
    "67": "2306.09869v3",
    "68": "1609.03126v4",
    "69": "2305.07015v4",
    "70": "2304.06720v4",
    "71": "2309.11321v1",
    "72": "2309.14934v1",
    "73": "2212.02024v3",
    "74": "2312.10113v1",
    "75": "2309.10556v2",
    "76": "2211.13524v1",
    "77": "1902.05687v4",
    "78": "1907.05600v3",
    "79": "2112.10741v3",
    "80": "2301.07093v2",
    "81": "2303.13756v1",
    "82": "2309.17102v2",
    "83": "2304.11829v2",
    "84": "2305.12966v4",
    "85": "2204.06125v1",
    "86": "2303.09618v2",
    "87": "2307.06949v2",
    "88": "2108.02938v2",
    "89": "2301.11699v3",
    "90": "2104.07636v2",
    "91": "1611.07004v3",
    "92": "2308.00906v1",
    "93": "2212.06909v2",
    "94": "2210.02303v1",
    "95": "2210.09276v3",
    "96": "2303.16491v2",
    "97": "2102.09672v1",
    "98": "2006.09011v2",
    "99": "2206.00941v3",
    "100": "2304.06790v1",
    "101": "2304.03411v1",
    "102": "2312.06738v4",
    "103": "2309.03895v1",
    "104": "2305.18047v1",
    "105": "2211.09800v2",
    "106": "2211.13203v3",
    "107": "2308.06721v1",
    "108": "2302.04841v3",
    "109": "2309.00613v2",
    "110": "2305.01644v2",
    "111": "2309.16608v1",
    "112": "2305.13501v3",
    "113": "1809.11096v2",
    "114": "2309.15103v2",
    "115": "2305.18676v1",
    "116": "2310.19145v1",
    "117": "2311.16711v2",
    "118": "2303.11306v2",
    "119": "2310.02848v1",
    "120": "2211.11018v2",
    "121": "2203.13131v1",
    "122": "2209.14792v1",
    "123": "2304.08465v1",
    "124": "1705.07057v4",
    "125": "2308.06571v1",
    "126": "2309.04372v2",
    "127": "2112.05744v4",
    "128": "2305.16807v2",
    "129": "1711.00937v2",
    "130": "2305.06710v4",
    "131": "2303.12346v1",
    "132": "2403.10983v2",
    "133": "2210.03142v3",
    "134": "2211.13227v1",
    "135": "2111.05826v2",
    "136": "2103.04559v2",
    "137": "2306.16894v2",
    "138": "2205.11487v1",
    "139": "2305.18286v1",
    "140": "2310.00426v3",
    "141": "1601.06759v3",
    "142": "2211.12572v1",
    "143": "2302.07979v2",
    "144": "2305.10474v3",
    "145": "2305.04441v1",
    "146": "2208.01626v1",
    "147": "2509.17367v1",
    "148": "2304.09748v1",
    "149": "2305.04651v1",
    "150": "2303.08714v3",
    "151": "2207.14626v2",
    "152": "2401.13627v2",
    "153": "2011.13456v2",
    "154": "2307.01952v1",
    "155": "2212.04711v2",
    "156": "2309.15818v3",
    "157": "2212.04489v2",
    "158": "2212.05034v1",
    "159": "2312.05039v1",
    "160": "2211.14305v2",
    "161": "2311.15127v1",
    "162": "2312.01725v1",
    "163": "2310.07204v1",
    "164": "2302.03011v1",
    "165": "2308.07863v1",
    "166": "2303.15649v3",
    "167": "2306.00983v1",
    "168": "2304.00186v5",
    "169": "2311.16432v2",
    "170": "2212.07603v2",
    "171": "2207.13038v1",
    "172": "2305.17489v2",
    "173": "2311.01410v2",
    "174": "1807.07688v3",
    "175": "2304.04344v1",
    "176": "2303.15403v2",
    "177": "2306.08276v1",
    "178": "2312.14611v1",
    "179": "2212.08698v1",
    "180": "2305.16322v3",
    "181": "2310.07222v1",
    "182": "2305.11147v3",
    "183": "2104.05358v1",
    "184": "1505.05770v6",
    "185": "2111.14822v3",
    "186": "2211.11319v1",
    "187": "2211.08332v4",
    "188": "2204.03458v2",
    "189": "2309.00398v2",
    "190": "1711.08447v4",
    "191": "2312.03667v1",
    "192": "2308.08947v1",
    "193": "2305.13819v2",
    "194": "2211.16152v2",
    "195": "2208.05003v1",
    "196": "2212.00490v2",
    "197": "2302.03027v1",
    "198": "2102.12092v2",
    "199": "2312.16794v2",
    "200": "2305.05947v1"
  }
}