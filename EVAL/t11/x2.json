{
  "survey": "Diffusion models have emerged as a transformative force in image editing and synthesis, offering significant advancements over traditional generative techniques like GANs. This survey provides a comprehensive overview of diffusion models, particularly denoising diffusion probabilistic models (DDPM), which excel in generating high-fidelity images through iterative refinement processes. The survey highlights the models' efficacy in text-guided image editing and their flexibility in applications such as image super-resolution, inpainting, and style transfer. Key innovations include the integration of stochastic differential equations for enhanced editing capabilities and frameworks like Composer for creative recombination. Despite challenges in computational complexity and scalability, diffusion models demonstrate superior performance in generating diverse and high-quality outputs. The survey also explores the models' potential across domains like virtual try-on and healthcare, underscoring their versatility. Future directions emphasize optimizing real-time applications, enhancing robustness and adaptability, and integrating with other generative models to expand their utility. The survey concludes by affirming the critical role of diffusion models in advancing generative AI, advocating for continued innovation to address existing limitations and explore new applications.\n\nIntroduction Significance of Diffusion Models Diffusion models represent a significant advancement in image editing and synthesis, enhancing the generation of high-quality visuals by effectively addressing the complexities of data distributions that challenge traditional generative techniques like Generative Adversarial Networks (GANs) [1]. Notably, denoising diffusion probabilistic models (DDPM) have demonstrated exceptional performance in unconditional image generation, marking a pivotal shift in image editing methodologies [2]. The iterative refinement process intrinsic to diffusion models progressively transforms noise into coherent images, which is crucial for high-quality synthesis. This capability is particularly advantageous in text-guided image editing, enabling semantic attribute manipulation based on textual inputs and broadening creative possibilities [3]. The adaptability of diffusion models is further illustrated by frameworks such as Composer, which decompose images into representative factors for creative recombination. Moreover, diffusion models have improved upon the limitations of ordinary differential equations (ODE) in image editing, leveraging stochastic differential equations (SDE) for enhanced performance [4]. Their utility extends to image super-resolution, effectively addressing over-smoothing and artifacts prevalent in existing methods [5]. Comprehensive surveys underline the effectiveness of diffusion models in generative modeling, contributing to the production of high-quality samples [6]. The rapid evolution and growing body of literature on diffusion models emphasize their critical role in advancing visual computing and image editing, particularly in overcoming the limitations of prior methods in both image and video generation [7]. Objectives of the Survey This survey aims to provide a thorough exploration of diffusion models in image editing, focusing on recent technological advancements and methodological innovations. It seeks to bridge understanding gaps by examining the core principles and mathematical formulations of diffusion models, highlighting their applications in image generation akin to models such as Google's Imagen and OpenAI's DALL-E 2. The survey categorizes models based on input modalities and discusses the challenges and solutions encountered in current image generation technologies. Furthermore, it reviews efficient sampling techniques, improvements in likelihood estimation, and the integration of diffusion models with other generative frameworks. By consolidating recent research, this survey serves as a valuable resource for researchers and practitioners, offering insights into ongoing challenges and future directions for diffusion models in image editing [8,9,10,11,6]. The survey also aims to unify understanding of diffusion models applied to image super-resolution amidst a burgeoning literature volume [12]. Additionally, it highlights the versatility of diffusion models across various domains, including imagery, text, speech, biology, and healthcare [8]. Moreover, this survey endeavors to explore the potential of diffusion models for precise control in image editing tasks, allowing users to specify changes at a per-pixel level, representing a significant advancement in user-directed image manipulation capabilities [13]. By focusing on these objectives, the survey aims to deepen the understanding of diffusion models in image editing and synthesis while addressing critical components, applications, and architectural designs relevant to other media, such as video generation. Overview of the Survey Structure This survey is structured to provide an in-depth examination of diffusion models in image editing and synthesis, emphasizing their transformative impact on generative modeling. It delves into the core principles and mathematical formulations, explores architectural design choices, and discusses methods for maintaining consistency. Applications are categorized based on input modalities, and advancements in state-of-the-art techniques, including text-to-image generation, are reviewed. Recent developments in training and evaluation practices are summarized, addressing computational costs and suggesting future research directions to enhance the efficacy of diffusion models in generative tasks [10,9]. The survey commences with an introduction that highlights the significance of diffusion models, followed by objectives that guide the exploration of technological advancements and methodological innovations. The subsequent section, Background and Core Concepts, delves into the foundational principles of diffusion models, generative models, and neural networks, providing insights into the iterative refinement process that transforms noise into coherent images [2]. This section also discusses the evolution of these technologies and their profound impact on image manipulation [1]. Advancements in Diffusion Models for Image Editing are explored next, highlighting key innovations, frameworks, and techniques that have emerged in recent years, enhancing the quality and precision of image editing tasks. This section covers methodological advancements, computational efficiency improvements, control and precision in image generation, and the integration of human feedback [3]. Applications of Diffusion Models in Image Synthesis are examined, showcasing their utility in virtual try-on, inpainting, style transfer, and other innovative editing methodologies. Examples illustrate how these models achieve specific image synthesis objectives, offering advantages over traditional methods [4]. The Challenges and Limitations section identifies computational complexity, scalability issues, and fidelity concerns, while exploring potential solutions and ongoing research aimed at overcoming these challenges. Finally, Future Directions speculate on emerging trends, potential breakthroughs, and areas for further research, underscoring the importance of continued innovation in generative models and neural networks [7]. This section discusses optimization for real-time applications, robustness enhancements, advancements in training techniques, exploration of new domains, integration with other models, and improvements in user experience. The Conclusion synthesizes key findings and insights, emphasizing the transformative impact of diffusion models on both image and video editing and synthesis. It highlights the necessity for ongoing research and development in this dynamic field, particularly in addressing challenges such as computational costs, temporal consistency, and the need for advanced structural and content fidelity in video edits. By reflecting on advancements in text-to-video generation and the capabilities of models like Google's Imagen and OpenAI's DALL-E 2, the Conclusion reinforces the importance of continued exploration of novel training methodologies and evaluation metrics to enhance model performance and unlock new possibilities in generative media technologies [10,14,6].The following sections are organized as shown in . Background and Core Concepts Fundamentals of Diffusion Models Diffusion models, notably diffusion probabilistic models (DPMs), have drastically impacted generative modeling by outperforming in image synthesis. With denoising diffusion probabilistic models (DDPMs) as a prime example, these models begin with Gaussian noise that is iteratively refined to mirror the original data distribution, yielding highly detailed and coherent images [7,15]. This dual-phase technique—adding Gaussian noise progressively and then refining it back—has proven especially effective in text-to-image synthesis, as demonstrated in innovative applications [16,17,18]. In text-guided image editing, diffusion models address significant challenges of user control and alignment with semantic prompts while maintaining image integrity [19]. Techniques such as DiffStyler use dual diffusion methods for nuanced style integration during the diffusion process, showcasing the model's versatility in balancing content and style [20]. These advancements extend to video synthesis, where diffusion models employ keyframes and local models to maintain content consistency in high-resolution outputs [7]. Diffusion models also excel in conditional control integration, refining pretrained models to incorporate specific user constraints, enhancing the flexibility and utility of these models for diverse generative tasks [15]. As the paradigms evolve, diffusion models continue to push the boundaries of expressive and user-aligned image generation [7]. Advancements in Generative Models Recent generative model developments have significantly enhanced image editing through innovative methodologies. Hierarchical approaches offer explicit image representation, improving photorealism and diversity [21]. Conditional frameworks are transforming specificity in text-guided synthesis, with models like Forgedit optimizing vision-language interactivity [22,23]. Multi-modal guidance systems improve image inpainting by enhancing control and flexibility [24]. The DrawBench framework redefines the evaluation of text-to-image models by integrating comprehensive performance dimensions [25]. User-specific content integration is exemplified through mechanisms like Perfusion and hypernetwork innovations for rapid personalization [26,27]. Collectively, these advancements underline the dynamic progress in generative model methodologies and benchmarks. Neural Networks in Image Editing Neural networks are central to image editing advancements, enabling sophisticated visual generation through diffusion models. Classifier-free guided models, distilled for efficiency, demonstrate significant improvements over traditional generative adversarial networks (GANs) [28,29]. DPM-Solver optimizes sampling processes, crucially enhancing real-time capabilities [30]. Neural networks enhance interpretability and user instruction precision in image editing through multimodal language models [31]. Recent neural network integrations allow text-driven and pixel-wise guidance for creative and precise image manipulation, redefining AI-driven image editing [32,13,33,14,34]. Principles of Image Synthesis and Editing Image synthesis using diffusion models relies on iterative refinement processes. Techniques like Iterative Latent Variable Refinement (ILVR) provide critical control over generative outcomes, enhancing multi-domain and exemplar-based tasks without retraining [1,35]. Diffusion models excel in maintaining semantic coherence in text-guided modifications and optimize fine-grained control [36,37]. Blended Latent Diffusion Model (BLDM) offers efficient local editing by harnessing low-dimensional latent spaces, minimizing computational costs while maintaining high-quality output [38]. Denoising Diffusion Null-Space Model (DDNM) enhances restoration tasks through learned variances, addressing super-resolution and deblurring challenges [39,40]. Diffusion models also innovate in video synthesis, facilitating thorough control over visual fidelity and temporal consistency [14,36]. Evaluation methodologies such as DrawBench incorporate human judgment for nuanced model assessment [10,41]. Techniques like segmentation masks in iEdit provide precision in localized edits, underscoring the evolving sophistication of diffusion frameworks [19]. Diffusion models stand as robust frameworks, integrating user feedback across various data types for producing precise, user-aligned visuals. Frameworks like HIVE ensure alignment with user preferences, demonstrating their dynamic adaptability from static image synthesis to dynamic video generation [10,36]. As capabilities evolve, diffusion models are poised to address a broader spectrum of generative modeling challenges. In recent years, the field of image editing has witnessed remarkable advancements, particularly through the development of diffusion models. These innovations can be effectively categorized into several key areas: framework, methodological advancements, computational efficiency, control and precision, and the integration of human feedback. illustrates this hierarchical structure, providing a visual representation that highlights the significant contributions and techniques within each category. This figure not only enhances our understanding of the advancements made but also underscores how these developments collectively improve image and video editing capabilities, precision, and efficiency. Thus, the integration of these innovations marks a pivotal moment in the evolution of digital editing technologies. Advancements in Diffusion Models for Image Editing Framework Innovations Recent innovations in diffusion model frameworks have significantly enhanced image editing capabilities. DiVAE integrates diffusion models with VQ-VAE architecture, improving image quality through superior encoding techniques [42]. This synergy between diffusion processes and variational autoencoders facilitates high-fidelity image generation. HyperDreamBooth advances personalized image generation, achieving rapid personalization—25 times faster than DreamBooth and 125 times faster than Textual Inversion [27]. This efficiency underscores diffusion models' capacity for swift, precise image modifications. As illustrated in , these advancements can be categorized into three main areas: image editing, video synthesis, and diffusion models. Each category highlights key methods that have contributed to enhanced capabilities in image and video generation, showcasing their unique features and improvements over existing techniques. Video synthesis frameworks have also evolved. MagicVideo employs a novel 3D U-Net design and a pre-trained VAE, reducing computational burden [43]. AlignYourLatent ensures temporal consistency across video frames, addressing coherence in dynamic sequences [44]. DifFace simplifies restoration processes through a transition distribution, enhancing speed and output quality [45]. StructureAware leverages joint training on video and image data, ensuring structural integrity in video edits [14]. Diffusion Autoencoders introduce dual latent code encoding, improving representation learning for nuanced image synthesis [46]. LaVie uses cascaded latent diffusion models for high-quality video generation [47], while Make-A-Video enhances efficiency with a spatial-temporal pipeline [48]. Show-1 combines pixel-based and latent-based VDMs for improved video synthesis quality [49]. ResDiff utilizes CNN predictions to guide DPMs, enhancing precision in image editing [50]. NUWA-XL trains on long videos for efficient, coherent segment generation [51]. Imagen Video employs cascading diffusion models for high-definition video generation [7]. These innovations collectively demonstrate dynamic progress in diffusion models, enhancing utility and performance in image and video editing tasks. By integrating methodologies like retrieval-augmented diffusion models, these advancements significantly enhance efficiency and versatility in image synthesis and editing, leveraging multimodal models like CLIP for text-guided synthesis. Structural and content-guided video diffusion models address temporal consistency challenges, expanding applications across domains such as video, imagery, text, and speech, while offering insights into computational cost management and future directions for longer video generation [10,14,8,34]. Methodological Advancements Recent methodological advancements in diffusion models have enhanced image editing precision, efficiency, and user control. DiffStyler's dual diffusion processing architecture offers precise stylization manipulation [20]. Astyle-bas8 improves image quality and latent space interpretability by disentangling high-level attributes from variations [52], allowing targeted modifications and better generative process understanding. In text-to-image synthesis, IIR enhances the editability-fidelity trade-off [16]. Imagen Video integrates super-resolution techniques for improved video quality and coherence [7]. Coser's 'Ali-in-Attention' scheme consolidates conditional information for enhanced super-resolution [53]. VQ-Diffusion's mask-and-replace strategy addresses error accumulation, improving image quality [17]. Weakly-supervised learning in iEdit constructs a dataset from LAION-5B for enhanced edit precision [19]. The Glide benchmark advances photorealistic image generation from textual descriptions [15]. These advancements illustrate diffusion models' dynamic evolution in image editing, enhancing precision, efficiency, and user control. LayerDiffusion leverages semantic-based layered control for non-rigid editing, while Differential Diffusion enables granular pixel-level customization without model retraining, validated through quantitative and qualitative comparisons and user studies [13,54]. By integrating innovative techniques, these advancements push generative modeling boundaries, enabling sophisticated, user-aligned image editing capabilities. Enhancements in Computational Efficiency Recent advancements in diffusion models have improved computational efficiency, reducing resource demands for real-time applications. MagicVideo models video distributions in a low-dimensional latent space, reducing computational requirements [43]. DiffIR uses a compact IR prior representation, reducing iteration numbers for accurate estimations [55]. WaveDiff employs wavelet decomposition for faster diffusion model training and inference [28], streamlining the generative process. Analytic-DPM speeds up inference by 20x to 80x with optimized sampling techniques [56]. DPM-Solver achieves high-quality samples with fewer function evaluations [30]. Negative Prompting Inference (NPI) enables ultrafast image editing, maintaining fidelity while reducing computation time [57]. Moecontroller uses a mixture-of-expert approach for efficient text-guided image editing [58]. These enhancements optimize diffusion models, making them practical across various applications. By reducing computational demands and improving speeds, these advancements facilitate broader diffusion model adoption in diverse domains, enhancing efficiency and accessibility in fields like healthcare, biology, and other scientific disciplines, fostering innovation and creativity in Artificial Intelligence for General Creativity (AIGC) [8,9,59,10,11]. Control and Precision in Image Generation Innovative techniques in diffusion models have enhanced control and precision in image generation, enabling detailed, user-aligned modifications. Edit-friendly noise maps facilitate perfect image reconstruction and diverse editing operations [60]. DragonDiffusion constructs classifier guidance based on feature correspondence loss, maintaining coherence and fidelity [61]. Prompt-Free Diffusion leverages visual context for improved image quality [62]. Hierarchical approaches generate CLIP image embeddings from text captions for precise alignment between textual inputs and visual outputs [21]. OMG manages occlusions, enhancing identity preservation during image generation [63]. LayerDiffusion introduces semantic-based layered control for non-rigid editing, while Differential Diffusion provides granular pixel-level change control without model retraining. Recent text-driven image manipulation developments optimize efficiency, enabling rapid, high-quality edits feasible on user devices [33,13,54]. These advancements push generative modeling boundaries, facilitating sophisticated, user-aligned image editing capabilities. Integration of Human Feedback and Instruction Integrating human feedback and instruction into diffusion models has refined image editing outcomes. HIVE collects human feedback to learn a reward function aligning with user preferences [64]. InstructDiffusion allows real-time image manipulation based on user commands [65]. MagicRemover uses attention guidance to constrain sampling, enhancing denoising stability [66]. InstructPix2Pix guides localized edits based on text instructions [67]. InstructAny2Pix uses multi-modal encoders and LLMs to align generated images with user instructions [68]. ImageBrush utilizes visual instructions for accurate image manipulations [69]. MGIE integrates human feedback through expressive commands, showcasing large language models' potential in interpreting complex user instructions [31]. WarpDiffusion improves garment transfer processes with local texture attention, retaining details based on human feedback [70]. ShadowDiffusion incorporates degradation prior, improving output quality [71]. These advancements illustrate progress in integrating human feedback and instruction into diffusion models, enhancing refined, user-aligned image edits. By integrating cutting-edge methodologies, generative modeling advancements transform human creativity into tangible outputs across diverse domains. Innovations like the Image Information Removal module enhance text-to-image editing by selectively erasing irrelevant details, preserving essential content, achieving superior editability-fidelity trade-offs. Efficient algorithms for real-time text-driven image manipulation reduce computational costs, making advanced image editing accessible on user devices. Collectively, these advancements push generative modeling boundaries, facilitating sophisticated, responsive image editing capabilities with profound implications for creative industries [10,33,16,8]. Applications of Diffusion Models in Image Synthesis Diffusion models have become pivotal in image synthesis, showcasing versatility in various applications, notably in virtual try-on technologies that enhance garment visualization and revolutionize user experiences in fashion and retail. Virtual Try-On Applications In virtual try-on applications, diffusion models generate realistic images of individuals in selected garments, facilitating seamless clothing integration into personal images, a feature increasingly utilized in online retail [72]. A significant challenge is to transform garments while preserving their identity amid spatial misalignments [72]. GP-VTON addresses this with its Local-Flow Global-Parsing (LFGP) module, enabling precise garment part warping while maintaining overall coherence, thus enhancing semantic accuracy and minimizing texture distortion [73]. StableVITON overcomes earlier limitations by achieving semantic correspondence between clothing and the human body in latent space, preserving clothing details with a pre-trained diffusion model and enhancing detail preservation via zero cross-attention blocks [74]. Traditional reliance on human parsing often results in unrealistic try-on images with artifacts [75], highlighting the need for advanced techniques that maintain visual fidelity. As illustrated in , the hierarchical organization of key concepts within virtual try-on technologies emphasizes the role of diffusion models in enhancing image realism, semantic accuracy, and detail preservation. The figure also highlights the challenges faced, such as spatial misalignment, texture distortion, and parsing artifacts, alongside the innovative solutions developed, including the LFGP module, zero cross-attention techniques, and AnyDoor customization. Diffusion models offer robust frameworks to overcome these challenges, enhancing realism and user experience in virtual try-on scenarios. AnyDoor exemplifies diffusion models' versatility by enabling image customization and manipulation, enhancing user interaction and satisfaction [76]. WarpDiffusion improves synthesis quality at garment-skin boundaries, enhancing realistic effects like wrinkles and shadows, and integrates into existing methodologies for high-fidelity results with reduced resources. Platforms like Versatile Diffusion demonstrate the applicability of diffusion models across modalities, offering features like style and semantic disentanglement. These advancements illustrate diffusion models' potential to revolutionize virtual try-on technology, setting new benchmarks [77,70,8,10,78]. Image Inpainting Techniques Diffusion models have advanced image inpainting, enabling seamless restoration of missing or corrupted regions with visually coherent and semantically plausible content, achieving high-quality results [79]. PowerPaint enhances precision by using learnable task prompts to focus on specific inpainting targets [80], demonstrating task-specific guidance's potential in refining outcomes. AnyDoor showcases diffusion models' versatility in zero-shot object-level customization, allowing seamless target object integration while preserving shape and appearance [76]. Differential diffusion offers granular pixel-level control, while text-driven manipulation accelerates real-time edits with reduced computational costs. The Palette framework excels in diverse image-to-image translation tasks, outperforming traditional methods, and a unified evaluation protocol furthers research [33,81,6,13]. By optimizing frameworks, diffusion models push image inpainting boundaries. Style Transfer and Customization Diffusion models have transformed style transfer and customization by enhancing artistic style transformations' quality and efficiency. StyleDiffusion introduces content and style disentanglement, allowing precise manipulation in image synthesis [82]. Inversion-based techniques, like InST, refine artistic styles' capturing and transferring, demonstrating efficiency and fidelity [83]. These methods exemplify diffusion models' ability to achieve seamless, high-quality style transformations, expanding creative possibilities. Advancements in diffusion models illustrate their impact on style transfer technology, enhancing realism, flexibility, and user experience. By leveraging inversion-based techniques and content-style disentanglement, these models facilitate sophisticated artistic transformations, efficiently learning and transferring unique attributes without complex textual descriptions. They improve control over content and style separation, resulting in interpretable and flexible artistic outputs [10,82,8,83]. Image Restoration and Translation Diffusion models have advanced image restoration and translation by addressing traditional techniques' limitations under diverse conditions. The mean-reverting stochastic differential equation (MR-SDE) exemplifies this by simulating the reverse image degradation process, effectively restoring original images to high fidelity [84]. This approach highlights diffusion models' potential to tackle image degradation challenges, providing a robust restoration framework, particularly where traditional methods fail to capture nuanced details necessary for high-quality outputs [85]. In image translation, diffusion models enable seamless domain conversion while preserving essential features. Innovative methods like disentangled style and content representation address content preservation during reverse diffusion. Techniques like splicing Vision Transformer keys and content preservation loss ensure accurate content maintenance. Image-guided style transfer is enhanced by matching classification tokens, and text-driven translation benefits from CLIP loss. Semantic divergence loss and resampling strategies accelerate semantic changes, outperforming state-of-the-art models in text-guided and image-guided tasks. Diffusion models' flexibility in handling diverse translation tasks underscores their utility across applications, from artistic transformations to domain-specific adaptations [37,86]. These advancements illustrate diffusion models' impact on image restoration and translation, enhancing realism, flexibility, and user experience. By addressing computational efficiency and fidelity loss, these models enable high-quality text-driven manipulations, real-world image super-resolution, and adaptable generation processes, significantly improving visual quality and user adaptability across various applications [33,87,6]. Innovative Editing Approaches Advancements in diffusion models have introduced innovative editing approaches that enhance image synthesis and editing capabilities. Stylediffusion offers interpretability and controllability in style transfer, producing high-quality, customizable results [82]. In virtual try-on applications, the Teacher-Tutor-Student knowledge distillation method enhances image quality, presenting a viable alternative to parser-based techniques [75]. This underscores knowledge distillation's importance in improving realism and fidelity. The IA framework introduces a mask-free inpainting approach with a user-friendly interface for generating high-quality content based on prompts [88], streamlining the inpainting process. AnyDoor proposes a diffusion-based generation method for effortless object teleportation into new scenes at user-defined locations and shapes [76], demonstrating diffusion models' versatility in seamless object integration. Methods for generating high-fidelity cartoon images into existing diffusion models without additional training exemplify their adaptability [18]. These innovative approaches showcase diffusion models' advancements, enhancing quality, flexibility, and user experience in image and video synthesis. Recent developments include structure and content-guided video editing for temporal consistency, real-time text-driven manipulation, and fine-grained editing with pixel-wise guidance, offering powerful tools for content creation and modification [10,33,14,32]. By integrating novel methodologies and optimizing existing techniques, these advancements push generative modeling boundaries in image editing. Challenges and Limitations Computational Complexity and Resource Intensity Diffusion models, especially denoising diffusion probabilistic models (DDPMs), are challenged by their computational demands and resource intensity due to their iterative nature, which involves simulating a Markov chain over numerous steps [7]. This iterative process requires substantial computational resources, hindering scalability and practical deployment [30]. The inefficiency is exacerbated by sequential evaluations of large neural networks during sampling, which involves multiple evaluations of class-conditional and unconditional instances, increasing computational load [89]. The stochastic generative process necessitates precise control over image generation, further complicating computational demands [20]. Inefficiencies during the inference phase of classifier-free guided diffusion models pose challenges for real-time applications and resource-constrained environments [26]. As illustrated in , the primary challenges faced by diffusion models are highlighted, focusing on computational complexity, inference limitations, and super-resolution issues. This figure categorizes the iterative process demands, real-time application limits, and the struggle for high-quality image restoration, emphasizing the need for advancements in efficiency and fidelity. Additionally, existing methods struggle with mask selection complexity and hole-filling quality in image inpainting, limiting effectiveness [90]. Inconsistencies and artifacts in edited images arise from inadequate integration of noised target images with diffusion latent variables [91]. Text-to-image generation faces limitations such as unidirectional bias and error accumulation during the image generation process [17]. Traditional super-resolution methods often fail to leverage global semantic context, resulting in insufficient detail restoration [53]. A core obstacle is the tendency of models to obscure information about the original image during encoding, complicating the retention of crucial attributes during editing [16]. Despite these challenges, advancements like DiffIR show promise in reducing computational costs and iterations needed for high-quality image restoration compared to traditional methods [5]. However, inefficiencies and misalignment in existing diffusion models during image deblurring continue to hinder performance and increase computational costs [51]. The significant time required for optimization remains a critical barrier for ultrafast editing [92]. Addressing these computational challenges is essential for expanding the applicability of diffusion models across domains, enhancing accessibility and utility while effectively managing resource demands and maximizing output fidelity. Scalability and Generalization Diffusion models face significant challenges in scalability and generalization, particularly in complex image synthesis tasks. LayerDiffusion struggles with achieving perfect integration for highly complex images or textual descriptions, which can impede effective output generation [54]. Methods like Subject-Driven Generation heavily depend on the quality and diversity of mined image clusters, affecting performance with less common subjects [93]. Inconsistencies in DDIM reconstruction undermine the efficacy of tuning-free methods in delivering accurate and consistent edits [94]. Variations in output quality arise, especially with intricate or abstract editing instructions not included in the training data, as seen in Learning-to-Edit frameworks [36]. UniControl's reliance on pretrained models highlights challenges in scaling to more complex tasks, where the initial model's quality significantly influences the final output [95]. DiVAE, effective in enhancing image quality through improved encoding techniques, faces challenges in scaling for larger datasets or varying synthesis conditions [42]. This limitation emphasizes the need for robust frameworks that adapt to diverse data environments without sacrificing performance. Existing diffusion models lack structured approaches for semantically manipulating generated images, restricting usability in applications requiring precise edits [96]. These challenges necessitate ongoing research to improve scalability and generalization capabilities of diffusion models. Despite advancements in image generation models like Google's Imagen, OpenAI's DALL-E 2, and stable diffusion models, unresolved issues persist, including improved handling of structured data, maintaining temporal consistency in video generation, and managing computational costs. Open-source models and foundational mathematical principles offer a robust framework for academic analysis and enhancement, paving the way for future developments across diverse domains such as imagery, text, speech, biology, and healthcare [10,9,6,8]. Addressing these limitations will enable diffusion models to become more versatile and effective across a broader range of applications, facilitating precise and user-aligned image synthesis and editing. Fidelity and Quality of Outputs The fidelity and quality of outputs generated by diffusion models are critical in image synthesis and editing. Techniques like MR-SDE have achieved state-of-the-art results in image restoration, demonstrating potential for high-quality outputs even with suboptimal training data [84]. However, overall restoration results remain sensitive to the quality of pre-trained diffusion models, highlighting a limitation in adaptability across varied datasets [87]. Certain methodologies face challenges in preserving low-level details; for instance, the hierarchical approach of HDAE may struggle to capture intricate details due to its representational structure [97]. While some models excel in general scenarios, they may encounter limitations in conditions demanding extreme detail preservation, potentially leading to artifacts in specific contexts [98]. Innovations like DragonDiffusion emphasize the importance of intermediate features for maintaining editing precision, yet reliance on these features can hinder consistent output delivery [61]. This dependency raises concerns about diffusion models' ability to manage detail consistently across varied data inputs. The subjective nature of human judgment in evaluating image fidelity introduces variability in quality assessments, complicating universal standards for output evaluation [25]. This subjectivity can lead to inconsistencies in perceived image quality. Video synthesis applications, such as NUWA-XL, showcase diffusion models' capability to handle long-duration content. However, challenges persist in managing highly diverse or extended sequences, which may exceed tested parameters, leading to coherence issues [51]. While diffusion models exhibit significant promise in advancing image editing and synthesis, current research does not fully resolve performance issues across all applications, necessitating comprehensive evaluations to address these gaps [6]. The effectiveness of methods like iEdit is influenced by the quality of pseudo-target images used, impacting overall performance and underscoring the complexity of achieving consistent image fidelity [19]. Continued innovation and evaluation are essential to enhance fidelity and quality across diverse applications leveraging diffusion models. Innovative Solutions and Ongoing Research Recent research focuses on overcoming diffusion models' limitations in image editing and synthesis by optimizing computational efficiency, enhancing explainability, and exploring alternative input domains and conditioning techniques [12]. Enhancements in modular design, as proposed in [99], present promising avenues for future research, potentially applicable to other generative modeling techniques. Optimizing wavelet coefficients for diverse data types, as suggested in [100], could significantly alleviate computational burdens while maintaining or improving quality of generated samples. In image editing, automating inversion processes and reducing user input requirements, as explored in [101], offer opportunities to expand applicability to more complex tasks. Improving model adaptability to a broader range of image types and enhancing performance on intricate editing tasks remain critical areas for exploration [23]. Optimizing methods for broader application to inverse problems and computational efficiency, as discussed in [102], could enhance diffusion models' versatility. The generative capabilities of IA could be extended to accommodate more complex inpainting scenarios, as noted in [88], while advancements in task prompt design and applications of PowerPaint could address complex inpainting challenges [80]. For AnyDoor, future work could focus on enhancing robustness in complex scenes and integrating features for greater customization [76]. Research on the IIR module should explore additional attributes for preservation or editing, addressing current limitations in text-to-image synthesis [16]. These innovative solutions and ongoing research illustrate dynamic progress in addressing diffusion models' limitations. By integrating novel methodologies and optimizing existing processes, these advancements continue to push the boundaries of generative modeling, facilitating sophisticated, efficient, and user-aligned image editing capabilities. Future Directions Advancements in diffusion models necessitate a focus on future research trajectories that will influence their development and application. This section outlines strategies for optimizing these models for real-time applications, emphasizing the need for improved computational efficiency and adaptability to dynamic environments. Optimization for Real-Time Applications Recent developments in diffusion models reveal substantial potential for optimization, particularly in enhancing computational efficiency and adaptability. Current models have achieved notable progress in reducing computational costs, crucial for real-time deployment [28,5]. Future research should prioritize strategies aimed at further minimizing these costs. Frameworks like Tryondiffu highlight the necessity of optimizing models for real-time applications, especially in virtual try-on scenarios that demand adaptability to various garments and poses [14]. Enhancements in segmentation efficiency could improve responsiveness to user interactions [31]. As illustrated in , the hierarchical categorization of optimization strategies for real-time diffusion model applications emphasizes key aspects such as computational efficiency, adaptability, usability, and advanced techniques. This visual representation underscores the interconnectedness of these elements in driving the optimization process. Optimizing frameworks such as BDCE for real-time use promises breakthroughs in efficiency and usability [47]. Future investigations should also focus on developing conditioning controls and their integration to improve real-time performance [57]. Self-guidance techniques, including property extraction refinements and applications in complex generative tasks, are crucial for advancing real-time optimization efforts [103]. Exploring unsupervised or semi-supervised training approaches can reduce dependence on annotated datasets, streamlining real-time applications [38]. Enhancing task embeddings and expanding the scope of image editing tasks will further advance model capabilities in real-time settings [104]. Improvements in implicit masking techniques and diffusion step efficiency are essential for facilitating real-time processing [5]. Moreover, integrating real-time processing in varying weather conditions remains a promising area for exploration [26]. These advancements highlight ongoing progress in optimizing diffusion models for real-time applications. By integrating innovative methodologies and refining existing processes, the field continues to extend the frontiers of generative modeling, enabling more efficient and user-aligned real-time image editing capabilities. Future research may also focus on optimizing the decoupled cross-attention mechanism and its relevance in diverse generative contexts [31], alongside improvements in precision editing and real-time editing capacities [28]. Robustness and Adaptability Enhancements Enhancements in robustness and adaptability are increasingly central to the development of diffusion models, broadening their applicability across varied datasets and tasks. Improved Denoising Diffusion Probabilistic Models (IDDPM) exemplify this, efficiently generating high-quality samples with reduced computational resources [105]. This efficiency alleviates computational burdens and enhances adaptability for real-time applications. Future research should aim to extend the robustness of diffusion models to accommodate a wider range of datasets and tasks, refining their ability to handle diverse data types and conditions [106]. User-friendliness and fidelity during editing are critical, as highlighted by methods ensuring outputs closely align with user expectations [101]. Promising areas for future work include enhancing adaptability to various artistic styles and streamlining iterative editing processes [107]. Improving dialogue model understanding and expanding editable image features will advance robustness in interactive scenarios [108]. Additionally, optimizing computational efficiency and expanding applicability to diverse objects and contexts is vital [109]. Enhancing methods like DifFace for broader degradation types will further improve robustness in image restoration tasks [45]. These enhancements collectively demonstrate significant progress in improving the robustness and adaptability of diffusion models, evidenced by their performance in high-quality video content generation and structured data modeling. Recent surveys indicate advances in architectural designs, temporal dynamics modeling, and evaluation metrics, while addressing challenges such as generating longer videos and managing computational costs, highlighting the potential for further advancements in both general and domain-specific applications [10,9]. Advancements in Training and Evaluation Techniques Recent innovations in training and evaluation techniques for diffusion models have substantially enhanced their performance and efficiency. Optimizing the training process, particularly through normalizing flows, has shown promise in improving variational inference by constructing flexible approximate posterior distributions via invertible transformations, thus enhancing statistical inference quality. Additionally, advancements in text-to-image generation and image editing with natural language instructions underscore the importance of efficient training dynamics and data quality [110,36,111]. Enhancements in robustness against varied input conditions are critical for refining generative techniques and model scaling, ensuring diffusion models adapt to diverse input scenarios while maintaining output fidelity across domains including video, text, and structured data [77,8,9,10,6]. Further generalizations of methods to address a broader range of inverse problems enhance the versatility of diffusion models, enabling their application in complex generative tasks requiring precise output control. Architectural innovations and the integration of diverse input modalities facilitate the creation of high-quality content across various domains, including video generation and healthcare [10,8]. Optimizing input selection for evaluation processes represents a significant advancement in accurately reflecting model performance. By refining input criteria, diffusion models achieve consistent evaluations, facilitating improvements in training methodologies through diverse input modalities [10,14,9,99]. Vector quantization processes offer avenues for extending diffusion models to new domains, enhancing adaptability and precision in generating high-quality imagery, text, and applications in various fields [10,9,11,8]. Innovations in energy function design and performance improvements in unpaired image-to-image translation tasks are pivotal in refining training techniques, enabling high-quality transformations and addressing challenges like overfitting [16,86]. Future research could focus on enhancing CNN architectures and optimizing the joint training of conditional and unconditional models to improve classifier-free guidance methods [50,89]. Table provides a comprehensive comparison of representative benchmarks pertinent to the advancements in training and evaluation techniques for diffusion models, collectively illustrating the dynamic progress in training and evaluation techniques. These advancements highlight their transformative impact across diverse applications, setting the stage for future innovations in diffusion models [10,9]. Exploration of New Applications and Domains Exploring new applications and domains for diffusion models presents opportunities to expand their utility across various fields. Future research may enhance cognitive embedding processes and apply CoSeR beyond traditional image super-resolution contexts [53]. Refining dataset construction and weakly-supervised learning approaches could lead to localized editing applications in diverse contexts [19]. The scalability of diffusion models allows for novel domain applications, enhancing capabilities in managing complex actions and improving embedding efficiency [112]. Future work should focus on refining architectures and exploring additional applications beyond human faces [52]. Investigating additional guidance strategies and their impact on image generation, alongside expanding datasets for broader applicability, could foster new avenues for generative tasks [15]. The integration of novel methodologies and process refinements will continue to push the boundaries of generative modeling, facilitating efficient, adaptable, and user-aligned image editing capabilities. Integration with Other Generative Models Integrating diffusion models with other generative frameworks offers promising avenues for enhancing image synthesis and editing efficiency. The innovation in Diffbir, which decouples the restoration process into two stages, exemplifies the potential for synergy with latent diffusion models, allowing for user-controlled guidance during inference [113]. WaveDM achieves state-of-the-art performance, being over 100× faster than traditional image restoration methods, and its integration with other generative models could further enhance computational efficiency [114]. Future research should explore enhancements to the noise extraction process and additional editing capabilities within this framework [60]. Forgedit demonstrates state-of-the-art results in text-guided image editing, significantly improving speed and effectiveness, and its integration could enhance precision and responsiveness in user-directed editing [23]. The NPI innovation achieves equivalent reconstruction fidelity without lengthy optimization, streamlining the generative process and maintaining high-quality outputs [57]. Future research may explore stabilizing training and enhancing sample diversity, leveraging GANs alongside diffusion models to improve image diversity and fidelity [115]. Analytic-DPM offers potential for application in other generative models, enhancing robustness across various tasks [56]. These integrations illustrate the dynamic progress in combining diffusion models with other generative frameworks, enhancing efficiency, quality, and user experience in image synthesis and editing. By leveraging the strengths of various models, advancements in generative modeling have expanded the capabilities of image editing, with diffusion models leading the way in producing high-quality images and facilitating sophisticated attribute modifications without altering semantic content [8,116]. User Experience and Interface Improvements Advancements in diffusion model applications increasingly emphasize enhancing user experience and interface design, crucial for intuitive interactions with generative models. A key area of development is refining user interfaces to support real-time feedback mechanisms, allowing users to guide the image synthesis process interactively [31]. This capability improves user satisfaction by enabling precise modifications based on inputs. Streamlined user interfaces also focus on simplifying complex generative tasks, making them accessible to users with varying expertise levels. Visual instructions, as demonstrated in ImageBrush, reduce reliance on textual descriptions, offering a more intuitive image editing approach [69]. Future research could explore developing adaptive interfaces that dynamically adjust to user preferences, enhancing personalization and alignment with user needs [21]. By leveraging machine learning to analyze user behavior, applications can offer tailored interfaces that improve usability. Moreover, integrating multi-modal capabilities, as seen in InstructAny2Pix, enhances understanding of user directives and output quality [68]. This underscores the importance of accommodating diverse input types, ensuring versatile and responsive interfaces. These advancements illustrate the dynamic progress in enhancing user experience and interface design in diffusion model applications. Incorporating methodologies like Image Information Removal and leveraging multimodal large language models redefine generative modeling, addressing challenges such as overfitting and information leakage while enhancing precision in object-centric edits. These approaches facilitate intuitive image editing through expressive instructions and pixel-wise guidance, leading to improved editability-fidelity trade-offs across diverse domains [32,36,16,31,117]. Conclusion Diffusion models have emerged as pivotal in advancing generative AI, particularly in the realms of image editing and synthesis. Their ability to perform fine-grained editing tasks is exemplified by models like InstructEdit, which adeptly handle varied user instructions while ensuring high-quality output. Similarly, the BDCE framework sets a standard in enhancing low-light images by effectively addressing noise and computational challenges. These models demonstrate impressive adaptability, as seen with IP-Adapter, which achieves high performance with fewer parameters compared to fully fine-tuned counterparts. In the domain of super-resolution, IDM has outperformed existing approaches, effectively mitigating common issues such as over-smoothing. However, challenges related to computational demands and resource utilization persist, underscoring the need for ongoing research to refine these models for wider application. The future holds substantial promise for diffusion models, as they remain integral to the ongoing evolution of generative AI, driving further advancements in neural networks and fostering sophisticated image editing solutions aligned with user needs.",
  "reference": {
    "1": "2108.02938v2",
    "2": "2011.13456v2",
    "3": "2302.09778v2",
    "4": "2311.01410v2",
    "5": "2303.16491v2",
    "6": "2308.13142v1",
    "7": "2210.02303v1",
    "8": "2306.04139v2",
    "9": "2306.04139v2",
    "10": "2310.10647v2",
    "11": "2209.00796v15",
    "12": "2401.00736v3",
    "13": "2306.00950v2",
    "14": "2302.03011v1",
    "15": "2112.10741v3",
    "16": "2305.17489v2",
    "17": "2111.14822v3",
    "18": "2305.06710v4",
    "19": "2305.05947v1",
    "20": "2211.10682v2",
    "21": "2204.06125v1",
    "22": "1411.1784v1",
    "23": "2309.10556v2",
    "24": "2310.07222v1",
    "25": "2205.11487v1",
    "26": "2305.01644v2",
    "27": "2307.06949v2",
    "28": "2211.16152v2",
    "29": "2210.03142v3",
    "30": "2206.00927v3",
    "31": "2309.17102v2",
    "32": "2212.02024v3",
    "33": "2304.04344v1",
    "34": "2207.13038v1",
    "35": "2006.11239v2",
    "36": "2310.19145v1",
    "37": "2209.15264v2",
    "38": "2206.02779v2",
    "39": "2209.04747v6",
    "40": "2201.11793v3",
    "41": "2304.09748v1",
    "42": "2206.00386v1",
    "43": "2211.11018v2",
    "44": "2304.08818v2",
    "45": "2212.06512v4",
    "46": "2111.15640v3",
    "47": "2309.15103v2",
    "48": "2209.14792v1",
    "49": "2309.15818v3",
    "50": "2303.08714v3",
    "51": "2303.12346v1",
    "52": "1812.04948v3",
    "53": "2311.16512v4",
    "54": "2305.18676v1",
    "55": "2303.09472v3",
    "56": "2201.06503v3",
    "57": "2305.16807v2",
    "58": "2309.04372v2",
    "59": "2210.09292v3",
    "60": "2304.06140v3",
    "61": "2307.02421v2",
    "62": "2509.17367v1",
    "63": "2403.10983v2",
    "64": "2303.09618v2",
    "65": "2309.03895v1",
    "66": "2310.02848v1",
    "67": "2308.08947v1",
    "68": "2312.06738v4",
    "69": "2308.00906v1",
    "70": "2312.03667v1",
    "71": "2212.04711v2",
    "72": "1807.07688v3",
    "73": "2303.13756v1",
    "74": "2312.01725v1",
    "75": "2103.04559v2",
    "76": "2307.09481v2",
    "77": "2211.08332v4",
    "78": "2310.07204v1",
    "79": "1907.05600v3",
    "80": "2312.03594v4",
    "81": "2111.05826v2",
    "82": "2308.07863v1",
    "83": "2211.13203v3",
    "84": "2301.11699v3",
    "85": "2401.13627v2",
    "86": "2211.12572v1",
    "87": "2305.07015v4",
    "88": "2304.06790v1",
    "89": "2207.12598v1",
    "90": "2211.12446v2",
    "91": "2302.03027v1",
    "92": "2210.09276v3",
    "93": "2304.00186v5",
    "94": "2312.14611v1",
    "95": "2305.11147v3",
    "96": "2210.10960v2",
    "97": "2304.11829v2",
    "98": "2104.07636v2",
    "99": "2206.00364v2",
    "100": "2208.05003v1",
    "101": "2305.04441v1",
    "102": "2209.14687v4",
    "103": "2306.16894v2",
    "104": "2305.12966v4",
    "105": "2102.09672v1",
    "106": "1503.03585v8",
    "107": "2309.00613v2",
    "108": "2303.10073v2",
    "109": "2303.11306v2",
    "110": "2302.04841v3",
    "111": "1505.05770v6",
    "112": "2309.16608v1",
    "113": "2308.15070v3",
    "114": "2305.13819v2",
    "115": "1902.05687v4",
    "116": "2212.08698v1",
    "117": "2311.16432v2"
  },
  "chooseref": {
    "1": "2306.04139v2",
    "2": "1812.04948v3",
    "3": "2308.13142v1",
    "4": "2306.04139v2",
    "5": "2310.10647v2",
    "6": "2312.03594v4",
    "7": "2302.05543v3",
    "8": "2304.08818v2",
    "9": "2304.06140v3",
    "10": "2201.06503v3",
    "11": "2307.09481v2",
    "12": "2206.02779v2",
    "13": "2309.14709v3",
    "14": "2106.15282v3",
    "15": "2407.15886v2",
    "16": "2207.12598v1",
    "17": "2208.09392v1",
    "18": "2302.09778v2",
    "19": "1411.1784v1",
    "20": "2111.13606v1",
    "21": "2303.05125v1",
    "22": "2311.18608v2",
    "23": "2311.16512v4",
    "24": "2310.13165v2",
    "25": "1806.06137v3",
    "26": "1503.03585v8",
    "27": "2304.07090v1",
    "28": "2010.02502v4",
    "29": "2006.11239v2",
    "30": "2201.11793v3",
    "31": "2203.14206v1",
    "32": "2303.10073v2",
    "33": "2212.06512v4",
    "34": "2308.15070v3",
    "35": "2210.11427v1",
    "36": "2306.00950v2",
    "37": "2303.09472v3",
    "38": "2306.14685v4",
    "39": "2211.10682v2",
    "40": "2111.15640v3",
    "41": "2306.00219v2",
    "42": "2210.10960v2",
    "43": "2105.05233v4",
    "44": "2209.04747v6",
    "45": "2401.00736v3",
    "46": "2209.00796v15",
    "47": "2209.14687v4",
    "48": "2306.00986v3",
    "49": "2209.15264v2",
    "50": "2110.02711v6",
    "51": "2211.07825v1",
    "52": "2206.00386v1",
    "53": "2206.00927v3",
    "54": "2306.14435v6",
    "55": "2307.02421v2",
    "56": "2312.03771v1",
    "57": "2309.15664v1",
    "58": "2211.12446v2",
    "59": "2309.04907v1",
    "60": "2210.09292v3",
    "61": "2207.06635v5",
    "62": "2302.13848v2",
    "63": "2206.00364v2",
    "64": "2311.10089v1",
    "65": "2311.10709v2",
    "66": "2309.15807v1",
    "67": "2306.09869v3",
    "68": "1609.03126v4",
    "69": "2305.07015v4",
    "70": "2304.06720v4",
    "71": "2309.11321v1",
    "72": "2309.14934v1",
    "73": "2212.02024v3",
    "74": "2312.10113v1",
    "75": "2309.10556v2",
    "76": "2211.13524v1",
    "77": "1902.05687v4",
    "78": "1907.05600v3",
    "79": "2112.10741v3",
    "80": "2301.07093v2",
    "81": "2303.13756v1",
    "82": "2309.17102v2",
    "83": "2304.11829v2",
    "84": "2305.12966v4",
    "85": "2204.06125v1",
    "86": "2303.09618v2",
    "87": "2307.06949v2",
    "88": "2108.02938v2",
    "89": "2301.11699v3",
    "90": "2104.07636v2",
    "91": "1611.07004v3",
    "92": "2308.00906v1",
    "93": "2212.06909v2",
    "94": "2210.02303v1",
    "95": "2210.09276v3",
    "96": "2303.16491v2",
    "97": "2102.09672v1",
    "98": "2006.09011v2",
    "99": "2206.00941v3",
    "100": "2304.06790v1",
    "101": "2304.03411v1",
    "102": "2312.06738v4",
    "103": "2309.03895v1",
    "104": "2305.18047v1",
    "105": "2211.09800v2",
    "106": "2211.13203v3",
    "107": "2308.06721v1",
    "108": "2302.04841v3",
    "109": "2309.00613v2",
    "110": "2305.01644v2",
    "111": "2309.16608v1",
    "112": "2305.13501v3",
    "113": "1809.11096v2",
    "114": "2309.15103v2",
    "115": "2305.18676v1",
    "116": "2310.19145v1",
    "117": "2311.16711v2",
    "118": "2303.11306v2",
    "119": "2310.02848v1",
    "120": "2211.11018v2",
    "121": "2203.13131v1",
    "122": "2209.14792v1",
    "123": "2304.08465v1",
    "124": "1705.07057v4",
    "125": "2308.06571v1",
    "126": "2309.04372v2",
    "127": "2112.05744v4",
    "128": "2305.16807v2",
    "129": "1711.00937v2",
    "130": "2305.06710v4",
    "131": "2303.12346v1",
    "132": "2403.10983v2",
    "133": "2210.03142v3",
    "134": "2211.13227v1",
    "135": "2111.05826v2",
    "136": "2103.04559v2",
    "137": "2306.16894v2",
    "138": "2205.11487v1",
    "139": "2305.18286v1",
    "140": "2310.00426v3",
    "141": "1601.06759v3",
    "142": "2211.12572v1",
    "143": "2302.07979v2",
    "144": "2305.10474v3",
    "145": "2305.04441v1",
    "146": "2208.01626v1",
    "147": "2509.17367v1",
    "148": "2304.09748v1",
    "149": "2305.04651v1",
    "150": "2303.08714v3",
    "151": "2207.14626v2",
    "152": "2401.13627v2",
    "153": "2011.13456v2",
    "154": "2307.01952v1",
    "155": "2212.04711v2",
    "156": "2309.15818v3",
    "157": "2212.04489v2",
    "158": "2212.05034v1",
    "159": "2312.05039v1",
    "160": "2211.14305v2",
    "161": "2311.15127v1",
    "162": "2312.01725v1",
    "163": "2310.07204v1",
    "164": "2302.03011v1",
    "165": "2308.07863v1",
    "166": "2303.15649v3",
    "167": "2306.00983v1",
    "168": "2304.00186v5",
    "169": "2311.16432v2",
    "170": "2212.07603v2",
    "171": "2207.13038v1",
    "172": "2305.17489v2",
    "173": "2311.01410v2",
    "174": "1807.07688v3",
    "175": "2304.04344v1",
    "176": "2303.15403v2",
    "177": "2306.08276v1",
    "178": "2312.14611v1",
    "179": "2212.08698v1",
    "180": "2305.16322v3",
    "181": "2310.07222v1",
    "182": "2305.11147v3",
    "183": "2104.05358v1",
    "184": "1505.05770v6",
    "185": "2111.14822v3",
    "186": "2211.11319v1",
    "187": "2211.08332v4",
    "188": "2204.03458v2",
    "189": "2309.00398v2",
    "190": "1711.08447v4",
    "191": "2312.03667v1",
    "192": "2308.08947v1",
    "193": "2305.13819v2",
    "194": "2211.16152v2",
    "195": "2208.05003v1",
    "196": "2212.00490v2",
    "197": "2302.03027v1",
    "198": "2102.12092v2",
    "199": "2312.16794v2",
    "200": "2305.05947v1"
  }
}