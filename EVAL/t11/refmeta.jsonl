{"paper_id": 211146177, "title": "AUTO-ENCODING VARIATIONAL BAYES", "author_names": ["Romain Lopez", "Pierre Boyeau", "N. Yosef", "Michael I. Jordan", "J. Regier"], "venue": "", "abstract": null, "year": 2020, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 8142135, "title": "Pixel Recurrent Neural Networks", "author_names": ["Aäron van den Oord", "Nal Kalchbrenner", "K. Kavukcuoglu"], "venue": "International Conference on Machine Learning", "abstract": "Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.", "year": 2016, "publicationdate": "2016-01-25", "externalids": {}, "doi_lower": null}
{"paper_id": 20282961, "title": "Neural Discrete Representation Learning", "author_names": ["Aäron van den Oord", "O. Vinyals", "K. Kavukcuoglu"], "venue": "Neural Information Processing Systems", "abstract": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.", "year": 2017, "publicationdate": "2017-11-02", "externalids": {}, "doi_lower": null}
{"paper_id": 7166013, "title": "Masked Autoregressive Flow for Density Estimation", "author_names": ["G. Papamakarios", "Iain Murray", "Theo Pavlakou"], "venue": "Neural Information Processing Systems", "abstract": "Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.", "year": 2017, "publicationdate": "2017-05-19", "externalids": {}, "doi_lower": null}
{"paper_id": 229297973, "title": "Taming Transformers for High-Resolution Image Synthesis", "author_names": ["Patrick Esser", "Robin Rombach", "B. Ommer"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://git.io/JLlvY.", "year": 2020, "publicationdate": "2020-12-17", "externalids": {"DOI": "10.1109/CVPR46437.2021.01268"}, "doi_lower": "10.1109/cvpr46437.2021.01268"}
{"paper_id": 232035663, "title": "Zero-Shot Text-to-Image Generation", "author_names": ["A. Ramesh", "Mikhail Pavlov", "Gabriel Goh", "Scott Gray", "Chelsea Voss", "Alec Radford", "Mark Chen", "I. Sutskever"], "venue": "International Conference on Machine Learning", "abstract": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.", "year": 2021, "publicationdate": "2021-02-24", "externalids": {}, "doi_lower": null}
{"paper_id": 15876696, "title": "Energy-based Generative Adversarial Network", "author_names": ["J. Zhao", "Michaël Mathieu", "Yann LeCun"], "venue": "International Conference on Learning Representations", "abstract": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.", "year": 2016, "publicationdate": "2016-09-11", "externalids": {}, "doi_lower": null}
{"paper_id": 261560300, "title": "Generative Adversarial Nets", "author_names": ["I. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": "Neural Information Processing Systems", "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.", "year": 2014, "publicationdate": "2014-06-10", "externalids": {}, "doi_lower": null}
{"paper_id": 14888175, "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "author_names": ["Jascha Narain Sohl-Dickstein", "Eric A. Weiss", "Niru Maheswaranathan", "S. Ganguli"], "venue": "International Conference on Machine Learning", "abstract": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.", "year": 2015, "publicationdate": "2015-03-11", "externalids": {}, "doi_lower": null}
{"paper_id": 211022860, "title": "A Style-Based Generator Architecture for Generative Adversarial Networks", "author_names": ["Tero Karras", "S. Laine", "Timo Aila"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.", "year": 2020, "publicationdate": "2020-01-31", "externalids": {"DOI": "10.1109/TPAMI.2020.2970919"}, "doi_lower": "10.1109/tpami.2020.2970919"}
{"paper_id": 268725732, "title": "Image-to-Image Translation with Conditional Adversarial Networks", "author_names": ["Marjana Tahmid", "Md. Samiul Alam", "Namratha Rao", "Kazi Muhammad Asif Ashrafi"], "venue": "IEEE International WIE Conference on Electrical and Computer Engineering", "abstract": "Several syntheses of photographs based on label maps, restoration of objects using edge maps, and image colorizing, and many others traditionally requires designing individual/unique loss function for each task. These tasks have one thing in common: they can be treated as a single Image to the Image translation problem. Convolution neural networks (CNNs) have become popular for a wide variety of image-related problems. The Image-to-Image translation problem can be solved with one general approach, which is using the conditional adversarial network. We have implemented conditional adversarial networks. The specialty of this network is they acquire insight on a loss function to train this mapping in addition to learning the mapping to output image from input image.", "year": 2023, "publicationdate": "2023-11-25", "externalids": {"DOI": "10.1109/WIECON-ECE60392.2023.10456447"}, "doi_lower": "10.1109/wiecon-ece60392.2023.10456447"}
{"paper_id": 206770979, "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks", "author_names": ["Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A. Efros"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.", "year": 2017, "publicationdate": "2017-03-30", "externalids": {"DOI": "10.1109/ICCV.2017.244"}, "doi_lower": "10.1109/iccv.2017.244"}
{"paper_id": 52889459, "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis", "author_names": ["Andrew Brock", "Jeff Donahue", "K. Simonyan"], "venue": "International Conference on Learning Representations", "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.", "year": 2018, "publicationdate": "2018-09-27", "externalids": {}, "doi_lower": null}
{"paper_id": 219955663, "title": "Denoising Diffusion Probabilistic Models", "author_names": ["Jonathan Ho", "Ajay Jain", "P. Abbeel"], "venue": "Neural Information Processing Systems", "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL", "year": 2020, "publicationdate": "2020-06-19", "externalids": {}, "doi_lower": null}
{"paper_id": 231979499, "title": "Improved Denoising Diffusion Probabilistic Models", "author_names": ["Alex Nichol", "Prafulla Dhariwal"], "venue": "International Conference on Machine Learning", "abstract": "Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion", "year": 2021, "publicationdate": "2021-02-18", "externalids": {}, "doi_lower": null}
{"paper_id": 1152227, "title": "Estimation of Non-Normalized Statistical Models by Score Matching", "author_names": ["Aapo Hyvärinen"], "venue": "Journal of machine learning research", "abstract": null, "year": 2005, "publicationdate": "2005-12-01", "externalids": {}, "doi_lower": null}
{"paper_id": 5560643, "title": "A Connection Between Score Matching and Denoising Autoencoders", "author_names": ["Pascal Vincent"], "venue": "Neural Computation", "abstract": null, "year": 2011, "publicationdate": "2011-07-01", "externalids": {"DOI": "10.1162/NECO_a_00142"}, "doi_lower": "10.1162/neco_a_00142"}
{"paper_id": 196470871, "title": "Generative Modeling by Estimating Gradients of the Data Distribution", "author_names": ["Yang Song", "Stefano Ermon"], "venue": "Neural Information Processing Systems", "abstract": "We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.", "year": 2019, "publicationdate": "2019-07-12", "externalids": {}, "doi_lower": null}
{"paper_id": 227209335, "title": "Score-Based Generative Modeling through Stochastic Differential Equations", "author_names": ["Yang Song", "Jascha Narain Sohl-Dickstein", "Diederik P. Kingma", "Abhishek Kumar", "Stefano Ermon", "Ben Poole"], "venue": "International Conference on Learning Representations", "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.", "year": 2020, "publicationdate": "2020-11-26", "externalids": {}, "doi_lower": null}
{"paper_id": 219708245, "title": "Improved Techniques for Training Score-Based Generative Models", "author_names": ["Yang Song", "Stefano Ermon"], "venue": "Neural Information Processing Systems", "abstract": "Score-based generative models can produce high quality image samples comparable to GANs, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution (typically below 32x32), and can be unstable under some settings. We provide a new theoretical analysis of learning and sampling from score models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. To enhance stability, we also propose to maintain an exponential moving average of model weights. With these improvements, we can effortlessly scale score-based generative models to images with unprecedented resolutions ranging from 64x64 to 256x256. Our score-based models can generate high-fidelity samples that rival best-in-class GANs on various image datasets, including CelebA, FFHQ, and multiple LSUN categories.", "year": 2020, "publicationdate": "2020-06-16", "externalids": {}, "doi_lower": null}
{"paper_id": 260888585, "title": "Diffusion Models for Medical Image Analysis: A Comprehensive Survey", "author_names": ["A. Kazerouni", "Ehsan Khodapanah Aghdam", "Moein Heidari", "Reza Azad", "Mohsen Fayyaz", "I. Hacihaliloglu", "D. Merhof"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2211.07804"}, "doi_lower": "10.48550/arxiv.2211.07804"}
{"paper_id": 269606007, "title": "Video Diffusion Models: A Survey", "author_names": ["A. Melnik", "Michal Ljubljanac", "Cong Lu", "Qi Yan", "Weiming Ren", "Helge J. Ritter"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Diffusion generative models have recently become a powerful technique for creating and modifying high-quality, coherent video content. This survey provides a comprehensive overview of the critical components of diffusion models for video generation, including their applications, architectural design, and temporal dynamics modeling. The paper begins by discussing the core principles and mathematical formulations, then explores various architectural choices and methods for maintaining temporal consistency. A taxonomy of applications is presented, categorizing models based on input modalities such as text prompts, images, videos, and audio signals. Advancements in text-to-video generation are discussed to illustrate the state-of-the-art capabilities and limitations of current approaches. Additionally, the survey summarizes recent developments in training and evaluation practices, including the use of diverse video and image datasets and the adoption of various evaluation metrics to assess model performance. The survey concludes with an examination of ongoing challenges, such as generating longer videos and managing computational costs, and offers insights into potential future directions for the field. By consolidating the latest research and developments, this survey aims to serve as a valuable resource for researchers and practitioners working with video diffusion models. Website: https://github.com/ndrwmlnk/Awesome-Video-Diffusion-Models", "year": 2024, "publicationdate": "2024-05-06", "externalids": {"DOI": "10.48550/arXiv.2405.03150"}, "doi_lower": "10.48550/arxiv.2405.03150"}
{"paper_id": 266693843, "title": "Diffusion Models, Image Super-Resolution, and Everything: A Survey", "author_names": ["Brian B. Moser", "Arundhati S. Shanbhag", "Federico Raue", "Stanislav Frolov", "Sebastián M. Palacio", "Andreas Dengel"], "venue": "IEEE Transactions on Neural Networks and Learning Systems", "abstract": "Diffusion models (DMs) have disrupted the image super-resolution (SR) field and further closed the gap between image quality and human perceptual preferences. They are easy to train and can produce very high-quality samples that exceed the realism of those produced by previous generative methods. Despite their promising results, they also come with new challenges that need further research: high computational demands, comparability, lack of explainability, color shifts, and more. Unfortunately, entry into this field is overwhelming because of the abundance of publications. To address this, we provide a unified recount of the theoretical foundations underlying DMs applied to image SR and offer a detailed analysis that underscores the unique characteristics and methodologies within this domain, distinct from broader existing reviews in the field. This article articulates a cohesive understanding of DM principles and explores current research avenues, including alternative input domains, conditioning techniques, guidance mechanisms, corruption spaces, and zero-shot learning approaches. By offering a detailed examination of the evolution and current trends in image SR through the lens of DMs, this article sheds light on the existing challenges and charts potential future directions, aiming to inspire further innovation in this rapidly advancing area.", "year": 2024, "publicationdate": "2024-01-01", "externalids": {"DOI": "10.1109/TNNLS.2024.3476671"}, "doi_lower": "10.1109/tnnls.2024.3476671"}
{"paper_id": 249240430, "title": "DiVAE: Photorealistic Images Synthesis with Denoising Diffusion Decoder", "author_names": ["Jie Shi", "Chenfei Wu", "Jian Liang", "Xiang Liu", "Nan Duan"], "venue": "arXiv.org", "abstract": "Recently most successful image synthesis models are multi stage process to combine the advantages of different methods, which always includes a VAE-like model for faithfully reconstructing embedding to image and a prior model to generate image embedding. At the same time, diffusion models have shown be capacity to generate high-quality synthetic images. Our work proposes a VQ-VAE architecture model with a diffusion decoder (DiVAE) to work as the reconstructing component in image synthesis. We explore how to input image embedding into diffusion model for excellent performance and find that simple modification on diffusion's UNet can achieve it. Training on ImageNet, Our model achieves state-of-the-art results and generates more photorealistic images specifically. In addition, we apply the DiVAE with an Auto-regressive generator on conditional synthesis tasks to perform more human-feeling and detailed samples.", "year": 2022, "publicationdate": "2022-06-01", "externalids": {"DOI": "10.48550/arXiv.2206.00386"}, "doi_lower": "10.48550/arxiv.2206.00386"}
{"paper_id": 248097655, "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "author_names": ["A. Ramesh", "Prafulla Dhariwal", "Alex Nichol", "Casey Chu", "Mark Chen"], "venue": "arXiv.org", "abstract": "Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.", "year": 2022, "publicationdate": "2022-04-13", "externalids": {"DOI": "10.48550/arXiv.2204.06125"}, "doi_lower": "10.48550/arxiv.2204.06125"}
{"paper_id": 245335280, "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "author_names": ["Robin Rombach", "A. Blattmann", "Dominik Lorenz", "Patrick Esser", "B. Ommer"], "venue": "Computer Vision and Pattern Recognition", "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.", "year": 2021, "publicationdate": "2021-12-20", "externalids": {"DOI": "10.1109/CVPR52688.2022.01042"}, "doi_lower": "10.1109/cvpr52688.2022.01042"}
{"paper_id": 248986576, "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding", "author_names": ["Chitwan Saharia", "William Chan", "Saurabh Saxena", "Lala Li", "Jay Whang", "Emily L. Denton", "Seyed Kamyar Seyed Ghasemipour", "Burcu Karagol Ayan", "S. S. Mahdavi", "Raphael Gontijo Lopes", "Tim Salimans", "Jonathan Ho", "David J. Fleet", "Mohammad Norouzi"], "venue": "Neural Information Processing Systems", "abstract": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.", "year": 2022, "publicationdate": "2022-05-23", "externalids": {"DOI": "10.48550/arXiv.2205.11487"}, "doi_lower": "10.48550/arxiv.2205.11487"}
{"paper_id": 249145348, "title": "Classifier-Free Diffusion Guidance", "author_names": ["Jonathan Ho"], "venue": "arXiv.org", "abstract": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.", "year": 2022, "publicationdate": "2022-07-26", "externalids": {"DOI": "10.48550/arXiv.2207.12598"}, "doi_lower": "10.48550/arxiv.2207.12598"}
{"paper_id": 246016304, "title": "Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models", "author_names": ["Fan Bao", "Chongxuan Li", "Jun Zhu", "Bo Zhang"], "venue": "International Conference on Learning Representations", "abstract": "Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose Analytic-DPM, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a 20x to 80x speed up.", "year": 2022, "publicationdate": "2022-01-17", "externalids": {}, "doi_lower": null}
{"paper_id": 235619773, "title": "Cascaded Diffusion Models for High Fidelity Image Generation", "author_names": ["Jonathan Ho", "Chitwan Saharia", "William Chan", "David J. Fleet", "Mohammad Norouzi", "Tim Salimans"], "venue": "Journal of machine learning research", "abstract": "We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256, outperforming VQ-VAE-2.", "year": 2021, "publicationdate": "2021-05-30", "externalids": {}, "doi_lower": null}
{"paper_id": 234357997, "title": "Diffusion Models Beat GANs on Image Synthesis", "author_names": ["Prafulla Dhariwal", "Alex Nichol"], "venue": "Neural Information Processing Systems", "abstract": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion", "year": 2021, "publicationdate": "2021-05-11", "externalids": {}, "doi_lower": null}
{"paper_id": 245117331, "title": "More Control for Free! Image Synthesis with Semantic Diffusion Guidance", "author_names": ["Xihui Liu", "Dong Huk Park", "S. Azadi", "Gong Zhang", "Arman Chopikyan", "Yuxiao Hu", "Humphrey Shi", "Anna Rohrbach", "Trevor Darrell"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "Controllable image synthesis models allow creation of diverse images based on text instructions or guidance from a reference image. Recently, denoising diffusion probabilistic models have been shown to generate more realistic imagery than prior methods, and have been successfully demonstrated in unconditional and class-conditional settings. We investigate fine-grained, continuous control of this model class, and introduce a novel unified framework for semantic diffusion guidance, which allows either language or image guidance, or both. Guidance is injected into a pretrained unconditional diffusion model using the gradient of image-text or image matching scores, without re-training the diffusion model. We explore CLIP-based language guidance as well as both content and style-based image guidance in a unified framework. Our text-guided synthesis approach can be applied to datasets without associated text annotations. We conduct experiments on FFHQ and LSUN datasets, and show results on fine-grained text-guided image synthesis, synthesis of images related to a style or content reference image, and examples with both textual and image guidance.1", "year": 2021, "publicationdate": "2021-12-10", "externalids": {"DOI": "10.1109/WACV56688.2023.00037"}, "doi_lower": "10.1109/wacv56688.2023.00037"}
{"paper_id": 245335086, "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", "author_names": ["Alex Nichol", "Prafulla Dhariwal", "A. Ramesh", "Pranav Shyam", "Pamela Mishkin", "Bob McGrew", "I. Sutskever", "Mark Chen"], "venue": "International Conference on Machine Learning", "abstract": "Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.", "year": 2021, "publicationdate": "2021-12-20", "externalids": {}, "doi_lower": null}
{"paper_id": 251066705, "title": "Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models", "author_names": ["Robin Rombach", "A. Blattmann", "B. Ommer"], "venue": "arXiv.org", "abstract": "Novel architectures have recently improved generative image synthesis leading to excellent visual quality in various tasks. Of particular note is the field of ``AI-Art'', which has seen unprecedented growth with the emergence of powerful multimodal models such as CLIP. By combining speech and image synthesis models, so-called ``prompt-engineering'' has become established, in which carefully selected and composed sentences are used to achieve a certain visual style in the synthesized image. In this note, we present an alternative approach based on retrieval-augmented diffusion models (RDMs). In RDMs, a set of nearest neighbors is retrieved from an external database during training for each training instance, and the diffusion model is conditioned on these informative samples. During inference (sampling), we replace the retrieval database with a more specialized database that contains, for example, only images of a particular visual style. This provides a novel way to prompt a general trained model after training and thereby specify a particular visual style. As shown by our experiments, this approach is superior to specifying the visual style within the text prompt. We open-source code and model weights at https://github.com/CompVis/latent-diffusion .", "year": 2022, "publicationdate": "2022-07-26", "externalids": {"DOI": "10.48550/arXiv.2207.13038"}, "doi_lower": "10.48550/arxiv.2207.13038"}
{"paper_id": 251710469, "title": "Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise", "author_names": ["Arpit Bansal", "Eitan Borgnia", "Hong-Min Chu", "Jie Li", "Hamid Kazemi", "Furong Huang", "Micah Goldblum", "Jonas Geiping", "T. Goldstein"], "venue": "Neural Information Processing Systems", "abstract": "Standard diffusion models involve an image transform -- adding Gaussian noise -- and an image restoration operator that inverts this degradation. We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. The success of these fully deterministic models calls into question the community's understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference, and paves the way for generalized diffusion models that invert arbitrary processes. Our code is available at https://github.com/arpitbansal297/Cold-Diffusion-Models", "year": 2022, "publicationdate": "2022-08-19", "externalids": {"DOI": "10.48550/arXiv.2208.09392"}, "doi_lower": "10.48550/arxiv.2208.09392"}
{"paper_id": 252762155, "title": "On Distillation of Guided Diffusion Models", "author_names": ["Chenlin Meng", "Ruiqi Gao", "Diederik P. Kingma", "Stefano Ermon", "Jonathan Ho", "Tim Salimans"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Classifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALL.E 2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given a pre-trained classifier-free guided model, we first learn a single model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate images visually comparable to that of the original model using as few as 4 sampling steps on ImageNet $64\\times 64$ and CIFAR-10, achieving FID/IS scores comparable to that of the original model while being up to 256 times faster to sample from. For diffusion models trained on the latent-space (e.g., Stable Diffusion), our approach is able to generate high-fidelity images using as few as 1 to 4 denoising steps, accelerating inference by at least 10-fold compared to existing methods on ImageNet $256\\times 256$ and LAION datasets. We further demonstrate the effectiveness of our approach on text-guided image editing and inpainting, where our distilled model is able to generate high-quality results using as few as 2–4 denoising steps.", "year": 2022, "publicationdate": "2022-10-06", "externalids": {"DOI": "10.1109/CVPR52729.2023.01374"}, "doi_lower": "10.1109/cvpr52729.2023.01374"}
{"paper_id": 254069706, "title": "Wavelet Diffusion Models are fast and scalable Image Generators", "author_names": ["Hao Phung", "Quan Dao", "A. Tran"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Diffusion models are rising as a powerful solution for high-fidelity image generation, which exceeds GANs in quality in many circumstances. However, their slow training and inference speed is a huge bottleneck, blocking them from being used in real-time applications. A recent DiffusionGAN method significantly decreases the models' running time by reducing the number of sampling steps from thousands to several, but their speeds still largely lag behind the GAN counterparts. This paper aims to reduce the speed gap by proposing a novel wavelet-based diffusion scheme. We extract low-and-high frequency components from both image and feature levels via wavelet decomposition and adaptively handle these components for faster processing while maintaining good generation quality. Furthermore, we propose to use a reconstruction term, which effectively boosts the model training convergence. Experimental results on CelebA-HQ, CIFAR-10, LSUN-Church, and STL-10 datasets prove our solution is a stepping-stone to offering real-time and high-fidelity diffusion models. Our code and pre-trained checkpoints are available at https://github.com/VinAIResearch/WaveDiff.git.", "year": 2022, "publicationdate": "2022-11-29", "externalids": {"DOI": "10.1109/CVPR52729.2023.00983"}, "doi_lower": "10.1109/cvpr52729.2023.00983"}
{"paper_id": 253523371, "title": "Versatile Diffusion: Text, Images and Variations All in One Diffusion Model", "author_names": ["Xingqian Xu", "Zhangyang Wang", "Eric Zhang", "Kai Wang", "Humphrey Shi"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Recent advances in diffusion models have set an impressive milestone in many generation tasks, and trending works such as DALL-E2, Imagen, and Stable Diffusion have attracted great interest. Despite the rapid landscape changes, recent new approaches focus on extensions and performance rather than capacity, thus requiring separate models for separate tasks. In this work, we expand the existing single-flow diffusion pipeline into a multi-task multimodal network, dubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image, image-to-text, and variations in one unified model. The pipeline design of VD instantiates a unified multi-flow diffusion framework, consisting of sharable and swappable layer modules that enable the crossmodal generality beyond images and text. Through extensive experiments, we demonstrate that VD successfully achieves the following: a) VD outperforms the baseline approaches and handles all its base tasks with competitive quality; b) VD enables novel extensions such as disentanglement of style and semantics, dual- and multi-context blending, etc.; c) The success of our multi-flow multimodal framework over images and text may inspire further diffusion-based universal AI research. Our code and models are open-sourced at https://github.com/SHI-Labs/Versatile-Diffusion.", "year": 2022, "publicationdate": "2022-11-15", "externalids": {"DOI": "10.1109/ICCV51070.2023.00713"}, "doi_lower": "10.1109/iccv51070.2023.00713"}
{"paper_id": 233241040, "title": "Image Super-Resolution via Iterative Refinement", "author_names": ["Chitwan Saharia", "Jonathan Ho", "William Chan", "Tim Salimans", "David J. Fleet", "Mohammad Norouzi"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models (Ho et al. 2020), (Sohl-Dickstein et al. 2015) to image-to-image translation, and performs super-resolution through a stochastic iterative denoising process. Output images are initialized with pure Gaussian noise and iteratively refined using a U-Net architecture that is trained on denoising at various noise levels, conditioned on a low-resolution input image. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8× face super-resolution task on CelebA-HQ for which SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GAN baselines do not exceed a fool rate of 34%. We evaluate SR3 on a 4× super-resolution task on ImageNet, where SR3 outperforms baselines in human evaluation and classification accuracy of a ResNet-50 classifier trained on high-resolution images. We further show the effectiveness of SR3 in cascaded image generation, where a generative model is chained with super-resolution models to synthesize high-resolution images with competitive FID scores on the class-conditional 256×256 ImageNet generation challenge.", "year": 2021, "publicationdate": "2021-04-15", "externalids": {"DOI": "10.1109/TPAMI.2022.3204461"}, "doi_lower": "10.1109/tpami.2022.3204461"}
{"paper_id": 243938678, "title": "Palette: Image-to-Image Diffusion Models", "author_names": ["Chitwan Saharia", "William Chan", "Huiwen Chang", "Chris A. Lee", "Jonathan Ho", "Tim Salimans", "David J. Fleet", "Mohammad Norouzi"], "venue": "International Conference on Computer Graphics and Interactive Techniques", "abstract": "This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io/ for an overview of the results and code.", "year": 2021, "publicationdate": "2021-11-10", "externalids": {"DOI": "10.1145/3528233.3530757"}, "doi_lower": "10.1145/3528233.3530757"}
{"paper_id": 251197000, "title": "Restoring Vision in Adverse Weather Conditions With Patch-Based Denoising Diffusion Models", "author_names": ["Ozan Özdenizci", "R. Legenstein"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "Image restoration under adverse weather conditions has been of significant interest for various computer vision applications. Recent successful methods rely on the current progress in deep neural network architectural designs (e.g., with vision transformers). Motivated by the recent progress achieved with state-of-the-art conditional generative models, we present a novel patch-based image restoration algorithm based on denoising diffusion probabilistic models. Our patch-based diffusion modeling approach enables size-agnostic image restoration by using a guided denoising process with smoothed noise estimates across overlapping patches during inference. We empirically evaluate our model on benchmark datasets for image desnowing, combined deraining and dehazing, and raindrop removal. We demonstrate our approach to achieve state-of-the-art performances on both weather-specific and multi-weather image restoration, and experimentally show strong generalization to real-world test images.", "year": 2022, "publicationdate": "2022-07-29", "externalids": {"DOI": "10.1109/TPAMI.2023.3238179"}, "doi_lower": "10.1109/tpami.2023.3238179"}
{"paper_id": 257532290, "title": "ResDiff: Combining CNN and Diffusion Model for Image Super-Resolution", "author_names": ["Shuyao Shang", "Zhengyang Shan", "Guangxing Liu", "Jingling Zhang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Adapting the Diffusion Probabilistic Model (DPM) for direct image super-resolution is wasteful, given that a simple Convolutional Neural Network (CNN) can recover the main low-frequency content. Therefore, we present ResDiff, a novel Diffusion Probabilistic Model based on Residual structure for Single Image Super-Resolution (SISR). ResDiff utilizes a combination of a CNN, which restores primary low-frequency components, and a DPM, which predicts the residual between the ground-truth image and the CNN predicted image. In contrast to the common diffusion-based methods that directly use LR space to guide the noise towards HR space, ResDiff utilizes the CNN’s initial prediction to direct the noise towards the residual space between HR space and CNN-predicted space, which not only accelerates the generation process but also acquires superior sample quality. Additionally, a frequency-domain-based loss function for CNN is introduced to facilitate its restoration, and a frequency-domain guided diffusion is designed for DPM on behalf of predicting high-frequency details. The extensive experiments on multiple benchmark datasets demonstrate that ResDiff outperforms previous diffusion based methods in terms of shorter model convergence time, superior generation quality, and more diverse samples.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {"DOI": "10.48550/arXiv.2303.08714"}, "doi_lower": "10.48550/arxiv.2303.08714"}
{"paper_id": 257804739, "title": "Implicit Diffusion Models for Continuous Super-Resolution", "author_names": ["Sicheng Gao", "Xuhui Liu", "Bo-Wen Zeng", "Sheng Xu", "Yanjing Li", "Xiaonan Luo", "Jianzhuang Liu", "Xiantong Zhen", "Baochang Zhang"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Image super-resolution (SR) has attracted increasing attention due to its widespread applications. However, current SR methods generally suffer from over-smoothing and artifacts, and most work only with fixed magnifications. This paper introduces an Implicit Diffusion Model (IDM) for high-fidelity continuous image super-resolution. IDM integrates an implicit neural representation and a denoising diffusion model in a unified end-to-end framework, where the implicit neural representation is adopted in the decoding process to learn continuous-resolution representation. Furthermore, we design a scale-adaptive conditioning mechanism that consists of a low-resolution (LR) conditioning network and a scaling factor. The scaling factor regulates the resolution and accordingly modulates the proportion of the LR information and generated features in the final output, which enables the model to accommodate the continuous-resolution requirement. Extensive experiments validate the effectiveness of our IDM and demonstrate its superior performance over prior arts. The source code will be available at https://github.com/Ree1s/IDM.", "year": 2023, "publicationdate": "2023-03-29", "externalids": {"DOI": "10.1007/s11263-025-02462-y"}, "doi_lower": "10.1007/s11263-025-02462-y"}
{"paper_id": 254535902, "title": "ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal", "author_names": ["Lanqing Guo", "Chong Wang", "Wenhan Yang", "Siyu Huang", "Yufei Wang", "H. Pfister", "B. Wen"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Recent deep learning methods have achieved promising results in image shadow removal. However, their restored images still suffer from unsatisfactory boundary artifacts, due to the lack of degradation prior embedding and the deficiency in modeling capacity. Our work addresses these issues by proposing a unified diffusion framework that integrates both the image and degradation priors for highly effective shadow removal. In detail, we first propose a shadow degradation model, which inspires us to build a novel unrolling diffusion model, dubbed ShandowDiffusion. It remarkably improves the model's capacity in shadow removal via progressively refining the desired output with both degradation prior and diffusive generative prior, which by nature can serve as a new strong baseline for image restoration. Furthermore, ShadowDiffusion progressively refines the estimated shadow mask as an auxiliary task of the diffusion generator, which leads to more accurate and robust shadow-free image generation. We conduct extensive experiments on three popular public datasets, including ISTD, ISTD+, and SRD, to validate our method's effectiveness. Compared to the state-of-the-art methods, our model achieves a significant improvement in terms of PSNR, increasing from 31.69dB to 34. 73dB over SRD dataset. 11https://github.com/GuoLanqing/ShadowDiffusion", "year": 2022, "publicationdate": "2022-12-09", "externalids": {"DOI": "10.1109/CVPR52729.2023.01350"}, "doi_lower": "10.1109/cvpr52729.2023.01350"}
{"paper_id": 256358842, "title": "Image Restoration with Mean-Reverting Stochastic Differential Equations", "author_names": ["Ziwei Luo", "F. Gustafsson", "Zheng Zhao", "Jens Sjölund", "Thomas Bo Schön"], "venue": "International Conference on Machine Learning", "abstract": "This paper presents a stochastic differential equation (SDE) approach for general-purpose image restoration. The key construction consists in a mean-reverting SDE that transforms a high-quality image into a degraded counterpart as a mean state with fixed Gaussian noise. Then, by simulating the corresponding reverse-time SDE, we are able to restore the origin of the low-quality image without relying on any task-specific prior knowledge. Crucially, the proposed mean-reverting SDE has a closed-form solution, allowing us to compute the ground truth time-dependent score and learn it with a neural network. Moreover, we propose a maximum likelihood objective to learn an optimal reverse trajectory that stabilizes the training and improves the restoration results. The experiments show that our proposed method achieves highly competitive performance in quantitative comparisons on image deraining, deblurring, and denoising, setting a new state-of-the-art on two deraining datasets. Finally, the general applicability of our approach is further demonstrated via qualitative results on image super-resolution, inpainting, and dehazing. Code is available at https://github.com/Algolzw/image-restoration-sde.", "year": 2023, "publicationdate": "2023-01-27", "externalids": {"DOI": "10.48550/arXiv.2301.11699"}, "doi_lower": "10.48550/arxiv.2301.11699"}
{"paper_id": 257557425, "title": "DiffIR: Efficient Diffusion Model for Image Restoration", "author_names": ["Bin Xia", "Yulun Zhang", "Shiyin Wang", "Yitong Wang", "Xing Wu", "Yapeng Tian", "Wenming Yang", "L. Gool"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Diffusion model (DM) has achieved SOTA performance by modeling the image synthesis process into a sequential application of a denoising network. However, different from image synthesis, image restoration (IR) has a strong constraint to generate results in accordance with ground-truth. Thus, for IR, traditional DMs running massive iterations on a large model to estimate whole images or feature maps is inefficient. To address this issue, we propose an efficient DM for IR (DiffIR), which consists of a compact IR prior extraction network (CPEN), dynamic IR transformer (DIRformer), and denoising network. Specifically, DiffIR has two training stages: pretraining and training DM. In pretraining, we input ground-truth images into CPENS1 to capture a compact IR prior representation (IPR) to guide DIRformer. In the second stage, we train the DM to directly estimate the same IRP as pretrained CPENS1 only using LQ images. We observe that since the IPR is only a compact vector, DiffIR can use fewer iterations than traditional DM to obtain accurate estimations and generate more stable and realistic results. Since the iterations are few, our DiffIR can adopt a joint optimization of CPENS2, DIRformer, and denoising network, which can further reduce the estimation error influence. We conduct extensive experiments on several IR tasks and achieve SOTA performance while consuming less computational costs. Code is available at https://github.com/Zj-BinXia/DiffIR.", "year": 2023, "publicationdate": "2023-03-16", "externalids": {"DOI": "10.1109/ICCV51070.2023.01204"}, "doi_lower": "10.1109/iccv51070.2023.01204"}
{"paper_id": 236950721, "title": "ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models", "author_names": ["Jooyoung Choi", "Sungwon Kim", "Yonghyun Jeong", "Youngjune Gwon", "Sungroh Yoon"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Denoising diffusion probabilistic models (DDPM) have shown remarkable performance in unconditional image generation. However, due to the stochasticity of the generative process in DDPM, it is challenging to generate images with the desired semantics. In this work, we propose Iterative Latent Variable Refinement (ILVR), a method to guide the generative process in DDPM to generate high-quality images based on a given reference image. Here, the refinement of the generative process in DDPM enables a single DDPM to sample images from various sets directed by the reference image. The proposed ILVR method generates high-quality images while controlling the generation. The controllability of our method allows adaptation of a single DDPM without any additional learning in various image generation tasks, such as generation from various downsampling factors, multi-domain image translation, paint-to-image, and editing with scribbles.", "year": 2021, "publicationdate": "2021-08-06", "externalids": {"DOI": "10.1109/iccv48922.2021.01410"}, "doi_lower": "10.1109/iccv48922.2021.01410"}
{"paper_id": 258841180, "title": "WaveDM: Wavelet-Based Diffusion Models for Image Restoration", "author_names": ["Yi Huang", "Jiancheng Huang", "Jianzhuang Liu", "Mingfu Yan", "Yu Dong", "Jiaxi Lv", "Shifeng Chen"], "venue": "IEEE transactions on multimedia", "abstract": "Latest diffusion-based methods for many image restoration tasks outperform traditional models, but they encounter the long-time inference problem. To tackle it, this paper proposes a Wavelet-Based Diffusion Model (WaveDM). WaveDM learns the distribution of clean images in the wavelet domain conditioned on the wavelet spectrum of degraded images after wavelet transform, which is more time-saving in each step of sampling than modeling in the spatial domain. To ensure restoration performance, a unique training strategy is proposed where the low-frequency and high-frequency spectrums are learned using distinct modules. In addition, an Efficient Conditional Sampling (ECS) strategy is developed from experiments, which reduces the number of total sampling steps to around 5. Evaluations on twelve benchmark datasets including image raindrop removal, rain steaks removal, dehazing, defocus deblurring, demoiréing, and denoising demonstrate that WaveDM achieves state-of-the-art performance with the efficiency that is comparable to traditional one-pass methods and over 100× faster than existing image restoration methods using vanilla diffusion models.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.1109/TMM.2024.3359769"}, "doi_lower": "10.1109/tmm.2024.3359769"}
{"paper_id": 254125609, "title": "Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model", "author_names": ["Yinhuai Wang", "Jiwen Yu", "Jian Zhang"], "venue": "International Conference on Learning Representations", "abstract": "Most existing Image Restoration (IR) models are task-specific, which can not be generalized to different degradation operators. In this work, we propose the Denoising Diffusion Null-Space Model (DDNM), a novel zero-shot framework for arbitrary linear IR problems, including but not limited to image super-resolution, colorization, inpainting, compressed sensing, and deblurring. DDNM only needs a pre-trained off-the-shelf diffusion model as the generative prior, without any extra training or network modifications. By refining only the null-space contents during the reverse diffusion process, we can yield diverse results satisfying both data consistency and realness. We further propose an enhanced and robust version, dubbed DDNM+, to support noisy restoration and improve restoration quality for hard tasks. Our experiments on several IR tasks reveal that DDNM outperforms other state-of-the-art zero-shot IR methods. We also demonstrate that DDNM+ can solve complex real-world applications, e.g., old photo restoration.", "year": 2022, "publicationdate": "2022-12-01", "externalids": {"DOI": "10.48550/arXiv.2212.00490"}, "doi_lower": "10.48550/arxiv.2212.00490"}
{"paper_id": 254591838, "title": "DifFace: Blind Face Restoration With Diffused Error Contraction", "author_names": ["Zongsheng Yue", "Chen Change Loy"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "While deep learning-based methods for blind face restoration have achieved unprecedented success, they still suffer from two major limitations. First, most of them deteriorate when facing complex degradations out of their training data. Second, these methods require multiple constraints, e.g., fidelity, perceptual, and adversarial losses, which require laborious hyper-parameter tuning to stabilize and balance their influences. In this work, we propose a novel method named <italic>DifFace</italic> that is capable of coping with unseen and complex degradations more gracefully without complicated loss designs. The key of our method is to establish a posterior distribution from the observed low-quality (LQ) image to its high-quality (HQ) counterpart. In particular, we design a transition distribution from the LQ image to the intermediate state of a pre-trained diffusion model and then gradually transmit from this intermediate state to the HQ target by recursively applying a pre-trained diffusion model. The transition distribution only relies on a restoration backbone that is trained with <inline-formula><tex-math notation=\"LaTeX\">$L_{1}$</tex-math><alternatives><mml:math><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href=\"yue-ieq1-3432651.gif\"/></alternatives></inline-formula> loss on some synthetic data, which favorably avoids the cumbersome training process in existing methods. Moreover, the transition distribution can contract the error of the restoration backbone and thus makes our method more robust to unknown degradations. Comprehensive experiments show that <italic>DifFace</italic> is superior to current state-of-the-art methods, especially in cases with severe degradations.", "year": 2022, "publicationdate": "2022-12-13", "externalids": {"DOI": "10.1109/TPAMI.2024.3432651"}, "doi_lower": "10.1109/tpami.2024.3432651"}
{"paper_id": 249282628, "title": "Improving Diffusion Models for Inverse Problems using Manifold Constraints", "author_names": ["Hyungjin Chung", "Byeongsu Sim", "Dohoon Ryu", "J. C. Ye"], "venue": "Neural Information Processing Systems", "abstract": "Recently, diffusion models have been used to solve various inverse problems in an unsupervised manner with appropriate modifications to the sampling process. However, the current solvers, which recursively apply a reverse diffusion step followed by a projection-based measurement consistency step, often produce suboptimal results. By studying the generative sampling path, here we show that current solvers throw the sample path off the data manifold, and hence the error accumulates. To address this, we propose an additional correction term inspired by the manifold constraint, which can be used synergistically with the previous solvers to make the iterations close to the manifold. The proposed manifold constraint is straightforward to implement within a few lines of code, yet boosts the performance by a surprisingly large margin. With extensive experiments, we show that our method is superior to the previous methods both theoretically and empirically, producing promising results in many applications such as image inpainting, colorization, and sparse-view computed tomography. Code available https://github.com/HJ-harry/MCG_diffusion", "year": 2022, "publicationdate": "2022-06-02", "externalids": {"DOI": "10.48550/arXiv.2206.00941"}, "doi_lower": "10.48550/arxiv.2206.00941"}
{"paper_id": 252596252, "title": "Diffusion Posterior Sampling for General Noisy Inverse Problems", "author_names": ["Hyungjin Chung", "Jeongsol Kim", "Michael T. McCann", "M. Klasky", "J. C. Ye"], "venue": "International Conference on Learning Representations", "abstract": "Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring. Code available at https://github.com/DPS2022/diffusion-posterior-sampling", "year": 2022, "publicationdate": "2022-09-29", "externalids": {"DOI": "10.48550/arXiv.2209.14687"}, "doi_lower": "10.48550/arxiv.2209.14687"}
{"paper_id": 248006185, "title": "Video Diffusion Models", "author_names": ["Jonathan Ho", "Tim Salimans", "Alexey Gritsenko", "William Chan", "Mohammad Norouzi", "David J. Fleet"], "venue": "Neural Information Processing Systems", "abstract": "Generating temporally coherent high fidelity video is an important milestone in generative modeling research. We make progress towards this milestone by proposing a diffusion model for video generation that shows very promising initial results. Our model is a natural extension of the standard image diffusion architecture, and it enables jointly training from image and video data, which we find to reduce the variance of minibatch gradients and speed up optimization. To generate long and higher resolution videos we introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods. We present the first results on a large text-conditioned video generation task, as well as state-of-the-art results on established benchmarks for video prediction and unconditional video generation. Supplementary material is available at https://video-diffusion.github.io/", "year": 2022, "publicationdate": "2022-04-07", "externalids": {"DOI": "10.48550/arXiv.2204.03458"}, "doi_lower": "10.48550/arxiv.2204.03458"}
{"paper_id": 252715883, "title": "Imagen Video: High Definition Video Generation with Diffusion Models", "author_names": ["Jonathan Ho", "William Chan", "Chitwan Saharia", "Jay Whang", "Ruiqi Gao", "A. Gritsenko", "Diederik P. Kingma", "Ben Poole", "Mohammad Norouzi", "David J. Fleet", "Tim Salimans"], "venue": "arXiv.org", "abstract": "We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial super-resolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See https://imagen.research.google/video/ for samples.", "year": 2022, "publicationdate": "2022-10-05", "externalids": {"DOI": "10.48550/arXiv.2210.02303"}, "doi_lower": "10.48550/arxiv.2210.02303"}
{"paper_id": 258762178, "title": "Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models", "author_names": ["Songwei Ge", "Seungjun Nah", "Guilin Liu", "Tyler Poon", "Andrew Tao", "Bryan Catanzaro", "David Jacobs", "Jia-Bin Huang", "Ming-Yu Liu", "Y. Balaji"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Despite tremendous progress in generating high-quality images using diffusion models, synthesizing a sequence of animated frames that are both photorealistic and temporally coherent is still in its infancy. While off-the-shelf billion-scale datasets for image generation are available, collecting similar video data of the same scale is still challenging. Also, training a video diffusion model is computationally much more expensive than its image counterpart. In this work, we explore finetuning a pretrained image diffusion model with video data as a practical solution for the video synthesis task. We find that naively extending the image noise prior to video noise prior in video diffusion leads to sub-optimal performance. Our carefully designed video noise prior leads to substantially better performance. Extensive experimental validation shows that our model, Preserve Your Own COrrelation (PYoCo), attains SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It also achieves SOTA video generation quality on the small-scale UCF-101 benchmark with a 10× smaller model using significantly less computation than the prior art. The project page is available at https://research.nvidia.com/labs/dir/pyoco/.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {"DOI": "10.1109/ICCV51070.2023.02096"}, "doi_lower": "10.1109/iccv51070.2023.02096"}
{"paper_id": 258187553, "title": "Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models", "author_names": ["A. Blattmann", "Robin Rombach", "Huan Ling", "Tim Dockhorn", "Seung Wook Kim", "S. Fidler", "Karsten Kreis"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and finetuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution $512 \\times 1024$, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pretrained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to $1280 \\times 2048$. We show that the temporal layers trained in this way generalize to different finetuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page: https://nv-tlabs.github.io/VideoLDM/", "year": 2023, "publicationdate": "2023-04-18", "externalids": {"DOI": "10.1109/CVPR52729.2023.02161"}, "doi_lower": "10.1109/cvpr52729.2023.02161"}
{"paper_id": 257663639, "title": "NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation", "author_names": ["Sheng-Siang Yin", "Chenfei Wu", "Huan Yang", "Jianfeng Wang", "Xiaodong Wang", "Minheng Ni", "Zhengyuan Yang", "Linjie Li", "Shuguang Liu", "Fan Yang", "Jianlong Fu", "Gong Ming", "Lijuan Wang", "Zicheng Liu", "Houqiang Li", "Nan Duan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion architecture for eXtremely Long video generation. Most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the sequential generation is inefficient. Instead, our approach adopts a “coarse-to-fine” process, in which the video can be generated in parallel at the same granularity. A global diffusion model is applied to generate the keyframes across the entire time range, and then local diffusion models recursively fill in the content between nearby frames. This simple yet effective strategy allows us to directly train on long videos (3376 frames) to reduce the training-inference gap and makes it possible to generate all segments in parallel. To evaluate our model, we build FlintstonesHD dataset, a new benchmark for long video generation. Experiments show that our model not only generates high-quality long videos with both global and local coherence, but also decreases the average inference time from 7.55min to 26s (by 94.26%) at the same hardware setting when generating 1024 frames. The homepage link is [NUWA-XL](https://msra-nuwa.azurewebsites.net)", "year": 2023, "publicationdate": "2023-03-22", "externalids": {"DOI": "10.48550/arXiv.2303.12346"}, "doi_lower": "10.48550/arxiv.2303.12346"}
{"paper_id": 256615582, "title": "Structure and Content-Guided Video Synthesis with Diffusion Models", "author_names": ["Patrick Esser", "Johnathan Chiu", "Parmida Atighehchian", "Jonathan Granskog", "Anastasis Germanidis"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Text-guided generative diffusion models unlock powerful image creation and editing tools. Recent approaches that edit the content of footage while retaining structure require expensive re-training for every input or rely on error-prone propagation of image edits across frames.In this work, we present a structure and content-guided video diffusion model that edits videos based on descriptions of the desired output. Conflicts between user-provided content edits and structure representations occur due to insufficient disentanglement between the two aspects. As a solution, we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity. A novel guidance method, enabled by joint video and image training, exposes explicit control over temporal consistency. Our experiments demonstrate a wide variety of successes; fine-grained control over output characteristics, customization based on a few reference images, and a strong user preference towards results by our model.", "year": 2023, "publicationdate": "2023-02-06", "externalids": {"DOI": "10.1109/ICCV51070.2023.00675"}, "doi_lower": "10.1109/iccv51070.2023.00675"}
{"paper_id": 258180320, "title": "Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation", "author_names": ["Jie An", "Songyang Zhang", "Harry Yang", "Sonal Gupta", "Jia-Bin Huang", "Jiebo Luo", "Xiaoyue Yin"], "venue": "arXiv.org", "abstract": "We propose Latent-Shift -- an efficient text-to-video generation method based on a pretrained text-to-image generation model that consists of an autoencoder and a U-Net diffusion model. Learning a video diffusion model in the latent space is much more efficient than in the pixel space. The latter is often limited to first generating a low-resolution video followed by a sequence of frame interpolation and super-resolution models, which makes the entire pipeline very complex and computationally expensive. To extend a U-Net from image generation to video generation, prior work proposes to add additional modules like 1D temporal convolution and/or temporal attention layers. In contrast, we propose a parameter-free temporal shift module that can leverage the spatial U-Net as is for video generation. We achieve this by shifting two portions of the feature map channels forward and backward along the temporal dimension. The shifted features of the current frame thus receive the features from the previous and the subsequent frames, enabling motion learning without additional parameters. We show that Latent-Shift achieves comparable or better results while being significantly more efficient. Moreover, Latent-Shift can generate images despite being finetuned for T2V generation.", "year": 2023, "publicationdate": "2023-04-17", "externalids": {"DOI": "10.48550/arXiv.2304.08477"}, "doi_lower": "10.48550/arxiv.2304.08477"}
{"paper_id": 260887737, "title": "ModelScope Text-to-Video Technical Report", "author_names": ["Jiuniu Wang", "Hangjie Yuan", "Dayou Chen", "Yingya Zhang", "Xiang Wang", "Shiwei Zhang"], "venue": "arXiv.org", "abstract": "This paper introduces ModelScopeT2V, a text-to-video synthesis model that evolves from a text-to-image synthesis model (i.e., Stable Diffusion). ModelScopeT2V incorporates spatio-temporal blocks to ensure consistent frame generation and smooth movement transitions. The model could adapt to varying frame numbers during training and inference, rendering it suitable for both image-text and video-text datasets. ModelScopeT2V brings together three components (i.e., VQGAN, a text encoder, and a denoising UNet), totally comprising 1.7 billion parameters, in which 0.5 billion parameters are dedicated to temporal capabilities. The model demonstrates superior performance over state-of-the-art methods across three evaluation metrics. The code and an online demo are available at \\url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}.", "year": 2023, "publicationdate": "2023-08-12", "externalids": {"DOI": "10.48550/arXiv.2308.06571"}, "doi_lower": "10.48550/arxiv.2308.06571"}
{"paper_id": 261494083, "title": "VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation", "author_names": ["Xin Li", "Wenqing Chu", "Ye Wu", "Weihang Yuan", "Fanglong Liu", "Qi Zhang", "Fu Li", "Haocheng Feng", "Errui Ding", "Jingdong Wang"], "venue": "arXiv.org", "abstract": "In this paper, we present VideoGen, a text-to-video generation approach, which can generate a high-definition video with high frame fidelity and strong temporal consistency using reference-guided latent diffusion. We leverage an off-the-shelf text-to-image generation model, e.g., Stable Diffusion, to generate an image with high content quality from the text prompt, as a reference image to guide video generation. Then, we introduce an efficient cascaded latent diffusion module conditioned on both the reference image and the text prompt, for generating latent video representations, followed by a flow-based temporal upsampling step to improve the temporal resolution. Finally, we map latent video representations into a high-definition video through an enhanced video decoder. During training, we use the first frame of a ground-truth video as the reference image for training the cascaded latent diffusion module. The main characterises of our approach include: the reference image generated by the text-to-image model improves the visual fidelity; using it as the condition makes the diffusion model focus more on learning the video dynamics; and the video decoder is trained over unlabeled video data, thus benefiting from high-quality easily-available videos. VideoGen sets a new state-of-the-art in text-to-video generation in terms of both qualitative and quantitative evaluation. See \\url{https://videogen.github.io/VideoGen/} for more samples.", "year": 2023, "publicationdate": "2023-09-01", "externalids": {"DOI": "10.48550/arXiv.2309.00398"}, "doi_lower": "10.48550/arxiv.2309.00398"}
{"paper_id": 253802030, "title": "Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths", "author_names": ["Yin-Yin He", "Tianyu Yang", "Yong Zhang", "Ying Shan", "Qifeng Chen"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2211.13221"}, "doi_lower": "10.48550/arxiv.2211.13221"}
{"paper_id": 262823915, "title": "LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models", "author_names": ["Yaohui Wang", "Xinyuan Chen", "Xin Ma", "Shangchen Zhou", "Ziqi Huang", "Yi Wang", "Ceyuan Yang", "Yinan He", "Jiashuo Yu", "Pe-der Yang", "Yuwei Guo", "Tianxing Wu", "Chenyang Si", "Yuming Jiang", "Cunjian Chen", "Chen Change Loy", "Bo Dai", "Dahua Lin", "Y. Qiao", "Ziwei Liu"], "venue": "International Journal of Computer Vision", "abstract": "This work aims to learn a high-quality text-to-video (T2V) generative model by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a highly desirable yet challenging task to simultaneously (a) accomplish the synthesis of visually realistic and temporally coherent videos while (b) preserving the strong creative generation nature of the pre-trained T2I model. To this end, we propose LaVie, an integrated video generation framework that operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model. Our key insights are two-fold: (1) We reveal that the incorporation of simple temporal self-attentions, coupled with rotary positional encoding, adequately captures the temporal correlations inherent in video data. (2) Additionally, we validate that the process of joint image-video fine-tuning plays a pivotal role in producing high-quality and creative outcomes. To enhance the performance of LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M, consisting of 25 million text-video pairs that prioritize quality, diversity, and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves state-of-the-art performance both quantitatively and qualitatively. Furthermore, we showcase the versatility of pre-trained LaVie models in various long video generation and personalized video synthesis applications. Project page: https://github.com/Vchitect/LaVie/.", "year": 2023, "publicationdate": "2023-09-26", "externalids": {"DOI": "10.1007/s11263-024-02295-1"}, "doi_lower": "10.1007/s11263-024-02295-1"}
{"paper_id": 268052632, "title": "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation", "author_names": [], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 265308987, "title": "GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning", "author_names": ["Jiaxi Lv", "Yi Huang", "Mingfu Yan", "Jiancheng Huang", "Jianzhuang Liu", "Yifan Liu", "Yafei Wen", "Xiaoxin Chen", "Shifeng Chen"], "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "abstract": "Recent advances in text-to-video generation have harnessed the power of diffusion models to create visually compelling content conditioned on text prompts. However, they usually encounter high computational costs and often struggle to produce videos with coherent physical motions. To tackle these issues, we propose GPT4Motion, a training-free framework that leverages the planning capability of large language models such as GPT, the physical simulation strength of Blender, and the excellent image generation ability of text-to-image diffusion models to enhance the quality of video synthesis. Specifically, GPT4Motion employs GPT-4 to generate a Blender script based on a user textual prompt, which commands Blender’s built-in physics engine to craft fundamental scene components that encapsulate coherent physical motions across frames. Then these components are inputted into Stable Diffusion to generate a video aligned with the textual prompt. Experimental results on three basic physical motion scenarios, including rigid object drop and collision, cloth draping and swinging, and liquid flow, demonstrate that GPT4Motion can generate high-quality videos efficiently in maintaining motion coherency and entity consistency. GPT4Motion offers new insights in text-to-video research, enhancing its quality and broadening its horizon for future explorations. Our homepage website is https://GPT4Motion.github.io.", "year": 2023, "publicationdate": "2023-11-21", "externalids": {"DOI": "10.1109/CVPRW63382.2024.00150"}, "doi_lower": "10.1109/cvprw63382.2024.00150"}
{"paper_id": 263151295, "title": "Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation", "author_names": ["David Junhao Zhang", "Jay Zhangjie Wu", "Jia-Wei Liu", "Rui Zhao", "L. Ran", "Yuchao Gu", "Difei Gao", "Mike Zheng Shou"], "venue": "International Journal of Computer Vision", "abstract": "Significant advancements have been achieved in the realm of large-scale pre-trained text-to-video Diffusion Models (VDMs). However, previous methods either rely solely on pixel-based VDMs, which come with high computational costs, or on latent-based VDMs, which often struggle with precise text-video alignment. In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation. Our model first uses pixel-based VDMs to produce a low-resolution video of strong text-video correlation. After that, we propose a novel expert translation method that employs the latent-based VDMs to further upsample the low-resolution video to high resolution, which can also remove potential artifacts and corruptions from low-resolution videos. Compared to latent VDMs, Show-1 can produce high-quality videos of precise text-video alignment; Compared to pixel VDMs, Show-1 is much more efficient (GPU memory usage during inference is 15 G vs. 72 G). Furthermore, our Show-1 model can be readily adapted for motion customization and video stylization applications through simple temporal attention layer finetuning. Our model achieves state-of-the-art performance on standard video generation benchmarks. Code of Show-1 is publicly available and more videos can be found here.", "year": 2023, "publicationdate": "2023-09-27", "externalids": {"DOI": "10.1007/s11263-024-02271-9"}, "doi_lower": "10.1007/s11263-024-02271-9"}
{"paper_id": 265312551, "title": "Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets", "author_names": ["A. Blattmann", "Tim Dockhorn", "Sumith Kulal", "Daniel Mendelevitch", "Maciej Kilian", "Dominik Lorenz"], "venue": "arXiv.org", "abstract": "We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for curating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/Stability-AI/generative-models .", "year": 2023, "publicationdate": "2023-11-25", "externalids": {"DOI": "10.48550/arXiv.2311.15127"}, "doi_lower": "10.48550/arxiv.2311.15127"}
{"paper_id": 265281059, "title": "Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning", "author_names": ["Rohit Girdhar", "Mannat Singh", "Andrew Brown", "Quentin Duval", "S. Azadi", "Sai Saketh Rambhatla", "Akbar Shah", "Xi Yin", "Devi Parikh", "Ishan Misra"], "venue": "European Conference on Computer Vision", "abstract": "We present Emu Video, a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image. We identify critical design decisions--adjusted noise schedules for diffusion, and multi-stage training that enable us to directly generate high quality and high resolution videos, without requiring a deep cascade of models as in prior work. In human evaluations, our generated videos are strongly preferred in quality compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing approach naturally lends itself to animating images based on a user's text prompt, where our generations are preferred 96% over prior work.", "year": 2023, "publicationdate": "2023-11-17", "externalids": {"DOI": "10.48550/arXiv.2311.10709"}, "doi_lower": "10.48550/arxiv.2311.10709"}
{"paper_id": 252070859, "title": "Diffusion Models: A Comprehensive Survey of Methods and Applications", "author_names": ["Ling Yang", "Zhilong Zhang", "Shenda Hong", "Runsheng Xu", "Yue Zhao", "Yingxia Shao", "Wentao Zhang", "Ming-Hsuan Yang", "Bin Cui"], "venue": "ACM Computing Surveys", "abstract": "Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy", "year": 2022, "publicationdate": "2022-09-02", "externalids": {"DOI": "10.1145/3626235"}, "doi_lower": "10.1145/3626235"}
{"paper_id": 252090040, "title": "A Survey on Generative Diffusion Model", "author_names": ["Hanqun Cao", "Cheng Tan", "Zhangyang Gao", "Yilun Xu", "Guangyong Chen", "P. Heng", "Stan Z. Li"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2209.02646"}, "doi_lower": "10.48550/arxiv.2209.02646"}
{"paper_id": 252918532, "title": "Efficient Diffusion Models for Vision: A Survey", "author_names": ["A. Ulhaq", "Naveed Akhtar", "Ganna Pogrebna"], "venue": "arXiv.org", "abstract": "Diffusion Models (DMs) have demonstrated state-of-the-art performance in content generation without requiring adversarial training. These models are trained using a two-step process. First, a forward - diffusion - process gradually adds noise to a datum (usually an image). Then, a backward - reverse diffusion - process gradually removes the noise to turn it into a sample of the target distribution being modelled. DMs are inspired by non-equilibrium thermodynamics and have inherent high computational complexity. Due to the frequent function evaluations and gradient calculations in high-dimensional spaces, these models incur considerable computational overhead during both training and inference stages. This can not only preclude the democratization of diffusion-based modelling, but also hinder the adaption of diffusion models in real-life applications. Not to mention, the efficiency of computational models is fast becoming a significant concern due to excessive energy consumption and environmental scares. These factors have led to multiple contributions in the literature that focus on devising computationally efficient DMs. In this review, we present the most recent advances in diffusion models for vision, specifically focusing on the important design aspects that affect the computational efficiency of DMs. In particular, we emphasize the recently proposed design choices that have led to more efficient DMs. Unlike the other recent reviews, which discuss diffusion models from a broad perspective, this survey is aimed at pushing this research direction forward by highlighting the design strategies in the literature that are resulting in practicable models for the broader research community. We also provide a future outlook of diffusion models in vision from their computational efficiency viewpoint.", "year": 2022, "publicationdate": "2022-10-07", "externalids": {"DOI": "10.48550/arXiv.2210.09292"}, "doi_lower": "10.48550/arxiv.2210.09292"}
{"paper_id": 268610677, "title": "A Survey of Text-to-Image Diffusion Models in Generative AI", "author_names": ["Siddharth Kandwal", "V. Nehra"], "venue": "Confluence", "abstract": "From dreamscapes to photorealistic portraits, text-to-image generation pushes the boundaries of AI creativity. This survey navigates diverse techniques, such as GANs, VAEs, and Diffusion models, uncovering their potential for transforming textual descriptions into captivating visuals. These models have significantly advanced the field, but they are not without their limitations. One notable issue is data bias, which could potentially lead to a deficiency in variety and cultural awareness in the produced visuals. Furthermore, recognising the significance of mitigating data bias in generative models, this report offers insights and strategies to address this pressing issue. It explores approaches that leverage inclusive datasets, fairness-aware training techniques, and ethical considerations. These methods aim to bridge the gap between the technological advancements in image generation and the imperative need for inclusivity and cultural sensitivity.", "year": 2024, "publicationdate": "2024-01-18", "externalids": {"DOI": "10.1109/Confluence60223.2024.10463372"}, "doi_lower": "10.1109/confluence60223.2024.10463372"}
{"paper_id": 261214460, "title": "A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions", "author_names": ["Tianyi Zhang", "Zheng Wang", "Jing Huang", "Mohiuddin Muhammad Tasnim", "Wei Shi"], "venue": "arXiv.org", "abstract": "Recently, there has been significant progress in the development of large models. Following the success of ChatGPT, numerous language models have been introduced, demonstrating remarkable performance. Similar advancements have also been observed in image generation models, such as Google's Imagen model, OpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive capabilities in generating images. However, similar to large language models, these models still encounter unresolved challenges. Fortunately, the availability of open-source stable diffusion models and their underlying mathematical principles has enabled the academic community to extensively analyze the performance of current image generation models and make improvements based on this stable diffusion framework. This survey aims to examine the existing issues and the current solutions pertaining to image generation models.", "year": 2023, "publicationdate": "2023-08-25", "externalids": {"DOI": "10.48550/arXiv.2308.13142"}, "doi_lower": "10.48550/arxiv.2308.13142"}
{"paper_id": 263835355, "title": "State of the Art on Diffusion Models for Visual Computing", "author_names": ["Ryan Po", "Wang Yifan", "Vladislav Golyanik", "Kfir Aberman", "J. Barron", "Amit Bermano", "Eric R. Chan", "Tali Dekel", "Aleksander Holynski", "Angjoo Kanazawa", "C. K. Liu", "Lingjie Liu", "B. Mildenhall", "M. Nießner", "Bjorn Ommer", "C. Theobalt", "Peter Wonka", "Gordon Wetzstein"], "venue": "Computer graphics forum (Print)", "abstract": "The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion‐based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state‐of‐the‐art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Moreover, we give a comprehensive overview of the rapidly growing literature on diffusion‐based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Finally, we discuss available datasets, metrics, open challenges, and social implications. This STAR provides an intuitive starting point to explore this exciting topic for researchers, artists, and practitioners alike.", "year": 2023, "publicationdate": "2023-10-11", "externalids": {"DOI": "10.1111/cgf.15063"}, "doi_lower": "10.1111/cgf.15063"}
{"paper_id": 259501520, "title": "A Comprehensive Survey on Generative Diffusion Models for Structured Data", "author_names": ["Heejoon Koo", "To Eun Kim"], "venue": "", "abstract": "In recent years, generative diffusion models have achieved a rapid paradigm shift in deep generative models by showing groundbreaking performance across various applications. Meanwhile, structured data, encompassing tabular and time series data, has been received comparatively limited attention from the deep learning research community, despite its omnipresence and extensive applications. Thus, there is still a lack of literature and its reviews on structured data modelling via diffusion models, compared to other data modalities such as visual and textual data. To address this gap, we present a comprehensive review of recently proposed diffusion models in the field of structured data. First, this survey provides a concise overview of the score-based diffusion model theory, subsequently proceeding to the technical descriptions of the majority of pioneering works that used structured data in both data-driven general tasks and domain-specific applications. Thereafter, we analyse and discuss the limitations and challenges shown in existing works and suggest potential research directions. We hope this review serves as a catalyst for the research community, promoting developments in generative diffusion models for structured data.", "year": 2023, "publicationdate": "2023-06-07", "externalids": {}, "doi_lower": null}
{"paper_id": 265039918, "title": "A Survey on Generative Diffusion Models", "author_names": ["Hanqun Cao", "Cheng Tan", "Zhangyang Gao", "Yilun Xu", "Guangyong Chen", "P. Heng", "Stan Z. Li"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "abstract": "Deep generative models have unlocked another profound realm of human creativity. By capturing and generalizing patterns within data, we have entered the epoch of all-encompassing Artificial Intelligence for General Creativity (AIGC). Notably, diffusion models, recognized as one of the paramount generative models, materialize human ideation into tangible instances across diverse domains, encompassing imagery, text, speech, biology, and healthcare. To provide advanced and comprehensive insights into diffusion, this survey comprehensively elucidates its developmental trajectory and future directions from three distinct angles: the fundamental formulation of diffusion, algorithmic enhancements, and the manifold applications of diffusion. Each layer is meticulously explored to offer a profound comprehension of its evolution. Structured and summarized approaches are presented here.", "year": 2022, "publicationdate": "2022-09-06", "externalids": {"DOI": "10.1109/TKDE.2024.3361474"}, "doi_lower": "10.1109/tkde.2024.3361474"}
{"paper_id": 258179432, "title": "MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing", "author_names": ["Ming Cao", "Xintao Wang", "Zhongang Qi", "Ying Shan", "Xiaohu Qie", "Yinqiang Zheng"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Despite the success in large-scale text-to-image generation and text-conditioned image editing, existing methods still struggle to produce consistent generation and editing results. For example, generation approaches usually fail to synthesize multiple images of the same objects/characters but with different views or poses. Meanwhile, existing editing methods either fail to achieve effective complex nonrigid editing while maintaining the overall textures and identity, or require time-consuming fine-tuning to capture the image-specific appearance. In this paper, we develop MasaCtrl, a tuning-free method to achieve consistent image generation and complex non-rigid image editing simultaneously. Specifically, MasaCtrl converts existing self-attention in diffusion models into mutual self-attention, so that it can query correlated local contents and textures from source images for consistency. To further alleviate the query confusion between foreground and background, we propose a mask-guided mutual self-attention strategy, where the mask can be easily extracted from the cross-attention maps. Extensive experiments show that the proposed MasaCtrl can produce impressive results in both consistent image generation and complex non-rigid real image editing.", "year": 2023, "publicationdate": "2023-04-17", "externalids": {"DOI": "10.1109/ICCV51070.2023.02062"}, "doi_lower": "10.1109/iccv51070.2023.02062"}
{"paper_id": 247763065, "title": "Denoising Likelihood Score Matching for Conditional Score-based Data Generation", "author_names": ["Chen-Hao Chao", "Wei-Fang Sun", "Bo Wun Cheng", "Yi-Chen Lo", "Chia-Che Chang", "Yu-Lun Liu", "Yu-Lin Chang", "Chia-Ping Chen", "Chun-Yi Lee"], "venue": "International Conference on Learning Representations", "abstract": "Many existing conditional score-based data generation methods utilize Bayes' theorem to decompose the gradients of a log posterior density into a mixture of scores. These methods facilitate the training procedure of conditional score models, as a mixture of scores can be separately estimated using a score model and a classifier. However, our analysis indicates that the training objectives for the classifier in these methods may lead to a serious score mismatch issue, which corresponds to the situation that the estimated scores deviate from the true ones. Such an issue causes the samples to be misled by the deviated scores during the diffusion process, resulting in a degraded sampling quality. To resolve it, we formulate a novel training objective, called Denoising Likelihood Score Matching (DLSM) loss, for the classifier to match the gradients of the true log likelihood density. Our experimental evidence shows that the proposed method outperforms the previous methods on both Cifar-10 and Cifar-100 benchmarks noticeably in terms of several key evaluation metrics. We thus conclude that, by adopting DLSM, the conditional scores can be accurately modeled, and the effect of the score mismatch issue is alleviated.", "year": 2022, "publicationdate": "2022-03-27", "externalids": {"DOI": "10.48550/arXiv.2203.14206"}, "doi_lower": "10.48550/arxiv.2203.14206"}
{"paper_id": 249240415, "title": "Elucidating the Design Space of Diffusion-Based Generative Models", "author_names": ["Tero Karras", "M. Aittala", "Timo Aila", "S. Laine"], "venue": "Neural Information Processing Systems", "abstract": "We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.", "year": 2022, "publicationdate": "2022-06-01", "externalids": {"DOI": "10.48550/arXiv.2206.00364"}, "doi_lower": "10.48550/arxiv.2206.00364"}
{"paper_id": 249282317, "title": "DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps", "author_names": ["Cheng Lu", "Yuhao Zhou", "Fan Bao", "Jianfei Chen", "Chongxuan Li", "Jun Zhu"], "venue": "Neural Information Processing Systems", "abstract": "Diffusion probabilistic models (DPMs) are emerging powerful generative models. Despite their high-quality generation performance, DPMs still suffer from their slow sampling as they generally need hundreds or thousands of sequential function evaluations (steps) of large neural networks to draw a sample. Sampling from DPMs can be viewed alternatively as solving the corresponding diffusion ordinary differential equations (ODEs). In this work, we propose an exact formulation of the solution of diffusion ODEs. The formulation analytically computes the linear part of the solution, rather than leaving all terms to black-box ODE solvers as adopted in previous works. By applying change-of-variable, the solution can be equivalently simplified to an exponentially weighted integral of the neural network. Based on our formulation, we propose DPM-Solver, a fast dedicated high-order solver for diffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for both discrete-time and continuous-time DPMs without any further training. Experimental results show that DPM-Solver can generate high-quality samples in only 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in 10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10 dataset, and a $4\\sim 16\\times$ speedup compared with previous state-of-the-art training-free samplers on various datasets.", "year": 2022, "publicationdate": "2022-06-02", "externalids": {"DOI": "10.48550/arXiv.2206.00927"}, "doi_lower": "10.48550/arxiv.2206.00927"}
{"paper_id": 277918729, "title": "Linear Multistep Solver Distillation for Fast Sampling of Diffusion Models", "author_names": ["Yuchen Liang", "Xiangzhong Fang", "Hanting Chen", "Yunhe Wang"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2025, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 244714856, "title": "Vector Quantized Diffusion Model for Text-to-Image Synthesis", "author_names": ["Shuyang Gu", "Dong Chen", "Jianmin Bao", "Fang Wen", "Bo Zhang", "Dongdong Chen", "Lu Yuan", "B. Guo"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a better image quality. The code and models are available at https://github.com/cientgu/VQ-Diffusion.", "year": 2021, "publicationdate": "2021-11-29", "externalids": {"DOI": "10.1109/CVPR52688.2022.01043"}, "doi_lower": "10.1109/cvpr52688.2022.01043"}
{"paper_id": 259341735, "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis", "author_names": ["Dustin Podell", "Zion English", "Kyle Lacey", "A. Blattmann", "Tim Dockhorn", "Jonas Muller", "Joe Penna", "Robin Rombach"], "venue": "International Conference on Learning Representations", "abstract": "We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models", "year": 2023, "publicationdate": "2023-07-04", "externalids": {}, "doi_lower": null}
{"paper_id": 263334265, "title": "PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis", "author_names": ["Junsong Chen", "Jincheng Yu", "Chongjian Ge", "Lewei Yao", "Enze Xie", "Yue Wu", "Zhongdao Wang", "James T. Kwok", "Ping Luo", "Huchuan Lu", "Zhenguo Li"], "venue": "International Conference on Learning Representations", "abstract": "The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PIXART-$\\alpha$, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024px resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that separately optimize pixel dependency, text-image alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PIXART-$\\alpha$'s training speed markedly surpasses existing large-scale T2I models, e.g., PIXART-$\\alpha$ only takes 10.8% of Stable Diffusion v1.5's training time (675 vs. 6,250 A100 GPU days), saving nearly \\$300,000 (\\$26,000 vs. \\$320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%. Extensive experiments demonstrate that PIXART-$\\alpha$ excels in image quality, artistry, and semantic control. We hope PIXART-$\\alpha$ will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.", "year": 2023, "publicationdate": "2023-09-30", "externalids": {"DOI": "10.48550/arXiv.2310.00426"}, "doi_lower": "10.48550/arxiv.2310.00426"}
{"paper_id": 263151865, "title": "Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack", "author_names": ["Xiaoliang Dai", "Ji Hou", "Chih-Yao Ma", "Sam S. Tsai", "Jialiang Wang", "Rui Wang", "Peizhao Zhang", "Simon Vandenhende", "Xiaofang Wang", "Abhimanyu Dubey", "Matthew Yu", "Abhishek Kadian", "Filip Radenovic", "D. Mahajan", "Kunpeng Li", "Yue Zhao", "Vladan Petrovic", "Mitesh Kumar Singh", "Simran Motwani", "Yiqian Wen", "Yi-Zhe Song", "Roshan Sumbaly", "Vignesh Ramanathan", "Zijian He", "Péter Vajda", "Devi Parikh"], "venue": "arXiv.org", "abstract": "Training text-to-image models with web scale image-text pairs enables the generation of a wide range of visual concepts from text. However, these pre-trained models often face challenges when it comes to generating highly aesthetic images. This creates the need for aesthetic alignment post pre-training. In this paper, we propose quality-tuning to effectively guide a pre-trained model to exclusively generate highly visually appealing images, while maintaining generality across visual concepts. Our key insight is that supervised fine-tuning with a set of surprisingly small but extremely visually appealing images can significantly improve the generation quality. We pre-train a latent diffusion model on $1.1$ billion image-text pairs and fine-tune it with only a few thousand carefully selected high-quality images. The resulting model, Emu, achieves a win rate of $82.9\\%$ compared with its pre-trained only counterpart. Compared to the state-of-the-art SDXLv1.0, Emu is preferred $68.4\\%$ and $71.3\\%$ of the time on visual appeal on the standard PartiPrompts and our Open User Input benchmark based on the real-world usage of text-to-image models. In addition, we show that quality-tuning is a generic approach that is also effective for other architectures, including pixel diffusion and masked generative transformer models.", "year": 2023, "publicationdate": "2023-09-27", "externalids": {"DOI": "10.48550/arXiv.2309.15807"}, "doi_lower": "10.48550/arxiv.2309.15807"}
{"paper_id": 258999204, "title": "StyleDrop: Text-to-Image Generation in Any Style", "author_names": ["Kihyuk Sohn", "Nataniel Ruiz", "Kimin Lee", "Daniel Castro Chin", "Irina Blok", "Huiwen Chang", "Jarred Barber", "Lu Jiang", "Glenn Entis", "Yuanzhen Li", "Yuan Hao", "Irfan Essa", "Michael Rubinstein", "Dilip Krishnan"], "venue": "arXiv.org", "abstract": "Pre-trained large text-to-image models synthesize impressive images with an appropriate use of text prompts. However, ambiguities inherent in natural language and out-of-distribution effects make it hard to synthesize image styles, that leverage a specific design pattern, texture or material. In this paper, we introduce StyleDrop, a method that enables the synthesis of images that faithfully follow a specific style using a text-to-image model. The proposed method is extremely versatile and captures nuances and details of a user-provided style, such as color schemes, shading, design patterns, and local and global effects. It efficiently learns a new style by fine-tuning very few trainable parameters (less than $1\\%$ of total model parameters) and improving the quality via iterative training with either human or automated feedback. Better yet, StyleDrop is able to deliver impressive results even when the user supplies only a single image that specifies the desired style. An extensive study shows that, for the task of style tuning text-to-image models, StyleDrop implemented on Muse convincingly outperforms other methods, including DreamBooth and textual inversion on Imagen or Stable Diffusion. More results are available at our project website: https://styledrop.github.io", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.48550/arXiv.2306.00983"}, "doi_lower": "10.48550/arxiv.2306.00983"}
{"paper_id": 253254800, "title": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers", "author_names": ["Y. Balaji", "Seungjun Nah", "Xun Huang", "Arash Vahdat", "Jiaming Song", "Qinsheng Zhang", "Karsten Kreis", "M. Aittala", "Timo Aila", "S. Laine", "Bryan Catanzaro", "Tero Karras", "Ming-Yu Liu"], "venue": "arXiv.org", "abstract": "Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I's\"paint-with-words\"capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at https://deepimagination.cc/eDiff-I/", "year": 2022, "publicationdate": "2022-11-02", "externalids": {"DOI": "10.48550/arXiv.2211.01324"}, "doi_lower": "10.48550/arxiv.2211.01324"}
{"paper_id": 258108187, "title": "Expressive Text-to-Image Generation with Rich Text", "author_names": ["Songwei Ge", "Taesung Park", "Jun-Yan Zhu", "Jia-Bin Huang"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Plain text has become a prevalent interface for text-to-image synthesis. However, its limited customization options hinder users from accurately describing desired outputs. For example, plain text makes it hard to specify continuous quantities, such as the precise RGB color value or importance of each word. Furthermore, creating detailed text prompts for complex scenes is tedious for humans to write and challenging for text encoders to interpret. To address these challenges, we propose using a rich-text editor supporting formats such as font style, size, color, and footnote. We extract each word’s attributes from rich text to enable local style control, explicit token reweighting, precise color rendering, and detailed region synthesis. We achieve these capabilities through a region-based diffusion process. We first obtain each word’s region based on attention maps of a diffusion process using plain text. For each region, we enforce its text attributes by creating region-specific detailed prompts and applying region-specific guidance, and maintain its fidelity against plain-text generation through region-based injections. We present various examples of image generation from rich text and demonstrate that our method outperforms strong baselines with quantitative evaluations.", "year": 2023, "publicationdate": "2023-04-13", "externalids": {"DOI": "10.1109/ICCV51070.2023.00694"}, "doi_lower": "10.1109/iccv51070.2023.00694"}
{"paper_id": 255942528, "title": "GLIGEN: Open-Set Grounded Text-to-Image Generation", "author_names": ["Yuheng Li", "Haotian Liu", "Qingyang Wu", "Fangzhou Mu", "Jianwei Yang", "Jianfeng Gao", "Chunyuan Li", "Yong Jae Lee"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms existing supervised layout-to-image baselines by a large margin.", "year": 2023, "publicationdate": "2023-01-17", "externalids": {"DOI": "10.1109/CVPR52729.2023.02156"}, "doi_lower": "10.1109/cvpr52729.2023.02156"}
{"paper_id": 247628171, "title": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors", "author_names": ["Oran Gafni", "Adam Polyak", "Oron Ashual", "Shelly Sheynin", "Devi Parikh", "Yaniv Taigman"], "venue": "European Conference on Computer Vision", "abstract": "Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of 512x512 pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: (i) Scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote.", "year": 2022, "publicationdate": "2022-03-24", "externalids": {"DOI": "10.48550/arXiv.2203.13131"}, "doi_lower": "10.48550/arxiv.2203.13131"}
{"paper_id": 254018089, "title": "SpaText: Spatio-Textual Representation for Controllable Image Generation", "author_names": ["Omri Avrahami", "Thomas Hayes", "Oran Gafni", "Sonal Gupta", "Yaniv Taigman", "Devi Parikh", "D. Lischinski", "Ohad Fried", "Xiaoyue Yin"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText — a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition, we show how to extend the classifier-free guidance method in diffusion models to the multi-conditional case and present an alternative accelerated inference algorithm. Finally, we offer several automatic evaluation metrics and use them, in addition to FID scores and a user study, to evaluate our method and show that it achieves state-of-the-art results on image generation with free-form textual scene control.", "year": 2022, "publicationdate": "2022-11-25", "externalids": {"DOI": "10.1109/CVPR52729.2023.01762"}, "doi_lower": "10.1109/cvpr52729.2023.01762"}
{"paper_id": 256827727, "title": "Adding Conditional Control to Text-to-Image Diffusion Models", "author_names": ["Lvmin Zhang", "Anyi Rao", "Maneesh Agrawala"], "venue": "IEEE International Conference on Computer Vision", "abstract": "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.", "year": 2023, "publicationdate": "2023-02-10", "externalids": {"DOI": "10.1109/ICCV51070.2023.00355"}, "doi_lower": "10.1109/iccv51070.2023.00355"}
{"paper_id": 258888112, "title": "Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models", "author_names": ["Shihao Zhao", "Dongdong Chen", "Yen-Chun Chen", "Jianmin Bao", "Shaozhe Hao", "Lu Yuan", "K. K. Wong"], "venue": "Neural Information Processing Systems", "abstract": "Text-to-Image diffusion models have made tremendous progress over the past two years, enabling the generation of highly realistic images based on open-domain text descriptions. However, despite their success, text descriptions often struggle to adequately convey detailed controls, even when composed of long and complex texts. Moreover, recent studies have also shown that these models face challenges in understanding such complex texts and generating the corresponding images. Therefore, there is a growing need to enable more control modes beyond text description. In this paper, we introduce Uni-ControlNet, a unified framework that allows for the simultaneous utilization of different local controls (e.g., edge maps, depth map, segmentation masks) and global controls (e.g., CLIP image embeddings) in a flexible and composable manner within one single model. Unlike existing methods, Uni-ControlNet only requires the fine-tuning of two additional adapters upon frozen pre-trained text-to-image diffusion models, eliminating the huge cost of training from scratch. Moreover, thanks to some dedicated adapter designs, Uni-ControlNet only necessitates a constant number (i.e., 2) of adapters, regardless of the number of local or global controls used. This not only reduces the fine-tuning costs and model size, making it more suitable for real-world deployment, but also facilitate composability of different conditions. Through both quantitative and qualitative comparisons, Uni-ControlNet demonstrates its superiority over existing methods in terms of controllability, generation quality and composability. Code is available at \\url{https://github.com/ShihaoZhaoZSH/Uni-ControlNet}.", "year": 2023, "publicationdate": "2023-05-25", "externalids": {"DOI": "10.48550/arXiv.2305.16322"}, "doi_lower": "10.48550/arxiv.2305.16322"}
{"paper_id": 258762776, "title": "UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild", "author_names": ["Can Qin", "Shu Zhang", "Ning Yu", "Yihao Feng", "Xinyi Yang", "Yingbo Zhou", "Haiquan Wang", "Juan Carlos Niebles", "Caiming Xiong", "S. Savarese", "Stefano Ermon", "Yun Fu", "Ran Xu"], "venue": "Neural Information Processing Systems", "abstract": "Achieving machine autonomy and human control often represent divergent objectives in the design of interactive AI systems. Visual generative foundation models such as Stable Diffusion show promise in navigating these goals, especially when prompted with arbitrary languages. However, they often fall short in generating images with spatial, structural, or geometric controls. The integration of such controls, which can accommodate various visual conditions in a single unified model, remains an unaddressed challenge. In response, we introduce UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image (C2I) tasks within a singular framework, while still allowing for arbitrary language prompts. UniControl enables pixel-level-precise image generation, where visual conditions primarily influence the generated structures and language prompts guide the style and context. To equip UniControl with the capacity to handle diverse visual conditions, we augment pretrained text-to-image diffusion models and introduce a task-aware HyperNet to modulate the diffusion models, enabling the adaptation to different C2I tasks simultaneously. Trained on nine unique C2I tasks, UniControl demonstrates impressive zero-shot generation abilities with unseen visual conditions. Experimental results show that UniControl often surpasses the performance of single-task-controlled methods of comparable model sizes. This control versatility positions UniControl as a significant advancement in the realm of controllable visual generation.", "year": 2023, "publicationdate": "2023-05-18", "externalids": {"DOI": "10.48550/arXiv.2305.11147"}, "doi_lower": "10.48550/arxiv.2305.11147"}
{"paper_id": 257038979, "title": "Composer: Creative and Controllable Image Synthesis with Composable Conditions", "author_names": ["Lianghua Huang", "Di Chen", "Yu Liu", "Yujun Shen", "Deli Zhao", "Jingren Zhou"], "venue": "International Conference on Machine Learning", "abstract": "Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability. This work offers a new generation paradigm that allows flexible control of the output image, such as spatial layout and palette, while maintaining the synthesis quality and model creativity. With compositionality as the core idea, we first decompose an image into representative factors, and then train a diffusion model with all these factors as the conditions to recompose the input. At the inference stage, the rich intermediate representations work as composable elements, leading to a huge design space (i.e., exponentially proportional to the number of decomposed factors) for customizable content creation. It is noteworthy that our approach, which we call Composer, supports various levels of conditions, such as text description as the global information, depth map and sketch as the local guidance, color histogram for low-level details, etc. Besides improving controllability, we confirm that Composer serves as a general framework and facilitates a wide range of classical generative tasks without retraining. Code and models will be made available.", "year": 2023, "publicationdate": "2023-02-20", "externalids": {"DOI": "10.48550/arXiv.2302.09778"}, "doi_lower": "10.48550/arxiv.2302.09778"}
{"paper_id": 256900833, "title": "T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models", "author_names": ["Chong Mou", "Xintao Wang", "Liangbin Xie", "Jing Zhang", "Zhongang Qi", "Ying Shan", "Xiaohu Qie"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., structure and color) is needed. In this paper, we aim to ``dig out\" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn low-cost T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and generalization ability. Extensive experiments demonstrate that our T2I-Adapter has promising generation quality and a wide range of applications. Our code is available at https://github.com/TencentARC/T2I-Adapter.", "year": 2023, "publicationdate": "2023-02-16", "externalids": {"DOI": "10.48550/arXiv.2302.08453"}, "doi_lower": "10.48550/arxiv.2302.08453"}
{"paper_id": 258887939, "title": "Prompt-Free Diffusion: Taking “Text” Out of Text-to-Image Diffusion Models", "author_names": ["Xingqian Xu", "Jiayi Guo", "Zhangyang Wang", "Gao Huang", "Irfan Essa", "Humphrey Shi"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Text-to-image (T2I) research has grown explosively in the past year, owing to the large-scale pre-trained diffusion models and many emerging personalization and editing approaches. Yet, one pain point persists: the text prompt engineering, and searching high-quality text prompts for customized results is more art than science. Moreover, as commonly argued: “an image is worth a thousand words” - the attempt to describe a desired image with texts often ends up being ambiguous and cannot comprehensively cover delicate visual details, hence necessitating more additional controls from the visual domain. In this paper, we take a bold step forward: taking “Text” out of a pretrained T2I diffusion model, to reduce the burdensome prompt engineering efforts for users. Our proposed frame-work, Prompt-Free Diffusion, relies on only visual inputs to generate new images: it takes a reference image as “context”, an optional image structural conditioning, and an initial noise, with absolutely no text prompt. The core architecture behind the scene is Semantic Context Encoder (SeeCoder), substituting the commonly used CLIP-based or LLM-based text encoder. The reusability of SeeCoder also makes it a convenient drop-in component: one can also pre-train a SeeCoder in one T2I model and reuse it for another. Through extensive experiments, Prompt-Free Diffusion is experimentally found to (i) outperform prior exemplar-based image synthesis approaches; (ii) perform on par with state-of-the-art T2I models using prompts following the best practice; and (iii) be naturally extensible to other downstream applications such as anime figure generation and virtual try-on, with promising quality. Our code and models will be open-sourced.", "year": 2023, "publicationdate": "2023-05-25", "externalids": {"DOI": "10.1109/CVPR52733.2024.00829"}, "doi_lower": "10.1109/cvpr52733.2024.00829"}
{"paper_id": 251253049, "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", "author_names": ["Rinon Gal", "Yuval Alaluf", "Y. Atzmon", "Or Patashnik", "Amit H. Bermano", "Gal Chechik", "D. Cohen-Or"], "venue": "International Conference on Learning Representations", "abstract": "Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new\"words\"in the embedding space of a frozen text-to-image model. These\"words\"can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io", "year": 2022, "publicationdate": "2022-08-02", "externalids": {"DOI": "10.48550/arXiv.2208.01618"}, "doi_lower": "10.48550/arxiv.2208.01618"}
{"paper_id": 251800180, "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation", "author_names": ["Nataniel Ruiz", "Yuanzhen Li", "Varun Jampani", "Y. Pritch", "Michael Rubinstein", "Kfir Aberman"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for “personalization” of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/", "year": 2022, "publicationdate": "2022-08-25", "externalids": {"DOI": "10.1109/CVPR52729.2023.02155"}, "doi_lower": "10.1109/cvpr52729.2023.02155"}
{"paper_id": 258960192, "title": "Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models", "author_names": ["Yuchao Gu", "Xintao Wang", "Jay Zhangjie Wu", "Yujun Shi", "Yunpeng Chen", "Zihan Fan", "Wuyou Xiao", "Rui Zhao", "Shuning Chang", "Wei Wu", "Yixiao Ge", "Ying Shan", "Mike Zheng Shou"], "venue": "Neural Information Processing Systems", "abstract": "Public large-scale text-to-image diffusion models, such as Stable Diffusion, have gained significant attention from the community. These models can be easily customized for new concepts using low-rank adaptations (LoRAs). However, the utilization of multiple concept LoRAs to jointly support multiple customized concepts presents a challenge. We refer to this scenario as decentralized multi-concept customization, which involves single-client concept tuning and center-node concept fusion. In this paper, we propose a new framework called Mix-of-Show that addresses the challenges of decentralized multi-concept customization, including concept conflicts resulting from existing single-client LoRA tuning and identity loss during model fusion. Mix-of-Show adopts an embedding-decomposed LoRA (ED-LoRA) for single-client tuning and gradient fusion for the center node to preserve the in-domain essence of single concepts and support theoretically limitless concept fusion. Additionally, we introduce regionally controllable sampling, which extends spatially controllable sampling (e.g., ControlNet and T2I-Adaptor) to address attribute binding and missing object problems in multi-concept sampling. Extensive experiments demonstrate that Mix-of-Show is capable of composing multiple customized concepts with high fidelity, including characters, objects, and scenes.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {"DOI": "10.48550/arXiv.2305.18292"}, "doi_lower": "10.48550/arxiv.2305.18292"}
{"paper_id": 259847576, "title": "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models", "author_names": ["Nataniel Ruiz", "Yuanzhen Li", "Varun Jampani", "Wei Wei", "Tingbo Hou", "Y. Pritch", "N. Wadhwa", "Michael Rubinstein", "Kfir Aberman"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth—a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual Inversion, using as few as one reference image, with the same quality and style diversity as DreamBooth. Also our method yields a model that is 10,000x smaller than a normal DreamBooth model.", "year": 2023, "publicationdate": "2023-07-13", "externalids": {"DOI": "10.1109/CVPR52733.2024.00624"}, "doi_lower": "10.1109/cvpr52733.2024.00624"}
{"paper_id": 268512816, "title": "OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models", "author_names": ["Zhe Kong", "Yong Zhang", "Tianyu Yang", "Tao Wang", "Kaihao Zhang", "Bizhu Wu", "Guanying Chen", "Wei Liu", "Wenhan Luo"], "venue": "European Conference on Computer Vision", "abstract": "Personalization is an important topic in text-to-image generation, especially the challenging multi-concept personalization. Current multi-concept methods are struggling with identity preservation, occlusion, and the harmony between foreground and background. In this work, we propose OMG, an occlusion-friendly personalized generation framework designed to seamlessly integrate multiple concepts within a single image. We propose a novel two-stage sampling solution. The first stage takes charge of layout generation and visual comprehension information collection for handling occlusions. The second one utilizes the acquired visual comprehension information and the designed noise blending to integrate multiple concepts while considering occlusions. We also observe that the initiation denoising timestep for noise blending is the key to identity preservation and layout. Moreover, our method can be combined with various single-concept models, such as LoRA and InstantID without additional tuning. Especially, LoRA models on civitai.com can be exploited directly. Extensive experiments demonstrate that OMG exhibits superior performance in multi-concept personalization.", "year": 2024, "publicationdate": "2024-03-16", "externalids": {"DOI": "10.48550/arXiv.2403.10983"}, "doi_lower": "10.48550/arxiv.2403.10983"}
{"paper_id": 254408780, "title": "Multi-Concept Customization of Text-to-Image Diffusion", "author_names": ["Nupur Kumari", "Bingliang Zhang", "Richard Zhang", "Eli Shechtman", "Jun-Yan Zhu"], "venue": "Computer Vision and Pattern Recognition", "abstract": "While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (~ 6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works in both qualitative and quantitative evaluations, while being memory and computationally efficient.", "year": 2022, "publicationdate": "2022-12-08", "externalids": {"DOI": "10.1109/CVPR52729.2023.00192"}, "doi_lower": "10.1109/cvpr52729.2023.00192"}
{"paper_id": 259262648, "title": "Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics", "author_names": ["Anton Voronov", "Mikhail Khoroshikh", "Artem Babenko", "Max Ryabinin"], "venue": "Neural Information Processing Systems", "abstract": "Text-to-image generation models represent the next step of evolution in image synthesis, offering a natural way to achieve flexible yet fine-grained control over the result. One emerging area of research is the fast adaptation of large text-to-image models to smaller datasets or new visual concepts. However, many efficient methods of adaptation have a long training time, which limits their practical applications, slows down experiments, and spends excessive GPU resources. In this work, we study the training dynamics of popular text-to-image personalization methods (such as Textual Inversion or DreamBooth), aiming to speed them up. We observe that most concepts are learned at early stages and do not improve in quality later, but standard training convergence metrics fail to indicate that. Instead, we propose a simple drop-in early stopping criterion that only requires computing the regular training objective on a fixed set of inputs for all training iterations. Our experiments on Stable Diffusion for 48 different concepts and three personalization methods demonstrate the competitive performance of our approach, which makes adaptation up to 8 times faster with no significant drops in quality.", "year": 2023, "publicationdate": "2023-02-09", "externalids": {}, "doi_lower": null}
{"paper_id": 257219968, "title": "ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation", "author_names": ["Yuxiang Wei", "Yabo Zhang", "Zhilong Ji", "Jinfeng Bai", "Lei Zhang", "W. Zuo"], "venue": "IEEE International Conference on Computer Vision", "abstract": "In addition to the unprecedented ability in imaginary creation, large text-to-image models are expected to take customized concepts in image generation. Existing works generally learn such concepts in an optimization-based manner, yet bringing excessive computation or memory burden. In this paper, we instead propose a learning-based encoder, which consists of a global and a local mapping networks for fast and accurate customized text-to-image generation. In specific, the global mapping network projects the hierarchical features of a given image into multiple \"new\" words in the textual word embedding space, i.e., one primary word for well-editable concept and other auxiliary words to exclude irrelevant disturbances (e.g., background). In the meantime, a local mapping network injects the encoded patch features into cross attention layers to provide omitted details, without sacrificing the editability of primary concepts. We compare our method with existing optimization-based approaches on a variety of user-defined concepts, and demonstrate that our method enables high-fidelity inversion and more robust editability with a significantly faster encoding process. Our code is publicly available at https://github.com/csyxwei/ELITE.", "year": 2023, "publicationdate": "2023-02-27", "externalids": {"DOI": "10.1109/ICCV51070.2023.01461"}, "doi_lower": "10.1109/iccv51070.2023.01461"}
{"paper_id": 257427549, "title": "Cones: Concept Neurons in Diffusion Models for Customized Generation", "author_names": ["Zhiheng Liu", "Ruili Feng", "Kai Zhu", "Yifei Zhang", "Kecheng Zheng", "Yu Liu", "Deli Zhao", "Jingren Zhou", "Yang Cao"], "venue": "International Conference on Machine Learning", "abstract": "Human brains respond to semantic features of presented stimuli with different neurons. It is then curious whether modern deep neural networks admit a similar behavior pattern. Specifically, this paper finds a small cluster of neurons in a diffusion model corresponding to a particular subject. We call those neurons the concept neurons. They can be identified by statistics of network gradients to a stimulation connected with the given subject. The concept neurons demonstrate magnetic properties in interpreting and manipulating generation results. Shutting them can directly yield the related subject contextualized in different scenes. Concatenating multiple clusters of concept neurons can vividly generate all related concepts in a single image. A few steps of further fine-tuning can enhance the multi-concept capability, which may be the first to manage to generate up to four different subjects in a single image. For large-scale applications, the concept neurons are environmentally friendly as we only need to store a sparse cluster of int index instead of dense float32 values of the parameters, which reduces storage consumption by 90\\% compared with previous subject-driven generation methods. Extensive qualitative and quantitative studies on diverse scenarios show the superiority of our method in interpreting and manipulating diffusion models.", "year": 2023, "publicationdate": "2023-03-09", "externalids": {"DOI": "10.48550/arXiv.2303.05125"}, "doi_lower": "10.48550/arxiv.2303.05125"}
{"paper_id": 257913352, "title": "Subject-driven Text-to-Image Generation via Apprenticeship Learning", "author_names": ["Wenhu Chen", "Hexiang Hu", "Yandong Li", "Nataniel Rui", "Xuhui Jia", "Ming-Wei Chang", "William W. Cohen"], "venue": "Neural Information Processing Systems", "abstract": "Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with in-context learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by apprenticeship learning, where a single apprentice model is learned from data generated by a massive number of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train a massive number of expert models, each specializing in a different subject. The apprentice model SuTI then learns to imitate the behavior of these fine-tuned experts. SuTI can generate high-quality and customized subject-specific images 20x faster than optimization-based SoTA methods. On the challenging DreamBench and DreamBench-v2, our human evaluation shows that SuTI significantly outperforms existing models like InstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen and DreamBooth, especially on the subject and text alignment aspects.", "year": 2023, "publicationdate": "2023-04-01", "externalids": {"DOI": "10.48550/arXiv.2304.00186"}, "doi_lower": "10.48550/arxiv.2304.00186"}
{"paper_id": 258041269, "title": "InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning", "author_names": ["Jing Shi", "Wei Xiong", "Zhe Lin", "H. J. Jung"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Recent advances in personalized image generation have enabled pre-trained text-to-image models to learn new concepts from specific image sets. However, these methods often necessitate extensive test-time finetuning for each new concept, leading to inefficiencies in both time and scalability. To address this challenge, we introduce Instant-Booth, an innovative approach leveraging existing text-to-image models for instantaneous text-guided image personalization, eliminating the need for test-time finetuning. This efficiency is achieved through two primary innovations. Firstly, we utilize an image encoder that transforms input images into a global embedding to grasp the general concept. Secondly, we integrate new adapter layers into the pre-trained model, enhancing its ability to capture intricate identity details while maintaining language coherence. Significantly, our model is trained exclusively on textimage pairs, without reliance on concept-specific paired images. When benchmarked against existing finetuning-based personalization techniques like DreamBooth and TextualInversion, InstantBooth not only shows comparable proficiency in aligning language with image, maintaining image quality, and preserving the identity but also boasts a 100-fold increase in generation speed. Project Page: https://jshi31.github.io/InstantBooth/", "year": 2023, "publicationdate": "2023-04-06", "externalids": {"DOI": "10.1109/CVPR52733.2024.00816"}, "doi_lower": "10.1109/cvpr52733.2024.00816"}
{"paper_id": 258436985, "title": "Key-Locked Rank One Editing for Text-to-Image Personalization", "author_names": ["Yoad Tewel", "Rinon Gal", "Gal Chechik", "Y. Atzmon"], "venue": "International Conference on Computer Graphics and Interactive Techniques", "abstract": "Text-to-image models (T2I) offer a new level of flexibility by allowing users to guide the creative process through natural language. However, personalizing these models to align with user-provided visual concepts remains a challenging problem. The task of T2I personalization poses multiple hard challenges, such as maintaining high visual fidelity while allowing creative control, combining multiple personalized concepts in a single image, and keeping a small model size. We present Perfusion, a T2I personalization method that addresses these challenges using dynamic rank-1 updates to the underlying T2I model. Perfusion avoids overfitting by introducing a new mechanism that “locks” new concepts’ cross-attention Keys to their superordinate category. Additionally, we develop a gated rank-1 approach that enables us to control the influence of a learned concept during inference time and to combine multiple concepts. This allows runtime efficient balancing of visual-fidelity and textual-alignment with a single 100KB trained model. Importantly, it can span different operating points across the Pareto front without additional training. We compare our approach to strong baselines and demonstrate its qualitative and quantitative strengths.", "year": 2023, "publicationdate": "2023-05-02", "externalids": {"DOI": "10.1145/3588432.3591506"}, "doi_lower": "10.1145/3588432.3591506"}
{"paper_id": 258546711, "title": "DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven Text-to-Image Generation", "author_names": ["Hong Chen", "Yipeng Zhang", "Xin Wang", "Xuguang Duan", "Yuwei Zhou", "Wenwu Zhu"], "venue": "International Conference on Learning Representations", "abstract": "Subject-driven text-to-image generation aims to generate customized images of the given subject based on the text descriptions, which has drawn increasing attention. Existing methods mainly resort to finetuning a pretrained generative model, where the identity-relevant information (e.g., the boy) and the identity-irrelevant information (e.g., the background or the pose of the boy) are entangled in the latent embedding space. However, the highly entangled latent embedding may lead to the failure of subject-driven text-to-image generation as follows: (i) the identity-irrelevant information hidden in the entangled embedding may dominate the generation process, resulting in the generated images heavily dependent on the irrelevant information while ignoring the given text descriptions; (ii) the identity-relevant information carried in the entangled embedding can not be appropriately preserved, resulting in identity change of the subject in the generated images. To tackle the problems, we propose DisenBooth, an identity-preserving disentangled tuning framework for subject-driven text-to-image generation. Specifically, DisenBooth finetunes the pretrained diffusion model in the denoising process. Different from previous works that utilize an entangled embedding to denoise each image, DisenBooth instead utilizes disentangled embeddings to respectively preserve the subject identity and capture the identity-irrelevant information. We further design the novel weak denoising and contrastive embedding auxiliary tuning objectives to achieve the disentanglement. Extensive experiments show that our proposed DisenBooth framework outperforms baseline models for subject-driven text-to-image generation with the identity-preserved embedding. Additionally, by combining the identity-preserved embedding and identity-irrelevant embedding, DisenBooth demonstrates more generation flexibility and controllability", "year": 2023, "publicationdate": "2023-05-05", "externalids": {"DOI": "10.48550/arXiv.2305.03374"}, "doi_lower": "10.48550/arxiv.2305.03374"}
{"paper_id": 258960099, "title": "Photoswap: Personalized Subject Swapping in Images", "author_names": ["Jing Gu", "Yilin Wang", "Nanxuan Zhao", "Tsu-Jui Fu", "Wei Xiong", "Qing Liu", "Zhifei Zhang", "He Zhang", "Jianming Zhang", "Hyun-Sun Jung", "Xin Wang"], "venue": "Neural Information Processing Systems", "abstract": "In an era where images and visual content dominate our digital landscape, the ability to manipulate and personalize these images has become a necessity. Envision seamlessly substituting a tabby cat lounging on a sunlit window sill in a photograph with your own playful puppy, all while preserving the original charm and composition of the image. We present Photoswap, a novel approach that enables this immersive image editing experience through personalized subject swapping in existing images. Photoswap first learns the visual concept of the subject from reference images and then swaps it into the target image using pre-trained diffusion models in a training-free manner. We establish that a well-conceptualized visual subject can be seamlessly transferred to any image with appropriate self-attention and cross-attention manipulation, maintaining the pose of the swapped subject and the overall coherence of the image. Comprehensive experiments underscore the efficacy and controllability of Photoswap in personalized subject swapping. Furthermore, Photoswap significantly outperforms baseline methods in human ratings across subject swapping, background preservation, and overall quality, revealing its vast application potential, from entertainment to professional editing.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {"DOI": "10.48550/arXiv.2305.18286"}, "doi_lower": "10.48550/arxiv.2305.18286"}
{"paper_id": 264815845, "title": "CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models", "author_names": ["Ziyang Yuan", "Mingdeng Cao", "Xintao Wang", "Zhongang Qi", "Chun Yuan", "Ying Shan"], "venue": "arXiv.org", "abstract": "Incorporating a customized object into image generation presents an attractive feature in text-to-image generation. However, existing optimization-based and encoder-based methods are hindered by drawbacks such as time-consuming optimization, insufficient identity preservation, and a prevalent copy-pasting effect. To overcome these limitations, we introduce CustomNet, a novel object customization approach that explicitly incorporates 3D novel view synthesis capabilities into the object customization process. This integration facilitates the adjustment of spatial position relationships and viewpoints, yielding diverse outputs while effectively preserving object identity. Moreover, we introduce delicate designs to enable location control and flexible background control through textual descriptions or specific user-defined images, overcoming the limitations of existing 3D novel view synthesis methods. We further leverage a dataset construction pipeline that can better handle real-world objects and complex backgrounds. Equipped with these designs, our method facilitates zero-shot object customization without test-time optimization, offering simultaneous control over the viewpoints, location, and background. As a result, our CustomNet ensures enhanced identity preservation and generates diverse, harmonious outputs.", "year": 2023, "publicationdate": "2023-10-30", "externalids": {"DOI": "10.48550/arXiv.2310.19784"}, "doi_lower": "10.48550/arxiv.2310.19784"}
{"paper_id": 261081082, "title": "Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models to Learn Any Unseen Style", "author_names": ["Haoming Lu", "Hazarapet Tunanyan", "Kai Wang", "Shant Navasardyan", "Zhangyang Wang", "Humphrey Shi"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Diffusion models have demonstrated impressive capability of text-conditioned image synthesis, and broader application horizons are emerging by personalizing those pretrained diffusion models toward generating some specialized target object or style. In this paper, we aim to learn an unseen style by simply fine-tuning a pre-trained diffusion model with a handful of images (e.g., less than 10), so that the fine-tuned model can generate high-quality images of arbitrary objects in this style. Such extremely lowshot fine-tuning is accomplished by a novel toolkit of finetuning techniques, including text-to-image customized data augmentations, a content loss to facilitate content-style disentanglement, and sparse updating that focuses on only a few time steps. Our framework, dubbed Specialist Diffusion, is plug-and-play to existing diffusion model backbones and other personalization techniques. We demonstrate it to outperform the latest few-shot personalization alternatives of diffusion models such as Textual Inversion [7] and DreamBooth [24], in terms of learning highly sophisticated styles with ultra-sample-efficient tuning. We further show that Specialist Diffusion can be integrated on top of textual inversion to boost performance further, even on highly unusual styles. Our codes are available at: https://github.com/Picsart-AI-Research/Specialist-Diffusion.", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.1109/CVPR52729.2023.01371"}, "doi_lower": "10.1109/cvpr52729.2023.01371"}
{"paper_id": 260886966, "title": "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models", "author_names": ["Hu Ye", "Jun Zhang", "Siyi Liu", "Xiao Han", "Wei Yang"], "venue": "arXiv.org", "abstract": "Recent years have witnessed the strong power of large text-to-image diffusion models for the impressive generative capability to create high-fidelity images. However, it is very tricky to generate desired images using only text prompt as it often involves complex prompt engineering. An alternative to text prompt is image prompt, as the saying goes:\"an image is worth a thousand words\". Although existing methods of direct fine-tuning from pretrained models are effective, they require large computing resources and are not compatible with other base models, text prompt, and structural controls. In this paper, we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pretrained text-to-image diffusion models. The key design of our IP-Adapter is decoupled cross-attention mechanism that separates cross-attention layers for text features and image features. Despite the simplicity of our method, an IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fully fine-tuned image prompt model. As we freeze the pretrained diffusion model, the proposed IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. With the benefit of the decoupled cross-attention strategy, the image prompt can also work well with the text prompt to achieve multimodal image generation. The project page is available at \\url{https://ip-adapter.github.io}.", "year": 2023, "publicationdate": "2023-08-13", "externalids": {"DOI": "10.48550/arXiv.2308.06721"}, "doi_lower": "10.48550/arxiv.2308.06721"}
{"paper_id": 253734791, "title": "VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models", "author_names": ["Ajay Jain", "Amber Xie", "P. Abbeel"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Diffusion models have shown impressive results in text-to-image synthesis. Using massive datasets of captioned images, diffusion models learn to generate raster images of highly diverse objects and scenes. However, designers frequently use vector representations of images like Scalable Vector Graphics (SVGs) for digital icons or art. Vector graphics can be scaled to any size, and are compact. We show that a text-conditioned diffusion model trained on pixel representations of images can be used to generate SVG-exportable vector graphics. We do so without access to large datasets of captioned SVGs. By optimizing a differentiable vector graphics rasterizer, our method, VectorFusion, distills abstract semantic knowledge out of a pretrained diffusion model. Inspired by recent text-to-3D work, we learn an SVG consistent with a caption using Score Distillation Sampling. To accelerate generation and improve fidelity, VectorFusion also initializes from an image sample. Experiments show greater quality than prior work, and demonstrate a range of styles including pixel art and sketches.", "year": 2022, "publicationdate": "2022-11-21", "externalids": {"DOI": "10.1109/CVPR52729.2023.00190"}, "doi_lower": "10.1109/cvpr52729.2023.00190"}
{"paper_id": 259252217, "title": "DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models", "author_names": ["Ximing Xing", "Chuan Wang", "Haitao Zhou", "Jing Zhang", "Qian Yu", "Dong Xu"], "venue": "Neural Information Processing Systems", "abstract": "Even though trained mainly on images, we discover that pretrained diffusion models show impressive power in guiding sketch synthesis. In this paper, we present DiffSketcher, an innovative algorithm that creates \\textit{vectorized} free-hand sketches using natural language input. DiffSketcher is developed based on a pre-trained text-to-image diffusion model. It performs the task by directly optimizing a set of B\\'ezier curves with an extended version of the score distillation sampling (SDS) loss, which allows us to use a raster-level diffusion model as a prior for optimizing a parametric vectorized sketch generator. Furthermore, we explore attention maps embedded in the diffusion model for effective stroke initialization to speed up the generation process. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual details of the subject drawn. Our experiments show that DiffSketcher achieves greater quality than prior work. The code and demo of DiffSketcher can be found at https://ximinng.github.io/DiffSketcher-project/.", "year": 2023, "publicationdate": "2023-06-26", "externalids": {"DOI": "10.48550/arXiv.2306.14685"}, "doi_lower": "10.48550/arxiv.2306.14685"}
{"paper_id": 272724984, "title": "Vector Graphics Generation via Mutually Impulsed Dual-Domain Diffusion", "author_names": ["Zhongyin Zhao", "Ye Chen", "Zhangli Hu", "Xuanhong Chen", "Bingbing Ni"], "venue": "Computer Vision and Pattern Recognition", "abstract": null, "year": 2024, "publicationdate": "2024-06-16", "externalids": {"DOI": "10.1109/CVPR52733.2024.00423"}, "doi_lower": "10.1109/cvpr52733.2024.00423"}
{"paper_id": 233476433, "title": "SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models", "author_names": ["Haoying Li", "Yifan Yang", "Meng Chang", "H. Feng", "Zhi-hai Xu", "Qi Li", "Yue-ting Chen"], "venue": "Neurocomputing", "abstract": null, "year": 2021, "publicationdate": "2021-04-30", "externalids": {"DOI": "10.1016/j.neucom.2022.01.029"}, "doi_lower": "10.1016/j.neucom.2022.01.029"}
{"paper_id": 260538927, "title": "Unsupervised Medical Image Translation with Adversarial Diffusion Models", "author_names": ["Muzaffer Özbey", "Onat Dalmaz", "Salman UH Dar", "H. Bedel", "Şaban Özturk", "A. Güngör", "Tolga Cukur", "A. Salman", "agungor sozturk", "cukur"], "venue": "", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 258179052, "title": "Refusion: Enabling Large-Size Realistic Image Restoration with Latent-Space Diffusion Models", "author_names": ["Ziwei Luo", "Fredrik K. Gustafsson", "Zhengli Zhao", "Jens Sjolund", "T. Schon"], "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "abstract": "This work aims to improve the applicability of diffusion models in realistic image restoration. Specifically, we enhance the diffusion model in several aspects such as network architecture, noise level, denoising steps, training image size, and optimizer/scheduler. We show that tuning these hyperparameters allows us to achieve better performance on both distortion and perceptual scores. We also propose a U-Net based latent diffusion model which performs diffusion in a low-resolution latent space while preserving high-resolution information from the original input for the decoding process. Compared to the previous latent-diffusion model which trains a VAE-GAN to compress the image, our proposed U-Net compression strategy is significantly more stable and can recover highly accurate images without relying on adversarial optimization. Importantly, these modifications allow us to apply diffusion models to various image restoration tasks, including real-world shadow removal, HR non-homogeneous dehazing, stereo super-resolution, and bokeh effect transformation. By simply replacing the datasets and slightly changing the noise network, our model, named Refusion, is able to deal with large-size images (e.g., 6000 × 4000 × 3 in HR dehazing) and produces good results on all the above restoration problems. Our Refusion achieves the best perceptual performance in the NTIRE 2023 Image Shadow Removal Challenge and wins 2nd place overall.", "year": 2023, "publicationdate": "2023-04-17", "externalids": {"DOI": "10.1109/CVPRW59228.2023.00169"}, "doi_lower": "10.1109/cvprw59228.2023.00169"}
{"paper_id": 258832675, "title": "Hierarchical Integration Diffusion Model for Realistic Image Deblurring", "author_names": ["Zheng Chen", "Yulun Zhang", "Ding Liu", "Bin Xia", "Jinjin Gu", "L. Kong", "Xin Yuan"], "venue": "Neural Information Processing Systems", "abstract": "Diffusion models (DMs) have recently been introduced in image deblurring and exhibited promising performance, particularly in terms of details reconstruction. However, the diffusion model requires a large number of inference iterations to recover the clean image from pure Gaussian noise, which consumes massive computational resources. Moreover, the distribution synthesized by the diffusion model is often misaligned with the target results, leading to restrictions in distortion-based metrics. To address the above issues, we propose the Hierarchical Integration Diffusion Model (HI-Diff), for realistic image deblurring. Specifically, we perform the DM in a highly compacted latent space to generate the prior feature for the deblurring process. The deblurring process is implemented by a regression-based method to obtain better distortion accuracy. Meanwhile, the highly compact latent space ensures the efficiency of the DM. Furthermore, we design the hierarchical integration module to fuse the prior into the regression-based model from multiple scales, enabling better generalization in complex blurry scenarios. Comprehensive experiments on synthetic and real-world blur datasets demonstrate that our HI-Diff outperforms state-of-the-art methods. Code and trained models are available at https://github.com/zhengchen1999/HI-Diff.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.48550/arXiv.2305.12966"}, "doi_lower": "10.48550/arxiv.2305.12966"}
{"paper_id": 251468170, "title": "Wavelet Score-Based Generative Modeling", "author_names": ["Florentin Guth", "Simon Coste", "Valentin De Bortoli", "S. Mallat"], "venue": "Neural Information Processing Systems", "abstract": "Score-based generative models (SGMs) synthesize new data samples from Gaussian white noise by running a time-reversed Stochastic Differential Equation (SDE) whose drift coefficient depends on some probabilistic score. The discretization of such SDEs typically requires a large number of time steps and hence a high computational cost. This is because of ill-conditioning properties of the score that we analyze mathematically. We show that SGMs can be considerably accelerated, by factorizing the data distribution into a product of conditional probabilities of wavelet coefficients across scales. The resulting Wavelet Score-based Generative Model (WSGM) synthesizes wavelet coefficients with the same number of time steps at all scales, and its time complexity therefore grows linearly with the image size. This is proved mathematically over Gaussian distributions, and shown numerically over physical processes at phase transition and natural image datasets.", "year": 2022, "publicationdate": "2022-08-09", "externalids": {"DOI": "10.48550/arXiv.2208.05003"}, "doi_lower": "10.48550/arxiv.2208.05003"}
{"paper_id": 262825263, "title": "Bootstrap Diffusion Model Curve Estimation for High Resolution Low-Light Image Enhancement", "author_names": ["Jiancheng Huang", "Yi-fan Liu", "Shifeng Chen"], "venue": "Pacific Rim International Conference on Artificial Intelligence", "abstract": "Learning-based methods have attracted a lot of research attention and led to significant improvements in low-light image enhancement. However, most of them still suffer from two main problems: expensive computational cost in high resolution images and unsatisfactory performance in simultaneous enhancement and denoising. To address these problems, we propose BDCE, a bootstrap diffusion model that exploits the learning of the distribution of the curve parameters instead of the normal-light image itself. Specifically, we adopt the curve estimation method to handle the high-resolution images, where the curve parameters are estimated by our bootstrap diffusion model. In addition, a denoise module is applied in each iteration of curve adjustment to denoise the intermediate enhanced result of each iteration. We evaluate BDCE on commonly used benchmark datasets, and extensive experiments show that it achieves state-of-the-art qualitative and quantitative performance.", "year": 2023, "publicationdate": "2023-09-26", "externalids": {"DOI": "10.48550/arXiv.2309.14709"}, "doi_lower": "10.48550/arxiv.2309.14709"}
{"paper_id": 258615282, "title": "Exploiting Diffusion Prior for Real-World Image Super-Resolution", "author_names": ["Jianyi Wang", "Zongsheng Yue", "Shangchen Zhou", "Kelvin C. K. Chan", "Chen Change Loy"], "venue": "International Journal of Computer Vision", "abstract": "We present a novel approach to leverage prior knowledge encapsulated in pre-trained text-to-image diffusion models for blind super-resolution. Specifically, by employing our time-aware encoder, we can achieve promising restoration results without altering the pre-trained synthesis model, thereby preserving the generative prior and minimizing training cost. To remedy the loss of fidelity caused by the inherent stochasticity of diffusion models, we employ a controllable feature wrapping module that allows users to balance quality and fidelity by simply adjusting a scalar value during the inference process. Moreover, we develop a progressive aggregation sampling strategy to overcome the fixed-size constraints of pre-trained diffusion models, enabling adaptation to resolutions of any size. A comprehensive evaluation of our method using both synthetic and real-world benchmarks demonstrates its superiority over current state-of-the-art approaches. Code and models are available at https://github.com/IceClear/StableSR.", "year": 2023, "publicationdate": "2023-05-11", "externalids": {"DOI": "10.1007/s11263-024-02168-7"}, "doi_lower": "10.1007/s11263-024-02168-7"}
{"paper_id": 261276317, "title": "DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior", "author_names": ["X. Lin", "Jingwen He", "Zi-Yuan Chen", "Zhaoyang Lyu", "Ben Fei", "Bo Dai", "Wanli Ouyang", "Y. Qiao", "Chao Dong"], "venue": "arXiv.org", "abstract": "We present DiffBIR, a general restoration pipeline that could handle different blind image restoration tasks in a unified framework. DiffBIR decouples blind image restoration problem into two stages: 1) degradation removal: removing image-independent content; 2) information regeneration: generating the lost image content. Each stage is developed independently but they work seamlessly in a cascaded manner. In the first stage, we use restoration modules to remove degradations and obtain high-fidelity restored results. For the second stage, we propose IRControlNet that leverages the generative ability of latent diffusion models to generate realistic details. Specifically, IRControlNet is trained based on specially produced condition images without distracting noisy content for stable generation performance. Moreover, we design a region-adaptive restoration guidance that can modify the denoising process during inference without model re-training, allowing users to balance realness and fidelity through a tunable guidance scale. Extensive experiments have demonstrated DiffBIR's superiority over state-of-the-art approaches for blind image super-resolution, blind face restoration and blind image denoising tasks on both synthetic and real-world datasets. The code is available at https://github.com/XPixelGroup/DiffBIR.", "year": 2023, "publicationdate": "2023-08-29", "externalids": {"DOI": "10.48550/arXiv.2308.15070"}, "doi_lower": "10.48550/arxiv.2308.15070"}
{"paper_id": 265466453, "title": "CoSeR: Bridging Image and Language for Cognitive Super-Resolution", "author_names": ["Haoze Sun", "Wenbo Li", "Jianzhuang Liu", "Haoyu Chen", "Renjing Pei", "X. Zou", "Youliang Yan", "Yujiu Yang"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Existing super-resolution (SR) models primarily focus on restoring local texture details, often neglecting the global semantic information within the scene. This oversight can lead to the omission of crucial semantic details or the intro-duction of inaccurate textures during the recovery process. In our work, we introduce the Cognitive Super-Resolution (CoSeR) framework, empowering SR models with the ca-pacity to comprehend low-resolution images. We achieve this by marrying image appearance and language under-standing to generate a cognitive embedding, which not only activates prior information from large text-to-image diffusion models but also facilitates the generation of high-quality reference images to optimize the SR process. To fur-ther improve image fidelity, we propose a novel condition injection scheme called “Ali-in-Attention ”, consolidating all conditional information into a single module. Conse-quently, our method successfully restores semantically cor-rect and photorealistic details, demonstrating state-of-the-art performance across multiple benchmarks. Project page: https://coser-main.github.io/", "year": 2023, "publicationdate": "2023-11-27", "externalids": {"DOI": "10.1109/CVPR52733.2024.02444"}, "doi_lower": "10.1109/cvpr52733.2024.02444"}
{"paper_id": 267199774, "title": "Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild", "author_names": ["Fanghua Yu", "Jinjin Gu", "Zheyuan Li", "Jinfan Hu", "Xiangtao Kong", "Xintao Wang", "Jingwen He", "Yu Qiao", "Chao Dong"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image restoration method that harnesses generative prior and the power of model scaling up. Lever-aging multi-modal techniques and advanced generative prior, SUPIR marks a significant advance in intelligent and realistic image restoration. As a pivotal catalyst within SUPIR, model scaling dramatically enhances its capabil-ities and demonstrates new potential for image restoration. We collect a dataset comprising 20 million high-resolution, high-quality images for model training, each en-riched with descriptive text annotations. SUPIR provides the capability to restore images guided by textual prompts, broadening its application scope and potential. Moreover, we introduce negative-quality prompts to further improve perceptual quality. We also develop a restoration-guided sampling method to suppress the fidelity issue encountered in generative-based restoration. Experiments demonstrate SUPIR's exceptional restoration effects and its novel capac-ity to manipulate restoration through textual prompts.", "year": 2024, "publicationdate": "2024-01-24", "externalids": {"DOI": "10.1109/CVPR52733.2024.02425"}, "doi_lower": "10.1109/cvpr52733.2024.02425"}
{"paper_id": 259298715, "title": "Pseudoinverse-Guided Diffusion Models for Inverse Problems", "author_names": ["Jiaming Song", "Arash Vahdat", "M. Mardani", "Jan Kautz"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 119152310, "title": "Deep null space learning for inverse problems: convergence analysis and rates", "author_names": ["Johannes Schwab", "Stephan Antholzer", "M. Haltmeier"], "venue": "Inverse Problems", "abstract": "Recently, deep learning based methods appeared as a new paradigm for solving inverse problems. These methods empirically show excellent performance but lack of theoretical justification; in particular, no results on the regularization properties are available. In particular, this is the case for two-step deep learning approaches, where a classical reconstruction method is applied to the data in a first step and a trained deep neural network is applied to improve results in a second step. In this paper, we close the gap between practice and theory for a particular network structure in a two-step approach. For that purpose, we propose using so-called null space networks and introduce the concept of -regularization. Combined with a standard regularization method as reconstruction layer, the proposed deep null space learning approach is shown to be a -regularization method; convergence rates are also derived. The proposed null space network structure naturally preserves data consistency which is considered as key property of neural networks for solving inverse problems.", "year": 2018, "publicationdate": "2018-06-15", "externalids": {"DOI": "10.1088/1361-6420/aaf14a"}, "doi_lower": "10.1088/1361-6420/aaf14a"}
{"paper_id": 254018290, "title": "GAN Prior based Null-Space Learning for Consistent Super-Resolution", "author_names": ["Yinhuai Wang", "Yu Fei Hu", "Jiwen Yu", "Jian Zhang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Consistency and realness have always been the two critical issues of image super-resolution. While the realness has been dramatically improved with the use of GAN prior, the state-of-the-art methods still suffer inconsistencies in local structures and colors (e.g., tooth and eyes). In this paper, we show that these inconsistencies can be analytically eliminated by learning only the null-space component while fixing the range-space part. Further, we design a pooling-based decomposition (PD), a universal range-null space decomposition for super-resolution tasks, which is concise, fast, and parameter-free. PD can be easily applied to state-of-the-art GAN Prior based SR methods to eliminate their inconsistencies, neither compromise the realness nor bring extra parameters or computational costs. Besides, our ablation studies reveal that PD can replace pixel-wise losses for training and achieve better generalization performance when facing unseen downsamplings or even real-world degradation. Experiments show that the use of PD refreshes state-of-the-art SR performance and speeds up the convergence of training up to 2~10 times.", "year": 2022, "publicationdate": "2022-11-24", "externalids": {"DOI": "10.48550/arXiv.2211.13524"}, "doi_lower": "10.48550/arxiv.2211.13524"}
{"paper_id": 267960812, "title": "Deep Learning in Virtual Try-On: A Comprehensive Survey", "author_names": ["Tasin Islam", "A. Miron", "Xiaohui Liu", "Yongmin Li"], "venue": "IEEE Access", "abstract": "Virtual try-on technology has gained significant importance in the retail industry due to its potential to transform the way customers interact with products and make purchase decisions. It allows users to virtually try on clothing and accessories, providing a realistic representation of how the items would look and fit without the need for physical interaction. The ability to virtually try on products addresses common challenges associated with online shopping, such as uncertainty about fit and style, ultimately enhancing the overall customer experience and satisfaction. As a result, virtual try-on technology has the potential to reduce returns and optimise conversion rates for businesses, making it a valuable tool in the e-commerce landscape. In this paper, we provide a comprehensive review of deep learning based virtual try-on models, focusing on their functionality, technical details, dataset usage, weaknesses, and impact on customer satisfaction. The models are categorised into three main types: image-based, multi-pose, and video virtual try-on models, with detailed examples and technical summaries provided for each category. Additionally, we identify and discuss similarities and differences in these methods. Furthermore, we examine the datasets currently available for building and evaluating virtual try-on models, including the number of images/videos and their resolutions. We present the commonly used methods for both qualitative and quantitative evaluations, comparing synthesised images with previous work and performing quantitative evaluations across various metrics and benchmark datasets. We discuss the weaknesses of current deep learning based virtual try-on models, including challenges in preserving clothing characteristics and textures, the level of accuracy of applying the clothing to the person, and the preservation of facial identities. Additionally, we address dataset bias, particularly the domination of female models, limited diversity in clothing featured, and relatively simple and clean backgrounds in the datasets, which can negatively impact the model’s ability to handle challenging situations. Moreover, we explore the impact of virtual try-ons on customer satisfaction, highlighting the benefits that customers can enjoy, which also reduces returns and optimises conversion rates for businesses.", "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.1109/ACCESS.2024.3368612"}, "doi_lower": "10.1109/access.2024.3368612"}
{"paper_id": 265050510, "title": "Image-Based Virtual Try-On: A Survey", "author_names": ["Dan Song", "Xuanpu Zhang", "Juan Zhou", "Wei-zhi Nie", "Ruofeng Tong", "Mohan Kankanhalli", "Anan Liu"], "venue": "International Journal of Computer Vision", "abstract": "Image-based virtual try-on aims to synthesize a naturally dressed person image with a clothing image, which revolutionizes online shopping and inspires related topics within image generation, showing both research significance and commercial potential. However, there is a gap between current research progress and commercial applications and an absence of comprehensive overview of this field to accelerate the development. In this survey, we provide a comprehensive analysis of the state-of-the-art techniques and methodologies in aspects of pipeline architecture, person representation and key modules such as try-on indication, clothing warping and try-on stage. We additionally apply CLIP to assess the semantic alignment of try-on results, and evaluate representative methods with uniformly implemented evaluation metrics on the same dataset. In addition to quantitative and qualitative evaluation of current open-source methods, unresolved issues are highlighted and future research directions are prospected to identify key trends and inspire further exploration. The uniformly implemented evaluation metrics, dataset and collected methods will be made public available at https://github.com/little-misfit/Survey-Of-Virtual-Try-On.", "year": 2023, "publicationdate": "2023-11-08", "externalids": {"DOI": "10.1007/s11263-024-02305-2"}, "doi_lower": "10.1007/s11263-024-02305-2"}
{"paper_id": 4532827, "title": "VITON: An Image-Based Virtual Try-on Network", "author_names": ["Xintong Han", "Zuxuan Wu", "Zhe Wu", "Ruichi Yu", "L. Davis"], "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "abstract": "We present an image-based VIirtual Try-On Network (VITON) without using 3D information in any form, which seamlessly transfers a desired clothing item onto the corresponding region of a person using a coarse-to-fine strategy. Conditioned upon a new clothing-agnostic yet descriptive person representation, our framework first generates a coarse synthesized image with the target clothing item overlaid on that same person in the same pose. We further enhance the initial blurry clothing area with a refinement network. The network is trained to learn how much detail to utilize from the target clothing item, and where to apply to the person in order to synthesize a photo-realistic image in which the target item deforms naturally with clear visual patterns. Experiments on our newly collected Zalando dataset demonstrate its promise in the image-based virtual try-on task over state-of-the-art generative models.1", "year": 2017, "publicationdate": "2017-11-22", "externalids": {"DOI": "10.1109/CVPR.2018.00787"}, "doi_lower": "10.1109/cvpr.2018.00787"}
{"paper_id": 49901141, "title": "Toward Characteristic-Preserving Image-based Virtual Try-On Network", "author_names": ["Bochao Wang", "Huabing Zhang", "Xiaodan Liang", "Yimin Chen", "Liang Lin", "Meng Yang"], "venue": "European Conference on Computer Vision", "abstract": "Image-based virtual try-on systems for fitting a new in-shop clothes into a person image have attracted increasing research attention, yet is still challenging. A desirable pipeline should not only transform the target clothes into the most fitting shape seamlessly but also preserve well the clothes identity in the generated image, that is, the key characteristics (e.g. texture, logo, embroidery) that depict the original clothes. However, previous image-conditioned generation works fail to meet these critical requirements towards the plausible virtual try-on performance since they fail to handle large spatial misalignment between the input image and target clothes. Prior work explicitly tackled spatial deformation using shape context matching, but failed to preserve clothing details due to its coarse-to-fine strategy. In this work, we propose a new fully-learnable Characteristic-Preserving Virtual Try-On Network (CP-VTON) for addressing all real-world challenges in this task. First, CP-VTON learns a thin-plate spline transformation for transforming the in-shop clothes into fitting the body shape of the target person via a new Geometric Matching Module (GMM) rather than computing correspondences of interest points as prior works did. Second, to alleviate boundary artifacts of warped clothes and make the results more realistic, we employ a Try-On Module that learns a composition mask to integrate the warped clothes and the rendered image to ensure smoothness. Extensive experiments on a fashion dataset demonstrate our CP-VTON achieves the state-of-the-art virtual try-on performance both qualitatively and quantitatively.", "year": 2018, "publicationdate": "2018-07-20", "externalids": {"DOI": "10.1007/978-3-030-01261-8_36"}, "doi_lower": "10.1007/978-3-030-01261-8_36"}
{"paper_id": 204959889, "title": "ClothFlow: A Flow-Based Model for Clothed Person Generation", "author_names": ["Xintong Han", "Weilin Huang", "Xiaojun Hu", "Matthew R. Scott"], "venue": "IEEE International Conference on Computer Vision", "abstract": "We present ClothFlow, an appearance-flow-based generative model to synthesize clothed person for posed-guided person image generation and virtual try-on. By estimating a dense flow between source and target clothing regions, ClothFlow effectively models the geometric changes and naturally transfers the appearance to synthesize novel images as shown in Figure 1. We achieve this with a three-stage framework: 1) Conditioned on a target pose, we first estimate a person semantic layout to provide richer guidance to the generation process. 2) Built on two feature pyramid networks, a cascaded flow estimation network then accurately estimates the appearance matching between corresponding clothing regions. The resulting dense flow warps the source image to flexibly account for deformations. 3) Finally, a generative network takes the warped clothing regions as inputs and renders the target view. We conduct extensive experiments on the DeepFashion dataset for pose-guided person image generation and on the VITON dataset for the virtual try-on task. Strong qualitative and quantitative results validate the effectiveness of our method.", "year": 2019, "publicationdate": "2019-10-01", "externalids": {"DOI": "10.1109/ICCV.2019.01057"}, "doi_lower": "10.1109/iccv.2019.01057"}
{"paper_id": 232147187, "title": "Parser-Free Virtual Try-on via Distilling Appearance Flows", "author_names": ["Yuying Ge", "Yibing Song", "Ruimao Zhang", "Chongjian Ge", "Wei Liu", "P. Luo"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Image virtual try-on aims to fit a garment image (target clothes) to a person image. Prior methods are heavily based on human parsing. However, slightly-wrong segmentation results would lead to unrealistic try-on images with large artifacts. A recent pioneering work employed knowledge distillation to reduce the dependency of human parsing, where the try-on images produced by a parser-based method are used as supervisions to train a \"student\" network without relying on segmentation, making the student mimic the try-on ability of the parser-based model. However, the image quality of the student is bounded by the parser-based model. To address this problem, we propose a novel approach, \"teacher-tutor-student\" knowledge distillation, which is able to produce highly photo-realistic images without human parsing, possessing several appealing advantages compared to prior arts. (1) Unlike existing work, our approach treats the fake images produced by the parser-based method as \"tutor knowledge\", where the artifacts can be corrected by real \"teacher knowledge\", which is extracted from the real person images in a self-supervised way. (2) Other than using real images as supervisions, we formulate knowledge distillation in the try-on problem as distilling the appearance flows between the person image and the garment image, enabling us to find accurate dense correspondences between them to produce high-quality results. (3) Extensive evaluations show large superiority of our method (see Fig. 1).", "year": 2021, "publicationdate": "2021-03-08", "externalids": {"DOI": "10.1109/CVPR46437.2021.00838"}, "doi_lower": "10.1109/cvpr46437.2021.00838"}
{"paper_id": 257757040, "title": "GP-VTON: Towards General Purpose Virtual Try-On via Collaborative Local-Flow Global-Parsing Learning", "author_names": ["Zhenyu Xie", "Zaiyu Huang", "Xin Dong", "Fuwei Zhao", "Haoye Dong", "Xijin Zhang", "Feida Zhu", "Xiaodan Liang"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Image-based Virtual Try-ON aims to transfer an in-shop garment onto a specific person. Existing methods employ a global warping module to model the anisotropic deformation for different garment parts, which fails to preserve the semantic information of different parts when receiving challenging inputs (e.g, intricate human poses, difficult garments). Moreover, most of them directly warp the input garment to align with the boundary of the preserved region, which usually requires texture squeezing to meet the boundary shape constraint and thus leads to texture distortion. The above inferior performance hinders existing methods from real-world applications. To address these problems and take a step towards real-world virtual try-on, we propose a General-Purpose Virtual Try-ON framework, named GP-VTON, by developing an innovative Local-Flow Global-Parsing (LFGP) warping module and a Dynamic Gradient Truncation (DGT) training strategy. Specifically, compared with the previous global warping mechanism, LFGP employs local flows to warp garments parts individually, and assembles the local warped results via the global garment parsing, resulting in reasonable warped parts and a semantic-correct intact garment even with challenging inputs. On the other hand, our DGT training strategy dynamically truncates the gradient in the overlap area and the warped garment is no more required to meet the boundary constraint, which effectively avoids the texture squeezing problem. Furthermore, our GP-VTON can be easily extended to multi-category scenario and jointly trained by using data from different garment categories. Extensive experiments on two high-resolution benchmarks demonstrate our superiority over the existing state-of-the-art methods.11Code is available at gp-vton.", "year": 2023, "publicationdate": "2023-03-24", "externalids": {"DOI": "10.1109/CVPR52729.2023.02255"}, "doi_lower": "10.1109/cvpr52729.2023.02255"}
{"paper_id": 265714588, "title": "WarpDiffusion: Efficient Diffusion Model for High-Fidelity Virtual Try-on", "author_names": ["Xujie Zhang", "Xiu Li", "Michael C. Kampffmeyer", "Xin Dong", "Zhenyu Xie", "Feida Zhu", "Haoye Dong", "Xiaodan Liang"], "venue": "arXiv.org", "abstract": "Image-based Virtual Try-On (VITON) aims to transfer an in-shop garment image onto a target person. While existing methods focus on warping the garment to fit the body pose, they often overlook the synthesis quality around the garment-skin boundary and realistic effects like wrinkles and shadows on the warped garments. These limitations greatly reduce the realism of the generated results and hinder the practical application of VITON techniques. Leveraging the notable success of diffusion-based models in cross-modal image synthesis, some recent diffusion-based methods have ventured to tackle this issue. However, they tend to either consume a significant amount of training resources or struggle to achieve realistic try-on effects and retain garment details. For efficient and high-fidelity VITON, we propose WarpDiffusion, which bridges the warping-based and diffusion-based paradigms via a novel informative and local garment feature attention mechanism. Specifically, WarpDiffusion incorporates local texture attention to reduce resource consumption and uses a novel auto-mask module that effectively retains only the critical areas of the warped garment while disregarding unrealistic or erroneous portions. Notably, WarpDiffusion can be integrated as a plug-and-play component into existing VITON methodologies, elevating their synthesis quality. Extensive experiments on high-resolution VITON benchmarks and an in-the-wild test set demonstrate the superiority of WarpDiffusion, surpassing state-of-the-art methods both qualitatively and quantitatively.", "year": 2023, "publicationdate": "2023-12-06", "externalids": {"DOI": "10.48550/arXiv.2312.03667"}, "doi_lower": "10.48550/arxiv.2312.03667"}
{"paper_id": 259164412, "title": "TryOnDiffusion: A Tale of Two UNets", "author_names": ["Luyang Zhu", "Dawei Yang", "Tyler Lixuan Zhu", "F. Reda", "William Chan", "Chitwan Saharia", "Mohammad Norouzi", "Ira Kemelmacher-Shlizerman"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Given two images depicting a person and a garment worn by another person, our goal is to generate a visualization of how the garment might look on the input person. A key challenge is to synthesize a photorealistic detail-preserving visualization of the garment, while warping the garment to accommodate a significant body pose and shape change across the subjects. Previous methods either focus on garment detail preservation without effective pose and shape variation, or allow tryon with the desired shape and pose but lack garment details. In this paper, we propose a diffusion-based architecture that unifies two UN ets (referred to as Parallel-UNet), which allows us to preserve garment details and warp the garment for significant pose and body change in a single network. The key ideas behind Parallel-UNet include: 1) garment is warped implicitly via a cross attention mechanism, 2) garment warp and person blend happen as part of a unified process as opposed to a sequence of two separate tasks. Experimental results indicate that TryOnDiffusion achieves state-of-the-art performance both qualitatively and quantitatively.", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.1109/CVPR52729.2023.00447"}, "doi_lower": "10.1109/cvpr52729.2023.00447"}
{"paper_id": 258840871, "title": "LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On", "author_names": ["Davide Morelli", "Alberto Baldrati", "Giuseppe Cartella", "Marcella Cornia", "Marco Bertini", "R. Cucchiara"], "venue": "ACM Multimedia", "abstract": "The rapidly evolving fields of e-commerce and metaverse continue to seek innovative approaches to enhance the consumer experience. At the same time, recent advancements in the development of diffusion models have enabled generative networks to create remarkably realistic images. In this context, image-based virtual try-on, which consists in generating a novel image of a target model wearing a given in-shop garment, has yet to capitalize on the potential of these powerful generative solutions. This work introduces LaDI-VTON, the first Latent Diffusion textual Inversion-enhanced model for the Virtual Try-ON task. The proposed architecture relies on a latent diffusion model extended with a novel additional autoencoder module that exploits learnable skip connections to enhance the generation process preserving the model's characteristics. To effectively maintain the texture and details of the in-shop garment, we propose a textual inversion component that can map the visual features of the garment to the CLIP token embedding space and thus generate a set of pseudo-word token embeddings capable of conditioning the generation process. Experimental results on Dress Code and VITON-HD datasets demonstrate that our approach outperforms the competitors by a consistent margin, achieving a significant milestone for the task. Source code and trained models are publicly available at: https://github.com/miccunifi/ladi-vton.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.1145/3581783.3612137"}, "doi_lower": "10.1145/3581783.3612137"}
{"paper_id": 265609458, "title": "Stable VITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On", "author_names": ["Jeongho Kim", "Gyojung Gu", "Minho Park", "S. Park", "J. Choo"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Given a clothing image and a person image, an image-based virtual try-on aims to generate a customized image that appears natural and accurately reflects the character-istics of the clothing image. In this work, we aim to expand the applicability of the pre-trained diffusion model so that it can be utilized independently for the virtual try-on task. The main challenge is to preserve the clothing details while effectively utilizing the robust generative capability of the pre-trained model. In order to tackle these issues, we propose StableVITON, learning the semantic correspon-dence between the clothing and the human body within the latent space of the pre-trained diffusion model in an end-to-end manner. Our proposed zero cross-attention blocks not only preserve the clothing details by learning the semantic correspondence but also generate high-fidelity images by utilizing the inherent knowledge of the pre-trained model in the warping process. Through our proposed novel attention total variation loss and applying augmentation, we achieve the sharp attention map, resulting in a more precise representation of clothing details. Stable VITON out-performs the baselines in qualitative and quantitative evaluation, showing promising quality in arbitrary person images. Our code is available at https://github.com/rlawjdghek/StableVITON.", "year": 2023, "publicationdate": "2023-12-04", "externalids": {"DOI": "10.1109/CVPR52733.2024.00781"}, "doi_lower": "10.1109/cvpr52733.2024.00781"}
{"paper_id": 269484238, "title": "MMTryon: Multi-Modal Multi-Reference Control for High-Quality Fashion Generation", "author_names": ["Xujie Zhang", "Ente Lin", "Xiu Li", "Yuxuan Luo", "Michael C. Kampffmeyer", "Xin Dong", "Xiaodan Liang"], "venue": "arXiv.org", "abstract": "This paper introduces MMTryon, a multi-modal multi-reference VIrtual Try-ON (VITON) framework, which can generate high-quality compositional try-on results by taking a text instruction and multiple garment images as inputs. Our MMTryon addresses three problems overlooked in prior literature: 1) Support of multiple try-on items. Existing methods are commonly designed for single-item try-on tasks (e.g., upper/lower garments, dresses). 2)Specification of dressing style. Existing methods are unable to customize dressing styles based on instructions (e.g., zipped/unzipped, tuck-in/tuck-out, etc.) 3) Segmentation Dependency. They further heavily rely on category-specific segmentation models to identify the replacement regions, with segmentation errors directly leading to significant artifacts in the try-on results. To address the first two issues, our MMTryon introduces a novel multi-modality and multi-reference attention mechanism to combine the garment information from reference images and dressing-style information from text instructions. Besides, to remove the segmentation dependency, MMTryon uses a parsing-free garment encoder and leverages a novel scalable data generation pipeline to convert existing VITON datasets to a form that allows MMTryon to be trained without requiring any explicit segmentation. Extensive experiments on high-resolution benchmarks and in-the-wild test sets demonstrate MMTryon's superiority over existing SOTA methods both qualitatively and quantitatively. MMTryon's impressive performance on multi-item and style-controllable virtual try-on scenarios and its ability to try on any outfit in a large variety of scenarios from any source image, opens up a new avenue for future investigation in the fashion community.", "year": 2024, "publicationdate": "2024-05-01", "externalids": {"DOI": "10.48550/arXiv.2405.00448"}, "doi_lower": "10.48550/arxiv.2405.00448"}
{"paper_id": 271334679, "title": "CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models", "author_names": ["Zheng Chong", "Xiao Dong", "Haoxiang Li", "Shiyue Zhang", "Wenqing Zhang", "Xujie Zhang", "Hanqing Zhao", "Xiaodan Liang"], "venue": "International Conference on Learning Representations", "abstract": "Virtual try-on methods based on diffusion models achieve realistic effects but often require additional encoding modules, a large number of training parameters, and complex preprocessing, which increases the burden on training and inference. In this work, we re-evaluate the necessity of additional modules and analyze how to improve training efficiency and reduce redundant steps in the inference process. Based on these insights, we propose CatVTON, a simple and efficient virtual try-on diffusion model that transfers in-shop or worn garments of arbitrary categories to target individuals by concatenating them along spatial dimensions as inputs of the diffusion model. The efficiency of CatVTON is reflected in three aspects: (1) Lightweight network. CatVTON consists only of a VAE and a simplified denoising UNet, removing redundant image and text encoders as well as cross-attentions, and includes just 899.06M parameters. (2) Parameter-efficient training. Through experimental analysis, we identify self-attention modules as crucial for adapting pre-trained diffusion models to the virtual try-on task, enabling high-quality results with only 49.57M training parameters. (3) Simplified inference. CatVTON eliminates unnecessary preprocessing, such as pose estimation, human parsing, and captioning, requiring only a person image and garment reference to guide the virtual try-on process, reducing over 49% memory usage compared to other diffusion-based methods. Extensive experiments demonstrate that CatVTON achieves superior qualitative and quantitative results compared to baseline methods and demonstrates strong generalization performance in in-the-wild scenarios, despite being trained solely on public datasets with 73K samples.", "year": 2024, "publicationdate": "2024-07-21", "externalids": {"DOI": "10.48550/arXiv.2407.15886"}, "doi_lower": "10.48550/arxiv.2407.15886"}
{"paper_id": 244909410, "title": "DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation", "author_names": ["Gwanghyun Kim", "Taesung Kwon", "Jong-Chul Ye"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Recently, GAN inversion methods combined with Contrastive Language-Image Pretraining (CLIP) enables zeroshot image manipulation guided by text prompts. However, their applications to diverse real images are still difficult due to the limited GAN inversion capability. Specifically, these approaches often have difficulties in reconstructing images with novel poses, views, and highly variable contents compared to the training data, altering object identity, or producing unwanted image artifacts. To mitigate these problems and enable faithful manipulation of real images, we propose a novel method, dubbed DiffusionCLIP, that performs textdriven image manipulation using diffusion models. Based on full inversion capability and high-quality image generation power of recent diffusion models, our method performs zeroshot image manipulation successfully even between unseen domains and takes another step towards general application by manipulating images from a widely varying ImageNet dataset. Furthermore, we propose a novel noise combination method that allows straightforward multi-attribute manipulation. Extensive experiments and human evaluation confirmed robust and superior manipulation performance of our methods compared to the existing baselines. Code is available at https://github.com/gwang-kim/DiffusionCLIP.git", "year": 2021, "publicationdate": "2021-10-06", "externalids": {"DOI": "10.1109/CVPR52688.2022.00246"}, "doi_lower": "10.1109/cvpr52688.2022.00246"}
{"paper_id": 253018703, "title": "Diffusion Models already have a Semantic Latent Space", "author_names": ["Mingi Kwon", "Jaeseok Jeong", "Youngjung Uh"], "venue": "International Conference on Learning Representations", "abstract": "Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we introduce a principled design of the generative process for versatile editing and quality boost ing by quantifiable measures: editing strength of an interval and quality deficiency at a timestep. Our method is applicable to various architectures (DDPM++, iD- DPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN- bedroom, and METFACES). Project page: https://kwonminki.github.io/Asyrp/", "year": 2022, "publicationdate": "2022-10-20", "externalids": {"DOI": "10.48550/arXiv.2210.10960"}, "doi_lower": "10.48550/arxiv.2210.10960"}
{"paper_id": 258049148, "title": "Towards Real-time Text-driven Image Manipulation with Unconditional Diffusion Models", "author_names": ["Nikita Starodubcev", "Dmitry Baranchuk", "Valentin Khrulkov", "Artem Babenko"], "venue": "arXiv.org", "abstract": "Recent advances in diffusion models enable many powerful instruments for image editing. One of these instruments is text-driven image manipulations: editing semantic attributes of an image according to the provided text description. % Popular text-conditional diffusion models offer various high-quality image manipulation methods for a broad range of text prompts. Existing diffusion-based methods already achieve high-quality image manipulations for a broad range of text prompts. However, in practice, these methods require high computation costs even with a high-end GPU. This greatly limits potential real-world applications of diffusion-based image editing, especially when running on user devices. In this paper, we address efficiency of the recent text-driven editing methods based on unconditional diffusion models and develop a novel algorithm that learns image manipulations 4.5-10 times faster and applies them 8 times faster. We carefully evaluate the visual quality and expressiveness of our approach on multiple datasets using human annotators. Our experiments demonstrate that our algorithm achieves the quality of much more expensive methods. Finally, we show that our approach can adapt the pretrained model to the user-specified image and text description on the fly just for 4 seconds. In this setting, we notice that more compact unconditional diffusion models can be considered as a rational alternative to the popular text-conditional counterparts.", "year": 2023, "publicationdate": "2023-04-10", "externalids": {"DOI": "10.48550/arXiv.2304.04344"}, "doi_lower": "10.48550/arxiv.2304.04344"}
{"paper_id": 253735322, "title": "DiffStyler: Controllable Dual Diffusion for Text-Driven Image Stylization", "author_names": ["Nisha Huang", "Yu-xin Zhang", "Fan Tang", "Chongyang Ma", "Haibin Huang", "Yong Zhang", "Weiming Dong", "Changsheng Xu"], "venue": "IEEE Transactions on Neural Networks and Learning Systems", "abstract": "Despite the impressive results of arbitrary image-guided style transfer methods, text-driven image stylization has recently been proposed for transferring a natural image into a stylized one according to textual descriptions of the target style provided by the user. Unlike the previous image-to-image transfer approaches, text-guided stylization progress provides users with a more precise and intuitive way to express the desired style. However, the huge discrepancy between cross-modal inputs/outputs makes it challenging to conduct text-driven image stylization in a typical feed-forward CNN pipeline. In this article, we present DiffStyler, a dual diffusion processing architecture to control the balance between the content and style of the diffused results. The cross-modal style information can be easily integrated as guidance during the diffusion process step-by-step. Furthermore, we propose a content image-based learnable noise on which the reverse denoising process is based, enabling the stylization results to better preserve the structure information of the content image. We validate the proposed DiffStyler beyond the baseline methods through extensive qualitative and quantitative experiments. The code is available at https://github.com/haha-lisa/Diffstyler.", "year": 2022, "publicationdate": "2022-11-19", "externalids": {"DOI": "10.1109/TNNLS.2023.3342645"}, "doi_lower": "10.1109/tnnls.2023.3342645"}
{"paper_id": 260900064, "title": "StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models", "author_names": ["Zhizhong Wang", "Lei Zhao", "Wei Xing"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Content and style (C-S) disentanglement is a fundamental problem and critical challenge of style transfer. Existing approaches based on explicit definitions (e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable nor easy to control, resulting in entangled representations and less satisfying results. In this paper, we propose a new C-S disentangled framework for style transfer without using previous assumptions. The key insight is to explicitly extract the content information and implicitly learn the complementary style information, yielding interpretable and controllable C-S disentanglement and style transfer. A simple yet effective CLIP-based style disentanglement loss coordinated with a style reconstruction prior is introduced to disentangle C-S in the CLIP image space. By further leveraging the powerful style removal and generative ability of diffusion models, our framework achieves superior results than state of the art and flexible C-S disentanglement and trade-off control. Our work provides new insights into the C-S disentanglement in style transfer and demonstrates the potential of diffusion models for learning well-disentangled C-S characteristics.", "year": 2023, "publicationdate": "2023-08-15", "externalids": {"DOI": "10.1109/ICCV51070.2023.00706"}, "doi_lower": "10.1109/iccv51070.2023.00706"}
{"paper_id": 233210328, "title": "UNIT-DDPM: UNpaired Image Translation with Denoising Diffusion Probabilistic Models", "author_names": ["Hiroshi Sasaki", "Chris G. Willcocks", "T. Breckon"], "venue": "arXiv.org", "abstract": "We propose a novel unpaired image-to-image translation method that uses denoising diffusion probabilistic models without requiring adversarial training. Our method, UNpaired Image Translation with Denoising Diffusion Probabilistic Models (UNIT-DDPM), trains a generative model to infer the joint distribution of images over both domains as a Markov chain by minimising a denoising score matching objective conditioned on the other domain. In particular, we update both domain translation models simultaneously, and we generate target domain images by a denoising Markov Chain Monte Carlo approach that is conditioned on the input source domain images, based on Langevin dynamics. Our approach provides stable model training for image-to-image translation and generates high-quality image outputs. This enables state-of-the-art Fr\\'echet Inception Distance (FID) performance on several public datasets, including both colour and multispectral imagery, significantly outperforming the contemporary adversarial image-to-image translation methods.", "year": 2021, "publicationdate": "2021-04-12", "externalids": {}, "doi_lower": null}
{"paper_id": 264405630, "title": "CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation", "author_names": ["Sihan Xu", "Ziqiao Ma", "Yidong Huang", "Honglak Lee", "Joyce Chai"], "venue": "Neural Information Processing Systems", "abstract": "Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual prompt. Cyclenet is a practical framework, which is robust even with very limited training data (around 2k) and requires minimal computational resources (1 GPU) to train. Project homepage: https://cyclenetweb.github.io/", "year": 2023, "publicationdate": "2023-10-19", "externalids": {"DOI": "10.48550/arXiv.2310.13165"}, "doi_lower": "10.48550/arxiv.2310.13165"}
{"paper_id": 244729224, "title": "Diffusion Autoencoders: Toward a Meaningful and Decodable Representation", "author_names": ["Konpat Preechakul", "Nattanat Chatthee", "Suttisak Wizadwongsa", "Supasorn Suwajanakorn"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Diffusion probabilistic models (DPMs) have achieved remarkable quality in image generation that rivals GANs'. But unlike GANs, DPMs use a set of latent variables that lack semantic meaning and cannot serve as a useful representation for other tasks. This paper explores the possibility of using DPMs for representation learning and seeks to extract a meaningful and decodable representation of an input image via autoencoding. Our key idea is to use a learnable encoder for discovering the high-level semantics, and a DPM as the decoder for modeling the remaining stochastic variations. Our method can encode any image into a two-part latent code where the first part is semantically meaningful and linear, and the second part captures stochastic details, allowing near-exact reconstruction. This capability enables challenging applications that currently foil GAN-based methods, such as attribute manipulation on real images. We also show that this two-level encoding improves denoising efficiency and naturally facilitates various downstream tasks including few-shot conditional sampling. Please visit our page: https://Diff-AE.github.io/", "year": 2021, "publicationdate": "2021-11-30", "externalids": {"DOI": "10.1109/CVPR52688.2022.01036"}, "doi_lower": "10.1109/cvpr52688.2022.01036"}
{"paper_id": 258298850, "title": "Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation", "author_names": ["Zeyu Lu", "Chengyue Wu", "Xinyuan Chen", "Yaohui Wang", "Y. Qiao", "Xihui Liu"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "Diffusion models have attained impressive visual quality for image synthesis. However, how to probe and manipulate the latent space of diffusion models has not been extensively explored. Prior work diffusion autoencoders encode the semantic representations with a single latent code, neglecting the low-level details and leading to entangled representations. To mitigate those limitations, we propose Hierarchical Diffusion Autoencoders (HDAE) that exploits the coarse-to-fine feature hierarchy for the latent space of diffusion models. Our HDAE converges 2+ times faster and encodes richer and more comprehensive coarse-to-fine representations of images. The hierarchical latent space inherently disentangles different semantic levels of features. Furthermore, we propose a truncated feature based approach for disentangled image manipulation. We demonstrate the effectiveness of our proposed HDAE with extensive experiments and applications on image reconstruction, style mixing, controllable interpolation, image editing, and multi-modal semantic image synthesis. The code will be released upon acceptance.", "year": 2023, "publicationdate": "2023-04-24", "externalids": {"DOI": "10.1109/WACV57701.2024.00529"}, "doi_lower": "10.1109/wacv57701.2024.00529"}
{"paper_id": 250526607, "title": "EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations", "author_names": ["Min Zhao", "Fan Bao", "Chongxuan Li", "Jun Zhu"], "venue": "Neural Information Processing Systems", "abstract": "Score-based diffusion models (SBDMs) have achieved the SOTA FID results in unpaired image-to-image translation (I2I). However, we notice that existing methods totally ignore the training data in the source domain, leading to sub-optimal solutions for unpaired I2I. To this end, we propose energy-guided stochastic differential equations (EGSDE) that employs an energy function pretrained on both the source and target domains to guide the inference process of a pretrained SDE for realistic and faithful unpaired I2I. Building upon two feature extractors, we carefully design the energy function such that it encourages the transferred image to preserve the domain-independent features and discard domain-specific ones. Further, we provide an alternative explanation of the EGSDE as a product of experts, where each of the three experts (corresponding to the SDE and two feature extractors) solely contributes to faithfulness or realism. Empirically, we compare EGSDE to a large family of baselines on three widely-adopted unpaired I2I tasks under four metrics. EGSDE not only consistently outperforms existing SBDMs-based methods in almost all settings but also achieves the SOTA realism results without harming the faithful performance. Furthermore, EGSDE allows for flexible trade-offs between realism and faithfulness and we improve the realism results further (e.g., FID of 51.04 in Cat to Dog and FID of 50.43 in Wild to Dog on AFHQ) by tuning hyper-parameters. The code is available at https://github.com/ML-GSAI/EGSDE.", "year": 2022, "publicationdate": "2022-07-14", "externalids": {"DOI": "10.48550/arXiv.2207.06635"}, "doi_lower": "10.48550/arxiv.2207.06635"}
{"paper_id": 254246927, "title": "Fine-grained Image Editing by Pixel-wise Guidance Using Diffusion Models", "author_names": ["Naoki Matsunaga", "Masato Ishii", "Akio Hayakawa", "Kenji Suzuki", "T. Narihira"], "venue": "arXiv.org", "abstract": "Our goal is to develop fine-grained real-image editing methods suitable for real-world applications. In this paper, we first summarize four requirements for these methods and propose a novel diffusion-based image editing framework with pixel-wise guidance that satisfies these requirements. Specifically, we train pixel-classifiers with a few annotated data and then infer the segmentation map of a target image. Users then manipulate the map to instruct how the image will be edited. We utilize a pre-trained diffusion model to generate edited images aligned with the user's intention with pixel-wise guidance. The effective combination of proposed guidance and other techniques enables highly controllable editing with preserving the outside of the edited area, which results in meeting our requirements. The experimental results demonstrate that our proposal outperforms the GAN-based method for editing quality and speed.", "year": 2022, "publicationdate": "2022-12-05", "externalids": {"DOI": "10.48550/arXiv.2212.02024"}, "doi_lower": "10.48550/arxiv.2212.02024"}
{"paper_id": 253802085, "title": "Paint by Example: Exemplar-based Image Editing with Diffusion Models", "author_names": ["Binxin Yang", "Shuyang Gu", "Bo Zhang", "Ting Zhang", "Xuejin Chen", "Xiaoyan Sun", "Dong Chen", "Fang Wen"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Language-guided image editing has achieved great success recently. In this paper, we investigate exemplar-guided image editing for more precise control. We achieve this goal by leveraging self-supervised training to disentangle and re-organize the source image and the exemplar. However, the naive approach will cause obvious fusing artifacts. We carefully analyze it and propose a content bottleneck and strong augmentations to avoid the trivial solution of directly copying and pasting the exemplar image. Meanwhile, to ensure the controllability of the editing process, we design an arbitrary shape mask for the exemplar image and leverage the classifier-free guidance to increase the similarity to the exemplar image. The whole framework involves a single forward of the diffusion model without any iterative optimization. We demonstrate that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity. The code and pretrained models are available at https://github.com/Fantasy-Studio/Paint-by-Example.", "year": 2022, "publicationdate": "2022-11-23", "externalids": {"DOI": "10.1109/CVPR52729.2023.01763"}, "doi_lower": "10.1109/cvpr52729.2023.01763"}
{"paper_id": 258212902, "title": "Reference-based Image Composition with Sketch via Structure-aware Diffusion Model", "author_names": ["Kangyeol Kim", "S. Park", "Junsoo Lee", "J. Choo"], "venue": "arXiv.org", "abstract": "Recent remarkable improvements in large-scale text-to-image generative models have shown promising results in generating high-fidelity images. To further enhance editability and enable fine-grained generation, we introduce a multi-input-conditioned image composition model that incorporates a sketch as a novel modal, alongside a reference image. Thanks to the edge-level controllability using sketches, our method enables a user to edit or complete an image sub-part with a desired structure (i.e., sketch) and content (i.e., reference image). Our framework fine-tunes a pre-trained diffusion model to complete missing regions using the reference image while maintaining sketch guidance. Albeit simple, this leads to wide opportunities to fulfill user needs for obtaining the in-demand images. Through extensive experiments, we demonstrate that our proposed method offers unique use cases for image manipulation, enabling user-driven modifications of arbitrary scenes.", "year": 2023, "publicationdate": "2023-03-31", "externalids": {"DOI": "10.48550/arXiv.2304.09748"}, "doi_lower": "10.48550/arxiv.2304.09748"}
{"paper_id": 260005035, "title": "ObjectStitch: Object Compositing with Diffusion Model", "author_names": ["Yi-Zhe Song", "Zhifei Zhang", "Zhe Lin", "Scott D. Cohen", "Brian L. Price", "Jianming Zhang", "S. Kim", "Daniel G. Aliaga"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Object compositing based on 2D images is a challenging problem since it typically involves multiple processing stages such as color harmonization, geometry correction and shadow generation to generate realistic results. Furthermore, annotating training data pairs for compositing requires substantial manual effort from professionals, and is hardly scalable. Thus, with the recent advances in generative models, in this work, we propose a selfsupervised framework for object compositing by leveraging the power of conditional diffusion models. Our framework can hollistically address the object compositing task in a unified model, transforming the viewpoint, geometry, color and shadow of the generated object while requiring no manual labeling. To preserve the input object's characteristics, we introduce a content adaptor that helps to maintain categori-cal semantics and object appearance. A data augmentation method is further adopted to improve the fidelity of the generator. Our method outperforms relevant baselines in both realism and faithfulness of the synthesized result images in a user study on various real-world images.", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.1109/CVPR52729.2023.01756"}, "doi_lower": "10.1109/cvpr52729.2023.01756"}
{"paper_id": 259144837, "title": "Paste, Inpaint and Harmonize via Denoising: Subject-Driven Image Editing with Pre-Trained Diffusion Model", "author_names": ["X. Zhang", "Jiaxian Guo", "Paul Yoo", "Yutaka Matsuo", "Yusuke Iwasawa"], "venue": "arXiv.org", "abstract": "Text-to-image generative models have attracted rising attention for flexible image editing via user-specified descriptions. However, text descriptions alone are not enough to elaborate the details of subjects, often compromising the subjects' identity or requiring additional per-subject fine-tuning. We introduce a new framework called \\textit{Paste, Inpaint and Harmonize via Denoising} (PhD), which leverages an exemplar image in addition to text descriptions to specify user intentions. In the pasting step, an off-the-shelf segmentation model is employed to identify a user-specified subject within an exemplar image which is subsequently inserted into a background image to serve as an initialization capturing both scene context and subject identity in one. To guarantee the visual coherence of the generated or edited image, we introduce an inpainting and harmonizing module to guide the pre-trained diffusion model to seamlessly blend the inserted subject into the scene naturally. As we keep the pre-trained diffusion model frozen, we preserve its strong image synthesis ability and text-driven ability, thus achieving high-quality results and flexible editing with diverse texts. In our experiments, we apply PhD to both subject-driven image editing tasks and explore text-driven scene generation given a reference subject. Both quantitative and qualitative comparisons with baseline methods demonstrate that our approach achieves state-of-the-art performance in both tasks. More qualitative results can be found at \\url{https://sites.google.com/view/phd-demo-page}.", "year": 2023, "publicationdate": "2023-06-13", "externalids": {"DOI": "10.48550/arXiv.2306.07596"}, "doi_lower": "10.48550/arxiv.2306.07596"}
{"paper_id": 266053929, "title": "DreamInpainter: Text-Guided Subject-Driven Image Inpainting with Diffusion Models", "author_names": ["Shaoan Xie", "Yang Zhao", "Zhisheng Xiao", "Kelvin C.K. Chan", "Yandong Li", "Yanwu Xu", "Kun Zhang", "Tingbo Hou"], "venue": "arXiv.org", "abstract": "This study introduces Text-Guided Subject-Driven Image Inpainting, a novel task that combines text and exemplar images for image inpainting. While both text and exemplar images have been used independently in previous efforts, their combined utilization remains unexplored. Simultaneously accommodating both conditions poses a significant challenge due to the inherent balance required between editability and subject fidelity. To tackle this challenge, we propose a two-step approach DreamInpainter. First, we compute dense subject features to ensure accurate subject replication. Then, we employ a discriminative token selection module to eliminate redundant subject details, preserving the subject's identity while allowing changes according to other conditions such as mask shape and text prompts. Additionally, we introduce a decoupling regularization technique to enhance text control in the presence of exemplar images. Our extensive experiments demonstrate the superior performance of our method in terms of visual quality, identity preservation, and text control, showcasing its effectiveness in the context of text-guided subject-driven image inpainting.", "year": 2023, "publicationdate": "2023-12-05", "externalids": {"DOI": "10.48550/arXiv.2312.03771"}, "doi_lower": "10.48550/arxiv.2312.03771"}
{"paper_id": 259951373, "title": "AnyDoor: Zero-shot Object-level Image Customization", "author_names": ["Xi Chen", "Lianghua Huang", "Yu Liu", "Yujun Shen", "Deli Zhao", "Hengshuang Zhao"], "venue": "Computer Vision and Pattern Recognition", "abstract": "This work presents AnyDoor, a diffusion-based image generator with the power to teleport target objects to new scenes at user-specified locations with desired shapes. Instead of tuning parameters for each object, our model is trained only once and effortlessly generalizes to diverse object-scene combinations at the inference stage. Such a challenging zero-shot setting requires an adequate characterization of a certain object. To this end, we complement the commonly used identity feature with detail features, which are carefully designed to maintain appearance details yet allow versatile local variations (e.g., lighting, orientation, posture, etc.), supporting the object in favorably blending with different surroundings. We further propose to borrow knowledge from video datasets, where we can observe various forms (i.e., along the time axis) of a single object, leading to stronger model generalizability and robustness. Extensive experiments demonstrate the superiority of our approach over existing alternatives as well as its great potential in real-world applications, such as virtual try-on, shape editing, and object swapping. Code is released at github.com/ali-vilab/AnyDoor.", "year": 2023, "publicationdate": "2023-07-18", "externalids": {"DOI": "10.1109/CVPR52733.2024.00630"}, "doi_lower": "10.1109/cvpr52733.2024.00630"}
{"paper_id": 262066278, "title": "Face Aging via Diffusion-based Editing", "author_names": ["Xiangyi Chen", "St'ephane Lathuiliere"], "venue": "British Machine Vision Conference", "abstract": "In this paper, we address the problem of face aging: generating past or future facial images by incorporating age-related changes to the given face. Previous aging methods rely solely on human facial image datasets and are thus constrained by their inherent scale and bias. This restricts their application to a limited generatable age range and the inability to handle large age gaps. We propose FADING, a novel approach to address Face Aging via DIffusion-based editiNG. We go beyond existing methods by leveraging the rich prior of large-scale language-image diffusion models. First, we specialize a pre-trained diffusion model for the task of face age editing by using an age-aware fine-tuning scheme. Next, we invert the input image to latent noise and obtain optimized null text embeddings. Finally, we perform text-guided local age editing via attention control. The quantitative and qualitative analyses demonstrate that our method outperforms existing approaches with respect to aging accuracy, attribute preservation, and aging quality.", "year": 2023, "publicationdate": "2023-09-20", "externalids": {"DOI": "10.48550/arXiv.2309.11321"}, "doi_lower": "10.48550/arxiv.2309.11321"}
{"paper_id": 257834185, "title": "PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models", "author_names": ["Vidit Goel", "E. Peruzzo", "Yifan Jiang", "Dejia Xu", "N. Sebe", "Trevor Darrell", "Zhangyang Wang", "Humphrey Shi"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2303.17546"}, "doi_lower": "10.48550/arxiv.2303.17546"}
{"paper_id": 254535802, "title": "SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model", "author_names": ["Shaoan Xie", "Zhifei Zhang", "Zhe Lin", "T. Hinz", "Kun Zhang"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Generic image inpainting aims to complete a corrupted image by borrowing surrounding information, which barely generates novel content. By contrast, multi-modal inpainting provides more flexible and useful controls on the inpainted content, e.g., a text prompt can be used to describe an object with richer attributes, and a mask can be used to constrain the shape of the inpainted object rather than being only considered as a missing area. We propose a new diffusion-based model named SmartBrush for completing a missing region with an object using both text and shape-guidance. While previous work such as DALLE-2 and Stable Diffusion can do text-guided inapinting they do not support shape guidance and tend to modify background texture surrounding the generated object. Our model incorporates both text and shape guidance with precision control. To preserve the background better, we propose a novel training and sampling strategy by augmenting the diffusion U-net with object-mask prediction. Lastly, we introduce a multi-task training strategy by jointly training inpainting with text-to-image generation to leverage more training data. We conduct extensive experiments showing that our model outperforms all baselines in terms of visual quality, mask controllability, and background preservation.", "year": 2022, "publicationdate": "2022-12-09", "externalids": {"DOI": "10.1109/CVPR52729.2023.02148"}, "doi_lower": "10.1109/cvpr52729.2023.02148"}
{"paper_id": 258960274, "title": "Text-to-image Editing by Image Information Removal", "author_names": ["Zhongping Zhang", "Jian Zheng", "Jacob Zhiyuan Fang", "Bryan A. Plummer"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "Diffusion models have demonstrated impressive performance in text-guided image generation. Current methods that leverage the knowledge of these models for image editing either fine-tune them using the input image (e.g., Imagic) or incorporate structure information as additional constraints (e.g., ControlNet). However, fine-tuning large-scale diffusion models on a single image can lead to severe overfitting issues and lengthy inference time. Information leakage from pretrained models also make it challenging to preserve image content not related to the text input. Additionally, methods that incorporate structural guidance (e.g., edge maps, semantic maps, keypoints) find retaining attributes like colors and textures difficult. Using the input image as a control could mitigate these issues, but since these models are trained via reconstruction, a model can simply hide information about the original image when encoding it to perfectly reconstruct the image without learning the editing task. To address these challenges, we propose a text-to-image editing model with an Image Information Removal module (IIR) that selectively erases color-related and texture-related information from the original image, allowing us to better preserve the text-irrelevant content and avoid issues arising from information hiding. Our experiments on CUB, Outdoor Scenes, and COCO reports our approach achieves the best editability-fidelity trade-off results. In addition, a user study on COCO shows that our edited images are preferred 35% more often than prior work.", "year": 2023, "publicationdate": "2023-05-27", "externalids": {"DOI": "10.1109/WACV57701.2024.00515"}, "doi_lower": "10.1109/wacv57701.2024.00515"}
{"paper_id": 265674753, "title": "A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting", "author_names": ["Junhao Zhuang", "Yanhong Zeng", "Wenran Liu", "Chun Yuan", "Kai Chen"], "venue": "European Conference on Computer Vision", "abstract": "Advancing image inpainting is challenging as it requires filling user-specified regions for various intents, such as background filling and object synthesis. Existing approaches focus on either context-aware filling or object synthesis using text descriptions. However, achieving both tasks simultaneously is challenging due to differing training strategies. To overcome this challenge, we introduce PowerPaint, the first high-quality and versatile inpainting model that excels in multiple inpainting tasks. First, we introduce learnable task prompts along with tailored fine-tuning strategies to guide the model's focus on different inpainting targets explicitly. This enables PowerPaint to accomplish various inpainting tasks by utilizing different task prompts, resulting in state-of-the-art performance. Second, we demonstrate the versatility of the task prompt in PowerPaint by showcasing its effectiveness as a negative prompt for object removal. Moreover, we leverage prompt interpolation techniques to enable controllable shape-guided object inpainting, enhancing the model's applicability in shape-guided applications. Finally, we conduct extensive experiments and applications to verify the effectiveness of PowerPaint. We release our codes and models on our project page: https://powerpaint.github.io/.", "year": 2023, "publicationdate": "2023-12-06", "externalids": {"DOI": "10.48550/arXiv.2312.03594"}, "doi_lower": "10.48550/arxiv.2312.03594"}
{"paper_id": 254636532, "title": "Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting", "author_names": ["Su Wang", "Chitwan Saharia", "Ceslee Montgomery", "J. Pont-Tuset", "Shai Noy", "S. Pellegrini", "Yasumasa Onoe", "Sarah Laszlo", "David J. Fleet", "Radu Soricut", "Jason Baldridge", "Mohammad Norouzi", "Peter Anderson", "William Chan"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Text-guided image editing can have a transformative impact in supporting creative applications. A key challenge is to generate edits that are faithful to input text prompts, while consistent with input images. We present Imagen Editor, a cascaded diffusion model built, by fine-tuning Imagen [36] on text-guided image inpainting. Imagen Editor's edits are faithful to the text prompts, which is accomplished by using object detectors to propose inpainting masks during training. In addition, Imagen Editor captures fine details in the input image by conditioning the cascaded pipeline on the original high resolution image. To improve qualitative and quantitative evaluation, we introduce EditBench, a systematic benchmark for text-guided image inpainting. EditBench evaluates inpainting edits on natural and generated images exploring objects, attributes, and scenes. Through extensive human evaluation on EditBench, we find that object-masking during training leads to across-the-board improvements in text-image alignment – such that Imagen Editor is preferred over DALL-E 2 [31] and Stable Diffusion [33] – and, as a cohort, these models are better at object-rendering than text-rendering, and handle material/color/size attributes better than count/shape attributes.", "year": 2022, "publicationdate": "2022-12-13", "externalids": {"DOI": "10.1109/CVPR52729.2023.01761"}, "doi_lower": "10.1109/cvpr52729.2023.01761"}
{"paper_id": 266149485, "title": "SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control", "author_names": ["Jaskirat Singh", "Jianming Zhang", "Qing Liu", "Cameron Smith", "Zhe Lin", "Liang Zheng"], "venue": "Computer Vision and Pattern Recognition", "abstract": "The field of generative image inpainting and object in-sertion has made significant progress with the recent advent of latent diffusion models. Utilizing a precise object mask can greatly enhance these applications. However, due to the challenges users encounter in creating high-fidelity masks, there is a tendency for these methods to rely on more coarse masks (e.g., bounding box) for these applications. This results in limited control and compromised background content preservation. To overcome these limitations, we introduce SmartMask, which allows any novice user to create detailed masks for precise object insertion. Combined with a ControlNet-Inpaint model, our experiments demonstrate that SmartMask achieves superior object insertion quality, preserving the background content more effectively than previous methods. Notably, unlike prior works the proposed approach can also be used even without user-mask guid-ance, which allows it to perform mask-free object insertion at diverse positions and scales. Furthermore, we find that when used iteratively with a novel instruction-tuning based planning model, SmartMask can be used to design detailed layouts from scratch. As compared with user-scribble based layout design, we observe that SmartMask allows for better quality outputs with layout-to-image generation methods.", "year": 2023, "publicationdate": "2023-12-08", "externalids": {"DOI": "10.1109/CVPR52733.2024.00621"}, "doi_lower": "10.1109/cvpr52733.2024.00621"}
{"paper_id": 263835196, "title": "Uni-paint: A Unified Framework for Multimodal Image Inpainting with Pretrained Diffusion Model", "author_names": ["Shiyuan Yang", "Xiaodong Chen", "Jing Liao"], "venue": "ACM Multimedia", "abstract": "Recently, text-to-image denoising diffusion probabilistic models (DDPMs) have demonstrated impressive image generation capabilities and have also been successfully applied to image inpainting. However, in practice, users often require more control over the inpainting process beyond textual guidance, especially when they want to composite objects with customized appearance, color, shape, and layout. Unfortunately, existing diffusion-based inpainting methods are limited to single-modal guidance and require task-specific training, hindering their cross-modal scalability. To address these limitations, we propose Uni-paint, a unified framework for multimodal inpainting that offers various modes of guidance, including unconditional, text-driven, stroke-driven, exemplar-driven inpainting, as well as a combination of these modes. Furthermore, our Uni-paint is based on pretrained Stable Diffusion and does not require task-specific training on specific datasets, enabling few-shot generalizability to customized images. We have conducted extensive qualitative and quantitative evaluations that show our approach achieves comparable results to existing single-modal methods while offering multimodal inpainting capabilities not available in other methods. Code is available at https://github.com/ysy31415/unipaint.", "year": 2023, "publicationdate": "2023-10-11", "externalids": {"DOI": "10.1145/3581783.3612200"}, "doi_lower": "10.1145/3581783.3612200"}
{"paper_id": 253581213, "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "author_names": ["Tim Brooks", "Aleksander Holynski", "Alexei A. Efros"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models—a language model (GPT-3) and a text-to-image model (Stable Diffusion)—to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.", "year": 2022, "publicationdate": "2022-11-17", "externalids": {"DOI": "10.1109/CVPR52729.2023.01764"}, "doi_lower": "10.1109/cvpr52729.2023.01764"}
{"paper_id": 261660692, "title": "MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers", "author_names": ["Sijia Li", "Chen Chen", "H. Lu"], "venue": "arXiv.org", "abstract": "Diffusion-model-based text-guided image generation has recently made astounding progress, producing fascinating results in open-domain image manipulation tasks. Few models, however, currently have complete zero-shot capabilities for both global and local image editing due to the complexity and diversity of image manipulation tasks. In this work, we propose a method with a mixture-of-expert (MOE) controllers to align the text-guided capacity of diffusion models with different kinds of human instructions, enabling our model to handle various open-domain image manipulation tasks with natural language instructions. First, we use large language models (ChatGPT) and conditional image synthesis models (ControlNet) to generate a large number of global image transfer dataset in addition to the instruction-based local image editing dataset. Then, using an MOE technique and task-specific adaptation training on a large-scale dataset, our conditional diffusion model can edit images globally and locally. Extensive experiments demonstrate that our approach performs surprisingly well on various image manipulation tasks when dealing with open-domain images and arbitrary human instructions. Please refer to our project page: [https://oppo-mente-lab.github.io/moe_controller/]", "year": 2023, "publicationdate": "2023-09-08", "externalids": {"DOI": "10.48550/arXiv.2309.04372"}, "doi_lower": "10.48550/arxiv.2309.04372"}
{"paper_id": 266348293, "title": "Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation", "author_names": ["Qin Guo", "Tianwei Lin"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Recently, diffusion-based methods, like InstructPix2Pix (IP2P), have achieved effective instruction-based image editing, requiring only natural language instructions from the user. However, these methods often inadvertently alter unintended areas and struggle with multi-instruction editing, resulting in compromised outcomes. To address these issues, we introduce the Focus on Your Instruction (FoI), a method designed to ensure precise and harmonious editing across multiple instructions without extra training or test-time optimization. In the FoI, we primarily emphasize two aspects: (1) precisely extracting regions of interest for each instruction and (2) guiding the denoising process to concentrate within these regions of interest. For the first objective, we identify the implicit grounding capability of IP2P from the cross-attention between instruction and image, then develop an effective mask extraction method. For the second objective, we introduce a cross attention modulation module for rough isolation of target editing regions and unrelated regions. Additionally, we introduce a mask-guided disentangle sampling strategy to further ensure clear region isolation. Experimental results demonstrate that FoI surpasses existing methods in both quantitative and qualitative evaluations, especially excelling in multi-instruction editing task. The code is available at https://github.com/guoqincode/Focus-on-Your-Instruction.", "year": 2023, "publicationdate": "2023-12-15", "externalids": {"DOI": "10.1109/CVPR52733.2024.00667"}, "doi_lower": "10.1109/cvpr52733.2024.00667"}
{"paper_id": 264803981, "title": "Learning to Follow Object-Centric Image Editing Instructions Faithfully", "author_names": ["Tuhin Chakrabarty", "Kanishk Singh", "Arkadiy Saakyan", "S. Muresan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Natural language instructions are a powerful interface for editing the outputs of text-to-image diffusion models. However, several challenges need to be addressed: 1) underspecification (the need to model the implicit meaning of instructions) 2) grounding (the need to localize where the edit has to be performed), 3) faithfulness (the need to preserve the elements of the image not affected by the edit instruction). Current approaches focusing on image editing with natural language instructions rely on automatically generated paired data, which, as shown in our investigation, is noisy and sometimes nonsensical, exacerbating the above issues. Building on recent advances in segmentation, Chain-of-Thought prompting, and visual question answering, we significantly improve the quality of the paired data. In addition, we enhance the supervision signal by highlighting parts of the image that need to be changed by the instruction. The model fine-tuned on the improved data is capable of performing fine-grained object-centric edits better than state-of-the-art baselines, mitigating the problems outlined above, as shown by automatic and human evaluations. Moreover, our model is capable of generalizing to domains unseen during training, such as visual metaphors.", "year": 2023, "publicationdate": "2023-10-29", "externalids": {"DOI": "10.48550/arXiv.2310.19145"}, "doi_lower": "10.48550/arxiv.2310.19145"}
{"paper_id": 261582721, "title": "InstructDiffusion: A Generalist Modeling Interface for Vision Tasks", "author_names": ["Zigang Geng", "Binxin Yang", "Tiankai Hang", "Chen Li", "Shuyang Gu", "Ting Zhang", "Jianmin Bao", "Zheng Zhang", "Han Hu", "Dongdong Chen", "Baining Guo"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We present InstructDiffusion, a unified and generic framework for aligning computer vision tasks with hu-man instructions. Unlike existing approaches that integrate prior knowledge and pre-define the output space (e.g., categories and coordinates) for each vision task, we cast diverse vision tasks into a human-intuitive image-manipulating pro-cess whose output space is a flexible and interactive pixel space. Concretely, the model is built upon the diffusion process and is trained to predict pixels according to user instructions, such as encircling the man's left shoulder in red or applying a blue mask to the left car. InstructDiffusion could handle a variety of vision tasks, including understanding tasks (such as segmentation and keypoint de-tection) and generative tasks (such as editing and enhance-ment) and outperforms prior methods on novel datasets. This represents a solid step towards a generalist modeling interface for vision tasks, advancing artificial general intelligence in the field of computer vision.", "year": 2023, "publicationdate": "2023-09-07", "externalids": {"DOI": "10.1109/CVPR52733.2024.01208"}, "doi_lower": "10.1109/cvpr52733.2024.01208"}
{"paper_id": 265221391, "title": "Emu Edit: Precise Image Editing via Recognition and Generation Tasks", "author_names": ["Shelly Sheynin", "Adam Polyak", "Uriel Singer", "Yuval Kirstain", "Amit Zohar", "Oron Ashual", "Devi Parikh", "Yaniv Taigman"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Instruction-based image editing holds immense potential for a variety of applications, as it enables users to perform any editing operation using a natural language instruction. However, current models in this domain often struggle with accurately executing user instructions. We present Emu Edit, a multitask image editing model which sets state-of-the-art results in instruction-based image editing. To develop Emu Edit we train it to multitask across an unprecedented range of tasks, such as region-based editing, free-form editing, and Computer Vision tasks, all of which are formulated as generative tasks. Additionally, to enhance Emu Edit's multitask learning abilities, we provide it with learned task embeddings which guide the generation process towards the correct edit type. Both these elements are essential for Emu Edit's outstanding performance. Furthermore, we show that Emu Edit can generalize to new tasks, such as image inpainting, super-resolution, and compositions of editing tasks, with just a few labeled examples. This capability offers a significant advantage in scenarios where high-quality samples are scarce. Lastly, to facilitate a more rigorous and informed assessment of instructable image editing models, we release a new challenging and versatile benchmark that includes seven different image editing tasks. 1", "year": 2023, "publicationdate": "2023-11-16", "externalids": {"DOI": "10.1109/CVPR52733.2024.00847"}, "doi_lower": "10.1109/cvpr52733.2024.00847"}
{"paper_id": 257623108, "title": "DialogPaint: A Dialog-based Image Editing Model", "author_names": ["Jingxuan Wei", "Shiyu Wu", "Xin Jiang", "Yequan Wang"], "venue": "arXiv.org", "abstract": "We introduce DialogPaint, a novel framework that bridges conversational interactions with image editing, enabling users to modify images through natural dialogue. By integrating a dialogue model with the Stable Diffusion image transformation technique, DialogPaint offers a more intuitive and interactive approach to image modifications. Our method stands out by effectively interpreting and executing both explicit and ambiguous instructions, handling tasks such as object replacement, style transfer, and color modification. Notably, DialogPaint supports iterative, multi-round editing, allowing users to refine image edits over successive interactions. Comprehensive evaluations highlight the robustness and versatility of our approach, marking a significant advancement in dialogue-driven image editing.", "year": 2023, "publicationdate": "2023-03-17", "externalids": {"DOI": "10.48550/arXiv.2303.10073"}, "doi_lower": "10.48550/arxiv.2303.10073"}
{"paper_id": 257985241, "title": "Inst-Inpaint: Instructing to Remove Objects with Diffusion Models", "author_names": ["Ahmet Burak Yildirim", "Vedat Baday", "Erkut Erdem", "Aykut Erdem", "A. Dundar"], "venue": "arXiv.org", "abstract": "Image inpainting task refers to erasing unwanted pixels from images and filling them in a semantically consistent and realistic way. Traditionally, the pixels that are wished to be erased are defined with binary masks. From the application point of view, a user needs to generate the masks for the objects they would like to remove which can be time-consuming and prone to errors. In this work, we are interested in an image inpainting algorithm that estimates which object to be removed based on natural language input and removes it, simultaneously. For this purpose, first, we construct a dataset named GQA-Inpaint for this task. Second, we present a novel inpainting framework, Inst-Inpaint, that can remove objects from images based on the instructions given as text prompts. We set various GAN and diffusion-based baselines and run experiments on synthetic and real image datasets. We compare methods with different evaluation metrics that measure the quality and accuracy of the models and show significant quantitative and qualitative improvements.", "year": 2023, "publicationdate": "2023-04-06", "externalids": {"DOI": "10.48550/arXiv.2304.03246"}, "doi_lower": "10.48550/arxiv.2304.03246"}
{"paper_id": 257622925, "title": "HIVE: Harnessing Human Feedback for Instructional Visual Editing", "author_names": ["Shu Zhang", "Xinyi Yang", "Yihao Feng", "Can Qin", "Chia-Chih Chen", "Ning Yu", "Zeyuan Chen", "Haiquan Wang", "S. Savarese", "Stefano Ermon", "Caiming Xiong", "Ran Xu"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1.1M training dataset, a 3.6K reward dataset for rewards learning, and a 1 K evaluation dataset to boost the performance of instructional image editing. We conduct extensive empirical experiments quantitatively and qualitatively, showing that HIVE is favored over previous state-of-the-art instructional image editing approaches by a large margin.", "year": 2023, "publicationdate": "2023-03-16", "externalids": {"DOI": "10.1109/CVPR52733.2024.00862"}, "doi_lower": "10.1109/cvpr52733.2024.00862"}
{"paper_id": 260379053, "title": "ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation", "author_names": ["Yasheng Sun", "Yifan Yang", "Houwen Peng", "Yifei Shen", "Yuqing Yang", "Hang-Rui Hu", "Lili Qiu", "H. Koike"], "venue": "Neural Information Processing Systems", "abstract": "While language-guided image manipulation has made remarkable progress, the challenge of how to instruct the manipulation process faithfully reflecting human intentions persists. An accurate and comprehensive description of a manipulation task using natural language is laborious and sometimes even impossible, primarily due to the inherent uncertainty and ambiguity present in linguistic expressions. Is it feasible to accomplish image manipulation without resorting to external cross-modal language information? If this possibility exists, the inherent modality gap would be effortlessly eliminated. In this paper, we propose a novel manipulation methodology, dubbed ImageBrush, that learns visual instructions for more accurate image editing. Our key idea is to employ a pair of transformation images as visual instructions, which not only precisely captures human intention but also facilitates accessibility in real-world scenarios. Capturing visual instructions is particularly challenging because it involves extracting the underlying intentions solely from visual demonstrations and then applying this operation to a new image. To address this challenge, we formulate visual instruction learning as a diffusion-based inpainting problem, where the contextual information is fully exploited through an iterative process of generation. A visual prompting encoder is carefully devised to enhance the model's capacity in uncovering human intent behind the visual instructions. Extensive experiments show that our method generates engaging manipulation results conforming to the transformations entailed in demonstrations. Moreover, our model exhibits robust generalization capabilities on various downstream tasks such as pose transfer, image translation and video inpainting.", "year": 2023, "publicationdate": "2023-08-02", "externalids": {"DOI": "10.48550/arXiv.2308.00906"}, "doi_lower": "10.48550/arxiv.2308.00906"}
{"paper_id": 266174467, "title": "InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following", "author_names": ["Shufan Li", "Harkanwar Singh", "Aditya Grover"], "venue": "arXiv.org", "abstract": "The ability to provide fine-grained control for generating and editing visual imagery has profound implications for computer vision and its applications. Previous works have explored extending controllability in two directions: instruction tuning with text-based prompts and multi-modal conditioning. However, these works make one or more unnatural assumptions on the number and/or type of modality inputs used to express controllability. We propose InstructAny2Pix, a flexible multi-modal instruction-following system that enables users to edit an input image using instructions involving audio, images, and text. InstructAny2Pix consists of three building blocks that facilitate this capability: a multi-modal encoder that encodes different modalities such as images and audio into a unified latent space, a diffusion model that learns to decode representations in this latent space into images, and a multi-modal LLM that can understand instructions involving multiple images and audio pieces and generate a conditional embedding of the desired output, which can be used by the diffusion decoder. Additionally, to facilitate training efficiency and improve generation quality, we include an additional refinement prior module that enhances the visual quality of LLM outputs. These designs are critical to the performance of our system. We demonstrate that our system can perform a series of novel instruction-guided editing tasks. The code is available at https://github.com/jacklishufan/InstructAny2Pix.git", "year": 2023, "publicationdate": "2023-12-11", "externalids": {"DOI": "10.48550/arXiv.2312.06738"}, "doi_lower": "10.48550/arxiv.2312.06738"}
{"paper_id": 263310303, "title": "Guiding Instruction-based Image Editing via Multimodal Large Language Models", "author_names": ["Tsu-Jui Fu", "Wenze Hu", "Xianzhi Du", "William Yang Wang", "Yinfei Yang", "Zhe Gan"], "venue": "International Conference on Learning Representations", "abstract": "Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.", "year": 2023, "publicationdate": "2023-09-29", "externalids": {"DOI": "10.48550/arXiv.2309.17102"}, "doi_lower": "10.48550/arxiv.2309.17102"}
{"paper_id": 266174392, "title": "SmartEdit: Exploring Complex Instruction-Based Image Editing with Multimodal Large Language Models", "author_names": ["Yuzhou Huang", "Liangbin Xie", "Xintao Wang", "Ziyang Yuan", "Xiaodong Cun", "Yixiao Ge", "Jiantao Zhou", "Chao Dong", "Rui Huang", "Ruimao Zhang", "Ying Shan"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Current instruction-based image editing methods, such as InstructPix2Pix, often fail to produce satisfactory results in complex scenarios due to their dependence on the simple CLIP text encoder in diffusion models. To rectify this, this paper introduces SmartEdit, a novel approach of instruction-based image editing that leverages Multimodal Large Language Models (MLLMs) to enhance its understanding and reasoning capabilities. However, direct integration of these elements still faces challenges in situations requiring complex reasoning. To mitigate this, we propose a Bidirectional Interaction Module (BIM) that enables comprehensive bidirectional information interactions between the input image and the MLLM output. During training, we initially incorporate perception data to boost the perception and understanding capabilities of diffusion models. Subsequently, we demonstrate that a small amount of complex instruction editing data can effectively stimulate SmartEdit’ s editing capabilities for more complex instructions. We further construct a new evaluation dataset, Reason-Edit, specifically tailored for complex instruction-based image editing. Both quantitative and qualitative results on this evaluation dataset indicate that our SmartEdit surpasses previous methods, paving the way for the practical application of complex instruction-based image editing.", "year": 2023, "publicationdate": "2023-12-11", "externalids": {"DOI": "10.1109/CVPR52733.2024.00799"}, "doi_lower": "10.1109/cvpr52733.2024.00799"}
{"paper_id": 258587826, "title": "iEdit: Localised Text-guided Image Editing with Weak Supervision", "author_names": ["Rumeysa Bodur", "Erhan Gundogdu", "Binod Bhattarai", "Tae-Kyun Kim", "M. Donoser", "Loris Bazzani"], "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "abstract": "Diffusion models (DMs) can generate realistic images with text guidance using large-scale datasets. However, they demonstrate limited controllability on the generated images. We introduce iEdit, a novel method for text-guided image editing conditioned on a source image and textual prompt. As a fully-annotated dataset with target images does not exist, previous approaches perform subject-specific fine-tuning at test time or adopt contrastive learning without a target image, leading to issues on preserving source image fidelity. We propose to automatically construct a dataset derived from LAION-5B, containing pseudo-target images and descriptive edit prompts. The dataset allows us to incorporate a weakly-supervised loss function, generating the pseudo-target image from the source image’s latent noise conditioned on the edit prompt. To encourage localised editing we propose a loss function that uses segmentation masks to guide the editing during training and optionally at inference. Trained with limited GPU resources on the constructed dataset, our model outperforms counterparts in image fidelity, CLIP alignment score, and qualitatively for both generated and real images.", "year": 2023, "publicationdate": "2023-05-10", "externalids": {"DOI": "10.1109/CVPRW63382.2024.00738"}, "doi_lower": "10.1109/cvprw63382.2024.00738"}
{"paper_id": 265466876, "title": "Text-Driven Image Editing via Learnable Regions", "author_names": ["Yuanze Lin", "Yi-Wen Chen", "Yi-Hsuan Tsai", "Lu Jiang", "Ming-Hsuan Yang"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Language has emerged as a natural interface for image editing. In this paper, we introduce a method for region- based image editing driven by textual prompts, without the need for user-provided masks or sketches. Specifically, our approach leverages an existing pre-trained text-to-image model and introduces a bounding box generator to iden-tify the editing regions that are aligned with the textual prompts. We show that this simple approach enables flex-ible editing that is compatible with current image generation models, and is able to handle complex prompts featuring multiple objects, complex sentences, or lengthy para- graphs. We conduct an extensive user study to compare our method against state-of-the-art methods. The experiments demonstrate the competitive performance of our method in manipulating images with high fidelity and realism that correspond to the provided language descriptions. Our project webpage can be found at: https://yuanzelin.me/LearnableRegions_page.", "year": 2023, "publicationdate": "2023-11-28", "externalids": {"DOI": "10.1109/CVPR52733.2024.00674"}, "doi_lower": "10.1109/cvpr52733.2024.00674"}
{"paper_id": 258865947, "title": "ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation", "author_names": ["Dongxu Yue", "Qin Guo", "Munan Ning", "Jiaxi Cui", "Yuesheng Zhu", "Liuliang Yuan"], "venue": "arXiv.org", "abstract": "Editing real facial images is a crucial task in computer vision with significant demand in various real-world applications. While GAN-based methods have showed potential in manipulating images especially when combined with CLIP, these methods are limited in their ability to reconstruct real images due to challenging GAN inversion capability. Despite the successful image reconstruction achieved by diffusion-based methods, there are still challenges in effectively manipulating fine-gained facial attributes with textual instructions.To address these issues and facilitate convenient manipulation of real facial images, we propose a novel approach that conduct text-driven image editing in the semantic latent space of diffusion model. By aligning the temporal feature of the diffusion model with the semantic condition at generative process, we introduce a stable manipulation strategy, which perform precise zero-shot manipulation effectively. Furthermore, we develop an interactive system named ChatFace, which combines the zero-shot reasoning ability of large language models to perform efficient manipulations in diffusion semantic latent space. This system enables users to perform complex multi-attribute manipulations through dialogue, opening up new possibilities for interactive image editing. Extensive experiments confirmed that our approach outperforms previous methods and enables precise editing of real facial images, making it a promising candidate for real-world applications. Project page: https://dongxuyue.github.io/chatface/", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.14742"}, "doi_lower": "10.48550/arxiv.2305.14742"}
{"paper_id": 252968124, "title": "UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image", "author_names": ["Dani Valevski", "Matan Kalman", "Yossi Matias", "Yaniv Leviathan"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2210.09477"}, "doi_lower": "10.48550/arxiv.2210.09477"}
{"paper_id": 258888143, "title": "Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models", "author_names": ["Jooyoung Choi", "Yunjey Choi", "Yunji Kim", "Junho Kim", "Sung-Hoon Yoon"], "venue": "arXiv.org", "abstract": "Text-to-image diffusion models can generate diverse, high-fidelity images based on user-provided text prompts. Recent research has extended these models to support text-guided image editing. While text guidance is an intuitive editing interface for users, it often fails to ensure the precise concept conveyed by users. To address this issue, we propose Custom-Edit, in which we (i) customize a diffusion model with a few reference images and then (ii) perform text-guided editing. Our key discovery is that customizing only language-relevant parameters with augmented prompts improves reference similarity significantly while maintaining source similarity. Moreover, we provide our recipe for each customization and editing process. We compare popular customization methods and validate our findings on two editing methods using various datasets.", "year": 2023, "publicationdate": "2023-05-25", "externalids": {"DOI": "10.48550/arXiv.2305.15779"}, "doi_lower": "10.48550/arxiv.2305.15779"}
{"paper_id": 263134058, "title": "KV Inversion: KV Embeddings Learning for Text-Conditioned Real Image Action Editing", "author_names": ["Jiancheng Huang", "Yifan Liu", "Jin Qin", "Shifeng Chen"], "venue": "Chinese Conference on Pattern Recognition and Computer Vision", "abstract": "Text-conditioned image editing is a recently emerged and highly practical task, and its potential is immeasurable. However, most of the concurrent methods are unable to perform action editing, i.e. they can not produce results that conform to the action semantics of the editing prompt and preserve the content of the original image. To solve the problem of action editing, we propose KV Inversion, a method that can achieve satisfactory reconstruction performance and action editing, which can solve two major problems: 1) the edited result can match the corresponding action, and 2) the edited object can retain the texture and identity of the original real image. In addition, our method does not require training the Stable Diffusion model itself, nor does it require scanning a large-scale dataset to perform time-consuming training.", "year": 2023, "publicationdate": "2023-09-28", "externalids": {"DOI": "10.48550/arXiv.2309.16608"}, "doi_lower": "10.48550/arxiv.2309.16608"}
{"paper_id": 253581838, "title": "Supplementary Materials for: NULL-text Inversion for Editing Real Images using Guided Diffusion Models", "author_names": ["Ron Mokady", "Amir Hertz", "Kfir Aberman", "Y. Pritch", "D. Cohen-Or"], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 263151874, "title": "Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing", "author_names": ["Kai Wang", "Fei Yang", "Shiqi Yang", "Muhammad Atif Butt", "Joost van de Weijer"], "venue": "Neural Information Processing Systems", "abstract": "Large-scale text-to-image generative models have been a ground-breaking development in generative AI, with diffusion models showing their astounding ability to synthesize convincing images following an input text prompt. The goal of image editing research is to give users control over the generated images by modifying the text prompt. Current image editing techniques are susceptible to unintended modifications of regions outside the targeted area, such as on the background or on distractor objects which have some semantic or visual relationship with the targeted object. According to our experimental findings, inaccurate cross-attention maps are at the root of this problem. Based on this observation, we propose Dynamic Prompt Learning (DPL) to force cross-attention maps to focus on correct noun words in the text prompt. By updating the dynamic tokens for nouns in the textual input with the proposed leakage repairment losses, we achieve fine-grained image editing over particular objects while preventing undesired changes to other image regions. Our method DPL, based on the publicly available Stable Diffusion, is extensively evaluated on a wide range of images, and consistently obtains superior results both quantitatively (CLIP score, Structure-Dist) and qualitatively (on user-evaluation). We show improved prompt editing results for Word-Swap, Prompt Refinement, and Attention Re-weighting, especially for complex multi-object scenes.", "year": 2023, "publicationdate": "2023-09-27", "externalids": {"DOI": "10.48550/arXiv.2309.15664"}, "doi_lower": "10.48550/arxiv.2309.15664"}
{"paper_id": 254854155, "title": "Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models", "author_names": ["Qiucheng Wu", "Yujian Liu", "Handong Zhao", "Ajinkya Kale", "T. Bui", "Tong Yu", "Zhe Lin", "Yang Zhang", "Shiyu Chang"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Generative models have been widely studied in computer vision. Recently, diffusion models have drawn substantial attention due to the high quality of their generated images. A key desired property of image generative models is the ability to disentangle different attributes, which should enable modification towards a style without changing the semantic content, and the modification parameters should generalize to different images. Previous studies have found that generative adversarial networks (GANs) are inherently endowed with such disentanglement capability, so they can perform disentangled image editing without re-training or fine-tuning the network. In this work, we explore whether diffusion models are also inherently equipped with such a capability. Our finding is that for stable diffusion models, by partially changing the input text embedding from a neutral description (e.g., “a photo of person”) to one with style (e.g., “a photo of person with smile”) while fixing all the Gaussian random noises introduced during the denoising process, the generated images can be modified towards the target style without changing the semantic content. Based on this finding, we further propose a simple, light-weight image editing algorithm where the mixing weights of the two text embeddings are optimized for style matching and content preservation. This entire process only involves optimizing over around 50 parameters and does not fine-tune the diffusion model itself. Experiments show that the proposed method can modify a wide range of attributes, with the performance outperforming diffusion-model-based image-editing algorithms that require fine-tuning. The optimized weights generalize well to different images. Our code is publicly available at https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement.", "year": 2022, "publicationdate": "2022-12-16", "externalids": {"DOI": "10.1109/CVPR52729.2023.00189"}, "doi_lower": "10.1109/cvpr52729.2023.00189"}
{"paper_id": 258556958, "title": "Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models", "author_names": ["Wenkai Dong", "Song Xue", "Xiaoyue Duan", "Shumin Han"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Recently large-scale language-image models (e.g., text-guided diffusion models) have considerably improved the image generation capabilities to generate photorealistic images in various domains. Based on this success, current image editing methods use texts to achieve intuitive and versatile modification of images. To edit a real image using diffusion models, one must first invert the image to a noisy latent from which an edited image is sampled with a target text prompt. However, most methods lack one of the following: user-friendliness (e.g., additional masks or precise descriptions of the input image are required), generalization to larger domains, or high fidelity to the input image. In this paper, we design an accurate and quick inversion technique, Prompt Tuning Inversion, for text-driven image editing. Specifically, our proposed editing method consists of a reconstruction stage and an editing stage. In the first stage, we encode the information of the input image into a learnable conditional embedding via Prompt Tuning Inversion. In the second stage, we apply classifier-free guidance to sample the edited image, where the conditional embedding is calculated by linearly interpolating between the target embedding and the optimized one obtained in the first stage. This technique ensures a superior trade-off between editability and high fidelity to the input image of our method. For example, we can change the color of a specific object while preserving its original shape and background under the guidance of only a target text prompt. Extensive experiments on ImageNet demonstrate the superior editing performance of our method compared to the state-of-the-art baselines.", "year": 2023, "publicationdate": "2023-05-08", "externalids": {"DOI": "10.1109/ICCV51070.2023.00683"}, "doi_lower": "10.1109/iccv51070.2023.00683"}
{"paper_id": 257771440, "title": "StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing", "author_names": ["Senmao Li", "Joost van de Weijer", "Taihang Hu", "F. Khan", "Qibin Hou", "Yaxing Wang", "Jian Yang"], "venue": "arXiv.org", "abstract": "A significant research effort is focused on exploiting the amazing capacities of pretrained diffusion models for the editing of images.They either finetune the model, or invert the image in the latent space of the pretrained model. However, they suffer from two problems: (1) Unsatisfying results for selected regions and unexpected changes in non-selected regions.(2) They require careful text prompt editing where the prompt should include all visual objects in the input image.To address this, we propose two improvements: (1) Only optimizing the input of the value linear network in the cross-attention layers is sufficiently powerful to reconstruct a real image. (2) We propose attention regularization to preserve the object-like attention maps after reconstruction and editing, enabling us to obtain accurate style editing without invoking significant structural changes. We further improve the editing technique that is used for the unconditional branch of classifier-free guidance as used by P2P. Extensive experimental prompt-editing results on a variety of images demonstrate qualitatively and quantitatively that our method has superior editing capabilities compared to existing and concurrent works. See our accompanying code in Stylediffusion: \\url{https://github.com/sen-mao/StyleDiffusion}.", "year": 2023, "publicationdate": "2023-03-28", "externalids": {"DOI": "10.26599/CVM.2025.9450462"}, "doi_lower": "10.26599/cvm.2025.9450462"}
{"paper_id": 257427673, "title": "Inversion-based Style Transfer with Diffusion Models", "author_names": ["Yu-xin Zhang", "Nisha Huang", "Fan Tang", "Haibin Huang", "Chongyang Ma", "Weiming Dong", "Changsheng Xu"], "venue": "Computer Vision and Pattern Recognition", "abstract": "The artistic style within a painting is the means of expression, which includes not only the painting material, colors, and brushstrokes, but also the high-level attributes, including semantic elements and object shapes. Previous arbitrary example-guided artistic image generation methods often fail to control shape changes or convey elements. Pre-trained text-to-image synthesis diffusion probabilistic models have achieved remarkable quality but often require extensive textual descriptions to accurately portray the attributes of a particular painting. The uniqueness of an artwork lies in the fact that it cannot be adequately explained with normal language. Our key idea is to learn the artistic style directly from a single painting and then guide the synthesis without providing complex textual descriptions. Specifically, we perceive style as a learnable textual description of a painting. We propose an inversion-based style transfer method (InST), which can efficiently and accurately learn the key information of an image, thus capturing and transferring the artistic style of a painting. We demonstrate the quality and efficiency of our method on numerous paintings of various artists and styles. Codes are available at https://github.com/zyxElsa/InST.", "year": 2022, "publicationdate": "2022-11-23", "externalids": {"DOI": "10.1109/CVPR52729.2023.00978"}, "doi_lower": "10.1109/cvpr52729.2023.00978"}
{"paper_id": 259342813, "title": "DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models", "author_names": ["Chong Mou", "Xintao Wang", "Jie Song", "Ying Shan", "Jian Zhang"], "venue": "International Conference on Learning Representations", "abstract": "Despite the ability of existing large-scale text-to-image (T2I) models to generate high-quality images from detailed textual descriptions, they often lack the ability to precisely edit the generated or real images. In this paper, we propose a novel image editing method, DragonDiffusion, enabling Drag-style manipulation on Diffusion models. Specifically, we construct classifier guidance based on the strong correspondence of intermediate features in the diffusion model. It can transform the editing signals into gradients via feature correspondence loss to modify the intermediate representation of the diffusion model. Based on this guidance strategy, we also build a multi-scale guidance to consider both semantic and geometric alignment. Moreover, a cross-branch self-attention is added to maintain the consistency between the original image and the editing result. Our method, through an efficient design, achieves various editing modes for the generated or real images, such as object moving, object resizing, object appearance replacement, and content dragging. It is worth noting that all editing and content preservation signals come from the image itself, and the model does not require fine-tuning or additional modules. Our source code will be available at https://github.com/MC-E/DragonDiffusion.", "year": 2023, "publicationdate": "2023-07-05", "externalids": {"DOI": "10.48550/arXiv.2307.02421"}, "doi_lower": "10.48550/arxiv.2307.02421"}
{"paper_id": 259252555, "title": "DragDiffusion: Harnessing Diffusion Models for Interactive Point-Based Image Editing", "author_names": ["Yujun Shi", "Chuhui Xue", "Jiachun Pan", "Wenqing Zhang", "Vincent Y. F. Tan", "Song Bai"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Accurate and controllable image editing is a challenging task that has attracted significant attention recently. Notably, DRAGGAN developed by Pan et al. (2023) [33] is an interactive point-based image editing framework that achieves impressive editing results with pixel-level precision. However, due to its reliance on generative adversarial networks (GANs), its generality is limited by the capacity of pretrained GAN models. In this work, we extend this editing framework to diffusion models and propose a novel approach Dragdiffusion. By harnessing large-scale pretrained diffusion models, we greatly enhance the applicability of interactive point-based editing on both real and diffusion-generated images. Unlike other diffusion-based editing methods that provide guidance on diffusion latents of multiple time steps, our approach achieves efficient yet accurate spatial control by optimizing the latent of only one time step. This novel design is motivated by our observations that UNet features at a specific time step provides sufficient semantic and geometric information to support the drag-based editing. Moreover, we introduce two additional techniques, namely identity-preserving fine-tuning and reference-latent-control, to further preserve the identity of the original image. Lastly, we present a challenging benchmark dataset called DRAGBENCH─ the first benchmark to evaluate the performance of interactive point-based image editing methods. Experiments across a wide range of challenging cases (e.g., images with multiple objects, diverse object categories, various styles, etc.) demonstrate the versatility and generality of Dragdiffusion. Code and the Dragbench dataset: https://github.com/Yujun-Shi/DragDiffusion.", "year": 2023, "publicationdate": "2023-06-26", "externalids": {"DOI": "10.1109/CVPR52733.2024.00844"}, "doi_lower": "10.1109/cvpr52733.2024.00844"}
{"paper_id": 258170014, "title": "Delta Denoising Score", "author_names": ["Amir Hertz", "Kfir Aberman", "D. Cohen-Or"], "venue": "IEEE International Conference on Computer Vision", "abstract": "This paper introduces Delta Denoising Score (DDS) a novel diffusion-based scoring technique which optimizes a parametric model for the task of image editing. Unlike the existing Score Distillation Sampling (SDS), which queries the generative model with a single image-text pair, DDS utilizes an additional fixed query of a reference image-text pair to generate delta scores that represent the difference between the outputs of the two queries. By estimating noisy gradient directions introduced by SDS using the source image and its text description, DDS provides cleaner gradient directions that modify the edited portions of the image while leaving others unchanged, thereby yielding a distilled edit of the source image. The analysis presented in this paper supports the power of the new score for image-to-image translation. We further show that the new score can be used to train an effective zero-shot image translation model. The experimental results show that the proposed loss term outperforms existing methods in terms of stability and quality, highlighting its potential for real-world applications.", "year": 2023, "publicationdate": "2023-04-14", "externalids": {"DOI": "10.1109/ICCV51070.2023.00221"}, "doi_lower": "10.1109/iccv51070.2023.00221"}
{"paper_id": 252668838, "title": "Diffusion-based Image Translation using Disentangled Style and Content Representation", "author_names": ["Gihyun Kwon", "Jong-Chul Ye"], "venue": "International Conference on Learning Representations", "abstract": "Diffusion-based image translation guided by semantic texts or a single target image has enabled flexible style transfer which is not limited to the specific domains. Unfortunately, due to the stochastic nature of diffusion models, it is often difficult to maintain the original content of the image during the reverse diffusion. To address this, here we present a novel diffusion-based unsupervised image translation method using disentangled style and content representation. Specifically, inspired by the splicing Vision Transformer, we extract intermediate keys of multihead self attention layer from ViT model and used them as the content preservation loss. Then, an image guided style transfer is performed by matching the [CLS] classification token from the denoised samples and target image, whereas additional CLIP loss is used for the text-driven style transfer. To further accelerate the semantic change during the reverse diffusion, we also propose a novel semantic divergence loss and resampling strategy. Our experimental results show that the proposed method outperforms state-of-the-art baseline models in both text-guided and image-guided translation tasks.", "year": 2022, "publicationdate": "2022-09-30", "externalids": {"DOI": "10.48550/arXiv.2209.15264"}, "doi_lower": "10.48550/arxiv.2209.15264"}
{"paper_id": 265506264, "title": "Contrastive Denoising Score for Text-Guided Latent Diffusion Image Editing", "author_names": ["Hyelin Nam", "Gihyun Kwon", "Geon Yeong Park", "Jong Chul Ye"], "venue": "Computer Vision and Pattern Recognition", "abstract": "With the remarkable advent of text-to-image diffusion models, image editing methods have become more diverse and continue to evolve. A promising recent approach in this realm is Delta Denoising Score (DDS) - an image editing technique based on Score Distillation Sampling (SDS) framework that leverages the rich generative prior of text-to-image diffusion models. However, relying solely on the difference between scoring functions is insufficient for preserving specific structural elements from the original image, a crucial aspect of image editing. To address this, here we present an embarrassingly simple yet very powerful modification of DDS, called Contrastive Denoising Score (CDS), for latent diffusion models (LDM). Inspired by the similarities and differences between DDS and the contrastive learning for unpaired image-to-image translation(CUT), we introduce a straightforward approach using CUT loss within the DDS framework. Rather than employing auxiliary networks as in the original CUT approach, we leverage the intermediate features of LDM, specifically those from the self-attention layers, which possesses rich spatial information. Our approach enables zero-shot image-to-image translation and neural radiance field (NeRF) editing, achieving structural correspondence between the input and output while maintaining content controllability. Qualitative results and comparisons demonstrates the effectiveness of our proposed method. Project page: https://hyelinnam.github.io/CDS/", "year": 2023, "publicationdate": "2023-11-30", "externalids": {"DOI": "10.1109/CVPR52733.2024.00878"}, "doi_lower": "10.1109/cvpr52733.2024.00878"}
{"paper_id": 263620873, "title": "Magicremover: Tuning-free Text-guided Image inpainting with Diffusion Models", "author_names": ["Si-hang Yang", "Lu Zhang", "Liqian Ma", "Yu Liu", "JingJing Fu", "You He"], "venue": "arXiv.org", "abstract": "Image inpainting aims to fill in the missing pixels with visually coherent and semantically plausible content. Despite the great progress brought from deep generative models, this task still suffers from i. the difficulties in large-scale realistic data collection and costly model training; and ii. the intrinsic limitations in the traditionally user-defined binary masks on objects with unclear boundaries or transparent texture. In this paper, we propose MagicRemover, a tuning-free method that leverages the powerful diffusion models for text-guided image inpainting. We introduce an attention guidance strategy to constrain the sampling process of diffusion models, enabling the erasing of instructed areas and the restoration of occluded content. We further propose a classifier optimization algorithm to facilitate the denoising stability within less sampling steps. Extensive comparisons are conducted among our MagicRemover and state-of-the-art methods including quantitative evaluation and user study, demonstrating the significant improvement of MagicRemover on high-quality image inpainting. We will release our code at https://github.com/exisas/Magicremover.", "year": 2023, "publicationdate": "2023-10-04", "externalids": {"DOI": "10.48550/arXiv.2310.02848"}, "doi_lower": "10.48550/arxiv.2310.02848"}
{"paper_id": 252918469, "title": "Imagic: Text-Based Real Image Editing with Diffusion Models", "author_names": ["Bahjat Kawar", "Shiran Zada", "Oran Lang", "Omer Tov", "Hui-Tang Chang", "Tali Dekel", "Inbar Mosseri", "M. Irani"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently limited to one of the following: specific editing types (e.g., object overlay, style transfer), synthetically generated images, or requiring multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-based semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down, cause a bird to spread its wings, etc. – each within its single high-resolution user-provided natural image. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, called Imagic, leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of Imagic on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework. To better assess performance, we introduce TEdBench, a highly challenging image editing benchmark. We conduct a user study, whose findings show that human raters prefer Imagic to previous leading editing methods on TEdBench.", "year": 2022, "publicationdate": "2022-10-17", "externalids": {"DOI": "10.1109/CVPR52729.2023.00582"}, "doi_lower": "10.1109/cvpr52729.2023.00582"}
{"paper_id": 262055174, "title": "Forgedit: Text Guided Image Editing via Learning and Forgetting", "author_names": ["Shiwen Zhang", "Shuai Xiao", "Weilin Huang"], "venue": "arXiv.org", "abstract": "Text-guided image editing on real or synthetic images, given only the original image itself and the target text prompt as inputs, is a very general and challenging task. It requires an editing model to estimate by itself which part of the image should be edited, and then perform either rigid or non-rigid editing while preserving the characteristics of original image. In this paper, we design a novel text-guided image editing method, named as Forgedit. First, we propose a vision-language joint optimization framework capable of reconstructing the original image in 30 seconds, much faster than previous SOTA and much less overfitting. Then we propose a novel vector projection mechanism in text embedding space of Diffusion Models, which is capable to control the identity similarity and editing strength seperately. Finally, we discovered a general property of UNet in Diffusion Models, i.e., Unet encoder learns space and structure, Unet decoder learns appearance and identity. With such a property, we design forgetting mechanisms to successfully tackle the fatal and inevitable overfitting issues when fine-tuning Diffusion Models on one image, thus significantly boosting the editing capability of Diffusion Models. Our method, Forgedit, built on Stable Diffusion, achieves new state-of-the-art results on the challenging text-guided image editing benchmark: TEdBench, surpassing the previous SOTA methods such as Imagic with Imagen, in terms of both CLIP score and LPIPS score. Codes are available at https://github.com/witcherofresearch/Forgedit", "year": 2023, "publicationdate": "2023-09-19", "externalids": {"DOI": "10.48550/arXiv.2309.10556"}, "doi_lower": "10.48550/arxiv.2309.10556"}
{"paper_id": 254408758, "title": "SINE: SINgle Image Editing with Text-to-Image Diffusion Models", "author_names": ["Zhixing Zhang", "Ligong Han", "Arna Ghosh", "Dimitris N. Metaxas", "Jian Ren"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Recent works on diffusion models have demonstrated a strong capability for conditioning image generation, e.g., text-guided image synthesis. Such success inspires many efforts trying to use large-scale pre-trained diffusion models for tackling a challenging problem-real image editing. Works conducted in this area learn a unique textual token corresponding to several images containing the same object. However, under many circumstances, only one image is available, such as the painting of the Girl with a Pearl Earring. Using existing works on fine-tuning the pre-trained diffusion models with a single image causes severe overfitting issues. The information leakage from the pre-trained diffusion models makes editing can not keep the same content as the given image while creating new features depicted by the language guidance. This work aims to address the problem of single-image editing. We propose a novel model-based guidance built upon the classifier-free guidance so that the knowledge from the model trained on a single image can be distilled into the pre-trained diffusion model, enabling content creation even with one given image. Additionally, we propose a patch-based fine-tuning that can effectively help the model generate images of arbitrary resolution. We provide extensive experiments to validate the design choices of our approach and show promising editing capabilities, including changing style, content addition, and object manipulation. Our code is made publicly available here.", "year": 2022, "publicationdate": "2022-12-08", "externalids": {"DOI": "10.1109/CVPR52729.2023.00584"}, "doi_lower": "10.1109/cvpr52729.2023.00584"}
{"paper_id": 258557880, "title": "ReGeneration Learning of Diffusion Models with Rich Prompts for Zero-Shot Image Translation", "author_names": ["Yupei Lin", "Senyang Zhang", "Xiaojun Yang", "Xiao Wang", "Yukai Shi"], "venue": "arXiv.org", "abstract": "Large-scale text-to-image models have demonstrated amazing ability to synthesize diverse and high-fidelity images. However, these models are often violated by several limitations. Firstly, they require the user to provide precise and contextually relevant descriptions for the desired image modifications. Secondly, current models can impose significant changes to the original image content during the editing process. In this paper, we explore ReGeneration learning in an image-to-image Diffusion model (ReDiffuser), that preserves the content of the original image without human prompting and the requisite editing direction is automatically discovered within the text embedding space. To ensure consistent preservation of the shape during image editing, we propose cross-attention guidance based on regeneration learning. This novel approach allows for enhanced expression of the target domain features while preserving the original shape of the image. In addition, we introduce a cooperative update strategy, which allows for efficient preservation of the original shape of an image, thereby improving the quality and consistency of shape preservation throughout the editing process. Our proposed method leverages an existing pre-trained text-image diffusion model without any additional training. Extensive experiments show that the proposed method outperforms existing work in both real and synthetic image editing.", "year": 2023, "publicationdate": "2023-05-08", "externalids": {"DOI": "10.48550/arXiv.2305.04651"}, "doi_lower": "10.48550/arxiv.2305.04651"}
{"paper_id": 259076409, "title": "User-friendly Image Editing with Minimal Text Input: Leveraging Captioning and Injection Techniques", "author_names": ["Sunwoo Kim", "Wooseok Jang", "Hyunsung Kim", "Junho Kim", "Yunjey Choi", "Seung Wook Kim", "Gayeong Lee"], "venue": "arXiv.org", "abstract": "Recent text-driven image editing in diffusion models has shown remarkable success. However, the existing methods assume that the user's description sufficiently grounds the contexts in the source image, such as objects, background, style, and their relations. This assumption is unsuitable for real-world applications because users have to manually engineer text prompts to find optimal descriptions for different images. From the users' standpoint, prompt engineering is a labor-intensive process, and users prefer to provide a target word for editing instead of a full sentence. To address this problem, we first demonstrate the importance of a detailed text description of the source image, by dividing prompts into three categories based on the level of semantic details. Then, we propose simple yet effective methods by combining prompt generation frameworks, thereby making the prompt engineering process more user-friendly. Extensive qualitative and quantitative experiments demonstrate the importance of prompts in text-driven image editing and our method is comparable to ground-truth prompts.", "year": 2023, "publicationdate": "2023-06-05", "externalids": {"DOI": "10.48550/arXiv.2306.02717"}, "doi_lower": "10.48550/arxiv.2306.02717"}
{"paper_id": 258959425, "title": "InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions", "author_names": ["Qian Wang", "Biao Zhang", "Michael Birsak", "Peter Wonka"], "venue": "arXiv.org", "abstract": "Recent works have explored text-guided image editing using diffusion models and generated edited images based on text prompts. However, the models struggle to accurately locate the regions to be edited and faithfully perform precise edits. In this work, we propose a framework termed InstructEdit that can do fine-grained editing based on user instructions. Our proposed framework has three components: language processor, segmenter, and image editor. The first component, the language processor, processes the user instruction using a large language model. The goal of this processing is to parse the user instruction and output prompts for the segmenter and captions for the image editor. We adopt ChatGPT and optionally BLIP2 for this step. The second component, the segmenter, uses the segmentation prompt provided by the language processor. We employ a state-of-the-art segmentation framework Grounded Segment Anything to automatically generate a high-quality mask based on the segmentation prompt. The third component, the image editor, uses the captions from the language processor and the masks from the segmenter to compute the edited image. We adopt Stable Diffusion and the mask-guided generation from DiffEdit for this purpose. Experiments show that our method outperforms previous editing methods in fine-grained editing applications where the input image contains a complex object or multiple objects. We improve the mask quality over DiffEdit and thus improve the quality of edited images. We also show that our framework can accept multiple forms of user instructions as input. We provide the code at https://github.com/QianWangX/InstructEdit.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {"DOI": "10.48550/arXiv.2305.18047"}, "doi_lower": "10.48550/arxiv.2305.18047"}
{"paper_id": 258108162, "title": "An Edit Friendly DDPM Noise Space: Inversion and Manipulations", "author_names": ["Inbar Huberman-Spiegelglas", "V. Kulikov", "T. Michaeli"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Denoising diffusion probabilistic models (DDPMs) employ a sequence of white Gaussian noise samples to generate an image. In analogy with GANs, those noise maps could be considered as the latent code associated with the generated image. However, this native noise space does not possess a convenient structure, and is thus challenging to work with in editing tasks. Here, we propose an alternative latent noise space for DDPM that enables a wide range of editing operations via simple means, and present an inversion method for extracting these edit-friendly noise maps for any given image (real or synthetically generated). As opposed to the native DDPM noise space, the edit-friendly noise maps do not have a standard normal distribution and are not statistically independent across timesteps. However, they allow perfect reconstruction of any desired image, and simple transformations on them translate into meaningful manipulations of the output image (e.g. shifting, color edits). Moreover, in text-conditional models, fixing those noise maps while changing the text prompt, modifies semantics while retaining structure. We illustrate how this property enables text-based editing of real images via the diverse DDPM sampling scheme (in contrast to the popular non-diverse DDIM inversion). We also show how it can be used within existing diffusion-based editing methods to improve their quality and diversity. The code of the method is attached to this submission.", "year": 2023, "publicationdate": "2023-04-12", "externalids": {"DOI": "10.1109/CVPR52733.2024.01185"}, "doi_lower": "10.1109/cvpr52733.2024.01185"}
{"paper_id": 264935182, "title": "The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing", "author_names": ["Shen Nie", "Hanzhong Guo", "Cheng Lu", "Yuhao Zhou", "Chenyu Zheng", "Chongxuan Li"], "venue": "International Conference on Learning Representations", "abstract": "We present a unified probabilistic formulation for diffusion-based image editing, where a latent variable is edited in a task-specific manner and generally deviates from the corresponding marginal distribution induced by the original stochastic or ordinary differential equation (SDE or ODE). Instead, it defines a corresponding SDE or ODE for editing. In the formulation, we prove that the Kullback-Leibler divergence between the marginal distributions of the two SDEs gradually decreases while that for the ODEs remains as the time approaches zero, which shows the promise of SDE in image editing. Inspired by it, we provide the SDE counterparts for widely used ODE baselines in various tasks including inpainting and image-to-image translation, where SDE shows a consistent and substantial improvement. Moreover, we propose SDE-Drag -- a simple yet effective method built upon the SDE formulation for point-based content dragging. We build a challenging benchmark (termed DragBench) with open-set natural, art, and AI-generated images for evaluation. A user study on DragBench indicates that SDE-Drag significantly outperforms our ODE baseline, existing diffusion-based methods, and the renowned DragGAN. Our results demonstrate the superiority and versatility of SDE in image editing and push the boundary of diffusion-based editing methods.", "year": 2023, "publicationdate": "2023-11-02", "externalids": {"DOI": "10.48550/arXiv.2311.01410"}, "doi_lower": "10.48550/arxiv.2311.01410"}
{"paper_id": 265466786, "title": "LEDITS++: Limitless Image Editing Using Text-to-Image Models", "author_names": ["Manuel Brack", "Felix Friedrich", "Katharina Kornmeier", "Linoy Tsaban", "P. Schramowski", "K. Kersting", "Apolin'ario Passos"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Text-to-image diffusion models have recently received increasing interest for their astonishing ability to produce high-fidelity images from solely text inputs. Subsequent research efforts aim to exploit and apply their capabilities to real image editing. However, existing image-to-image methods are often inefficient, imprecise, and of limited versatility. They either require time-consuming fine-tuning, deviate unnecessarily strongly from the input image, and/or lack support for multiple, simultaneous edits. To address these issues, we introduce LEdits++, an efficient yet versatile and precise textual image manipulation technique. LEdits++'s novel inversion approach requires no tuning nor optimization and produces high-fidelity results with a few diffusion steps. Second, our methodology supports multiple simultaneous edits and is architecture-agnostic. Third, we use a novel implicit masking technique that limits changes to relevant image regions. We propose the novel TEdBench++ benchmark as part of our exhaustive evaluation. Our results demonstrate the capabilities of LEdits++ and its improvements over previous methods.", "year": 2023, "publicationdate": "2023-11-28", "externalids": {"DOI": "10.1109/CVPR52733.2024.00846"}, "doi_lower": "10.1109/cvpr52733.2024.00846"}
{"paper_id": 261494286, "title": "Iterative Multi-granular Image Editing using Diffusion Models", "author_names": ["K. J. Joseph", "Prateksha Udhayanan", "Tripti Shukla", "Aishwarya Agarwal", "S. Karanam", "Koustava Goswami", "Balaji Vasan Srinivasan", "Adobe Research"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "Recent advances in text-guided image synthesis has dramatically changed how creative professionals generate artistic and aesthetically pleasing visual assets. To fully support such creative endeavors, the process should possess the ability to: 1) iteratively edit the generations and 2) control the spatial reach of desired changes (global, local or anything in between). We formalize this pragmatic problem setting as Iterative Multi-granular Editing. While there has been substantial progress with diffusion-based models for image synthesis and editing, they are all one shot (i.e., no iterative editing capabilities) and do not naturally yield multi-granular control (i.e., covering the full spectrum of local-to-global edits). To overcome these drawbacks, we propose EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent iteration strategy, which re-purposes a pre-trained diffusion model to facilitate iterative editing. This is complemented by a gradient control operation for multi-granular control. We introduce a new benchmark dataset to evaluate our newly proposed setting. We conduct exhaustive quantitatively and qualitatively evaluation against recent state-of-the-art approaches adapted to our task, to being out the mettle of EMILIE. We hope our work would attract attention to this newly identified, pragmatic problem setting.", "year": 2023, "publicationdate": "2023-09-01", "externalids": {"DOI": "10.1109/WACV57701.2024.00792"}, "doi_lower": "10.1109/wacv57701.2024.00792"}
{"paper_id": 258947366, "title": "Negative-Prompt Inversion: Fast Image Inversion for Editing with Text-Guided Diffusion Models", "author_names": ["Daiki Miyake", "Akihiro Iohara", "Yuriko Saito", "Toshiyuki TANAKA"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "In image editing employing diffusion models, it is crucial to preserve the reconstruction fidelity to the original image while changing its style. Although existing methods ensure reconstruction fidelity through optimization, a drawback of these is the significant amount of time required for optimization. In this paper, we propose negative-prompt inversion, a method capable of achieving equivalent reconstruction solely through forward propagation without optimization, thereby enabling ultrafast editing processes. We experimentally demonstrate that the reconstruction fidelity of our method is comparable to that of existing methods, allowing for inversion at a resolution of 512 pixels and with 50 sampling steps within approximately 5 seconds, which is more than 30 times faster than null-text inversion. Reduction of the computation time by the proposed method further allows us to use a larger number of sampling steps in diffusion models to improve the reconstruction fidelity with a moderate increase in computation time.", "year": 2023, "publicationdate": "2023-05-26", "externalids": {"DOI": "10.1109/WACV61041.2025.00207"}, "doi_lower": "10.1109/wacv61041.2025.00207"}
{"paper_id": 259287564, "title": "ProxEdit: Improving Tuning-Free Real Image Editing with Proximal Guidance", "author_names": ["Ligong Han", "Song Wen", "Qi Chen", "Zhixing Zhang", "Kunpeng Song", "Mengwei Ren", "Ruijiang Gao", "Yuxiao Chen", "Ding Liu", "Qilong Zhangli", "Anastasis Stathopoulos", "Xiaoxiao He", "Jindong Jiang", "Zhaoyang Xia", "Akash Srivastava", "Dimitris N. Metaxas"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "DDIM inversion has revealed the remarkable potential of real image editing within diffusion-based methods. However, the accuracy of DDIM reconstruction degrades as larger classifier-free guidance (CFG) scales being used for enhanced editing. Null-text inversion (NTI) optimizes null embeddings to align the reconstruction and inversion trajectories with larger CFG scales, enabling real image editing with cross-attention control. Negative-prompt inversion (NPI) further offers a training-free closed-form solution of NTI. However, it may introduce artifacts and is still constrained by DDIM reconstruction quality. To overcome these limitations, we propose proximal guidance and incorporate it to NPI with cross-attention control. We enhance NPI with a regularization term and inversion guidance, which reduces artifacts while capitalizing on its training-free nature. Additionally, we extend the concepts to incorporate mutual self-attention control, enabling geometry and layout alterations in the editing process. Our method provides an efficient and straightforward approach, effectively addressing real image editing tasks with minimal computational overhead.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.1109/WACV57701.2024.00424"}, "doi_lower": "10.1109/wacv57701.2024.00424"}
{"paper_id": 258615416, "title": "Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator", "author_names": ["Jing Zhao", "Heliang Zheng", "Chaoyue Wang", "Long Lan", "Wanrong Huang", "Wenjing Yang"], "venue": "ACM Multimedia", "abstract": "Classifier-free guidance is an effective sampling technique in diffusion models that has been widely adopted. The main idea is to extrapolate the model in the direction of text guidance and away from null-text guidance. In this paper, we demonstrate that null-text guidance in diffusion models is secretly a cartoon-style creator, i.e., the generated images can be efficiently transformed into cartoons by simply perturbing the null-text guidance. Specifically, we proposed two disturbance methods, i.e., Rollback disturbance (Back-D) and Image disturbance (Image-D), to construct misalignment between the noisy images used for predicting null-text guidance and text guidance (subsequently referred to as null-text noisy image and text noisy imageb respectively) in the sampling process. Back-D achieves cartoonization by altering the noisb level of the null-text noisy image via replacing xt with xl + Δ t. Image-D, alternatively, produces high-fidelity, diverse cartoons by defining xt as a clean input image, which further improves the incorporation of finer image details. Through comprehensive experiments, we delved into the principle of noise disturbing for null-text and uncovered that the efficacy of disturbance depends on the correlation between the null-text noisy image and the source image. Moreover, the proposed methods, which can generate cartoon images and cartoonize specific ones, are training-free and easily integrated as a plug-and-play component in any classifier-free guided diffusion model. The project page is available at https://nulltextforcartoon.github.io/.", "year": 2023, "publicationdate": "2023-05-11", "externalids": {"DOI": "10.1145/3581783.3612588"}, "doi_lower": "10.1145/3581783.3612588"}
{"paper_id": 253761481, "title": "EDICT: Exact Diffusion Inversion via Coupled Transformations", "author_names": ["Bram Wallace", "Akash Gokul", "N. Naik"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Finding an initial noise vector that produces an input image when fed into the diffusion process (known as inversion) is an important problem in denoising diffusion models (DDMs), with applications for real image editing. The standard approach for real image editing with inversion uses denoising diffusion implicit models (DDIMs [29]) to deterministically noise the image to the intermediate state along the path that the denoising would follow given the original conditioning. However, DDIM inversion for real images is unstable as it relies on local linearization assumptions, which result in the propagation of errors, leading to incorrect image reconstruction and loss of content. To alleviate these problems, we propose Exact Diffusion Inversion via Coupled Transformations (EDICT), an inversion method that draws inspiration from affine coupling layers. EDICT enables mathematically exact inversion of real and model-generated images by maintaining two coupled noise vectors which are used to invert each other in an alternating fashion. Using Stable Diffusion [25], a state-of-the-art latent diffusion model, we demonstrate that EDICT successfully reconstructs real images with high fidelity. On complex image datasets like MS-COCO, EDICT reconstruction significantly outperforms DDIM, improving the mean square error of reconstruction by a factor of two. Using noise vectors inverted from real images, EDICT enables a wide range of image edits—from local and global semantic edits to image stylization—while maintaining fidelity to the original image structure. EDICT requires no model training/finetuning, prompt tuning, or extra data and can be combined with any pretrained DDM.", "year": 2022, "publicationdate": "2022-11-22", "externalids": {"DOI": "10.1109/CVPR52729.2023.02158"}, "doi_lower": "10.1109/cvpr52729.2023.02158"}
{"paper_id": 266648419, "title": "A Latent Space of Stochastic Diffusion Models for Zero-Shot Image Editing and Guidance", "author_names": ["Chen Henry Wu", "Fernando De la Torre"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Diffusion models generate images by iterative denoising. Recent work has shown that by making the denoising process deterministic, one can encode real images into latent codes of the same size, which can be used for image editing. This paper explores the possibility of defining a latent space even when the denoising process remains stochastic. Recall that, in stochastic diffusion models, Gaussian noises are added in each denoising step, and we can concatenate all the noises to form a latent code. This results in a latent space of much higher dimensionality than the original image. We demonstrate that this latent space of stochastic diffusion models can be used in the same way as that of deterministic diffusion models in two applications. First, we propose CycleDiffusion, a method for zero-shot and unpaired image editing using stochastic diffusion models, which improves the performance over its deterministic counterpart. Second, we demonstrate unified, plug-and-play guidance in the latent spaces of deterministic and stochastic diffusion models.1", "year": 2023, "publicationdate": "2023-10-01", "externalids": {"DOI": "10.1109/ICCV51070.2023.00678"}, "doi_lower": "10.1109/iccv51070.2023.00678"}
{"paper_id": 257766537, "title": "Training-free Content Injection using h-space in Diffusion Models", "author_names": ["Jaeseok Jeong", "Mingi Kwon", "Youngjung Uh"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "Diffusion models (DMs) synthesize high-quality images in various domains. However, controlling their generative process is still hazy because the intermediate variables in the process are not rigorously studied. Recently, the bottleneck feature of the U-Net, namely h-space, is found to convey the semantics of the resulting image. It enables StyleCLIP-like latent editing within DMs. In this paper, we explore further usage of h-space beyond attribute editing, and introduce a method to inject the content of one image into another image by combining their features in the generative processes. Briefly, given the original generative process of the other image, 1) we gradually blend the bottleneck feature of the content with proper normalization, and 2) we calibrate the skip connections to match the injected content. Unlike custom-diffusion approaches, our method does not require time-consuming optimization or fine-tuning. Instead, our method manipulates intermediate features within a feed-forward generative process. Furthermore, our method does not require supervision from external networks. Project page: https://curryjung.github.io/DiffStyle/", "year": 2023, "publicationdate": "2023-03-27", "externalids": {"DOI": "10.1109/WACV57701.2024.00507"}, "doi_lower": "10.1109/wacv57701.2024.00507"}
{"paper_id": 266374649, "title": "Lightning-Fast Image Inversion and Editing for Text-to-Image Diffusion Models", "author_names": ["Dvir Samuel", "Barak Meiri", "Haggai Maron", "Yoad Tewel", "Nir Darshan", "S. Avidan", "Gal Chechik", "Rami Ben-Ari"], "venue": "International Conference on Learning Representations", "abstract": "Diffusion inversion is the problem of taking an image and a text prompt that describes it and finding a noise latent that would generate the exact same image. Most current deterministic inversion techniques operate by approximately solving an implicit equation and may converge slowly or yield poor reconstructed images. We formulate the problem by finding the roots of an implicit equation and devlop a method to solve it efficiently. Our solution is based on Newton-Raphson (NR), a well-known technique in numerical analysis. We show that a vanilla application of NR is computationally infeasible while naively transforming it to a computationally tractable alternative tends to converge to out-of-distribution solutions, resulting in poor reconstruction and editing. We therefore derive an efficient guided formulation that fastly converges and provides high-quality reconstructions and editing. We showcase our method on real image editing with three popular open-sourced diffusion models: Stable Diffusion, SDXL-Turbo, and Flux with different deterministic schedulers. Our solution, Guided Newton-Raphson Inversion, inverts an image within 0.4 sec (on an A100 GPU) for few-step models (SDXL-Turbo and Flux.1), opening the door for interactive image editing. We further show improved results in image interpolation and generation of rare objects.", "year": 2023, "publicationdate": "2023-12-19", "externalids": {}, "doi_lower": null}
{"paper_id": 266521621, "title": "Tuning-Free Inversion-Enhanced Control for Consistent Image Editing", "author_names": ["Xiaoyue Duan", "Shuhao Cui", "Guoliang Kang", "Baochang Zhang", "Zhengcong Fei", "Mingyuan Fan", "Junshi Huang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Consistent editing of real images is a challenging task, as it requires performing non-rigid edits (e.g., changing postures) to the main objects in the input image without changing their identity or attributes. To guarantee consistent attributes, some existing methods fine-tune the entire model or the textual embedding for structural consistency, but they are time-consuming and fail to perform non-rigid edits. Other works are tuning-free, but their performances are weakened by the quality of Denoising Diffusion Implicit Model (DDIM) reconstruction, which often fails in real-world scenarios. In this paper, we present a novel approach called Tuning-free Inversion-enhanced Control (TIC), which directly correlates features from the inversion process with those from the sampling process to mitigate the inconsistency in DDIM reconstruction. Specifically, our method effectively obtains inversion features from the key and value features in the self-attention layers, and enhances the sampling process by these inversion features, thus achieving accurate reconstruction and content-consistent editing. To extend the applicability of our method to general editing scenarios, we also propose a mask-guided attention concatenation strategy that combines contents from both the inversion and the naive DDIM editing processes. Experiments show that the proposed method outperforms previous works in reconstruction and consistent editing, and produces impressive results in various settings.", "year": 2023, "publicationdate": "2023-12-22", "externalids": {"DOI": "10.48550/arXiv.2312.14611"}, "doi_lower": "10.48550/arxiv.2312.14611"}
{"paper_id": 258999925, "title": "Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images", "author_names": ["P. Gholami", "R. Xiao"], "venue": "arXiv.org", "abstract": "Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpainting techniques and editing software for fine-tuning AI-generated imagery.", "year": 2023, "publicationdate": "2023-05-31", "externalids": {"DOI": "10.48550/arXiv.2306.00219"}, "doi_lower": "10.48550/arxiv.2306.00219"}
{"paper_id": 258999106, "title": "Diffusion Self-Guidance for Controllable Image Generation", "author_names": ["Dave Epstein", "A. Jabri", "Ben Poole", "Alexei A. Efros", "Aleksander Holynski"], "venue": "Neural Information Processing Systems", "abstract": "Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. We also show that self-guidance can be used to edit real images. For results and an interactive demo, see our project page at https://dave.ml/selfguidance/", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.48550/arXiv.2306.00986"}, "doi_lower": "10.48550/arxiv.2306.00986"}
{"paper_id": 251252882, "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "author_names": ["Amir Hertz", "Ron Mokady", "J. Tenenbaum", "Kfir Aberman", "Y. Pritch", "D. Cohen-Or"], "venue": "International Conference on Learning Representations", "abstract": "Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.", "year": 2022, "publicationdate": "2022-08-02", "externalids": {"DOI": "10.48550/arXiv.2208.01626"}, "doi_lower": "10.48550/arxiv.2208.01626"}
{"paper_id": 256616002, "title": "Zero-shot Image-to-Image Translation", "author_names": ["Gaurav Parmar", "Krishna Kumar Singh", "Richard Zhang", "Yijun Li", "Jingwan Lu", "Jun-Yan Zhu"], "venue": "International Conference on Computer Graphics and Interactive Techniques", "abstract": "Large-scale text-to-image generative models have shown their remarkable ability to synthesize diverse, high-quality images. However, directly applying these models for real image editing remains challenging for two reasons. First, it is hard for users to craft a perfect text prompt depicting every visual detail in the input image. Second, while existing models can introduce desirable changes in certain regions, they often dramatically alter the input content and introduce unexpected changes in unwanted regions. In this work, we introduce pix2pix-zero, an image-to-image translation method that can preserve the original image’s content without manual prompting. We first automatically discover editing directions that reflect desired edits in the text embedding space. To preserve the content structure, we propose cross-attention guidance, which aims to retain the cross-attention maps of the input image throughout the diffusion process. Finally, to enable interactive editing, we distill the diffusion model into a fast conditional GAN. We conduct extensive experiments and show that our method outperforms existing and concurrent works for both real and synthetic image editing. In addition, our method does not need additional training for these edits and can directly use the existing pre-trained text-to-image diffusion model.", "year": 2023, "publicationdate": "2023-02-06", "externalids": {"DOI": "10.1145/3588432.3591513"}, "doi_lower": "10.1145/3588432.3591513"}
{"paper_id": 253801961, "title": "Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation", "author_names": ["Narek Tumanyan", "Michal Geyer", "Shai Bagon", "Tali Dekel"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Large-scale text-to-image generative models have been a revolutionary breakthrough in the evolution of generative AI, synthesizing diverse images with highly complex visual concepts. However, a pivotal challenge in leveraging such models for real-world content creation is providing users with control over the generated content. In this paper, we present a new framework that takes text-to- image synthesis to the realm of image-to-image translation - given a guidance image and a target text prompt as input, our method harnesses the power of a pre-trained text-to-image diffusion model to generate a new image that complies with the target text, while preserving the semantic layout of the guidance image. Specifically, we observe and empirically demonstrate that fine-grained control over the generated structure can be achieved by manipulating spatial features and their self-attention inside the model. This results in a simple and effective approach, where features extracted from the guidance image are directly injected into the generation process of the translated image, requiring no training or fine-tuning. We demonstrate high-quality results on versatile text-guided image translation tasks, including translating sketches, rough drawings and animations into realistic images, changing the class and appearance of objects in a given image, and modifying global qualities such as lighting and color.", "year": 2022, "publicationdate": "2022-11-22", "externalids": {"DOI": "10.1109/CVPR52729.2023.00191"}, "doi_lower": "10.1109/cvpr52729.2023.00191"}
{"paper_id": 260125230, "title": "TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition", "author_names": ["Shilin Lu", "Yanzhu Liu", "A. Kong"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Text-driven diffusion models have exhibited impressive generative capabilities, enabling various image editing tasks. In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seamlessly integrate user-provided objects into a specific visual context. Current diffusion-based methods often involve costly instance-based optimization or finetuning of pre-trained models on customized datasets, which can potentially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring additional training, finetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no information, to facilitate text-driven diffusion models in accurately inverting real images into latent representations, forming the basis for compositing. Our experiments show that equipping Stable Diffusion with the exceptional prompt outperforms state-of-the-art inversion methods on various datasets (CelebA-HQ, COCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile visual domains. Code is available at https://github.com/Shilin-LU/TF-ICON", "year": 2023, "publicationdate": "2023-07-24", "externalids": {"DOI": "10.1109/ICCV51070.2023.00218"}, "doi_lower": "10.1109/iccv51070.2023.00218"}
{"paper_id": 258960627, "title": "Conditional Score Guidance for Text-Driven Image-to-Image Translation", "author_names": ["Hyunsoo Lee", "Minsoo Kang", "Bohyung Han"], "venue": "Neural Information Processing Systems", "abstract": "We present a novel algorithm for text-driven image-to-image translation based on a pretrained text-to-image diffusion model. Our method aims to generate a target image by selectively editing the regions of interest in a source image, defined by a modifying text, while preserving the remaining parts. In contrast to existing techniques that solely rely on a target prompt, we introduce a new score function that additionally considers both the source image and the source text prompt, tailored to address specific translation tasks. To this end, we derive the conditional score function in a principled manner, decomposing it into the standard score and a guiding term for target image generation. For the gradient computation of the guiding term, we assume a Gaussian distribution of the posterior distribution and estimate its mean and variance to adjust the gradient without additional training. In addition, to improve the quality of the conditional score guidance, we incorporate a simple yet effective mixup technique, which combines two cross-attention maps derived from the source and target latents. This strategy is effective for promoting a desirable fusion of the invariant parts in the source image and the edited regions aligned with the target prompt, leading to high-fidelity target image generation. Through comprehensive experiments, we demonstrate that our approach achieves outstanding image-to-image translation performance on various tasks.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {"DOI": "10.48550/arXiv.2305.18007"}, "doi_lower": "10.48550/arxiv.2305.18007"}
{"paper_id": 259187874, "title": "Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models", "author_names": ["Geon Yeong Park", "Jeongsol Kim", "Beomsu Kim", "Sang Wan Lee", "Jong-Chul Ye"], "venue": "Neural Information Processing Systems", "abstract": "Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation tasks, including multi-concept generation, text-guided image inpainting, and real and synthetic image editing.", "year": 2023, "publicationdate": "2023-06-16", "externalids": {"DOI": "10.48550/arXiv.2306.09869"}, "doi_lower": "10.48550/arxiv.2306.09869"}
{"paper_id": 254125549, "title": "Shape-Guided Diffusion with Inside-Outside Attention", "author_names": ["Dong Huk Park", "Grace Luo", "C. Toste", "S. Azadi", "Xihui Liu", "M. Karalashvili", "Anna Rohrbach", "Trevor Darrell"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "We introduce precise object silhouette as a new constraint in text-to-image diffusion models, which we dub Shape-Guided Diffusion. Our training-free method uses an Inside-Outside Attention mechanism during the inversion and generation process to apply a shape constraint to the cross- and self-attention maps. Our mechanism designates which spatial region is the object (inside) vs. background (outside) then associates edits to the correct region. We demonstrate the efficacy of our method on the shape-guided editing task, where the model must replace an object according to a text prompt and object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness without a degradation in text alignment or image realism according to both automatic metrics and annotator ratings. Our data and code will be made available at https://shape-guided-diffusion.github.io.", "year": 2022, "publicationdate": "2022-12-01", "externalids": {"DOI": "10.1109/WACV57701.2024.00415"}, "doi_lower": "10.1109/wacv57701.2024.00415"}
{"paper_id": 266435854, "title": "HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models", "author_names": ["Hayk Manukyan", "Andranik Sargsyan", "Barsegh Atanyan", "Zhangyang Wang", "Shant Navasardyan", "Humphrey Shi"], "venue": "International Conference on Learning Representations", "abstract": "Recent progress in text-guided image inpainting, based on the unprecedented success of text-to-image diffusion models, has led to exceptionally realistic and visually plausible results. However, there is still significant potential for improvement in current text-to-image inpainting models, particularly in better aligning the inpainted area with user prompts and performing high-resolution inpainting. Therefore, we introduce HD-Painter, a training free approach that accurately follows prompts and coherently scales to high resolution image inpainting. To this end, we design the Prompt-Aware Introverted Attention (PAIntA) layer enhancing self-attention scores by prompt information resulting in better text aligned generations. To further improve the prompt coherence we introduce the Reweighting Attention Score Guidance (RASG) mechanism seamlessly integrating a post-hoc sampling strategy into the general form of DDIM to prevent out-of-distribution latent shifts. Moreover, HD-Painter allows extension to larger scales by introducing a specialized super-resolution technique customized for inpainting, enabling the completion of missing regions in images of up to 2K resolution. Our experiments demonstrate that HD-Painter surpasses existing state-of-the-art approaches quantitatively and qualitatively across multiple metrics and a user study. Code is publicly available at: https://github.com/Picsart-AI-Research/HD-Painter", "year": 2023, "publicationdate": "2023-12-21", "externalids": {"DOI": "10.48550/arXiv.2312.14091"}, "doi_lower": "10.48550/arxiv.2312.14091"}
{"paper_id": 258960309, "title": "FISEdit: Accelerating Text-to-image Editing via Cache-enabled Sparse Diffusion Inference", "author_names": ["Zihao Yu", "Haoyang Li", "Fangcheng Fu", "Xupeng Miao", "Bin Cui"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Due to the recent success of diffusion models, text-to-image generation is becoming increasingly popular and achieves a wide range of applications. Among them, text-to-image editing, or continuous text-to-image generation, attracts lots of attention and can potentially improve the quality of generated images. It's common to see that users may want to slightly edit the generated image by making minor modifications to their input textual descriptions for several rounds of diffusion inference. However, such an image editing process suffers from the low inference efficiency of many existing diffusion models even using GPU accelerators.\n\nTo solve this problem, we introduce Fast Image Semantically Edit (FISEdit), a cached-enabled sparse diffusion model inference engine for efficient text-to-image editing. The key intuition behind our approach is to utilize the semantic mapping between the minor modifications on the input text and the affected regions on the output image. For each text editing step, FISEdit can 1) automatically identify the affected image regions and 2) utilize the cached unchanged regions' feature map to accelerate the inference process. For the former, we measure the differences between cached and ad hoc feature maps given the modified textual description, extract the region with significant differences, and capture the affected region by masks. For the latter, we develop an efficient sparse diffusion inference engine that only computes the feature maps for the affected region while reusing the cached statistics for the rest of the image. Finally, extensive empirical results show that FISEdit can be 3.4 times and 4.4 times faster than existing methods on NVIDIA TITAN RTX and A100 GPUs respectively, and even generates more satisfactory images.", "year": 2023, "publicationdate": "2023-05-27", "externalids": {"DOI": "10.48550/arXiv.2305.17423"}, "doi_lower": "10.48550/arxiv.2305.17423"}
{"paper_id": 249394540, "title": "Blended Latent Diffusion", "author_names": ["Omri Avrahami", "Ohad Fried", "Dani Lischinski"], "venue": "ACM Transactions on Graphics", "abstract": "The tremendous progress in neural image generation, coupled with the emergence of seemingly omnipotent vision-language models has finally enabled text-based interfaces for creating and editing images. Handling generic images requires a diverse underlying generative model, hence the latest works utilize diffusion models, which were shown to surpass GANs in terms of diversity. One major drawback of diffusion models, however, is their relatively slow inference time. In this paper, we present an accelerated solution to the task of local text-driven editing of generic images, where the desired edits are confined to a user-provided mask. Our solution leverages a text-to-image Latent Diffusion Model (LDM), which speeds up diffusion by operating in a lower-dimensional latent space and eliminating the need for resource-intensive CLIP gradient calculations at each diffusion step. We first enable LDM to perform local image edits by blending the latents at each step, similarly to Blended Diffusion. Next we propose an optimization-based solution for the inherent inability of LDM to accurately reconstruct images. Finally, we address the scenario of performing local edits using thin masks. We evaluate our method against the available baselines both qualitatively and quantitatively and demonstrate that in addition to being faster, it produces more precise results.", "year": 2022, "publicationdate": "2022-06-06", "externalids": {"DOI": "10.1145/3592450"}, "doi_lower": "10.1145/3592450"}
{"paper_id": 259287333, "title": "PFB-Diff: Progressive Feature Blending Diffusion for Text-driven Image Editing", "author_names": ["Wenjing Huang", "Shikui Tu", "Lei Xu"], "venue": "Neural Networks", "abstract": "Diffusion models have demonstrated their ability to generate diverse and high-quality images, sparking considerable interest in their potential for real image editing applications. However, existing diffusion-based approaches for local image editing often suffer from undesired artifacts due to the latent-level blending of the noised target images and diffusion latent variables, which lack the necessary semantics for maintaining image consistency. To address these issues, we propose PFB-Diff, a Progressive Feature Blending method for Diffusion-based image editing. Unlike previous methods, PFB-Diff seamlessly integrates text-guided generated content into the target image through multi-level feature blending. The rich semantics encoded in deep features and the progressive blending scheme from high to low levels ensure semantic coherence and high quality in edited images. Additionally, we introduce an attention masking mechanism in the cross-attention layers to confine the impact of specific words to desired regions, further improving the performance of background editing and multi-object replacement. PFB-Diff can effectively address various editing tasks, including object/background replacement and object attribute editing. Our method demonstrates its superior performance in terms of editing accuracy and image quality without the need for fine-tuning or training. Our implementation is available at https://github.com/CMACH508/PFB-Diff.", "year": 2023, "publicationdate": "2023-06-28", "externalids": {"DOI": "10.48550/arXiv.2306.16894"}, "doi_lower": "10.48550/arxiv.2306.16894"}
{"paper_id": 253018768, "title": "DiffEdit: Diffusion-based semantic image editing with mask guidance", "author_names": ["Guillaume Couairon", "Jakob Verbeek", "Holger Schwenk", "M. Cord"], "venue": "International Conference on Learning Representations", "abstract": "Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images.", "year": 2022, "publicationdate": "2022-10-20", "externalids": {"DOI": "10.48550/arXiv.2210.11427"}, "doi_lower": "10.48550/arxiv.2210.11427"}
{"paper_id": 257102889, "title": "Region-Aware Diffusion for Zero-shot Text-driven Image Editing", "author_names": ["Nisha Huang", "Fan Tang", "Weiming Dong", "Tong-Yee Lee", "Changsheng Xu"], "venue": "arXiv.org", "abstract": "Image manipulation under the guidance of textual descriptions has recently received a broad range of attention. In this study, we focus on the regional editing of images with the guidance of given text prompts. Different from current mask-based image editing methods, we propose a novel region-aware diffusion model (RDM) for entity-level image editing, which could automatically locate the region of interest and replace it following given text prompts. To strike a balance between image fidelity and inference speed, we design the intensive diffusion pipeline by combing latent space diffusion and enhanced directional guidance. In addition, to preserve image content in non-edited regions, we introduce regional-aware entity editing to modify the region of interest and preserve the out-of-interest region. We validate the proposed RDM beyond the baseline methods through extensive qualitative and quantitative experiments. The results show that RDM outperforms the previous approaches in terms of visual quality, overall harmonization, non-editing region content preservation, and text-image semantic consistency. The codes are available at https://github.com/haha-lisa/RDM-Region-Aware-Diffusion-Model.", "year": 2023, "publicationdate": "2023-02-23", "externalids": {"DOI": "10.48550/arXiv.2302.11797"}, "doi_lower": "10.48550/arxiv.2302.11797"}
{"paper_id": 254685897, "title": "Text-Guided Mask-Free Local Image Retouching", "author_names": ["Zerun Liu", "Fan Zhang", "Jingxuan He", "Jin Wang", "Zhangye Wang", "Lechao Cheng"], "venue": "IEEE International Conference on Multimedia and Expo", "abstract": "In the realm of multi-modality, text-guided image retouching techniques emerged with the advent of deep learning. Most currently available text-guided methods, however, rely on object-level supervision to confine the region of interest that may be updated. This not only makes it more challenging to develop these algorithms but also limits how widely deep learning can be used for image retouching. In this paper, we offer a text-guided mask-free image retouching approach that yields consistent results to address this concern. Specifically, we propose a two-stage mask-free training paradigm tailored for text-guided image retouching tasks. In the first stage, an unified mask is proposed according to the query description, and then several candidate images are generated with the provided mask and the conditional description based on diffusion model. Extensive experiments have shown that our method can produce high-quality images based on spoken language.", "year": 2022, "publicationdate": "2022-12-15", "externalids": {"DOI": "10.1109/ICME55011.2023.00473"}, "doi_lower": "10.1109/icme55011.2023.00473"}
{"paper_id": 258999295, "title": "Differential Diffusion: Giving Each Pixel Its Strength", "author_names": ["E. Levin", "Ohad Fried"], "venue": "Computer graphics forum (Print)", "abstract": "Diffusion models have revolutionized image generation and editing, producing state‐of‐the‐art results in conditioned and unconditioned image synthesis. While current techniques enable user control over the degree of change in an image edit, the controllability is limited to global changes over an entire edited region. This paper introduces a novel framework that enables customization of the amount of change per pixel or per image region. Our framework can be integrated into any existing diffusion model, enhancing it with this capability. Such granular control opens up a diverse array of new editing capabilities, such as control of the extent to which individual objects are modified, or the ability to introduce gradual spatial changes. Furthermore, we showcase the framework's effectiveness in soft‐inpainting—the completion of portions of an image while subtly adjusting the surrounding areas to ensure seamless integration. Additionally, we introduce a new tool for exploring the effects of different change quantities. Our framework operates solely during inference, requiring no model training or fine‐tuning. We demonstrate our method with the current open state‐of‐the‐art models, and validate it via both quantitative and qualitative comparisons, and a user study. Our code is published and integrated into several platforms.", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.1111/cgf.70040"}, "doi_lower": "10.1111/cgf.70040"}
{"paper_id": 261031162, "title": "Watch Your Steps: Local Image and Scene Editing by Text Instructions", "author_names": ["Ashkan Mirzaei", "Tristan Aumentado-Armstrong", "Marcus A. Brubaker", "J. Kelly", "Alex Levinshtein", "K. Derpanis", "Igor Gilitschenski"], "venue": "European Conference on Computer Vision", "abstract": "Denoising diffusion models have enabled high-quality image generation and editing. We present a method to localize the desired edit region implicit in a text instruction. We leverage InstructPix2Pix (IP2P) and identify the discrepancy between IP2P predictions with and without the instruction. This discrepancy is referred to as the relevance map. The relevance map conveys the importance of changing each pixel to achieve the edits, and is used to to guide the modifications. This guidance ensures that the irrelevant pixels remain unchanged. Relevance maps are further used to enhance the quality of text-guided editing of 3D scenes in the form of neural radiance fields. A field is trained on relevance maps of training views, denoted as the relevance field, defining the 3D region within which modifications should be made. We perform iterative updates on the training views guided by rendered relevance maps from the relevance field. Our method achieves state-of-the-art performance on both image and NeRF editing tasks. Project page: https://ashmrz.github.io/WatchYourSteps/", "year": 2023, "publicationdate": "2023-08-17", "externalids": {"DOI": "10.48550/arXiv.2308.08947"}, "doi_lower": "10.48550/arxiv.2308.08947"}
{"paper_id": 244714366, "title": "Blended Diffusion for Text-driven Editing of Natural Images", "author_names": ["Omri Avrahami", "D. Lischinski", "Ohad Fried"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Natural language offers a highly intuitive interface for image editing. In this paper, we introduce the first solution for performing local (region-based) edits in generic natural images, based on a natural language description along with an ROI mask. We achieve our goal by leveraging and combining a pretrained language-image model (CLIP), to steer the edit towards a user-provided text prompt, with a denoising diffusion probabilistic model (DDPM) to generate natural-looking results. To seamlessly fuse the edited region with the unchanged parts of the image, we spatially blend noised versions of the input image with the local text-guided diffusion latent at a progression of noise levels. In addition, we show that adding augmentations to the diffusion process mitigates adversarial results. We compare against several baselines and related methods, both qualitatively and quantitatively, and show that our method outperforms these solutions in terms of overall realism, ability to preserve the background and matching the text. Finally, we show several text-driven editing applications, including adding a new object to an image, removing/replacing/altering existing objects, background replacement, and image extrapolation.", "year": 2021, "publicationdate": "2021-11-29", "externalids": {"DOI": "10.1109/CVPR52688.2022.01767"}, "doi_lower": "10.1109/cvpr52688.2022.01767"}
{"paper_id": 266573944, "title": "ZONE: Zero-Shot Instruction-Guided Local Editing", "author_names": ["Shanglin Li", "Bo-Wen Zeng", "Yutang Feng", "Sicheng Gao", "Xuhui Liu", "Jiaming Liu", "Li Lin", "Xu Tang", "Yao Hu", "Jianzhuang Liu", "Baochang Zhang"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Recent advances in vision-language models like Stable Diffusion have shown remarkable power in creative image synthesis and editing. However, most existing text-to-image editing methods encounter two obstacles: First, the text prompt needs to be carefully crafted to achieve good results, which is not intuitive or user-friendly. Second, they are in-sensitive to local edits and can irreversibly affect non-edited regions, leaving obvious editing traces. To tackle these problems, we propose a Zero-shot instructiON-guided local image Editing approach, termed ZONE. We first convert the editing intent from the user-provided instruction (e.g., “make his tie blue”) into specific image editing regions through InstructPix2Pix. We then propose a Region-loll scheme for precise image layer extraction from an off-the-shelf segment model. We further develop an edge smoother based on FFT for seamless blending between the layer and the image. Our method allows for arbitrary manipulation of a specific region with a single instruction while preserving the rest. Extensive experiments demonstrate that our Z ONE achieves remarkable local editing results and user-friendliness, outperforming state-of-the-art methods. Code is available at https://github.com/ls1001006/ZONE.", "year": 2023, "publicationdate": "2023-12-28", "externalids": {"DOI": "10.1109/CVPR52733.2024.00598"}, "doi_lower": "10.1109/cvpr52733.2024.00598"}
{"paper_id": 258170322, "title": "Inpaint Anything: Segment Anything Meets Image Inpainting", "author_names": ["Tao Yu", "Runsen Feng", "Ruoyu Feng", "Jinming Liu", "Xin Jin", "Wenjun Zeng", "Zhibo Chen"], "venue": "arXiv.org", "abstract": "Modern image inpainting systems, despite the significant progress, often struggle with mask selection and holes filling. Based on Segment-Anything Model (SAM), we make the first attempt to the mask-free image inpainting and propose a new paradigm of ``clicking and filling'', which is named as Inpaint Anything (IA). The core idea behind IA is to combine the strengths of different models in order to build a very powerful and user-friendly pipeline for solving inpainting-related problems. IA supports three main features: (i) Remove Anything: users could click on an object and IA will remove it and smooth the ``hole'' with the context; (ii) Fill Anything: after certain objects removal, users could provide text-based prompts to IA, and then it will fill the hole with the corresponding generative content via driving AIGC models like Stable Diffusion; (iii) Replace Anything: with IA, users have another option to retain the click-selected object and replace the remaining background with the newly generated scenes. We are also very willing to help everyone share and promote new projects based on our Inpaint Anything (IA). Our codes are available at https://github.com/geekyutao/Inpaint-Anything.", "year": 2023, "publicationdate": "2023-04-13", "externalids": {"DOI": "10.48550/arXiv.2304.06790"}, "doi_lower": "10.48550/arxiv.2304.06790"}
{"paper_id": 256390079, "title": "SEGA: Instructing Diffusion using Semantic Dimensions", "author_names": ["Manuel Brack", "Felix Friedrich", "Dominik Hintersdorf", "Lukas Struppek", "P. Schramowski", "K. Kersting"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": "2023-01-28", "externalids": {"DOI": "10.48550/arXiv.2301.12247"}, "doi_lower": "10.48550/arxiv.2301.12247"}
{"paper_id": 259316918, "title": "LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance", "author_names": ["Linoy Tsaban", "Apolin'ario Passos"], "venue": "arXiv.org", "abstract": "Recent large-scale text-guided diffusion models provide powerful image-generation capabilities. Currently, a significant effort is given to enable the modification of these images using text only as means to offer intuitive and versatile editing. However, editing proves to be difficult for these generative models due to the inherent nature of editing techniques, which involves preserving certain content from the original image. Conversely, in text-based models, even minor modifications to the text prompt frequently result in an entirely distinct result, making attaining one-shot generation that accurately corresponds to the users intent exceedingly challenging. In addition, to edit a real image using these state-of-the-art tools, one must first invert the image into the pre-trained models domain - adding another factor affecting the edit quality, as well as latency. In this exploratory report, we propose LEDITS - a combined lightweight approach for real-image editing, incorporating the Edit Friendly DDPM inversion technique with Semantic Guidance, thus extending Semantic Guidance to real image editing, while harnessing the editing capabilities of DDPM inversion as well. This approach achieves versatile edits, both subtle and extensive as well as alterations in composition and style, while requiring no optimization nor extensions to the architecture.", "year": 2023, "publicationdate": "2023-07-02", "externalids": {"DOI": "10.48550/arXiv.2307.00522"}, "doi_lower": "10.48550/arxiv.2307.00522"}
{"paper_id": 257255250, "title": "Collage Diffusion", "author_names": ["Vishnu Sarukkai", "Linden Li", "Arden Ma", "Christopher R'e", "Kayvon Fatahalian"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "We seek to give users precise control over diffusion-based image generation by modeling complex scenes as sequences of layers, which define the desired spatial arrangement and visual attributes of objects in the scene. Collage Diffusion harmonizes the input layers to make objects fit together—the key challenge involves minimizing changes in the positions and key visual attributes of the input layers while allowing other attributes to change in the harmonization process. We ensure that objects are generated in the correct locations by modifying text-image cross-attention with the layers’ alpha masks. We preserve key visual attributes of input layers by learning specialized text representations per layer and by extending prior diffusion-based control mechanisms to operate on layers. Layer input allows users to control the extent of image harmonization on a per-object basis, and users can even iteratively edit individual objects in generated images while keeping other objects fixed. By leveraging the rich information present in layer input, Collage Diffusion generates globally harmonized images that maintain desired object characteristics better than prior approaches.", "year": 2023, "publicationdate": "2023-03-01", "externalids": {"DOI": "10.1109/WACV57701.2024.00416"}, "doi_lower": "10.1109/wacv57701.2024.00416"}
{"paper_id": 268512724, "title": "IMPRINT: Generative Object Compositing by Learning Identity-Preserving Representation", "author_names": ["Yizhi Song", "Zhifei Zhang", "Zhe L. Lin", "Scott Cohen", "Brian L. Price", "Jianming Zhang", "Soo Ye Kim", "He Zhang", "Wei Xiong", "Daniel G. Aliaga"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Generative object compositing emerges as a promising new avenue for compositional image editing. However, the requirement of object identity preservation poses a significant challenge, limiting practical usage of most existing methods. In response, this paper introduces IMPRINT, a novel diffusion-based generative model trained with a two-stage learning framework that decouples learning of identity preservation from that of compositing. The first stage is targeted for context-agnostic, identity-preserving pretraining of the object encoder, enabling the encoder to learn an embedding that is both view-invariant and conducive to enhanced detail preservation. The subsequent stage leverages this representation to learn seamless harmonization of the object composited to the background. In addition, IMPRINT incorporates a shape-guidance mechanism offering user-directed control over the compositing process. Extensive experiments demonstrate that IMPRINT significantly outperforms existing methods and various baselines on identity preservation and composition quality. Project page: https://song630.github.io/IMPRINT-Project-Page/", "year": 2024, "publicationdate": "2024-03-15", "externalids": {"DOI": "10.1109/CVPR52733.2024.00769"}, "doi_lower": "10.1109/cvpr52733.2024.00769"}
{"paper_id": 268296977, "title": "PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering", "author_names": ["Yibin Wang", "Weizhong Zhang", "Jianwei Zheng", "Cheng Jin"], "venue": "ACM Multimedia", "abstract": "Image composition involves seamlessly integrating given objects into a specific visual context. Current training-free methods rely on composing attention weights from several samplers to guide the generator. However, since these weights are derived from disparate contexts, their combination leads to coherence confusion and loss of appearance information. These issues worsen with their excessive focus on background generation, even when unnecessary in this task. This not only impedes their swift implementation but also compromises foreground generation quality. Moreover, these methods introduce unwanted artifacts in the transition area. In this paper, we formulate image composition as a subject-based local editing task, solely focusing on foreground generation. At each step, the edited foreground is combined with the noisy background to maintain scene consistency. To address the remaining issues, we propose PrimeComposer, a faster training-free diffuser that composites the images by well-designed attention steering across different noise levels. This steering is predominantly achieved by our Correlation Diffuser, utilizing its self-attention layers at each step. Within these layers, the synthesized subject interacts with both the referenced object and background, capturing intricate details and coherent relationships. This prior information is encoded into the attention weights, which are then integrated into the self-attention layers of the generator to guide the synthesis process. Besides, we introduce a Region-constrained Cross-Attention to confine the impact of specific subject-related tokens to desired regions, addressing the unwanted artifacts shown in the prior method thereby further improving the coherence in the transition area. Our method exhibits the fastest inference efficiency and extensive experiments demonstrate our superiority both qualitatively and quantitatively. The code is available at https://github.com/CodeGoat24/PrimeComposer.", "year": 2024, "publicationdate": "2024-03-08", "externalids": {"DOI": "10.1145/3664647.3680848"}, "doi_lower": "10.1145/3664647.3680848"}
{"paper_id": 3568073, "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation", "author_names": ["Tero Karras", "Timo Aila", "S. Laine", "J. Lehtinen"], "venue": "International Conference on Learning Representations", "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.", "year": 2017, "publicationdate": "2017-10-27", "externalids": {}, "doi_lower": null}
{"paper_id": 208617800, "title": "StarGAN v2: Diverse Image Synthesis for Multiple Domains", "author_names": ["Yunjey Choi", "Youngjung Uh", "Jaejun Yoo", "Jung-Woo Ha"], "venue": "Computer Vision and Pattern Recognition", "abstract": "A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset are available at https://github.com/clovaai/stargan-v2.", "year": 2019, "publicationdate": "2019-12-04", "externalids": {"DOI": "10.1109/cvpr42600.2020.00821"}, "doi_lower": "10.1109/cvpr42600.2020.00821"}
{"paper_id": 8317437, "title": "LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop", "author_names": ["F. Yu", "Yinda Zhang", "Shuran Song", "Ari Seff", "Jianxiong Xiao"], "venue": "arXiv.org", "abstract": "While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset.", "year": 2015, "publicationdate": "2015-06-10", "externalids": {}, "doi_lower": null}
{"paper_id": 21693606, "title": "WikiArt Emotions: An Annotated Dataset of Emotions Evoked by Art", "author_names": ["Saif M. Mohammad", "S. Kiritchenko"], "venue": "International Conference on Language Resources and Evaluation", "abstract": null, "year": 2018, "publicationdate": "2018-05-01", "externalids": {}, "doi_lower": null}
{"paper_id": 268296038, "title": "StyleGAN-Fusion: Diffusion Guided Domain Adaptation of Image Generators", "author_names": ["Kunpeng Song", "Ligong Han", "Bingchen Liu", "Dimitris N. Metaxas", "Ahmed Elgammal"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "Can a text-to-image diffusion model be used as a training objective for adapting a GAN generator to another domain? In this paper, we show that the classifier-free guidance can be leveraged as a critic and enable generators to distill knowledge from large-scale text-to-image diffusion models. Generators can be efficiently shifted into new domains indicated by text prompts without access to groundtruth samples from target domains. We demonstrate the effectiveness and controllability of our method through extensive experiments. Although not trained to minimize CLIP loss, our model achieves equally high CLIP scores and significantly lower FID than prior work on short prompts, and outperforms the baseline qualitatively and quantitatively on long and complicated prompts. To our best knowledge, the proposed method is the first attempt at incorporating large-scale pre-trained diffusion models and distillation sampling for text-driven image generator domain adaptation and gives a quality previously beyond possible. Moreover, we extend our work to 3D-aware style-based generators and DreamBooth guidance. For code and more visual samples, please visit our Project Webpage.", "year": 2024, "publicationdate": "2024-01-03", "externalids": {"DOI": "10.1109/WACV57701.2024.00537"}, "doi_lower": "10.1109/wacv57701.2024.00537"}
{"paper_id": 232428282, "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", "author_names": ["Or Patashnik", "Zongze Wu", "Eli Shechtman", "D. Cohen-Or", "D. Lischinski"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping text prompts to input-agnostic directions in StyleGAN’s style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.", "year": 2021, "publicationdate": "2021-03-31", "externalids": {"DOI": "10.1109/ICCV48922.2021.00209"}, "doi_lower": "10.1109/iccv48922.2021.00209"}
{"paper_id": 231591445, "title": "Learning Transferable Visual Models From Natural Language Supervision", "author_names": ["Alec Radford", "Jong Wook Kim", "Chris Hallacy", "A. Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark", "Gretchen Krueger", "I. Sutskever"], "venue": "International Conference on Machine Learning", "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.", "year": 2021, "publicationdate": "2021-02-26", "externalids": {}, "doi_lower": null}
{"paper_id": 198897678, "title": "Interpreting the Latent Space of GANs for Semantic Face Editing", "author_names": ["Yujun Shen", "Jinjin Gu", "Xiaoou Tang", "Bolei Zhou"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Despite the recent advance of Generative Adversarial Networks (GANs) in high-fidelity image synthesis, there lacks enough understanding of how GANs are able to map a latent code sampled from a random distribution to a photo-realistic image. Previous work assumes the latent space learned by GANs follows a distributed representation but observes the vector arithmetic phenomenon. In this work, we propose a novel framework, called InterFaceGAN, for semantic face editing by interpreting the latent semantics learned by GANs. In this framework, we conduct a detailed study on how different semantics are encoded in the latent space of GANs for face synthesis. We find that the latent code of well-trained generative models actually learns a disentangled representation after linear transformations. We explore the disentanglement between various semantics and manage to decouple some entangled semantics with subspace projection, leading to more precise control of facial attributes. Besides manipulating gender, age, expression, and the presence of eyeglasses, we can even vary the face pose as well as fix the artifacts accidentally generated by GAN models. The proposed method is further applied to achieve real image manipulation when combined with GAN inversion methods or some encoder-involved models. Extensive results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable facial attribute representation.", "year": 2019, "publicationdate": "2019-07-25", "externalids": {"DOI": "10.1109/CVPR42600.2020.00926"}, "doi_lower": "10.1109/cvpr42600.2020.00926"}
{"paper_id": 208290975, "title": "Image2StyleGAN++: How to Edit the Embedded Images?", "author_names": ["Rameen Abdal", "Yipeng Qin", "Peter Wonka"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We propose Image2StyleGAN++, a flexible image editing framework with many applications. Our framework extends the recent Image2StyleGAN in three ways. First, we introduce noise optimization as a complement to the W+ latent space embedding. Our noise optimization can restore high frequency features in images and thus significantly improves the quality of reconstructed images, e.g. a big increase of PSNR from 20 dB to 45 dB. Second, we extend the global W+ latent space embedding to enable local embeddings. Third, we combine embedding with activation tensor manipulation to perform high quality local edits along with global semantic edits on images. Such edits motivate various high quality image editing applications, e.g. image reconstruction, image inpainting, image crossover, local style transfer, image editing using scribbles, and attribute level feature transfer. Examples of the edited images are shown across the paper for visual inspection.", "year": 2019, "publicationdate": "2019-11-26", "externalids": {"DOI": "10.1109/cvpr42600.2020.00832"}, "doi_lower": "10.1109/cvpr42600.2020.00832"}
{"paper_id": 218971783, "title": "Language Models are Few-Shot Learners", "author_names": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "R. Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Ma-teusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "I. Sutskever", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "year": 2020, "publicationdate": "2020-05-28", "externalids": {}, "doi_lower": null}
{"paper_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "author_names": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "Ed H. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "venue": "Neural Information Processing Systems", "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 256390509, "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "author_names": ["Junnan Li", "Dongxu Li", "S. Savarese", "Steven C. H. Hoi"], "venue": "International Conference on Machine Learning", "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.", "year": 2023, "publicationdate": "2023-01-30", "externalids": {"DOI": "10.48550/arXiv.2301.12597"}, "doi_lower": "10.48550/arxiv.2301.12597"}
{"paper_id": 259950998, "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "author_names": ["Hugo Touvron", "Louis Martin", "Kevin R. Stone", "Peter Albert", "Amjad Almahairi", "Yasmine Babaei", "Niko-lay Bashlykov", "Soumya Batra", "Prajjwal Bhargava", "Shruti Bhosale", "D. Bikel", "Lukas Blecher", "Cris-tian Cantón Ferrer", "Moya Chen", "Guillem Cucurull", "David Esiobu", "Jude Fernandes", "J. Fu", "Wenyin Fu", "Brian Fuller", "Cynthia Gao", "Vedanuj Goswami", "Naman Goyal", "A. Hartshorn", "Saghar Hosseini", "Rui Hou", "Hakan Inan", "Marcin Kardas", "Viktor Kerkez", "Madian Khabsa", "Isabel M. Kloumann", "A. Korenev", "Punit Singh Koura", "M. Lachaux", "Thibaut Lavril", "Jenya Lee", "Diana Liskovich", "Yinghai Lu", "Yuning Mao", "Xavier Martinet", "Todor Mihaylov", "Pushkar Mishra", "Igor Molybog", "Yixin Nie", "Andrew Poulton", "J. Reizenstein", "Rashi Rungta", "Kalyan Saladi", "A. Schelten", "Ruan Silva", "Eric Michael Smith", "R. Subramanian", "Xia Tan", "Binh Tang", "Ross Taylor", "Adina Williams", "Jian Xiang Kuan", "Puxin Xu", "Zhengxu Yan", "Iliyan Zarov", "Yuchen Zhang", "Angela Fan", "M. Kambadur", "Sharan Narang", "Aur'elien Rodriguez", "Robert Stojnic", "Sergey Edunov", "Thomas Scialom"], "venue": "arXiv.org", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "year": 2023, "publicationdate": "2023-07-18", "externalids": {}, "doi_lower": null}
{"paper_id": 254877310, "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "author_names": ["Yizhong Wang", "Yeganeh Kordi", "Swaroop Mishra", "Alisa Liu", "Noah A. Smith", "Daniel Khashabi", "Hannaneh Hajishirzi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10560"}, "doi_lower": "10.48550/arxiv.2212.10560"}
{"paper_id": 152282269, "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering", "author_names": ["Drew A. Hudson", "Christopher D. Manning"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages Visual Genome scene graph structures to create 22M diverse reasoning questions, which all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate question biases. Accompanying the dataset is a suite of new metrics that evaluate essential qualities such as consistency, grounding and plausibility. A careful analysis is performed for baselines as well as state-of-the-art models, providing fine-grained results for different question types and topologies. Whereas a blind LSTM obtains a mere 42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%, offering ample opportunity for new research to explore. We hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding of vision and language.", "year": 2019, "publicationdate": "2019-02-25", "externalids": {"DOI": "10.1109/CVPR.2019.00686"}, "doi_lower": "10.1109/cvpr.2019.00686"}
{"paper_id": 244072324, "title": "CR-Fill: Generative Image Inpainting with Auxiliary Contextual Reconstruction", "author_names": ["Yu Zeng", "Zhe L. Lin", "Huchuan Lu", "Vishal M. Patel"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Recent deep generative inpainting methods use attention layers to allow the generator to explicitly borrow feature patches from the known region to complete a missing region. Due to the lack of supervision signals for the correspondence between missing regions and known regions, it may fail to find proper reference features, which often leads to artifacts in the results. Also, it computes pair-wise similarity across the entire feature map during inference bringing a significant computational overhead. To address this issue, we propose to teach such patch-borrowing behavior to an attention-free generator by joint training of an auxiliary contextual reconstruction task, which encourages the generated output to be plausible even when reconstructed by surrounding regions. The auxiliary branch can be seen as a learnable loss function, i.e. named as contextual reconstruction (CR) loss, where query-reference feature similarity and reference-based reconstructor are jointly optimized with the inpainting generator. The auxiliary branch ( i.e. CR loss) is required only during training, and only the inpainting generator is required during the inference. Experimental results demonstrate that the proposed inpainting model compares favourably against the state-of-the-art in terms of quantitative and visual performance. Code is available at https://github.com/zengxianyu/crfill.", "year": 2021, "publicationdate": "2021-10-01", "externalids": {"DOI": "10.1109/ICCV48922.2021.01390"}, "doi_lower": "10.1109/iccv48922.2021.01390"}
{"paper_id": 258564264, "title": "ImageBind One Embedding Space to Bind Them All", "author_names": ["Rohit Girdhar", "Alaaeldin El-Nouby", "Zhuang Liu", "Mannat Singh", "Kalyan Vasudev Alwala", "Armand Joulin", "Ishan Misra"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications ‘out-of-the-box’ including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.", "year": 2023, "publicationdate": "2023-05-09", "externalids": {"DOI": "10.1109/CVPR52729.2023.01457"}, "doi_lower": "10.1109/cvpr52729.2023.01457"}
{"paper_id": 258070012, "title": "Marketing with ChatGPT: Navigating the Ethical Terrain of GPT-Based Chatbot Technology", "author_names": ["Pablo Rivas"], "venue": "Applied Informatics", "abstract": "ChatGPT is an AI-powered chatbot platform that enables human users to converse with machines. It utilizes natural language processing and machine learning algorithms, transforming how people interact with AI technology. ChatGPT offers significant advantages over previous similar tools, and its potential for application in various fields has generated attention and anticipation. However, some experts are wary of ChatGPT, citing ethical implications. Therefore, this paper shows that ChatGPT has significant potential to transform marketing and shape its future if certain ethical considerations are taken into account. First, we argue that ChatGPT-based tools can help marketers create content faster and potentially with quality similar to human content creators. It can also assist marketers in conducting more efficient research and understanding customers better, automating customer service, and improving efficiency. Then we discuss ethical implications and potential risks for marketers, consumers, and other stakeholders, that are essential for ChatGPT-based marketing; doing so can help revolutionize marketing while avoiding potential harm to stakeholders.", "year": 2023, "publicationdate": "2023-04-10", "externalids": {"DOI": "10.3390/ai4020019"}, "doi_lower": "10.3390/ai4020019"}
{"paper_id": 247794227, "title": "Image Segmentation Using Text and Image Prompts", "author_names": ["Timo Lüddecke", "Alexander S. Ecker"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Image segmentation is usually addressed by training a model for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive as it requires re-training the model on a dataset that encompasses these expressions. Here we propose a system that can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text or an image. This approach enables us to create a unified model (trained once) for three common segmentation tasks, which come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation. We build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense prediction. After training on an extended version of the PhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on an additional image expressing the query. We analyze different variants of the latter image-based prompts in detail. This novel hybrid input allows for dynamic adaptation not only to the three segmentation tasks mentioned above, but to any binary segmentation task where a text or image query can be formulated. Finally, we find our system to adapt well to generalized queries involving affordances or properties. Code is available at https://eckerlab.org/code/CLIPSeg", "year": 2021, "publicationdate": "2021-12-18", "externalids": {"DOI": "10.1109/CVPR52688.2022.00695"}, "doi_lower": "10.1109/cvpr52688.2022.00695"}
{"paper_id": 247292561, "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection", "author_names": ["Hao Zhang", "Feng Li", "Shilong Liu", "Lei Zhang", "Hang Su", "Jun-Juan Zhu", "L. Ni", "H. Shum"], "venue": "International Conference on Learning Representations", "abstract": "We present DINO (\\textbf{D}ETR with \\textbf{I}mproved de\\textbf{N}oising anch\\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves $49.4$AP in $12$ epochs and $51.3$AP in $24$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of $\\textbf{+6.0}$\\textbf{AP} and $\\textbf{+2.7}$\\textbf{AP}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \\texttt{val2017} ($\\textbf{63.2}$\\textbf{AP}) and \\texttt{test-dev} (\\textbf{$\\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \\url{https://github.com/IDEACVR/DINO}.", "year": 2022, "publicationdate": "2022-03-07", "externalids": {"DOI": "10.48550/arXiv.2203.03605"}, "doi_lower": "10.48550/arxiv.2203.03605"}
{"paper_id": 258762550, "title": "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold", "author_names": ["Xingang Pan", "A. Tewari", "Thomas Leimkühler", "Lingjie Liu", "Abhimitra Meka", "C. Theobalt"], "venue": "International Conference on Computer Graphics and Interactive Techniques", "abstract": "Synthesizing visual content that meets users’ needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects. Existing approaches gain controllability of generative adversarial networks (GANs) via manually annotated training data or a prior 3D model, which often lack flexibility, precision, and generality. In this work, we study a powerful yet much less explored way of controlling GANs, that is, to \"drag\" any points of the image to precisely reach target points in a user-interactive manner, as shown in Fig.1. To achieve this, we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object’s rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion.", "year": 2023, "publicationdate": "2023-05-18", "externalids": {"DOI": "10.1145/3588432.3591500"}, "doi_lower": "10.1145/3588432.3591500"}
{"paper_id": 246411402, "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "author_names": ["Junnan Li", "Dongxu Li", "Caiming Xiong", "S. Hoi"], "venue": "International Conference on Machine Learning", "abstract": "Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 257622938, "title": "SUD2: Supervision by Denoising Diffusion Models for Image Reconstruction", "author_names": ["Matthew A. Chan", "Sean I. Young", "Christopher A. Metzler"], "venue": "arXiv.org", "abstract": "Many imaging inverse problems$\\unicode{x2014}$such as image-dependent in-painting and dehazing$\\unicode{x2014}$are challenging because their forward models are unknown or depend on unknown latent parameters. While one can solve such problems by training a neural network with vast quantities of paired training data, such paired training data is often unavailable. In this paper, we propose a generalized framework for training image reconstruction networks when paired training data is scarce. In particular, we demonstrate the ability of image denoising algorithms and, by extension, denoising diffusion models to supervise network training in the absence of paired training data.", "year": 2023, "publicationdate": "2023-03-16", "externalids": {"DOI": "10.48550/arXiv.2303.09642"}, "doi_lower": "10.48550/arxiv.2303.09642"}
{"paper_id": 258236522, "title": "Supervision by Denoising", "author_names": ["Sean I. Young", "Adrian V. Dalca", "Enzo Ferrante", "P. Golland", "Christopher A. Metzler", "B. Fischl", "J. E. Iglesias"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "Learning-based image reconstruction models, such as those based on the U-Net, require a large set of labeled images if good generalization is to be guaranteed. In some imaging domains, however, labeled data with pixel- or voxel-level label accuracy are scarce due to the cost of acquiring them. This problem is exacerbated further in domains like medical imaging, where there is no single ground truth label, resulting in large amounts of repeat variability in the labels. Therefore, training reconstruction networks to generalize better by learning from both labeled and unlabeled examples (called semi-supervised learning) is problem of practical and theoretical interest. However, traditional semi-supervised learning methods for image reconstruction often necessitate handcrafting a differentiable regularizer specific to some given imaging problem, which can be extremely time-consuming. In this work, we propose “supervision by denoising” (SUD), a framework to supervise reconstruction models using their own denoised output as labels. SUD unifies stochastic averaging and spatial denoising techniques under a spatio-temporal denoising framework and alternates denoising and model weight update steps in an optimization framework for semi-supervision. As example applications, we apply SUD to two problems from biomedical imaging—anatomical brain reconstruction (3D) and cortical parcellation (2D)—to demonstrate a significant improvement in reconstruction over supervised-only and ensembling baselines.", "year": 2022, "publicationdate": "2022-02-07", "externalids": {"DOI": "10.1109/TPAMI.2023.3299789"}, "doi_lower": "10.1109/tpami.2023.3299789"}
{"paper_id": 246240274, "title": "RePaint: Inpainting using Denoising Diffusion Probabilistic Models", "author_names": ["Andreas Lugmayr", "Martin Danelljan", "Andrés Romero", "F. Yu", "R. Timofte", "L. Gool"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image infor-mation. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks. Re-Paint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions. Github Repository: git.io/RePaint", "year": 2022, "publicationdate": "2022-01-24", "externalids": {"DOI": "10.1109/CVPR52688.2022.01117"}, "doi_lower": "10.1109/cvpr52688.2022.01117"}
{"paper_id": 262045903, "title": "Gradpaint: Gradient-Guided Inpainting with Diffusion Models", "author_names": ["Asya Grechka", "Guillaume Couairon", "Matthieu Cord"], "venue": "Computer Vision and Image Understanding", "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have recently achieved remarkable results in conditional and unconditional image generation. The pre-trained models can be adapted without further training to different downstream tasks, by guiding their iterative denoising process at inference time to satisfy additional constraints. For the specific task of image inpainting, the current guiding mechanism relies on copying-and-pasting the known regions from the input image at each denoising step. However, diffusion models are strongly conditioned by the initial random noise, and therefore struggle to harmonize predictions inside the inpainting mask with the real parts of the input image, often producing results with unnatural artifacts. Our method, dubbed GradPaint, steers the generation towards a globally coherent image. At each step in the denoising process, we leverage the model's\"denoised image estimation\"by calculating a custom loss measuring its coherence with the masked input image. Our guiding mechanism uses the gradient obtained from backpropagating this loss through the diffusion model itself. GradPaint generalizes well to diffusion models trained on various datasets, improving upon current state-of-the-art supervised and unsupervised methods.", "year": 2023, "publicationdate": "2023-09-18", "externalids": {"DOI": "10.48550/arXiv.2309.09614"}, "doi_lower": "10.48550/arxiv.2309.09614"}
{"paper_id": 259298715, "title": "Pseudoinverse-Guided Diffusion Models for Inverse Problems", "author_names": ["Jiaming Song", "Arash Vahdat", "M. Mardani", "Jan Kautz"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 258041305, "title": "Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models", "author_names": ["Guanhua Zhang", "Jiabao Ji", "Yang Zhang", "Mo Yu", "T. Jaakkola", "Shiyu Chang"], "venue": "International Conference on Machine Learning", "abstract": "Image inpainting refers to the task of generating a complete, natural image based on a partially revealed reference image. Recently, many research interests have been focused on addressing this problem using fixed diffusion models. These approaches typically directly replace the revealed region of the intermediate or final generated images with that of the reference image or its variants. However, since the unrevealed regions are not directly modified to match the context, it results in incoherence between revealed and unrevealed regions. To address the incoherence problem, a small number of methods introduce a rigorous Bayesian framework, but they tend to introduce mismatches between the generated and the reference images due to the approximation errors in computing the posterior distributions. In this paper, we propose COPAINT, which can coherently inpaint the whole image without introducing mismatches. COPAINT also uses the Bayesian framework to jointly modify both revealed and unrevealed regions, but approximates the posterior distribution in a way that allows the errors to gradually drop to zero throughout the denoising steps, thus strongly penalizing any mismatches with the reference image. Our experiments verify that COPAINT can outperform the existing diffusion-based methods under both objective and subjective metrics. The codes are available at https://github.com/UCSB-NLP-Chang/CoPaint/.", "year": 2023, "publicationdate": "2023-04-06", "externalids": {"DOI": "10.48550/arXiv.2304.03322"}, "doi_lower": "10.48550/arxiv.2304.03322"}
{"paper_id": 257767119, "title": "DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency", "author_names": ["Zalan Fabian", "Berk Tınaz", "M. Soltanolkotabi"], "venue": "International Conference on Machine Learning", "abstract": "Diffusion models have established new state of the art in a multitude of computer vision tasks, including image restoration. Diffusion-based inverse problem solvers generate reconstructions of exceptional visual quality from heavily corrupted measurements. However, in what is widely known as the perception-distortion trade-off, the price of perceptually appealing reconstructions is often paid in declined distortion metrics, such as PSNR. Distortion metrics measure faithfulness to the observation, a crucial requirement in inverse problems. In this work, we propose a novel framework for inverse problem solving, namely we assume that the observation comes from a stochastic degradation process that gradually degrades and noises the original clean image. We learn to reverse the degradation process in order to recover the clean image. Our technique maintains consistency with the original measurement throughout the reverse process, and allows for great flexibility in trading off perceptual quality for improved distortion metrics and sampling speedup via early-stopping. We demonstrate the efficiency of our method on different high-resolution datasets and inverse problems, achieving great improvements over other state-of-the-art diffusion-based methods with respect to both perceptual and distortion metrics.", "year": 2023, "publicationdate": "2023-03-25", "externalids": {"DOI": "10.48550/arXiv.2303.14353"}, "doi_lower": "10.48550/arxiv.2303.14353"}
{"paper_id": 4555207, "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "author_names": ["M. Sandler", "Andrew G. Howard", "Menglong Zhu", "A. Zhmoginov", "Liang-Chieh Chen"], "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.", "year": 2018, "publicationdate": "2018-01-13", "externalids": {"DOI": "10.1109/CVPR.2018.00474"}, "doi_lower": "10.1109/cvpr.2018.00474"}
{"paper_id": 237513361, "title": "Resolution-robust Large Mask Inpainting with Fourier Convolutions", "author_names": ["Roman Suvorov", "Elizaveta Logacheva", "Anton Mashikhin", "Anastasia Remizova", "Arsenii Ashukha", "Aleksei Silvestrov", "Naejin Kong", "Harshith Goka", "Kiwoong Park", "V. Lempitsky"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "Modern image inpainting systems, despite the significant progress, often struggle with large missing areas, complex geometric structures, and high-resolution images. We find that one of the main reasons for that is the lack of an effective receptive field in both the inpainting network and the loss function. To alleviate this issue, we propose a new method called large mask inpainting (LaMa). LaMa is based on i) a new inpainting network architecture that uses fast Fourier convolutions (FFCs), which have the image-wide receptive field; ii) a high receptive field perceptual loss; iii) large training masks, which unlocks the potential of the first two components. Our inpainting network improves the state-of-the-art across a range of datasets and achieves excellent performance even in challenging scenarios, e.g. completion of periodic structures. Our model generalizes surprisingly well to resolutions that are higher than those seen at train time, and achieves this at lower parameter&time costs than the competitive baselines. The code is available at https://github.com/saic-mdal/lama.", "year": 2021, "publicationdate": "2021-09-15", "externalids": {"DOI": "10.1109/WACV51458.2022.00323"}, "doi_lower": "10.1109/wacv51458.2022.00323"}
{"paper_id": 263622002, "title": "EditVal: Benchmarking Diffusion Based Text-Guided Image Editing Methods", "author_names": ["Samyadeep Basu", "Mehrdad Saberi", "S. Bhardwaj", "Atoosa Malemir Chegini", "Daniela Massiceti", "Maziar Sanjabi", "S. Hu", "S. Feizi"], "venue": "arXiv.org", "abstract": "A plethora of text-guided image editing methods have recently been developed by leveraging the impressive capabilities of large-scale diffusion-based generative models such as Imagen and Stable Diffusion. A standardized evaluation protocol, however, does not exist to compare methods across different types of fine-grained edits. To address this gap, we introduce EditVal, a standardized benchmark for quantitatively evaluating text-guided image editing methods. EditVal consists of a curated dataset of images, a set of editable attributes for each image drawn from 13 possible edit types, and an automated evaluation pipeline that uses pre-trained vision-language models to assess the fidelity of generated images for each edit type. We use EditVal to benchmark 8 cutting-edge diffusion-based editing methods including SINE, Imagic and Instruct-Pix2Pix. We complement this with a large-scale human study where we show that EditVall's automated evaluation pipeline is strongly correlated with human-preferences for the edit types we considered. From both the human study and automated evaluation, we find that: (i) Instruct-Pix2Pix, Null-Text and SINE are the top-performing methods averaged across different edit types, however {\\it only} Instruct-Pix2Pix and Null-Text are able to preserve original image properties; (ii) Most of the editing methods fail at edits involving spatial operations (e.g., changing the position of an object). (iii) There is no `winner' method which ranks the best individually across a range of different edit types. We hope that our benchmark can pave the way to developing more reliable text-guided image editing tools in the future. We will publicly release EditVal, and all associated code and human-study templates to support these research directions in https://deep-ml-research.github.io/editval/.", "year": 2023, "publicationdate": "2023-10-03", "externalids": {"DOI": "10.48550/arXiv.2310.02426"}, "doi_lower": "10.48550/arxiv.2310.02426"}
{"paper_id": 14113767, "title": "Microsoft COCO: Common Objects in Context", "author_names": ["Tsung-Yi Lin", "M. Maire", "Serge J. Belongie", "James Hays", "P. Perona", "Deva Ramanan", "Piotr Dollár", "C. L. Zitnick"], "venue": "European Conference on Computer Vision", "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.", "year": 2014, "publicationdate": "2014-05-01", "externalids": {"DOI": "10.1007/978-3-319-10602-1_48"}, "doi_lower": "10.1007/978-3-319-10602-1_48"}
{"paper_id": 233296711, "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning", "author_names": ["Jack Hessel", "Ari Holtzman", "Maxwell Forbes", "Ronan Le Bras", "Yejin Choi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.", "year": 2021, "publicationdate": "2021-04-18", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.595"}, "doi_lower": "10.18653/v1/2021.emnlp-main.595"}
{"paper_id": 257636562, "title": "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering", "author_names": ["Yushi Hu", "Benlin Liu", "Jungo Kasai", "Yizhong Wang", "Mari Ostendorf", "Ranjay Krishna", "Noah A. Smith"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Despite thousands of researchers, engineers, and artists actively working on improving text-to-image generation models, systems often fail to produce images that accurately align with the text inputs. We introduce TIFA (Text-to-Image Faithfulness evaluation with question Answering), an automatic evaluation metric that measures the faithfulness of a generated image to its text input via visual question answering (VQA). Specifically, given a text input, we automatically generate several question-answer pairs using a language model. We calculate image faithfulness by checking whether existing VQA models can answer these questions using the generated image. TIFA is a reference-free metric that allows for fine-grained and interpretable evaluations of generated images. TIFA also has better correlations with human judgments than existing metrics. Based on this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse text inputs and 25K questions across 12 categories (object, counting, etc.). We present a comprehensive evaluation of existing text-to-image models using TIFA v1.0 and highlight the limitations and challenges of current models. For instance, we find that current text-to-image models, despite doing well on color and material, still struggle in counting, spatial relations, and composing multiple objects. We hope our benchmark will help carefully measure the research progress in text-to-image synthesis and provide valuable insights for further research. 1", "year": 2023, "publicationdate": "2023-03-21", "externalids": {"DOI": "10.1109/ICCV51070.2023.01866"}, "doi_lower": "10.1109/iccv51070.2023.01866"}
{"paper_id": 265466173, "title": "Adversarial Diffusion Distillation", "author_names": ["Axel Sauer", "Dominik Lorenz", "A. Blattmann", "Robin Rombach"], "venue": "European Conference on Computer Vision", "abstract": "We introduce Adversarial Diffusion Distillation (ADD), a novel training approach that efficiently samples large-scale foundational image diffusion models in just 1-4 steps while maintaining high image quality. We use score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal in combination with an adversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps. Our analyses show that our model clearly outperforms existing few-step methods (GANs, Latent Consistency Models) in a single step and reaches the performance of state-of-the-art diffusion models (SDXL) in only four steps. ADD is the first method to unlock single-step, real-time image synthesis with foundation models. Code and weights available under https://github.com/Stability-AI/generative-models and https://huggingface.co/stabilityai/ .", "year": 2023, "publicationdate": "2023-11-28", "externalids": {"DOI": "10.48550/arXiv.2311.17042"}, "doi_lower": "10.48550/arxiv.2311.17042"}
{"paper_id": 267028569, "title": "One-Step Diffusion Distillation via Deep Equilibrium Models", "author_names": ["Zhengyang Geng", "Ashwini Pokle", "J. Z. Kolter", "Paul Micaelli", "Arash Vahdat", "Hongxu Yin", "Jan Kautz", "Pavlo Molchanov", "Alex Nichol", "Prafulla Dhariwal", "Aditya Ramesh", "Pranav Shyam", "Pamela Mishkin", "Bob McGrew", "I. Sutskever", "Emilio Parisotto", "Francis Song", "Jack W. Rae", "Razvan Pascanu", "Caglar Gulcehre", "Siddhant M. Jayakumar", "Max Jaderberg", "Raphael Lopez Kaufman", "Aidan Clark", "Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zem-ing Lin", "N. Gimelshein", "L. Antiga", "Alban Des-maison", "Andreas Kopf", "Edward Yang", "Zachary DeVito", "Martin Raison", "Alykhan Tejani", "Sasank Chilamkurthy", "Alec Radford", "Jeffrey Wu", "R. Child", "D. Luan", "Colin Raffel", "Noam Shazeer", "A. Roberts", "K. Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu", "Casey Chu"], "venue": "Neural Information Processing Systems", "abstract": "Diffusion models excel at producing high-quality samples but naively require hundreds of iterations, prompting multiple attempts to distill the generation process into a faster network. However, many existing approaches suffer from a variety of challenges: the process for distillation training can be complex, often requiring multiple training stages, and the resulting models perform poorly when utilized in single-step generative applications. In this paper, we introduce a simple yet effective means of distilling diffusion models directly from initial noise to the resulting image. Of particular importance to our approach is to leverage a new Deep Equilibrium (DEQ) model as the distilled architecture: the Generative Equilibrium Transformer (GET). Our method enables fully offline training with just noise/image pairs from the diffusion model while achieving superior performance compared to existing one-step methods on comparable training budgets. We demonstrate that the DEQ architecture is crucial to this capability, as GET matches a $5\\times$ larger ViT in terms of FID scores while striking a critical balance of computational cost and image quality. Code, checkpoints, and datasets are available.", "year": 2023, "publicationdate": "2023-12-12", "externalids": {"DOI": "10.48550/arXiv.2401.08639"}, "doi_lower": "10.48550/arxiv.2401.08639"}
{"paper_id": 257280191, "title": "Consistency Models", "author_names": ["Yang Song", "Prafulla Dhariwal", "Mark Chen", "I. Sutskever"], "venue": "International Conference on Machine Learning", "abstract": "Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.", "year": 2023, "publicationdate": "2023-03-02", "externalids": {"DOI": "10.1007/978-1-4842-1329-2_9"}, "doi_lower": "10.1007/978-1-4842-1329-2_9"}
{"paper_id": 263831037, "title": "Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference", "author_names": ["Simian Luo", "Yiqin Tan", "Longbo Huang", "Jian Li", "Hang Zhao"], "venue": "arXiv.org", "abstract": "Latent Diffusion models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling process is computationally intensive and leads to slow generation. Inspired by Consistency Models (song et al.), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (rombach et al). Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently distilled from pre-trained classifier-free guided diffusion models, a high-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training. Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference. Project Page: https://latent-consistency-models.github.io/", "year": 2023, "publicationdate": "2023-10-06", "externalids": {}, "doi_lower": null}
{"paper_id": 265466277, "title": "MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices", "author_names": ["Yang Zhao", "Yanwu Xu", "Zhisheng Xiao", "Tingbo Hou"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2311.16567"}, "doi_lower": "10.48550/arxiv.2311.16567"}
{"paper_id": 258999690, "title": "SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds", "author_names": ["Yanyu Li", "Huan Wang", "Qing Jin", "Ju Hu", "Pavlo Chemerys", "Yun Fu", "Yanzhi Wang", "S. Tulyakov", "Jian Ren"], "venue": "Neural Information Processing Systems", "abstract": "Text-to-image diffusion models can create stunning images from natural language descriptions that rival the work of professional artists and photographers. However, these models are large, with complex network architectures and tens of denoising iterations, making them computationally expensive and slow to run. As a result, high-end GPUs and cloud-based inference are required to run diffusion models at scale. This is costly and has privacy implications, especially when user data is sent to a third party. To overcome these challenges, we present a generic approach that, for the first time, unlocks running text-to-image diffusion models on mobile devices in less than $2$ seconds. We achieve so by introducing efficient network architecture and improving step distillation. Specifically, we propose an efficient UNet by identifying the redundancy of the original model and reducing the computation of the image decoder via data distillation. Further, we enhance the step distillation by exploring training strategies and introducing regularization from classifier-free guidance. Our extensive experiments on MS-COCO show that our model with $8$ denoising steps achieves better FID and CLIP scores than Stable Diffusion v$1.5$ with $50$ steps. Our work democratizes content creation by bringing powerful text-to-image diffusion models to the hands of users.", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.48550/arXiv.2306.00980"}, "doi_lower": "10.48550/arxiv.2306.00980"}
{"paper_id": 235458009, "title": "LoRA: Low-Rank Adaptation of Large Language Models", "author_names": ["J. E. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Weizhu Chen"], "venue": "International Conference on Learning Representations", "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.", "year": 2021, "publicationdate": "2021-06-17", "externalids": {}, "doi_lower": null}
{"paper_id": 253420366, "title": "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models", "author_names": ["P. Schramowski", "Manuel Brack", "Bjorn Deiseroth", "K. Kersting"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment.11Code available at https://huggingface.co/docs/diffusers/api/pipelines/stable.diffusion.safe", "year": 2022, "publicationdate": "2022-11-09", "externalids": {"DOI": "10.1109/CVPR52729.2023.02157"}, "doi_lower": "10.1109/cvpr52729.2023.02157"}
{"paper_id": 266173984, "title": "Relightful Harmonization: Lighting-Aware Portrait Background Replacement", "author_names": ["Mengwei Ren", "Wei Xiong", "Jae Shin Yoon", "Zhixin Shu", "Jianming Zhang", "Hyunjoon Jung", "Guido Gerig", "He Zhang"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Portrait harmonization aims to composite a subject into a new background, adjusting its lighting and color to ensure harmony with the background scene. Existing harmo-nization techniques often only focus on adjusting the global color and brightness of the foreground and ignore crucial illumination cues from the background such as apparent lighting direction, leading to unrealistic compositions. We introduce Relightful Harmonization, a lighting-aware diffusion model designed to seamlessly harmonize sophisticated lighting effect for the foreground portrait using any back-ground image. Our approach unfolds in three stages. First, we introduce a lighting representation module that allows our diffusion model to encode lighting information from target image background. Second, we introduce an alignment network that aligns lighting features learned from image background with lighting features learned from panorama environment maps, which is a complete representation for scene illumination. Last, to further boost the photorealism of the proposed method, we introduce a novel data simulation pipeline that generates synthetic training pairs from a diverse range of natural images, which are used to refine the model. Our method outperforms existing benchmarks in visual fidelity and lighting coherence, showing superior generalization in real-world testing scenarios, highlighting its versatility and practicality.", "year": 2023, "publicationdate": "2023-12-11", "externalids": {"DOI": "10.1109/CVPR52733.2024.00617"}, "doi_lower": "10.1109/cvpr52733.2024.00617"}
{"paper_id": 251692123, "title": "Total relighting", "author_names": ["Rohit Pandey", "S. Escolano", "Christoph Rhemann", "P. Debevec", "S. Fanello"], "venue": "ACM Transactions on Graphics", "abstract": "We propose a novel system for portrait relighting and background replacement, which maintains high-frequency boundary details and accurately synthesizes the subject's appearance as lit by novel illumination, thereby producing realistic composite images for any desired scene. Our technique includes foreground estimation via alpha matting, relighting, and compositing. We demonstrate that each of these stages can be tackled in a sequential pipeline without the use of priors (e.g. known background or known illumination) and with no specialized acquisition techniques, using only a single RGB portrait image and a novel, target HDR lighting environment as inputs. We train our model using relit portraits of subjects captured in a light stage computational illumination system, which records multiple lighting conditions, high quality geometry, and accurate alpha mattes. To perform realistic relighting for compositing, we introduce a novel per-pixel lighting representation in a deep learning framework, which explicitly models the diffuse and the specular components of appearance, producing relit portraits with convincingly rendered non-Lambertian effects like specular highlights. Multiple experiments and comparisons show the effectiveness of the proposed approach when applied to in-the-wild images.", "year": 2021, "publicationdate": "2021-07-17", "externalids": {"DOI": "10.1145/3476576.3476588"}, "doi_lower": "10.1145/3476576.3476588"}
{"paper_id": 258108166, "title": "DiffusionRig: Learning Personalized Priors for Facial Appearance Editing", "author_names": ["Zheng Ding", "X. Zhang", "Zhihao Xia", "Lars Jebe", "Z. Tu", "Xiuming Zhang"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We address the problem of learning person-specific facial priors from a small number (e.g., 20) of portrait photos of the same person. This enables us to edit this specific person's facial appearance, such as expression and lighting, while preserving their identity and high-frequency facial details. Key to our approach, which we dub DiffusionRig, is a diffusion model conditioned on, or “rigged by,“ crude 3D face models estimated from single in-the-wild images by an off-the-shelf estimator. On a high level, DiffusionRig learns to map simplistic renderings of 3D face models to realistic photos of a given person. Specifically, DiffusionRig is trained in two stages: It first learns generic facial priors from a large-scale face dataset and then person-specific priors from a small portrait photo collection of the person of interest. By learning the CGI-to-photo mapping with such personalized priors,DiffusionRig can “rig“ the lighting, facial expression, head pose, etc. of a portrait photo, conditioned only on coarse 3D models while preserving this person's identity and other high-frequency characteristics. Qualitative and quantitative experiments show that DiffusionRig outperforms existing approaches in both identity preservation and photorealism. Please see the project website: https://diffusionrig.github.io for the supplemental material, video, code, and data.", "year": 2023, "publicationdate": "2023-04-13", "externalids": {"DOI": "10.1109/CVPR52729.2023.01225"}, "doi_lower": "10.1109/cvpr52729.2023.01225"}
{"paper_id": 258212631, "title": "DiFaReli++: Diffusion Face Relighting with Consistent Cast Shadows", "author_names": ["Puntawat Ponglertnapakorn", "Nontawat Tritrong", "Supasorn Suwajanakorn"], "venue": "", "abstract": "We introduce a novel approach to single-view face relighting in the wild, addressing challenges such as global illumination and cast shadows. A common scheme in recent methods involves intrinsically decomposing an input image into 3D shape, albedo, and lighting, then recomposing it with the target lighting. However, estimating these components is error-prone and requires many training examples with ground-truth lighting to generalize well. Our work bypasses the need for accurate intrinsic estimation and can be trained solely on 2D images without any light stage data, relit pairs, multi-view images, or lighting ground truth. Our key idea is to leverage a conditional diffusion implicit model (DDIM) for decoding a disentangled light encoding along with other encodings related to 3D shape and facial identity inferred from off-the-shelf estimators. We propose a novel conditioning technique that simplifies modeling the complex interaction between light and geometry. It uses a rendered shading reference along with a shadow map, inferred using a simple and effective technique, to spatially modulate the DDIM. Moreover, we propose a single-shot relighting framework that requires just one network pass, given pre-processed data, and even outperforms the teacher model across all metrics. Our method realistically relights in-the-wild images with temporally consistent cast shadows under varying lighting conditions. We achieve state-of-the-art performance on the standard benchmark Multi-PIE and rank highest in user studies.", "year": 2023, "publicationdate": "2023-04-19", "externalids": {}, "doi_lower": null}
{"paper_id": 258999493, "title": "FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models", "author_names": ["Hao Zhang", "Yanbo Xu", "Tianyuan Dai", "Yu-Wing", "Tai Chi-Keung Tang"], "venue": "Neural Information Processing Systems", "abstract": "The ability to create high-quality 3D faces from a single image has become increasingly important with wide applications in video conferencing, AR/VR, and advanced video editing in movie industries. In this paper, we propose Face Diffusion NeRF (FaceDNeRF), a new generative method to reconstruct high-quality Face NeRFs from single images, complete with semantic editing and relighting capabilities. FaceDNeRF utilizes high-resolution 3D GAN inversion and expertly trained 2D latent-diffusion model, allowing users to manipulate and construct Face NeRFs in zero-shot learning without the need for explicit 3D data. With carefully designed illumination and identity preserving loss, as well as multi-modal pre-training, FaceDNeRF offers users unparalleled control over the editing process enabling them to create and edit face NeRFs using just single-view images, text prompts, and explicit target lighting. The advanced features of FaceDNeRF have been designed to produce more impressive results than existing 2D editing approaches that rely on 2D segmentation maps for editable attributes. Experiments show that our FaceDNeRF achieves exceptionally realistic results and unprecedented flexibility in editing compared with state-of-the-art 3D face reconstruction and editing methods. Our code will be available at https://github.com/BillyXYB/FaceDNeRF.", "year": 2023, "publicationdate": "2023-06-01", "externalids": {}, "doi_lower": null}
{"paper_id": 265608717, "title": "Lasagna: Layered Score Distillation for Disentangled Object Relighting", "author_names": ["D. Bashkirova", "Arijit Ray", "Rupayan Mallick", "Sarah Adel Bargal", "Jianming Zhang", "Ranjay Krishna", "Kate Saenko"], "venue": "arXiv.org", "abstract": "Professional artists, photographers, and other visual content creators use object relighting to establish their photo's desired effect. Unfortunately, manual tools that allow relighting have a steep learning curve and are difficult to master. Although generative editing methods now enable some forms of image editing, relighting is still beyond today's capabilities; existing methods struggle to keep other aspects of the image -- colors, shapes, and textures -- consistent after the edit. We propose Lasagna, a method that enables intuitive text-guided relighting control. Lasagna learns a lighting prior by using score distillation sampling to distill the prior of a diffusion model, which has been finetuned on synthetic relighting data. To train Lasagna, we curate a new synthetic dataset ReLiT, which contains 3D object assets re-lit from multiple light source locations. Despite training on synthetic images, quantitative results show that Lasagna relights real-world images while preserving other aspects of the input image, outperforming state-of-the-art text-guided image editing methods. Lasagna enables realistic and controlled results on natural images and digital art pieces and is preferred by humans over other methods in over 91% of cases. Finally, we demonstrate the versatility of our learning objective by extending it to allow colorization, another form of image editing.", "year": 2023, "publicationdate": "2023-11-30", "externalids": {"DOI": "10.48550/arXiv.2312.00833"}, "doi_lower": "10.48550/arxiv.2312.00833"}
{"paper_id": 268512690, "title": "LightIt: Illumination Modeling and Control for Diffusion Models", "author_names": ["Peter Kocsis", "Julien Philip", "Kalyan Sunkavalli", "Matthias Nießner", "Yannick Hold-Geoffroy"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We introduce LightIt, a method for explicit illumination control for image generation. Recent generative methods lack lighting control, which is crucial to numerous artis-tic aspects of image generation such as setting the overall mood or cinematic appearance. To overcome these limi-tations, we propose to condition the generation on shading and normal maps. We model the lighting with single bounce shading, which includes cast shadows. We first train a shading estimation module to generate a dataset of real-world images and shading pairs. Then, we train a control network using the estimated shading and normals as input. Our method demonstrates high-quality image generation and lighting control in numerous scenes. Additionally, we use our generated dataset to train an identity-preserving re-lighting model, conditioned on an image and a target shading. Our method is the first that enables the generation of images with controllable, consistent lighting and performs on par with specialized relighting state-of-the-art methods.", "year": 2024, "publicationdate": "2024-03-15", "externalids": {"DOI": "10.1109/CVPR52733.2024.00894"}, "doi_lower": "10.1109/cvpr52733.2024.00894"}
{"paper_id": 270379694, "title": "Neural Gaffer: Relighting Any Object via Diffusion", "author_names": ["Haian Jin", "Yuan Li", "Fujun Luan", "Yuanbo Xiangli", "Sai Bi", "Kai Zhang", "Zexiang Xu", "Jin Sun", "Noah Snavely"], "venue": "Neural Information Processing Systems", "abstract": "Single-image relighting is a challenging task that involves reasoning about the complex interplay between geometry, materials, and lighting. Many prior methods either support only specific categories of images, such as portraits, or require special capture conditions, like using a flashlight. Alternatively, some methods explicitly decompose a scene into intrinsic components, such as normals and BRDFs, which can be inaccurate or under-expressive. In this work, we propose a novel end-to-end 2D relighting diffusion model, called Neural Gaffer, that takes a single image of any object and can synthesize an accurate, high-quality relit image under any novel environmental lighting condition, simply by conditioning an image generator on a target environment map, without an explicit scene decomposition. Our method builds on a pre-trained diffusion model, and fine-tunes it on a synthetic relighting dataset, revealing and harnessing the inherent understanding of lighting present in the diffusion model. We evaluate our model on both synthetic and in-the-wild Internet imagery and demonstrate its advantages in terms of generalization and accuracy. Moreover, by combining with other generative methods, our model enables many downstream 2D tasks, such as text-based relighting and object insertion. Our model can also operate as a strong relighting prior for 3D tasks, such as relighting a radiance field.", "year": 2024, "publicationdate": "2024-06-11", "externalids": {"DOI": "10.48550/arXiv.2406.07520"}, "doi_lower": "10.48550/arxiv.2406.07520"}
{"paper_id": 271544429, "title": "Retinex-Diffusion: On Controlling Illumination Conditions in Diffusion Models via Retinex Theory", "author_names": ["Xiaoyan Xing", "Vincent Tao Hu", "J. Metzen", "Konrad Groh", "Sezer Karaoglu", "Theo Gevers"], "venue": "arXiv.org", "abstract": "This paper introduces a novel approach to illumination manipulation in diffusion models, addressing the gap in conditional image generation with a focus on lighting conditions. We conceptualize the diffusion model as a black-box image render and strategically decompose its energy function in alignment with the image formation model. Our method effectively separates and controls illumination-related properties during the generative process. It generates images with realistic illumination effects, including cast shadow, soft shadow, and inter-reflections. Remarkably, it achieves this without the necessity for learning intrinsic decomposition, finding directions in latent space, or undergoing additional training with new datasets.", "year": 2024, "publicationdate": "2024-07-29", "externalids": {"DOI": "10.48550/arXiv.2407.20785"}, "doi_lower": "10.48550/arxiv.2407.20785"}
{"paper_id": 257900655, "title": "3D-aware Image Generation using 2D Diffusion Models", "author_names": ["Jianfeng Xiang", "Jiaolong Yang", "Binbin Huang", "Xin Tong"], "venue": "IEEE International Conference on Computer Vision", "abstract": "In this paper, we introduce a novel 3D-aware image generation method that leverages 2D diffusion models. We formulate the 3D-aware image generation task as multiview 2D image set generation, and further to a sequential unconditional–conditional multiview image generation process. This allows us to utilize 2D diffusion models to boost the generative modeling power of the method. Additionally, we incorporate depth information from monocular depth estimators to construct the training data for the conditional diffusion model using only still images.We train our method on a large-scale unstructured 2D image dataset, i.e., ImageNet, which is not addressed by previous methods. It produces high-quality images that significantly outperform prior methods. Furthermore, our approach showcases its capability to generate instances with large view angles, even though the training images are diverse and unaligned, gathered from \"in-the-wild\" realworld environments.1", "year": 2023, "publicationdate": "2023-03-31", "externalids": {"DOI": "10.1109/ICCV51070.2023.00226"}, "doi_lower": "10.1109/iccv51070.2023.00226"}
{"paper_id": 253098221, "title": "High-Resolution Image Editing via Multi-Stage Blended Diffusion", "author_names": ["J. Ackermann", "Minjun Li"], "venue": "arXiv.org", "abstract": "Diffusion models have shown great results in image generation and in image editing. However, current approaches are limited to low resolutions due to the computational cost of training diffusion models for high-resolution generation. We propose an approach that uses a pre-trained low-resolution diffusion model to edit images in the megapixel range. We first use Blended Diffusion to edit the image at a low resolution, and then upscale it in multiple stages, using a super-resolution model and Blended Diffusion. Using our approach, we achieve higher visual fidelity than by only applying off the shelf super-resolution methods to the output of the diffusion model. We also obtain better global consistency than directly using the diffusion model at a higher resolution.", "year": 2022, "publicationdate": "2022-10-24", "externalids": {"DOI": "10.48550/arXiv.2210.12965"}, "doi_lower": "10.48550/arxiv.2210.12965"}
{"paper_id": 251387787, "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "author_names": ["N. Bynagari"], "venue": "Asian Journal of Applied Science and Engineering", "abstract": "When it comes to the formation of real-looking images using some complex models, Generative Adversarial Networks do not disappoint. The complex models involved are often the types with infeasible maximum likelihoods. Be that as it may, there is not yet any proof for the convergence of GANs training. This paper proposes a TTUR (a two-time scale update rule) for training the Generative Adversarial Networks with a descent of stochastic gradient based on haphazard loss functions. The two time-scale update rule has separate learning rates for the generator and the discriminator. With the aid of the stochastic approximation theory, this paper demonstrates that the TTUR reaches a point of convergence under the influence of mild assumption to a kind of remote and stationary state known as Nash equilibrium. This unification or meeting point principle also applies to the widespread Adam optimization. This is a form or replacement optimization algorithm designed into stochastic gradient descent and used for tutoring the deep learning models in the system. For the Adam optimization theory, this paper evinces that it is in line with the dynamics of a weighty ball in a frictional state. Thus, we prove that it favours flat minina in the objective perspective of things. To carry out an evaluation of how GANs perform during the image creation process, this paper presents what we have termed the 'Fréchet Inception Distance\", also known as FID—a concept known to dwell on the resemblance between the images created and the real ones in a way that is more improved compared to the Inception Score. Experimentally, the TTUR helps in the bettering of DCGANs and Improved Wasserstein GANs (WGAN-GP). This makes it perform better than the traditional CelebA GAN training, LSUN Bedrooms, CIFAR-10, SVHN and the One Billion Word Benchmark.", "year": 2019, "publicationdate": "2019-04-25", "externalids": {"DOI": "10.18034/ajase.v8i1.9"}, "doi_lower": "10.18034/ajase.v8i1.9"}
{"paper_id": 3531856, "title": "Demystifying MMD GANs", "author_names": ["Mikolaj Binkowski", "Danica J. Sutherland", "M. Arbel", "A. Gretton"], "venue": "International Conference on Learning Representations", "abstract": "We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we clarify the situation with bias in GAN loss functions raised by recent work: we show that gradient estimators used in the optimization process for both MMD GANs and Wasserstein GANs are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramer GAN critic. Being an integral probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of GAN convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during GAN training.", "year": 2018, "publicationdate": "2018-01-04", "externalids": {}, "doi_lower": null}
{"paper_id": 259171761, "title": "DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data", "author_names": ["Stephanie Fu", "Netanel Y. Tamir", "Shobhita Sundaram", "Lucy Chai", "Richard Zhang", "Tali Dekel", "Phillip Isola"], "venue": "Neural Information Processing Systems", "abstract": "Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks.", "year": 2023, "publicationdate": "2023-06-15", "externalids": {"DOI": "10.48550/arXiv.2306.09344"}, "doi_lower": "10.48550/arxiv.2306.09344"}
{"paper_id": 264935263, "title": "Are These the Same Apple? Comparing Images Based on Object Intrinsics", "author_names": ["Klemen Kotar", "Stephen Tian", "Hong-Xing Yu", "Daniel L. K. Yamins", "Jiajun Wu"], "venue": "Neural Information Processing Systems", "abstract": "The human visual system can effortlessly recognize an object under different extrinsic factors such as lighting, object poses, and background, yet current computer vision systems often struggle with these variations. An important step to understanding and improving artificial vision systems is to measure image similarity purely based on intrinsic object properties that define object identity. This problem has been studied in the computer vision literature as re-identification, though mostly restricted to specific object categories such as people and cars. We propose to extend it to general object categories, exploring an image similarity metric based on object intrinsics. To benchmark such measurements, we collect the Common paired objects Under differenT Extrinsics (CUTE) dataset of $18,000$ images of $180$ objects under different extrinsic factors such as lighting, poses, and imaging conditions. While existing methods such as LPIPS and CLIP scores do not measure object intrinsics well, we find that combining deep features learned from contrastive self-supervised learning with foreground filtering is a simple yet effective approach to approximating the similarity. We conduct an extensive survey of pre-trained features and foreground extraction methods to arrive at a strong baseline that best measures intrinsic object-centric image similarity among current methods. Finally, we demonstrate that our approach can aid in downstream applications such as acting as an analog for human subjects and improving generalizable re-identification. Please see our project website at https://s-tian.github.io/projects/cute/ for visualizations of the data and demos of our metric.", "year": 2023, "publicationdate": "2023-11-01", "externalids": {"DOI": "10.48550/arXiv.2311.00750"}, "doi_lower": "10.48550/arxiv.2311.00750"}
