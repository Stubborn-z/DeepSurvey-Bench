{"name": "a2", "paperour": [4, 5, 3, 5, 5, 5, 5], "reason": ["4\n\nJustification:\n- Absence of Abstract: No Abstract is provided in the supplied text, so no objective is stated there. This alone prevents a top score since the rubric requires clarity in both the Abstract and Introduction.\n- Explicit objective in Introduction (Section 1.3, “Motivation for the Survey”):\n  - “this survey aims to consolidate recent advancements, analyze unresolved challenges, and identify future opportunities in the field.”\n  - “we aim to provide a foundational reference for researchers and practitioners.”\n  - “The survey will not only map the current MoE landscape but also identify pathways to harness its full potential, ensuring that sparse architectures advance AI's capabilities while promoting inclusivity and sustainability.”\n  These sentences clearly and explicitly state the survey’s objectives.\n- Additional clarity on scope (Section 1.4, “Scope and Structure of the Survey”):\n  - “This survey systematically examines the evolution, implementation, and impact of Mixture of Experts (MoE) in Large Language Models (LLMs)…” \n  This delineates the scope and reinforces the objective to comprehensively cover MoE’s evolution, implementation, and impact.\n- Linkage to core problems in the field (Section 1.3):\n  - The objective is supported by explicit articulation of key challenges such as “expert imbalance,” “routing instability,” “memory and latency constraints in edge devices,” and “bias amplification,” which grounds the survey’s aims in concrete problems.\n- Reason for not awarding 5:\n  - The lack of an Abstract with an explicit objective and the relatively broad/generic phrasing of some aims (e.g., “consolidate… analyze… identify… provide a foundational reference”) keep it from “high precision” across both required locations per the rubric.", "5\n\nEvidence of explicit classification:\n- “Core Gating Functions and Their Trade-offs\n  1. Softmax Gating…\n  2. Top-k Gating…\n  3. DSelect-k Gating…” (Section 2.2)\n- “Three principal routing strategies have gained prominence:\n  1. Top-k Gating…\n  2. Switch Routing…\n  3. Adaptive Routing…” (Section 3.1)\n- “Hierarchical MoE: Structured Specialization … Two dominant paradigms have emerged:\n  1. Pipeline MoE…\n  2. Localized MoE (LocMoE)…” (Section 3.2)\n- “Hybrid Architectures: Synergizing Parallelism … Expert + Tensor Parallelism … Data-Expert Hybrids” (Section 3.2)\n- “Soft and Dense MoE Variants … Soft MoE: Continuous Expert Blending … Dense-to-Sparse Transitions: DS-MoE” (Section 3.4)\n- “Load balancing is a critical challenge… This subsection explores techniques… \n  - Dynamic Expert Management\n  - Asynchronous Training Pipelines\n  - Clustering-Based Initialization\n  - Router Collapse Prevention\n  - Hybrid and Hierarchical Approaches” (Section 4.1)\n- “Regularization Strategies for Stable Training … \n  - Gating Dropout for Routing Stability\n  - Auxiliary Losses for Expert Specialization\n  - Consistency-Based Training\n  - Hybrid Regularization Approaches” (Section 4.2)\n- “Parameter-Efficient Fine-Tuning …\n  - Low-Rank Adaptation (LoRA) for MoE\n  - Sparse Upcycling and Expert Pruning\n  - Hybrid PEFT Approaches” (Section 4.3)\n\nEvidence of explicit development/evolution:\n- “This foundation of sparse MoE principles naturally extends into the hierarchical and hybrid architectures discussed next…” (Section 3.1)\n- “Building upon the foundational sparse MoE principles introduced in Section 3.1, hierarchical and hybrid MoE architectures have emerged as sophisticated extensions…” (Section 3.2)\n- “These architectures form a critical evolutionary step between sparse MoE fundamentals (Section 3.1) and next-generation dynamic routing approaches (Section 3.3)” (Section 3.2)\n- “Building upon the hierarchical and hybrid architectures discussed in Section 3.2, dynamic and adaptive routing strategies represent a critical advancement…” (Section 3.3)\n- “Emerging as a natural progression from the dynamic routing strategies discussed in Section 3.3, soft and dense Mixture-of-Experts (MoE) variants address fundamental limitations…” (Section 3.4)\n- “Building on the soft and dense MoE variants discussed in Section 3.4, this subsection reviews system-level optimizations…” (Section 3.5)\n- “Building on the scalability and efficiency optimizations discussed in Section 3.5, Mixture-of-Experts (MoE) architectures have demonstrated remarkable versatility in multimodal tasks…” (Section 3.6)\n- “Routing optimization lies at the core… This subsection bridges the parameter-efficient fine-tuning techniques discussed earlier… with the training stability challenges explored subsequently” (Section 4.4)\n\nThese fragments show a systematic classification across method families and a clearly staged evolution from sparse MoE, to hierarchical/hybrid, to dynamic routing, to soft/dense variants, to system-level optimizations and applications, with explicit cross-section “building upon” links that articulate the development trajectory.", "Score: 3\n\nEvidence and rationale:\n- The survey names several common datasets but provides only brief mentions without details on scale, scenarios, or labeling protocols. For example:\n  - “For vision-language tasks, models like [8] and [119] are tested on ImageNet-1k, COCO, and VQA benchmarks, measuring their ability to integrate multimodal inputs efficiently.”\n  - “Task-specific evaluations (GLUE, SuperGLUE) [47] and case studies (GShard, V-MoE) demonstrate efficiency gains [46], while scalability analyses confirm sublinear cost growth in trillion-parameter regimes [42].”\n  - “In multilingual and retrieval tasks, benchmarks like WMT and GLUE/SuperGLUE are adapted to evaluate dynamic resource allocation.”\n  - “Medical VQA … achieving state-of-the-art accuracy on VQA-RAD.” and “improving AUROC by 12% on MIMIC-III.”\n\n- The metrics coverage is broad and relevant, but often high-level and not tied to detailed experimental protocols:\n  - “we compare MoE and dense models across FLOPs, latency, and memory metrics [44].”\n  - “Multimodal MoE evaluation requires metrics beyond traditional accuracy: 1. Expert Utilization Efficiency … 2. Computational Cost: FLOPs, latency, and memory usage … 3. Cross-Domain Generalization.”\n  - “Proposed solutions include expert activation entropy to measure utilization balance and task-specific contribution scores to evaluate specialization…”\n  - “Throughput—tokens processed per second—favors MoEs in batch processing due to their inherent parallelism.”\n  - Task-specific metrics are mentioned (e.g., “BLEU scores” for translation; “higher ROUGE scores” in summarization; “AUROC” in clinical prediction), but without dataset scale, label schema, or annotation details.\n\n- The survey does justify why certain metrics are used (e.g., FLOPs/latency/memory for efficiency; expert utilization for MoE-specific behavior), and it identifies gaps and proposes new evaluation directions:\n  - “Benchmarking … requires metrics beyond traditional accuracy,” and “Key challenges persist: … Inconsistent Routing Strategies … Hardware Heterogeneity … Data Imbalance.”\n  - “Open problems in evaluation” propose “expert activation entropy” and “routing stability scores,” indicating awareness of MoE-specific needs.\n\nDowngrade reasons:\n- There is no systematic description of dataset scales, splits, labeling procedures, or scenario coverage for the cited benchmarks (e.g., GLUE/SuperGLUE, WMT, ImageNet/COCO/VQA, VQA-RAD, MIMIC-III).\n- Experimental settings and evaluation protocols are not detailed (e.g., batch sizes, hardware, versions of benchmarks, or annotation guidelines), limiting reproducibility and clarity.\n- While new metrics are proposed, their applicability and concrete computation methods are not fully specified.", "Score: 5/5\n\nJustification:\n- The survey provides a structured, multi-dimensional comparison across multiple axes: computational efficiency (FLOPs), latency, memory footprint, throughput, training stability, task performance, and deployment constraints.\n- It explicitly contrasts MoE with dense models and with traditional ensemble methods, detailing similarities, differences, advantages, and disadvantages, and it repeatedly uses clear comparative wording (“in contrast,” “however,” “conversely,” etc.).\n- Comparisons are organized into dedicated subsections (e.g., “Comparison with Dense Models,” “Comparison with Traditional Ensemble Methods,” “Comparative Metrics for MoE and Dense Models”) and reinforced with case studies and empirical trade-offs. The paper also highlights context-dependent preferences (e.g., uniform vs. heterogeneous tasks; small-batch latency vs. high-throughput scenarios) and acknowledges MoE-specific overheads (routing latency, load imbalance) versus dense/ensemble strengths (predictability, simplicity).\n- Overall, the comparisons are technically detailed and multi-perspective, not just listings, and include pros/cons and conditions under which each approach is favored.\n\nRepresentative contrastive quotes:\n- “In contrast, dense models uniformly activate all parameters per token, leading to linear computational growth—a bottleneck for trillion-parameter scaling [42].”\n- “However, MoE’s efficiency gains come with overheads: Routing Latency: Gating mechanisms introduce decision costs absent in dense models… Dense models provide predictable, uniform computation that simplifies acceleration but lacks MoE’s adaptive efficiency.”\n- “Dense models face quadratic cost growth with parameter counts, limiting scalability. However, they avoid MoE-specific challenges: Expert Collapse… Memory Overhead: Storing inactive experts requires sophisticated buffering techniques [42].”\n- “MoE Advantages: Excels in heterogeneous tasks… Dense Strengths: Outperforms in uniform processing tasks… Low-resource settings also favor dense models’ simpler architectures [38].”\n- “In contrast, MoE employs dynamic routing through a trainable gating network that selectively activates experts per input token…—a capability absent in static ensembles.”\n- “In contrast, traditional ensembles execute all sub-models for every input, leading to linear growth in computational cost.”\n- “This contrasts with ensemble methods, where sub-models are trained independently on bootstrapped data without mechanisms for conditional parameter updates.”\n- “However, joint training introduces unique challenges… These issues have no direct analogs in ensemble methods, which avoid such instability by design…”\n- “MoE excels in scenarios requiring scalable, input-adaptive computation… Conversely, ensembles offer simplicity and interpretability…”\n- “Soft MoE replaces discrete top-k routing with continuous expert combinations, resolving key instability issues in sparse MoE training… However, this comes at a potential cost to fine-grained specialization…”\n- “Dense models typically exhibit lower latency for small batch sizes due to fixed computation paths. However, MoEs excel in high-throughput scenarios…”\n- “Memory efficiency is a key strength of MoEs… In contrast, dense models struggle with comparable compression without significant performance degradation.”\n- “Dense models offer more predictable throughput due to uniform computation… Hybrid approaches… reduce throughput variability, achieving 1.5–1.86x speed-ups over pure MoEs.”\n- “However, performance gaps vary by task type… dense models sometimes outperform MoE variants… whereas MoE excels in semantic tasks (e.g., MNLI)…”\n- “MoE inference latency can surpass dense models in I/O-bound scenarios due to irregular memory access…”", "Score: 5/5\n\nEvidence of explicit reasoning (design trade-offs, “why” explanations, limitations, implications):\n- “The gating mechanism is pivotal to MoE's success, balancing expert specialization with load balancing.” (1.1)  \n- “The primary advantage of MoE lies in its ability to decouple model size from computational cost… The sublinear scaling property of MoE is particularly critical for trillion-parameter models…” (1.2)  \n- “MoE architectures not only reduce computational costs but also improve model performance through expert specialization… This dynamic specialization is absent in dense models, where all parameters are uniformly applied regardless of input complexity.” (1.2)  \n- “Softmax Gating… often leads to the ‘rich-get-richer’ problem… Top-k Gating… risks load imbalance… Expert-choice routing… where experts select tokens rather than vice versa.” (2.2)  \n- “However, sparsity introduces trade-offs: 1) Load Imbalance… 2) Token Dropping… 3) Redundancy…” (2.3)  \n- “MoE's efficiency gains come with overheads: Routing Latency… Load Imbalance… Dense models provide predictable, uniform computation that simplifies acceleration but lacks MoE's adaptive efficiency.” (2.4)  \n- “MoE Advantages… excels in heterogeneous tasks… Dense Strengths… outperforms in uniform processing tasks… Low-resource settings also favor dense models’ simpler architectures.” (2.4)  \n- “Dynamic routing… introduces decision costs absent in dense models… Two-stage training… Entropy regularization to encourage probabilistic routing diversity.” (2.2)  \n- “Conditional computation… enables sublinear FLOPs scaling… however… All-to-All communication required for routing tokens to experts…” (3.5)  \n- “DSelect-k… bridges the gap between discrete top-k routing and continuous soft MoE… optimizing routing policies end-to-end while preserving inference-time sparsity.” (3.3)  \n- “Soft MoE replaces discrete top-k routing with continuous expert combinations… preventing gradient starvation and expert collapse… at a potential cost to fine-grained specialization.” (3.4)  \n- “One of the primary bottlenecks… All-to-All communication… Hierarchical and Bi-level Routing… reduces congestion… Locality-Aware Expert Placement… converts partial inter-node communication to intra-node exchanges.” (3.5)  \n- “Routing fluctuation… where the gating network inconsistently assigns tokens… leads to oscillating expert assignments… Sparse Backpropagation… approximates gradients for inactive experts… reducing bias in gradient updates.” (4.5)  \n- “FLOPs alone do not fully capture trade-offs… MoEs introduce overhead from dynamic routing and sparse activation… Dense models typically exhibit lower latency for small batch sizes due to fixed computation paths.” (6.1)  \n- “While MoE architectures offer computational efficiency through sparse activation, their dynamic routing mechanisms introduce latency overhead that complicates deployment in time-sensitive applications.” (7.4)  \n- “MoE architectures rely on dynamic token routing… but this sparsity often leads to skewed utilization patterns… overutilized experts become computational bottlenecks… increasing latency and reducing effective model capacity.” (7.2)  \n- “MoE models… incur substantial energy costs due to… gating mechanisms… the energy required to route and activate experts dynamically often offsets the savings from conditional computation.” (7.1)  \n- “The ‘black-box’ nature of Mixture-of-Experts… conditional computation… obscures the reasoning behind expert selection and complicates efforts to audit or interpret model behavior.” (7.5)  \n- “Static routing… treats all tokens uniformly, creating inefficiencies… complexity-aware gating… dynamically allocating more experts to challenging tokens while streamlining simpler ones.” (8.1)  \n- “This combination creates a dual-path knowledge system where: 1) Parametric knowledge is selectively accessed through expert routing; 2) Non-parametric knowledge is dynamically retrieved from external sources.” (8.2)  \n- “Traditional metrics… fail to capture the dynamic interplay between gating mechanisms and sparsity patterns… proposed solutions include expert activation entropy and task-specific contribution scores.” (8.4)  \n- “Catastrophic forgetting in MoE models manifests uniquely due to sparse expert activation… dormant experts fail to retain task-specific knowledge… ‘expert collapse’—a phenomenon where dominant experts marginalize others.” (8.5)  \n- “MoE models require adaptations of traditional fairness metrics… Expert Utilization Parity… Routing Consistency… Intersectional Expert Coverage…” (9.2)  \n- “The sparse activation patterns in MoEs risk amplifying training data biases… the opaque nature of MoE decision-making further complicates transparency, making expert contributions difficult to audit.” (10.3)\n\nAssessment summary:\n- The survey repeatedly explains why key differences arise (e.g., routing-induced load imbalance, latency from All-to-All, specialization benefits, identifiability/collapse), articulates trade-offs (sparsity vs. stability, efficiency vs. latency, specialization vs. generalization, fairness vs. performance), and draws implications for deployment and ethics (hardware constraints, energy costs, explainability, societal inequities).  \n- It also connects algorithmic choices to system-level effects and ethical outcomes, and proposes concrete mitigations (auxiliary losses, expert-choice, soft/differentiable routing, hierarchical routing, quantization/offloading, fairness-aware evaluation), which is strong analytical practice.  \n- Minor limitations: in a few places, task-specific claims (e.g., dense strengths in certain reasoning tasks) are asserted without detailed evidence, and some theoretical points are high-level. However, these do not materially diminish the breadth and depth of the critical reasoning.\n\nResearch guidance value: High\n- The analysis surfaces actionable design trade-offs, identifies failure modes (collapse, routing fluctuation, latency bottlenecks), and ties them to concrete remedies and open problems (MoE-specific metrics, adaptive routing theory, hardware co-design, fairness-aware routing).  \n- Future directions are well-scoped (dynamic allocation, MoE-RAG integration, low-resource adaptations, standardized benchmarks), offering clear guidance for researchers and practitioners.", "Score: 5/5\n\nJustification:\nThe survey identifies multiple major research gaps spanning theory, algorithms, systems, evaluation, ethics, and deployment, and it repeatedly analyzes both underlying causes and potential real-world impact. The gaps are stated explicitly across sections and are tied to concrete failure modes (e.g., expert collapse, routing instability), system bottlenecks (e.g., all-to-all communication, memory bandwidth), and societal risks (e.g., bias amplification in healthcare/legal contexts). It also articulates why these gaps matter (e.g., hindering objective comparisons, degrading training stability and fairness, blocking edge deployment) and proposes preliminary mitigation avenues, showing depth rather than merely listing problems.\n\nRepresentative gap statements (direct quotes):\n\n- Benchmarking and standardization gaps:\n  - “The absence of standardized benchmarks for MoE-specific metrics—such as expert utilization efficiency and cross-domain generalization—has hindered objective model comparisons.” (Sec. 1.3)\n  - “Key challenges persist: 1. Inconsistent Routing Strategies… 2. Hardware Heterogeneity… 3. Data Imbalance…” (Sec. 5.5)\n\n- Core algorithmic and theoretical gaps:\n  - “Issues like expert imbalance… and routing instability… require careful mitigation.” (Sec. 1.3)\n  - “Theoretical gaps remain in several areas: — Generalization and sparsity… a rigorous theoretical linkage is lacking. — Hierarchical architectures… are underexplored… — Adaptive routing theory… formal guarantees… remain open.” (Sec. 2.6)\n  - “Theoretical analyses of MoE training dynamics remain sparse.” (Sec. 4.5)\n\n- Routing/load-balancing limitations and their causes:\n  - “While hierarchical and hybrid MoEs represent significant progress, they introduce new complexities: Synchronization Overhead… Dynamic Load Imbalance… Hardware Constraints…” (Sec. 3.2)\n  - “Persistent limitations include soft MoE’s blurred specialization boundaries… and DS-MoE’s sensitive sparsification scheduling.” (Sec. 3.4)\n  - “the interplay between dynamic routing and hardware constraints—such as memory bandwidth—requires deeper study.” (Sec. 4.6)\n\n- System and deployment bottlenecks:\n  - “Despite these advancements, challenges remain in balancing scalability with model quality.” (Sec. 3.5)\n  - “MoE models… are hampered by high energy consumption, substantial carbon footprints, and scalability constraints.” (Sec. 7.1)\n  - “The scalability of MoE models is constrained by both hardware limitations and algorithmic inefficiencies.” (Sec. 7.1)\n\n- Failure modes and ethical risks:\n  - “‘Expert collapse’… arises when routing gradients fail to promote diversity…” (Sec. 7.2)\n  - “The ‘black-box’ nature of Mixture-of-Experts (MoE) decision-making poses significant challenges for transparency and explainability…” (Sec. 7.5)\n  - “Catastrophic forgetting in MoE models manifests uniquely due to sparse expert activation.” (Sec. 8.5)\n\n- Evaluation and long-term adaptation gaps:\n  - “Current evaluation frameworks lack metrics to quantify expert utilization efficiency…” (Sec. 8.4)\n  - “Dynamic domain adaptation… [and] stability-plasticity trade-off… remain unresolved.” (paraphrased from Sec. 8.5; see “Dynamic Domain Adaptation and Routing Scalability” and “Stability-Plasticity Trade-off in Continual Learning”)\n\nWhy this merits full credit:\n- Breadth: The survey covers gaps across the MoE lifecycle: foundational theory (identifiability, convergence), training (routing instability, load imbalance), inference (communication/memory bottlenecks), evaluation (MoE-specific metrics, cross-domain generalization), and ethics (bias, transparency), as well as long-term adaptation.\n- Depth: For each gap, it analyzes causes and impact. Examples include:\n  - Cause: dynamic top-k routing and capacity constraints → Effect: “uneven expert utilization,” “router collapse,” and “training instability,” which “degrade model performance” and “prolong training times” (Secs. 2.2, 3.2, 4.5, 7.2).\n  - Cause: irregular memory access and all-to-all communication → Effect: “latency,” “I/O bottlenecks,” and “hardware underutilization,” blocking edge/real-time deployment (Secs. 3.5, 4.6, 7.4).\n  - Cause: sparse, conditional computation with skewed data → Effect: “bias amplification,” “performance disparities in low-resource languages,” and “ethical risks in healthcare/legal domains” (Secs. 1.3, 7.3, 9.1–9.3).\n  - Cause: lack of unified metrics → Effect: “hindered objective comparisons” and difficulty auditing fairness and specialization (Secs. 1.3, 5.5, 8.4, 7.5).\n\nOverall, the manuscript not only identifies numerous major gaps but consistently explains why they occur and how they impact both research progress and real-world deployment, meeting the highest bar of the rubric.", "Score: 5/5\n\n- “The future of MoE scaling lies in hybrid approaches, such as [26], which combines MoE with parameter-efficient fine-tuning to further reduce resource demands.”\n- “Interdisciplinary approaches offer promising solutions: integrating MoE with retrieval-augmented generation (RAG), as proposed in [35], could enhance factual accuracy, while hardware co-design strategies from [36] may optimize memory efficiency.”\n- “Emerging trends include: Integration with retrieval-augmented generation for knowledge-aware routing.”\n- “Emerging trends include: Hardware-dynamic gating (e.g., MoQE) to adapt to real-time resource constraints [17].”\n- “Future work may explore: Biologically inspired mechanisms like those in [79].”\n- “Future work may explore: Integration with retrieval-augmented frameworks ([80]).”\n- “Future work may explore: Dynamic routing policies that bridge sparse and dense regimes, anticipating the hybrid approaches of Section 3.4.”\n- “Future work may integrate Section 3.3's adaptive routing with conditional density adjustment [48], or combine retrieval-augmented experts [87] for hybrid parametric/non-parametric reasoning.”\n- “Future work could explore adaptive expert allocation [91] and tighter integration with hardware-specific optimizations [92].”\n- “Future research could explore hybrid architectures combining MoEs with retrieval-augmented generation (RAG), enabling dynamic integration of external knowledge (e.g., medical databases) to enhance expert decisions.”\n- “Future research could explore dynamic regularization, where loss weights and dropout rates adapt during training, or leverage meta-learning to automate hyperparameter selection.”\n- “Dynamic LoRA Rank Allocation: Adapting LoRA matrix ranks based on expert activation patterns to reduce redundancy [40].”\n- “Cross-Task Expert Sharing: Leveraging shared experts across related tasks to improve transfer learning [9].”\n- “Quantization-Aware PEFT: Integrating quantization with LoRA to reduce memory footprint [16].”\n- “Future directions could explore hierarchical routing pipelines, where preliminary complexity classification informs expert selection—an approach that would synergize with RAG systems' need for context-aware computation.”\n- “Real-time specialization techniques, such as those suggested by [100], could enable on-the-fly expert adaptation using meta-learning.”\n- “Three promising research avenues emerge: Joint Retrieval-Expert Optimization: Developing end-to-end training for dynamic knowledge integration.”\n- “Three promising research avenues emerge: Lightweight Hybrid Architectures: Extending edge-computing approaches from [11].”\n- “Three promising research avenues emerge: Cross-Modal Systems: Expanding multimodal integration demonstrated in [29].”\n- “Future research should prioritize: Lightweight Routing Mechanisms: Developing hardware-efficient routers, building on [6].”\n- “Future research should prioritize: Energy-Aware Expert Placement: Optimizing expert placement across edge hierarchies, extending [11].”\n- “Future research should prioritize: Cross-Modal Sparse Coordination: Jointly optimizing sparse activation across modalities, inspired by [29].”\n- “Future research should prioritize: Federated MoE Training: Exploring FL-friendly MoE architectures, guided by insights like those in [38].”\n- “Future work should: Develop unified evaluation toolkits integrating MoE-specific metrics (e.g., [137]).”\n- “Future work should: Create synthetic benchmarks to stress-test scalability, as proposed in [138].”\n- “Future work should: Leverage human-AI collaboration for auditing, following [131]'s structured evaluation approach.”\n- “We propose four research avenues: Dynamic Expert Allocation: Extending [91] with meta-learning to predict expert utility across temporal shifts.”\n- “We propose four research avenues: Memory-Augmented Routing: Integrating external memory modules ([35]) to preserve knowledge, complementing Section 8.4's attribution-based faithfulness metrics.”\n- “We propose four research avenues: Continual Learning Regularization: Augmenting [31] with constraints to penalize forgetting in dormant experts, akin to expert-centric evaluation metrics.”\n- “We propose four research avenues: Benchmarking Lifelong Adaptation: Developing benchmarks ([121]) that measure both plasticity (new task performance) and stability (old task retention), bridging the gap between Sections 8.4 and 8.5.”\n- “Future work should prioritize MoE-specific benchmarks incorporating expert diversity scores and load-balancing indices.”\n- “Ethical frameworks and bias mitigation… Developing fairness-aware frameworks—combining pre-processing, in-training regularization, and post-processing interventions—is critical.”\n- “Research should explore real-time specialization mechanisms, drawing inspiration from bidirectional attention models [41] and retrieval-augmented generation (RAG) techniques [48].”\n- “Community-driven initiatives… are needed to establish shared benchmarks and toolkits.”"]}
