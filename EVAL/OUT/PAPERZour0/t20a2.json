{"name": "a2", "paperour": [4, 5, 4, 4, 5, 5, 5], "reason": ["4\n\nJustification:\n- Abstract: Not provided in the submitted content. Therefore, the presence of an explicit research objective in the Abstract cannot be verified. This absence prevents a top score.\n\n- Introduction, Section 1.4 “Survey Objectives and Contributions”: The research objective is explicitly and precisely stated in specific sentences:\n  - “this survey aims to provide a comprehensive and interdisciplinary synthesis of Graph Retrieval-Augmented Generation (GRAG).”\n  - “we establish a cohesive framework for understanding GRAG’s advancements while identifying critical gaps and proposing actionable future directions.”\n  - “Below, we delineate the survey’s key objectives and contributions, which bridge the foundational insights of Section 1.3 with the broader implications for research and practice.”\n\n  The section further provides concrete, explicit objectives and contributions, demonstrating clarity and scope:\n  - “We consolidate techniques from graph neural networks (GNNs), transformer-based architectures [48], and hybrid retrieval-generation pipelines, offering a unified taxonomy for GRAG frameworks.”\n  - “we identify underexplored areas in GRAG…”\n  - “we outline transformative research avenues…”\n  - “This survey makes the following novel contributions: 1. Comprehensive Taxonomy… 2. Domain-Specific Insights… 3. Critical Evaluation… 4. Roadmap for Advancement.”\n\n- Introduction, Section 1.1 “Background and Motivation”: The motivation is well explained and tightly linked to the objective by detailing core problems in the field (hallucinations, parametric limitations, need for grounding, scalability, privacy, evaluation shortcomings):\n  - “A critical challenge lies in generative models’ reliance on parametric knowledge… susceptible to hallucinations…” \n  - “To address these limitations, retrieval-augmented generation (RAG) frameworks have emerged…” \n  - “The integration of structured knowledge graphs further elevates this paradigm…”\n  - “However, challenges persist. Scalability remains a hurdle… Noise… Privacy concerns… balancing retrieval accuracy with generation quality…”\n\nOverall, the Introduction clearly and explicitly presents the research objective with precise sentences and strong linkage to the field’s core problems. However, because no Abstract is provided in the content and thus the objective cannot be confirmed there, the score is reduced from a 5 to a 4.", "5\n\nEvidence of explicit classification:\n- “This subsection provides an overview of key methodologies in graph representation learning, including shallow embeddings, graph neural networks (GNNs), and graph transformers…” (Section 2.1 Graph Representation Learning)\n- Section headings and structured breakdown in 2.1: “Shallow Embeddings”, “Graph Neural Networks (GNNs)”, “Graph Transformers”, “Comparative Analysis and Hybrid Approaches”, showing a systematic taxonomy of methods.\n- “We examine three primary retrieval paradigms: dense/sparse retrieval, graph embeddings, and similarity search…” (Section 2.2 Retrieval Mechanisms in Graphs)\n- “This subsection systematically examines the spectrum of retrieval techniques, including dense and sparse methods, graph embedding paradigms, and similarity search algorithms…” (Section 3.1 Retrieval Strategies in GRAG)\n- “This subsection systematically examines advanced methods for integrating graph data, including attention mechanisms, hierarchical aggregation, and hybrid fusion strategies…” (Section 3.2 Augmentation Techniques for Graph Integration)\n- “Hybrid architectures in Graph Retrieval-Augmented Generation (GRAG) combine the complementary strengths of Graph Neural Networks (GNNs) and Large Language Models (LLMs)…” (Section 3.3 Hybrid Architectures for Generation)\n\nEvidence of explicit development/evolution:\n- “This subsection… highlighting their evolution, strengths, and limitations…” (Section 2.1 opening sentence)\n- “In summary, graph representation learning has evolved from shallow embeddings to sophisticated GNNs and graph transformers…” (Section 2.1 conclusion)\n- “Building upon the retrieval strategies discussed in Section 3.1, augmentation techniques in Graph Retrieval-Augmented Generation (GRAG) bridge the gap…” (Section 3.2 opening)\n- “Building upon the augmentation techniques discussed in Section 3.2, hybrid architectures in Graph Retrieval-Augmented Generation (GRAG) combine…” (Section 3.3 opening)\n- “Dynamic and adaptive retrieval methods serve as a critical bridge between the hybrid architectures discussed in Section 3.3 and the scalability optimizations explored in Section 3.5.” (Section 3.4 opening)\n- “Building upon the dynamic and adaptive retrieval methods discussed in Section 3.4, this subsection examines how GRAG systems address computational and memory bottlenecks…” (Section 3.5 opening)\n\nThese fragments collectively show an explicit, systematic classification and a clearly articulated evolution path: from foundational representations (shallow → GNNs → transformers), to retrieval paradigms, to augmentation, to hybrid architectures, then to dynamic retrieval and scalability—each stage explicitly linked with “building upon” language that indicates methodological development.", "4\n\nEvidence:\n- “MS MARCO (Microsoft Machine Reading Comprehension) … is a cornerstone for evaluating retrieval and generation capabilities … Its large-scale query-document pairs and human-annotated relevance judgments make it ideal for assessing both retrieval accuracy and contextual generation quality. The dataset’s diversity—spanning factoid, exploratory, and complex queries—ensures robust evaluation across varied scenarios.” (Section 6.9)\n- “BEIR’s heterogeneous collection of 18 datasets, including BioASQ and Robust04, is designed to test zero-shot retrieval performance, making it invaluable for assessing GRAG generalizability.” (Section 6.9)\n- “Specialized datasets, such as those derived from DrugBank or PubMed, are critical for evaluating GRAG systems in high-stakes domains like biomedicine. These knowledge graphs encode complex relationships (e.g., drug-disease-protein interactions), challenging models to preserve semantic and structural fidelity during retrieval and generation.” (Section 6.9)\n- “Dynamic datasets like Taobao’s user-item interactions and Twitter’s follower networks test GRAG systems in scenarios requiring real-time adaptation to evolving node relationships.” (Section 6.9)\n- “TREC Conversational Assistance Track (CAsT) evaluates GRAG systems in multi-turn conversational search, emphasizing context retention and structured knowledge integration.” (Section 6.9)\n- “FEVER (Fact Extraction and Verification) rigorously assesses GRAG systems’ ability to verify claims against knowledge graphs, prioritizing truthfulness and precision.” (Section 6.9)\n- “Precision and recall are fundamental IR metrics… In GRAG, these metrics evaluate the retrieval component, ensuring alignment between retrieved graph-structured data and query intent.” (Section 6.1)\n- “MAP and MRR … particularly relevant for multi-hop retrieval in GRAG, where the sequence of retrieved graph nodes impacts generation.” (Section 6.1)\n- “nDCG addresses binary relevance limitations by incorporating graded scores… However, nDCG’s dependence on human-annotated judgments poses scalability challenges for GRAG…” (Section 6.1)\n- “BLEU, ROUGE, and METEOR … assess fluency and factual accuracy … However, they ignore graph topology and factual consistency…” (Section 6.1)\n- “Human evaluation remains the gold standard … Human judges can verify whether responses are grounded in retrieved graph data…” (Section 6.1)\n- “nDCG … is particularly suited for GRAG systems where the ranking of retrieved graph components … directly influences generation quality… normalization by Ideal DCG (IDCG) ensures comparability across queries.” (Section 6.2)\n- “MRR … is ideal for GRAG tasks where a single high-confidence retrieval suffices—such as question answering or fact verification.” (Section 6.2)\n- “Embedding-based approaches quantify semantic alignment… Graph-aware embeddings further refine this evaluation by measuring the proximity of generated text embeddings to relevant subgraph structures.” (Section 6.3)\n- “LLMs … score responses based on their adherence to KG-derived facts… Few-shot prompting with LLMs can automate evaluations efficiently.” (Section 6.3)\n- “In biomedical knowledge graphs … metrics like entity linking accuracy and pathway coherence…” and “recommendation systems employ personalization@k … and relevance@k … Innovations like retrieval-augmented BLEU (RA-BLEU) … penalizing outputs that deviate from retrieved evidence.” (Section 6.4)\n- “Fairness metrics … Demographic Parity … Equalized Odds … Disparate Impact … Consistency Score … Counterfactual Discrepancy …” (Section 6.5)\n- “Adversarial Success Rate (ASR)… Robust Precision@k… Domain Transfer Accuracy… Zero-Shot Recall@k… Task Transferability…” (Section 6.6)\n- “Training Time per Epoch… Inference Latency… FLOPs… Memory Consumption…” and “Long Range Graph Benchmark (LRGB) … evaluates scalability and long-range interaction modeling using large-scale datasets.” (Section 6.7)\n- “Three primary methodologies dominate human evaluation … Direct Assessment … Comparative Evaluation … Task-Based Evaluation …” (Section 6.8)\n\nRationale:\n- Strengths:\n  - Extensive metric coverage across traditional (precision/recall, MAP/MRR, nDCG), semantic/LLM-based, task-specific (e.g., RA-BLEU, personalization@k), fairness, robustness/generalization, efficiency, and human-centric evaluations, with clear limitations and applicability discussed (e.g., “they ignore graph topology,” “nDCG’s dependence on human-annotated judgments poses scalability challenges”).\n  - Multiple benchmark datasets and challenges are named and contextualized (MS MARCO/TREC DL, BEIR, DrugBank/PubMed, Taobao/Twitter, TREC CAsT, FEVER, HotpotQA), including scenario descriptions (zero-shot, multi-turn, multi-hop) and labeling (human-annotated relevance), plus justification of why they matter (e.g., “assessing GRAG generalizability,” “truthfulness and precision”).\n\n- Reasons for downgrading from 5:\n  - Dataset descriptions often lack detailed scale and labeling specifics beyond general statements; e.g., while “large-scale query-document pairs and human-annotated relevance judgments” is stated for MS MARCO/TREC, precise sizes and labeling schema details are not provided for most datasets.\n  - Several domain-specific datasets (biomedical KGs, social/e-commerce graphs) are described at a high level without concrete statistics or annotation protocols.\n  - No experimental results are presented (as expected in a survey), which limits empirical grounding of the dataset/metric discussions.", "Score: 4\n\nJustification:\nThe section provides clear, explicit comparative analysis across three method families (shallow embeddings, GNNs, graph transformers). It articulates advantages and disadvantages and contrasts them along multiple dimensions such as expressivity, scalability, global dependency handling, and computational cost. The “Comparative Analysis and Hybrid Approaches” paragraph summarizes trade-offs in a contrastive manner. However, similarities are only implicitly stated (all methods aim to encode graph structure), and the comparison remains somewhat high-level without deeper technical criteria (e.g., formal capacity, training dynamics, memory footprints quantified, or benchmarked performance). Thus, it meets most but not all criteria for a fully structured, multi-dimensional comparison.\n\nContrastive quotes:\n- “The trade-offs between these paradigms are evident: shallow embeddings are lightweight but lack expressivity; GNNs balance efficiency and expressivity but struggle with global dependencies; graph transformers excel in global reasoning but are computationally costly.”\n- “While computationally efficient and scalable to large graphs, shallow embeddings struggle with dynamic graphs and higher-order semantic relationships due to their static nature.”\n- “Variants like Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and GraphSAGE excel in tasks such as node classification and generative modeling […] However, they face scalability challenges due to memory and computational overhead during message passing, and their performance degrades with long-range dependencies.”\n- “Graph transformers […] capture complex interactions across the entire graph. […] While graph transformers address the over-smoothing problem of deep GNNs, their quadratic complexity limits scalability.”", "Score: 5/5\n\nEvidence of explicit, technically grounded critical reasoning (design trade-offs, why differences arise, limitations and implications):\n- “The trade-offs between these paradigms are evident: shallow embeddings are lightweight but lack expressivity; GNNs balance efficiency and expressivity but struggle with global dependencies; graph transformers excel in global reasoning but are computationally costly.” (Section 2.1, Comparative Analysis)\n- “Techniques like graph sampling mitigate these issues but often trade accuracy for efficiency.” (Section 2.1, GNNs)\n- “Graph transformers address the over-smoothing problem of deep GNNs, [but] their quadratic complexity limits scalability.” (Section 2.1, Graph Transformers)\n- “GRAG systems must balance retrieval granularity with computational cost… Dense retrieval methods… offer high accuracy but require expensive similarity searches. In contrast, sparse retrieval techniques… trade minor accuracy losses for significant speedups.” (Section 3.5, Efficiency Trade-offs)\n- “Approximation methods, such as graph sparsification or low-rank factorization, are vital for efficiency… [and] can reduce computational costs… without accuracy loss.” (Section 3.5, Approximation and Pruning)\n- “Over-reliance on retrieval can lead to disjointed responses, while excessive generation may introduce hallucinations or factual inaccuracies.” (Section 1.3, Balancing Retrieval Accuracy and Generation Quality)\n- “The trade-off between retrieval latency and generation speed is a critical consideration, especially in real-time applications…” (Section 2.4, Interplay Between Retrieval and Generation)\n- “Frequent retrievals risk latency in large graphs. Solutions include hybrid triggering systems that pair lightweight attention-based scoring… with periodic deep retrievals.” (Section 3.4, Challenges at the Scalability-Adaptability Frontier)\n- “Noise in graph data originates from diverse sources… leading to misaligned entity-relationship mappings… The impact of noisy data manifests in two critical phases: (1) Retrieval Degradation… (2) Generation Distortion.” (Section 5.2, Sources and Propagation; Cascading Effects)\n- “Hybrid retrieval… enhances robustness against embedding noise… though at the cost of increased latency.” (Section 5.2, Mitigation Strategies and Trade-offs)\n- “The integration of these embeddings into generative models further strains resources… creating memory conflicts… approaches like dynamic retrieval augmentation selectively inject graph information but still face scalability limits with massive graphs.” (Section 5.1, Memory Constraints)\n- “Evaluating GRAG systems in dynamic settings is complicated by the inadequacy of traditional metrics… in capturing the timeliness of retrieved information.” (Section 7.1, Challenges in Dynamic Graph Adaptation)\n- “Their static nature limits their applicability to GRAG, where generated outputs are dynamic and often lack direct comparability to fixed ground-truth documents… [and] they fail to measure synergistic effects, such as coherent integration of retrieved data into responses.” (Section 6.1, Limitations of Traditional Metrics)\n- “MRR offers simplicity and interpretability, [but] its narrow focus on top-rank performance may overlook valuable lower-ranked context.” (Section 6.2, Comparative Analysis)\n- “Traditional metrics like Hits@K or Mean Reciprocal Rank (MRR) may not fully capture the semantic validity of generated links.” (Section 4.4, Challenges and Limitations)\n\nAssessment:\n- The survey consistently goes beyond description to explain why methodological differences arise (e.g., locality vs global dependency handling across shallow embeddings, GNNs, and transformers), articulates design trade-offs (efficiency vs accuracy; latency vs relevance; robustness vs scalability), and surfaces limitations with implications for real-world deployment (noise propagation, memory conflicts, evaluation gaps). These are repeatedly supported by concrete mechanisms and consequences, satisfying the rubric’s requirements for deep critical reasoning.\n\nResearch guidance value: High", "Score: 5/5\n\nJustification:\n- Multiple major gaps are identified across scalability, noise/data quality, privacy and ethics, evaluation/benchmarking, dynamic/evolving graphs, multimodal fusion, adversarial robustness, and interdisciplinary deployment. These gaps are stated explicitly and revisited throughout the survey.\n- The paper provides detailed analysis of causes and potential impact. It explains why these gaps arise (e.g., computational prohibitive retrieval on large graphs, noise propagation through multi-hop reasoning, lack of unified benchmarks and metrics) and the consequences (e.g., degraded real-time responsiveness, erroneous outputs, privacy leaks, bias amplification, and limited generalizability). It also connects gaps to proposed future directions and mitigation strategies.\n\nRepresentative quotes of specific gap statements:\n- “Systematically building on the challenges discussed in Section 1.3, we identify underexplored areas in GRAG, informed by [51]. Key gaps include: 1. Scalability and Efficiency: While GRAG performs well on small-scale graphs, large-scale dynamic graphs remain a challenge. … 3. Evaluation Metrics: Current metrics often fail to capture GRAG’s multi-dimensional performance. … 4. Ethical and Privacy Concerns: GRAG’s reliance on graph data introduces privacy risks.” (Section 1.4)\n- “Scalability remains one of the most pressing challenges in Graph Retrieval-Augmented Generation (GRAG) systems, particularly when dealing with large-scale graph-structured data.” (Section 5.1)\n- “The retrieval phase in GRAG systems involves traversing and querying large-scale knowledge graphs, which becomes computationally intensive as graph size grows.” (Section 5.1)\n- “Storing and processing graph embeddings presents another critical bottleneck.” (Section 5.1)\n- “Real-world graphs are inherently dynamic, evolving through continuous additions of nodes, edges, and attributes.” (Section 5.1)\n- “The performance of Graph Retrieval-Augmented Generation (GRAG) systems is highly sensitive to the quality of their underlying graph-structured data, making noise and imperfections critical challenges to address.” (Section 5.2)\n- “The impact of noisy data manifests in two critical phases: 1. Retrieval Degradation … 2. Generation Distortion: Noisy subgraphs misguide generative models, producing factually inconsistent or incoherent outputs.” (Section 5.2)\n- “The integration of graph-structured knowledge into retrieval-augmented generation systems introduces unique privacy and ethical challenges…” (Section 5.3)\n- “GRAG systems face heightened privacy concerns due to the relational nature of graph data…” (Section 5.3)\n- “Dynamic graphs demand retrieval mechanisms capable of processing incremental updates without compromising system responsiveness.” (Section 5.4)\n- “A critical barrier in GRAG research is the absence of unified benchmark datasets that holistically evaluate both retrieval and generation components.” (Section 5.5)\n- “GRAG systems require metrics that simultaneously assess retrieval quality and generation coherence, but existing evaluation practices often treat these components in isolation.” (Section 5.5)\n- “Current GRAG evaluations predominantly focus on static graphs, neglecting dynamic or noisy environments.” (Section 5.5)\n- “Adversarial robustness constitutes a fundamental requirement for trustworthy GRAG deployment.” (Section 5.6)\n- “Despite these advances, critical gaps remain: Scalable Robustness… Domain Adaptation… Benchmarking Gaps… Multimodal Vulnerabilities.” (Section 5.6)\n- “Another critical challenge is the absence of standardized evaluation protocols across disciplines.” (Section 5.7)\n- “Benchmarking Gaps: Current benchmarks like [256] focus on graph-text tasks, lacking standards for broader multimodal evaluation.” (Section 7.2)\n\nImpact and causation are clearly articulated:\n- Scalability cause and impact: “Retrieving and processing graph-structured data efficiently becomes computationally prohibitive as graphs grow in size…” leading to “degrade real-time responsiveness.” (Section 1.3; Section 5.1)\n- Noise cause and impact: “Graph data often suffer from noise, incompleteness, and inconsistencies…” which “can propagate errors into the generation phase.” (Section 1.3; Section 5.2)\n- Privacy/ethics cause and impact: “GRAG systems face heightened privacy concerns due to the relational nature of graph data…” and “biases… can amplify… discriminatory outcomes.” (Section 5.3; echoes in Section 1.3)\n- Evaluation/benchmarking cause and impact: “absence of unified benchmark datasets…” and “existing evaluation practices often treat these components in isolation,” impeding fair comparison and progress. (Section 5.5)\n- Dynamic graphs cause and impact: “Incremental updates… are computationally expensive,” creating “latency” and “data freshness” challenges. (Section 5.4)\n- Multimodal gaps: “lacking standards for broader multimodal evaluation.” (Section 7.2)\n- Adversarial robustness gaps: “critical gaps remain… Benchmarking Gaps; Multimodal Vulnerabilities.” (Section 5.6)\n- Interdisciplinary deployment barriers: “heterogeneity of graph data structures” and the “absence of standardized evaluation protocols across disciplines.” (Section 5.7)\n\nGiven the breadth of gaps, explicit statements, and detailed analysis of causes and impacts, the survey meets the strictest criteria for comprehensive research gap identification.", "5\n\n\"Guided by [55], we outline transformative research avenues to address the challenges and opportunities in Section 1.3: 1. Dynamic Graph Adaptation: For real-time GRAG systems, we advocate incremental learning techniques, as explored in [56]. 2. Multimodal GRAG: Integrating text, images, and graph data could enhance generative fidelity, drawing lessons from [57]. 3. Human-Centric GRAG: Aligning outputs with user intent requires advances in explainable AI, paralleling insights from [58]. 4. Interdisciplinary Collaboration: Open-source tools and shared benchmarks are critical for GRAG adoption, exemplified by initiatives like [59].\"\n\n\"Addressing GRAG’s challenges requires interdisciplinary collaboration. Scalability may benefit from advances in graph compression [44], while noise mitigation could leverage adversarial training [45]. Ethical frameworks must evolve alongside technical innovations [46].\"\n\n\"Advancements in dynamic retrieval should prioritize: 1. Online Policy Updates: Real-time learning algorithms ([78]) to support the dynamic architectures of Section 3.3 and scalability needs of Section 3.5. 2. Cross-Domain Robustness: Extending adaptive methods to heterogeneous graphs, building on the modularity of hybrid systems while addressing efficiency constraints. 3. Explainable Retrieval: Developing interfaces that visualize retrieval decisions, enhancing trust in GRAG systems—a natural progression from the interpretability goals in Section 3.3.\"\n\n\"Future research could explore: - Federated Learning for GRAG: Decentralized graph processing and privacy preservation, extending the adaptive retrieval methods in Section 3.4. - Hardware-Aware Designs: Custom accelerators for GRAG pipelines, as outlined in [109]. - Quantum-Inspired Methods: Potential quantum algorithms for ultra-fast graph retrieval [110], addressing scalability challenges in real-time applications.\"\n\n\"Future research could explore integrating GRAG with federated learning frameworks to enhance privacy-preserving recommendations. Another promising direction is the use of GRAG for explainable recommendations, generating natural language justifications alongside suggestions to improve user trust.\"\n\n\"Future research should prioritize: 1. Multimodal GRAG: Integrating imaging and omics data with BKGs, as suggested by [15], to enable holistic patient modeling. 2. Federated GRAG: Decentralized frameworks like [137] to facilitate collaborative model training across hospitals without sharing sensitive data. 3. Explainability: Tools such as [138] to visualize GRAG’s reasoning paths, fostering clinician trust.\"\n\n\"Future work should focus on: 1. Multimodal GRAG: Extending retrieval to multimodal KGs (e.g., [43]). 2. Federated Retrieval: Privacy-preserving KG access, as suggested in [32]. 3. Human-in-the-Loop GRAG: Interactive refinement of retrieved contexts, inspired by [141].\"\n\n\"Addressing these challenges requires multi-faceted innovation: 1. Efficient Graph Representations; 2. Approximate Retrieval Methods; 3. Hardware Acceleration; 4. Modular Architectures.\"\n\n\"Key areas for advancement include: 1. Scalable privacy-preserving retrieval: Developing efficient DP techniques for dynamic graphs and optimizing FL architectures for distributed knowledge graphs [25]. 2. Bias-aware generation pipelines: Integrating fairness metrics into GRAG training loops and creating domain-specific bias benchmarks [178]. 3. Standardized compliance protocols: Establishing evaluation frameworks for privacy and ethics, similar to synthetic data validation standards [179].\"\n\n\"Advancing dynamic GRAG systems requires synergies with adjacent research areas: 1. Temporal Graph Foundations: Integrating temporal graph neural networks (TGNNs) with attention mechanisms to prioritize critical updates while preserving historical context [56]. 2. Decentralized Learning: Federated frameworks for privacy-sensitive domains (extending Section 5.3’s privacy solutions) [94]. 3. Dynamic Benchmarks: Developing evaluation protocols that simulate real-world update scenarios—a prerequisite for addressing Section 5.5’s benchmarking gaps [78].\"\n\n\"Dynamic graph adaptation in GRAG opens several research avenues: 1. Federated Learning for Decentralized Updates; 2. Multimodal Integration; 3. Benchmark Development; 4. Human-in-the-Loop Frameworks.\"\n\n\"To advance federated GRAG, the following research directions are critical: 1. Hybrid Federated-Centralized Architectures; 2. Dynamic Graph Federated Learning; 3. Cross-Domain Federated GRAG; 4. Explainability and Trust; 5. Benchmarking and Standardization.\"\n\n\"To advance ethical and fair GRAG systems, the following priorities emerge: 1. Interdisciplinary Collaboration; 2. Standardized Benchmarks; 3. Human-in-the-Loop GRAG; 4. Regulatory Frameworks; 5. Bias Auditing Tools.\"\n\n\"Future research should explore hierarchical retrieval architectures, dynamic graph partitioning, and sparse attention mechanisms [103] to balance performance with computational overhead.\"\n\n\"Temporal graph neural networks or incremental embedding methods could address this gap, enabling GRAG to adapt to changing structures without retraining.\"\n\n\"Unified evaluation protocols—extending efforts like the Long Range Graph Benchmark [165]—should assess both retrieval accuracy and generation quality across tasks such as dialogue systems or recommendations.\"\n\n\"Future research could adapt GNN explainability methods [222] to GRAG, focusing on subgraph-based explanations or attention visualization to trace retrieval influences on generation.\""]}
