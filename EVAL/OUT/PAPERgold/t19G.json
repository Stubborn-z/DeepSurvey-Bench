{"name": "G", "paperour": [4, 4, 4, 5, 4, 5, 4], "reason": ["Score: 4/5\n\nExplanation:\n\nResearch Objective Clarity:\n- The paper’s core objectives are stated clearly in the Introduction. The authors explicitly position the work as “a comprehensive overview and detailed discussion of the current research progress on continual LLMs,” and further articulate concrete contributions:\n  - “we for the first time divide it into two directions of continuity… Vertical continuity and Horizontal continuity,” which establishes a novel taxonomy for organizing the field.\n  - “we delineate three key stages… Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT),” which specifies the survey’s structure and scope.\n  - “we present a compilation of publicly available evaluation protocols and benchmarks,” and “we conclude our survey with a discussion… and prospective research directions,” which clarifies deliverables beyond taxonomy.\n- These statements collectively provide a clear and specific research direction anchored in the core issues of continual learning for LLMs (e.g., catastrophic forgetting, domain/time shift, adaptation pipelines).\n- Minor limitation: There is no explicit Abstract section provided. A well-structured abstract would further enhance upfront clarity by concisely enumerating the contributions and scope; its absence reduces immediate accessibility of the objectives.\n\nBackground and Motivation:\n- The Introduction thoroughly motivates the survey by highlighting widely recognized limitations of current LLM practice:\n  - “LLMs are typically trained on static, pre-collected datasets… leading to gradual performance degradation over time and across different content domains,” foregrounding the problem.\n  - “re-collecting pre-training data and re-training models… is prohibitively expensive,” directly motivating the need for continual learning approaches.\n  - The authors summarize CL fundamentals and challenges: “models have limited or no access to previous data… This challenge, known as catastrophic forgetting,” and review established CL techniques (replay, regularization, architecture expansion), giving sufficient conceptual context.\n- The background ties directly to the objectives by arguing why a new perspective (vertical/horizontal continuity) is needed for LLMs and how it reframes existing paradigms (e.g., domain-incremental learning). This linkage is explicit in passages such as “Separating vertical and horizontal CL… offers a robust framework for analyzing complex CL paradigms in language models.”\n- Overall, the motivation is well grounded, with clear references to practical drivers (time-varying data, domain specialization, cost constraints).\n\nPractical Significance and Guidance Value:\n- The Introduction and framing emphasize practical relevance:\n  - The proposed vertical pipeline (CPT → DAP → CFT) mirrors real supplier–consumer production pipelines and acknowledges data/resource constraints across entities, offering actionable guidance for practitioners.\n  - The promise to “present a compilation of publicly available evaluation protocols and benchmarks” and to discuss “emergent properties… changes in the roles of conventional CL types and memory constraints… and prospective research directions” signals high guidance value for both researchers and engineers.\n  - The authors highlight gaps and call for community action: “underexplored research area of continually developing LLMs, especially in the field of CPT and DAP,” and “needs for… practical, accessible… evaluation benchmarks,” which reinforces practical significance.\n- The survey’s contribution set (taxonomy, staged pipeline, evaluation resources, research agenda) is directly mapped to practitioners’ needs, improving its utility.\n\nReasons for not awarding 5/5:\n- The missing Abstract weakens initial objective clarity and makes readers rely on the Introduction to extract contributions.\n- Some claims (e.g., “for the first time divide…”) would benefit from a sharper comparative positioning against prior surveys cited (e.g., biesialska2020continual, ke2023continual, wu2024continual) to substantiate novelty.\n- A brief, enumerated summary of contributions (bulleted) at the end of the Introduction would improve scannability and precision.\n\nOverall, the Introduction clearly articulates objectives, motivation, and practical value; with the addition of a concise Abstract and crisper comparative framing, it would reach the highest standard.", "4\n\nExplanation:\n- Method Classification Clarity: The paper presents a clear and reasonably comprehensive classification framework for continual learning in LLMs, centered on two orthogonal axes—vertical and horizontal continuity—and three vertical stages. In “Continual Learning Meets Large Language Models: An Overview,” the authors state “we for the first time divide it into two directions of continuity… Vertical continuity… Horizontal continuity,” and then clarify each with definitions and associated challenges (“Vertical Forgetting” and “Horizontal Forgetting”). This high-level split is innovative and helps structure a complex space. Following this, “Learning Stages of Continual Large Language Models” introduces a three-stage vertical pipeline—Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT)—and the remainder of the survey adheres to this staging: CPT is analyzed by distributional shifts (Language, Content, Temporal) in “Distributional Shifts in CPT,” DAP is organized by application domains in “Different Domains of DAP,” and CFT is further divided into CIT, CMR, CMA, and CMLLMs in “Continual Fine-Tuning (CFT).”\n  - The classification is strengthened by the consistent use of summary tables, which make categories and dimensions explicit. For CPT, the table (“Summary of existing studies on Continual Pre-training of LLMs”) annotates each work by the type of shift and CL technique (rehearsal, regularization, architecture expansion) and evaluation coverage, reinforcing the taxonomy in practice. For DAP, the large table groups methods by domain and indicates training process, CL techniques, and CL evaluation (backward/forward transfer). For CFT, the table explicitly categorizes works by CFT type (General CFT, CIT, CMR, CMA, CMLLMs), X-IL scenario (TIL/DIL/CIL), and CL technique choices. Together, these tables reflect a coherent classification system that maps methods to scenarios, stages, and techniques.\n  - The paper also grounds the classification in established CL concepts. “Continual Learning” provides a concise primer on CL types (TIL/DIL/CIL) and core technique families (replay-based, regularization-based, architecture-based), setting up the later sections to consistently refer to these notions. This cross-referencing improves clarity.\n\n- Evolution of Methodology: The survey does a good job highlighting trends and evolution, though some connections could be more systematically traced.\n  - It explicitly identifies developmental trends via observation blocks. For CPT, “General Observations on CPT” (OBS-1 to OBS-3) notes the nascent state of CPT-specific techniques, the limited diversity of techniques deployed (mostly architecture expansion), and the gap between experimental sequences and real production scales. For DAP, “General Observation on DAP” similarly points out that most work adopts single-stage DAP, that DAP is increasingly interpreted through the lens of CL (adoption of replay/parameter-expansion), and that technique diversity remains limited. For CFT, “General Observations on CFT” discusses the shift of attention from CIL toward TIL and DIL and the broader adoption of CL techniques in CFT compared to CPT/DAP—this is explicitly framed as a trend.\n  - The survey draws methodological connections within families. For MoE-based CPT, it relates DEMix (“incrementally trains and integrates new experts… with probabilistic domain mixture”) to Lifelong-MoE (“token-level gating and frozen experts with KD”), showing an evolution in how mixture-of-experts is leveraged for continual domain addition and forgetting mitigation. In CIT, it traces the progression from simple replay (CT0) to more selective replay (KPIG, SSR), dynamic replay with regularization (DynaInst), and PEFT-based isolation like O-LoRA and SAPT. In CMR, it moves from classic editing (ROME, gradient-based editors) to retrieval-/memory-augmented designs (GRACE, Larimar) and side-memory approaches (WISE), articulating the “reliability-locality-efficiency” triangle and how newer methods address it.\n  - The “Discussion” section further synthesizes trends: “Conventional Types of Incremental Learning” argues the field’s shift from CIL to TIL/DIL in LLM contexts and blurring boundaries in instruction tuning; “Roles of Memory in Continual LLMs” explains the changing role of memory constraints in industrial LLM training, emphasizing efficient adaptation and replay over storage minimization; “Prospective Directions” lists theoretical gaps, efficient replay needs, controllable memory, and custom preference models, mapping likely future evolutions.\n\n- Why not a 5: While the classification is clear and the survey captures many evolutionary threads, some evolutionary connections are not fully systematized across the entire field. For example:\n  - Chronological progression is not consistently traced across all subareas; instead, evolution is conveyed via observations and selective comparisons (e.g., DEMix vs Lifelong-MoE, CT0 → KPIG/SSR), which is informative but not uniformly applied across CPT/DAP/CFT.\n  - The boundary between DAP and CPT is acknowledged as blurred (“It is arguably reasonable to categorize … into CPT rather than DAP. Nevertheless… we include them”), which is honest but can reduce categorical crispness.\n  - In DAP, while domains are well organized (legal, medical, financial, scientific, code, other), the survey often emphasizes presence/absence of replay or PEFT rather than systematically detailing how techniques evolved over time within each domain.\n  - The innovative vertical/horizontal framework is strong conceptually, but the survey could more explicitly map how method families migrate or transform along these axes over time, and provide a more explicit lineage of technique evolution per stage.\n\nOverall, the paper offers a clear, novel classification and captures key methodological trends with extensive coverage and structured observations. The evolution narrative is present and meaningful, but not fully systematic across all categories, which justifies a score of 4.", "Score: 4\n\nExplanation:\nThe paper provides broad and fairly detailed coverage of datasets and evaluation metrics for continual learning in LLMs, but a few important areas could be expanded to reach a fully comprehensive level.\n\nStrengths supporting the score:\n- Diversity and detail of datasets:\n  - The “Evaluation Protocols and Datasets” section explicitly states “we provide a comprehensive review of the datasets available for benchmarking continual LLMs” and presents a sidewaystable that spans multiple continual learning settings. It lists datasets across:\n    - Continual pre-training with temporal and content shifts (e.g., TimeLMs, CC-RecentNews, TemporalWiki, DAPT, CPT benchmark, DEMix, DAS), each with type, shift, domain, number of stages, scale, sources, applications, and code links.\n    - Continual instruction tuning (SuperNI, CITB, NATURAL-INSTRUCTION, TRACE) and multimodal instruction tuning (CoIN).\n    - Continual model alignment (IMDB, HH-RLHF, Reddit TL;DR, a suite of QA/RC/translation tasks) and continual model refinement (FEVER, VitaminC, zsRE, T-REx, NQ, CounterFact, SCOTUS).\n  - This breadth demonstrates awareness of the key application scenarios in continual LLMs (CPT, CIT, CMA, CMR), and the table columns provide practical metadata (e.g., scale, sources, code pointers), which directly meets the “Diversity” and “detail” criteria in the rubric.\n  - Earlier tables (e.g., tabulations under CPT and DAP sections) also summarize evaluation setups used in those works, further reinforcing dataset coverage with how they were applied.\n\n- Diversity and rationality of metrics:\n  - The “Evaluation Metrics of Continual Learning” section covers the canonical CL metrics: Overall Performance (OP), Forgetting (F), Backward Transfer (BWT), and Forward Transfer (FWT), explaining their intent and how they quantify different aspects of continual learning behavior.\n  - The “Continual LLMs’ Evaluation Protocols” subsection introduces LAMA for probing factual knowledge, FUAR for CPT under temporal shifts (addressing OP’s limitations when knowledge must be both updated and retained), and TRACE’s “X-Delta” metrics for ability-wise forward transfer in instruction tuning. These are targeted and academically sound extensions tailored to LLM-specific continual scenarios.\n  - Within tabulations in the DAP section (tab:dap), the paper shows typical evaluation metrics used in practice: Loss/Perplexity (pre-training quality), Zero-shot/Few-shot accuracy, Human Evaluation, retrieval-related metrics, etc. This demonstrates both downstream applicability and practical relevance, aligning the choice of metrics with the objectives of CPT/DAP/CFT.\n\n- Rationale of dataset choices:\n  - The paper clearly articulates scope choices: “We intentionally exclude datasets used for domain-adaptive pre-training LLMs in vertical domains such as legal, medical, and financial, unless they are specifically designed for continual domain-adaptive pre-training. Furthermore, we omit datasets used in general continual fine-tuning…” This is reasonable given the stated goal to benchmark continual LLM behaviors rather than catalog all domain corpora and keeps the focus on continual settings (temporal/domain shift streams, instruction sequences, alignment/editing streams).\n\nAreas for improvement preventing a score of 5:\n- Labeling and annotation details: While datasets are listed with scale and sources, the review rarely discusses labeling methodology, annotation quality, splits, or known biases for instruction, alignment, and refinement datasets. For instance, SuperNI/CITB/NATURAL-INSTRUCTION are listed, but the annotation processes or quality controls are not analyzed; similarly for preference datasets (HH-RLHF) and factual-editing corpora (CounterFact, zsRE).\n- Multimodal metrics: For vision-language continual settings, the paper does not delve into standard multimodal metrics (e.g., CIDEr, BLEU, SPICE, grounding/localization metrics) or how forgetting/transfer is best operationalized in MLLMs. The CoIN dataset is included and the CMLLMs section discusses methods, but evaluation metrics for multimodal continual learning are not covered in comparable detail to text-only metrics.\n- Vertical vs. horizontal forgetting metrics: Although the survey defines vertical and horizontal forgetting conceptually and introduces FUAR for CPT temporal shifts, there is no unified, recommended metric set specifically operationalizing vertical vs. horizontal forgetting across the broader stratified pipeline. Most analyses revert to general CL metrics (F, BWT, FWT).\n- Practical efficiency axes: The paper discusses in the “Roles of Memory” section that continual LLM evaluation should consider computational efficiency (updates, FLOPs) and replay efficiency, but these are not formalized as evaluation metrics nor reflected in the dataset/metric table. System-level metrics (compute, memory footprint, energy) and their standardized reporting would strengthen practical applicability.\n\nOverall, the section meets most requirements for a score of 4: it covers multiple datasets and metrics across key continual LLM scenarios with fair detail, and the selected metrics are academically sound and aligned with the paper’s objectives. The missing granularity on labeling methods, multimodal metric treatment, vertical/horizontal forgetting quantification, and system-level efficiency metrics prevents a top score.", "5\n\nExplanation:\n- The paper delivers a systematic and well-structured comparison of methods across multiple meaningful dimensions throughout the sections following the Introduction—namely “Background and Related Work,” “Continual Learning Meets Large Language Models: An Overview,” and the detailed stage-wise analyses of CPT, DAP, and CFT. It also includes large summary tables that encode method attributes and facilitate direct comparison.\n- Clear multi-dimensional comparison:\n  - In “Background and Related Work” and “Continual Learning,” the survey categorizes continual learning techniques into replay-based, regularization-based, and architecture-based, and discusses their assumptions, design choices, and trade-offs (e.g., memory usage, stability, generalization) rather than listing them superficially. This is evident in the subsection “Techniques of Continual Learning,” which explains the objective and constraints of each approach (e.g., “Replay-based methods adopt the relaxed memory constraint… valued for their simplicity, stability, and high performance…”, “Regularization-based methods adopt a regularization term… to balance the past knowledge retention and current knowledge learning,” “Architecture-based methods… considered the most efficient form of CL… can achieve zero-forgetting when task IDs are available”).\n  - The paper introduces a coherent global framework in “Continual Learning Meets Large Language Models: An Overview,” distinguishing “Vertical Continuity” versus “Horizontal Continuity,” and explicitly defines their objectives and challenges. This provides a principled lens to compare methods by application scenario and assumptions. The definitions and the articulation of “Vertical Forgetting” vs “Horizontal Forgetting” clarify differences in optimization goals and data constraints (“Vertical continuity… hierarchical structure… pre-training → domain-adaptive training → downstream fine-tuning,” “Horizontal continuity… continual adaptation across time and domains… long task sequences and abrupt distributional shift”).\n- Strong, technically grounded, stage-wise comparisons:\n  - Continual Pre-Training (CPT):\n    - The table “Summary of existing studies on Continual Pre-training of LLMs” codifies comparisons across Dist. Shift, number of domains, CL technique adoption (Rehearsal/Param. Reg./Arch. Exp.), LLM architecture, and evaluation settings (pre-training vs downstream), enabling structured cross-method analysis.\n    - The section “General Observations on CPT” offers concrete, comparative insights:\n      - “OBS-2: The diversity of CL techniques incorporated in CPT remains limited. Most practical implementations… primarily focus on architecture expansion… with only a few explicitly utilizing replay… and parameter regularization.” This identifies commonalities and distinctions in strategy adoption.\n      - “OBS-3: There is an apparent gap… longest sequence of pre-training stages explored is 8… falls short of real-world scenarios,” contrasting methodological scope with practical needs.\n    - “Distributional Shifts in CPT” organizes methods by language/content/temporal shifts and contrasts architectures and objectives:\n      - DEMix vs Lifelong-MoE: The survey explicitly contrasts domain mixing strategies and gating (“DEMix… parameter-free probabilistic approach to dynamically estimate a weighted mixture of domains,” vs “Lifelong-MoE… token-level gating function… KD loss to regulate updates… robust against horizontal forgetting”), explaining architectural differences and their impact.\n      - Replay efficacy and overfitting: “LLPT… finds [experience] replay ineffective in the case of CPT, due to potential overfitting,” which is contrasted with other works that successfully apply replay under different assumptions (e.g., ibrahim2024simple’s re-warming + replay).\n      - Temporal methods (TLMs) vs CPT: The paper explains how injecting explicit temporal information (prefixes, special tokens) changes the modeling objective and alleviates conflicts in knowledge updates (e.g., “TempoT5… updating differential sets substantially enhances new knowledge acquisition and updates… various CL techniques prove effective…”).\n  - Domain-Adaptive Pre-training (DAP):\n    - The extensive table “Summary of the existing studies that leverage Domain-Adaptive Pre-training of LLMs” compares 41 studies along training pipelines (PT→DAP→SFT, unified vs multi-stage), LLM architecture, and CL technique usage (replay, parameter reg., architecture expansion), plus whether backward/forward transfer is evaluated (Loss/Perplexity/ZS/FS/FT/HE).\n    - “General Observation on DAP” provides analytical takeaways:\n      - “OBS-1: DAP predominantly occurs in a single stage… only one employs two stages,” highlighting methodological gaps and assumptions about pipeline design.\n      - “OBS-2: …the notion of interpreting DAP through the lens of CL… widely embraced,” noting how replay/data mixing and PEFT are used implicitly to mitigate vertical forgetting.\n      - “OBS-3: …diversity of techniques is limited… only replay and parameter expansion (LoRA/Layer/Block) utilized,” comparing breadth of technique adoption and pointing out missing alternatives.\n    - Domain-specific subsections compare concrete design choices, data dependencies, and learning strategies:\n      - Legal/Medical/Financial/Scientific/Code domains: The survey details mixing ratios (e.g., SaulLM’s inclusion of general data ~2%; Layer Llama’s replay; Me-Llama’s ~25% general data during DAP), use of PEFT (LoRA, adapters), and pipeline variations (e.g., HuatuoGPT-II’s unified DAP+SFT vs two-stage DAP → SFT), explaining pros/cons around forgetting and adaptation.\n      - Code domain contrasts architectural assumptions and objectives: e.g., Code Llama uses NL replay to preserve NL understanding; IRCoder applies LoRA over intermediate representations to improve multilingual transferability; Llama Pro uses Block expansion for resilience against vertical forgetting—each grounded in architecture/objective differences.\n      - Data efficiency comparisons: “xie2023efficient… utilizes only 10% of the data yet outperforms full-data training,” explicitly contrasting sampling strategies and efficiency metrics.\n  - Continual Fine-Tuning (CFT):\n    - The table “Summary of the existing studies on Continual Fine-Tuning LLMs” categorizes methods into General CFT, CIT, CMR, CMA, and CMLLMs, and compares X-IL type (TIL/DIL/CIL), techniques (replay/reg/arch-exp/others), and evaluation metrics (Avg. Acc, Bwd. Trans., Fwd. Trans.).\n    - “General Observations on CFT” goes beyond listing by characterizing the shift in scenario focus (CIL → TIL/DIL), linking assumptions to application contexts and model capabilities.\n    - Subsections contrast architectures, objectives, and constraints:\n      - General CFT: differences in representation-level resilience vs decision-layer forgetting (e.g., “zero or near-zero forgetting is only observed at the representation level… additional measures necessary”), and method-specific solutions (CTR’s TSM/KSM modules, CIRCLE’s difficulty-based replay, LFPT5’s prompt tuning on pseudo-examples).\n      - CIT: compares replay variants and PEFT-based strategies (e.g., O-LoRA’s orthogonal subspace to minimize interference; SAPT’s Shared Attentive Learning & Selection), noting cost and memory trade-offs.\n      - CMR: clearly contrasts retrieval-based memory methods (GRACE, Larimar) versus parameter-editing approaches (ROME/MELO/WilKE/WISE), articulating reliability–locality–efficiency objectives and highlighting issues like “toxicity buildup” and “locate-and-edit” limitations.\n      - CMA: contrasts RL-based (AMA, CPPO) and SL-based (COPF/DPO-style) approaches, explicitly discussing the “Alignment Tax” and design strategies (adaptive model averaging vs weighting schemes) to balance reward optimization with knowledge retention.\n      - CMLLMs: analyzes why traditional CL may be suboptimal (regularization works only under joint instruction tuning; replay/model expansion are more consistent), and presents tailored architectural tactics (EProj projector expansion; Fwd-Prompt dual-projection of gradients; MoE/LoRA to mitigate forgetting).\n- The survey identifies contradictions and nuanced findings rather than presenting uniform conclusions:\n  - CPT content shift contradiction: “cossu2022continual… observe… trivial sequential pre-training does not exhibit severe forgetting,” juxtaposed against LLPT and other studies finding consistent forgetting—this explicit contrast adds rigor to the comparison.\n  - Replay efficacy differences across settings (e.g., LLPT’s overfitting concern vs ibrahim2024simple’s simple replay + lr re-warm/re-decay sufficiency), explained in terms of assumptions and training regimes.\n- Overall, the paper avoids superficial listing by:\n  - Providing a unifying framework (vertical/horizontal continuity) and stage-wise breakdowns (CPT/DAP/CFT).\n  - Using structured, information-rich tables and “OBS” summaries to synthesize commonalities, distinctions, advantages, and disadvantages.\n  - Explaining differences in architecture (MoE vs adapters vs LoRA vs block expansion), objectives (temporal knowledge update, domain adaptation vs downstream alignment), assumptions (task indices, memory availability, data accessibility), and application scenarios (supplier vs consumer pipeline, domain specialization vs general-purpose capabilities).\n\nThese elements collectively match the 5-point criterion: the paper presents systematic, detailed, and technically grounded comparisons across multiple dimensions, with clear identification of pros/cons, similarities/differences, and scenario-specific assumptions.", "Score: 4\n\nExplanation:\n\nOverall, the survey provides meaningful and technically grounded analytical interpretation across methods and stages, often going beyond descriptive summary to explain causes, trade-offs, and limitations. However, the depth of analysis is uneven across some subsections and occasionally remains at a high level without fully unpacking underlying mechanisms or reconciling contradictory findings. Below are specific section-based reasons supporting this score.\n\nStrengths in critical analysis and interpretive insight:\n\n- Section “Continual Learning Meets Large Language Models: An Overview” and “Vertical Continuity (Vertical Continual Learning)”:\n  - The paper introduces vertical vs. horizontal continuity as a framing device that synthesizes disparate research lines (CPT, DAP, CFT) and clarifies distinct forgetting risks. It explains “Vertical Forgetting” with two fundamental causes—“Task Heterogeneity” and “Inaccessible Upstream Data”—and ties them to concrete methodological implications such as freezing shared parameters or reformulating downstream tasks, and proxy data generation (e.g., “To address the challenge of inaccessible upstream data, existing methods either use public datasets or generate pseudo-examples…”). This is a clear causal analysis rather than mere summary.\n\n- Section “Horizontal Continuity (Horizontal Continual Learning)”:\n  - Identifies core drivers of horizontal forgetting: “Long Task Sequences” and “Abrupt Distributional Shift,” explaining why longer sequences imply more updates and forgetting, and why unconstrained shifts produce larger performance drops. This reflects an understanding of fundamental causes and design implications (e.g., the use of ensembles for longer sequences and constraints for abrupt shifts).\n\n- Section “Continual Pre-Training (CPT): Effectiveness and Efficiency”:\n  - Offers interpretive commentary on necessity (“not only demonstrated the necessity of CPT…”) and computational constraints, connecting CPT to classic CL mitigations and data-efficiency strategies (ELLE, novelty/diversity sampling). The survey critically contrasts joint retraining vs. CPT and recognizes real-world limitations, which demonstrates analytical synthesis of practical constraints and methodological choices.\n\n- Section “Distributional Shifts in CPT”:\n  - Language shift: Notes consistent forward transfer and persistent forgetting, and evaluates CL techniques (freezing, LoRA, IA3) with the conclusion that addressing horizontal forgetting remains non-trivial—acknowledging limits and assumptions.\n  - Content shift: Presents nuanced analysis of replay’s limitations in CPT due to overfitting (e.g., “contrary to common understanding… experience replay … is ineffective … due to potential overfitting”), and explains why learning rate re-warming/re-decay plus selective replay can match full retraining (ibrahim2024simple). It interprets DEMix/Lifelong-MoE architectural choices (experts, gating, KD) as mechanisms for mitigating forgetting and enhancing transfer—this is technically grounded.\n  - Temporal shift: Offers a particularly insightful explanation that multi-task “upper bound” does not hold under temporal shifts due to conflicting facts (e.g., “Messi… Barcelona” vs. later “Inter Miami”). It frames CPT’s objectives (retain, acquire, update) and critically evaluates replay and parameter expansion in this context. This is a strong example of explaining fundamental causes and design trade-offs.\n\n- Section “Domain-Adaptive Pre-training (DAP): General Observation on DAP”:\n  - The survey makes explicit observations about single-stage bias and limited diversity of CL techniques, and recognizes that many works implicitly apply CL (data mixing/replay) without naming it. This meta-analysis synthesizes practices across research lines and surfaces a gap (algorithmic diversity) and a methodological trend (implicit CL through replay).\n\n- DAP domain subsections (Legal, Medical, Financial, Scientific, Code, Others):\n  - Legal/Medical: Discusses replay ratios (e.g., SaulLM 2%, PMC-LLaMA 5%, Me-Llama ~25%), parameter-efficient expansion (AF Adapter, LoRA), and the risk of vertical forgetting with large domain corpora—explicitly connecting data mixture choices to trade-offs between domain specialization and general ability.\n  - Financial: Analyzes data-efficiency (xie2023efficient), proposing sampling by distribution similarity d_{HΔH} or novelty/diversity when unknown—this is a theoretically motivated design choice. It also questions whether very large DAP datasets are necessary (Xuanyuan 2.0’s small domain ratio), which is reflective and interpretive.\n  - Code: Offers a sophisticated synthesis of structural advantages (hierarchical pipeline) and unique challenges (strict grammar vs. natural language) that complicate DAP and motivate alternate objectives and architectures (e.g., Code Llama’s pseudo-replay of natural language; Llama Pro’s block expansion; IRCoder’s LoRA grounded in IRs). This shows an understanding of domain-specific assumptions and trade-offs.\n  - Other domains: Highlights alternative DAP paradigms (AdaptLLM’s intrinsic QA transformation; Tag-LLM’s tag training without base weight changes) as routes to reduce forgetting—again, going beyond summary to interpret methodological implications.\n\n- Section “Continual Fine-Tuning (CFT): General Observations”:\n  - Frames the field-level trend—shift from CIL to TIL/DIL—then offers reasons tied to LLM task nature and instruction tuning, and notes the blurring boundary between TIL and DIL in instruction settings. This is a synthesis across CL scenarios tailored to LLM properties.\n\n- Section “General CFT”:\n  - Provides interpretive insights about representation-level “anti-forgetting” vs decision-layer drift and proposes targeted remedies (representation constraints, task-appropriate heads/modules). It critiques naive sequential finetuning and offers learning rate scheduling and classifier freezing/pre-allocation as pragmatic strategies aligned with observed LLM resilience. This is technically grounded commentary.\n\n- Section “Continual Instruction Tuning (CIT)”:\n  - Not just a catalog—analyzes replay efficiency upgrades (KPIG, SSR synthetic replay), orthogonal subspace learning (O-LoRA), and layered shared-attention (SAPT), explicitly discussing resource trade-offs (parameter storage, GPU memory) vs. effectiveness—solid analysis of design trade-offs.\n\n- Section “Continual Model Refinement (CMR)”:\n  - Synthesizes the reliability-locality-efficiency triad and explains how retrieval-activated editing and side-memory address the “impossible triangle” (WISE). It also surfaces a deeper mechanism problem (hase2023does: “location for storing the fact may not coincide with the best place for editing it”), challenging the “locate-and-edit” paradigm—this is thoughtful, mechanism-level critique.\n\n- Section “Continual Model Alignment (CMA)”:\n  - Identifies and analyzes “Alignment Tax,” then contrasts RL-based (AMA layer ratio balancing; CPPO weighting) vs SL-based (COPF adapting DPO) approaches with the explicit goal of balancing rewards and retention. This is design trade-off reasoning specific to alignment.\n\n- Section “Continual Multimodal LLMs (CMLLMs)”:\n  - Goes beyond listing: analyzes causes of forgetting via SVD on embeddings, minority collapse, and hallucination, then connects them to design fixes (EProj projector expansion with TIR; Fwd-Prompt gradient projection to residual/pre-trained subspaces). Notes that regularization may only work under certain joint-tuning conditions—this shows nuance and limitation awareness.\n\n- Section “Discussion”:\n  - “Roles of Memory in Continual LLMs” offers a well-argued reframing: shift from storage to computational efficiency in real pipelines, and proposes efficient adaptation and replay selection as primary constraints—this is a strategic, systems-level interpretive insight.\n  - Prospective directions synthesize theory gaps, efficient replay, controllable memory, and user preference trade-offs (IBCL), again extending beyond summary to forward-looking guidance grounded in observed limitations.\n\nReasons for not awarding a 5:\n\n- Depth is uneven across areas. In some subsections, the survey stops at high-level observations without fully unpacking mechanisms or reconciling conflicting empirical findings (e.g., replay’s mixed efficacy in CPT is noted, but a more thorough causal investigation—conditions under which replay overfits, model/data scales, sampling strategies—remains underdeveloped).\n- A few analytical claims are insightful but not consistently backed by systematic evidence or detailed technical derivations (e.g., the strong statement that MTL upper bounds “do not fully hold” under temporal shifts is conceptually sound and exemplified with conflicting facts, but cross-method comparative mechanics are not deeply dissected).\n- While the framing of vertical vs horizontal continuity is novel and useful, the discussion sometimes relies on broad categories and general recommendations (freezing, adapters, data mixing) rather than deeply analyzing when each approach is optimal and why, given model scale, data regime, and objective specifics.\n\nIn summary, the paper delivers substantial analytical reasoning and synthesis across research lines, clearly discussing causes, trade-offs, and limitations for many methods and scenarios. The uneven depth across topics and occasional reliance on high-level commentary rather than detailed mechanistic explanation keeps it from the “exceptional” level, hence a score of 4.", "Score: 5\n\nExplanation:\nThe survey systematically identifies and analyzes research gaps across multiple dimensions (data, methods, theory, evaluation, systems/practice) and explains why these issues matter and how they impact the field’s progress. The gaps are not only listed but also contextualized with reasons, consequences, and preliminary evidence. Specific supporting parts include:\n\n- Introduction: The authors explicitly state unmet needs and motivate gaps at a high level. For example, “We emphasize the needs for increased attention from the community, including the development of practical, accessible, and widely acknowledged evaluation benchmarks. Additionally, methodologies need to be tailored to address forgetting in emerging LLM learning paradigms.” This frames both evaluation/data and methodological gaps and their impact on reliable LLM development.\n\n- Section “Continual Learning Meets Large Language Models” (Vertical/Horizontal Continuity):\n  - Vertical Continuity: The paper formalizes “vertical forgetting” and pinpoints two structural gaps—“Task Heterogeneity” and “Inaccessible Upstream Data”—explaining why they arise in real pipelines and how they challenge applying standard CL methods (“This scenario is even more challenging than the strict memory constraint presented in conventional CL… To address… existing methods either use public datasets or generate pseudo-examples…”). This shows depth on causes, constraints, and practical workarounds.\n  - Horizontal Continuity: The paper defines “horizontal forgetting” and analyzes its drivers—“Long Task Sequences” and “Abrupt Distributional Shift”—clarifying the impact on continual adaptation over time and domains and pointing to the need for stronger CL constraints and ensemble approaches.\n\n- Section “Continual Pre-Training (CPT)”:\n  - General Observations on CPT:\n    - OBS-1: “The development of advanced techniques tailored specifically for CPT is at the starting stage and warrants further exploration.” This identifies a methods gap.\n    - OBS-2: “The diversity of CL techniques incorporated in CPT remains limited.” This highlights insufficient methodological breadth.\n    - OBS-3: “There is an apparent gap between the existing studies and the real production environment of CPT… longest sequence… falls short of real-world scenarios… investigating CPT in a task-boundary-free data stream setting is an important avenue…” This is a strong analysis of practice-oriented gaps (scale, streaming, duration) and their impact on validity in deployment.\n  - Distributional Shifts (Language/Content/Temporal): The paper discusses conflicting empirical findings (e.g., overfitting with replay in CPT, opposite conclusions in cossu2022continual) and the special challenges of temporal contradictions (e.g., “CKL… must simultaneously achieve three objectives: retention… acquisition… update”). These nuanced analyses show why gaps matter (risk of forgetting, mis-updating, inefficiencies) and where standard assumptions break down.\n\n- Section “Domain-Adaptive Pre-training (DAP)”:\n  - General Observation on DAP:\n    - OBS-1: “DAP predominantly occurs in a single stage… Continual DAP… seldom explored.” Identifies a structural/process gap.\n    - OBS-2/OBS-3: The authors note that DAP is often treated implicitly as CL without robust evaluation or diverse techniques, and explicitly call for “more sophisticated CL techniques for not just DAP, but general vertical continual learning.” They also analyze the widespread but shallow adoption of replay/mixing without recognizing it as CL, which affects methodological rigor and comparability.\n  - Domain breakdowns (Legal, Medical, Financial, Scientific, Code): The paper repeatedly highlights risks of vertical forgetting and proposes or surveys mitigations (e.g., replay ratios, adapters/LoRA) while pointing to unanswered questions like data mixing strategies, efficiency (quality over quantity), and pipeline compatibility (e.g., in code LLMs). For instance: “This prompts a pertinent question… Is a large DAP dataset necessary for developing a domain-specific LLM?” and “the problem definition and conventional architectures of existing Code LLMs may present challenges of compatibility for DAP deployment, and need to be addressed in the future.” This shows impact analysis on training cost, design choices, and downstream performance.\n\n- Section “Continual Fine-Tuning (CFT)”:\n  - General Observations: The paper notes a “transition in focus from CIL to TIL and DIL,” explains why (generative nature of LLMs), and discusses implications for method choice and evaluation. This is a thoughtful re-framing of scenario priorities and their practical relevance.\n  - Subsections:\n    - Continual Instruction Tuning (CIT): The survey points out the need for more efficient replay and PEFT-based approaches, and analyzes trade-offs (e.g., “While regularization-based and architectural-based methods require additional parameter storage and GPU memory… they remain for CIT due to the simplicity and effectiveness”), indicating where current solutions fall short and why that matters.\n    - Continual Model Refinement (CMR): The paper explicitly flags open questions and risks: “the exploration of CMR of LLMs remains open,” “the location for storing the fact may not coincide with the best place for editing… could become a significant concern for CMR,” and “Other questions… are yet to be answered.” This is a clear identification of method-level unknowns with direct impact on reliability, locality, and generalization.\n    - Continual Model Alignment (CMA): The survey analyzes the “Alignment Tax,” its causes (conflicts between CL techniques and RLHF/SL alignment), and introduces work that mitigates it. The discussion underscores why the gap matters (alignment degrades capabilities) and the need for CL-aware alignment algorithms.\n    - Continual Multimodal LLMs (CMLLMs): The paper states that MLLMs “still suffer from catastrophic forgetting… and negative forward transfer,” and shows that standard CL methods are not always optimal, motivating specialized solutions (e.g., projector expansion, MoE). This assesses method suitability and impact on generalization and stability.\n\n- Section “Evaluation Protocols and Datasets”: The paper catalogs available datasets/protocols and earlier calls (from the Introduction) for “practical, accessible, and widely acknowledged evaluation benchmarks,” demonstrating attention to gaps in data/benchmarking and their importance for comparability and progress.\n\n- Section “Discussion”:\n  - Intriguing Properties Emergent in Continual LLMs: The survey points to “anticipatory recovering” and suggests it “could pave the way for research into more complex structured learning environments,” identifying a novel phenomenon and its potential impact on theory and design.\n  - Conventional Types of Incremental Learning: It analyzes the declining focus on CIL and roles for vocabulary/routing expansion, clarifying scenario fit and implications for method transferability.\n  - Roles of Memory in Continual LLMs: The authors argue to “reassess the existing memory constraint and prioritize optimizing computational efficiency… by restricting the number of updates and FLOPs,” while also highlighting strict-memory scenarios (privacy) and the need for online CL. This is a deep shift-of-focus analysis with concrete impact on algorithm design and deployment constraints.\n  - Prospective Directions: Four well-argued future work items—Theories of Continual LLMs, Efficient Replay, Controllable Memory, Custom Preferences—each with rationale and potential impact.\n    - Theories: “there is a notable gap in research focusing on continually learning LLMs with robust theoretical guarantees,” which is crucial for principled design and risk assessment.\n    - Efficient Replay: “replaying past experiences without specific design can lead to inefficient updates… slow convergence,” with concrete leads (KPIG, forgetting forecasting).\n    - Controllable Memory: Motivated by machine unlearning and operational rollbacks, with proposed memory mechanisms (Kanerva Machine, Hopfield Networks) and operations—impactful for compliance, interpretability, and maintainability.\n    - Custom Preferences: Highlights the need for Pareto-optimal customization (IBCL) and the service-oriented context—impact on user-centric deployment.\n\nOverall, the survey meets the 5-point criteria: it comprehensively identifies major gaps across data, methods, theory, evaluation, and practice; it provides detailed analysis of why these gaps exist, why they matter, and how they affect the field’s development; and it proposes concrete directions with clear potential impact.", "4\n\nExplanation:\n\n- Overall assessment:\n  The paper proposes several forward-looking research directions that are explicitly grounded in identified gaps and real-world constraints. These directions are concrete and aligned with practitioner needs (compute efficiency, privacy/compliance, unlearning, and personalization). However, the analysis of potential impact and the actionable path for each direction is somewhat brief, and some directions are familiar in the broader CL literature (e.g., “theory building” and “efficient replay”), which reduces the novelty. Hence, a score of 4 rather than 5.\n\n- Evidence that the paper identifies gaps and real-world issues:\n  - Introduction: “We emphasize the needs for increased attention from the community, including the development of practical, accessible, and widely acknowledged evaluation benchmarks. Additionally, methodologies need to be tailored to address forgetting in emerging LLM learning paradigms.” This sets clear unmet needs (benchmarks and tailored methods).\n  - Continual Pre-Training (CPT) > General Observations on CPT:\n    - “OBS-1: The development of advanced techniques tailored specifically for CPT is at the starting stage and warrants further exploration.”\n    - “OBS-2: The diversity of CL techniques incorporated in CPT remains limited.”\n    - “OBS-3: There is an apparent gap between the existing studies and the real production environment of CPT… investigating CPT in a task-boundary-free data stream setting is an important avenue for research…”\n  - Domain-Adaptive Pre-training (DAP) > General Observation on DAP:\n    - “OBS-1: DAP predominantly occurs in a single stage. Continual DAP… is seldom explored.”\n    - “OBS-3: Further research of more sophisticated CL techniques for not just DAP, but general vertical continual learning is much needed.”\n  - Discussion > Roles of Memory in Continual LLMs:\n    - The paper connects to realistic pipelines: “institutions with access to training data may opt to retain full access without restricting memory size… the challenge shifts from storage efficiency to computational efficiency.”\n    - It also covers strict constraints (privacy/compliance): “Continual learning under the strict memory constraint is also driven by data privacy concerns, where preserving data on the server side is prohibited.”\n\n- Forward-looking directions clearly proposed and linked to the gaps:\n  1) Discussion > Prospective Directions > Theories of Continual LLMs:\n     - “There is a notable gap in research focusing on continually learning LLMs with robust theoretical guarantees and understanding the forgetting behaviors of LLMs from a theoretical perspective.”\n     - This directly addresses the earlier-identified lack of tailored CPT/DAP methodologies and helps build principled foundations for LLM-specific forgetting and transfer. It aligns with real-world needs (predictability, guarantees for production systems).\n  2) Discussion > Prospective Directions > Efficient Replay for Knowledge Retention for Continual LLMs:\n     - Motivated by compute/efficiency constraints in “Roles of Memory”: “prioritize optimizing computational efficiency… by restricting the number of updates and FLOPs.”\n     - The direction is concrete: “More sophisticated and accurate data mixing strategies and efficient replay sample selection mechanisms are needed…”\n     - Cites practical mechanisms (e.g., KPIG, forgetting forecasting), showing a path toward actionable, efficient adaptation.\n  3) Discussion > Prospective Directions > Continual LLMs with Controllable Memory:\n     - Real-world motivation: unlearning and rollback. “This example illustrates the benefits of equipping LLMs with an external, controllable memory.”\n     - Specific suggestions: “integrating the Kanerva Machine… Other memory systems like Hopfield Networks hold promise…”\n     - This is forward-looking and innovative in the LLM context, with clear practical value (auditability, compliance, and reversibility).\n  4) Discussion > Prospective Directions > Continual LLMs with Custom Preferences:\n     - Real-world need: personalization and different trade-offs. “Efficiently building customized LLMs for individual users and offering flexible adjustment options is a challenging task.”\n     - Concrete proposal: IBCL and Pareto-optimal models “by combining two model posteriors in the parameter space,” indicating a viable research path toward personalized continual alignment.\n\n- Additional forward-looking elements supporting the score:\n  - Discussion > Roles of Memory in Continual LLMs:\n    - Actionable guidance that reflects real pipelines: “reassess the existing memory constraint and prioritize optimizing computational efficiency… by restricting the number of updates and FLOPs.”\n  - Discussion > Intriguing Properties Emergent in Continual LLMs:\n    - “anticipatory recovering” points to emergent behaviors in large models, opening new research topics on sequential memorization and structured learning as scale increases.\n\n- Why not a full 5:\n  - While the directions are well-motivated and relevant, the analysis of academic/practical impact is relatively brief in several places. For example, “More sophisticated and accurate data mixing strategies…” and “integrating the Kanerva Machine…” are promising but lack a detailed experimental roadmap, metrics, or deployment considerations.\n  - Some directions (theory building, efficient replay) are important but not wholly novel in CL; the paper does not fully unpack the unique LLM-specific causes and implications beyond citing the need.\n  - The call for “development of practical, accessible… evaluation benchmarks” in the Introduction is strong, but the Prospective Directions section does not return to propose concrete benchmark designs tailored to vertical/horizontal continuity, which would have further strengthened the actionability.\n\nIn summary, the paper clearly identifies the key gaps and real-world constraints and proposes forward-looking, relevant research directions with concrete suggestions. The depth and novelty are good but not uniformly exceptional across all directions, yielding a solid 4 out of 5."]}
