{"name": "G", "paperour": [4, 5, 3, 5, 4, 4, 4], "reason": ["Score: 4\n\nExplanation:\n\n- Research Objective Clarity:\n  - The Introduction clearly states the survey’s aim and scope. The sentence “Our survey aims to address this deficit by providing a clear and comprehensive overview of MoE in LLMs, introducing a novel taxonomy that organizes recent progress into three categories: algorithm, system, and application.” explicitly defines the primary objective and the organizational framework.\n  - The authors further operationalize the objective with concrete coverage promises: “Under this taxonomy, we first delve into MoE algorithmic advancements… we provide a comprehensive overview of MoE system design… Additionally, we overview the applications of MoE across various domains…”\n  - The roadmap at the end of the Introduction (“The remainder of this survey is organized as follows…”) reinforces a clear direction by mapping sections to the proposed taxonomy.\n  - Minor reduction from a perfect score: The Abstract is not present in the provided text. Without an abstract to summarize objectives and contributions up front, clarity for readers at first glance is diminished.\n\n- Background and Motivation:\n  - The Introduction provides strong background, linking MoE to the scaling challenges of transformer-based LLMs (“Recognizing a scaling law… it is imperative to identify and implement efficient methodologies for the sustainable scaling of LLMs. The concept of mixture of experts… has undergone extensive exploration…”).\n  - It motivates relevance with contemporary, concrete examples and trend evidence (“As depicted in Figure [timeline], MoE has maintained a robust trajectory of growth, particularly notable in 2024 with the advent of Mixtral-8x7B… Grok-1, DBRX, Arctic, DeepSeek-V2…”).\n  - The gap analysis is explicit and well-argued: prior surveys are identified and their limitations are explained (“…two surveys preceding our work: the first, published in August 2012… dense MoE…; the second, released in September 2022… predates the major developments following the ‘ChatGPT moment’…”). This directly motivates the need for an updated, focused review.\n\n- Practical Significance and Guidance Value:\n  - The survey promises actionable guidance: a “novel taxonomy,” “collections of available open-source implementations, hyperparameter configurations, and empirical evaluations,” and coverage of systems issues (computation, communication, storage) that practitioners face.\n  - The Introduction connects academic concepts to practical deployment considerations (“With the gradual convergence of model architecture design in industrial products, system design has emerged as a pivotal factor…”), indicating the review’s utility beyond theory.\n  - Later sections (as previewed in the Introduction) emphasize bridging research-practice gaps and challenges/opportunities, suggesting clear guidance value for both researchers and engineers.\n\n- Suggestions to reach a 5:\n  - Provide a concise Abstract that explicitly states the survey’s objectives, contributions (e.g., taxonomy, comparative tables, design patterns), scope boundaries (primarily LLMs but with cross-domain references), and the time window covered.\n  - Clarify inclusion/exclusion criteria and methodology for literature selection (e.g., sources, time period, domains considered), and make the LLM focus versus broader domains more explicit to avoid ambiguity.\n  - Explicitly list the key contributions in bullet points at the end of the Introduction (e.g., taxonomy, synthesis of gating/expert designs, system practices, curated benchmarks), to sharpen perceived guidance value.", "Score: 5\n\nExplanation:\n- Method Classification Clarity:\n  - The paper explicitly proposes a clear, top-level taxonomy early on: “Our survey aims to… introduce a novel taxonomy that organizes recent progress into three categories: algorithm, system, and application.” This organizational decision is consistently reflected throughout the core survey sections (“Taxonomy of Mixture of Experts,” “Algorithm Design of Mixture of Experts,” “System Design of Mixture of Experts,” and “Applications of Mixture of Experts”).\n  - Within “Algorithm Design,” the classification is further refined into well-defined sub-axes that mirror the design choices practitioners face:\n    - Gating Function: carefully split into sparse, dense, and soft, with sparse further divided into token-choice (trainable), non-trainable token-choice, and expert-choice. This hierarchy is clearly diagrammed in the taxonomy figure and justified in prose (e.g., the motivation for sparse gating’s conditional computation and auxiliary losses; the rationale for non-trainable gating and expert-choice).\n    - Experts: subdivided into network types (FFN, attention, others), hyperparameters (expert count, expert size, MoE layer frequency), activation functions, and shared expert. Each subcategory is backed by representative models and tables (e.g., the large comparative table of FFN expert configurations across GShard, Switch, GLaM, Mixtral, DeepSeekMoE, etc.).\n    - Mixture of Parameter-Efficient Experts (MoPE): classified by placement within the Transformer stack (FFN, attention, block-level, whole-layer), which is both intuitive and operationally useful. The figure illustrating the MoPE taxonomy by placement further clarifies the boundaries of each class.\n    - Training & Inference Schemes: clearly delineated into Original, Dense-to-Sparse, Sparse-to-Dense, and Expert Models Merging, with an accompanying schematic figure that makes the transitions and relationships visually explicit.\n    - Derivatives: grouped coherently around ideas that extend MoE principles (e.g., WideNet, SUT, MoT, SMoP, Lifelong-MoE, MoD), distinguishing these from core MoE algorithms while showing conceptual lineage.\n  - The “System Design” section mirrors the algorithm taxonomy with equal clarity in the systems dimension: computation, communication, and storage. It anchors the discussion in expert parallelism and then shows hybrid parallel combinations (data+expert+tensor, data+expert+pipeline, expert+tensor), with a schematic figure that clarifies how these strategies compose. This maintains a coherent classification that maps to real deployment concerns.\n\n- Evolution of Methodology:\n  - Historical trajectory is established up front with a timeline and context: moving from early dense MoE (Jacobs 1991; Jordan 1994) to sparsely-gated MoE (Shazeer 2017) and its integration into modern Transformer LLMs (GShard, Switch, GLaM), followed by the post-ChatGPT wave (Mixtral-8x7B, DBRX, DeepSeek-V2, Qwen1.5-MoE, Jamba). The introduction and timeline figure make the developmental arc explicit.\n  - Gating evolution is systematically presented:\n    - From top-k (Shazeer et al.) to competitive top-1 gating (Switch), stabilization via auxiliary losses (GShard’s L_aux), z-loss (ST-MoE) for numerical stability, mutual-information-based routing (Mod-Squad, ModuleFormer), and improved routers (Attention Router in Yuan 2.0-M32).\n    - Load balancing evolution: expert capacity constraints (GShard), batch-prioritized routing (V-MoE, ST-MoE), identification and mitigation of sequence-position bias and token dropping (“Drop-towards-the-End”) and “Early Routing Learning” (OpenMoE).\n    - Alternative paradigms: discrete optimization (BASE, S-BASE, DSelect-k), sentence-level routing, representation collapse mitigation, surrogate-model-based router training (FLEXTRON), adaptive k experts per token (AdaMoE, DYNMoE), and inference-time engagement of unchosen experts (SCMoE).\n    - Non-trainable gating progression: hash-based routing, THOR, domain-mapped DEMix, explicit language routing (M2M-100), and hybrid domain+random strategies (PanGu-Σ), contrasting with trained multilingual gating (NLLB).\n    - Expert-choice routing (MoEEC, Brainformers) as a distinct evolutionary branch that solves load balancing without auxiliary losses.\n    - Soft MoE evolution: token merging (Soft MoE; addressing training stability and balancing at the cost of autoregressive complications), expert merging (SMEAR, Lory) that reintroduces full differentiability and shows strong results in pretraining and PEFT contexts. The paper clearly positions soft MoE as an evolutionary response to discrete routing instability.\n  - Expert architecture evolution:\n    - From FFN-only experts towards attention-aware MoE (MoA, DS-MoE, JetMoE, ModuleFormer) with shared KV projections for compute efficiency and specialization.\n    - Activation functions evolution mirrors dense LLM practice (ReLU → GeLU → GeGLU → SwiGLU), acknowledging how activation advances translate into MoE practice.\n    - The “Shared Expert” pattern evolves from Residual-MoE (DeepSpeed-MoE) to multi-shared-expert designs (DeepSeekMoE, Qwen1.5-MoE), hypernetwork variants (HyperMoE), and architecture changes enabling communication/computation overlap (ScMoE, Arctic hybrid).\n  - Hyperparameter evolution and trends are explicitly synthesized:\n    - Expert count: early thousands → stabilized ~64 experts (GLaM and subsequent models), with pyramid and fine-grained segmentation variants (DeepSeekMoE/Qwen/DBRX).\n    - Expert size: trend toward larger intermediate dimensions and fine-grained segmentation for improved specialization and flexible activation combinations; PEER’s large pool of tiny experts as a new frontier.\n    - MoE layer frequency: shift from alternating to every-layer placement (post-Mixtral) with careful exceptions (e.g., skipping the first layer for balance convergence). The survey compares strategies like V-MoE’s late-layer placement, MoE-LLaVA’s alternating placement, ST-MoE’s added dense FFN, and hybrid layer compositions (Brainformers; Jamba’s attention–Mamba ratio).\n  - Training & inference evolution:\n    - Dense-to-Sparse: upcycling, DTS-Gate, hybrid dense training/sparse inference (DS-MoE), progressive activation (SMoE-Dropout), expert construction strategies (MoEfication, MoEBERT, EMoE).\n    - Sparse-to-Dense: distillation innovations (Switch, OneS), pruning/trimming/averaging of expert weights and conversion back to dense FFNs (ModuleFormer, EWA, unified compression framework).\n    - Expert Models Merging: BTM → BTX (merging multiple domain experts into a unified MoE) and FoE, showing a coherent line from ensembling to integrated MoE mergers.\n  - System evolution is also systematically laid out:\n    - From the foundational expert parallelism (GShard) to hybrid parallel strategies, then to kernel-level and compiler-level optimizations (DeepSpeed-MoE, FastMoE, Tutel, HetuMoE, MegaBlocks, PIT), and memory-footprint-aware designs (ScatterMoE).\n    - Communication evolution tackles the All-to-All bottleneck with hierarchical all-to-all, topology-aware routing, expert affinity exploitation, pipelining with increasing overlap (Lancet), GPU-initiated communication, and architecture-level decoupling (ScMoE, Arctic hybrid).\n    - Storage evolution addresses HBM constraints with offloading, prefetching, and buffer-sharing/recomputation to reduce activation memory.\n  - The survey repeatedly highlights trends and turning points (e.g., post-Mixtral increasing every-layer MoE adoption; the move from thousands of experts to dozens; stabilization techniques for routing; communication-driven shared expert designs), showing the field’s direction and the reasons behind each shift.\n\nOverall, the paper’s classification is clear, layered, and actionable; and the methodological evolution is systematically traced across algorithmic, architectural, training/inference, and systems dimensions. It substantiates each evolutionary step with concrete model examples, tables, and figures (taxonomy, timelines, parallelism schematics). While the taxonomy is rich and dense, the authors consistently explain the inherent connections and motivations behind each branch, fulfilling the criteria for the highest score.", "3\n\nDetailed explanation:\n- Diversity of datasets and metrics:\n  - The survey touches multiple domains (NLP, CV, RecSys, Multimodal) and mentions several standard LLM benchmarks, but overall the dataset and metric coverage is limited and scattered.\n    - In Section “Algorithm Design of Mixture of Experts” under Hyperparameters, Table “A collection of recent open-source models…” lists performance on MMLU, GSM8K, MATH, and HumanEval for various open-source MoE models. This shows awareness of common LLM benchmarks and includes different shot settings (e.g., “MMLU (5-shot), GSM8K (5-shot) … HumanEval (0-shot)” and occasional 8-shot settings in the table).\n    - In “Soft” gating (Expert Merging) the survey cites SMEAR evaluations on “T5-GLUE and ResNet-DomainNet” and notes throughput impacts, which introduces GLUE and DomainNet as datasets and hints at efficiency metrics (throughput).\n    - In “Sparse-to-Dense” (OneS) it references retention of MoE benefits “on ImageNet and NLP datasets,” adding a major CV dataset.\n    - In “Applications”:\n      - NLP: tasks are described (machine translation, open-domain QA, code generation, math problem solving), but datasets are not explicitly named beyond benchmarks like MMLU/GSM8K/MATH/HumanEval listed earlier.\n      - CV: V-MoE is discussed for “image recognition tasks” and elsewhere ImageNet is mentioned; pMoE describes “segment each input image into n patches,” but specific datasets and metrics (e.g., ImageNet top-1/top-5) are not tabulated.\n      - RecSys: the survey explains multi-task recommendation and frameworks (MMoE, PLE, AdaMCT, M^3oE) but does not identify public datasets or standard metrics (e.g., AUC, NDCG, CTR) used in evaluations.\n      - Multimodal: models like LIMoE, MoE-LLaVA, MoCLE, Uni-MoE are covered, but concrete datasets (e.g., COCO, VQAv2) and metrics (e.g., accuracy on VQA, CIDEr/ROUGE for captioning) are not enumerated.\n  - Missing important datasets/metrics:\n    - For machine translation, no WMT datasets or BLEU/COMET metrics are summarized.\n    - For QA/reading comprehension (e.g., SQuAD, SuperGLUE) and broader reasoning (e.g., Big-Bench/BIG-bench), coverage is absent.\n    - For CV, explicit reporting of standard metrics (ImageNet top-1/top-5) and datasets beyond ImageNet/DomainNet is minimal.\n    - For RecSys, public benchmarks and core metrics (AUC, NDCG@K, HR@K, CTR) are not discussed.\n    - For Multimodal, common datasets (COCO, VQAv2, GQA, TextCaps) and associated metrics are not covered.\n    - System evaluation metrics (throughput, tokens/s, latency, communication bandwidth, memory footprint) are mentioned qualitatively across “System Design” (e.g., All-to-All overhead, overlapping communication/computation in Tutel/FasterMoE/PipeMoE/MPipeMoE) but not organized with quantitative metric definitions or standardized reporting.\n\n- Rationality of datasets and metrics:\n  - Where benchmarks are included (the open models table under Hyperparameters), the choices (MMLU, GSM8K, MATH, HumanEval) are academically sound and widely used for LLMs. The table includes shot settings, which is useful, but lacks definitions, task descriptions, or rationale connecting these benchmarks to MoE-specific characteristics (e.g., whether MoE particularly aids reasoning vs. code vs. multilingual tasks).\n  - In “Soft” and “Sparse-to-Dense,” references to GLUE, DomainNet, and ImageNet are reasonable, but the survey does not provide dataset scale, labeling methods, or evaluation protocols. Statements like “SMEAR-equipped models surpass those with metadata-based or gradient-estimated gating” and “OneS retains 61.7% of MoE’s benefits on ImageNet and 88.2% on NLP datasets” are informative but high-level; they do not ground the reader in metric definitions or dataset specifics.\n  - The survey explains auxiliary losses and load balancing metrics very well (Table on auxiliary losses, and the formal definition of load-balancing loss in “Background on Mixture of Experts” → “Sparse MoE”), but these are training objectives rather than evaluation metrics, and they are not tied to a systematic evaluation section with quantitative comparisons.\n  - Across Applications, the rationale for chosen datasets/metrics per domain is not elaborated (e.g., why certain benchmarks were prioritized, what evaluation protocols were used, how MoE’s conditional computation relates to improvements on specific metrics).\n\nWhy the score is 3:\n- The survey does include multiple widely recognized LLM benchmarks (MMLU, GSM8K, MATH, HumanEval) and mentions GLUE, DomainNet, and ImageNet, which indicates some diversity across domains. It also touches on efficiency notions (throughput) and training objectives (auxiliary/load-balancing loss).\n- However, it does not provide detailed dataset descriptions (scale, splits, labeling, application scenarios) or a comprehensive catalog of key datasets and metrics across all covered domains. Many core datasets/metrics (MT BLEU/COMET, QA/reading comprehension, RecSys AUC/NDCG/CTR, Multimodal COCO/VQAv2 metrics) are missing, and there is little justification or methodical discussion of evaluation protocols and the suitability of metrics for MoE-specific claims.\n- Consequently, the coverage is limited and lacks detail, and the rationale is only partly developed, aligning with a score of 3 according to the rubric.", "Score: 5\n\nExplanation:\nThe survey delivers a systematic, technically grounded, and multi-dimensional comparison of Mixture-of-Experts (MoE) methods, consistently contrasting architectures, objectives, assumptions, advantages, and limitations across algorithmic and systems perspectives. The comparisons are organized into clear sections with taxonomies, equations, figures, and tables, avoiding superficial listings.\n\nKey evidence supporting the score:\n\n- Multi-dimensional organization and taxonomy\n  - The “Taxonomy of Mixture of Experts” section presents a structured tree that separates Algorithm, System, and Application, and further decomposes Algorithm into Gating Function (Sparse/Dense/Soft, Token-choice vs Expert-choice; trainable vs non-trainable), Experts (network types, hyperparameters, activation, shared experts), MoPE, and Training & Inference Schemes. This layout (accompanied by the taxonomy figure) provides a global framework for comparing methods along multiple axes rather than listing them piecemeal.\n\n- Gating function: detailed comparisons of mechanisms, trade-offs, and assumptions\n  - In “Gating Function,” the paper systematically distinguishes Sparse vs Dense vs Soft gating, and further splits Sparse into Token-Choice (trainable and non-trainable) and Expert-Choice, explicitly analyzing implications:\n    - Token-Choice Gating (trainable): It explains top-k vs top-1 (Switch) and the rationale for k>1; provides a comparative table of auxiliary losses with coefficients (“Table: Auxiliary Loss for Token-Choice Gating”), and discusses why L_aux may destabilize at larger scale and how z-loss mitigates that (“ST-MoE identified limitations with L_aux… z-loss… penalizing large logits, reducing roundoff errors”).\n    - Expert Capacity and routing policies compare BPR vs sequential routing, capacity limits, token overflow, and “Drop-towards-the-End” phenomenon in OpenMoE—highlighting concrete operational differences and their effects (“OpenMoE identifies a tendency… Context-independent Specialization… Early Routing Learning”).\n    - Other advancements (BASE/S-BASE/DSelect-k) contrast discrete vs smooth optimization and linear assignment/optimal transport formulations (“re-envisions token-to-expert allocation… equal quantity of tokens”, “improved smoothness over top-k”); these explicitly differentiate objectives (load balance optimality vs differentiability/stability).\n    - Dynamic/adaptive gating (AdaMoE, DYNMoE) is compared to fixed-k routing, explaining the assumption that token complexity should drive varying numbers of experts and how null experts/thresholds operationalize that.\n    - Non-trainable Token-Choice Gating is contrasted with trainable routers on parameters and balance (“no additional gating network parameters… full load balancing… but domain matching may be suboptimal; uses probabilistic mixtures at inference”).\n    - Expert-Choice Gating explicitly states pros/cons (“circumvents auxiliary losses, ensures uniform distribution… however may result in uneven token coverage; some tokens processed multiple times or not at all”), linking design choice to expected behavior and trade-offs.\n  - These comparisons span architectural form (token-choice vs expert-choice), optimization approach (discrete vs smooth), training stability (z-loss, MI-based losses), objectives (load balance vs task alignment), and practical constraints (capacity, token dropping) rather than just enumeration.\n\n- Experts: architectural alternatives and rationale\n  - In “Experts – Network Types,” the survey contrasts FFN-based experts (dominant) with attention-based MoA and others (CNN), and explains the underlying assumption/evidence for FFN specialization and sparsity (“self-attention layers exhibit lower sparsity… DS-MoE analysis… neuron activation sparsity and Emergent Modularity”), giving reasons for when and why FFN MoE is preferable.\n  - For MoA, the paper clarifies compute-sharing assumptions and design (“share K/V to reduce complexity; experts differ in Q and output projection”), showing architectural trade-offs rather than just naming methods.\n\n- Hyperparameters: explicit trade-off analyses and trends\n  - The “Hyperparameters” section compares expert count, expert size, and MoE placement frequency, with grounded observations:\n    - Expert count: trend from thousands to ≤64; domain shift and fine-tuning sensitivity; GLaM’s 64 experts with top-2 as balance of quality/efficiency.\n    - Expert size and fine-grained segmentation: DeepSeekMoE’s 1/8 FFN size per expert + more experts and higher top-k to refine knowledge decomposition and specialization; contrasted with coarse-grained designs; also PEER’s tiny experts with product-key retrieval vs conventional MoE.\n    - Placement frequency trade-offs: quality vs systems overhead (1/2 vs 1/4 vs every layer), and empirical outcomes (V-MoE in later layers gives minimal performance loss with speed; DeepSeek excludes first layer due to load balancing convergence; MoE-LLaVA finds alternating better; ST-MoE adds dense FFN for quality).\n  - The inclusion of a large comparative configuration table of MoE models (expert counts, sizes, placements, activations) strengthens the rigor of comparisons.\n\n- Shared expert design: motivation and pros/cons\n  - “Shared Expert” analyzes residual-MoE and CMR as strategies to combine a fixed FFN with a gated expert without increasing communication beyond top-1, and interprets the gated path as error correction—clearly articulating benefits and why it is now mainstream. It contrasts single vs multiple shared experts (DeepSeek/Qwen), and follow-up innovations (PAD-Net, HyperMoE) with their objectives (reduce redundancy, capture cross-expert/layer info).\n\n- MoPE (mixture of parameter-efficient experts): comparisons by placement and behavior\n  - The MoPE section introduces a placement-based taxonomy (attention, FFN, block, every layer), provides forward-process equations for FFN/attention integration to clarify architectural distinctions, and discusses method-specific goals and remedies (e.g., LoRAMoE’s split expert groups and localized balancing to mitigate forgetting; MoELoRA’s contrastive loss to stabilize routing; SiRA’s capacity/aux loss for load balance; MoV’s IA^3 vectors to reduce compute; MoLA’s layer-wise expert allocation for redundancy/efficiency).\n  - While this part is somewhat more method-descriptive, it still highlights the central trade-offs (parameter efficiency, routing stability, multi-task conflict, expert redundancy) and how design choices aim to mitigate them.\n\n- Training and inference schemes: side-by-side pipeline comparisons\n  - “Training & Inference Scheme” contrasts Dense-to-Sparse, Sparse-to-Dense, and Expert Model Merging:\n    - Dense-to-Sparse: RMoE’s alignment need; sparse upcycling’s initialization and gating normalization nuances (helps ViT but hurts LMs); Skywork-MoE’s upcycling vs scratch findings; DS-MoE’s dense training with sparse inference; SMoE-Dropout’s progressive expert activation. These comparisons explicitly explain why approaches may succeed or fail depending on model type and training dynamics.\n    - Sparse-to-Dense: Switch’s distillation (weight transfer from non-expert layers and probability mixing), OneS’s knowledge gathering + distillation pipeline, and pruning approaches (ModuleFormer, task-specific expert elimination) with clear deployment motivation and the quality retention vs parameter count trade-off.\n    - Expert Models Merging: BTM vs BTX—communication-efficient independent training vs MoE consolidation of FFNs with gating—clarifying objectives and how BTX combines strengths of BTM and MoE.\n\n- System-level comparisons across computation, communication, and storage\n  - The “System Design” section provides an organized contrast of challenges and solutions:\n    - Computation: load imbalance (and dynamic expert placement/shadowing), overhead of gate/encode/decode vs optimized GEMM, specialized kernels (DeepSpeed-MoE, Tutel), block-sparse kernels (MegaBlocks) vs compiler PIT and its PIT tiling, and their limitations; ScatterMoE’s design to reduce memory footprint and maintain PyTorch tensor transparency. This ties system artifacts to algorithmic structure.\n    - Communication: explains why All-to-All is dominant, hierarchical All-to-All to leverage intra-node bandwidth, topology-aware routing to reduce inter-node traffic, pipelining overlap limits and how Lancet and ScMoE/Arctic restructure computation to extend overlap. It clearly compares the assumptions and practical constraints behind each strategy.\n    - Storage: outlines HBM constraints, offloading with prefetch (SE-MoE, Pre-gated MoE, EdgeMoE) vs activation/buffer sharing/recompute (MPipeMoE), making deployment trade-offs explicit.\n  - An open-source frameworks table and discussion of hybrid parallelism (data/tensor/expert/pipeline) further reinforce structured, comparative system insights.\n\n- Explicit articulation of pros/cons and limitations\n  - Throughout the sections, the survey repeatedly states advantages and disadvantages, not just features. Examples include:\n    - Expert-choice gating: avoids aux loss but risks uneven coverage (“However, this method may result in uneven token coverage…”).\n    - Non-trainable gating: parameter-free and balanced but domain categorization can be suboptimal (DEMix) and mitigated via probabilistic mixtures at inference.\n    - Soft MoE: fully differentiable and balanced but token merging complicates autoregressive decoders (“future tokens… inaccessible during inference”); SMEAR merges expert parameters to avoid non-differentiable gating but trades throughput.\n    - Upcycling: normalization helps in vision but hurts language; scratch training outperforms upcycled; expert similarity as a diagnostic.\n    - Placement frequency: every-layer MoE improves capacity but adds system overhead; alternating can be better in LVLMs.\n\nWhy not a lower score:\n- The review avoids mere enumeration by consistently explaining the why behind design choices and their implications (e.g., stability, load balance, generalization, deployment constraints). It leverages equations, figures, and tables to ground differences technically. Comparisons are conducted along clear dimensions (routing mechanism, trainability, capacity and load strategies, expert type/size/placement, training/inference transitions, and system bottlenecks). While some subsections (e.g., MoPE) are more method-centric than comparative, they still frame the motivations and trade-offs. Taken together, the coverage and structure align with a comprehensive, well-structured comparative review.", "Score: 4\n\nExplanation:\n\nThe survey delivers meaningful, technically grounded analysis across multiple method families, but the depth is uneven: several parts synthesize causes and trade-offs very well (especially gating, capacity/auxiliary losses, soft vs sparse routing, training/inference schemes, and system bottlenecks), while other parts remain largely descriptive (e.g., activation functions, some hyperparameter discussions, and portions of the MoPE taxonomy). Below are concrete examples supporting this score.\n\nStrengths in critical analysis and interpretive insight:\n- Fundamental causes and trade-offs in gating:\n  - In “Background on Mixture of Experts” and “Algorithm Design—Gating Function,” the paper explains why sparse token-choice gating leads to load imbalance and training instability and introduces the mathematical load-balancing auxiliary loss with explicit formulation. The equations and the discussion of the optimal condition provide technically grounded reasoning for the mechanism behind imbalance and its mitigation.\n  - The discussion of “Expert Capacity for Token-Choice Gating” links capacity limits to token overflow and describes practical behaviors such as the “Drop-towards-the-End” phenomenon, “Context-independent Specialization,” and “Early Routing Learning” (reported by OpenMoE). These observations go beyond description, pointing to underlying routing dynamics (later-sequence tokens dropped due to capacity saturation; token-ID-level specialization emerging early).\n  - The comparison of top-k vs top-1 gating (Switch vs others) and the rationale for top-k (“consult multiple experts, weigh and integrate contributions”) vs top-1 (competitive with stability gains) shows interpretive commentary on design choices. The adoption of z-loss in ST-MoE is tied to numerical stability of exponential functions—this is a clear, technically grounded causal explanation for training instability mitigation.\n  - BASE/S-BASE (linear assignment/optimal transport) are interpreted as reframing routing to enforce equal token counts per expert; DSelect-k is explained as smoothing discrete top-k to address convergence/statistical issues. This demonstrates synthesis of related lines (discrete optimization → smooth/transport-based formulations) with reasons tied to optimization properties.\n  - The “Expert-Choice Gating” section highlights why it avoids auxiliary losses (uniform token per expert) and its trade-off (uneven coverage, some tokens processed multiple times or not at all), explicitly interpreting the computational implications (adaptive computation to certain tokens).\n  - Non-trainable gating (hash, THOR, DEMix, language/domain routing) is framed with its benefits (“no additional gating params; full load balancing”) and limitations (domain categorization suboptimal, generalization issues), showing balanced critique.\n\n- Dense vs sparse vs soft MoE:\n  - The “Dense” subsection acknowledges performance benefits but ties them to computational cost, and gives context where dense activation remains attractive (e.g., LoRA-MoE fine-tuning due to low overhead), articulating a pragmatic trade-off.\n  - The “Soft” subsection explains the discrete-optimization root cause of instability and over-reliance on heuristic losses, then frames soft token/expert merging as retaining differentiability and training stability. It explicitly notes why token merging complicates autoregressive decoders (“future tokens inaccessible”), and why expert merging (SMEAR) offers gradient-based training without large compute increases; Lory’s causal-segment routing is positioned as addressing autoregressive constraints and specialization. This is a particularly strong example of technical causality and design trade-offs.\n\n- Experts and architectural choices:\n  - The “Experts—Network Types” section explains with evidence why FFN modules are the preferred MoE target (higher sparsity and domain specificity than attention; emergent modularity and neuron co-activation patterns), linking empirical observations (Pan et al., Zhang et al.) to architectural decisions. This connects mechanism (sparsity/modularity) to design (place MoE in FFN).\n  - The “Attention” subsection discusses mixture-of-attention (MoA) design choices (sharing key/value projections to reduce compute; expert variability in query/output projections) and how later works generalize this, demonstrating a causal link between cost constraints and shared projections.\n\n- Hyperparameters and placement trade-offs:\n  - In “Hyperparameters,” the paper explains trends in expert count (hundreds → ~64), effects of domain shift on sparse MoE quality, and the rationale for fine-grained expert segmentation (DeepSeekMoE/Qwen/DBRX), relating segmentation to knowledge decomposition and flexible activation combinations. It also analyzes placement frequency (1/2 vs 1/4 vs every layer), tying these to system overhead and performance, and cites studies that found minimal performance loss with fewer MoE layers but improved speed (V-MoE), or stability issues in first layers (DeepSeekMoE), or quality gains from adjacent dense FFN (ST-MoE). This is a good synthesis of architectural layout vs compute/quality trade-offs.\n\n- Training/inference schemes:\n  - The “Training & Inference Scheme” section provides substantive analysis of dense-to-sparse transitions (residual alignment, sparse upcycling’s performance regression and mitigation via gating normalization—then noting the difference in language vs vision), and compares training-from-scratch vs upcycling using expert similarity as a diagnostic. It interprets DS-MoE’s hybrid dense-training/sparse-inference rationale by tying observed sparsity differences (attention less sparse than FFN) to activation decisions at inference. Sparse-to-dense distillation is discussed with quantified trade-offs (preserving % of quality gains at % of parameters), and mechanism-level proposals (knowledge gathering via SVD, top-k aggregation; expert pruning/averaging to revert to dense). These are strong examples of reflective and mechanism-aware commentary.\n\n- System design and bottlenecks:\n  - The “System Design” sections analyze All-to-All as the dominant bottleneck, differentiate intra-node vs inter-node topology and bandwidth impacts, and discuss hierarchical All-to-All, topology-aware routing, expert placement and pipelining. Lancet’s critique that overlap is fundamentally bounded (and how to extend overlap via partitioning non-MoE compute) shows an understanding of underlying dependency graphs rather than surface-level description. The compiler/kernel discussions (PIT, MegaBlocks) explain specific tiling/transform properties (Permutation Invariant Transformation) and the memory/translation costs in sparse frameworks, then motivate ScatterMoE’s approach to minimize footprint and extend beyond FFN. This is technically grounded and attentive to constraints rooted in hardware and communication patterns.\n\n- Challenges & Opportunities:\n  - This section synthesizes mechanisms (discrete gating → instability; sparse ops → poor hardware support; communication cost as bottleneck; specialization/collaboration tensions; generalization/overfitting; interpretability) and aligns them with promising directions (shared experts, better gating, overlapping comm/comp, hardware-aware sparsity, architecture search). It’s reflective and integrative rather than merely summarizing.\n\nAreas where analysis is more descriptive or underdeveloped:\n- “Activation Function” largely lists a progression (ReLU → GeLU → GeGLU/SwiGLU, plus RMSNorm, GQA, RoPE) without analyzing why certain activations interact differently with MoE routing or expert specialization, nor the implications of GLU variants for gating stability or FFN sparsity.\n- Portions of “Hyperparameters” and “Open models table” are more catalog-like; while trends are noted, some claims could benefit from deeper causal reasoning (e.g., why domain shift affects sparse MoE more than dense beyond brief empirical statements).\n- The MoPE taxonomy is valuable, but some subsections (e.g., attention and transformer-block MoPE) emphasize method descriptions and reported benefits; fewer parts explicitly dig into the fundamental causes (e.g., how LoRA rank and gating dynamics trade off diversity vs redundancy, or when clustering-driven routing reduces task conflicts mechanistically).\n- “Derivatives” offers examples (MoT, SMoP, Lifelong-MoE, MoD) with brief motivations; it could further analyze the deep coupling between these derivatives and core MoE mechanisms (e.g., token mixing’s effect on gradient signals to routers and expert boundaries, or stick-breaking halting’s interaction with sparse routing).\n\nOverall, the survey shows clear effort to interpret mechanisms, explain trade-offs, and synthesize across algorithmic, training, and systems lines. The depth is strong in several core areas (gating, soft vs sparse/dense, training/inference transitions, system bottlenecks), while other parts are more descriptive or uneven. This matches the 4-point criterion: meaningful analytical interpretation with some arguments underdeveloped in places.\n\nResearch guidance value:\n- High. The paper not only catalogs methods but also highlights practical system bottlenecks (All-to-All, kernel choices), training transitions (dense-to-sparse, upcycling), and architectural design trade-offs (expert count/size/placement, shared experts). The explicit equations for load balancing, critical phenomena (drop-towards-the-end), and stability tools (z-loss) provide actionable insights. Tables of auxiliary losses and open-source systems further support implementation choices. To improve guidance even more, the paper could add consolidated design recommendations (e.g., when to favor top-1 vs top-2, capacity settings by batch length distribution, router normalization practices by domain) and more comparative ablations linking choices to observed outcomes.", "Score: 4\n\nExplanation:\nThe “Challenges & Opportunities” section systematically identifies a broad set of research gaps and offers clear, high-level rationales for why they matter, with pointers to relevant prior work. It covers algorithmic issues (e.g., gating, load balancing, expert specialization), systems concerns (e.g., communication, storage, computational kernels), and aspects tied to model behavior and deployment (e.g., robustness, interpretability, integration with existing frameworks). However, while the coverage is comprehensive, most analyses are concise and remain at a high level; the section does not consistently delve into the deeper background or quantify the impact of each gap on the field. It touches the data dimension (generalization, out-of-distribution, task conflicts), but does not provide a dedicated discussion of data-related gaps (e.g., standardized benchmarks, routing trace datasets, evaluation protocols), which keeps it from a 5.\n\nSpecific parts supporting the score:\n- Training stability and load balancing (Section “Challenges & Opportunities,” paragraph “Training Stability and Load Balancing”):\n  - Identifies the discrete gating issue and its consequences: “the discrete nature of assigning a fixed number of experts to tokens leads to significant challenges in maintaining balanced workloads of experts and training stability… Load imbalances… can hinder expert specialization and further degrade model performance.”\n  - Suggests directions (regularization and innovative gating): “future studies should focus on more effective regularization techniques… or innovative gating algorithms… that encourage equitable load distribution…”\n  - Impact: connects imbalance directly to degraded performance and specialization.\n\n- Scalability and communication overhead (“Scalability and Communication Overhead”):\n  - Frames communication as a bottleneck: “the trade-off between model complexity… and the communication overhead represents a significant bottleneck in distributed training processes.”\n  - Points to concrete system approaches (DeepSpeed, FasterMoE, ScMoE) and parameter-sharing ideas as promising: “shared expert approach… holds promise for reducing the volume of data transmitted…”\n  - Impact: directly ties communication inefficiency to scalability limits.\n\n- Expert specialization and collaboration (“Expert Specialization and Collaboration”):\n  - States why specialization matters and the limitation of simple top‑k aggregation: “Relying solely on a sparsely computed weighted sum… can overlook the intricate internal relationships… Exploring new mechanisms for… specialization and collaboration… is crucial…”\n  - Impact: links collaboration shortcomings to missed performance gains.\n\n- Sparse activation and computational efficiency (“Sparse Activation and Computational Efficiency”):\n  - Identifies hardware and implementation barriers: “non-uniformity of sparse operations within hardware accelerators… optimizing the balance between activating a select top‑k subset… entails intricate coordination.”\n  - Calls for hardware optimization research: “pressing need for further research into hardware optimization techniques…”\n  - Impact: ties inability to realize theoretical efficiency to practical performance limits.\n\n- Generalization and robustness (“Generalization and Robustness”):\n  - Notes overfitting risks: “propensity for sparse MoE architectures to overfit to specific tasks or datasets…”\n  - Suggests remedies (regularization, multi-task tuning, meta-learning directions): “Future endeavors could explore innovative regularization methods… refined multi-task learning… meta-learning concepts…”\n  - Impact: highlights reduced transfer to unseen data as a core limitation.\n\n- Interpretability and transparency (“Interpretability and Transparency”):\n  - Explains why interpretability is critical: “particularly problematic in contexts where comprehending the rationale behind the model’s decisions is essential.”\n  - Connects interpretability to practical issues: “address underlying challenges such as load balancing… and mitigation of knowledge redundancy.”\n  - Impact: positions interpretability as enabling diagnosis and trust.\n\n- Optimal expert architecture (“Optimal Expert Architecture”):\n  - Identifies architectural choices and gaps (hybrids, layer-wise allocation): “exploration of various hybrids… remains nascent… strategic allocation of a varying number of experts across different layers… presents an area ripe for investigation.”\n  - Proposes automated search: “development of automated architecture search methods… is imperative.”\n  - Impact: relates architecture to multi-task efficacy and cost.\n\n- Integration with existing frameworks (“Integration with Existing Frameworks”):\n  - Motivates the need: “enable adaptation… without necessitating training from scratch… significantly reduce resource consumption.”\n  - Notes current limitations of PEFT-MoE blends and suggests modular components: “methods may compromise model performance or complicate… Advancing… modular and plug-and-play MoE components is essential.”\n  - Impact: connects integration challenges with practical deployment and adoption.\n\nWhy this is a 4, not a 5:\n- Depth: Most gaps are presented with clear rationale and suggested directions, but the analysis is relatively brief and does not deeply unpack the background mechanics or provide concrete metrics for impact (e.g., how much training instability affects convergence rates, how communication overhead scales with specific topologies).\n- Data dimension: While “Generalization and Robustness” touches datasets and OOD concerns, the section does not explicitly discuss data-centric gaps such as lack of standardized MoE benchmarks, routing trace datasets for analysis, or dataset curation practices that impact expert specialization, which would strengthen a data-oriented perspective.\n- Impact discussion: The potential impact is stated for several gaps (performance degradation, bottlenecks, adoption barriers), but generally remains qualitative without deeper exploration of the magnitude or downstream consequences on research trajectories and industrial deployment.\n\nOverall, the section offers a comprehensive inventory of key gaps across algorithms, systems, and practice, with clear statements of why they matter and what directions seem promising, but the analysis depth and data-centric treatment are not sufficient for a top score.", "Score: 4/5\n\nExplanation:\n- Overall assessment: The paper’s “Challenges & Opportunities” chapter presents a broad, well-structured set of forward-looking directions grounded in clearly articulated gaps in current MoE research and in real-world constraints (scalability, communication, deployment, hardware). The directions span algorithmic, system, and application concerns and generally align with practical needs. However, most suggestions remain high-level and do not consistently provide concrete, actionable paths or a deep analysis of academic/practical impact, which prevents a full score.\n\n- Evidence from specific parts and sentences:\n  - Chapter “Challenges & Opportunities” → Training Stability and Load Balancing:\n    - The authors identify a core gap (“the discrete nature of assigning a fixed number of experts to tokens leads to significant challenges in maintaining balanced workloads of experts and training stability”) and propose forward-looking directions: “future studies should focus on more effective regularization techniques … or innovative gating algorithms … that encourage equitable load distribution among experts and enhance model training stability.” \n    - This directly ties an algorithmic shortcoming to a new research direction with practical implications (more stable training for large-scale LLM MoE).\n\n  - Chapter “Challenges & Opportunities” → Scalability and Communication Overhead:\n    - Real-world bottlenecks are clearly articulated (“the trade-off between model complexity … and the communication overhead represents a significant bottleneck in distributed training processes”).\n    - The paper proposes directions that align with system-level needs: “develop and implement effective strategies that enhance the efficiency of information transfer from system aspect or streamline information exchange without compromising model performance from algorithm aspect.” It points to concrete ideas already shown to help (e.g., “the shared expert approach … holds promise for reducing the volume of data transmitted … while concurrently enhancing model performance”), thus mapping gaps to researchable, practice-relevant solutions.\n\n  - Chapter “Challenges & Opportunities” → Expert Specialization and Collaboration:\n    - The gap is well framed (“Relying solely on a sparsely computed weighted sum … can overlook the intricate internal relationships …”). \n    - The suggested direction (“exploring new mechanisms for enhancing both the specialization and collaboration among experts”) is forward-looking and motivated by observed shortcomings in current practice, though still broad.\n\n  - Chapter “Challenges & Opportunities” → Sparse Activation and Computational Efficiency:\n    - A clear hardware-level real-world issue is cited (“non-uniformity of sparse operations within hardware accelerators”). \n    - The proposed direction (“further research into hardware optimization techniques that more adeptly accommodate sparse computations”) is aligned with practical deployment constraints and industry needs.\n\n  - Chapter “Challenges & Opportunities” → Generalization and Robustness:\n    - The authors highlight a known gap (“propensity for sparse MoE architectures to overfit … undermines their ability to generalize”) and propose directions (“innovative regularization methods, refined multi-task learning frameworks, or the incorporation of meta-learning concepts”). The suggestions are relevant but not deeply elaborated in terms of concrete methodologies or impact analyses.\n\n  - Chapter “Challenges & Opportunities” → Interpretability and Transparency:\n    - The problem is well motivated (“inherent complexity … poses significant challenges to interpretability”), with a concrete suggestion: “pressing need … for methods and tools that can effectively visualize and explain the behavior of individual experts … as well as the nature of their interactions.” This offers a specific research topic with clear practical value (trustworthy AI, debugging, diagnostics).\n\n  - Chapter “Challenges & Opportunities” → Optimal Expert Architecture:\n    - The paper identifies design-space gaps (“exploration of various hybrids … remains nascent” and “strategic allocation of a varying number of experts across different layers … presents an area ripe for investigation”).\n    - It proposes a specific, actionable direction: “development of automated architecture search methods specifically designed for MoE models.” This is a concrete new topic that can be operationalized and has clear academic and practical impact.\n\n  - Chapter “Challenges & Opportunities” → Integration with Existing Frameworks:\n    - The authors ground this in real-world constraints (“enable adaptation of LLMs to MoE architecture without necessitating training from scratch” and potential issues with parallel strategies).\n    - They propose practical directions: “Advancing the development of modular and plug-and-play MoE components … optimizing these components for training and deployment across diverse computing environments and hardware platforms.” This is well aligned with industry deployment needs.\n\n- Why not 5/5:\n  - While the directions are comprehensive and well aligned with gaps and real-world constraints, the analysis of their potential academic/practical impact is mostly implicit and not deeply developed. Many suggestions remain broad (e.g., “more effective regularization,” “innovative gating,” “hardware optimization”), without detailed research agendas, metrics, or step-by-step actionable pathways. The paper rarely quantifies impact or offers specific experimental designs or benchmarks to operationalize the directions.\n\nIn sum, the paper presents multiple forward-looking and relevant future directions tied to clear gaps across algorithmic, system, and application layers, earning a 4/5. Further specificity, deeper impact analysis, and actionable research roadmaps would raise it to a 5/5."]}
