{"name": "G", "paperour": [5, 5, 5, 5, 5, 5, 5], "reason": ["Score: 5\n\nExplanation:\nThe Introduction clearly and specifically articulates the surveyâ€™s research objectives, provides substantial background and motivation, and demonstrates strong academic and practical significance.\n\n- Research Objective Clarity:\n  - The paper explicitly states its core aim and scope: â€œIn this survey, we categorize, summarize, and discuss each of these areas of researchâ€ (Introduction), referring to metrics for bias evaluation, datasets for bias evaluation, and techniques for bias mitigation. \n  - It further clarifies how it will organize the field: â€œMetrics for bias evaluation are organized by the underlying data structureâ€¦ Datasets are similarly categorizedâ€¦ Techniques for bias mitigation are organized by the stage of intervention: pre-processing, in-training, intra-processing, and post-processing.â€ This shows a precise, methodical plan.\n  - The section â€œThe key contributions of this work are as follows:â€ lists five concrete contributions (definitions and taxonomy of bias/fairness, taxonomy of metrics, taxonomy and compilation of datasets, taxonomy of mitigation techniques, and open problems), underscoring objective specificity and coverage.\n  - The authors clearly delineate scope and inclusion criteria: â€œwe focus solely on bias issues in LLMs for Englishâ€¦ restrict our search to works that propose novel closed-form metrics, datasets, or mitigation techniquesâ€¦â€ and define what constitutes an LLM (later formalized in the problem section). This reduces ambiguity about what is and isnâ€™t covered.\n\n- Background and Motivation:\n  - The Introduction provides thorough motivation rooted in the state of the field: it explains the rise of LLMs and their paradigm shift (â€œThe rise and rapid advancement of large language modelsâ€¦ has fundamentally changed language technologiesâ€¦â€), then immediately grounds the need for the survey in harms and biases (â€œLLMs inherit stereotypes, misrepresentations, derogatory and exclusionary languageâ€¦ These harms are forms of â€˜social biasâ€™â€¦â€).\n  - It cites extensive prior work documenting harms and underscores why technical measures (metrics, datasets, mitigations) are needed and prolific, setting up the rationale for the surveyâ€™s taxonomies.\n  - The authors also situate their survey among existing literature, noting gaps and how their work extends breadth and depth (â€œour work provides increased breadth and depthâ€¦ comprehensive overviewâ€¦ synthesizing diverse bodies of workâ€¦â€), which clarifies the necessity and contribution.\n\n- Practical Significance and Guidance Value:\n  - The survey promises actionable resources and guidance for practitioners: â€œEach taxonomy provides a reference for researchers and practitioners to identify which metrics, datasets, or mitigations may be appropriateâ€¦ understand the tradeoffsâ€¦â€ This is a direct statement of practical utility.\n  - It commits to formalization and unified notation to enhance comparability (â€œWe formalize metrics mathematically with a unified notationâ€¦â€), which is valuable for rigorous evaluation and implementation.\n  - It provides a public repository of datasets (â€œWe share publicly-available datasets here: https://github.com/i-gallegos/Fair-LLM-Benchmarkâ€), increasing accessibility and real-world applicability.\n  - The Introduction maps the paper structure (â€œIn the remainder of the article, we first formalizeâ€¦ then provide taxonomiesâ€¦ Finally, we discuss open problemsâ€¦â€), giving readers a clear guide to how to use the survey.\n\nNote: An explicit Abstract was not included in the provided excerpt. Nonetheless, the Introduction fulfills the role of defining objectives, motivation, and significance with sufficient clarity and depth to merit a top score under the criteria.", "Score: 5\n\nExplanation:\n- Method classification clarity is excellent and consistently applied across the paper. The authors explicitly introduce three coherent taxonomiesâ€”metrics, datasets, and mitigation techniquesâ€”and ground them in a formal framework of bias/fairness for LLMs.\n  - In â€œOverview of Taxonomiesâ€ the paper concisely previews the three taxonomies and their subcategories (Metrics by embeddings/probabilities/generated text; Datasets by counterfactual inputs/prompts; Mitigation by pre-/in-training/intra-/post-processing). This signals a clear organizational logic and prepares the reader for systematic coverage.\n  - â€œTaxonomy of Metrics for Bias Evaluationâ€ is structured by what the metric uses from the model (embeddings, probabilities, generated text), a principled choice that matches typical levels of access to LLMs. The subsection â€œFacets of Evaluation of Biases: Metrics and Datasetsâ€ explicitly disentangles datasets from metrics and formalizes their relationship (â€œFor an arbitrary dataset D, there is a subset of evaluation metrics Ïˆ(D)â€¦â€)â€”showing inherent connections across categories and improving clarity. Each metric class is further subdivided and mathematically formalized (â€œWord Embedding Metrics,â€ â€œSentence Embedding Metrics,â€ â€œMasked Token Methods,â€ â€œPseudo-Log-Likelihood Methods,â€ â€œGenerated Text-Based Metricsâ€), with consistent notation, examples, and limits.\n  - â€œTaxonomy of Datasets for Bias Evaluationâ€ reflects data structure as the organizing principle (counterfactual inputs and prompts) and then separates masked token vs. unmasked sentence pairs and sentence completions vs. QA. This mirrors how evaluation tasks are set up in practice and aligns well with the previous metrics taxonomy (e.g., masked token datasets naturally pair with masked-token/probability metrics).\n  - â€œTaxonomy of Techniques for Bias Mitigationâ€ organizes methods by intervention stage, which is both intuitive and aligned with the development/deployment lifecycle detailed earlier in â€œBias in the Development and Deployment Life Cycle.â€ The pre-/in-/intra-/post-processing division creates clear, non-overlapping buckets, and the granular subcategories (e.g., within pre-processing: data augmentation, filtering/reweighting, generation, instruction tuning, projection-based) further enhance clarity.\n\n- Evolution of methodology is systematically presented, with trends and lineage within and across categories:\n  - Metrics evolution is explicit. The narrative progresses from static word embeddings (WEAT) to contextualized embeddings (SEAT, CEAT), and then to probability-based methods (PLL and its variants CPS, CAT, AUL/AULA, LMB), and finally to generated-text metrics (distribution, classifier, lexicon). The subsections consistently explain how later methods extend or address limitations of earlier ones. For example, â€œSentence Embedding Metricsâ€ shows WEATâ€™s extension to contextualized encoders (SEAT) and further to sampling-based effect sizes (CEAT). â€œPseudo-Log-Likelihood Methodsâ€ traces from CrowS-Pairs Score to CAT to AUL/AULA, explaining why each modification (e.g., attention-weighted variants) was introduced. These are direct indications of methodological evolution.\n  - Dataset evolution is well-argued. Under â€œCounterfactual Inputs,â€ the paper moves from early coreference-focused Winogender/WinoBias to larger/more diverse GAP, BUG, and StereoSet, then to broader unmasked sentence pairs (CrowS-Pairs, Equity Evaluation Corpus, RedditBias, HolisticBias, WinoQueer, PANDA, Bias NLI). The â€œDiscussion and Limitationsâ€ section cites critiques (e.g., blodgett2021stereotyping) and the resulting adjustments and expansions (e.g., GAP, BUG), thereby revealing how earlier weaknesses motivated newer datasets. The â€œPromptsâ€ section similarly shows development from RealToxicityPrompts and BOLD toward QA-style evaluations (BBQ, UnQover) and IR-oriented tests (Grep-BiasIR), indicating diversification of evaluation paradigms as the field matured.\n  - Mitigation techniques show a clear trajectory from simpler pre-processing (word swaps/CDA, filtering) to more sophisticated data generation and instruction tuning, and then to training-time methods that alter objectives (embedding/attention/distribution equalization, contrastive/adversarial/reinforcement learning), and finally to inference-stage controls (decoding modifications, modular debiasing networks) and post-hoc rewriting. The â€œLoss Function Modificationâ€ subsection is particularly strong in showing methodological trends: it groups techniques (embeddings, attention, predicted token distributions) and then expands to dropout, contrastive, adversarial, and reinforcement learningâ€”demonstrating how debiasing objectives have broadened from simple invariance constraints to more nuanced learning paradigms.\n  - The paper repeatedly ties methods back to their limitations and how subsequent approaches address them. For metrics and datasets, each major section includes â€œDiscussion and Limitationsâ€ and â€œRecommendations,â€ which highlight weaknesses (e.g., weak correlation of embedding/probability metrics with downstream biases; reliability concerns with coreference/anti-stereotype datasets; classifier biases in toxicity detection) and point to newer directions. This pattern evidences a systematic depiction of the fieldâ€™s evolution driven by identified gaps.\n  - The â€œBias in the Development and Deployment Life Cycleâ€ and â€œFairness Desiderata for LLMsâ€ sections provide a conceptual scaffold that clarifies why the downstream taxonomies are organized as they are (e.g., intervention stages map onto design/deployment points; metrics/datasets reflect varying model access and task structures). This strengthens the sense of an overarching development path rather than a mere enumeration.\n\n- Inherent connections and trends:\n  - The explicit mapping between datasets and metrics (â€œÏˆ(D) âŠ‚ Î¨â€) and the continual emphasis on compatibility (e.g., masked-token datasets with masked-token probability metrics; sentence pairs adaptable to generated text metrics) demonstrate thoughtful cross-category integration.\n  - Multiple places show how early ideas evolve into more general frameworks: static to contextual embeddings; PLL score variants; moving from word lists and binary attributes to non-binary/multigroup fairness (e.g., Auto-Debias Jensen-Shannon divergence); from prompt instructions/control tokens to continuous prompt tuning; from single-task debiasers to modular networks that can be swapped in at inference.\n  - The â€œOpen Problems & Challengesâ€ section synthesizes trajectories and points to future trends (participatory design, better fairness desiderata for NLP, improved evaluation validity, hybrid mitigation, understanding bias mechanisms), further evidencing the authorsâ€™ grasp of the fieldâ€™s direction.\n\nOverall, the paperâ€™s method/related-work sections present clear, principled classifications and convincingly trace methodological progression across metrics, datasets, and mitigation techniques. The consistent structure, formalization, critical discussions, and forward-looking recommendations collectively reveal technological advancements and development trends, fully justifying a score of 5.", "Score: 5\n\nExplanation:\nThe survey provides comprehensive coverage of both datasets and evaluation metrics, and it does so with clear structure, detailed descriptions, and critical discussion of rationale and limitations.\n\n- Diversity of metrics:\n  - The survey organizes metrics into three major familiesâ€”embedding-based, probability-based, and generated text-basedâ€”reflecting different levels of model access and output types. This is introduced explicitly in â€œTaxonomy of Metrics for Bias Evaluationâ€ and summarized again in â€œOverview of Taxonomiesâ€ (compactenum listing of Embedding-Based, Probability-Based, and Generated Text-Based metrics).\n  - Embedding-based metrics: It covers WEAT, SEAT, CEAT, and Sentence Bias Score with formulas and operational details (e.g., the WEAT effect size and adaptation to contextualized embeddings; â€œWord Embedding Metricsâ€ and â€œSentence Embedding Metricsâ€). These sections provide equations (e.g., s(a, W1, W2), WEAT(A1, A2, W1, W2)) and adaptations from word to sentence-level encoders, indicating strong technical grounding.\n  - Probability-based metrics: It includes masked token methods (DisCo, LPBS, CBS) and pseudo-log-likelihood methods (CrowS-Pairs Score, CAT, iCAT, AUL/AULA, LMB), each with precise definitions and formulas (e.g., PLL(S) = Î£ log P(s|S\\{s};Î¸), CPS(S), CAT(S), AUL(S), attention-weighted AULA), in â€œProbability-Based Metrics.â€ The explanations highlight how metrics compare stereotypes vs. anti-stereotypes, and how normalization or prior correction is handled (LPBS).\n  - Generated text-based metrics: It covers distribution-based (Co-Occurrence Bias Score, Demographic Representation, Stereotypical Associations, Social Group Substitutions), classifier-based (Perspective API-derived EMT/TP/TF, Score Parity, Counterfactual Sentiment Bias, Regard Score, Full Gen Bias), and lexicon-based (HONEST, Psycholinguistic Norms, Gender Polarity) approaches in â€œGenerated Text-Based Metrics.â€ These include concrete operationalizations, e.g., EMT and TF formulas, and tie-ins to auxiliary classifiers.\n  - The survey does not only enumerate metrics; it consistently discusses limitations and validity concerns (e.g., for embedding-based metrics: weak correlation with downstream bias, dependence on design choices; for probability-based: template limitations, stereotype/anti-stereotype validity; for generated text: decoding sensitivity, classifier bias, lexicon coarseness). See â€œDiscussion and Limitationsâ€ subsections after each metric family and the cross-cutting â€œRecommendationsâ€ section.\n\n- Diversity of datasets:\n  - The surveyâ€™s dataset taxonomy (â€œTaxonomy of Datasets for Bias Evaluationâ€) splits datasets by structure (counterfactual inputs: masked tokens vs. unmasked sentences; prompts: sentence completions vs. question-answering), clearly mapping to compatible metric families (as emphasized in â€œFacets of Evaluation of Biases: Metrics and Datasetsâ€ and the Definition of Evaluation Metric Ïˆ(ğ’Ÿ)).\n  - Counterfactual masked tokens: Winogender (720 sentences over 60 occupations with she/he/they), WinoBias (3,160 sentences, 40 occupations; Type 1/2), WinoBias+ (3,167 instances; gender-neutral), GAP (8,908 pronoun-name pairs), GAP-Subjective (8,908), BUG (108,419 sentences), StereoSet (16,995 crowdsourced intra/inter-sentence items, multiple axes), BEC-Pro (5,400 profession templates). These entries include scale numbers, task framing, and labeling/collection methods (e.g., crowdsourced StereoSet; Wikipedia-derived GAP).\n  - Counterfactual unmasked sentences: CrowS-Pairs (1,508 pairs across nine bias types), Equity Evaluation Corpus (8,640 templates, gender and race), RedditBias (11,873 sentences, human annotation), HolisticBias (460,000 prompts across 13 axes with participatory process), WinoQueer (45,540 community-sourced pairs), Bias-STS-B (STS-B adaptation), PANDA (98,583 perturbations), Bias NLI (entails/contradicts neutrality across gender/nationality/religion). The survey often notes the curation method (e.g., community-sourced WinoQueer; participatory HolisticBias; RedditBias with human annotations), demonstrating attention to labeling and provenance.\n  - Prompts for generation: RealToxicityPrompts (100,000 prefixes scored by Perspective API, quartile sampling), BOLD (23,679 Wikipedia-truncated prompts across five domains), HONEST (420 multilingual cloze prompts), TrustGPT (prompt types for toxicity and disparity assessment). QA-style prompts: BBQ (58,492 QA items across nine groups, ambiguous/disambiguated contexts), UnQover (underspecified QA templates), HolisticBias in QA framing, and Grep-BiasIR (118 gender-neutral IR queries + 708 doc variations). These entries include dataset scales, collection sources (web text, Wikipedia), and labeling strategies (Perspective API scores, crowdsourcing, participatory curation).\n\n- Rationality and applicability:\n  - The survey motivates why metrics should be chosen based on model access and dataset structure (Ïˆ(ğ’Ÿ) âŠ‚ Î¨; â€œFacets of Evaluation of Biases: Metrics and Datasetsâ€ and â€œOverview of Taxonomiesâ€), helping practitioners make sound selections. It emphasizes the distinction between datasets and metrics and explicitly shows how multiple metrics can apply to a single dataset (e.g., CrowS-Pairs with PLL and AUL/AULA).\n  - It critically evaluates measurement validity and ecological relevance. For metrics, it warns that embedding/probability metrics may be weak proxies for downstream harms and should be accompanied by task-level evaluations (â€œRecommendationsâ€). For generated text metrics, it discusses decoding sensitivity (â€œakyurek2022challengesâ€) and classifier biases (toxicity and sentiment misclassification against dialects and stigmatized groups). For datasets, it interrogates reliability/validity issues (e.g., Winogender/WinoBias/StereoSet/CrowS-Pairs critiques citing â€œblodgett2021stereotypingâ€), generalizability (US-centric occupational data), and ambiguity in coreference tests (â€œDiscussion and Limitationsâ€ under datasets; â€œRecommendationsâ€ to check construct/content/ecological validity).\n  - The survey provides actionable guidance: report model specs and prompts; construct metrics grounded in real-world harm and power dynamics; ensure datasetsâ€™ validity/generalizability (â€œRecommendationsâ€ under metrics and datasets). It also links a consolidated repository of publicly-available datasets (https://github.com/i-gallegos/Fair-LLM-Benchmark), enhancing practical utility.\n\n- Coverage of labeling and application contexts:\n  - Many entries specify how datasets were built and labeled (e.g., StereoSet crowdsourcing; RedditBias human annotations; WinoQueer community-sourced; RealToxicityPrompts scored by Perspective API; BOLD scraped/truncated Wikipedia), and they span application scenarios (coreference resolution, open-ended generation, QA, IR, NLI, toxicity detection). The survey explicitly connects task types to bias manifestations (see â€œBias in NLP Tasksâ€) and the evaluation facets section.\n\n- Balance and criticality:\n  - Beyond enumeration, the survey consistently weights pros/cons and articulates where each metric/dataset is strong or weak, which supports academically sound and practically meaningful selection. It also includes formal definitions and unified notation for metrics, improving clarity and comparability.\n\nMinor limitations noted by the authors (and relevant to scoring but not disqualifying):\n- The focus is primarily on English LLMs (acknowledged in â€œLimitationsâ€), though some multilingual elements appear (HONEST).\n- As a survey, it does not run new experiments, but its evaluative depth on metrics/datasets compensates.\n\nGiven the breadth of datasets and metrics, their detailed descriptions (including scale, scenarios, and labeling), and the strong, critical rationale for selection and use, this section merits the highest score.", "Score: 5\n\nExplanation:\nThe survey delivers a systematic, well-structured, and technically grounded comparison of methods across multiple, meaningful dimensions, clearly articulating advantages, disadvantages, commonalities, and distinctions. It avoids superficial listing by formalizing methods, specifying assumptions, and discussing limitations with rigor.\n\nEvidence from specific sections and sentences:\n\n- Systematic organization across multiple dimensions:\n  - Taxonomy of Metrics for Bias Evaluation: The paper categorizes metrics by what they use from the model (embeddings, probabilities, generated text) and further breaks these down into subtypes (â€œEmbedding-Based Metrics,â€ â€œProbability-Based Metrics,â€ â€œGenerated Text-Based Metricsâ€), each with definitions, equations, examples, and a â€œDiscussion and Limitationsâ€ subsection. See â€œTaxonomy of Metrics based on What They Useâ€ and the three metric subsections. The authors explicitly note the datasetâ€“metric relationship and compatibility: â€œIn the literature, many works refer to the metric as the datasetâ€¦Therefore, whenever possible, we decompose the dataset from the metricâ€¦â€ (Facets of Evaluation of Biases: Metrics and Datasets).\n  - Taxonomy of Datasets for Bias Evaluation: The dataset taxonomy is organized by data structure (â€œCounterfactual Inputsâ€ vs â€œPromptsâ€; â€œMasked Tokensâ€ vs â€œUnmasked Sentencesâ€), connecting to which metrics are appropriate. This is stated clearly: â€œwe organize datasets by their data structure,â€ and the paper repeatedly maps dataset structures to compatible metric families (e.g., Section â€œCounterfactual Inputsâ€ noting suitability for PLL and masked token metrics; â€œPromptsâ€ noting suitability for generated text-based metrics).\n  - Taxonomy of Techniques for Bias Mitigation: Mitigations are categorized by intervention stage (â€œPre-Processing,â€ â€œIn-Training,â€ â€œIntra-Processing,â€ â€œPost-Processingâ€), with granular subcategories (e.g., â€œData Augmentation,â€ â€œLoss Function Modification,â€ â€œDecoding Strategy Modification,â€ â€œRewritingâ€). This provides a clear comparison framework based on modeling perspective and learning strategy.\n\n- Clear descriptions of advantages and disadvantages, with rigorous â€œDiscussion and Limitationsâ€ per method class:\n  - Embedding-based metrics: The survey critically notes weak or inconsistent relationships with downstream tasks (â€œgoldfarb2021intrinsic find no reliable correlation at allâ€¦ bias in representations and bias in downstream applications should not be conflatedâ€¦â€; â€œdelobelle2022measuringâ€¦ recommend avoiding embedding-based metrics at allâ€ in â€œDiscussion and Limitationsâ€ of Embedding-Based Metrics).\n  - Probability-based metrics: It highlights template dependence, questionable fairness criteria in stereotype/anti-stereotype comparisons, and binary group assumptions (â€œMasked token metrics rely on templatesâ€¦ can lack generalizability and reliabilityâ€; â€œselecting between two sentences may not fully capture the tendency of a model to produce stereotypical outputsâ€¦â€ in â€œDiscussion and Limitationsâ€ of Probability-Based Metrics).\n  - Generated text-based metrics: It discusses decoding parameter sensitivity (â€œdecoding parametersâ€¦ can drastically change the level of biasâ€), classifier bias and evolution (â€œtoxicity classifiers may disproportionately flag African-American Englishâ€¦ automatic toxicity detection are not staticâ€), and lexicon coarseness (â€œlexicon-based metrics may be overly coarseâ€¦ overlook relational patternsâ€) in â€œDiscussion and Limitationsâ€ of Generated Text-Based Metrics.\n  - Datasets: The paper addresses construct, content, and ecological validity (â€œblodgett2021stereotyping highlight several severe shortcomingsâ€¦ ambiguitiesâ€¦ inconsistent perturbationsâ€; â€œgeneralizabilityâ€¦ often situated in the United States contextâ€) in â€œDiscussion and Limitationsâ€ for â€œCounterfactual Inputsâ€ and â€œPrompts.â€\n  - Mitigation techniques:\n    - Pre-Processing: It notes reliance on word lists, binary assumptions, and risk of erasure (â€œword listsâ€¦ unscalableâ€¦ assume binary or immutable social groupingsâ€¦ flatten pertinent power imbalancesâ€) in â€œDiscussion and Limitationsâ€ of Pre-Processing Mitigation.\n    - In-Training: It discusses computational expense and catastrophic forgetting, and contrasts effectiveness by targeted component (â€œattention may be one of the primary ways that bias is encodedâ€¦ attention-based loss function modifications may be more effectiveâ€) in â€œDiscussion and Limitationsâ€ of In-Training Mitigation.\n    - Intra-Processing: It highlights the challenge of balancing debiasing with output diversity and the risks of biased heuristics (â€œminority voices are often disproportionately filteredâ€¦ detoxifying may amplify bias by not generating minority dialectsâ€) in â€œDiscussion and Limitationsâ€ of Intra-Processing Mitigation.\n    - Post-Processing: It discusses subjectivity in rewriting and risks of erasure and dependence on parallel corpora (â€œdetermination of which outputs to rewrite is subjectiveâ€¦ removal of protected attributes can erase important contextsâ€) in â€œDiscussion and Limitationsâ€ of Post-Processing Mitigation.\n\n- Identification of commonalities and distinctions:\n  - The paper repeatedly contrasts methods by architecture, objective, and assumptions:\n    - Architecture/component differences: Adapters vs full fine-tuning (â€œdebiasing adapter modules, called ADELEâ€¦ only the injected layers are updatedâ€) and attention vs embeddings (â€œSome evidence indicates attention layersâ€¦ propose loss functions that modify attention weightsâ€) in â€œArchitecture Modificationâ€ and â€œLoss Function Modification.â€\n    - Objective differences: Detailed loss formulations and training paradigms, including distance/projection/MI-based regularizers, entropy-based attention, equalizing output distributions, CLP, contrastive learning, adversarial learning, and reinforcement learning (â€œLoss Function Modificationâ€ subsections with unified notation and equations; e.g., Jensenâ€“Shannon divergence, KL terms, MI minimization, entropy maximization).\n    - Assumptions and fairness criteria: The authors explicitly surface fairness assumptions (â€œnearly all metrics presented here employ some notion of invarianceâ€¦â€; â€œmost metrics assume binary social groupsâ€¦ requiring equal word predictions may not fully capture all forms of biasâ€) in â€œRecommendationsâ€ and â€œDiscussion and Limitationsâ€ for metrics.\n\n- Technical grounding and depth:\n  - Formalization: The survey â€œformalize[s] metrics mathematically with a unified notationâ€ (Introduction contributions) and provides numerous equations for losses and metrics (e.g., WEAT/SEAT/CEAT definitions; PLL/CAT/AUL/AULA; diverse regularization terms in â€œLoss Function Modificationâ€). This demonstrates technical rigor in contrasting learning objectives and method mechanics.\n  - Explicit modeling trade-offs: The paper discusses performanceâ€“fairness trade-offs and the risk of catastrophic forgetting (â€œFuture work can better characterize this performance-fairness trade-offâ€¦ catastrophic forgettingâ€) in â€œImproving Mitigation Effortsâ€ and â€œExploring Theoretical Limits.â€\n\n- Avoidance of superficial listing:\n  - Beyond enumerating, the survey synthesizes (â€œWe characterize the relationship between evaluation metrics and datasetsâ€ in Introduction contributions; â€œWe identify limitations of each classâ€¦â€; â€œRecommendationsâ€ sections for metrics, datasets, and mitigations) and explains why differences matter for downstream applications, data dependence, and fairness assumptions.\n\nOverall, the paper meets the 5-point criteria by providing a comprehensive, structured, and technically detailed comparison across modeling perspective, data dependency, learning strategy, and application contexts, with explicit pros/cons, assumptions, and architectural distinctions.", "5\n\nExplanation:\n- The paper provides deep, technically grounded critical analysis that goes beyond summarization, consistently explaining underlying mechanisms, assumptions, and trade-offs across methods. This is evident in the repeated â€œDiscussion and Limitationsâ€ subsections and the cross-cutting recommendations in the metrics, datasets, and mitigation sections.\n\n- Explains fundamental causes of method differences:\n  - Embedding-based metrics: The authors explicitly analyze why intrinsic bias measures may not reflect downstream harms, citing mechanisms such as independence between protected-attribute associations and downstream performance (â€œSeveral works point out that biases in the embedding space have only weak or inconsistent relationships with biases in downstream tasksâ€¦ goldfarb2021intrinsic find no reliable correlation at allâ€¦ associationsâ€¦ can be independent of downstream performance disparities,â€ in â€œEmbedding-Based Metrics â€” Discussion and Limitationsâ€). They also identify design choices (templates, seed words, representation type) as root causes for instability (â€œembedding-based measures of bias can be highly dependent on different design choicesâ€¦â€).\n  - Probability-based metrics: They analyze why PLL-based metrics may mislead, grounding it in the stereotype/anti-stereotype constructionâ€™s weak theoretical link to fairness (â€œThe notion that stereotype and anti-stereotype sentencesâ€¦ should be selected at equal ratesâ€¦ is not obviousâ€¦ and may depend heavily on the conceptualization,â€ in â€œPseudo-Log-Likelihood Methods â€” Discussion and Limitationsâ€).\n  - Generated text metrics: They diagnose parameter sensitivity (temperature, length, top-k) as a cause of contradictory results (â€œdecoding parametersâ€¦ can drastically change the level of bias,â€ in â€œGenerated Text-Based Metrics â€” Discussion and Limitationsâ€), and articulate specific failure modes for each class (distribution metrics fail to capture use-mention; classifier metrics inherit their own biases; lexicon metrics are overly coarse).\n\n- Analyzes design trade-offs, assumptions, and limitations:\n  - Datasets: The paper provides a critical reliability/validity audit of widely used resources (Winogender, WinoBias, StereoSet, CrowS-Pairs), pointing to ambiguous stereotype content and inconsistent perturbations (â€œIn nearly half of all instancesâ€¦ ambiguitiesâ€¦ raising questions whether they are valid indicatorsâ€¦â€ and a concrete example of Ethiopia in StereoSet, in â€œCounterfactual Inputs â€” Discussion and Limitationsâ€). They also flag generalizability issues (US-centric occupation data) and task interpretability problems (choosing between two sentences may not reflect generation propensity).\n  - Pre-processing mitigation: They detail why CDA/CDS can be normatively and technically problematicâ€”limited word lists, grammatical errors, binary/immutable group assumptions, proxy conflation, and power imbalance flattening (â€œData augmentation methods can be particularly problematic when they assume binary or immutable social groupingsâ€¦ Merely masking or replacing identity words flattens pertinent power imbalances,â€ in â€œPre-Processing Mitigation â€” Discussion and Limitationsâ€).\n  - In-training mitigation: They identify computational feasibility and catastrophic forgetting as key trade-offs for fine-tuning, while also relating effectiveness to which model components encode bias (embedding vs attention) (â€œone of the biggest limitationsâ€¦ computational expenseâ€¦ catastrophic forgettingâ€¦ attention may be one of the primary ways that bias is encoded in LLMs,â€ in â€œIn-Training Mitigation â€” Discussion and Limitationsâ€).\n  - Intra-processing mitigation: They articulate the central tension between detoxification and preserving minority dialects/language diversity (â€œtechniques that reduce toxicity can in turn amplify bias by not generating minority dialects like African-American English,â€ in â€œIntra-Processing Mitigation â€” Discussion and Limitationsâ€).\n  - Post-processing mitigation: They highlight the subjective nature of rewriting decisions, risks of erasing identities, and dependence on parallel corpora (â€œrewriting techniques are themselves prone to exhibiting biasâ€¦ The removal of protected attributes can also erase important contexts,â€ in â€œPost-Processing Mitigation â€” Discussion and Limitationsâ€).\n\n- Synthesizes relationships across research lines:\n  - The metricâ€“dataset decomposition and formalization of Ïˆ(ğ’Ÿ) explicitly connect evaluation instruments to data structures, correcting a common conflation in prior work (â€œwe decompose the dataset from the metricâ€¦ For an arbitrary dataset ğ’Ÿ, there is a subset of evaluation metrics Ïˆ(ğ’Ÿ)â€¦,â€ in â€œFacets of Evaluation of Biases: Metrics and Datasetsâ€). This synthesis helps readers understand compatibility and constraints across tools.\n  - The fairness desiderata (fairness through unawareness, invariance, equal associations, replicated distributions) provide an interpretive lens that reappears in later critiques (e.g., cautioning that invariance-based criteria may flatten socially meaningful differences in â€œRecommendationsâ€ for metrics and in pre-/in-training mitigation discussions).\n  - The stage-based taxonomy (pre-, in-, intra-, post-processing) is repeatedly used to relate where bias arises (life cycle section) and how mitigation aligns with available access/constraints, showing thoughtful integration across literature and practice.\n\n- Technically grounded explanatory commentary:\n  - The paper cites specific mechanisms (e.g., attention entropy, mutual information regularizers, projection to nullspaces, contrastive/adversarial/reinforcement formulations) and spells out equations and optimization targets, then critiques their fairness implications and operational assumptions (e.g., mutual information between encodings and protected attributes, Jensenâ€“Shannon divergence equalization across groups).\n  - It connects empirical observations to theoretical concerns (e.g., classifier bias in toxicity detectors affecting evaluation validity; decoding strategy manipulation risks; pruningâ€™s mixed effects on bias).\n\n- Reflective insights and development trends:\n  - The â€œOpen Problems & Challengesâ€ section offers mature commentary on power imbalances and technical solutionism, participatory research design, the risks of universal benchmarks, disaggregated analysis, and theoretical limits (fairness guarantees, performanceâ€“fairness Pareto frontiers). This moves beyond description to articulate research direction with explicit normative considerations (â€œtechnical solutionsâ€¦ are incompleteâ€¦ design technologies with the needs of vulnerable groups in mind,â€ â€œbenchmarks should be approached with cautionâ€¦ may be at odds with the normativeâ€¦ contextual nature of biasâ€).\n\nTogether, these elements justify a score of 5: the paper consistently explains why methods differ, what assumptions they bake in, how design choices produce observed behaviors, and how to interpret those differences across the landscape. It ties mechanisms to harms, offers concrete examples, and proposes actionable guidance and research trajectories, all with strong analytical depth.", "Score: 5\n\nExplanation:\nThe surveyâ€™s â€œOpen Problems & Challengesâ€ section comprehensively identifies and analyzes research gaps across data, methods, theory, and socio-technical dimensions, and it consistently explains why these gaps matter and how they impact the field. Specific supporting parts include:\n\n- Addressing Power Imbalances (Section: Open Problems & Challenges â†’ Addressing Power Imbalances)\n  - The paper argues that â€œtechnical solutionsâ€¦ are incompleteâ€ and calls for centering marginalized communities and participatory designs. It explains why this is important (to disrupt harmful power hierarchies and align systems with lived experiences) and outlines the impact (e.g., participatory datasets like HolisticBias and WinoQueer, governance and data sovereignty practices that influence how resources are collected and used). It also highlights the need to broaden values beyond Western/US-centric assumptions and to expand language resources and documentation practices (e.g., datasheets), which directly affect dataset validity and inclusivity.\n\n- Conceptualizing Fairness for NLP (Section: Open Problems & Challenges â†’ Conceptualizing Fairness for NLP)\n  - The survey proposes developing fairness desiderata tailored to NLP tasks (especially generation and representational harms), and explains the impact: more precise, context-grounded fairness criteria that avoid â€œabstract notions of fairnessâ€ and instead target concrete injustices. It also details rethinking social group definitions and disaggregation to avoid reinforcing social constructions and erasing identities (e.g., nonbinary, trans), which affects evaluation metrics, datasets, and mitigation strategies. The discussion on recognizing distinct social groups cautions against treating groups as interchangeable, explaining the practical and ethical consequences for methods that erase identity cues.\n\n- Refining Evaluation Principles (Section: Open Problems & Challenges â†’ Refining Evaluation Principles)\n  - The paper calls for reporting standards and acknowledges that different metrics, model hyperparameters, and datasets can produce â€œcontradictory conclusions,â€ explaining why this undermines validity and comparability. It assesses the benefits and harms of comprehensive benchmarks, warning against â€œuniversalityâ€ and demonstrating how benchmarks can obscure nuanced harms and reflect dominant group values; this shows clear depth on background and impact. It analyzes reliability and validity issues of widely used datasets (e.g., coreference, stereotype pairs), and proposes alternatives (living datasets, audits, adversarial testing), indicating concrete pathways to improve evaluation. It also urges expanding evaluation beyond toxicity and group fairness to cover underused harms and fairness notions.\n\n- Improving Mitigation Efforts (Section: Open Problems & Challenges â†’ Improving Mitigation Efforts)\n  - The survey identifies scalability bottlenecks (e.g., reliance on word lists, human annotation) and proposes directions to expand resources while retaining community-in-the-loop practicesâ€”showing both why it matters (limits coverage of biases and groups) and the potential impact (broader, more robust mitigation). It calls for hybrid mitigation techniques across stages (pre-, in-, intra-, post-processing) to address the observation that debiasing at one stage may not persist downstream, and emphasizes studying mechanisms of bias within LLMs (e.g., attention heads, layers) to target interventions more effectively.\n\n- Exploring Theoretical Limits (Section: Open Problems & Challenges â†’ Exploring Theoretical Limits)\n  - The survey emphasizes the need for theoretical guarantees for fairnessâ€”currently lackingâ€”and explains the impact: moving beyond purely empirical claims to provable assurances of fairness. It further analyzes performanceâ€“fairness trade-offs (e.g., Pareto frontier analyses, disaggregated impacts) and reframes fairness as a necessary criterion rather than a hindrance to standard performance, directly addressing key methodological gaps and their implications.\n\nAdditionally, earlier sections reinforce these gaps with detailed limitations and recommendations:\n- Metrics (Sections: Taxonomy of Metrics â†’ Discussion and Limitations; Recommendations): They analyze weak correlations between intrinsic (embedding/probability) measures and downstream harms, classifier biases (toxicity/sentiment), and decoding sensitivityâ€”explaining why relying on such measures can mislead practice and what needs development (task-grounded, theoretically informed metrics).\n- Datasets (Sections: Taxonomy of Datasets â†’ Discussion and Limitations; Recommendations): They discuss construct/content/ecological validity issues (e.g., stereotypes poorly defined, US-centric context), the impact on evaluation fidelity, and call for broader, living datasets with participatory design.\n- Mitigations (Sections: Taxonomy of Techniques â†’ Discussion and Limitations; Recommendations): They examine assumptions (e.g., binary group swaps, erasure of identity), catastrophic forgetting, and decoding trade-offs that can suppress minority dialects, explicitly connecting these to harms and suggesting more nuanced objective functions and validations.\n\nTaken together, the paper not only lists gaps but provides sustained, multi-angle analysis of why each gap is critical and how addressing it will affect the development and reliability of fair LLMs. It spans data (collection, documentation, language resources), methods (evaluation metrics, mitigation strategies across all stages), theory (fairness desiderata, guarantees), and governance/participation, meeting the criteria for a 5-point rating.", "Score: 5\n\nExplanation:\nThe surveyâ€™s â€œOpen Problems & Challengesâ€ section presents a comprehensive, forward-looking agenda grounded in clearly articulated research gaps and real-world concerns, and it offers concrete, innovative directions with actionable suggestions. Below are the specific elements and passages supporting this score:\n\n1) Tight integration of key issues, gaps, and real-world needs\n- Addressing power imbalances: The authors explicitly frame technical work within the broader social context, emphasizing who holds power and who is harmed. They call for â€œbringing marginalized communities into the forefront of LLM decision-making and system developmentâ€ and â€œdeveloping participatory research designsâ€ (Open Problems & Challenges â†’ Addressing Power Imbalances). This directly ties research agendas to real-world needs and community impacts.\n- Expanding language resources: They highlight US-/English-centric biases and urge inclusive data practices and documentation (Open Problems & Challenges â†’ Addressing Power Imbalances â†’ Expanding language resources), citing governance, privacy, consent, and data sovereignty. This is a clear gap-to-need connection.\n\n2) Innovative directions with clear suggestions and linkages to practice\n- Community-in-the-loop frameworks: â€œDeveloping participatory research designsâ€ and â€œestablishing community-in-the-loop research frameworksâ€ (Open Problems & Challenges â†’ Addressing Power Imbalances) are concrete, innovative directions that change how research is done and align directly with practitionersâ€™ needs to build systems that do not perpetuate harm.\n- Conceptualizing fairness for NLP: They propose developing â€œfairness desiderataâ€ tailored to NLP and generated text, moving â€œbeyond abstract notions of fairnessâ€ and specifying harms and contexts (Open Problems & Challenges â†’ Conceptualizing Fairness for NLP â†’ Developing fairness desiderata). This introduces new research topics (defining task-specific fairness criteria) and provides practical guidance.\n- Rethinking social group definitions and disaggregation: The call to â€œleverage disaggregated analysis,â€ include intersectional identities, and â€œexpand its scope to groups and subgroups it has ignored or neglectedâ€ (Open Problems & Challenges â†’ Conceptualizing Fairness for NLP â†’ Rethinking social group definitions; Recognizing distinct social groups) is both forward-looking and actionable. It directly addresses the gap of binary, oversimplified demographic modeling.\n- Refining evaluation principles: They propose â€œestablishing reporting standards,â€ considering â€œliving datasets,â€ audits, adversarial testing, and ablation studies, and caution against â€œuniversalâ€ benchmarks (Open Problems & Challenges â†’ Refining Evaluation Principles). The discussion is specific and practical (e.g., reference to checklists, GEM, Dynabench), advancing both academic rigor and industry practice.\n- Improving mitigation efforts: They recommend â€œdeveloping hybrid techniquesâ€ across the pipeline and â€œunderstanding mechanisms of bias within LLMsâ€ (Open Problems & Challenges â†’ Improving Mitigation Efforts). These are concrete research directions that address a central gapâ€”limited cross-stage mitigation and sparse mechanistic understanding.\n- Exploring theoretical limits: Calls for â€œestablishing fairness guaranteesâ€ and â€œanalyzing performance-fairness trade-offsâ€ (Open Problems & Challenges â†’ Exploring Theoretical Limits) identify essential theoretical gaps and propose specific threads (e.g., Pareto frontier analysis) that can guide clear, methodologically rigorous future work.\n\n3) Specific topics and actionable paths\n- The paper repeatedly uses â€œFuture work canâ€¦â€ with concrete proposals:\n  - â€œFuture work can leverage disaggregated analysisâ€¦â€ (Conceptualizing Fairness for NLP).\n  - â€œFuture work can examine methods to resolve reliability and validity issuesâ€¦â€ and explore â€œaudits, adversarial testing, and ablation studiesâ€ (Refining Evaluation Principles).\n  - â€œFuture work can investigate hybrid mitigation techniquesâ€¦â€ and â€œresearch into how and in which componentsâ€¦ LLMs encode biasâ€ (Improving Mitigation Efforts).\n  - â€œFuture work can better characterize this performance-fairness trade-off,â€ including Pareto analysis (Exploring Theoretical Limits).\n- They reference practical frameworks and tools to operationalize suggestions (e.g., datasheets, documentation practices, living benchmarks like GEM/Dynabench), which makes the path forward actionable rather than abstract.\n\n4) Academic and practical impact discussion\n- Academic impact: The proposed fairness desiderata, disaggregated evaluation, reporting standards, and theoretical guarantees advance the scholarly rigor and measurement validity of the field.\n- Practical impact: Community participation, data governance, and reporting checklists directly inform practitioners on better development, deployment, and documentation practices; hybrid mitigation and decoding considerations address immediate engineering challenges in real systems.\n\n5) Breadth and depth\n- The section spans socio-technical governance, theory, metrics, datasets, training and inference methods, and deployment practices. It does so with depth, identifying causes of current gaps (e.g., US-centric datasets, flawed toxicity classifiers, ambiguous benchmarks) and offering multiple, concrete remedies.\n\nIn sum, the â€œOpen Problems & Challengesâ€ section and the targeted â€œRecommendationsâ€ embedded in prior taxonomies meet the highest bar of prospectiveness: they are forward-looking, tied to existing gaps and real-world harms, propose innovative and specific topics, and outline practical steps and anticipated impacts. Hence, a score of 5 is warranted."]}
