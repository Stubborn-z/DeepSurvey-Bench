{
  "authors": [
    "Yunfan Gao",
    "Yun Xiong",
    "Xinyu Gao",
    "Kangxiang Jia",
    "Jinliu Pan",
    "Yuxi Bi",
    "Yi Dai",
    "Jiawei Sun",
    "Qianyu Guo",
    "Meng Wang",
    "Haofen Wang"
  ],
  "literature_review_title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
  "year": "2023",
  "date": "2023-12-18",
  "category": "cs.CL",
  "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter\nchallenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the generation,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval, the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces up-to-date evaluation framework and\nbenchmark. At the end, this article delineates the challenges currently faced\nand points out prospective avenues for research and development.",
  "structure": [
    {
      "section_title": "Introduction/Pre-section",
      "level": "1",
      "content": "\\pdfoutput=1 \\documentclass[11pt]{article} \\usepackage[utf8]{inputenc} times algorithm \\usepackage[noend]{algorithmic} nicefrac booktabs footmisc \\usepackage[shortlabels]{enumitem} \\usepackage[margin=1in]{geometry} comment booktabs,tabularx multirow textcomp \\usepackage[numbers]{natbib} graphicx color \\usepackage[hidelinks]{hyperref} wrapfig amssymb,amsmath,amsthm \\usepackage[capitalise,noabbrev]{cleveref} array url \\usepackage[strict]{changepage} makecell titlesec setspace bm bbm threeparttable subfigure Def{Definition} \\usepackage[T1]{fontenc} \\myparagraph[1]{\\smallskip \\it {#1}} titletoc longtable \\sketch[1]{[{darkgreen{#1}}]} \\todo[1]{[{darkred{TODO: #1}}]} \\titleformat*{\\subparagraph}{\\itshape} \\repeatcaption[2]{% \\thefigure{#1}% list=no% #2 (repeated from \\cpageref{#1)}% figure{-1}% } \\newcommand {\\norm}[1]{\\| #1 \\|} \\R{R} \\DeclareMathOperator*{\\E}{E} \\BO{O} \\BTh{\\Theta} \\loss{\\ell} \\SUB[1]{\\ENSURE -0.15in #1} \\algfont[1]{#1} \\algorithmicensure{} \\newsavebox\\actorsfigure A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT Ce Zhou$^{1$The authors contributed equally to this research. Correspondence to Ce Zhou(\\url{zhouce@msu.edu) and Qian Li (liqian@act.buaa.edu.cn).} \\and Qian Li$^{2*}$ \\and Chen Li$^{2*}$ \\and Jun Yu$^{3*}$ \\and Yixin Liu$^{3*}$ \\and Guangjing Wang$^{1}$ \\and Kai Zhang$^{3}$ \\and Cheng Ji$^{2}$ \\and Qiben Yan$^{1}$ \\and Lifang He$^{3}$ \\and Hao Peng$^{2}$ \\and Jianxin Li$^{2}$ \\and Jia Wu$^{4}$ \\and Ziwei Liu$^{5}$ \\and Pengtao Xie$^{6}$ \\and Caiming Xiong$^{7}$ \\and Jian Pei$^{8}$ \\and Philip S. Yu$^{9}$ \\and Lichao Sun$^{3}$ \\and\\\\ $^{1$Michigan State University, $^{2}$Beihang University, $^{3}$Lehigh University,}\\\\$^{4$Macquarie University, $^{5}$Nanyang Technological University, $^{6}$University of California San Diego,}\\\\\\small$^{7}$Salesforce AI Research,{$^{8}$Duke University, $^{9}$University of Illinois at Chicago} } \\date{} document spacing{1.1} \\maketitle spacing \\pagebreak small \\tableofcontents small \\parskip{0.5em} \\pagebreak",
      "origin_cites_number": 0
    },
    {
      "section_title": "Introduction",
      "level": "1",
      "content": "Pretrained Foundation Models (PFMs) are regarded as essential and significant components of Artificial Intelligence (AI) in the era of big data. The foundation model is first named in~bommasani2021opportunities, which means a broader class of models and their functions. PFMs are extensively studied in the three major AI fields: natural language processing (NLP)~chowdhury2003natural, computer vision (CV)~forsyth2011computer and graph learning (GL)~bondy1976graph. PFMs are powerful general models that are effective in various fields or across fields. They have demonstrated great potential in learning feature representations in various learning tasks, such as text classification~qiu2020pre, text generation~li2021pretrained, image classification~han2020survey, object detection~sanchez2020review, and graph classification~hu2019pre. PFMs show superior performance for training on multiple tasks with large-scale corpus and fine-tuning it to similar small-scale tasks, making it possible to initiate rapid data processing.",
      "origin_cites_number": 9
    },
    {
      "section_title": "PFMs and Pretraining",
      "level": "2",
      "content": "PFMs are built upon the pretraining technique, which aims to train a general model using large amounts of data and tasks that can be fine-tuned easily in different downstream applications. The idea of pretraining originates from transfer learning~zhuang2020comprehensive in CV tasks. Recognizing the effectiveness of pretraining in the field of CV, people have begun to use pretraining technology to enhance model performance in other areas. When pretraining techniques are applied to the NLP domain, well-trained language models (LMs) can capture rich knowledge beneficial for downstream tasks, such as long-term dependencies, hierarchical relationships, etc. In addition, the significant advantage of pretraining in the NLP field is that training data can be derived from any unlabeled text corpus, that is, there is an unlimited amount of training data in the pretraining process. Early pretraining is a static technique, such as NNLM~DBLP:journals/jmlr/BengioDVJ03 and Word2vec~DBLP:journals/corr/abs-1301-3781, but static methods were difficult to adapt to different semantic environments. Therefore, dynamic pretraining techniques are proposed, such as BERT~DBLP:conf/naacl/DevlinCLT19, XLNet~DBLP:conf/nips/YangDYCSL19, etc. Fig.~history_evolution depicts the history and evolution of PFMs in the NLP, CV, and GL domains. The PFMs based on the pretraining technique use large corpora to learn generic semantic representations. With the introduction of these pioneering works, various PFMs have emerged and been applied to downstream tasks and applications. A great example of PFM application is ChatGPT\\url{https://openai.com/blog/chatgpt/}. ChatGPT is fine-tuned from the generative pretrained transformer GPT-3.5, which was trained on a blend of text and code~chen2021evaluating, neelakantan2022text. ChatGPT applies reinforcement learning from human feedback (RLHF) christiano2017deep, stiennon2020learning, which has become a promising way to align large language models (LLMs) with a human's intent~ouyang2022training. The surprisingly superior performance of ChatGPT may lead to a tipping point for a shift of training paradigm for each type of PFMs -- applying instruction aligning techniques, e.g., reinforcement learning (RL), prompt tuning brown2020language, lester2021power, schick2021exploiting, and chain-of-thought (COT) zhang2022automatic, weichain, to move towards artificial general intelligence. We focus on reviewing PFMs for text, image, and graph, which is a relatively mature research taxonomy. For text, it is a multi-purpose LM to predict the next word or character in a sequence. For example, PFMs can be used for machine translation, question-answering systems, topic modeling, sentiment analysis, etc. For image, it is similar to PFMs on text, which uses huge datasets to train a big model suitable for many CV tasks. For graphs, a similar pretraining idea is also applied to obtain PFMs, which are used for many downstream tasks. Apart from the PFMs for a specific data domain, we also review and state some other advanced PFMs, such as the PFMs for speech, video, and cross-domain data, and multimodal PFMs. An exemplary illustration is the GPT-4 model, as described by OpenAI~openai2023gpt4, which is a massive multimodal language model that can process both text and image inputs and generate text outputs. GPT-4 has demonstrated human-level performance on various professional and academic evaluation tasks. Moreover, there is a growing trend in PFMs that deals with multimodal data, known as unified PFMs. This term refers to models that can handle different types of data such as text, images, and audio. In this regard, we provide a definition of unified PFMs and a review of the current state-of-the-art models in recent research. Notable examples include OFA~wang2022unifying, UNIFIED-IO~lu2022unified, FLAVA~singh2022flava, BEiT-3~wang2022image, and others. According to the features of existing PFMs, we conclude that the PFMs have the following two major advantages. First, minor fine-tuning is required to enhance the model performance on downstream tasks. Second, the PFMs have already been vetted on the quality aspect. Instead of building a model from scratch to solve a similar problem, we can apply PFMs to task-related datasets. %Because of the advantages of PFMs, there are The great promise of PFMs has inspired a wealth of related work to focus on the model efficiency~DBLP:conf/iclr/ClarkLLM20, security~nlp52019arxiv, nlp62020acl, nlp32019acl, wang2023glint and compression~DBLP:conf/rep4nlp/GordonDA20, DBLP:conf/iclr/LanCGGSS20. figure*[!t] \\centering \\includegraphics[width=1\\linewidth]{pictures/times.pdf} The history and evolution of PFMs. figure*",
      "origin_cites_number": 18
    },
    {
      "section_title": "Contribution and Organization",
      "level": "2",
      "content": "There are several survey studies~han2021pre, sanchez2020review, qiu2020pre, li2021pretrained, han2020survey, bommasani2021opportunities that have reviewed the pretrained models for some specific areas such as text generation~li2021pretrained, visual transformer~han2020survey, objection detection~sanchez2020review. Bommasani et.al.~bommasani2021opportunities summarize the opportunities and risks of the foundation model. However, existing works did not achieve a comprehensive review of PFMs in different areas (e.g., CV, NLP, GL, Speech, Video) and different aspects such as pretraining tasks, efficiency, efficacy, and privacy. In this survey, we specifically track the evolution of PFMs in the NLP domain, as well as how pretraining is transferred to and adopted by CV and GL. Compared with other surveys, there is no comprehensive introduction and analysis of existing PFMs from all three fields. Unlike reviews of previous pretrained models, we summarize existing models ranging from traditional models to PFMs with recent works in the three domains. Traditional models emphasize static feature learning. Dynamic PFMs give an introduction to structures, which is the mainstream research. We further present some other research for PFMs, including other advanced and unified PFMs, model efficiency and compression, security, and privacy. Finally, we summarize future research challenges and open problems in different domains. We also comprehensively present the related evaluation metrics and datasets in Appendix~\\ref{Evaluation_Metrics and~datasets}. In summary, the main contributions are as follows: itemize \\item We present a solid and up-to-date review of the development of PFM in NLP, CV, and GL. Over the review, we discuss and provide insights about the generalized PFM design and pretraining methodology among the three major application domains. \\item We summarize the development of PFMs in other multimedia areas such as speech and video. Besides, we discuss advanced topics about PFMs, including unified PFMs, model efficiency and compression, and security and privacy. \\item Through the review of PFMs in various modalities for different tasks, we discuss the main challenges and opportunities for future research of very large models in the big data era, which guides a new generation of collaborative and interactive intelligence based on PFMs. itemize The rest of the survey is organized as follows. Section~Section 2 introduces the basic components. Sections~Section 3,~Section 4 and~Section 5 summarize the existing PFMs in NLP, CV and GL, respectively. Sections~Section 6,~Section 7 introduce other advanced research for PFMs, including advanced and unified PFMs, model efficiency and compression, as well as security and privacy, respectively. Furthermore, we summarize the main challenges for PFMs in Section~Section 11 before concluding the survey in Section~Section 12.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Basic Components",
      "level": "1",
      "content": "The general conceptual architecture of PFMs is shown in Fig.~fig:ptm_concepts. The PFMs are huge neural network models, which are all about neural information processing. The specific designs of PFMs vary according to the data modality and task requirements in different areas. Transformer is a mainstream model architecture design for PFMs in many areas such as NLP and CV. Training large models need to have various datasets for model pretraining. After training the PFMs, the model should be fine-tuned to satisfy downstream requirements such as efficacy, efficiency, and privacy. In this section, we introduce the basic model architectures, concepts, and settings of PFMs in NLP, CV, and GL domains. For the introduction of a more detailed component, please refer to Appendix~\\ref{Components}. figure* \\centering \\includegraphics[width=1\\linewidth]{figures/ptm_concepts.png} The general conceptual architecture of PFMs: data, model, and system. figure*",
      "origin_cites_number": 0
    },
    {
      "section_title": "Transformer for PFMs",
      "level": "2",
      "content": "The Transformer~vaswani2017attention is an innovative architecture that facilitates the transfer of weighted representation knowledge between various neural units. It relies solely on attention mechanisms and doesn't use recurrent or convolutional architectures. The attention mechanism is a crucial component of the Transformer as it assigns weights to all the encoded input representations and learns the most important part of the input data. The output of the attention is obtained by taking the weighted sum of the values, and the weights are calculated using the compatibility function of the query with the corresponding key~vaswani2017attention. Numerous attention mechanisms~guo2022attention have been developed in large models. For instance, in natural language processing, self-attention is created to connect various positions in a single sequence for generating a representation of the same sequence. Transformer leverages a mask matrix to provide an attention mechanism based on self-attention, in which the mask matrix specifies which words can ``see'' each other. Transformer is an important structure for PFMs in NLP, CV, and GL areas. For NLP, the Transformer can help solve the long-range dependency issues when processing sequential input data. For example, the GPT-3~brown2020language is a generative model based on the transformer. For CV, the Vision Transformer (ViT)~dosovitskiy2020image is proposed to represent an image to a series of image patches, which is similar to a series of word embeddings. For GL, the Graph Transformer Networks (GTN)~yun2019graph are employed to learn new graph structures and powerful node representations without domain knowledge. Transformers become scalable enough to drive ground-breaking capabilities for PFMs thanks to the transformer structures to achieve higher parallelization. The ViT-22B model~vitransform22b, for instance, has about 22B parameters, and the largest language models can have upwards of 100B parameters (e.g., GPT-3 has 175B and PaLM~chowdhery2022palm has 540B parameters).",
      "origin_cites_number": 8
    },
    {
      "section_title": "Learning Mechanisms for PFMs",
      "level": "2",
      "content": "Deep learning models in CV have been shown a large margin to outperform traditional learning models in most tasks, including the common classification, recognition, detection, and segmentation tasks and the specific matching, tracking, and sequence prediction. These learning methods are not only available in CV, but also in NLP and GL. Supervised Learning Suppose we are given a training dataset $\\bm X$ containing $\\{(x_i, y_i)\\}_{i=1}^{n}$ to represent the original data in training dataset, where $x_i$ denotes the $i$-th training sample, and $y_i$ denotes the corresponding label. The complete network is to learn a function $f(x;\\bm \\theta)$ by minimizing the objective function as follows. equation \\arg\\min_{\\bm \\theta} \\ \\ 1{n}\\sum\\nolimits_{i=1}^{n}{L(f(x_i;\\bm \\theta),y_i)} +\\lambda\\Omega(\\bm\\theta), equation where $L$ and $\\Omega$ represent the predefined loss function and a regularization term, respectively. The function $f$ has a nested form like equation aligned \\bm h_{1}(x_i)&=g(x_i^\\top\\omega_1+b_1), \\\\ \\bm h_{l+1}(x_i)&=g(\\bm h_l(x_i)^\\top\\omega_l+b_l), l = 1, 2, \\cdots, N aligned equation where $l$ is the index of layer in deep learning model and $N$ is the number of layers, which means that $\\bm\\theta = \\{\\bm\\omega_l,b_l, l = 1, 2, \\cdots, N\\}$. Semi-Supervised Learning Assume we are given another unlabelled dataset $\\bm Z = \\{\\bm z_i\\}_{i=1}^{m}$ in addition to the previous dataset with human labels. If we want to utilize both datasets to learn an ideal network, the learning process can be formulated as equation \\arg\\min_{\\bm\\theta} \\ \\ 1{n}\\sum\\nolimits_{i=1}^n{L(f(\\bm x_i;\\bm\\theta),y_i) + \\\\ 1{m}\\sum\\nolimits_{i=1}^m{L^{\\prime}(f^{\\prime}(z_i;\\bm\\theta^{\\prime}),R(\\bm z_i, \\bm X))}} + \\lambda\\Omega(\\bm\\theta), equation where $R$ is a relation function defining the targets for unlabelled data, and then these pseudo-labels are integrated into the end-to-end training process. $f^{\\prime}$ is an encoder to learn a new representation for the original data in the dataset $\\bm Z$. Specifically, if there is no label to any data in the training process, we can learn from the properties inside the data itself via the internal distance or the designed pretext tasks, which are known as unsupervised learning and self-supervised learning(SSL), respectively. The latter is our main focus discussed in detail in Section~sec:Learning by Generation. Weakly-Supervised Learning The weakly-supervised method is the balance between fully-supervised learning and SSL according to the dependence on human labels. The SSL designs special pretext tasks %You'd better define this term in case the beginner may not know what it is to serve as the supervised learning, but the fully supervised learning utilizes existing labels attached to the data. However, both of them can learn good visual features and perform well on specific downstream tasks. Suppose there are inaccurate $K$ labels for the dataset, and any label can be attached to a data sample. Thus, we denote the true label of image ${\\bm x}_i$ as ${\\bm y}_i\\in\\{0,1\\}^K, i=1,2,\\cdots,n$, and any entry of ${\\bm y}_i$ could be $0$ or $1$. Here we need to minimize the total $nK$ loss terms , which are formulated as follows. equation \\arg\\min_{\\bm\\theta} \\ \\ 1{nK}\\sum\\nolimits_{i=1}^n\\sum\\nolimits_{k=1}^K{L(f(\\bm x_i;\\bm\\theta),y_i^k)} + \\lambda\\Omega(\\bm\\theta), equation where $\\left[y_i^1, y_i^2, \\cdots, y_i^K\\right] = {\\bm y}_i$, and $L$ could be a loss function suitable for binomial classification problem. For any entry in ${\\bm y}_i$, computing the loss function of the one-versus-all binomial classification is needed. Self-Supervised Learning SSL utilizes the information in the data itself to learn essential feature representations for different tasks. By applying the self-defined pseudo labels, it can avoid the cost of manually labeling large datasets for PFMs. In NLP, the language models can be trained by predicting masked characters, words, or sentences. Variational autoencoder (VAE) and generative adversarial network (GAN) are two types of generative SSL methods, which are to reconstruct the data itself. Besides, contrastive learning, as a type of discriminative SSL method, is widely applied in CV, NLP, and GL. The main idea of contrastive learning is to learn the prior knowledge distribution of the data itself with the aid of various methods such as data augmentation. In this way, contrastive learning can learn a model that makes similar instances closer in the projected space, and dissimilar instances farther apart in the projected space. Here we show a simple version of contrastive loss: equation L_c(x_i, x_j, \\theta) = m \\| f_\\theta(x_i) - f_\\theta(x_j) \\|^2_2 \\\\ + (1-m)\\max(0, \\epsilon - \\|f_\\theta(x_i) - f_\\theta(x_j)\\|_2)^2 equation where $m$ is 1 if two samples have the same label, otherwise 0, and $\\epsilon$ is the upper bound distance. Reinforcement Learning RL is another type of learning paradigm that models the learning process as a sequential interaction between an agent and an environment, where a RL agent seeks to learn an optimal policy for sequential decision-making problems. Specifically, at each time interaction step $t$, the agent receives a state $s_t$ in a state space $S$, and selects an action $a_t$ from an action space $A$, following a policy $\\pi_{\\theta}(a_t|s_t): A\\rightarrowS$ parameterized by $\\theta$. Then the agent receives a scalar immediate reward $r_t=r(s_t,a_t)$ and the next state $s_{t+1}$ according to the environment dynamics, where $r(s,a)$ is the reward function. For each episode, this process continues until the agent reaches a terminal state. After an episode is finished, the RL agent will restart to begin a new episode. The return for each state is discounted, accumulated reward with the discount factor $\\gamma \\in (0,1]$, $R_t=R(s_t,a_t)= \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}.$ The agent aims to maximize the expectation of such long-term return from each state, equation \\max_{\\theta}{E_{s_t}[R_t|s_t,a_t=\\pi_{\\theta}(s_t)]}. equation",
      "origin_cites_number": 0
    },
    {
      "section_title": "Pretraining Tasks for PFMs",
      "level": "2",
      "content": "Pretraining is an initialization framework, which generally needs to be used in conjunction with fine-tuning downstream tasks. In the scheme of pretraining and finetuning, the parameters of the model are trained on pre-set tasks to capture specific attributes, structure, and community information. The pretrained features can assist downstream tasks, provide sufficient information, and speed up the convergence of the model.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Pretraining Tasks for NLP",
      "level": "3",
      "content": "The pretraining tasks can be divided into five categories according to the learning methods: Mask Language Modeling (MLM), Denoising AutoEncoder (DAE), Replaced Token Detection (RTD), Next Sentence Prediction (NSP), Sentence Order Prediction (SOP). RTD, NSP, and SOP are contrastive learning methods, which assume that the observed samples are more semantically similar than the random samples. Mask Language Modeling (MLM). MLM erases some words randomly in the input sequence and then predicts these erased words during pretraining. Typical examples include BERT~DBLP:conf/naacl/DevlinCLT19 and SpanBERT~DBLP:journals/tacl/JoshiCLWZL20. Denoising AutoEncoder (DAE). DAE is used to add noise to the original corpus and reconstruct the original input using the corpus containing noise. BART~DBLP:conf/acl/LewisLGGMLSZ20 is a representative example. Replaced Token Detection (RTD). RTD is a discriminant task that determines whether the LM has replaced the current token. This task is introduced in ELECTRA~clark2020electra. By training the model to distinguish whether a token has been replaced or not, the model can acquire language knowledge. Next Sentence Prediction (NSP). In order to make the model understand the correlation between the two sentences and capture sentence-level representations, a NSP task is introduced. The PFM inputs two sentences from different documents and checks whether the order of the sentences is correct. A typical example is BERT. Sentence Order Prediction (SOP). Different from NSP, SOP uses two contiguous fragments from a document as positive samples and the exchange order of the two fragments as negative samples. The PFMs can better model the correlation between sentences, such as ALBERT~lan2019albert.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Pretraining Tasks for CV",
      "level": "3",
      "content": "There are many pretraining tasks created for CV to learn the feature space, which is based on SSL. It utilizes pretext tasks that contain human-designed labels, like jigsaw puzzles or the comparison of various patches from images. This enables the generalization of learned representations to a range of downstream tasks. Specific Pretext Task. A pretext task also referred to as a predefined task, is created for the encoder networks to perform during the pretraining phase. The network is trained by predicting the answer to a special pretext task. Based on particular features of the data, pseudo labels are generated for the fictitious task. Then, using guided learning techniques, the encoder networks are trained to solve the pretext task. For example, inpainting aims to pretrain models by predicting the missed center part. Frame Order Learning Task. Learning frame order from videos involves frame processing through time steps, which can serve as the pretraining task for CV. This issue usually relates to completing pretextual exercises that can aid in the acquisition of visual temporal representations. Data Generation Task. The representational capabilities within the generative adversarial networks (GANs) can also be used in the pretraining tasks. Projecting data back into the latent space, as demonstrated by BiGANs~donahue2016adversarial, is helpful for auxiliary supervised discrimination tasks by acting as feature representations. Data Reconstruction Task. Since the images can be divided into patches inspired by the natural language, some pretraining tasks for NLP can also be used in CV, e.g., the autoencoder-based masked prediction. The original image is first divided into a few patches and discrete visual tokens are used to encode each patch. The visual tokens from the masked patches are outputted in the second stage to match the corresponding visual tokens from the fixed tokenizer. Miscellaneous. To train the PFMs in CV, additional pretraining tasks are suggested. For instance, based on contrastive learning, encoder networks are used for pretraining on various data augmentation. The parameters are trained by maximizing the distance between negative pairs (e.g., pairs with different labels) and minimizing the distance between positive pairs (e.g., pairs with the same labels). To pretrain the parameters of the backbone network, the DeepClustering~caron2018deep method divides the representations into various clusters and labels these clusters as supervised signals.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Pretraining Tasks for GL",
      "level": "3",
      "content": "The pre-set tasks in GL are similar to other pretext tasks. However, they can be supervised or unsupervised depending on the design. According to the pretraining purpose and potential motivation in GL, such tasks can be divided into the following categories: Graph Information Completion. This task refers to firstly masking part of the information in the input graph, and then recovering the masked information based on the analysis of the remaining information distribution. Similar tasks also exist in CV and NLP, and their goals are to fill in hidden pixels or words, respectively. Graph Property Prediction. Different from directly modeling the information of the input graph, this task aims to provide a variety of self-supervised signals by mining the potential properties of the input graph. Specifically, on the one hand, it considers node attributes, local substructure, and connectivity information to provide predictive regression tasks; on the other hand, it assigns pseudo-labels to nodes through information such as clusters, structure density, and attribute similarity to provide classification tasks. Graph Consistency Analysis. The goal of this task is to maximize the consistency between samples with similar semantic information in the graph embedding and minimize the agreement between samples with unrelated semantic information. In the actual scenario, it can be divided into consistency analysis of context/self/cross-scale according to different model training strategies. Miscellaneous. Compared with using only one pretext task, some methods have designed some integration mechanisms to incorporate the advantages of multiple pretext tasks into a unified framework. Besides, some graph data in specific fields have unique self-supervised signals with practical significance that can be used for pretraining under targeted design. In summary, the transformer is an important component of the large model architecture, which helps learn the important features and mine intrinsic structure in data. Different learning mechanisms can be used for training PFMs according to the datasets and specific tasks. Especially, SSL is a promising mechanism to learn knowledge embeddings from the data considering the large scale of unlabeled data in various areas. RL provides a new way to fine-tune the PFMs for downstream tasks by optimizing a policy (model) against the reward model. How to design effective and efficient tasks for PFMs to master the knowledge behind the data is an important research topic.",
      "origin_cites_number": 0
    },
    {
      "section_title": "PFMs for Natural Language Processing",
      "level": "1",
      "content": "NLP is a research field that integrates linguistics and computer science. Its main research tasks include part-of-speech tagging, named entity recognition, semantic role labeling, machine translation, question answering, sentiment analysis, text summarization, text classification, relationship extraction, event extraction, etc. The idea of PFM first gained popularity in NLP. Then CV and GL adopt the promising pretraining technology. The PFM trains on a large benchmark dataset and is fine-tuned on the primary task dataset to obtain a model which can solve new similar tasks. It models syntactic and semantic representations of words simultaneously and changes the representation of polysemous words dynamically according to different input contexts. PFM learns a rich knowledge of grammar and semantic reasoning with better results. Numerous PFMs have been proposed in the past few years, as shown in Table~\\ref{text}. In this section, we first introduce word representation learning models including the autoregressive language model (LM), contextual LM, and permuted LM. Then, we present the neural network architectures for the PFM designing method and the masking designing method. Besides, we summarize boosting methods for enhancing model performance, multi-task learning, and different downstream tasks. Finally, we introduce the instruction-aligning methods, e.g. RLHF and Chain-of-Thoughts, which are applied in PFMs, such as ChatGPT, to provide outputs that more closely match human preferences and are less harmful.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Word Representations Methods",
      "level": "2",
      "content": "Many large-scale pretrained models have achieved better performance than humans in question answering, machine reading comprehension, and natural language reasoning, which indicates that the current construction approach of PFMs is practical. The existing pretraining LMs are mainly divided into three branches according to the word representations approach: (1) autoregressive LM, (2) contextual LM, and (3) permuted LM. The word prediction direction and contextual information are the most important factors among these three branches. %These representations approaches are also transferable to the PFMs in CV and GL. Autoregressive Language Model The autoregressive LM predicts the next possible word based on the preceding word or the last possible word based on the succeeding word. It is selected as a feature extractor and text representations are extracted from the former words. Thus, it has better performance in NLG tasks such as text summarization and machine translation. For a sequence, $T=[w_{1}, w_{2}, \\ldots, w_{N}]$, the probability of a given word calculated as follows: equation p\\left(w_{1}, w_{2}, \\ldots, w_{N}\\right)=\\prod_{i=1}^{N} p\\left(w_{i} \\mid w_{1}, w_{2}, \\ldots, w_{i-1}\\right), equation where $i>1$ and $N$ is the length of the input sequence. The GPT~radford2018improving adopts a two-stage method of self-supervised pretraining and supervised fine-tuning and uses stacked Transformer~vaswani2017attention as its decoder. As a follow-up, the OpenAI team continues to expand GPT, proposes the GPT-2~radford2019language and increases the number of stacked Transformer layers to 48 layers. The total number of parameters reached 1.5 billion. GPT-2 also introduces multi-task learning~caruana1997multitask. The GPT-2 has a considerable model capacity and can be adjusted for different task models rather than fine-tuning them. However, GPT-2 also uses an autoregressive LM. Therefore, it improves the performance of the model without increasing the cost dramatically. Due to the lack of contextual modeling ability with a one-way Transformer, the main performance improvement of GPT-2 comes from the combined effect of multi-task pretraining, super-large datasets, and super-large models. Task-based datasets for fine-tuning are still needed for specific downstream tasks. Increasing the training scale of the LM can lead to a significant enhancement in task-independent performance. Hence, GPT-3~brown2020language was developed, which features a model size of 175 billion parameters and is trained with 45 Terabytes of data. This enables it to exhibit good performance without the need for fine-tuning for specific downstream tasks. Contextual Language Model The autoregressive LM only uses the information above or below and cannot use the information above and below at the same time. ELMO~peters2018deep only uses bi-directional Long Short-Term Memory (LSTM), which is a concatenation of two unidirectional LSTMs in backward and forward. The contextual LM predictions are based on contextual words. It uses a Transformer encoder, and the upper and lower layers of the model are all directly connected to each other due to the self-attention mechanism. For a sequence of words $T$, the probability of a given word calculates as follows equation p\\left(w_{1}, w_{2}, \\ldots, w_{N}\\right)=\\prod_{i=1}^{N} p\\left(w_{i} \\mid w_{1}, w_{2}, \\ldots, w_{N}\\right). equation BERT~DBLP:conf/naacl/DevlinCLT19 uses a stacked multi-layer bi-directional Transformer as the basic structure, and WordPiece~wu2016google as a word segmentation method. The model input consists of three parts: word embedding, segment embedding, and position embedding. It uses a bi-directional Transformer as a feature extractor, which offsets the defect of ELMO and GPT. However, the shortcomings of BERT are also not to be ignored. The bidirectional Transformer structure does not eliminate the constraints of the self-encoding model. Its vast number of model parameters are very unfriendly to devices with low computing resources and are challenging to deploy and apply. Furthermore, the hidden language modeling in pretraining will lead to inconsistencies with the input of the model in the fine-tuning stage. Most PFMs need more training tasks and a larger corpus. Aiming at the problem of insufficient training, Liu et al.~DBLP:journals/corr/abs-1907-11692 propose the RoBERTa. It uses a larger batch size and unlabeled data. Furthermore, it trains the model for a longer time, removes the NSP task, and adds long sequence training. In processing text input, different from BERT, Byte Pair Encoding (BPE)~sennrich2015neural is adopted for word segmentation. BPE uses a different mask mode for each input sequence, even if the input sequence is the same. Permuted Language Model The modeling method with a contextual LM can be regarded as the autoencoding model. However, due to the inconsistency in the training stage and fine-tuning stage, the performance of the autoencoding model is poor in the Natural Language Generation (NLG) task. Permuted LM aims to combine the advantages of the autoregressive LM and the autoencoder LM. It improves the defects of the two models to a great extent and can be used as a basic idea for the construction of future pretraining target tasks. For a given input sequence $T=[w_{1},w_{2}... ,w_{N}]$, the formal representation of the target function of the permuted LM is as follows equation \\max _{\\theta} E_{z \\sim Z_{N}}\\left[\\sum_{t=1}^{N} \\log p_{\\theta}\\left(x_{z_{T=t}} \\mid x_{z_{T<t}}\\right)\\right], equation where $\\theta$ is the shared parameter in all permutations, $Z_{N}$ represents the set of all possible permutations of the input sequence $T$, and $z_{T=t}$ and $z_{T<t}$ represents the $t$-th element and the $[1, 2, \\ldots, t-1]$ elements of a permutation $z \\in Z_{N}$. MLM represented by BERT can implement bi-directional coding well. However, MLM uses the mask marking during pretraining but not during fine-tuning, which resulted in inconsistent data during pretraining and fine-tuning. To achieve bi-directional coding and avoid the problems of MLM, the permuted LM is proposed. permuted LM is based on the autoregressive LM, which avoids the influence of inconsistent data. However, unlike traditional autoregressive models, permuted LM no longer models sequences in order. It gives all possible permutations of sequences to maximize the expected logarithmic likelihood of the sequence. In this way, any position can take advantage of contextual information from all positions, making permuted LM implement bidirectional encoding. The most common permuted LM models are XLNET~DBLP:conf/nips/YangDYCSL19 and MPNet~DBLP:conf/nips/Song0QLL20. XLNET is a PFM based on a permuted language modeling approach, which incorporates two crucial techniques from Transformer-XL: relative positional encoding and the segment recurrence mechanism. In contrast, MPNet combines Masked Language Modeling (MLM) and permuted language modeling to predict token dependencies, using auxiliary position information as input to enable the model to view a complete sentence and reduce position differences. These two models represent significant advancements in the field of PFMs.",
      "origin_cites_number": 12
    },
    {
      "section_title": "Model Architecture Designing Methods",
      "level": "2",
      "content": "ELMO adopts a multi-layer RNN structure. Each layer is a bi-directional LSTM structure composed of a forward and backward LM. The maximum likelihood of these two directions is taken as the objective function. Compared with the word vector method, ELMO introduces contextual information and improves the polysemy problem, but ELMO's overall ability to extract linguistic features is weak. figure*[!t] \\centering \\includegraphics[width=\\linewidth]{pictures/picture55.pdf} The architectures of BART~\\cite{DBLP:conf/acl/LewisLGGMLSZ20: generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder). An autoregressive decoder is used to determine the likelihood of the original document after the corrupted document (on the left) has been encoded using a bidirectional model.} figure* The application research of PFMs has two main directions. One is PFMs with fine-tuning (e.g., BERT), and the other one is PFMs with zero/few-shot prompts (e.g., GPT). BERT uses a bi-directional encoder in Transformer to predict which words are masked and determine whether two sentences are contextual. However, the document is encoded bidirectionally and missing tokens are predicted independently, which reduces the generation ability~DBLP:conf/acl/LewisLGGMLSZ20. GPT uses an autoregressive decoder as a feature extractor to predict the next word based on the first few words and solve downstream tasks using fine-tuning, so it is more suitable for text-generation tasks. However, GPT only uses the former words for prediction, which cannot learn bidirectional interaction information. Different from these models, BART~DBLP:conf/acl/LewisLGGMLSZ20 is a noise-reducing autoencoder built by seq2seq model adopting the encoder-decoder structure, as shown in Fig.~ELMO-GPT-BART from ~DBLP:conf/acl/LewisLGGMLSZ20. Pretraining mainly includes using noise to destroy text and using the seq2seq model to rebuild the original text. The encoding layer adopts a bi-directional Transformer. It adopts five modes of adding noise: (1) single word mask; (2) word deletion; (3) span mask; (4) sentence rearrangement; (5) document rearrangement. In the encoder part, the sequence has been masked before inputting it into the encoder. Then, the decoder restores the original sequence according to the encoding representation output by the encoder and the sequence that has not been masked. The addition of a series of noise patterns makes the performance of BART in sequence generation and natural language reasoning tasks significantly improved.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Masking Designing Methods",
      "level": "2",
      "content": "The attention mechanism first aggregates essential words into sentence vectors, and vital sentence vectors into text vectors, which allows the model to pay different attention to different inputs~li2022survey. For BERT, as a bidirectional encoding LM, any two words in an input sentence can see each other. However, it hinders the ability of BERT model to learn NLG tasks. Joshi et al.~DBLP:journals/tacl/JoshiCLWZL20 propose SpanBERT based on RoBERTa, which adopts the idea of dynamic masking and single segment pretraining, as shown in Fig.~SpanBERT from ~DBLP:journals/tacl/JoshiCLWZL20. The span mask and the Span Boundary Objective (SBO) are also proposed to mask words of a certain length. The target task of the span-boundary is to restore all the masked span (tokens) by the observed tokens at both ends. The training stage uses the dynamic mask strategy proposed in the RoBERTa, instead of the mask during the data preprocessing. Unlike BERT, SpanBERT randomly covers up a continuous text and adds the SBO training target. It predicts the span using the token closest to the span boundary and eliminates the NSP pretraining task. The BERT and GPT can only separate the training encoder and decoder without joint training in the NLG task. Song et al.~song2019mass propose the masked seq2seq pretraining model MASS. In the training stage, the input sequence of the encoder is randomly masked as a continuous segment of length $k$. The masked segment will be recovered through the MASS decoder. UniLM~dong2019unified completes the learning of the NLG model by designing a different mask for two sentences in the input data. For the first sentence, UniLM uses the same structure as the Transformer encoder making each word notice its preceding and following words. For the second sentence, each word can only notice all the words in the first sentence and the preceding words in the current sentence. Thus, the first and second sentences of the model input form the classic seq2seq pattern. figure*[!t] \\centering \\includegraphics[width=\\linewidth]{pictures/picture66.pdf} The architecture of SpanBERT~\\cite{DBLP:journals/tacl/JoshiCLWZL20.} figure*",
      "origin_cites_number": 6
    },
    {
      "section_title": "Boosting Methods",
      "level": "2",
      "content": "Boosting on Model Performance Most of the popular pretraining models need lots of pretraining data, which imposes huge requirements on the hardware, making it challenging to retrain, and only fine-tuning can be done to the model. To solve these problems, some models appear. For example, ERNIE Tiny released by Baidu is a miniaturized ERNIE~sun2019ernie, that reduces the number of layers and increases the prediction speed by 4.3 times with a slight decrease in accuracy. Lan et al. propose the ALBERT~lan2019albert to reduce memory consumption and training speed. However, it is undeniable that no matter what kind of compression is done for these large-scale models, the performance of the models in these tasks will deteriorate sharply. It requires paying attention to the efficient representation of high-level semantic and grammatical information and lossless compression in future works. By using word-embedded parameter factorization and hidden parameter sharing between layers, ALBERT significantly reduces the number of parameters of the model without performance loss. It proposes the training task of SOP, which predicts the order of the two sentences to improve the performance. Boosting for Multi-task Learning ERNIE(Baidu)~sun2019ernie is mainly composed of two parts, the Transformer encoder and task embedding. In the Transformer encoder, the self-attention mechanism is used to capture the context information of each token and generate context representation embedding. Task embedding is a technique that applies different characteristics to a task. ERNIE 2.0~sun2020ernie introduces multi-task learning to realize the pretraining of lexical, grammar, and semantics. ERNIE 2.0 uses seven different pretraining tasks, covering three aspects: word level, sentence level, and semantic level. It uses continual learning, making the knowledge in the previous training task retained and enabling the model to acquire long-distance memory. It uses a Transformer encoder and introduces task embedding, enabling the model to distinguish different tasks in the continual learning process. UniLM~dong2019unified uses three pretraining tasks: unidirectional LM, bidirectional LM, and encoder-decoder LM. It can simultaneously complete three kinds of target tasks in the pretraining stage through the self-attention layer mask mechanism. In the training stage, UniLM adopts the small-segment mask strategy proposed by SpanBERT, and the loss function is composed of the loss functions of the above three pretraining tasks. To maintain the contribution consistency on all loss functions, the three pretraining tasks are trained simultaneously. Modeling and parameter sharing of multiple tasks make LMs achieve good generalization ability in Natural Language Understanding (NLU) and NLG tasks. Boosting for Different Downstream Tasks The pretraining models tend to be large-sized, so how to match different downstream tasks is equally important. Some pretraining models that are trained on specialized corpora have appeared~DBLP:journals/taslp/CuiCLQY21, DBLP:conf/emnlp/DiaoBSZW20, tsai2019small. Cui et al.~DBLP:journals/taslp/CuiCLQY21 propose the BERT-whole word masking model (BERT-WWM). They directly use BERT in Chinese to be masked randomly according to the original MLM training, resulting in the loss of semantic information. Since there is no explicit language boundary in Chinese, it is easy to lose significant meaning. ZEN~DBLP:conf/emnlp/DiaoBSZW20 is a text encoder based on BERT, which adopts N-gram to enhance performance and effectively integrates considerable granular text information with fast convergence speed and good performance. Tsai et al.~tsai2019small propose an oriented multilingual sequence labeling model for sequence labeling tasks. The knowledge distillation method is adopted to achieve better performance in the two tasks: part of speech labeling and morphological attribute prediction for multiple low-resource languages. The inference time is shortened by 27 times. Examples: ChatGPT and Bard figure* \\centering \\includegraphics[width=1\\linewidth]{figures/chatgpt_RL.png} Boosting GPT-3.5 to ChatGPT using Reinforcement Learning from Human Feedback. figure* As shown in Fig.~fig:chatgpt_RL, ChatGPT is fine-tuned based on the PFM GPT-3.5 using RLHF. ChatGPT uses a different data collection setup compared to InstructGPT. First, a large dataset with prompts and the desired output behaviors is collected. The dataset is used to fine-tune GPT-3.5 with supervised learning. Second, given the fine-tuned model and a prompt, the model will generate several model outputs. A labeler gives the desired score and ranks the output to compose a comparison dataset, which is used to train the reward model. Finally, the fine-tuned model (ChatGPT) is optimized against the reward model using the Proximal Policy Optimization (PPO)schulman2017proximal RL algorithm. Another experimental conversational PFM, the Bard~\\url{https://blog.google/technology/ai/bard-google-ai-search-updates/}, is developed by Google. Bard is based on the LM for Dialogue Applications (LaMDA). LaMDA~thoppilan2022lamda is built upon the Transformer, which is pretrained on 1.56T words of dialog data and web text. Safety and factual grounding are two main challenges for conversational AI, LaMDA applies the approaches that fine-tuning with high-quality annotated data and external knowledge sources to improve model performance. table*[!htbp] \\centering Summary of PFMs in NLP. The pretraining task includes language model (LM), masked LM (MLM), permuted LM (PLM), denoising autoencoder (DAE), knowledge graphs (KG), and knowledge embedding (KE). \\textwidth{!}{ tabular{llllllll} \\hline Year & Conference & Model & Architecture & Embedding & Training method & Code \\\\ \\hline 2013 & NeurIPS & Skip-Gram~mikolov2013distributed & Word2Vec & Probabilistic & - & https://github.com/tensorflow/models{https://github.com/.../models} \\\\ \\hline 2014 & EMNLP & GloVe~DBLP:conf/emnlp/PenningtonSM14 & Word2Vec & Probabilistic & - & - \\\\ \\hline 2015 & NeurIPS & LM-LSTM~dai2015semi & LSTM & Probabilistic & LM & https://github.com/stanfordnlp/GloVe{https://github.com/.../GloVe} \\\\ \\hline 2016 & IJCAI & Shared LSTM~liu2016recurrent & LSTM & Probabilistic & LM & https://github.com/tensorflow/models/tree/master/research/adversarial\\_text{https://github.com/.../adversarial\\_text} \\\\ \\hline 2017 & TACL & FastText~bojanowski2017enriching & Word2Vec & Probabilistic & - & https://github.com/facebookresearch/fastText{https://github.com/.../fastText} \\\\ \\hline 2017 & NeurIPS & CoVe~mccann2017learned & LSTM+Seq2Seq & Probabilistic & - & https://github.com/salesforce/cove{https://github.com/.../cove} \\\\ \\hline 2018 & NAACL-HLT & ELMO~peters2018deep & LSTM & Contextual & LM & https://allennlp.org/elmo{https://allennlp.org/elmo} \\\\ \\hline 2018 & NAACL-HLT & BERT~DBLP:conf/naacl/DevlinCLT19 & Transformer Encoder & Contextual & MLM & https://github.com/google-research/bert{https://github.com/.../bert} \\\\ \\hline 2018 & & OpenAI GPT~radford2018improving & Transformer Decoder & Autoregressive & LM & https://github.com/openai/finetune-transformer-lm{https://github.com/...transformer-lm} \\\\ \\hline 2019 & ACL & ERNIE(THU) & Transformer Encoder & Contextual & MLM & https://github.com/PaddlePaddle/ERNIE{https://github.com/.../ERNIE} \\\\ \\hline 2019 & ACL & Transformer-XL~dai2019transformer & Transformer-XL & Contextual & - & https://github.com/kimiyoung/transformer-xl{https://github.com/.../transformer-xl} \\\\ \\hline 2019 & ICLR & InfoWord~kong2019mutual & Transformer Encoder & Contextual & MLM & - \\\\ \\hline 2019 & ICLR & StructBERT~wang2019structbert & Transformer Encoder & Contextual & MLM & - \\\\ \\hline 2019 & ICLR & ALBERT ~lan2019albert & Transformer Encoder & Contextual & MLM & https://github.com/google-research/ALBERT{https://github.com/.../ALBERT} \\\\ \\hline 2019 & ICLR & WKLM~xiong2019pretrained & Transformer Encoder & Contextual & MLM & - \\\\ \\hline 2019 & ICML & MASS~song2019mass & Transformer & Contextual & MLM(Seq2Seq) & https://github.com/microsoft/MASS{https://github.com/.../MASS} \\\\ \\hline 2019 & EMNLP-IJCNLP & KnowBERT~peters2019knowledge & Transformer Encoder & Contextual & MLM & https://github.com/allenai/kb{https://github.com/.../kb} \\\\ \\hline 2019 & EMNLP-IJCNLP & Unicoder~huang2019unicoder & Transformer Encoder & Contextual & MLM+TLM & - \\\\ \\hline 2019 & EMNLP-IJCNLP & MultiFit~eisenschlos2019multifit & QRNN & Probabilistic & LM & https://github.com/n-waves/multifit{https://github.com/.../multifit} \\\\ \\hline 2019 & EMNLP-IJCNLP & SciBERT~beltagy2019scibert & Transformer Encoder & Contextual & MLM & https://github.com/allenai/scibert{https://github.com/.../scibert} \\\\ \\hline 2019 & EMNLP-IJCNLP & BERT-PKD~sun2019patient & Transformer Encoder & Contextual & MLM & https://github.com/intersun/PKD-for-BERT-Model-Compression{https://github.com/...Compression} \\\\ \\hline 2019 & NeurIPS & Xlnet~DBLP:conf/nips/YangDYCSL19 & Transformer-XL Encoder & Permutation & PLM & https://github.com/zihangdai/xlnet{https://github.com/.../xlnet} \\\\ \\hline 2019 & NeurIPS & UNILM~dong2019unified & LSTM + Transformer & Contextual & LM + MLM & https://github.com/microsoft/unilm{https://github.com/.../unilm} \\\\ \\hline 2019 & NeurIPS & XLM~lample2019cross & Transformer Encoder & Contextual & MLM+CLM+TLM & https://github.com/facebookresearch/XLM{https://github.com/.../XLM} \\\\ \\hline 2019 & OpenAI Blog & GPT-2~radford2019language & Transformer Decoder & Autoregressive & LM & https://github.com/openai/gpt-2{https://github.com/.../gpt-2} \\\\ \\hline 2019 & arXiv & RoBERTa~DBLP:journals/corr/abs-1907-11692 & Transformer Encoder & Contextual & MLM & https://github.com/pytorch/fairseq{https://github.com/.../fairseq} \\\\ \\hline 2019 & arXiv & ERNIE(Baidu)~sun2019ernie & Transformer Encoder & Contextual & MLM+DLM & https://github.com/PaddlePaddle/ERNIE{https://github.com/.../ERNIE} \\\\ \\hline 2019 & EMC2@NeurIPS & Q8BERT~DBLP:conf/nips/ZafrirBIW19 & Transformer Encoder & Contextual & MLM & https://github.com/IntelLabs/nlp-architect/blob/master/nlp\\_architect/models/transformers/quantized\\_bert.py{https://github.com/.../quantized\\_bert.py} \\\\ \\hline 2019 & arXiv & DistilBERT~sanh2019distilbert & Transformer Encoder & Contextual & MLM & https://github.com/huggingface/transformers/tree/master/examples/research\\_projects/distillation{https://github.com/.../distillation} \\\\ \\hline 2020 & ACL & fastBERT~liu2020fastbert & Transformer Encoder & Contextual & MLM & https://github.com/autoliuweijie/FastBERT{https://github.com/.../FastBERT} \\\\ \\hline 2020 & ACL & SpanBERT~DBLP:journals/tacl/JoshiCLWZL20 & Transformer Encoder & Contextual & MLM & https://github.com/facebookresearch/SpanBERT{https://github.com/.../SpanBERT} \\\\ \\hline 2020 & ACL & BART~DBLP:conf/acl/LewisLGGMLSZ20 & Transformer & En: Contextual & DAE & https://github.com/huggingface/transformers{https://github.com/.../transformers} \\\\ & & & & De: Autoregressive & & \\\\ \\hline 2020 & ACL & CamemBERT~DBLP:conf/acl/MartinMSDRCSS20 & Transformer Encoder & Contextual & MLM(WWM) & https://camembert-model.fr{https://camembert-model.fr} \\\\ \\hline 2020 & ACL & XLM-R~DBLP:conf/acl/ConneauKGCWGGOZ20 & Transformer Encoder & Contextual & MLM & https://github.com/facebookresearch/XLM{https://github.com/.../XLM} \\\\ \\hline 2020 & ICLR & Reformer~DBLP:conf/iclr/KitaevKL20 & Reformer & Permutation & - & https://github.com/google/trax/tree/master/trax/models/reformer{https://github.com/.../reformer} \\\\ \\hline 2020 & ICLR & ELECTRA~clark2020electra & Transformer Encoder & Contextual & MLM & https://github.com/google-research/electra{https://github.com/.../electra} \\\\ \\hline 2020 & AAAI & Q-BERT~shen2020q & Transformer Encoder & Contextual & MLM & - \\\\ \\hline 2020 & AAAI & XNLG~chi2020cross & Transformer & Contextual & MLM+DAE & https://github.com/CZWin32768/xnlg{https://github.com/.../xnlg} \\\\ \\hline 2020 & AAAI & K-BERT~liu2020k & Transformer Encoder & Contextual & MLM & https://github.com/autoliuweijie/K-BERT{https://github.com/.../K-BERT} \\\\ \\hline 2020 & AAAI & ERNIE 2.0~sun2020ernie & Transformer Encoder & Contextual & MLM & https://github.com/PaddlePaddle/ERNIE{https://github.com/.../ERNIE} \\\\ \\hline 2020 & NeurIPS & GPT-3~brown2020language & Transformer Decoder & Autoregressive & LM & https://github.com/openai/gpt-3{https://github.com/.../gpt-3} \\\\ \\hline 2020 & NeurIPS & MPNet~DBLP:conf/nips/Song0QLL20 & Transformer Encoder & Permutation & MLM+PLM & https://github.com/microsoft/MPNet{https://github.com/.../MPNet} \\\\ \\hline 2020 & NeurIPS & ConvBERT~DBLP:conf/nips/JiangYZCFY20 & Mixed Attention & Contextual & - & https://github.com/yitu-opensource/ConvBert{https://github.com/.../ConvBert} \\\\ \\hline 2020 & NeurIPS & MiniLM~DBLP:conf/nips/WangW0B0020 & Transformer Encoder & Contextual & MLM & https://github.com/microsoft/unilm/tree/master/minilm{https://github.com/.../minilm} \\\\ \\hline 2020 & TACL & mBART~DBLP:journals/tacl/LiuGGLEGLZ20 & Transformer & Contextual & DAE & https://github.com/pytorch/fairseq/tree/master/examples/mbart{https://github.com/.../mbart} \\\\ \\hline 2020 & COLING & CoLAKE~DBLP:conf/coling/SunSQGHHZ20 & Transformer Encoder & Contextual & MLM+KE & https://github.com/txsun1997/CoLAKE{https://github.com/.../CoLAKE} \\\\ \\hline 2020 & LREC & FlauBERT~DBLP:conf/lrec/LeVFSCLACBS20 & Transformer Encoder & Contextual & MLM & https://github.com/getalp/Flaubert{https://github.com/.../Flaubert} \\\\ \\hline 2020 & EMNLP & GLM~DBLP:conf/emnlp/ShenMHLTC20 & Transformer Encoder & Contextual & MLM+KG & https://github.com/THUDM/GLM{https://github.com/.../GLM} \\\\ \\hline 2020 & EMNLP (Findings) & TinyBERT~DBLP:conf/emnlp/JiaoYSJCL0L20 & Transformer & Contextual & MLM & https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT{https://github.com/.../TinyBERT} \\\\ \\hline 2020 & EMNLP (Findings) & RobBERT~DBLP:conf/emnlp/DelobelleWB20 & Transformer Encoder & Contextual & MLM & https://github.com/iPieter/RobBERT{https://github.com/.../RobBERT} \\\\ \\hline 2020 & EMNLP (Findings) & ZEN~DBLP:conf/emnlp/DiaoBSZW20 & Transformer Encoder & Contextual & MLM & https://github.com/sinovation/ZEN{https://github.com/.../ZEN} \\\\ \\hline 2020 & EMNLP (Findings) & BERT-MK~DBLP:conf/emnlp/HeZXJLYX20 & KG-Transformer Encoder & Contextual & MLM & - \\\\ \\hline 2020 & RepL4NLP@ACL & CompressingBERT~DBLP:conf/rep4nlp/GordonDA20 & Transformer Encoder & Contextual & MLM(Pruning) & https://github.com/mitchellgordon95/bert-prune{https://github.com/.../bert-prune} \\\\ \\hline 2020 & JMLR & T5~DBLP:journals/jmlr/RaffelSRLNMZLL20 & Transformer & Contextual & MLM(Seq2Seq) & https://github.com/google-research/text-to-text-transfer-transformer{https://github.com/...transformer} \\\\ \\hline 2021 & T-ASL & BERT-wwm-Chinese~DBLP:journals/taslp/CuiCLQY21 & Transformer Encoder & Contextual & MLM & https://github.com/ymcui/Chinese-BERT-wwm{https://github.com/...BERT-wwm} \\\\ \\hline 2021 & EACL & PET~DBLP:conf/eacl/SchickS21 & Transformer Encoder & Contextual & MLM & https://github.com/timoschick/pet{https://github.com/.../pet} \\\\ \\hline 2021 & TACL & KEPLER~DBLP:journals/tacl/WangGZZLLT21 & Transformer Encoder & Contextual & MLM+KE & https://github.com/THU-KEG/KEPLER{https://github.com/.../KEPLER} \\\\ \\hline 2021 & EMNLP & SimCSE~DBLP:journals/corr/abs-2104-08821 & Transformer Encoder & Contextual & MLM+KE & https://github.com/princeton-nlp/SimCSE{https://github.com/.../SimCSE} \\\\ \\hline 2021 & ICML & GLaM~du2022glam & Transformer & Autoregressive & LM & - \\\\ \\hline 2021 & arXiv & XLM-E~chi2021xlm & Transformer & Contextual & MLM & \\\\ \\hline 2021 & arXiv & T0~sanh2021multitask & Transformer & Contextual & MLM & https://github.com/bigscience-workshop/t-zero{https://github.com/.../T0} \\\\ \\hline 2021 & arXiv & Gopher~rae2021scaling & Transformer & Autoregressive & LM & - \\\\ \\hline 2022 & arXiv & MT-NLG~smith2022using & Transformer & Contextual & MLM & - \\\\ \\hline 2022 & arXiv & LaMDA~thoppilan2022lamda & Transformer Decoder & Autoregressive & LM & https://github.com/conceptofmind/LaMDA-rlhf-pytorch{https://github.com/.../LaMDA} \\\\ \\hline 2022 & arXiv & Chinchilla~hoffmann2022training & Transformer & Autoregressive & LM & - \\\\ \\hline 2022 & arXiv & PaLM~chowdhery2022palm & Transformer & Autoregressive & LM & https://github.com/lucidrains/PaLM-pytorch{https://github.com/.../PaLM} \\\\ \\hline 2022 & arXiv & OPT~zhang2022opt & Transformer Decoder & Autoregressive & LM & https://github.com/facebookresearch/metaseq{https://github.com/.../MetaSeq} \\\\ \\hline tabular } table*",
      "origin_cites_number": 77
    },
    {
      "section_title": "Instruction-Aligning Methods",
      "level": "2",
      "content": "Instruction-aligning methods aim to let the LM follow human intents and generate meaningful outputs. The general approach is fine-tuning the pretrained LM with high-quality corpus in a supervised manner. To further improve the usefulness and harmlessness of LMs, some works introduce RL into the fine-tuning procedure so that LMs could revise their responses according to human or AI feedback. Both supervised and RL approaches can leverage chain-of-thought weichain style reasoning to improve the human-judged performance and transparency of AI decision-making. Supervised Fine-Tuning (SFT) SFT is a well-established technique to unlock knowledge and apply it to specific real-world, even unseen tasks. The template for SFT is composed of input-output pairs and an instruction weifinetuned. For example, given the instruction ``Translate this sentence to Spanish:'' and an input ``The new office building was built in less than three months.'', we want the LM to generate the target ``El nuevo edificio de oficinas se construy en tres meses.''. The template is commonly humanmade including unnatural instructions honovich2022unnatural and natural instructions wang2022super, mishra2022cross, or bootstrap based on a seed corpus wang2022self. Ethical and social risks of harm from LMs are significant concerns in SFT weidinger2021ethical. LaMDA, the largest LM to date, thus relies on crowdworker annotated data for providing a safety assessment of any generated LaMDA response in three conversation categories: natural, sensitive, and adversarial. The list of rules serves further safety fine-tuning and evaluation purposes. Reinforcement Learning from Feedback RL has been applied to enhance various models in NLP tasks such as machine translation kiegeland2021revisiting, summarization stiennon2020learning, dialogue generation jaques2020human, image captioning rennie2017self, question generation pang2020text, text-games hausknecht2020interactive, and more snell2022offline, lu2022quark, uc2022survey. RL is a helpful method for optimizing non-differentiable objectives in language generation tasks by treating them as sequential decision-making problems. However, there is a risk of overfitting to metrics that use neural networks, leading to nonsensical samples that score well on the metrics ramamurthy2022reinforcement. RL is also used to align LMs with human preferences wu2021recursively, nakano2021webgpt, glaese2022improving. InstructGPT proposes to fine-tune large models with PPO against a trained reward model to align LMs with human preference ouyang2022training, which is the same method applied by ChatGPT named RLHF. Specifically, the reward model is trained with comparison data of human labelers' manual rankings of outputs. For each of them, the reward model or machine labeler calculates a reward, which is used to update the LM using PPO. More details are illustrated in Fig. fig:chatgpt_RL. One of the recent breakthroughs in PFM technology is GPT-4~openai2023gpt4, which follows a pretraining approach to predict the subsequent token in a document and then undergoes RLHF fine-tuning. %Although the disparity between GPT-3.5 and GPT-4 is not significant, the post-training phase utilizing RLHF, which is the same as with GPT-3.5, makes a considerable difference. As the task complexity increases, GPT-4 outperforms GPT-3.5 in terms of reliability, creativity, and capability to handle more nuanced instructions. Sparrow glaese2022improving, developed by DeepMind, also utilizes RLHF that reduces the risk of unsafe and inappropriate answers. Despite some promising results using RLHF by incorporating fluency, progress in this field is impeded by a lack of publicly available benchmarks and implementation resources, resulting in a perception that RL is a difficult approach for NLP. Therefore, an open-source library named RL4LMs~ramamurthy2022reinforcement is introduced recently, which consists of building blocks for fine-tuning and evaluating RL algorithms on LM-based generation. Besides human feedback, one of the latest dialogue agents -- Claude favors Constitutional AI bai2022constitutional where the reward model is learned via RL from AI Feedback (RLAIF). Both the critiques and the AI feedback are steered by a small set of principles drawn from a constitution, the specification of a short list of principles or instructions, which is the only thing provided by humans in Claude. The AI feedback focuses on controlling the outputs to be less harmful by explaining its objections to dangerous queries. Chain-of-Thoughts Chain-of-thought (CoT) prompting is a technique for improving the reasoning ability of LLMs by prompting them to generate a series of intermediate steps that lead to the final answer of a multi-step problem. The CoT is a series of intermediate reasoning steps, which can significantly improve the ability of LLMs to perform complex reasoning weichain, chung2022scaling, kojima2022large. Besides, fine-tuning with CoT shows slightly more harmless compared to without CoT bai2022constitutional. CoT prompting is an emergent property of model scale, meaning it works better with larger and more powerful language models. It is also possible to fine-tune models on CoT reasoning datasets to enhance this capability further and stimulate better interpretability. In a CoT prompting experiment, a prompt is provided to the model that outlines a multi-step problem. The prompt might pose a question such as ``After selling 30 out of his 100 chickens and 10 out of his 20 pigs, how many animals does a farmer have left?'' The model then generates a sequence of intermediate reasoning steps, for example, ``The farmer has 100-30=70 chickens remaining'' and ``The farmer has 20-10=10 pigs remaining,'' before generating the final answer, such as ``The farmer has 70+10=80 animals remaining.'' CoT prompting has demonstrated its efficacy in improving the performance of LLMs on various reasoning tasks, such as arithmetic, symbolic reasoning, and common sense. It is a promising technique that can enhance the ability of language models to reason about complicated problems.",
      "origin_cites_number": 22
    },
    {
      "section_title": "Summary",
      "level": "2",
      "content": "The neural probabilistic LM uses a neural network to estimate the parameters of the probabilistic LM, which reduces the size of the model parameters while enlarging the number of context windows. With the help of a neural network, the LM does not need to improve the smoothing algorithm to alleviate the performance bottleneck continuously. Since the training target is unsupervised, a corpus with a large amount of data is enough for training. The negative sampling technique in the training process provides a new idea for the follow-up study of the target task in the LM. Furthermore, the neural probabilistic LM promotes the further development of downstream task research because of its good representation capability and training efficiency. After the pretraining LM, especially the BERT model, is proposed, the research in language modeling has entered a new phase. The bidirectional LM, the hidden LM, and the sorted LM adopted by the bidirectional LM have successfully modeled the grammatical and semantic information in natural language at a deeper level. ChatGPT is another milestone work in PFMs using RL. %According to the benchmark test results, t The presentation ability of PFMs is qualitatively better than that of the neural probabilistic LM. It even exceeds that of humans in some tasks.",
      "origin_cites_number": 0
    },
    {
      "section_title": "PFMs for Computer Vision",
      "level": "1",
      "content": "With the popularity of PFM used in NLP, it motivates researchers to start exploring PFM in CV. The term ``pretraining'' has not been clearly defined within the realm of deep learning research in CV. This word is first used in convolution-based networks when we adjust the parameters on a more general dataset such as ImageNet, which can make other tasks train to start with a warm-up initialization and thus converge with faster speed. In contrast to early CNN-based transfer learning techniques that rely on pretrained datasets with supervised signals, our examination of PFM centers on SSL which utilizes human-designed labels, such as Jigsaw puzzles, or the comparison of different patches from images as pretext tasks. This allows for learned representations to be generalized to various downstream tasks, including classification, detection, recognition, segmentation, etc. However, it is costly to rely on data annotations when the learning tasks become more complicated, making the labeling process more arduous and time-consuming than the actual learning. This is where SSL is urgently needed and how it can further fuel the progress of deep learning methods. To reduce the dependency on data labeling, unlabeled data are trained with self-supervision by matching, contrasting, or generating in SSL. figure* \\centering \\includegraphics[width=1\\linewidth]{figures/pipline_zip.pdf} The general pipeline for SSL. The top part represents the pretraining, and the bottom stream obtains transferred parameters from above to learn downstream supervised tasks. figure* The general pipeline of SSL is shown in Fig. fig:pipline. During the pretraining stage, a pretext task is designed for the encoder networks to solve. The artificial labels for this pretext task are automatically generated based on specific attributes of the data, such as image patches from the same origin being labeled as ``positive'' and those from different origins as ``negative''. Then, the encoder networks are trained to solve the pretext task by supervised learning methods. Since shallow layers extract fine-grained details such as edges, angles, and textures, while deeper layers capture task-related high-level features such as semantic information or image contents, learned encoders on pretext tasks can be transferred to downstream supervised tasks. During this stage, the parameters of the backbone are fixed, and only a simple classifier, such as a two-layer Multi-Layer Perceptron (MLP), needs to be learned. Considering the limited workload in the downstream training stage, this learning process is commonly referred to as fine-tuning. In summary, the representations learned during the pretraining stage in SSL can be reused on other downstream tasks and achieve comparable results. In this section, we introduce different tasks for pretraining PFMs in CV. The PFMs can be trained by specific pretext tasks, frame order, generation, reconstruction, memory bank, sharing, clustering and so on. We summarize the PFMs proposed in CV in Table~\\ref{tab:pretraining model for image}.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Learning by Specific Pretext Task",
      "level": "2",
      "content": "In the early stage of unsupervised learning, the network is trained by designing a special pretext task and predicting the answer to this task. Dosovitskiy et al.~dosovitskiy2014discriminative, DFB16 pretrain the Exemplar CNN to discriminate the different patches from the unlabelled data. The experiments prove the designs can learn useful representations transferred to the standard recognition assignments. In the method based on context prediction~doersch2015unsupervised, a handcrafted supervised signal about the position information serves as the label for the pair classification. Inpainting~pathak2016context aims to pretrain models by predicting the missed center part. Because inpainting is a semantic-based prediction, another decoder is linked to the context encoder in this manner. Furthermore, the standard pixel-by-pixel reconstruction process of the decoder can be transferred to any other downstream inpainting tasks. Specifically, Colorization~zhang2016colorful is a method that evaluates how colorization as a pretext task can help to learn semantic representation for downstream tasks. It is also known as the cross-channel encoding since different image channels serve as input and the output is discriminated. Similarly, Split-Brain Autoencoder~zhang2017split also learns representations in a self-supervised way by forcing the network to solve cross-channel prediction tasks. Jigsaw~noroozi2016unsupervised is proposed to pretrain the designed Context-Free Network (CFN) in a self-supervised manner by first designing the Jigsaw puzzle as a pretext task. Completing Damaged Jigsaw Puzzles (CDJP)~kim2018learning learns image representation by complicating pretext tasks furthermore, in which puzzles miss one piece and the other pieces contain incomplete color. Following the idea of designing efficient and effective pretext tasks, Noroozi et al.~noroozi2017representation use counting visual primitives as a special pretext task and outperform previous SOTA models on regular benchmarks. NAT~bojanowski2017unsupervised learns representation by aligning the output of backbone CNN to low-dimensional noise. RotNet~gidaris2018unsupervised is designed to predict different rotations of images. figure*[!htp] \\centering \\includegraphics[width=0.9\\linewidth]{figures/CPC_zip.pdf} Contrastive Predictive Coding~\\cite{oord2018representation. The input sequence can represent both images and videos.} figure*",
      "origin_cites_number": 11
    },
    {
      "section_title": "Learning by Frame Order",
      "level": "2",
      "content": "The learning of sequence data such as videos always involves frame processing through time steps. This problem often connects with solving pretext tasks that can help to learn visual temporal representations. Contrastive Predictive Coding (CPC)~oord2018representation is the first model to learn data representations by predicting the future in latent space. This model can be fed with data in any modalities, like speech, images, text, etc. The components of CPC are shown in Fig. fig:cpc from~oord2018representation, where the $x_t$ represents the input sequence of observations, $z_t$ is a sequence of latent representations after the encoder $g_{enc}$, and $c_t$ is a context latent representation that summarizes all the latent sequence $z_{\\le t}$ after an autoregressive model $g_{ar}$. Unlike the traditional model predicts future frames $x_{t+k}$ by a generative model $p_k(x_{t+k}|c_t)$, CPC models a \"density ratio\" $f_k$ to represent the mutual information between the context latent representation $c_t$ and future frame $x_{t+k}$: equation f_k(x_{t+k},c_t)\\propto p(x_{t+k}|c_t) / x_{t+k}. equation After the encoding of recurrent neural networks, $z_t$ and $c_t$ can both be chosen for the downstream tasks as needed. The encoder and autoregressive model are trained by InfoNCE~oord2018representation as follows equation L=-E_X[\\log f_k(x_{t+k},c_t) / \\sum\\nolimits_{x_j\\in X}f_k(x_j,c_t)], equation where $X$ denotes the training dataset containing both positive and negative samples. The density ratio $f_k$ can be estimated by optimizing $L$. CPC v2 revisits and improves CPC~henaff2020data by pretraining on unsupervised representations, and its representation generality can be transferred to data-efficient downstream tasks.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Learning by Generation",
      "level": "2",
      "content": "Although many existing applications are popular after the development of the GAN-based approach, the representation abilities inside the GANs are not entirely exploited due to the absence of a feature encoder. Thus, Bidirectional Generative Adversarial Networks (BiGANs)~donahue2016adversarial is proposed to project data back into the latent space, which is useful for auxiliary supervised discrimination tasks via serving as feature representations. Based on BiGANs, BigBiGAN~DBLP:conf/nips/DonahueS19 first achieves the SOTA in unsupervised representation learning on ImageNet by adding an encoder and modifying the discriminator. As shown in Fig. fig:bigbigan from~DBLP:conf/nips/DonahueS19, the traditional components of GANs (encoder $E$ and generator $G$) are used to produce data-latent pairs, denoted as $(x\\sim P_{x},\\textbf{z}\\simE(x))$ and $(\\textbf{x}\\simG(z),z\\sim P_{z})$. The final loss $\\ell$ is defined as the sum of data-specific term $s_{x},s_{z}$ and data-joint term $s_{xz}$. The introduced discriminator $D$ (Adversarially Learned Inference (ALI)~dumoulin2016adversarially, or BiGAN~donahue2016adversarial) learns to discriminate between pairs from the raw data, latent distribution and encoded vector. figure*[t] \\centering \\includegraphics[width=0.8\\linewidth]{figures/BigBiGAN_zip.pdf} The structure of the BigBiGAN framework~\\cite{DBLP:conf/nips/DonahueS19.} figure*",
      "origin_cites_number": 6
    },
    {
      "section_title": "Learning by Reconstruction",
      "level": "2",
      "content": "The iGPT chen2020generative and ViT dosovitskiy2020image models have demonstrated the feasibility of adapting the pretext task of masked prediction using auto-encoder from language to image data. BEiT bao2021beit is the first to demonstrate that autoencoder-based masked prediction can outperform DINO caron2021emerging, a conventional SOTA method without pretraining techniques. Specifically, BEiT consists of two stages: token embedding with discrete variational autoencoder (dVAE) ramesh2021zero, and tokenizer training with masked image prediction. In the first stage, the original image is split into some patches and encoded using discrete tokens, which is different from BERT since image patches don't have off-the-shelf tokens as words in NLP. In the second stage, the BEiT encoder takes a corrupted image containing unmasked and masked patches, and then the visual tokens of the masked patches are outputted to match the corresponding visual tokens from the fixed tokenizer. Despite its success, the separation between masked prediction and autoencoder training induces that the whole framework is not end-to-end and hinders learning effectiveness and efficiency. figure* \\centering \\includegraphics[width=0.8\\linewidth]{figures/memorybank_zip.pdf} The general pipeline for the Memory Bank Method~\\cite{wu2018unsupervised.} figure* To migrate this issue, MAE he2022masked proposes an end-to-end simple solution by predicting the masked patches directly from the unmasked ones with the Mean Squared Error (MSE) loss. It's worth noting that MAE uses a masking ratio of 75\\%, which is significantly higher than that of BERT (typically 15\\%). Ablation study suggests that higher masking ratios are beneficial for both fine-tuning and linear probing. Concurrently, SimMIM xie2022simmim proposes a similar autoencoder-based solution as MAE, in which they also confirm that a higher marking ratio and leveraging random masking strategy helps improve performance. The major difference is how they partition the responsibility of representation encoding and pretext prediction in the autoencoder. Since the decoder of SimMIM is simple, the encoder of SimMIM synchronously conducts both of them. On the contrary, the encoder in MAE solely undertakes the role of representation encoding, and the decoder is responsible for pretext prediction. Recently, Meta AI announces the Segment Anything Model (SAM)~kirillov2023segment which prompts users to specify what to segment in an image, allowing for a wide range of segmentation tasks without the need for additional training. SAM employs an MAE pretrained ViT-H~dosovitskiy2020image image encoder that runs once per image and produces an image embedding, as well as a prompt encoder that embeds input prompts such as clicks or boxes. Following that, a lightweight transformer-based mask decoder predicts object masks from image and prompt embeddings. The results show that SAM can generate high-quality masks from a single foreground point that are typically just modestly inferior to the manually annotated ground truth. It routinely achieves strong quantitative and qualitative outcomes on a wide range of downstream tasks using a zero-shot transfer approach and prompt engineering. Leveraging ViT in MAE poses a serious inefficiency issue, where decreasing the patch size results in a quadratic increase in computing resources. To address the problem, there are two important solutions: (1) hierarchical ViT and (2) local attention. In the first direction, hierarchical ViT (hViT) was introduced, which utilizes a shrinking pyramid structure and techniques like shifted windows liu2021swin to reduce computational demands. Unfortunately, hViT cannot be directly applied to enable MAE pretraining because the local window attention used in hViT makes it difficult to handle randomly masked patches as in MAE. Recently, Uniform Masking MAE (UM-MAE) li2022uniform is proposed to empower MAE with hViTs, which introduces a two-stage pipeline: sampling and masking. It starts by randomly sampling a portion of patches (25\\% reported in the paper) from each block, and then follows by masking additional patches on top of the sampled ones. The first step helps to maintain common elements across different local windows, while the second step prevents shortcuts for pixel reconstruction from nearby low-level features, making the task more difficult. Another direction to improve efficiency focuses on reducing the input size by putting the attention of the network into some local small windows of the image. Motivated by the observation that local knowledge is sufficient for reconstructing masked patches, Local masked reconstruction (LoMaR) chen2022efficient was proposed. Rather than using the entire image for mask reconstruction, LoMaR samples a number of small windows and focuses attention on local regions, which outperforms MAE on downstream tasks in terms of learning efficiency.",
      "origin_cites_number": 13
    },
    {
      "section_title": "Learning by Memory Bank",
      "level": "2",
      "content": "Non-Parametric Instance Discrimination (NPID)~wu2018unsupervised is the first method that utilizes the instances to learn representations for downstream tasks. The detailed pipeline is shown in Fig. fig:memorybank. The feature representations are stored in the memory bank for the convenience of computation because the instance-level classification objective needs all images in the training dataset. For any image $x$ with feature representation $v=f_\\theta(x)$, its probability of being recognized as $i$-th example is: equation P(i|v) = exp(v_i^{T}v/\\tau) / \\sum\\nolimits_{j=1}^n exp(v_j^{T}v/\\tau), equation where $v_i$ or $v_j$ is the representation of $i$-th or $j$-th sample, which serves as a substitute for the parametric class prototype (i.e., weights of a classifier). Addtionally, $\\tau$ is the temperature parameter borrowed from the knowledege distillation~hinton2015distilling. Local Aggregation (LA)~zhuang2019local is another method that trains a CNN encoder to embed raw images into a lower dimension space -- embedding space. When a metric of local aggregation is maximized, similar data instances move together in the embedding space while dissimilar instances move apart. Based on NPID, Pretext Invariant Representation Learning (PIRL, pronounced as ``pearl'')~~misra2020self is proposed to argue that semantic representations are invariant under pretext transformation tasks. Suppose the original view and transformed view of images are denoted as $I$ and $I^{t}$, respectively. These sample views are fed into a CNN encoder, and the total empirical loss on the training dataset $D$ can be defined as: equation L_{total}(\\theta;D)=E_{t\\simT}\\left[1{|D|}\\sum\\nolimits_{I\\inD}L(V_I,V_{I^{t}})\\right], equation where $T$ denotes the different transformations of images. The loss encourages the representation of image $I$ to be similar to that of $I^t$, and the representation of $I^t$ to be dissimilar to that of different images $I'$, as shown in the dotted box of Fig.~fig:two-stream. Therefore, more negative sample pairs contribute to improving the scalability of the gradient and lead to the final learned encoder with stronger representation ability. That is the reason why the memory bank is introduced to store more previous representations for subsequent comparison. figure* \\centering \\includegraphics[width=0.85\\linewidth]{figures/sum_cropped.pdf} Summary of all two-stream models, including contrastive learning and memory-bank-based methods. figure*",
      "origin_cites_number": 4
    },
    {
      "section_title": "Learning by Sharing",
      "level": "2",
      "content": "SSL prefers using two encoder networks for the different data augmentation, and then pretrains the parameters by maximizing the distance between negative pairs or minimizing the distance between positive pairs. Fig.~fig:two-stream shows the two-stream models for all contrastive learning frameworks. The transformation $t$ on the orginal input image $I$ generates the view $v$, similarly, its counterpart ${t}'$ generates ${v}'$. In general, two different or same encoders $f_\\theta$ and $f'_\\xi$ are used to extract contrastive representations. The subsequent MLP heads $g_\\theta$ and $g'_\\xi$ are used to learn more combinations that are beneficial to the contrastive loss. It is noticed that MLP and memory bank could be removed or preserved under different settings. In terms of the shared encoder, SSL can be divided into two categories: 1) Soft Sharing that two encoders share with similar but different parameters ($f_\\theta \\neq f'_\\xi$); 2) Hard Sharing that two encoders maintain the same architectures and parameters ($f_\\theta = f'_\\xi$). Soft Sharing. Facebook AI Research (FAIR) presents Momentum Contrast (MoCo)~he2020momentum by using momentum to control the slight difference between two encoders. As shown in Fig. fig:moco, one of the encoders is served as a dictionary look-up task that generates a queue of encoded data samples $\\{k_0, k_1, \\cdots\\}$. Another encoder generates encoded query $\\{q_0, q_1, \\cdots\\}$ with the training batch updated. The similarity is measured by the dot product of the new coming encoded query $q$ and the encoded keys stored in the dictionary queue. Suppose there are $K$ keys stored in the queue before the new key comes. The $K$ keys are treated as negative samples to the query of the new key. To combine the contrastive loss on both negative and positive samples, InfoNCE Loss ~oord2018representation is used for the pretraining in MoCo. The key design in MoCo for soft parameter sharing is called momentum update. He et al.~he2020momentum suggest that the direct parameter change of key encoder (i.e., momentum encoder) to query encoder loses the necessary consistency and yields poor results. The momentum encoder parameter $\\theta_k$ is updated as: equation \\theta_k = m\\theta_k + (1-m)\\theta_q, equation where the query encoder parameter $\\theta_q$ is learned directly from the gradients of new coming instance, and $m\\in[0, 1)$ is a hyper-parameter that controls the consistency ($\\theta_k$ is more consistent if $m$ is closer to $1$). Inspired by the design of SimCLR~chen2020improved, in MoCo v2~chen2020improved, the FAIR team introduces an MLP projection head after encoders and utilizes more data augmentation techniques to improve the performance. The further improvements are from that: 1) embedded linear classifier bridges the gap between unsupervised and supervised pretraining representations; 2) more contrastive samples are feasible from both the larger training batch and stronger data augmentation. DeepMind proposed Bootstrap Your Own Latent (BYOL)~grill2020bootstrap that contains representation, projection, and discrimination stages to achieve a new SOTA without using negative samples. They understand the discrimination between different views of raw images as necessary prevention from collapse during the pretraining. However, they argue that many negative samples are not indispensable to prevent this collapse. As shown in the left part of Fig. fig:two-stream, there are two streams in BYOL with different parameters. The online network (top green) updates parameters by comparing the prediction generated itself and the regression target provided by the target network. Then the parameters of the target model (bottom red) are updated the same as Eq.~(momentum-update), i.e., $\\xi\\gets\\tau\\xi+(1-\\tau)\\theta$, where $\\tau$ is the target decay rate to control the degree of parameter changing in the target network. Therefore, the target network can also be understood as a momentum encoder. Here, $\\xi$ in the target model is the parameter $\\theta_k$ in momentum encoder, and $\\theta$ in the online network denotes the parameter $\\theta_q$ in the query encoder. figure*[t] \\centering \\includegraphics[width=0.75\\linewidth]{figures/MoCo_zip.pdf} The general pipeline of MoCo~\\cite{he2020momentum, which is also a two-stream framework with different parameters.} figure* Hard Sharing. SimCLR~chen2020simple is proposed by Brain Team in Google Research which utilizes the hard parameter-sharing architecture. This simple framework can also be concluded in Fig. fig:two-stream, in which we can see that representations of different views of the same image are learned in the network $f(\\cdot)$. This base encoder shares the parameters with each other. Thus, memory bank and momentum setting to learn key and query encoders are not necessary, which contributes to a simpler backbone architecture and easier learning strategy. The loss function to maximize the similarity between different views of the same image (positive pairs) is defined as equation \\ell_{i,j}=-\\log exp(sim(z_i,z_j)/\\tau) \\\\ / \\sum\\nolimits_{k=1}^{2N}1_{[k\\neq i]}exp(sim(z_i,z_k)/\\tau), equation where $(i,j)$ is a pair of positive samples, $\\tau$ is an introduced hyper-parameter called temperature parameter~wu2018unsupervised, and $1_{[k\\neq i]}\\in\\{0,1\\}$ is an indicator function to control the denominator containing only negative pairs. To avoid the dependence on a large number of explicit pairwise feature comparisons, Swapping Assignments between multiple Views of the same image (SwAV)~caron2020unsupervised is proposed as an online algorithm by Inria and FAIR. SwAV introduces clustering to substitute the previous comparison between pairs, which gains more memory with the help of non-queue architecture. In this method, the clustering prototype joins the computation of the defined loss function. This prototype is encoded as the concatenation of vectors learned through the backpropagation in CNNs. Thus, there is no need for SwAV to compare the encoded representations between different views. Based on the existing SwAV, a novel model called SElf-supERvised (SEER)~goyal2021self aims to learn a pretrained encoder from any random image and unbounded dataset in the wild. The base network is RegNetY architectures~radosavovic2020designing trained with the SwAV SSL method~caron2020unsupervised. This method proves that the SSL is not specific to a curated dataset such as ImageNet, and the scalability of recent RegNet releases the limitation of traditional backbones such as ResNet. In addition, this method encourages the research community to explore more backbones suitable for universal SSL. Attracting the attention in the recent SSL, FAIR conducts empirical experiments on the SSL by utilizing the structure of Simple Siamese (SimSiam) networks. This method~chen2021exploring can avoid the design of negative sample pairs, large batches (or memory banks), and momentum encoders in traditional contrastive learning. The two encoders in Fig. fig:two-stream with identical parameters that process two different views $t$ and $t^{\\prime}$ of image $x$ are substituted by the only siamese network. MLP predictor $g$ is used for one of the view representations, and then the stop-gradient operation is applied to another view representation. figure*[t] \\centering \\includegraphics[width=0.7\\linewidth]{figures/clustering_zip.pdf} The key pipeline for the DeepCluster model~\\cite{caron2018deep.} figure*",
      "origin_cites_number": 15
    },
    {
      "section_title": "Learning by Clustering",
      "level": "2",
      "content": "DeepCluster~caron2018deep is the first model that adopts the clustering algorithm for large-scale dataset learning. This method groups the representations into different clusters and labels these clusters as supervised signals to pretrain the parameters of the backbone network. It demonstrates SOTA performance on a wide range of standard transferred tasks used in unsupervised learning. When it comes to the connection between contrastive learning and clustering, SwAV~caron2020unsupervised has utilized prototypes that serve as a clustering center to help classify the sample pairs during pretraining, while Prototypical Contrastive Learning (PCL)~DBLP:conf/iclr/0001ZXH21 first targets bridging contrastive learning with clustering. Compared to instance discrimination as pretext tasks learning low-level representations, clustering can help to encode more semantic information. Then more semantic-based downstream tasks will benefit from it. As shown in Fig. fig:clustering, prototypical contrastive learning uses prototypes to substitute one of the views of generated samples in NCE loss (Eq. (nce-loss)), which is the proposed ProtoNCE loss in PCL. In addition, PCL is also a method based on soft parameter sharing, in which the momentum encoder is updated as Eq.(momentum-update). table*[t] \\tiny \\centering Summary of the PFMs in CV. \\textwidth{!}{ threeparttable tabular{llllllll} \\hline Year & Conference & Method & Pretext Task & Architecture & Downstream Task1 & Code \\\\ \\hline 2014 & NeurIPS & Exemplar-CNN~dosovitskiy2014discriminative,DFB16 & discrimination & CNN & cla, rec & https://lmb.informatik.uni-freiburg.de/resources/binaries/nips2014\\_ExemplarCNN.zip{https://lmb.informatik.uni-freiburg.de/...} \\\\ \\hline 2015 & ICCV & Context~doersch2015unsupervised & context prediction & CNN & cla, det, clu & https://github.com/cdoersch/deepcontext{https://github.com/.../deepcontext} \\\\ \\hline 2016 & CVPR & Inpainting~pathak2016context & inpainting & GAN, CNN & cla, det, seg, inp & https://github.com/pathak22/context-encoder{https://github.com/.../context-encoder} \\\\ \\hline 2016 & ECCV & Colorization~zhang2016colorful & colorization & CNN & cla, det, seg & https://github.com/richzhang/colorization{https://github.com/.../colorization} \\\\ \\hline 2016 & ECCV & Jigsaw~noroozi2016unsupervised & Jigsaw puzzles & CNN & cla, det, seg, ret & https://github.com/MehdiNoroozi/JigsawPuzzleSolver{https://github.com/.../JigsawPuzzleSolver} \\\\ \\hline 2017 & CVPR & Split-Brain~zhang2017split & channel prediction & CNN & cla, det, seg & https://richzhang.github.io/splitbrainauto{https://richzhang.github.io/splitbrainauto} \\\\ \\hline 2017 & ICCV & Counting~noroozi2017representation & counting & CNN & cla, det, seg, ret & https://github.com/clvrai/Representation-Learning-by-Learning-to-Count{https://github.com/clvrai/...} \\\\ \\hline 2017 & ICML & NAT~bojanowski2017unsupervised & noise & CNN & cla, det & - \\\\ \\hline 2017 & ICLR & BiGAN~donahue2016adversarial & generation & GAN, CNN & cla, det, seg & https://github.com/jeffdonahue/bigan{https://github.com/.../bigan} \\\\ \\hline 2018 & WACV & CDJP~kim2018learning & Jigsaw puzzles & CNN & cla, det, seg & - \\\\ \\hline 2018 & ICLR & RotNet~zhang2016colorful & rotation & NIN, CNN & cla, det, seg & https://github.com/gidariss/FeatureLearningRotNet{https://github.com/gidariss/...} \\\\ \\hline 2018 & arXiv & CPC~oord2018representation & patch overlapping & CNN, GRU & cla & - \\\\ \\hline 2018 & CVPR & NPID~wu2018unsupervised & instance discrimination & CNN & cla & https://github.com/zhirongw/lemniscate.pytorch{https://github.com/.../lemniscate.pytorch} \\\\ \\hline 2018 & ECCV & DeepCluster~caron2018deep & clustering & CNN & cla, det, seg & https://github.com/facebookresearch/deepcluster{https://github.com/.../deepcluster} \\\\ \\hline 2019 & ICCV & LA~zhuang2019local & local aggregation & CNN & rec, det & https://github.com/neuroailab/LocalAggregation{https://github.com/.../LocalAggregation} \\\\ \\hline 2019 & NeurIPS & BigBiGAN~DBLP:conf/nips/DonahueS19 & generation & GAN, CNN & gen, cla & https://tfhub.dev/s?publisher=deepmind\\&q=bigbigan{https://tfhub.dev/...bigbigan} \\\\ \\hline 2019 & CVPR & AET~zhang2019aet & transformation & CNN & cla & https://github.com/maple-research-lab/AET{https://github.com/.../AET} \\\\ \\hline 2019 & NeurIPS & AMDIM~bachman2019learning & discrimination & CNN & cla & https://github.com/Philip-Bachman/amdim-public{https://github.com/.../amdim-public} \\\\ \\hline 2020 & CVPR & ClusterFit~yan2020clusterfit & clustering & CNN & cla, seg & - \\\\ \\hline 2020 & ICML & CPC v2~henaff2020data & patch overlapping & CNN & cla, det & - \\\\ \\hline 2020 & CVPR & PIRL~misra2020self & Jigsaw puzzles & CNN & cla, rec, dec & https://github.com/facebookresearch/vissl/tree/master/projects/PIRL{https://github.com/.../PIRL} \\\\ \\hline 2020 & CVPR & MoCo~he2020momentum & discrimination & CNN & cla, rec, dec, pos, seg & https://github.com/facebookresearch/moco{https://github.com/.../moco} \\\\ \\hline 2021 & ICLR & PCL~DBLP:conf/iclr/0001ZXH21 & clustering & CNN & cla, det & https://github.com/salesforce/PCL{https://github.com/.../PCL} \\\\ \\hline 2020 & arXiv & MoCo v2~chen2020improved & discrimination & CNN & cla, dec & https://github.com/facebookresearch/moco{https://github.com/.../moco} \\\\ \\hline 2020 & ICLR & SeLa~asano2019self & self-labelling & CNN & cla, det, seg & https://github.com/yukimasano/self-label{https://github.com/.../self-label} \\\\ \\hline 2020 & ICML & SimCLR~chen2020simple & discrimination & CNN & cla & https://github.com/google-research/simclr{https://github.com/.../simclr} \\\\ \\hline 2020 & NeurIPS & SimCLR v2~chen2020big & self-distillation~hinton2015distilling & CNN & cla & https://github.com/google-research/simclr{https://github.com/.../simclr} \\\\ \\hline 2020 & ECCV & CMC~tian2019contrastive & view matching~cubuk2019randaugment & CNN & cla, seg & https://hobbitlong.github.io/CMC/{https://hobbitlong.github.io/CMC} \\\\ \\hline 2020 & NeurIPS & InfoMin~tian2020makes & discrimination & CNN & cla, det, loc, seg & https://hobbitlong.github.io/InfoMin/{https://hobbitlong.github.io/InfoMin} \\\\ \\hline 2020 & NeurIPS & SwAV~caron2020unsupervised & cropping & CNN, Transformer & cla, det & https://github.com/facebookresearch/swav{https://github.com/.../swav} \\\\ \\hline 2020 & NeurIPS & BYOL~grill2020bootstrap & discrimination & CNN & cla, det, seg & https://github.com/deepmind/deepmind-research/tree/master/byol{https://github.com/.../byol} \\\\ \\hline 2021 & arXiv & MoCo v3~chen2021empirical & discrimination & CNN, Transformer & cla & - \\\\ \\hline 2021 & ICLR & ReLIC~DBLP:conf/iclr/MitrovicMWBB21 & discrimination & CNN & cla, rel & - \\\\ \\hline 2021 & ICLR & PCL v2~DBLP:conf/iclr/0001ZXH21 & clustering & CNN & cla, det & https://github.com/salesforce/PCL{https://github.com/.../PCL} \\\\ \\hline 2021 & CVPR & SimSiam~chen2021exploring & discrimination & CNN & cla, det, seg & https://github.com/facebookresearch/simsiam{https://github.com/.../simsiam} \\\\ \\hline 2021 & ICML & DirectPred~DBLP:conf/icml/TianCG21 & discrimination & CNN & cla & https://github.com/facebookresearch/luckmatters/tree/main/ssl{https://github.com/.../ssl} \\\\ \\hline 2021 & ICCV & DINO~caron2021emerging & discrimination & CNN, Transformer & cla, seg & https://github.com/facebookresearch/dino{https://github.com/.../dino} \\\\ \\hline 2021 & arXiv & MoBY~xie2021self & discrimination & CNN, Transformer & cla, det, seg & https://github.com/SwinTransformer/Transformer-SSL{https://github.com/.../Transformer-SSL} \\\\ \\hline 2021 & NeurIPS & MST~li2021mst & token prediction & CNN, Transformer & cla, det, seg & - \\\\ \\hline 2022 & ICLR & BEiT~bao2022beit & token prediction & Transformer & cla, seg & https://github.com/microsoft/unilm/tree/master/beit{https://github.com/.../beit} \\\\ \\hline 2022 & CVPR & MAE~he2022masked & reconstruction & Transformer & cla, det, seg & https://github.com/facebookresearch/mae{https://github.com/facebookresearch/mae}\\\\ \\hline 2022 & CVPR & SimMIM~xie2022simmim & reconstruction & Transformer & cla, det, seg & https://github.com/microsoft/SimMIM{https://github.com/microsoft/SimMIM}\\\\ \\hline 2022 & ArXiv & UM-MAE~li2022uniform & reconstruction & Transformer & cla, det, seg & https://github.com/implus/UM-MAE{https://github.com/implus/UM-MAE}\\\\ \\hline 2022 & ArXiv & LoMaR~chen2022efficient & reconstruction & Transformer & cla, det, seg & https://github.com/junchen14/LoMaR{https://github.com/junchen14/LoMaR}\\\\ \\hline 2022 & Arxiv & CAE~chen2022context & reconstruction & Transformer & cla, det, seg & https://github.com/lxtGH/CAE{https://github.com/lxtGH/CAE}\\\\ \\hline 2023 & AAAI & PeCo~dong2021peco & reconstruction & Transformer & cla, det, seg & -\\\\ \\hline 2023 & ArXiv & SAM~kirillov2023segment & reconstruction & Transformer & det, gen, seg & https://github.com/facebookresearch/segment-anything{https://github.com/facebookresearch/segment-anything}\\\\ \\hline tabular tablenotes \\tiny \\item[1] Downstream task types: classification (cla), recognition (rec), detection (det), localization (loc), segmentation (seg), clustering (clu), inpainting (inp), retrieval (ret), generation (gen), pose estimation (pos), reinforcement learning (rel). tablenotes threeparttable } table*",
      "origin_cites_number": 52
    },
    {
      "section_title": "Summary",
      "level": "2",
      "content": "This section extensively investigates recent progress in PFMs on images for representation learning, from the early perspective of designing pretext tasks for self-labeling to present contrastive loss-based SSL. The pipelines of the main methods are clearly illustrated. We hope this section can prepare the incoming researchers to acquire a basic understanding of this novel area and some worthwhile research direction. We believe the powerful generalization ability of PFMs would extremely reduce training computation overhead by ``pretraining once and transferring forever''. Recent transformer-based PFMs have gradually outperformed traditional training from scratch on target datasets. This discovery will spur further exploration and research into this exciting field.",
      "origin_cites_number": 0
    },
    {
      "section_title": "PFMs for Graph Learning",
      "level": "1",
      "content": "With the development of deep learning in graphs, the parameters (i.e., graph embedding) of the model began to increase rapidly. Therefore, large-scale labeled data is needed for training the models to avoid under-fitting or over-fitting. However, the cost of constructing large-scale labeled datasets for graphs is too subjective, expensive, and time-consuming, especially in domains that require professional knowledge and timeliness. While some semi-supervised approaches have temporarily mitigated the reliance of graph embedding models on label scale, they have not fundamentally resolved this problem. In recent times, researchers have turned their attention towards the application of PFMs in the field of graphs, inspired by their success in CV and NLP. However, for most graphs, obtaining large-scale pretraining data directly is challenging due to the unique nature of information such as nodes and edges. Therefore, recent studies have focused on utilizing the inherent information of a graph's attributes, topology, and community to enhance the effectiveness of the node's features. We have summarized the graph-related PFMs in Table~\\ref{tab:pretraining model for graph}.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Learning by Graph Information Completion",
      "level": "2",
      "content": "The essential motivation of pretraining based on graph information completion (GIC) is to mask part of the information of the input graph data and recover the masked information based on the unmasked graph data, so as to pretrain the graph embedding, as shown in Fig. fig:GIC_GPP. Similar ideas appeared earlier in the field of image and text processing. For instance, in image processing, information such as image pixels and colors are recovered to pretrain the image encoder; in text processing, many methods implement pretraining of word embeddings and encoders by recovering part of the information in a sentence based on context words. These methods inspire the design of graph completion tasks on graph PFMs. figure*[!t] \\centering \\subfigure[Graph Information Completion (GIC).]{ \\includegraphics[width=.4\\linewidth]{fig_graph/GIC.pdf}} \\centering \\subfigure[Graph Property Prediction (GPP).]{ \\includegraphics[width=.4\\linewidth]{fig_graph/GPP.pdf}} Graph Information Completion (GIC) and Graph Property Prediction (GPP). figure* Among them, You et al.~you2020icml are inspired by image inpainting, and first propose to cover them by removing the features of the target nodes, and then recover/predict the features of the masked nodes. In order to recover/predict the masked information, GraphCompetion~you2020icml is achieved by providing GCNs with unmasked node features (limited to the 2-layer GCNs of the second-order neighbors of each target node). The purpose of GraphCompetion's pretraining is to help the model better perform feature representation and teach the model to extract features from the context. You et al.~you2020icml propose the attribute mask task (namely, AttributeMask), which masks node attributes randomly, and then requires the self-supervising module to reconstruct the masked attributes. Jin et al.~jin2020arxiv think deeply about SSL on graph data, and propose the edge mask task (namely, EdgeMask), seeking to develop self-supervision in pairs based not only on a single node itself but on the connection between two nodes in the graph. In particular, EdgeMask randomly masks some edges and then asks the model to reconstruct the masked edges. In short, EdgeMask is expected to help GNN learn local connectivity information. Hu et al.~hu2020iclr propose a PFM that masks node and edge attributes and then predicts this masked information based on the adjacent structure.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Learning by Graph Consistency Analysis ",
      "level": "2",
      "content": "Different from the aforementioned methods that focus on individual elements in the graph, graph consistency analysis (GCA) mainly explores the consistency of the distribution of two elements in the graph. Specifically, the consistency of two elements with similar semantics should be significantly stronger than two elements with unrelated semantics, and this characteristic can be used to pretrain the graph model. According to the judgment object of consistency, such methods can be roughly divided into the following three categories. figure*[!t] \\centering \\subfigure[Context Consistency.]{ \\includegraphics[width=.47\\linewidth]{fig_graph/GCA_CC.pdf}} \\centering \\subfigure[Self Consistency.]{ \\includegraphics[width=.47\\linewidth]{fig_graph/GCA_SC.pdf}} Graph Consistency Analysis (GCA). figure* Context Consistency Based on the early homogeneity assumption, a mass of graph models tends to project contextual nodes to similar positions in semantic space. Such consistency of the context in the graph is also applied to the pretraining graph model, which attempts to adjust the node representation by capturing the distribution characteristics of the nodes in the context, as shown in Fig. fig:GCA (a). Random walk is an efficient method to acquire context. It can capture the distribution characteristics of different perspectives in the context by designing a variety of walk strategies. The DeepWalk~deepwalk2014kdd adopts a truncated random walk strategy to represent the node context as the form of a sequence of nodes. By introducing the idea of NLP into the network embedding model, DeepWalk regards the node sequence as a ``sentence'' and models it based on the skip-gram model, providing an unsupervised and scalable training method for node representation. Furthermore, on the basis of DeepWalk, node2vec~node2vec2016kdd uses two different parameter-controlled random walk strategies to obtain deviated node sequences to fully capture the context information. Different from randomly sampling nodes from the context, some recent methods directly consider the relationship between the node's k-order neighbor distribution (as positive examples) and non-adjacent nodes (as negative examples), and use this to train the graph model. LINE~line2015www respectively proposes first- and second-order proximity to describe the local similarity between pairs of nodes in the graph from different perspectives, and uses it to optimize node representation. Meanwhile, LINE uses negative sampling and edge sampling techniques to optimize the second-order traversal and excessive training storage overhead. VGAE~vgae2016nips introduces a variational autoencoder to encode graph structure data, and model the node first-order neighbor through a GCN encoder and a simple inner product decoder. Self Consistency In the field of NLP and CV, contrastive learning as an efficient self-supervised mechanism is widely used in the pretraining of models. In fact, the internal comparison mechanism of such methods is based on the mutual information estimation of the original graph data and the augmented graph data to maintain the consistency of the data itself, as shown in Fig. fig:GCA (b). Inspired by contrastive learning, some studies have begun to generate augmented samples of original data samples in the graph model. Among them, two augmented samples from the same original sample are regarded as positive pairs, and two augmented samples from different original samples are regarded as negative pairs. For node-level tasks, GCC~gcc2020kdd devises the pretext task as subgraph instance discrimination in and across networks. And GCC also enhances the ability of GNNs to learn the intrinsic and transferable structural representations by introducing contrastive learning. Specifically, GCC samples subgraphs from the whole graph as augmentations via random walk with restart and artificially designs positional node embedding as node initial features. As a novel graph representation learning model, GCA~gca2021www incorporates various priors for topological and semantic aspects of the graph to achieve adaptive contrastive augmentation. Specifically, GCA devises an enhancement scheme based on node centrality measures to highlight important connection structures, while corrupting node features by adding noise to specific nodes to lead the pretraining model to recognize underlying semantic information. For graph-level tasks, some studies have attempted to introduce more diverse contrastive learning strategies. Among them, You et al.~graphcl2020nips introduce four common graph augmentation tasks (i.e., node dropping, edge perturbation, attribute masking, and subgraph sampling) into the GL model based on underlying prior and propose a unified comparative learning framework: GraphCL. Meanwhile, GraphCL discusses in depth the role of data augmentation in comparative learning and gives experimental demonstration that joint multiple augmentation strategies can improve model performance. Cross Scale Consistency Unlike the above two methods that consider the consistency of elements in the same scale, contrasting elements in graph data of different scales can also be used to train graph models, e.g., node-subgraphs. Most of such methods have the idea of maximizing mutual information~gic2021pakdd,sugar2021www. Specifically, the readout function is usually used to obtain the summary of the graph/subgraph, and the MI estimator can be calculated using the Jensen-Shannon divergence. As a representative method, DGI~dgi2019iclr relies on maximizing the MI between the patch representation and the summary of the corresponding high-level graphs, which are all derived using the established graph convolutional network architecture, to learn the node representation. To generate negative samples on a single graph, DGI corrupts the original graph by randomly scrambling node features while keeping the structure unchanged. Similarly, Hassani and Khasahmadi propose CMVRL~cmvrl2020icml, which generates an additional structural view of a sample graph based on graph diffusion. The sample graph and a regular view are sub-sampled together, and the node representation and graph representation are learned based on two shared MLPs, and then contrast learning is achieved through the consistency loss provided by the discriminator. SUBG-CON~subcom2020icdm samples a series of context subgraphs from the original graph and inputs them to the encoder to obtain the pooled central node and subgraph representation. For the specified node, the context subgraph is expressed as a positive sample, and other randomly sampled subgraphs are expressed as a negative sample. The contrast loss of the latent space will force the encoder to identify positive samples and negative samples in order to distinguish different nodes based on regional structure information.",
      "origin_cites_number": 11
    },
    {
      "section_title": "Learning by Graph Property Prediction",
      "level": "2",
      "content": "Considering the attribute and structural information of the graph as the target of information completion, pretraining based on graph property prediction (GPP) can also be used to build the graph model in different forms. One of the most common methods is to generate self-supervised signals by exploring the auxiliary property in the graph data and to take the graph property prediction task as the pretraining task of the graph model. According to the different settings of the pretext task, it can roughly classify two categories: property regression and property classification. Property Regression (PR) In the graph model, different from the GIC mentioned above, property regression primarily focuses on mining the relationship between the broader numerical structure and property attributes within the graph. Specifically, this branch of methods extracts richer self-supervised signals in graph data for pretraining graph models. For example, similar but different from masking node attributes, the goal of NodeProperty~jin2020arxiv is to predict each node's auxiliary property in the graph, e.g., degree, local node importance, and local clustering coefficient. In other words, NodeProperty is used to encourage GNN to capture richer local structural information while optimizing the specific downstream tasks. Specifically, NodeProperty regards the node degree as a representative local node property, i.e., self-supervised signal, and takes other node properties as future work. Meanwhile, NodeProperty emphasizes that the intuition of devising self-supervised pretext tasks related to local node property is to ultimately guide the feature embedding of GNN (i.e., node representation) to save this information, which relies on the assumption that the node property information is relevant to the particular task. Property Classification (PC) Different from the property regression task, the task of property classification is usually implemented by defining pseudo-labels based on a certain distribution in the graph data, which is a typical self-supervised method. Among them, the structure density, similarity of node attributes, and difference between local and global distributions are the most commonly used. We will briefly introduce the application of such methods in GL pretraining. Among these methods, clustering is the most common and effective source of pseudo-labels. Among them, M3S~M3S2020aaai designs a multi-stage training strategy, using the idea of graph clustering to iteratively train the graph encoder, achieving enlarged labeled data with virtual labels in the case of very small samples. You et al.~you2020icml further propose two pretraining strategies. Among them, Node Clustering assigns $K$ (hyper-parameter) pseudo labels to nodes based on attribute clustering and pretrain node representation by node classification. In addition, You et al. also present Graph Partitioning based on the topology density assumption. In Graph Partitioning, the nodes of a graph are divided into approximately equal $K$ (hyper-parameter) subsets to minimize the number of edges connecting nodes among subsets, and then pseudo labels are provided for nodes. In addition to clustering methods, some researchers generate pseudo labels based on other statistical characteristics of graph data. For instance, in the molecular field, Rong et al.~grover2020nips use the molecular bonds of subgraphs and related statistical information to guide GNN to learn Context-Sensitive Properties (CSP) and then apply them to prediction. Rong et al.~grover2020nips propose a Motif Prediction (MP) task, which can be expressed as a multi-label classification problem, in which each motif corresponds to a label. Specifically, let's assume that $K$ motifs in molecular data are considered. For a specific molecule (abstracted as graph $G$), they use RDKit to detect whether each motif appears in $G$, and then take it as the target of the motif prediction task.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Learning by Masked Autoencoder",
      "level": "2",
      "content": "The masked autoencoder (MAE) is first applied in MAGE~tan2022mgae, the masked autoencoders for self-supervised learning on graphs. Following MAE he2022masked, MGAE operates on a partial network structure (without masked edges) that is based on convolutions. Besides, the decoder of MGAE is designed to model the cross-correlation between the head and tail nodes of an anchor edge. Empirical results demonstrate that MGAE performs better than traditional graph autoencoders and graph SSL approaches. Furthermore, GMAE hou2022graphmae extends this approach by using a transformer instead of convolutions and reconstructing the features of masked nodes rather than masked edges. In addition to empirical improvements, MaskGAE li2022maskgae further provides theoretical justifications for the potential benefits of masked graph modeling. Designing algorithms to accommodate graphs of various complex properties is a promising direction. For instance, to tackle the heterogeneous graphs scenario, HGMAE tian2022heterogeneous proposes meta-path masking and adaptive attribute masking with a dynamic mask to enable effective and stable learning on complex graph structure. Moreover, several training strategies are developed, including meta-path-based edge reconstruction to incorporate complex structural information, target attribute restoration to utilize various node attributes, and positional feature prediction to encode node positional information. Besides dealing with more complex graph structures, how to improve the learning efficiency of MAE on graph data remains an open question.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Other Learning Strategies on Graph Data",
      "level": "2",
      "content": "In addition to the above methods, there are lots of pretraining methods that use relatively novel or hybrid strategies. For example, CG$^3$~cg3aaai2021 generates an improved node representation by designing a semi-supervised consistency loss to maximize the consistency between different views of the same data or data from the same category. Next, CG$^3$ uses the graph generation loss related to the input feature to extract the potential deterministic relationship between the data feature and the input graph topology as a supplementary supervision signal for SSL. Based on the attention mechanism, Graph-Bert~graphbert2020arxiv trains itself to reconstruct node attributes and topological structure with sampled linkless subgraphs within their local contexts. GMI~gmi2020www extends the traditional mutual information computing idea from the vector space to the graph domain and proposes to jointly maximize feature mutual information (between the nodes embedding and raw features of its neighbors) and edge mutual information (embedding of two adjacent nodes) for graph representation learning. GPT-GNN~gptgnn2020kdd proposes a self-supervised graph generation task to guide itself to capture the topological and semantic attributes of the graph. GPT-GNN roughly divides the possibility of graph generation into attribute generation and edge generation to untangle the intrinsic dependence between node attributes and graph topology.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Summary",
      "level": "2",
      "content": "In the graph model, as traditional feature learning methods are often accompanied by information loss in the process of feature learning, and the information taken into consideration is relatively one-sided, the obtained graph representation is relatively rough and loses mass information. People began to focus on the distribution of data and attributes in the graph data as self-supervised signals to pretrain the graph model so that it can capture more valuable information. By transforming the distribution of nodes, attributes, and edges in the graph into different pretext tasks, and using GNNs for modeling, the graph model can fully fit the original distribution of the input graph. In lots of unsupervised or semi-supervised scenarios, such pretrained graph models have been proven to benefit downstream tasks. Besides, federated training large graph models~wang2023federated can be a promising solution for building pretrained foundation models. Currently, with the in-depth study of contrastive learning strategies, some work has attempted to apply contrastive learning in different forms to the pretraining of graph models. Through the consistency analysis of context, self, and cross-scale, this kind of method greatly improves the performance of the pretrained graph model on different graphs. table*[t] \\tiny \\centering Summary of PFMs in GL. 1.1{ tabular{llllll} \\hline Year & Conference & Method & Pretext Task & Encoder & Code \\\\ \\hline 2014 & KDD & DeepWalk~deepwalk2014kdd & GC-C & Shallow NN & https://github.com/phanein/deepwalk{https://github.com/phanein/deepwalk} \\\\\\hline 2015 & WWW & LINE~line2015www & GC-C & Shallow NN & https://github.com/tangjianpku/LINE{https://github.com/tangjianpku/LINE} \\\\\\hline 2016 & NeurIPS & VGAE~vgae2016nips & GC-C & GCN & - \\\\\\hline 2016 & KDD & node2vec~node2vec2016kdd & GC-C & Shallow NN & https://github.com/aditya-grover/node2vec{https://github.com/aditya-grover/node2vec} \\\\\\hline 2017 & NeurIPS & GraphSage~graphsage2017nips & GC-C & Shallow NN & https://github.com/williamleif/GraphSAGE{https://github.com/williamleif/GraphSAGE} \\\\\\hline 2018 & ICLR & DGI ~dgi2019iclr & GC-CS & GCN/SAGE & https://github.com/PetarV-/DGI{https://github.com/PetarV-/DGI} \\\\\\hline 2020 & ICML & GraphCompetion~you2020icml & GIC & GCN & https://github.com/Shen-Lab/SS-GCNs{https://github.com/Shen-Lab/SS-GCNs} \\\\\\hline 2020 & ICLR & AttMasking~hu2020iclr & GIC & GCN & http://snap.stanford.edu/gnn-pretrain{http://snap.stanford.edu/gnn-pretrain} \\\\\\hline 2020 & ICML & AttributeMask~you2020icml & GIC & GCN & https://github.com/Shen-Lab/SS-GCNs{https://github.com/Shen-Lab/SS-GCNs} \\\\\\hline 2020 & arXiv & EdgeMask~jin2020arxiv & GIC & GCN & https://github.com/ChandlerBang/SelfTask-GNN{https://github.com/ChandlerBang/SelfTask-GN} \\\\\\hline 2020 & arXiv & NodeProperty~jin2020arxiv & GPP-PR & GCN & https://github.com/ChandlerBang/SelfTask-GNN{https://github.com/ChandlerBang/SelfTask-GN} \\\\\\hline 2020 & AAAI & M3S~M3S2020aaai & GPP-PC & GCN & - \\\\\\hline 2020 & ICML & Node Clustering~you2020icml & GPP-PC & GCN & https://github.com/Shen-Lab/SS-GCNs{https://github.com/Shen-Lab/SS-GCNs} \\\\\\hline 2020 & ICML & Graph Partitioning~you2020icml & GPP-PC & GCN & https://github.com/Shen-Lab/SS-GCNs{https://github.com/Shen-Lab/SS-GCNs} \\\\\\hline 2020 & NeurIPS & CSP~grover2020nips & GPP-PC & GCN & - \\\\\\hline 2020 & NeurIPS & MP~grover2020nips & GPP-PC & GCN & - \\\\\\hline 2020 & NeurIPS & SELAR~selar2020nips & GC-C & GNN & https://github.com/mlvlab/SELAR{https://github.com/mlvlab/SELAR} \\\\\\hline 2020 & KDD & GCC~gcc2020kdd & GC-S & GIN & https://github.com/THUDM/GCC{https://github.com/THUDM/GCC} \\\\\\hline 2020 & NeurIPS & GraphCL ~graphcl2020nips & GC-S & GCN & https://github.com/CRIPAC-DIG/GCA{https://github.com/CRIPAC-DIG/GCA} \\\\\\hline 2020 & ICML & CMVRL ~cmvrl2020icml & GC-CS & GCN & - \\\\\\hline 2020 & ICDM & SUBG-CON ~subcom2020icdm & GC-CS & GCN & https://github.com/yzjiao/Subg-Con{https://github.com/yzjiao/Subg-Con} \\\\\\hline 2020 & ICLR & InfoGraph ~infog2020iclr & GC-CS & GCN & https://github.com/fanyun-sun/InfoGraph{https://github.com/fanyun-sun/InfoGraph} \\\\\\hline 2020 & AAAI & DMGI ~dmgiaaai2020 & GC-CS & GCN & https://github.com/pcy1302/DMGI{https://github.com/pcy1302/DMGI} \\\\\\hline 2020 & arXiv & Graph-Bert ~graphbert2020arxiv & Hybrid & Transformer & https://github.com/jwzhanggy/Graph-Bert{https://github.com/jwzhanggy/Graph-Bert} \\\\\\hline 2020 & WWW & GMI ~gmi2020www & Hybrid & GCN & - \\\\\\hline 2020 & KDD & Gpt-GNN ~gptgnn2020kdd & Hybrid & GNN & https://github.com/acbull/GPT-GNN{https://github.com/acbull/GPT-GNN} \\\\\\hline 2021 & ICML & JOAO ~joao2021icml & GC-S & GCN & https://github.com/Shen-Lab/GraphCL\\_Automated{https://github.com/Shen-Lab/GraphCL\\_Automated} \\\\\\hline 2021 & AAAI & CSSL ~cssl2021aaai & GC-S & GCN & https://github.com/UCSD-AI4H/GraphSSL{https://github.com/UCSD-AI4H/GraphSSL} \\\\\\hline 2021 & PAKDD & GIC ~gic2021pakdd & GC-CS & GCN & https://github.com/cmavro/Graph-InfoClust-GIC{https://github.com/cmavro/Graph-InfoClust-GIC} \\\\\\hline 2021 & WWW & SUGAR ~sugar2021www & GC-CS & GCN & https://github.com/RingBDStack/SUGAR{https://github.com/RingBDStack/SUGAR} \\\\\\hline 2021 & ICML & GraphLoG ~graphlog2021arxiv & GC-CS & GCN & https://github.com/DeepGraphLearning/GraphLoG{https://github.com/DeepGraphLearning/GraphLoG} \\\\\\hline 2021 & WWW & SLiCE ~slice2021www & GC-CS & GCN & https://github.com/pnnl/SLICE{https://github.com/pnnl/SLICE} \\\\\\hline 2021 & WSDM & BiGI ~bigi2021wsdm & GC-CS & GCN & https://github.com/caojiangxia/BiGI{https://github.com/caojiangxia/BiGI} \\\\\\hline 2021 & WWW & GCA ~gca2021www & GC-S & GCN & https://github.com/CRIPAC-DIG/GCA{https://github.com/CRIPAC-DIG/GCA}\\\\\\hline 2021 & KDD & HeCo ~heco2021kdd & GC-CS & GCN & https://github.com/liun-online/HeCo{https://github.com/liun-online/HeCo} \\\\\\hline 2021 & AAAI & CG$^3$ ~cg3aaai2021 & Hybrid & GCN & - \\\\\\hline 2021 & ICLR & SuperGAT~supergat2021iclr & GC-C & GAT & https://github.com/dongkwan-kim/SuperGAT{https://github.com/dongkwan-kim/SuperGAT} \\\\\\hline 2021 & KDD & MoCL ~mocl2021kdd & Hybrid & GNN & https://github.com/illidanlab/MoCL-DK{https://github.com/illidanlab/MoCL-DK} \\\\ \\hline 2022 & ArXiv & MGAE ~tan2022mgae & Maksed Edge Reconstruction & GCN &-\\\\ \\hline 2022 & KDD & GMAE ~hou2022graphmae & Maksed Node Reconstruction & Transformer &https://github.com/THUDM/GraphMAE{https://github.com/THUDM/GraphMAE} \\\\ \\hline 2022 & Arxiv & MaskGAE ~li2022maskgae & Partial Maksed Node Reconstruction& Transformer &https://github.com/EdisonLeeeee/MaskGAE{https://github.com/EdisonLeeeee/MaskGAE} \\\\ \\hline 2022 & Arxiv & HGMAE ~tian2022heterogeneous & Metapath Masking Reconstruction& Transformer & - \\\\ \\hline tabular } table*",
      "origin_cites_number": 43
    },
    {
      "section_title": "PFMs for Other Data Modality",
      "level": "1",
      "content": "With the rapid development of the PFMs, except for text, image, and graph, the PFMs have also carried out a lot of research on speech, video, text-image, and cross-data. Besides, researchers have started investigating the unified PFMs that incorporate all three mentioned domains recently. Therefore, in this section, we introduce some other advanced and unified PFMs.",
      "origin_cites_number": 0
    },
    {
      "section_title": "PFMs for Speech",
      "level": "2",
      "content": "In the field of speech, wav2vec~wav2vec2019isca obtains speech representation by capturing contextual information on large-scale unlabeled datasets, and fine-tuning on a few samples by noise comparison binary classification task, which greatly improves the performance of downstream tasks. Furthermore, vq-wav2vec~vq-wav2vec2020iclr and wav2vec 2.0~wav2vec22020nips propose a discrete unsupervised pretraining method on the basis of wav2vec, discretizing the original continuous speech signal, so that the methods in the mature NLP community can be migrated and applied. Meanwhile, lots of research have tried to design different mechanisms to use the representation obtained by speech pretraining as the initial input, and apply it to different tasks, e.g., automatic speech recognition~speechbert2020ieee,wav2vec22020nips, phoneme recognition~speechxlnet2020isca, and speech synthesis~chung2019ieee. In particular, the extensive application of spoken language understanding has promoted the research of joint pretraining of speech and text. For example, SpeechBERT~speechbert2020ieee applies MLM to speech and text pairs to perform representation learning on discrete information. Unlike~forcomp2020isca, which relies on a large amount of labeled data for joint pretraining, SPLAT~selfsup2021acl uses unlabeled speech data to pretrain the speech embedding module, and proposes a label-level alignment method suitable for label-level downstream tasks based on sequence alignment. MusicBERT~zeng2021musicbert is a pretrained model designed for processing music data. It was developed by training on a vast symbolic music corpus consisting of over one million songs. To improve the pretraining process with symbolic music data, MusicBERT employs several mechanisms, such as OctupleMIDI encoding and a bar-level masking strategy. Huang et al.~huang2020pop suggest incorporating a metrical structure in the input data, which allows Transformers to better recognize the hierarchical structure of music at the beat-bar-phrase level. AudioTransformer~verma2021audio is a model that enhances the performance of Transformer architectures by implementing certain techniques, such as pooling, which were previously used in convolutional networks. Verma et al.~verma2021audio demonstrate how they leverage multi-rate signal processing ideas based on wavelets to improve the Transformer embeddings and obtain better results.",
      "origin_cites_number": 13
    },
    {
      "section_title": "PFMs for Video",
      "level": "2",
      "content": "Video is similar to the RGB features of image and sequence information of the text. Many meaningful explorations in self-supervised video representation learning can not only perform efficiently well in video datasets but also generalize to the learning in other areas. Odd-One-Out Networks (O3N)~fernando2017self is a technique that targets to predict the odd video subsequence among real subsequences sampled from a video in a training dataset. The experiments are conducted by using different video-clip encoders for O3N to prove consistent improvements of this pretraining design. Similarly, Shuffle and Learn~misra2016shuffle aims to learn the correct temporal order from a sequence of frames in a video. However, Kim et al.~kim2019self designed a new self-supervised task called Space-Time Cubic Puzzles to train 3D CNNs. This task requires a pretrained backbone to arrange permuted 3D spatiotemporal crops. The performance of downstream tasks proves that effective video representations have been learned while solving such puzzles. Inspired by the contrastive learning in images, many pretraining models in the video also utilize the contrastive loss to learn video presentations for downstream tasks. Inter-Intra Contrastive (IIC) framework~tao2020self can learn video representations by using positive and negative pairs generated from different videos. Specifically, different modalities in the same video are treated as positive pairs, and video clips from different videos as negative ones. Temporal Contrastive Pretraining (TCP)~lorre2020temporal is another contrastive method based on CPC to learn video representations. Different from the existing GAN-based method that generates future frames for the video directly, TCP can predict the latent representation of future frames of the video, which is better for long-term predictions. Sequence Contrastive Learning (SeCo)~yao2020seco is a novel method considering both intra- and inter-frame instance discrimination in sequence order-based task.",
      "origin_cites_number": 6
    },
    {
      "section_title": "PFMs for Multimodal",
      "level": "2",
      "content": "The multimodal PFM among text and image can be divided into two categories: single-stream model and cross-stream model. The single-stream model refers to integrating text information and visual information at the beginning of the model. The Cross-stream model refers to text information and visual information encoded by two independent coding modules, respectively. Then different modal information is fused by mutual attention mechanism. Single-Stream Model VisualBERT~DBLP:journals/corr/abs-1908-03557 inputs text and images into the model simultaneously, which are aligned and fused using Transformer's self-attention mechanism. The input of the text is the same as BERT, and the input of the image is the image features extracted by Fasters-RCNN. VisualBERT also does pretraining and then fine-tuning the specific task. It adopts two pretraining tasks, namely MLM and sentence-image prediction, determining whether the input sentence describes the corresponding image. The structure of Unicoder-VL~DBLP:conf/aaai/LiDFGJ20 is very similar to VisualBERT, except for the processing of the image. Unicoder-VL extracts the image feature through Faster-RCNN and concatenates the feature with image position-encoding mapping to the same space. It enhances the image label prediction task, which predicts the categories of images. The pretraining task of VL-BERT~DBLP:conf/iclr/SuZCLLWD20 is the same as Unicoder-VL. The image input of VL-BERT includes four parts: the image region features extracted by Fasters-RCNN, the location of the region in the original image, location coding, fragment encoding, and [IMG] encoding. Cross-Stream Model In ViLBERT~DBLP:conf/nips/LuBPL19, the text and image modes are first encoded separately, and their outputs go through a standard attention module. This module is based on the Transformer structure. Still, in the self-attention mechanism, each module uses its query to calculate attention with the value and key of another module to integrate the information between different modules. The model is pretrained on two tasks. The first task is the mask task, which is the same as BERT. On the image side, the goal of the task is that when the region image is masked, the classification distribution of the output of the model can be as consistent as possible with the output distribution of the model used to extract the region features (such as Faster-RCNN). The second task is the language image matching task. DALL-E is a series of deep learning models developed by OpenAI to generate images from natural language prompts. The first version of DALL-E uses a transformer-based architecture, similar to the one used in the GPT LMs, to process the textual prompts and generate image-like representations. The model is trained on a dataset of images and their associated textual descriptions based on GPT-3. DALL-E 2~ramesh2022hierarchical is the improved version by employing contrastive language-image pretraining (CLIP) radford2021learning for capturing semantic association between image-text pairs and GLIDE diffusion model nichol2021glide for text-conditional image synthesis. Furthermore, GPT-4 is proposed by OpenAI recently. It is a large-scale multimodal model which adopts RLHF and demonstrates human-level performance on various professional and academic benchmarks. Based on the multi-modal data containing more available information than previous single-modality data, thus the performance of these models gets enhanced by combining with the SSL on the benchmark dataset. Cross and Learn~sayed2018cross is the first method that reveals crossmodal information as an alternative source of supervision and obtains powerful feature representations from combining crossmodal loss and diversity loss in both RGB and optical flow modalities. Different from the existing methods that learn feature representations from only a single task in cross-domain datasets, Ren and Lee et al.~ren2018cross propose a novel deep multi-task network to learn more generalizable visual representations to overcome the domain difference and further utilize the cross-domain information in different tasks. In that paper, the cross-domain datasets are real and synthetic datasets generated by a GAN-based network, while the multiple tasks are the predictions of the surface normal, depth, and instance contour in RGB images. This model performs better than any previous single-task-based SSL methods by learning general-purpose visual representations from cross-domain multi-task feature learning. Tian et al.~tian2020contrastive believe that a powerful representation is one that models cross-view factors from the perspective of humans view to understand the world. They propose Contrastive Multiview Coding (CMC) to learn a video representation by maximizing mutual information between different views of the same scene.",
      "origin_cites_number": 10
    },
    {
      "section_title": "PFM for Code Generation",
      "level": "2",
      "content": "Code generation with LLMs involves using pretrained language models to automatically generate code based on natural language descriptions of a desired program. This approach has the potential to greatly improve the efficiency of software development by reducing the need for manual coding and allowing developers to focus on higher-level tasks. The technique involves training large-scale language models on vast amounts of natural language text and then fine-tuning the models on specific programming tasks. By inputting natural language descriptions of code, the model can generate code snippets that are syntactically and semantically correct. Code generation with LLMs has been applied in various programming domains, including web development, NLP, and data analysis. The models used for code generation include GPT-4, T5, and Codex, among others. For example, Andrei et al.~zlotchevski2022exploring have investigated and assessed the fine-tuning of transformer models for personalized code generation. Specifically, they have evaluated the effectiveness of various personalization techniques in the domain of generating unit tests for Java methods and learning to personalize for a specific software project. Shailja et al.~thakur2022benchmarking assess the capacity of LLMs to generate Verilog that is useful. To achieve this, pretrained LLMs are fine-tuned on Verilog datasets collected from GitHub and Verilog textbooks. An evaluation framework is then constructed, consisting of test benches for functional analysis and a flow for testing the syntax of Verilog code generated in response to problems of varying degrees of difficulty. An open-source CodeGen LLM that has undergone fine-tuning has been shown to outperform the current leading commercial Codex LLM. The CodeGen~nijkamp2022codegen is a group of LLMs that have up to 16.1B parameters and can handle both natural language and programming language data. Additionally, they have released the training library JAX FORMER as open-source. Their work demonstrates that the model can perform as well as the previous state-of-the-art zero-shot Python code generation on HumanEval, showcasing the practical applications of the trained model. Synchromesh, introduced in the study by Poesia et al.~poesia2022synchromesh, adopts a novel approach called Target Similarity Tuning (TST) to retrieve a small set of examples from a training bank. Then, Synchromesh utilizes these examples to train a pretrained language model and generates programs by applying Constrained Semantic Decoding (CSD). CSD is a general framework that can restrict the output to valid programs in the target language. In this work, the authors show that the combined use of CSD and TST results in significant improvements in prediction accuracy, as well as preventing runtime errors. However, there are still some limitations to code generation with LLMs, such as the models' tendency to generate overly verbose or inefficient code and their inability to handle complex programming tasks. Nevertheless, the technology has shown significant promise and has the potential to revolutionize the software development industry.",
      "origin_cites_number": 4
    },
    {
      "section_title": "SOTA Unified PFMs",
      "level": "2",
      "content": "A big convergence of PFMs handling multiple modalities is emerging, such as backbone architecture, pretraining task, and model scaling up~wang2022image. Therefore, many unified PFMs proposed by researchers arise. A unified PFM is a unified model pretrained on unimodal and multimodal data with single or multiple transformers as the backbone, which has the ability to perform a large variety of downstream AI tasks, including unimodal tasks and multimodal tasks. There are currently three types of SOTA unified models based on model architectures. We defined them as the single-transformer model, multi-transformer model, and comb-transformer model. A single-transformer model refers to a PFM model which only has a large-scale transformer as its backbone, whereas a multi-transformer model refers to a PFM model having multiple transformers. A comb-transformer model is the PFM model with the combination of both single and multiple transformer structures. Single-transformer Model UNITER~chen2020uniter is a large-scale PFM for joint image-text embedding, which consists of an Image Embedder, a Text Embedder, and a multi-layer Transformer. It first encodes visual features and bounding box features for image regions using Image Embedder and tokens and positions using Text Embedder. Then, a Transformer module is applied to learn generalizable contextualized embeddings for images and text through four pretraining tasks. Instead of applying random joint masking to both modalities, conditional masking on pretraining tasks is used. Six vision-language tasks are selected as the downstream tasks. Uni-Perceiver~zhu2022uni is a single siamese model with shared parameters having the ability to process different modalities regarding vision and language tasks. Different task inputs and targets are encoded into unified token sequences with modality-specific tokenizers, which are then decoded by a modality-agnostic weight-sharing Transformer encoder into the shared representation space. Any perception task is modeled as finding the maximum likelihood target for each input through the similarity of their representations. Uni-Perceiver is pretrained on unimodal and multimodal tasks. The evaluation results on various downstream tasks show that the performance is close to SOTA methods by conducting prompt tuning on 1\\% of downstream task data. Gato~reed2022generalist builds a single large transformer sequence model that works as a multimodal, multi-task, multi-embodiment generalist policy. It can perform various tasks using a single neural network with the same set of weights. Gato is trained on 604 tasks, where different types of data, such as images, text, proprioception, joint torques, and other discrete and continuous observations and actions, are serialized into a flat sequence of tokens, batched, and processed by the transformer. During deployment, sampled tokens are assembled into different actions based on the context. OFA~wang2022unifying is a simple sequence-to-sequence learning framework with a unified instruction-based task representation that unifies various tasks. In the pretraining and finetuning stages, OFA requires no extra task-specific layers for downstream tasks to achieve Task-Agnostic. The Modality-Agnostic compute engine is a Transformer with the constraint that no learnable task- or modality-specific components are added to downstream tasks. OFA is pretrained on small-size image-text pairs to achieve crossmodal tasks while attaining highly competitive performances on unimodal tasks. UNIFIED-IO~lu2022unified is a sequence-to-sequence model using a unified architecture that performs large and diverse tasks. UNIFIED-IO is a transformer model where both the encoder and decoder are composed of stacked transformer layers. The unified architecture does not need specific task or modality branches, which is accomplished by homogenizing the input and output of each task into a sequence of discrete vocabulary tokens. It trains a single transformer-based architecture on over 90 diverse datasets in the vision and language fields. UNIFIED-IO is the first model to perform various tasks and produce strong results across 16 diverse benchmarks without finetuning. BEiT-3~wang2022image is a general-purpose multimodal pretrained model on language, vision, and vision-language tasks. The big convergence of BEiT-3 can be seen from three aspects, including backbone architecture, pretraining task, and model scaling up. It introduces a shared Multiway Transformer as backbone network performing masked data modeling on both unimodal and multimodal data. To process different modalities, every Multiway Transformer block has a shared self-attention module, and a pool of feed-forward networks. It is a giant-size foundation model that contains 1.9B parameters. Experimental results show that BEIT-3 can outperform SOTA models on both vision and vision-language tasks. Multi-transformer Model FLAVA~singh2022flava is an alignment model that targets all modalities at once and aims at solving vision and language tasks, and vision-language tasks. It utilizes a common transformer model architecture to learn strong representations from unimodal and multimodal data. An image encoder transformer is used to capture unimodal image representations. A text encoder transformer is adopted to process unimodal text information. A multimodal encoder transformer takes both encoded unimodal images and text as inputs and integrates their representations for multimodal reasoning. During pretraining, masked image modeling (MIM) and MLM losses are applied to the image and text encoders, respectively. On the other hand, masked multimodal modeling (MMM) and image-text matching (ITM) loss are used over paired image-text data. For downstream tasks, classification heads are applied to the outputs from the image, text, and multimodal encoders, respectively, for visual recognition, language understanding, and multimodal reasoning tasks. FLAVA shows good performance on 35 tasks across different domains. A noticeable advantage is that smaller datasets it used compared with other models. Comb-transformer Model UNIMO~li2020unimo can learn both single modality and multimodalities with one model to achieve robust and generalizable representations. It employs multi-layer self-attention Transformers to learn general textual and visual representations simultaneously and unifies them into the same semantic space via cross-modal contrastive learning (CMCL). The main idea behind CMCL is to keep paired image and text representations close to the representation space while keeping non-paired representations far away. All of them are encoded by the same unified-modal Transformer in pairs or individually, and the representations of images and texts are extracted to compute the contrastive loss.",
      "origin_cites_number": 9
    },
    {
      "section_title": "Other Advanced Topics on PFMs",
      "level": "1",
      "content": "As the number of parameters of the pretraining model increases, the pretraining model requires more memory and computing resources. It increases the training cost of PFMs and limits their deployment on resource-constrained devices. Therefore, to improve the efficiency of the pretraining model, PFM improves computational efficiency from the following two aspects: model efficiency and model compression. The model efficiency and compression of the PFM refer to simplifying the redundancy of model parameters and structure. Under the condition that the task completion degree is not affected, the model with fewer parameters and a more concise structure is obtained.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Model Efficiency",
      "level": "2",
      "content": "Model efficiency devotes to exploring more efficient pretraining methods to pretrain large-scale PFMs with a lower-cost solution. More efficient learning algorithms require more effective training methods and more efficient model architecture. Traditional pretraining tasks may be inefficient. For example, the commonly used masked token prediction task requires the model to predict masked tokens based on context. However, the masked tokens in the samples are usually a subset of the input tokens, and the model can only learn from this part of the tokens, so the training efficiency is low. To solve this problem, ELECTRA~DBLP:conf/iclr/ClarkLLM20 proposes an RTD task that predicts whether each input marker is replaced by other tokens, which enables the ELECTRA to train against all input tokens. In addition to effective training methods, more efficient architecture can also improve the efficiency of PFMS. For most PFMS based on the Transformer algorithm, a more efficient model architecture can be obtained by reducing the complexity of the Transformer algorithm.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Model Compression",
      "level": "2",
      "content": "Model compression requires less computing resources and memory. It is a potential approach to reduce the model size and enhance computation efficiency. The model compression strategy can be divided into two ways: parameter compression and structure compression. The methods of parameter compression include parameter pruning, parameter quantization, low-rank decomposition, and parameter sharing. Parameter pruning refers to designing evaluation criteria for model parameters to delete redundant parameters based on a sizeable PFM. For example, Compressing BERT~DBLP:conf/rep4nlp/GordonDA20 prunes BERT before training while maintaining the performance equivalent to that of the original model. Parameter quantization is the quantization of model parameters from 32-bit full-precision floating-point numbers to lower-order numbers. For example, Q8BERT~DBLP:conf/nips/ZafrirBIW19 uses 8-bit quantization to compress parameters fourfold with little impact on model performance. Low-rank decomposition is to reduce the dimension of a high-dimensional parameter vector into a sparse low-dimensional vector. Parameter sharing refers to the structured matrix or clustering methods to map model parameters and reduce the number of parameters. For example, the ALBERT~DBLP:conf/iclr/LanCGGSS20 uses decomposition-embedded parameterization and cross-layer parameter sharing to reduce the parameters in the model. Structure compression refers to compact networks and knowledge distillation. A compact network means reducing the number of parameters and calculations by designing a new compact network structure. Knowledge distillation refers to the transfer of knowledge from the larger teacher model to the smaller student model through the use of a soft label, etc. DistilBERT~DBLP:journals/corr/abs-1910-01108, for example, uses the knowledge distillation method to compress BERT, reducing the size of the BERT model by 40\\% while retaining 97\\% of its language comprehension.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Security and Privacy",
      "level": "2",
      "content": "The security risks, social bias, and data privacy in PFMs become an important research topic. Qiu et al.~qiu2020pre recognize that deep neural networks can be attacked by adversarial samples, which mislead the model to produce false predictions. Due to the excellent portability of pretraining models, they have been widely used in NLP, CV, and GL. However, it has been found that the pretraining model is susceptible to the influence of adversarial samples. A tiny interference of the original input may mislead the pretraining model to produce specific false predictions. Meanwhile, it is possible to recover the data samples by querying the PFMs which can cause privacy leakage. Generation Adversarial Samples The adversarial sample originates from the image. The adversarial samples of the image are hard to recognize with an invisible change. For example, only one pixel of the image is modified. Human beings do not easily detect such disturbance, but the neural network can identify the modified image, which is the original purpose of the adversarial sample. Some work has found that pretrained LMs are vulnerable in some scenarios. Jin et al.~jin2020bert successfully attack the three target models of BERT, CNN, and RNN by generating natural adversarial samples, which indicates that the current language processing model still has a large room for improvement in terms of security. However, it is difficult to achieve due to the distinct discreteness of languages in NLP. In particular, the generation of adversarial samples in the text must take into account linguistic characteristics to ensure that the sample's syntax and fluency are not harmed while affecting the model's output. For example,~nlp12020aaai uses adversarial samples to attack the fine-tuning stage of the BERT model for text classification and entailment successfully.~nlp22020acl combines the sememe-based word substitution method and search algorithm based on particle swarm optimization to generate adversarial samples. Model Defects Some unrelated human factors can also mislead the PFM to make wrong predictions. For example,~nlp32019acl discovers that the performance of BERT is limited in the reasoning task due to utilizing false statistical information in the dataset, which dramatically affects the performance by destroying this property.~nlp42019emnlp defines universal adversarial triggers. When triggers are connected to any input, it can induce the model to generate specific predictions. Backdoor Attacks There are still many methods to manipulate the predicted results of the pretraining model employing a backdoor attack.~nlp72020acl demonstrates that it is possible to construct a weight poisoning attack in which pretrained weights are injected. After the fine-tuning stage, the backdoor is exposed. Attackers manipulate model predictions easily by injecting arbitrary keywords.~nlp82020sp shows that PFMs in NLP can be manipulated by modifying the model corpus. The ``meaning'' of new words or existing words can be controlled by changing their weight parameters. Defense Against Attacks The human-in-the-loop method~nlp52019arxiv, nlp62020acl has been proposed and applied to generate more natural, efficient, and diversified adversarial samples. Some defense approaches have been proposed to defend against such attacks. nlp112021acl designs an auxiliary anomaly detection classifier and uses a multi-task learning procedure to defend against adversarial samples. On the other hand, some defects in the PFM may be inherited by the custom models in transfer learning, such as the adversarial vulnerabilities and backdoors mentioned above. To mitigate this issue, ~zhang2022remos proposes a relevant model slicing technique to reduce defect inheritance during transfer learning while retaining useful knowledge from the PFM. Data Privacy in PFMs LLMs and other PFMs have been trained on private datasets~carlini2021extracting. The researchers have discovered that by querying the massive LMs, it is feasible to recover specific training samples. An adversary may, for instance, obtain IRC discussions and personally identifiable information. Even worse, because large models have so many parameters, it is simple for PFM to memorize or learn private information, making larger models more prone to attack than smaller ones. Many PFMs such as the LLMs have been trained on private datasets. The researchers have found that it is possible to recover individual training examples by querying the LLMs. For instance, an adversary can extract examples including personally identifiable information, and Internet Relay Chat (IRC) conversations. Even worse, because of the billion parameters of large models, it is easy for PFM to learn private information, making the larger model more vulnerable than smaller models. We must take privacy-preserving measures into account during all PFM processes, including data processing, model training, model inference, and system deployment, in order to reduce the risks of privacy leakage.",
      "origin_cites_number": 12
    },
    {
      "section_title": "Future Research Challenges and Open Problems",
      "level": "1",
      "content": "The PFM can avoid training models from the scratch, which is a breakthrough from weak AI to general AI. At present, due to the characteristics of PFM such as large-scale parameters, a large amount of training data, and high computational complexity, there are still many technical challenges in PFMs. We summarize the future research challenges of PFMs from four perspectives: data, foundation, model design, and upstream and downstream tasks. Meanwhile, we point out some open problems in the future research direction.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Challenges on Data",
      "level": "2",
      "content": "Most pretrained datasets are for single modes and single languages. It is very important for the development of PFMs to construct pretrained datasets for multimodal, multi-lingual, and graph data. For the characteristics of these data, the existing technical challenges are as follows: Data Deficiencies Unlike NLP and CV, except for the reusable nodes in a few molecular and protein networks, most of the nodes and edges in the graph data do not have a large amount of unlabeled data for pretraining. Meanwhile, the pretraining research of the graph model is still in its initial state. Besides, data from the Internet of Things (IoT) will be enormous and contains rich physical world information. For example, inertial measurement unit sensor data can capture users' social activity information~wang2018socialite, han2019shad. The theoretical basis, various definitions of the pretext task, and the augmented design of contrastive learning are all imperfect, and new research urgently needs to be supplemented. Multimodal PFM Some research work has been done on multimodal PFMs, such as text and image, text and audio, etc. These are mostly PFMs between two modalities. At present, the learning of multimodal PFMs requires new multimodal datasets, which demand the establishment of the data between different modes. Thus, the construction of multimodal datasets is also an urgent problem to be solved. Multi-lingual PFM The multi-lingual PFM solves the resource shortage problem in multiple languages, and it aids in the achievement of new improvements in QA, text summarization, low-resource neural machine translation, and so on. However, the current PFM is still a mask LM. To improve the performance of the multi-LM, some suitable new tasks need to be added. In addition, multi-lingual vocabularies are much larger than single-language vocabularies, resulting in a sharp increase in model parameters to be learned.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Challenges on Foundation",
      "level": "2",
      "content": "For a PFM, a theoretical foundation is essential to model performance, whether it is a ``black box'' or ``white box'' method. The foundation studied mainly includes theoretical foundation, semantic understanding, and explicable exploration. Lack of Theoretical Foundation SSL in CV learns the experience from the NLP. There is no profound theory to support all kinds of tentative experiments, and further exploration has no handbook. Although there are several theoretical analysis that tries to understand the collapse of pretraining or the generalization ability of the learning representation, the lack of theoretical foundation is still a huge cloud upon the head of SSL. Semantic Understanding Does the pretrained LM learn the meaning of the language, or just rely on corpus learning? Many models perform well on various datasets with helpful information that can be extracted, where some approaches even exceed human levels. However, the performance is poor on domain datasets or relatively small datasets. The models cannot reach a better level of stability and match different downstream tasks. This means that the model cannot serve the real purpose of human language use.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Challenges on Model Design",
      "level": "2",
      "content": "Most existing structures of PFMs are tried for text, image, and graph. The primary method is to increase data, improve computation power, and design training procedures to achieve better results. How to make a trade-off between data, computing resources, and predictive performance is worth studying. Model Variety There are many attempts at model design, such as generation-based models in the CV area. However, GAN-based approaches are not popular for the following two reasons: 1) the discriminator has learned meaningful feature representations, but they are forgotten during training~chen2019self; 2) the mode collapse causes the generator to output samples in singular mode to cheat the discriminator. As a result, although researchers attempt to apply GAN-based approaches on SSL for pretraining, the difficulties in the convergence of discriminator and divergence of generator hinder development and progress in this area. Model Compression With the wide application of the Transformer and the pretraining model showing a general trend of growth, the computational complexity of the pretraining model has become the focus of attention. Due to the huge hardware requirements of model training and other reasons, the high threshold makes it difficult for researchers to train from scratch. BERT-base and GPT-3 contain about 108 million parameters and 175 billion parameters, respectively. It is not conducive to the development of relevant research work. There are some works for pretraining model compression, such as ALBERT having fewer parameters and better effect than BERT-base. The improvement models still require powerful computing equipment, making them difficult to apply universally. Reducing the high computing cost is one of the main challenges in future research. Model Robustness Although many researchers have designed different pretext tasks for the pretraining, the main problem remains on how to design robust pretext tasks and judge the performance before large-scale computations. In addition, how to compare these proposed methods fairly is also a big issue. As for NLP, deep neural networks are vulnerable to adversarial inputs because of their linear characteristics. Although pretraining models perform well on different NLP tasks, most are based on deep neural networks, which generally have poor robustness. Operations such as cutting and rotating do not change the nature of the image in CV. In contrast, operations such as adding, deleting, and substituting a word in the text are likely to affect the semantics of the text. Therefore, how to improve the robustness of the PFM in NLP is a technical challenge. Model Anti-attack The PFMs are vulnerable to attack by adversarial examples, which can easily mislead the model to produce specific false predictions. It is difficult to process due to the unique discreteness of language in the NLP field. Thus, the current PFMs have huge room for improvement in model anti-attack.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Challenges on Finetuning and Prompt",
      "level": "2",
      "content": "The pretrained model in NLP, CV, and GL fields can achieve good performance in most upstream tasks, but not all good in downstream tasks for fine-tuning and prompt. How to achieve consistent results both on upstream and downstream tasks is still a challenge for the PFMs. Saturation Phenomena Google Research~abnar2021exploring observed the nonlinear relationship between the performance of upstream and downstream tasks. The higher training accuracy with more data on the upstream tasks does not always lead to better performance on the target downstream tasks. This observation challenges the most intuitive understanding of the pretraining process. Even in the most extreme case, the performance of upstream and downstream is at odds. Pretext Task There are too many self-supervised tasks, also known as pretext tasks. The pretext task can be used for any downstream tasks, such as detection and classification. It is difficult to match the relationship between pretext tasks and downstream tasks. Task-based Graph Much of the pretraining on the graph is based on the task graph. Different tasks construct different graphs, where nodes need to be reused. This makes it impossible to pretrain on the graph by introducing as much data as NLP and CV.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Open Problems for Future PFMs",
      "level": "2",
      "content": "First of all, a big convergence of text, image, graph, and multimodal pretraining is expected. Till the survey is written, no work has considered the graph in their unified PFMs. All of the SOTA unified models mainly focus on the language, vision, and language-vision tasks, while neglecting the importance of the graph in the data domain. Second, a unified backbone architecture for unified PFMs in future research will become more popular. It can be seen that a unified PFM model which only has a large-scale transformer as its backbone, i.e., a single-transformer model, is more focused by researchers than other types of unified PFMs. Third, a unified PFM is expected to achieve SOTA transfer performance for all different tasks in all data domains, including text, image, graph, and multimodalities. Most unified PFMs are only outstanding in a single data domain, whereas the performance in other domains is not competitive. BEiT-3~wang2022image shows a great example in both vision and vision-language tasks towards this research direction. Besides, in terms of RL usage in PFMs, even though ChatGPT build the milestone in NLP, CV and GL do not have significant research published yet. More work in this direction is expected in the future.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Conclusion",
      "level": "1",
      "content": "Existing PFMs in text, image, and graph domains are principally summarized in this survey. Firstly, we introduce the basic components of NLP, CV, and GL. Then, we provide a summary of existing models designed for pretraining in the three domains and summarize the necessary information in terms of model structures. Furthermore, we study some other research for PFMs, including other advanced and unified PFMs, model efficiency and compression, and security and privacy. Finally, we present the main challenges and open problems in PFM research. \\pagebreak",
      "origin_cites_number": 0
    }
  ],
  "literature_review_id": 266359151,
  "meta_info": {
    "cite_counts": 182,
    "Conference_journal_name": "ArXiv",
    "influentialcitationcount": 157,
    "Author_info": {
      "Publicationsh": 11,
      "h_index": 5,
      "Citations": 2638,
      "Highly Influential Citations": 0
    },
    "all_cites_title": [
      "Dialog Dialog Generation Wizard of Wikipedia",
      "Large language models struggle to learn long-tail knowledge",
      "Siren's song in the ai ocean: A survey on hallucination in large language models",
      "Gar-meets-rag paradigm for zero-shot information retrieval",
      "Retrievalaugmented generation for knowledge-intensive nlp tasks",
      "Improving language models by retrieving from trillions of tokens",
      "Training language models to follow instructions with human feedback",
      "Query rewriting for retrieval-augmented large language models",
      "Advanced rag techniques: an illustrated overview",
      "Large language model based long-tail query rewriting in taobao search",
      "Take a step back: Evoking reasoning via abstraction in large language models",
      "Precise zero-shot dense retrieval without relevance labels",
      "Enhancing rag pipelines in haystack: Introducing diversityranker and lostinthemiddleranker",
      "Generate rather than retrieve: Large language models are strong context generators",
      "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy",
      "Knowledgpt: Enhancing large language models with retrieval and storage access on knowledge bases",
      "Forget rag, the future is rag-fusion",
      "Lift yourself up: Retrieval-augmented text generation with self memory",
      "Training data is more valuable than you think: A simple and effective method by retrieving from training data",
      "From classification to generation: Insights into crosslingual retrieval augmented icl",
      "Uprise: Universal prompt retrieval for improving zero-shot evaluation",
      "Promptagator: Few-shot dense retrieval from 8 examples",
      "Recitation-augmented language models",
      "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp",
      "Active retrieval augmented generation",
      "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
      "Bridging the preference gap between retrievers and llms",
      "Ra-dit: Retrievalaugmented dual instruction tuning",
      "Fine-tuning or retrieval? comparing knowledge injection in llms",
      "Copy is all you need",
      "Dense x retrieval: What retrieval granularity should we use?",
      "Divide & conquer for entailment-aware multi-hop evidence retrieval",
      "Diversify question generation with retrieval-augmented style transfer",
      "Prompt-guided retrieval augmentation for non-knowledge-intensive tasks",
      "Learning to filter context for retrieval-augmented generation",
      "Retrieval-augmented data augmentation for low-resource domain tasks",
      "Large language model is not a good few-shot information extractor, but a good reranker for hard samples",
      "Retrieval-augmented generative question answering for event argument extraction",
      "Learning to retrieve in-context examples for large language models",
      "Recommender systems with generative retrieval",
      "Language models as semantic indexers",
      "Context tuning for retrieval augmented generation",
      "Few-shot learning with retrieval augmented language models",
      "Raven: In-context learning with retrieval augmented encoderdecoder language models",
      "Shall we pretrain autoregressive language models with retrieval? a comprehensive study",
      "Instructretro: Instruction tuning post retrieval-augmented pretraining",
      "Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering",
      "Augmentation-adapted retriever improves generalization of language models as generic plug-in",
      "Making retrievalaugmented language models robust to irrelevant context",
      "Understanding retrieval augmentation for long-form question answering",
      "Chain-of-note: Enhancing robustness in retrieval-augmented language models",
      "Search-in-thechain: Towards accurate, credible and traceable large language models for knowledgeintensive tasks",
      "Optimizing retrieval-augmented reader models via token elimination",
      "Paperqa: Retrieval-augmented generative agent for scientific research",
      "The power of noise: Redefining retrieval for rag systems",
      "Iag: Induction-augmented generation framework for answering reasoning questions",
      "Nomiracl: Knowing when you don't know for robust multilingual retrieval-augmented generation",
      "Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models",
      "Self-knowledge guided retrieval augmentation for large language models",
      "Retrievalgeneration synergy augmented large language models",
      "Retrieval meets long context large language models",
      "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
      "Investigating the factual knowledge boundary of large language models with retrieval augmentation",
      "Raptor: Recursive abstractive processing for tree-organized retrieval",
      "In-context retrieval-augmented language models",
      "Retrieve-andsample: Document-level event argument extraction via hybrid retrieval augmentation",
      "Zemi: Learning zero-shot semi-parametric language models from multiple tasks",
      "Corrective retrieval augmented generation",
      "1-pager: One pass answer generation and evidence retrieval",
      "Prca: Fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter",
      "Open-source large language models are strong zero-shot query likelihood models for document ranking",
      "Recomp: Improving retrieval-augmented lms with compression and selective augmentation",
      "Replug: Retrieval-augmented black-box language models",
      "Enhancing llm intelligence with arm-rag: Auxiliary rationale memory for retrieval augmented generation",
      "Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems",
      "Augmented large language models with parametric knowledge guiding",
      "Structureaware language model pretraining improves dense retrieval on structured data",
      "Knowledge graph-augmented language models for knowledge-grounded dialogue generation",
      "Retrievalgeneration alignment for end-to-end task-oriented dialogue system",
      "Dual-feedback knowledge retrieval for task-oriented dialogue systems",
      "Fabula: Intelligence report generation using retrieval-augmented narrative construction",
      "Think and retrieval: A hypothesis knowledge graph enhanced medical large language models",
      "Knowledge-augmented language model verification",
      "Reasoning on graphs: Faithful and interpretable large language model reasoning",
      "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering",
      "Tablegpt: Towards unifying tables, nature language and commands into one gpt",
      "Iseeq: Information seeking question generation using dynamic meta-information retrieval and knowledge graphs",
      "Large language models can be easily distracted by irrelevant context",
      "Evaluating the ideal chunk size for a rag system using llamaindex",
      "Recursively split by character",
      "Advanced rag 01: Small-tobig retrieval",
      "Knowledge graph prompting for multi-document question answering",
      "Least-to-most prompting enables complex reasoning in large language models",
      "Chain-of-verification reduces hallucination in large language models",
      "Angle-optimized text embeddings",
      "Voyage's embedding models",
      "Retrieve anything to augment large language models",
      "Lost in the middle: How language models use long contexts",
      "Chatrec: Towards interactive and explainable llms-augmented recommender system",
      "Lingua: Addressing scenarios for live interpretation and automatic dubbing",
      "Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression",
      "Dense passage retrieval for open-domain question answering",
      "Large language model is not a good few-shot information extractor, but a good reranker for hard samples",
      "Chatlaw: Open-source legal large language model with integrated external knowledge bases",
      "Making retrievalaugmented language models robust to irrelevant context",
      "Chain of knowledge: A framework for grounding large language models with structured knowledge bases",
      "Auto-gpt for online decision making: Benchmarks and additional opinions",
      "Toolformer: Language models can teach themselves to use tools",
      "Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt",
      "Webgpt: Browserassisted question-answering with human feedback",
      "Natural questions: a benchmark for question answering research",
      "Exploring the integration strategies of retriever and large language models",
      "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "Squad: 100,000+ questions for machine comprehension of text",
      "Semantic parsing on freebase from question-answer pairs",
      "When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories",
      "Ms marco: A human-generated machine reading comprehension dataset",
      "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
      "Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps",
      "Musique: Multihop questions via single-hop question composition",
      "Eli5: Long form question answering",
      "The narrativeqa reading comprehension challenge",
      "A humaninspired reading agent with gist memory of very long contexts",
      "Asqa: Factoid questions meet long-form answers",
      "Qmsum: A new benchmark for query-based multi-domain meeting summarization",
      "A dataset of information-seeking questions and answers anchored in research papers",
      "Covid-qa: A question answering dataset for covid-19",
      "Cmb: A comprehensive medical benchmark in chinese",
      "Measuring massive multitask chinese understanding",
      "Quality: Question answering with long input texts, yes",
      "Think you have solved question answering? try arc, the ai2 reasoning challenge",
      "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
      "Wizard of wikipedia: Knowledge-powered conversational agents",
      "Large language models as source planner for personalized knowledge-grounded dialogue",
      "Large language models as source planner for personalized knowledge-grounded dialogue",
      "Long time no see! open-domain conversation with long-term persona memory",
      "Conditional generation and snapshot learning in neural dialogue systems",
      "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
      "Document-level event argument extraction by conditional generation",
      "Multisentence argument linking",
      "T-rex: A large scale alignment of natural language with knowledge base triples",
      "Zero-shot relation extraction via reading comprehension",
      "Hellaswag: Can a machine really finish your sentence?",
      "The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning",
      "Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph",
      "Measuring massive multitask language understanding",
      "Pointer sentinel mixture models",
      "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
      "Fever: a large-scale dataset for fact extraction and verification",
      "Explainable automated fact-checking for public health claims",
      "Neural text generation from structured data with application to the biography domain",
      "Wikiasp: A dataset for multi-domain aspect-based summarization",
      "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "Vio-lens: A novel dataset of annotated social network posts leading to different forms of communal violence and its evaluation",
      "Learning question classifiers",
      "Recursive deep models for semantic compositionality over a sentiment treebank",
      "Codesearchnet challenge: Evaluating the state of semantic code search",
      "Training verifiers to solve math word problems",
      "The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages",
      "Ralle: A framework for developing and evaluating retrieval-augmented large language models",
      "Building production-ready rag applications",
      "Evaluating rag part i: How to evaluate document retrieval",
      "Best practices for llm evaluation of rag applications",
      "Ragas: Automated evaluation of retrieval augmented generation",
      "Ares: An automated evaluation framework for retrieval-augmented generation systems",
      "A survey of techniques for maximizing llm performance",
      "Benchmarking large language models in retrieval-augmented generation",
      "Recall: A benchmark for llms robustness against external counterfactual knowledge",
      "Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models",
      "Retrieval meets long context large language models",
      "Memgpt: Towards llms as operating systems",
      "Efficient streaming language models with attention sinks",
      "Raft: Adapting language model to domain specific rag",
      "Scaling laws for neural language models",
      "Neurosymbolic language modeling with automaton-augmented retrieval",
      "Retrieval-augmented multimodal language modeling",
      "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "Visualize before you write: Imagination-guided open-ended text generation",
      "Generating synthetic speech from spokenvocab for speech translation",
      "Using external off-policy speech-to-text mappings in contextual end-to-end automated speech recognition",
      "Vid2seq: Large-scale pretraining of a visual language model for dense video captioning",
      "Retrieval-based prompt selection for code-related few-shot learning"
    ]
  }
}