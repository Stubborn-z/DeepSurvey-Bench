{"paper_id": 218971783, "title": "Language Models are Few-Shot Learners", "author_names": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "R. Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Ma-teusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "I. Sutskever", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "year": 2020, "publicationdate": "2020-05-28", "externalids": {}, "doi_lower": null}
{"paper_id": 247951931, "title": "PaLM: Scaling Language Modeling with Pathways", "author_names": ["A. Chowdhery", "Sharan Narang", "Jacob Devlin", "Maarten Bosma", "Gaurav Mishra", "Adam Roberts", "P. Barham", "Hyung Won Chung", "Charles Sutton", "Sebastian Gehrmann", "Parker Schuh", "Kensen Shi", "Sasha Tsvyashchenko", "Joshua Maynez", "Abhishek Rao", "Parker Barnes", "Yi Tay", "Noam Shazeer", "Vinodkumar Prabhakaran", "Emily Reif", "Nan Du", "Ben Hutchinson", "Reiner Pope", "James Bradbury", "Jacob Austin", "M. Isard", "Guy Gur-Ari", "Pengcheng Yin", "Toju Duke", "Anselm Levskaya", "S. Ghemawat", "Sunipa Dev", "H. Michalewski", "Xavier García", "Vedant Misra", "Kevin Robinson", "L. Fedus", "Denny Zhou", "Daphne Ippolito", "D. Luan", "Hyeontaek Lim", "Barret Zoph", "A. Spiridonov", "Ryan Sepassi", "David Dohan", "Shivani Agrawal", "Mark Omernick", "Andrew M. Dai", "Thanumalayan Sankaranarayana Pillai", "Marie Pellat", "Aitor Lewkowycz", "Erica Moreira", "R. Child", "Oleksandr Polozov", "Katherine Lee", "Zongwei Zhou", "Xuezhi Wang", "Brennan Saeta", "Mark Díaz", "Orhan Firat", "Michele Catasta", "Jason Wei", "K. Meier-Hellstern", "D. Eck", "J. Dean", "Slav Petrov", "Noah Fiedel"], "venue": "Journal of machine learning research", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "year": 2022, "publicationdate": "2022-04-05", "externalids": {}, "doi_lower": null}
{"paper_id": 270214176, "title": "A Survey on Large Language Models for Code Generation", "author_names": ["Juyong Jiang", "Fan Wang", "Jiasi Shen", "Sungju Kim", "Sunghun Kim"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and provide a quantitative and qualitative comparative analysis of experimental results of code LLMs, sourced from their original papers to ensure a fair comparison on the HumanEval, MBPP, and BigCodeBench benchmarks, across various levels of difficulty and types of programming tasks, to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey) to continuously document and disseminate the most recent advances in the field.", "year": 2024, "publicationdate": "2024-06-01", "externalids": {"DOI": "10.1145/3747588"}, "doi_lower": "10.1145/3747588"}
{"paper_id": 235417196, "title": "Scaling Vision with Sparse Mixture of Experts", "author_names": ["C. Riquelme", "J. Puigcerver", "Basil Mustafa", "Maxim Neumann", "Rodolphe Jenatton", "André Susano Pinto", "Daniel Keysers", "N. Houlsby"], "venue": "Neural Information Processing Systems", "abstract": "Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are\"dense\", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet.", "year": 2021, "publicationdate": "2021-06-10", "externalids": {}, "doi_lower": null}
{"paper_id": 199453025, "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks", "author_names": ["Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee"], "venue": "Neural Information Processing Systems", "abstract": "We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.", "year": 2019, "publicationdate": "2019-08-06", "externalids": {}, "doi_lower": null}
{"paper_id": 237386023, "title": "Learning to Prompt for Vision-Language Models", "author_names": ["Kaiyang Zhou", "Jingkang Yang", "Chen Change Loy", "Ziwei Liu"], "venue": "International Journal of Computer Vision", "abstract": "Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consuming—one needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP), we propose Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition. Concretely, CoOp models a prompt’s context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and class-specific context. Through extensive experiments on 11 datasets, we demonstrate that CoOp requires as few as one or two shots to beat hand-crafted prompts with a decent margin and is able to gain significant improvements over prompt engineering with more shots, e.g., with 16 shots the average gain is around 15% (with the highest reaching over 45%). Despite being a learning-based approach, CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts.", "year": 2021, "publicationdate": "2021-09-02", "externalids": {"DOI": "10.1007/s11263-022-01653-1"}, "doi_lower": "10.1007/s11263-022-01653-1"}
{"paper_id": 258291930, "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "author_names": ["Deyao Zhu", "Jun Chen", "Xiaoqian Shen", "Xiang Li", "Mohamed Elhoseiny"], "venue": "International Conference on Learning Representations", "abstract": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.", "year": 2023, "publicationdate": "2023-04-20", "externalids": {"DOI": "10.48550/arXiv.2304.10592"}, "doi_lower": "10.48550/arxiv.2304.10592"}
{"paper_id": 249674500, "title": "Emergent Abilities of Large Language Models", "author_names": ["Jason Wei", "Yi Tay", "Rishi Bommasani", "Colin Raffel", "Barret Zoph", "Sebastian Borgeaud", "Dani Yogatama", "Maarten Bosma", "Denny Zhou", "Donald Metzler", "Ed H. Chi", "Tatsunori Hashimoto", "O. Vinyals", "P. Liang", "J. Dean", "W. Fedus"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.", "year": 2022, "publicationdate": "2022-06-15", "externalids": {"DOI": "10.48550/arXiv.2206.07682"}, "doi_lower": "10.48550/arxiv.2206.07682"}
{"paper_id": 247778764, "title": "Training Compute-Optimal Large Language Models", "author_names": ["Jordan Hoffmann", "Sebastian Borgeaud", "A. Mensch", "Elena Buchatskaya", "Trevor Cai", "Eliza Rutherford", "Diego de Las Casas", "Lisa Anne Hendricks", "Johannes Welbl", "Aidan Clark", "Tom Hennigan", "Eric Noland", "Katie Millican", "George van den Driessche", "Bogdan Damoc", "Aurelia Guy", "Simon Osindero", "K. Simonyan", "Erich Elsen", "Jack W. Rae", "O. Vinyals", "L. Sifre"], "venue": "arXiv.org", "abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.", "year": 2022, "publicationdate": "2022-03-29", "externalids": {}, "doi_lower": null}
{"paper_id": 572361, "title": "Adaptive Mixtures of Local Experts", "author_names": ["R. Jacobs", "Michael I. Jordan", "S. Nowlan", "Geoffrey E. Hinton"], "venue": "Neural Computation", "abstract": null, "year": 1991, "publicationdate": "1991-03-01", "externalids": {"DOI": "10.1162/neco.1991.3.1.79"}, "doi_lower": "10.1162/neco.1991.3.1.79"}
{"paper_id": 67000854, "title": "Hierarchical Mixtures of Experts and the EM Algorithm", "author_names": ["M. I. Jordan", "R. Jacobs"], "venue": "Neural Computation", "abstract": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain.", "year": 1993, "publicationdate": "1993-08-01", "externalids": {"DOI": "10.1109/IJCNN.1993.716791"}, "doi_lower": "10.1109/ijcnn.1993.716791"}
{"paper_id": 529933, "title": "A Parallel Mixture of SVMs for Very Large Scale Problems", "author_names": ["R. Collobert", "Samy Bengio", "Yoshua Bengio"], "venue": "Neural Computation", "abstract": null, "year": 2001, "publicationdate": "2001-01-03", "externalids": {"DOI": "10.1162/089976602753633402"}, "doi_lower": "10.1162/089976602753633402"}
{"paper_id": 5062147, "title": "Infinite Mixtures of Gaussian Process Experts", "author_names": ["Carl E. Rasmussen", "Z. Ghahramani"], "venue": "Neural Information Processing Systems", "abstract": null, "year": 2001, "publicationdate": "2001-01-03", "externalids": {}, "doi_lower": null}
{"paper_id": 2044597, "title": "Nonlinear Models Using Dirichlet Process Mixtures", "author_names": ["B. Shahbaba", "Radford M. Neal"], "venue": "Journal of machine learning research", "abstract": "We introduce a new nonlinear model for classification, in which we model the joint distribution of response variable, y, and covariates, x, non-parametrically using Dirichlet process mixtures. We keep the relationship between y and x linear within each component of the mixture. The overall relationship becomes nonlinear if the mixture contains more than one component, with different regression coefficients. We use simulated data to compare the performance of this new approach to alternative methods such as multinomial logit (MNL) models, decision trees, and support vector machines. We also evaluate our approach on two classification problems: identifying the folding class of protein sequences and detecting Parkinson's disease. Our model can sometimes improve predictive accuracy. Moreover, by grouping observations into sub-populations (i.e., mixture components), our model can sometimes provide insight into hidden structure in the data.", "year": 2007, "publicationdate": "2007-03-10", "externalids": {"DOI": "10.5555/1577069.1755846"}, "doi_lower": "10.5555/1577069.1755846"}
{"paper_id": 11492613, "title": "Learning Factored Representations in a Deep Mixture of Experts", "author_names": ["D. Eigen", "Marc'Aurelio Ranzato", "I. Sutskever"], "venue": "International Conference on Learning Representations", "abstract": "Mixtures of Experts combine the outputs of several \"expert\" networks, each of which specializes in a different part of the input space. This is achieved by training a \"gating\" network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent (\"where\") experts at the first layer, and class-specific (\"what\") experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "year": 2013, "publicationdate": "2013-12-16", "externalids": {}, "doi_lower": null}
{"paper_id": 2865509, "title": "Generative Image Modeling Using Spatial LSTMs", "author_names": ["Lucas Theis", "M. Bethge"], "venue": "Neural Information Processing Systems", "abstract": "Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multidimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.", "year": 2015, "publicationdate": "2015-06-10", "externalids": {}, "doi_lower": null}
{"paper_id": 17686175, "title": "Distributed Gaussian Processes", "author_names": ["M. Deisenroth", "Jun Wei Ng"], "venue": "International Conference on Machine Learning", "abstract": "To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-theart sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efficient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufficient computing resources our distributed GP model can handle arbitrarily large data sets.", "year": 2015, "publicationdate": "2015-02-10", "externalids": {}, "doi_lower": null}
{"paper_id": 914027, "title": "Expert Gate: Lifelong Learning with a Network of Experts", "author_names": ["Rahaf Aljundi", "Punarjay Chakravarty", "T. Tuytelaars"], "venue": "Computer Vision and Pattern Recognition", "abstract": "In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process, data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision which expert to deploy at test time. We introduce a set of gating autoencoders that learn a representation for the task at hand, and, at test time, automatically forward the test sample to the relevant expert. This also brings memory efficiency as only one expert network has to be loaded into memory at any given time. Further, the autoencoders inherently capture the relatedness of one task to another, based on which the most relevant prior model to be used for training a new expert, with fine-tuning or learning-without-forgetting, can be selected. We evaluate our method on image classification and video prediction problems.", "year": 2016, "publicationdate": "2016-11-18", "externalids": {"DOI": "10.1109/CVPR.2017.753"}, "doi_lower": "10.1109/cvpr.2017.753"}
{"paper_id": 12462234, "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "author_names": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc V. Le", "Geoffrey E. Hinton", "J. Dean"], "venue": "International Conference on Learning Representations", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "year": 2017, "publicationdate": "2017-01-23", "externalids": {}, "doi_lower": null}
{"paper_id": 266844877, "title": "Mixtral of Experts", "author_names": ["Albert Q. Jiang", "Alexandre Sablayrolles", "Antoine Roux", "A. Mensch", "Blanche Savary", "Chris Bamford", "Devendra Singh Chaplot", "Diego de Las Casas", "Emma Bou Hanna", "Florian Bressand", "Gianna Lengyel", "Guillaume Bour", "Guillaume Lample", "Lélio Renard Lavaud", "Lucile Saulnier", "M. Lachaux", "Pierre Stock", "Sandeep Subramanian", "Sophia Yang", "Szymon Antoniak", "Teven Le Scao", "Théophile Gervet", "Thibaut Lavril", "Thomas Wang", "Timothée Lacroix", "William El Sayed"], "venue": "arXiv.org", "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.", "year": 2024, "publicationdate": "2024-01-08", "externalids": {"DOI": "10.48550/arXiv.2401.04088"}, "doi_lower": "10.48550/arxiv.2401.04088"}
{"paper_id": 240245712, "title": "AI: The path to the intelligent enterprise", "author_names": [], "venue": "Jul/Aug 2018", "abstract": null, "year": 2018, "publicationdate": "2018-08-31", "externalids": {"DOI": "10.1287/lytx.2018.04.08"}, "doi_lower": "10.1287/lytx.2018.04.08"}
{"paper_id": 269613809, "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model", "author_names": ["Zhihong Shao", "Damai Dai", "Daya Guo", "Bo Liu (Benjamin Liu)", "Zihan Wang", "Huajian Xin"], "venue": "arXiv.org", "abstract": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.", "year": 2024, "publicationdate": "2024-05-07", "externalids": {"DOI": "10.48550/arXiv.2405.04434"}, "doi_lower": "10.48550/arxiv.2405.04434"}
{"paper_id": 9922492, "title": "Twenty Years of Mixture of Experts", "author_names": ["S. E. Yüksel", "Joseph N. Wilson", "P. Gader"], "venue": "IEEE Transactions on Neural Networks and Learning Systems", "abstract": null, "year": 2012, "publicationdate": "2012-06-11", "externalids": {"DOI": "10.1109/TNNLS.2012.2200299"}, "doi_lower": "10.1109/tnnls.2012.2200299"}
{"paper_id": 245124124, "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts", "author_names": ["Nan Du", "Yanping Huang", "Andrew M. Dai", "Simon Tong", "Dmitry Lepikhin", "Yuanzhong Xu", "M. Krikun", "Yanqi Zhou", "Adams Wei Yu", "Orhan Firat", "Barret Zoph", "L. Fedus", "Maarten Bosma", "Zongwei Zhou", "Tao Wang", "Yu Emma Wang", "Kellie Webster", "Marie Pellat", "Kevin Robinson", "K. Meier-Hellstern", "Toju Duke", "Lucas Dixon", "Kun Zhang", "Quoc V. Le", "Yonghui Wu", "Z. Chen", "Claire Cui"], "venue": "International Conference on Machine Learning", "abstract": "Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.", "year": 2021, "publicationdate": "2021-12-13", "externalids": {}, "doi_lower": null}
{"paper_id": 231573431, "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "author_names": ["W. Fedus", "Barret Zoph", "Noam Shazeer"], "venue": "Journal of machine learning research", "abstract": "In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the\"Colossal Clean Crawled Corpus\"and achieve a 4x speedup over the T5-XXL model.", "year": 2021, "publicationdate": "2021-01-11", "externalids": {}, "doi_lower": null}
{"paper_id": 267413104, "title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models", "author_names": ["Fuzhao Xue", "Zian Andy Zheng", "Yao Fu", "Jinjie Ni", "Zangwei Zheng", "Wangchunshu Zhou", "Yang You"], "venue": "International Conference on Machine Learning", "abstract": "To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development. One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped. Finally, we rethink our design based on the above-mentioned observations and analysis. To facilitate future MoE LLM development, we propose potential strategies for mitigating the issues we found and further improving off-the-shelf MoE LLM designs.", "year": 2024, "publicationdate": "2024-01-29", "externalids": {"DOI": "10.48550/arXiv.2402.01739"}, "doi_lower": "10.48550/arxiv.2402.01739"}
{"paper_id": 260378993, "title": "From Sparse to Soft Mixtures of Experts", "author_names": ["J. Puigcerver", "C. Riquelme", "Basil Mustafa", "N. Houlsby"], "venue": "International Conference on Learning Representations", "abstract": "Sparse mixture of expert architectures (MoEs) scale model capacity without significant increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose Soft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoEs, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity (and performance) at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms dense Transformers (ViTs) and popular MoEs (Tokens Choice and Experts Choice). Furthermore, Soft MoE scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40x more parameters than ViT Huge/14, with only 2% increased inference time, and substantially better quality.", "year": 2023, "publicationdate": "2023-08-02", "externalids": {"DOI": "10.48550/arXiv.2308.00951"}, "doi_lower": "10.48550/arxiv.2308.00951"}
{"paper_id": 268891288, "title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training", "author_names": ["Zexuan Zhong", "Mengzhou Xia", "Danqi Chen", "Mike Lewis"], "venue": "arXiv.org", "abstract": "Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed (Muqeeth et al., 2023), which softly merges experts in the parameter space; nevertheless, its effectiveness was only demonstrated in downstream fine-tuning on classification tasks. In this paper, we present Lory, the first approach that scales such architectures to autoregressive language model pre-training. Lory introduces two key techniques: (1) a causal segment routing strategy that achieves high efficiency for expert merging operations while preserving the autoregressive nature of language models; (2) a similarity-based data batching method that encourages expert specialization by grouping similar documents in training instances. We pre-train a series of Lory models on 150B tokens from scratch, with up to 32 experts and 30B (1.5B active) parameters. Experimental results show significant performance gains over parameter-matched dense models on both perplexity (+13.9%) and a variety of downstream tasks (+1.5%-11.1%). Despite segment-level routing, Lory models achieve competitive performance compared to state-of-the-art MoE models with token-level routing. We further demonstrate that the trained experts in Lory capture domain-level specialization without supervision. Our work highlights the potential of fully-differentiable MoE architectures for language model pre-training and advocates future research in this area.", "year": 2024, "publicationdate": "2024-05-06", "externalids": {"DOI": "10.48550/arXiv.2405.03133"}, "doi_lower": "10.48550/arxiv.2405.03133"}
{"paper_id": 261697072, "title": "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning", "author_names": ["Ted Zadouri", "A. Ustun", "Arash Ahmadian", "Beyza Ermics", "Acyr F. Locatelli", "Sara Hooker"], "venue": "International Conference on Learning Representations", "abstract": "The Mixture of Experts (MoE) is a widely known neural architecture where an ensemble of specialized sub-models optimizes overall performance with a constant computational cost. However, conventional MoEs pose challenges at scale due to the need to store all experts in memory. In this paper, we push MoE to the limit. We propose extremely parameter-efficient MoE by uniquely combining MoE architecture with lightweight experts.Our MoE architecture outperforms standard parameter-efficient fine-tuning (PEFT) methods and is on par with full fine-tuning by only updating the lightweight experts -- less than 1% of an 11B parameters model. Furthermore, our method generalizes to unseen tasks as it does not depend on any prior task knowledge. Our research underscores the versatility of the mixture of experts architecture, showcasing its ability to deliver robust performance even when subjected to rigorous parameter constraints. Our code used in all the experiments is publicly available here: https://github.com/for-ai/parameter-efficient-moe.", "year": 2023, "publicationdate": "2023-09-11", "externalids": {"DOI": "10.48550/arXiv.2309.05444"}, "doi_lower": "10.48550/arxiv.2309.05444"}
{"paper_id": 265609776, "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-Rank Experts", "author_names": ["Jialin Wu", "Xia Hu", "Yaqing Wang", "Bo Pang", "Radu Soricut"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Large multi-modal models (LMMs) exhibit remarkable performance across numerous tasks. However, generalist LMMs often suffer from performance degradation when tuned over a large collection of tasks. Recent research suggests that Mixture of Experts (MoE) architectures are useful for instruction tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost of replicating and storing the expert models severely limits the number of experts we can use. We propose Omni-SMoLA, an architecture that uses the Soft MoE approach to (softly) mix many multimodal low rank experts, and avoids introducing a significant number of new parameters compared to conventional MoE models. The core intuition here is that the large model provides a foundational backbone, while different lightweight experts residually learn specialized knowledge, either per-modality or multimodally. Extensive experiments demonstrate that the SMoLA approach helps improve the generalist performance across a broad range of generative vision-and-language tasks, achieving new SoTA generalist performance that often matches or outperforms single specialized LMM baselines, as well as new SoTA specialist performance.", "year": 2023, "publicationdate": "2023-12-01", "externalids": {"DOI": "10.1109/CVPR52733.2024.01347"}, "doi_lower": "10.1109/cvpr52733.2024.01347"}
{"paper_id": 253153886, "title": "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning", "author_names": ["Yaqing Wang", "Subhabrata Mukherjee", "Xiaodong Liu", "Jing Gao", "Jianfeng Gao"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Standard fine-tuning of large pre-trained language models (PLMs) for downstream tasks requires updating hundreds of millions to billions of parameters, and storing a large copy of the PLM weights for every task resulting in increased cost for storing, sharing and serving the models. To address this, parameter-efficient fine-tuning (PEFT) techniques were introduced where small trainable components are injected in the PLM and updated during fine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of adaptation modules – given the underlying PEFT method of choice – introduced in each Transformer layer while keeping most of the PLM weights frozen. For instance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture of low rank decomposition matrices like LoRA to improve downstream task performance over the corresponding PEFT methods for fully supervised and few-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the same computational cost and the number of tunable parameters as the underlying PEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix outperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for both NLU and NLG tasks.", "year": 2022, "publicationdate": "2022-05-24", "externalids": {"DOI": "10.48550/arXiv.2210.17451"}, "doi_lower": "10.48550/arxiv.2210.17451"}
{"paper_id": 266362594, "title": "Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning", "author_names": ["Yunhao Gou", "Zhili Liu", "Kai Chen", "Lanqing Hong", "Hang Xu", "Aoxue Li", "Dit-Yan Yeung", "James T. Kwok", "Yu Zhang"], "venue": "arXiv.org", "abstract": "Instruction tuning of Large Vision-language Models (LVLMs) has revolutionized the development of versatile models with zero-shot generalization across a wide range of downstream vision-language tasks. However, the diversity of training tasks of different sources and formats would lead to inevitable task conflicts, where different tasks conflict for the same set of model parameters, resulting in sub-optimal instruction-following abilities. To address that, we propose the Mixture of Cluster-conditional LoRA Experts (MoCLE), a novel Mixture of Experts (MoE) architecture designed to activate the task-customized model parameters based on the instruction clusters. A separate universal expert is further incorporated to improve generalization capabilities of MoCLE for novel instructions. Extensive experiments on InstructBLIP and LLaVA demonstrate the effectiveness of MoCLE.", "year": 2023, "publicationdate": "2023-12-19", "externalids": {"DOI": "10.48550/arXiv.2312.12379"}, "doi_lower": "10.48550/arxiv.2312.12379"}
{"paper_id": 267759827, "title": "MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models", "author_names": ["Tongxu Luo", "Jiahe Lei", "Fangyu Lei", "Weihao Liu", "Shizhu He", "Jun Zhao", "Kang Liu"], "venue": "arXiv.org", "abstract": "Fine-tuning is often necessary to enhance the adaptability of Large Language Models (LLM) to downstream tasks. Nonetheless, the process of updating billions of parameters demands significant computational resources and training time, which poses a substantial obstacle to the widespread application of large-scale models in various scenarios. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) has emerged as a prominent paradigm in recent research. However, current PEFT approaches that employ a limited set of global parameters (such as LoRA, which adds low-rank approximation matrices to all weights) face challenges in flexibly combining different computational modules in downstream tasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon observed in MoE, we propose the utilization of contrastive learning to encourage experts to learn distinct features. We conducted experiments on 11 tasks in math reasoning and common-sense reasoning benchmarks. With the same number of parameters, our approach outperforms LoRA significantly. In math reasoning, MoELoRA achieved an average performance that was 4.2% higher than LoRA, and demonstrated competitive performance compared to the 175B GPT-3.5 on several benchmarks.", "year": 2024, "publicationdate": "2024-02-20", "externalids": {"DOI": "10.48550/arXiv.2402.12851"}, "doi_lower": "10.48550/arxiv.2402.12851"}
{"paper_id": 269293160, "title": "Mixture of LoRA Experts", "author_names": ["Xun Wu", "Shaohan Huang", "Furu Wei"], "venue": "International Conference on Learning Representations", "abstract": "LoRA has gained widespread acceptance in the fine-tuning of large pre-trained models to cater to a diverse array of downstream tasks, showcasing notable effectiveness and efficiency, thereby solidifying its position as one of the most prevalent fine-tuning techniques. Due to the modular nature of LoRA's plug-and-play plugins, researchers have delved into the amalgamation of multiple LoRAs to empower models to excel across various downstream tasks. Nonetheless, extant approaches for LoRA fusion grapple with inherent challenges. Direct arithmetic merging may result in the loss of the original pre-trained model's generative capabilities or the distinct identity of LoRAs, thereby yielding suboptimal outcomes. On the other hand, Reference tuning-based fusion exhibits limitations concerning the requisite flexibility for the effective combination of multiple LoRAs. In response to these challenges, this paper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses hierarchical control and unfettered branch selection. The MoLE approach not only achieves superior LoRA fusion performance in comparison to direct arithmetic merging but also retains the crucial flexibility for combining LoRAs effectively. Extensive experimental evaluations conducted in both the Natural Language Processing (NLP) and Vision&Language (V&L) domains substantiate the efficacy of MoLE.", "year": 2024, "publicationdate": "2024-04-21", "externalids": {"DOI": "10.48550/arXiv.2404.13628"}, "doi_lower": "10.48550/arxiv.2404.13628"}
{"paper_id": 254535822, "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints", "author_names": ["Aran Komatsuzaki", "J. Puigcerver", "J. Lee-Thorp", "Carlos Riquelme Ruiz", "Basil Mustafa", "J. Ainslie", "Yi Tay", "Mostafa Dehghani", "N. Houlsby"], "venue": "International Conference on Learning Representations", "abstract": "Training large, deep neural networks to convergence can be prohibitively expensive. As a result, often only a small selection of popular, dense models are reused across different contexts and tasks. Increasingly, sparsely activated models, which seek to decouple model size from computation costs, are becoming an attractive alternative to dense models. Although more efficient in terms of quality and computation cost, sparse models remain data-hungry and costly to train from scratch in the large scale regime. In this work, we propose sparse upcycling -- a simple way to reuse sunk training costs by initializing a sparsely activated Mixture-of-Experts model from a dense checkpoint. We show that sparsely upcycled T5 Base, Large, and XL language models and Vision Transformer Base and Large models, respectively, significantly outperform their dense counterparts on SuperGLUE and ImageNet, using only ~50% of the initial dense pretraining sunk cost. The upcycled models also outperform sparse models trained from scratch on 100% of the initial dense pretraining computation budget.", "year": 2022, "publicationdate": "2022-12-09", "externalids": {"DOI": "10.48550/arXiv.2212.05055"}, "doi_lower": "10.48550/arxiv.2212.05055"}
{"paper_id": 247958465, "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts", "author_names": ["Zhengyan Zhang", "Yankai Lin", "Zhiyuan Liu", "Peng Li", "Maosong Sun", "Jie Zhou"], "venue": "Findings", "abstract": "Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain. To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication. Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input. Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks. Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs. The source code of this paper can be obtained from https://github.com/thunlp/MoEfication.", "year": 2021, "publicationdate": "2021-10-05", "externalids": {"DOI": "10.18653/v1/2022.findings-acl.71"}, "doi_lower": "10.18653/v1/2022.findings-acl.71"}
{"paper_id": 246285330, "title": "One Student Knows All Experts Know: From Sparse to Dense", "author_names": ["Fuzhao Xue", "Xiaoxin He", "Xiaozhe Ren", "Yuxuan Lou", "Yang You"], "venue": "Tiny Papers @ ICLR", "abstract": "Human education system trains one student by multiple experts. Mixture-of-experts (MoE) is a powerful sparse architecture including multiple experts. However, sparse MoE model is easy to overfit, hard to deploy, and not hardware-friendly for practitioners. In this work, inspired by the human education model, we propose a novel task, knowledge integration, to obtain a dense student model (OneS) as knowledgeable as one sparse MoE. We investigate this task by proposing a general training framework including knowledge gathering and knowledge distillation. Specifically, to gather key knowledge from different pre-trained experts, we first investigate four different possible knowledge gathering methods, \\ie summation, averaging, Top-K Knowledge Gathering (Top-KG), and Singular Value Decomposition Knowledge Gathering (SVD-KG) proposed in this paper. We then refine the dense student model by knowledge distillation to offset the noise from gathering. On ImageNet, our OneS preserves $61.7\\%$ benefits from MoE and achieves $78.4\\%$ top-1 accuracy ImageNet with only $15$M parameters. On four natural language processing datasets, OneS obtains $88.2\\%$ MoE benefits and outperforms the best baseline by $51.7\\%$ using the same architecture and training data. In addition, compared with the MoE counterpart, OneS can achieve $3.7 \\times$ inference speedup due to less computation and hardware-friendly architecture.", "year": 2022, "publicationdate": "2022-01-26", "externalids": {}, "doi_lower": null}
{"paper_id": 249240535, "title": "Task-Specific Expert Pruning for Sparse Mixture-of-Experts", "author_names": ["Tianyu Chen", "Shaohan Huang", "Yuan Xie", "Binxing Jiao", "Daxin Jiang", "Haoyi Zhou", "Jianxin Li", "Furu Wei"], "venue": "arXiv.org", "abstract": "The sparse Mixture-of-Experts (MoE) model is powerful for large-scale pre-training and has achieved promising results due to its model capacity. However, with trillions of parameters, MoE is hard to be deployed on cloud or mobile environment. The inference of MoE requires expert parallelism, which is not hardware-friendly and communication expensive. Especially for resource-limited downstream tasks, such sparse structure has to sacrifice a lot of computing efficiency for limited performance gains. In this work, we observe most experts contribute scarcely little to the MoE fine-tuning and inference. We further propose a general method to progressively drop the non-professional experts for the target downstream task, which preserves the benefits of MoE while reducing the MoE model into one single-expert dense model. Our experiments reveal that the fine-tuned single-expert model could preserve 99.3% benefits from MoE across six different types of tasks while enjoying 2x inference speed with free communication cost.", "year": 2022, "publicationdate": "2022-06-01", "externalids": {"DOI": "10.48550/arXiv.2206.00277"}, "doi_lower": "10.48550/arxiv.2206.00277"}
{"paper_id": 268363969, "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM", "author_names": ["Sainbayar Sukhbaatar", "Olga Golovneva", "Vasu Sharma", "Hu Xu", "Xi Victoria Lin", "Baptiste Rozière", "Jacob Kahn", "Shang-Wen Li", "Wen-tau Yih", "Jason E. Weston", "Xian Li"], "venue": "arXiv.org", "abstract": "We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.", "year": 2024, "publicationdate": "2024-03-12", "externalids": {"DOI": "10.48550/arXiv.2403.07816"}, "doi_lower": "10.48550/arxiv.2403.07816"}
{"paper_id": 258833488, "title": "Lifelong Language Pretraining with Distribution-Specialized Experts", "author_names": ["Wuyang Chen", "Yan-Quan Zhou", "Nan Du", "Yanping Huang", "J. Laudon", "Z. Chen", "Claire Cu"], "venue": "International Conference on Machine Learning", "abstract": "Pretraining on a large-scale corpus has become a standard method to build general language models (LMs). Adapting a model to new data distributions targeting different downstream tasks poses significant challenges. Naive fine-tuning may incur catastrophic forgetting when the over-parameterized LMs overfit the new data but fail to preserve the pretrained features. Lifelong learning (LLL) aims to enable information systems to learn from a continuous data stream across time. However, most prior work modifies the training recipe assuming a static fixed network architecture. We find that additional model capacity and proper regularization are key elements to achieving strong LLL performance. Thus, we propose Lifelong-MoE, an extensible MoE (Mixture-of-Experts) architecture that dynamically adds model capacity via adding experts with regularized pretraining. Our results show that by only introducing a limited number of extra experts while keeping the computation cost constant, our model can steadily adapt to data distribution shifts while preserving the previous knowledge. Compared to existing lifelong learning approaches, Lifelong-MoE achieves better few-shot performance on 19 downstream NLP tasks.", "year": 2023, "publicationdate": "2023-05-20", "externalids": {"DOI": "10.48550/arXiv.2305.12281"}, "doi_lower": "10.48550/arxiv.2305.12281"}
{"paper_id": 264439523, "title": "Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation", "author_names": ["Szymon Antoniak", "Sebastian Jaszczur", "Michal Krutul", "Maciej Pi'oro", "Jakub Krajewski", "Jan Ludziejewski", "Tomasz Odrzyg'o'zd'z", "Marek Cygan"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2310.15961"}, "doi_lower": "10.48550/arxiv.2310.15961"}
{"paper_id": 268876220, "title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models", "author_names": ["David Raposo", "Sam Ritter", "Blake A. Richards", "T. Lillicrap", "Peter Humphreys", "Adam Santoro"], "venue": "arXiv.org", "abstract": "Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\\% faster to step during post-training sampling.", "year": 2024, "publicationdate": "2024-04-02", "externalids": {"DOI": "10.48550/arXiv.2404.02258"}, "doi_lower": "10.48550/arxiv.2404.02258"}
{"paper_id": 236428967, "title": "Go Wider Instead of Deeper", "author_names": ["Fuzhao Xue", "Ziji Shi", "Futao Wei", "Yuxuan Lou", "Yong Liu", "Yang You"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "More transformer blocks with residual connections have recently achieved impressive results on various tasks. To achieve better performance with fewer trainable parameters, recent methods are proposed to go shallower by parameter sharing or model compressing along with the depth. However, weak modeling capacity limits their performance. Contrastively, going wider by inducing more trainable matrixes and parameters would produce a huge model requiring advanced parallelism to train and inference. \n \n \n In this paper, we propose a parameter-efficient framework, going wider instead of deeper. Specially, following existing works, we adapt parameter sharing to compress along depth. But, such deployment would limit the performance. To maximize modeling capacity, we scale along model width by replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across transformer blocks, instead of sharing normalization layers, we propose to use individual layernorms to transform various semantic representations in a more parameter-efficient way. To evaluate our plug-and-run framework, we design WideNet and conduct comprehensive experiments on popular computer vision and natural language processing benchmarks. On ImageNet-1K, our best model outperforms Vision Transformer (ViT) by 1.5% with 0.72 times trainable parameters. Using 0.46 times and 0.13 times parameters, our WideNet can still surpass ViT and ViT-MoE by 0.8% and 2.1%, respectively. On four natural language processing datasets, WideNet outperforms ALBERT by 1.8% on average and surpass BERT using factorized embedding parameterization by 0.8% with fewer parameters.", "year": 2021, "publicationdate": "2021-07-25", "externalids": {"DOI": "10.1609/aaai.v36i8.20858"}, "doi_lower": "10.1609/aaai.v36i8.20858"}
{"paper_id": 263834790, "title": "Sparse Universal Transformer", "author_names": ["Shawn Tan", "Yikang Shen", "Zhenfang Chen", "Aaron C. Courville", "Chuang Gan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT'14 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50\\% reduction in computation during inference with very little performance decrease on formal language tasks.", "year": 2023, "publicationdate": "2023-10-11", "externalids": {"DOI": "10.48550/arXiv.2310.07096"}, "doi_lower": "10.48550/arxiv.2310.07096"}
{"paper_id": 266163946, "title": "SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts", "author_names": ["Joon-Young Choi", "Junho Kim", "Jun-Hyung Park", "Wing-Lam Mok", "SangKeun Lee"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Prompt tuning has emerged as a successful parameter-efficient alternative to the full fine-tuning of language models. However, prior works on prompt tuning often utilize long soft prompts of up to 100 tokens to improve performance, overlooking the inefficiency associated with extended inputs. In this paper, we propose a novel prompt tuning method SMoP ( S parse M ixture-o f-P rompts) that utilizes short soft prompts for efficient training and inference while maintaining performance gains typically induced from longer soft prompts. To achieve this, SMoP employs a gating mechanism to train multiple short soft prompts specialized in handling different subsets of the data, providing an alternative to relying on a single long soft prompt to cover the entire data. Experimental results demonstrate that SMoP outperforms baseline methods while reducing training and inference costs. We release our code at https://github.com/jyjohnchoi/SMoP .", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.emnlp-main.884"}, "doi_lower": "10.18653/v1/2023.emnlp-main.884"}
{"paper_id": 252780015, "title": "EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate", "author_names": ["Xiaonan Nie", "Xupeng Miao", "Shijie Cao", "Lingxiao Ma", "Qibin Liu", "Jilong Xue", "Youshan Miao", "Yi Liu", "Zhi Yang", "Bin Cui"], "venue": "", "abstract": "Mixture-of-experts (MoE) is becoming popular due to its success in improving the model quality, especially in Transformers. By routing tokens with a sparse gate to a few experts (i.e., a small pieces of the full model), MoE can easily increase the model parameters to a very large scale while keeping the computation cost in a constant level. Most existing works just initialize some random experts, set a fixed gating strategy (e.g., Top-k), and train the model from scratch in an ad-hoc way. We identify that these MoE models are suffering from the immature experts and unstable sparse gate, which are harmful to the convergence performance. In this paper, we propose an efficient end-to-end MoE training framework called EvoMoE. EvoMoE starts from training one single expert and gradually evolves into a large and sparse MoE structure. EvoMoE mainly contains two phases: the expert-diversify phase to train the base expert for a while and spawn multiple diverse experts from it, and the gate-sparsify phase to learn an adaptive sparse gate and activate a dynamic number of experts. EvoMoE naturally decouples the joint learning of both the experts and the sparse gate and focuses on learning the basic knowledge with a single expert at the early training stage. Then it diversifies the experts and continues to train the MoE with a novel Dense-to-Sparse gate (DTS-Gate). Specifically, instead of using a permanent sparse gate, DTS-Gate begins as a dense gate that routes tokens to all experts, then gradually and adaptively becomes sparser while routes to fewer experts. Evaluations are conducted on three popular models and tasks, including RoBERTa for masked language modeling task, GPT for language modeling task and Transformer for machine translation task. The results show that EvoMoE outperforms existing baselines, including Switch, BASE Layer, Hash Layer and StableMoE.", "year": 2021, "publicationdate": "2021-12-29", "externalids": {}, "doi_lower": null}
{"paper_id": 281681512, "title": "LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts", "author_names": ["Zhuang Yuan", "Yi Shen", "Yuexin Bian", "Qing Su", "Shihao Ji", "Yuanyuan Shi", "Fei Miao"], "venue": "arXiv.org", "abstract": "Recent studies have shown that combining parameter-efficient fine-tuning (PEFT) with mixture-of-experts (MoE) is an effective strategy for adapting large language models (LLMs) to the downstream tasks. However, most existing approaches rely on conventional TopK routing, which requires careful hyperparameter tuning and assigns a fixed number of experts to each token. In this work, we propose LD-MoLE, a Learnable Dynamic routing mechanism for Mixture of LoRA Experts that enables adaptive, token-dependent, and layer-wise expert allocation. Our method replaces the non-differentiable TopK selection with a differentiable routing function and a closed-form solution. Moreover, our design allows the model to adaptively determine the number of experts to activate for each token at different layers. In addition, we introduce an analytical sparsity control objective to regularize the number of activated experts. Extensive experiments on the Qwen3-1.7B and Llama-3.2-3B models show that LD-MoLE achieves the highest average scores compared to state-of-the-art baselines, across a diverse set of benchmarks. Our method not only achieves superior performance, but also demonstrates the ability to learn token-dependent and layer-wise expert allocation.", "year": 2025, "publicationdate": "2025-09-30", "externalids": {"DOI": "10.48550/arXiv.2509.25684"}, "doi_lower": "10.48550/arxiv.2509.25684"}
{"paper_id": 246473179, "title": "Unified Scaling Laws for Routed Language Models", "author_names": ["Aidan Clark", "Diego de Las Casas", "Aurelia Guy", "A. Mensch", "Michela Paganini", "Jordan Hoffmann", "Bogdan Damoc", "Blake A. Hechtman", "Trevor Cai", "Sebastian Borgeaud", "George van den Driessche", "Eliza Rutherford", "T. Hennigan", "Matthew G. Johnson", "Katie Millican", "Albin Cassirer", "Chris Jones", "Elena Buchatskaya", "D. Budden", "L. Sifre", "Simon Osindero", "O. Vinyals", "Jack W. Rae", "Erich Elsen", "K. Kavukcuoglu", "K. Simonyan"], "venue": "International Conference on Machine Learning", "abstract": "The performance of a language model has been shown to be effectively modeled as a power-law in its parameter count. Here we study the scaling behaviors of Routing Networks: architectures that conditionally use only a subset of their parameters while processing an input. For these models, parameter count and computational requirement form two independent axes along which an increase leads to better performance. In this work we derive and justify scaling laws defined on these two variables which generalize those known for standard language models and describe the performance of a wide range of routing architectures trained via three different techniques. Afterwards we provide two applications of these laws: first deriving an Effective Parameter Count along which all models scale at the same rate, and then using the scaling coefficients to give a quantitative comparison of the three routing techniques considered. Our analysis derives from an extensive evaluation of Routing Networks across five orders of magnitude of size, including models with hundreds of experts and hundreds of billions of parameters.", "year": 2022, "publicationdate": "2022-02-02", "externalids": {}, "doi_lower": null}
{"paper_id": 270379962, "title": "Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models", "author_names": ["Tianwen Wei", "Bo Zhu", "Liang Zhao", "Cheng Cheng", "Biye Li", "Weiwei Lü", "Peng Cheng", "Jianhao Zhang", "Xiaoyu Zhang", "Liang Zeng", "Xiaokun Wang", "Yutuan Ma", "Rui Hu", "Shuicheng Yan", "Han Fang", "Yahui Zhou"], "venue": "arXiv.org", "abstract": "In this technical report, we introduce the training methodologies implemented in the development of Skywork-MoE, a high-performance mixture-of-experts (MoE) large language model (LLM) with 146 billion parameters and 16 experts. It is initialized from the pre-existing dense checkpoints of our Skywork-13B model. We explore the comparative effectiveness of upcycling versus training from scratch initializations. Our findings suggest that the choice between these two approaches should consider both the performance of the existing dense checkpoints and the MoE training budget. We highlight two innovative techniques: gating logit normalization, which improves expert diversification, and adaptive auxiliary loss coefficients, allowing for layer-specific adjustment of auxiliary loss coefficients. Our experimental results validate the effectiveness of these methods. Leveraging these techniques and insights, we trained our upcycled Skywork-MoE on a condensed subset of our SkyPile corpus. The evaluation results demonstrate that our model delivers strong performance across a wide range of benchmarks.", "year": 2024, "publicationdate": "2024-06-03", "externalids": {"DOI": "10.48550/arXiv.2406.06563"}, "doi_lower": "10.48550/arxiv.2406.06563"}
{"paper_id": 268793596, "title": "Jamba: A Hybrid Transformer-Mamba Language Model", "author_names": ["Opher Lieber", "Barak Lenz", "Hofit Bata", "Gal Cohen", "Jhonathan Osin", "Itay Dalmedigos", "Erez Safahi", "S. Meirom", "Yonatan Belinkov", "Shai Shalev-Shwartz", "Omri Abend", "Raz Alon", "Tomer Asida", "Amir Bergman", "Roman Glozman", "Michael Gokhman", "Avshalom Manevich", "Nir Ratner", "N. Rozen", "Erez Shwartz", "Mor Zusman", "Y. Shoham"], "venue": "arXiv.org", "abstract": "We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.", "year": 2024, "publicationdate": "2024-03-28", "externalids": {"DOI": "10.48550/arXiv.2403.19887"}, "doi_lower": "10.48550/arxiv.2403.19887"}
{"paper_id": 266933338, "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models", "author_names": ["Damai Dai", "C. Deng", "Chenggang Zhao", "R. Xu", "Huazuo Gao", "Deli Chen", "Jiashi Li", "Wangding Zeng", "Xingkai Yu", "Yu Wu", "Zhenda Xie", "Y. K. Li", "Panpan Huang", "Fuli Luo", "C. Ruan", "Zhifang Sui", "W. Liang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.", "year": 2024, "publicationdate": "2024-01-11", "externalids": {"DOI": "10.48550/arXiv.2401.06066"}, "doi_lower": "10.48550/arxiv.2401.06066"}
{"paper_id": 254685665, "title": "Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners", "author_names": ["Z. Chen", "Yikang Shen", "Mingyu Ding", "Zhenfang Chen", "Hengshuang Zhao", "E. Learned-Miller", "Chuang Gan"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Optimization in multi-task learning (MTL) is more challenging than single-task learning (STL), as the gradient from different tasks can be contradictory. When tasks are related, it can be beneficial to share some parameters among them (cooperation). However, some tasks require additional parameters with expertise in a specific type of data or discrimination (specialization). To address the MTL challenge, we propose Mod-Squad, a new model that is Modularized into groups of experts (a ‘Squad’). This structure allows us to formalize cooperation and specialization as the process of matching experts and tasks. We optimize this matching process during the training of a single model. Specifically, we incorporate mixture of experts (MoE) layers into a transformer model, with a new loss that incorporates the mutual dependence between tasks and experts. As a result, only a small set of experts are activated for each task. This prevents the sharing of the entire backbone model between all tasks, which strengthens the model, especially when the training set size and the number of tasks scale up. More interestingly, for each task, we can extract the small set of experts as a standalone model that maintains the same performance as the large model. Extensive experiments on the Taskonomy dataset with 13 vision tasks and the PASCAL-Context dataset with 5 vision tasks show the superiority of our approach. The project page can be accessed at https://vis-www.cs.umass.edu/Mod-Squad.", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.1109/CVPR52729.2023.01138"}, "doi_lower": "10.1109/cvpr52729.2023.01138"}
{"paper_id": 248227505, "title": "StableMoE: Stable Routing Strategy for Mixture of Experts", "author_names": ["Damai Dai", "Li Dong", "Shuming Ma", "Bo Zheng", "Zhifang Sui", "Baobao Chang", "Furu Wei"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The Mixture-of-Experts (MoE) technique can scale up the model size of Transformers with an affordable computational overhead. We point out that existing learning-to-route MoE methods suffer from the routing fluctuation issue, i.e., the target expert of the same input may change along with training, but only one expert will be activated for the input during inference. The routing fluctuation tends to harm sample efficiency because the same input updates different experts but only one is finally used. In this paper, we propose StableMoE with two training stages to address the routing fluctuation problem. In the first training stage, we learn a balanced and cohesive routing strategy and distill it into a lightweight router decoupled from the backbone model. In the second training stage, we utilize the distilled router to determine the token-to-expert assignment and freeze it for a stable routing strategy. We validate our method on language modeling and multilingual machine translation. The results show that StableMoE outperforms existing MoE methods in terms of both convergence speed and performance.", "year": 2022, "publicationdate": "2022-04-18", "externalids": {"DOI": "10.48550/arXiv.2204.08396"}, "doi_lower": "10.48550/arxiv.2204.08396"}
{"paper_id": 259096191, "title": "ModuleFormer: Learning Modular Large Language Models From Uncurated Data", "author_names": ["Yikang Shen", "Zheyu Zhang", "Tianyou Cao", "Shawn Tan", "Zhenfang Chen", "Chuang Gan"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.04640"}, "doi_lower": "10.48550/arxiv.2306.04640"}
{"paper_id": 232428341, "title": "BASE Layers: Simplifying Training of Large, Sparse Models", "author_names": ["M. Lewis", "Shruti Bhosale", "Tim Dettmers", "Naman Goyal", "Luke Zettlemoyer"], "venue": "International Conference on Machine Learning", "abstract": "We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released at https://github.com/pytorch/fairseq/", "year": 2021, "publicationdate": "2021-03-30", "externalids": {}, "doi_lower": null}
{"paper_id": 235358484, "title": "DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning", "author_names": ["Hussein Hazimeh", "Zhe Zhao", "A. Chowdhery", "M. Sathiamoorthy", "Yihua Chen", "R. Mazumder", "Lichan Hong", "Ed H. Chi"], "venue": "Neural Information Processing Systems", "abstract": "The Mixture-of-Experts (MoE) architecture is showing promising results in improving parameter sharing in multi-task learning (MTL) and in scaling high-capacity neural networks. State-of-the-art MoE models use a trainable sparse gate to select a subset of the experts for each input example. While conceptually appealing, existing sparse gates, such as Top-k, are not smooth. The lack of smoothness can lead to convergence and statistical performance issues when training with gradient-based methods. In this paper, we develop DSelect-k: a continuously differentiable and sparse gate for MoE, based on a novel binary encoding formulation. The gate can be trained using first-order methods, such as stochastic gradient descent, and offers explicit control over the number of experts to select. We demonstrate the effectiveness of DSelect-k on both synthetic and real MTL datasets with up to $128$ tasks. Our experiments indicate that DSelect-k can achieve statistically significant improvements in prediction and expert selection over popular MoE gates. Notably, on a real-world, large-scale recommender system, DSelect-k achieves over $22\\%$ improvement in predictive performance compared to Top-k. We provide an open-source implementation of DSelect-k.", "year": 2021, "publicationdate": "2021-06-07", "externalids": {}, "doi_lower": null}
{"paper_id": 237593026, "title": "Scalable and Efficient MoE Training for Multitask Multilingual Models", "author_names": ["Young Jin Kim", "A. A. Awan", "Alexandre Muzio", "Andres Felipe Cruz Salinas", "Liyang Lu", "Amr Hendy", "Samyam Rajbhandari", "Yuxiong He", "H. Awadalla"], "venue": "arXiv.org", "abstract": "The Mixture of Experts (MoE) models are an emerging class of sparsely activated deep learning models that have sublinear compute costs with respect to their parameters. In contrast with dense models, the sparse architecture of MoE offers opportunities for drastically growing model size with significant accuracy gain while consuming much lower compute budget. However, supporting large scale MoE training also has its own set of system and modeling challenges. To overcome the challenges and embrace the opportunities of MoE, we first develop a system capable of scaling MoE models efficiently to trillions of parameters. It combines multi-dimensional parallelism and heterogeneous memory technologies harmoniously with MoE to empower 8x larger models on the same hardware compared with existing work. Besides boosting system efficiency, we also present new training methods to improve MoE sample efficiency and leverage expert pruning strategy to improve inference time efficiency. By combining the efficient system and training methods, we are able to significantly scale up large multitask multilingual models for language generation which results in a great improvement in model accuracy. A model trained with 10 billion parameters on 50 languages can achieve state-of-the-art performance in Machine Translation (MT) and multilingual natural language generation tasks. The system support of efficient MoE training has been implemented and open-sourced with the DeepSpeed library.", "year": 2021, "publicationdate": "2021-09-22", "externalids": {}, "doi_lower": null}
{"paper_id": 238531628, "title": "Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference", "author_names": ["Sneha Kudugunta", "Yanping Huang", "Ankur Bapna", "M. Krikun", "Dmitry Lepikhin", "Minh-Thang Luong", "Orhan Firat"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Sparse Mixture-of-Experts (MoE) has been a successful approach for scaling multilingual translation models to billions of parameters without a proportional increase in training computation. However, MoE models are prohibitively large and practitioners often resort to methods such as distillation for serving. In this work, we investigate routing strategies at different granularity (token, sentence, task) in MoE models to bypass distillation. Experiments on WMT and a web-scale dataset suggest that task-level routing (task-MoE) enables us to extract smaller, ready-to-deploy sub-networks from large sparse models. On WMT, our task-MoE with 32 experts (533M parameters) outperforms the best performing token-level MoE model (token-MoE) by +1.0 BLEU on average across 30 language pairs. The peak inference throughput is also improved by a factor of 1.9x when we route by tasks instead of tokens. While distilling a token-MoE to a smaller dense model preserves only 32% of the BLEU gains, our sub-network task-MoE, by design, preserves all the gains with the same inference cost as the distilled student model. Finally, when scaling up to 200 language pairs, our 128-expert task-MoE (13B parameters) performs competitively with a token-level counterpart, while improving the peak inference throughput by a factor of 2.6x.", "year": 2021, "publicationdate": "2021-09-24", "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.304"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.304"}
{"paper_id": 250425961, "title": "No Language Left Behind: Scaling Human-Centered Machine Translation", "author_names": ["Nllb team", "M. Costa-jussà", "James Cross", "Onur cCelebi", "Maha Elbayad", "Kenneth Heafield", "Kevin Heffernan", "Elahe Kalbassi", "Janice Lam", "Daniel Licht", "Jean Maillard", "Anna Sun", "Skyler Wang", "Guillaume Wenzek", "Alison Youngblood", "Bapi Akula", "Loïc Barrault", "Gabriel Mejia Gonzalez", "Prangthip Hansanti", "John Hoffman", "Semarley Jarrett", "Kaushik Ram Sadagopan", "Dirk Rowe", "Shannon L. Spruit", "C. Tran", "Pierre Yves Andrews", "Necip Fazil Ayan", "Shruti Bhosale", "Sergey Edunov", "Angela Fan", "Cynthia Gao", "Vedanuj Goswami", "F. Guzm’an", "Philipp Koehn", "Alexandre Mourachko", "C. Ropers", "Safiyyah Saleem", "Holger Schwenk", "Jeff Wang"], "venue": "arXiv.org", "abstract": "Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.", "year": 2022, "publicationdate": "2022-07-11", "externalids": {"DOI": "10.48550/arXiv.2207.04672"}, "doi_lower": "10.48550/arxiv.2207.04672"}
{"paper_id": 248266346, "title": "On the Representation Collapse of Sparse Mixture of Experts", "author_names": ["Zewen Chi", "Li Dong", "Shaohan Huang", "Damai Dai", "Shuming Ma", "Barun Patra", "Saksham Singhal", "Payal Bajaj", "Xia Song", "Furu Wei"], "venue": "Neural Information Processing Systems", "abstract": "Sparse mixture of experts provides larger model capacity while requiring a constant computational overhead. It employs the routing mechanism to distribute input tokens to the best-matched experts according to their hidden representations. However, learning such a routing mechanism encourages token clustering around expert centroids, implying a trend toward representation collapse. In this work, we propose to estimate the routing scores between tokens and experts on a low-dimensional hypersphere. We conduct extensive experiments on cross-lingual language model pre-training and fine-tuning on downstream tasks. Experimental results across seven multilingual benchmarks show that our method achieves consistent gains. We also present a comprehensive analysis on the representation and routing behaviors of our models. Our method alleviates the representation collapse issue and achieves more consistent routing than the baseline mixture-of-experts methods.", "year": 2022, "publicationdate": "2022-04-20", "externalids": {"DOI": "10.48550/arXiv.2204.09179"}, "doi_lower": "10.48550/arxiv.2204.09179"}
{"paper_id": 249538647, "title": "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs", "author_names": ["Jinguo Zhu", "Xizhou Zhu", "Wenhai Wang", "Xiaohua Wang", "Hongsheng Li", "Xiaogang Wang", "Jifeng Dai"], "venue": "Neural Information Processing Systems", "abstract": "To build an artificial neural network like the biological intelligence system, recent works have unified numerous tasks into a generalist model, which can process various tasks with shared parameters and do not have any task-specific modules. While generalist models achieve promising results on various benchmarks, they have performance degradation on some tasks compared with task-specialized models. In this work, we find that interference among different tasks and modalities is the main factor to this phenomenon. To mitigate such interference, we introduce the Conditional Mixture-of-Experts (Conditional MoEs) to generalist models. Routing strategies under different levels of conditions are proposed to take both the training/inference cost and generalization ability into account. By incorporating the proposed Conditional MoEs, the recently proposed generalist model Uni-Perceiver can effectively mitigate the interference across tasks and modalities, and achieves state-of-the-art results on a series of downstream tasks via prompt tuning on 1% of downstream data. Moreover, the introduction of Conditional MoEs still holds the generalization ability of generalist models to conduct zero-shot inference on new tasks, e.g., video-text retrieval and video caption. Code and pre-trained generalist models shall be released.", "year": 2022, "publicationdate": "2022-06-09", "externalids": {"DOI": "10.48550/arXiv.2206.04674"}, "doi_lower": "10.48550/arxiv.2206.04674"}
{"paper_id": 252815815, "title": "Mixture of Attention Heads: Selecting Attention Heads Per Token", "author_names": ["Xiaofeng Zhang", "Yikang Shen", "Zeyu Huang", "Jie Zhou", "Wenge Rong", "Zhang Xiong"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Mixture-of-Experts (MoE) networks have been proposed as an efficient way to scale up model capacity and implement conditional computing. However, the study of MoE components mostly focused on the feedforward layer in Transformer architecture. This paper proposes the Mixture of Attention Heads (MoA), a new architecture that combines multi-head attention with the MoE mechanism. MoA includes a set of attention heads that each has its own set of parameters. Given an input, a router dynamically selects a subset of k attention heads per token. This conditional computation schema allows MoA to achieve stronger performance than the standard multi-head attention layer. Furthermore, the sparsely gated MoA can easily scale up the number of attention heads and the number of parameters while preserving computational efficiency. Despite performance improvements, MoA also automatically differentiates heads’ utilities, providing a new perspective to discuss the model’s interpretability. We conducted experiments on several important tasks, including Machine Translation and Masked Language Modeling. Experiments have shown promising results on several tasks against strong baselines that involve large and very deep models.", "year": 2022, "publicationdate": "2022-10-11", "externalids": {"DOI": "10.48550/arXiv.2210.05144"}, "doi_lower": "10.48550/arxiv.2210.05144"}
{"paper_id": 270067526, "title": "Yuan 2.0-M32: Mixture of Experts with Attention Router", "author_names": ["Shaohua Wu", "Jiangang Luo", "Xi Chen", "Lingjun Li", "Xudong Zhao", "Tong Yu", "Chao Wang", "Yue Wang", "Fei Wang", "Weixu Qiao", "Houbo He", "Zeru Zhang", "Zeyu Sun", "Junxiong Mao", "Chong Shen"], "venue": "arXiv.org", "abstract": "Yuan 2.0-M32, with a similar base architecture as Yuan-2.0 2B, uses a mixture-of-experts architecture with 32 experts of which 2 experts are active. A new router network, Attention Router, is proposed and adopted for a more efficient selection of experts, which improves the accuracy compared to the model with classical router network. Yuan 2.0-M32 is trained with 2000B tokens from scratch, and the training computation consumption is only 9.25% of a dense model at the same parameter scale. Yuan 2.0-M32 demonstrates competitive capability on coding, math, and various domains of expertise, with only 3.7B active parameters of 40B in total, and 7.4 GFlops forward computation per token, both of which are only 1/19 of Llama3-70B. Yuan 2.0-M32 surpass Llama3-70B on MATH and ARC-Challenge benchmark, with accuracy of 55.89 and 95.8 respectively. The models and source codes of Yuan 2.0-M32 are released at Github1.", "year": 2024, "publicationdate": "2024-05-28", "externalids": {"DOI": "10.48550/arXiv.2405.17976"}, "doi_lower": "10.48550/arxiv.2405.17976"}
{"paper_id": 270619842, "title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "author_names": ["Zihao Zeng", "Yibo Miao", "Hongcheng Gao", "Hao Zhang", "Zhijie Deng"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Mixture of experts (MoE) has become the standard for constructing production-level large language models (LLMs) due to its promise to boost model capacity without causing significant overheads. Nevertheless, existing MoE methods usually enforce a constant top-k routing for all tokens, which is arguably restrictive because various tokens (e.g.,\"\"vs.\"apple\") may require various numbers of experts for feature abstraction. Lifting such a constraint can help make the most of limited resources and unleash the potential of the model for downstream tasks. In this sense, we introduce AdaMoE to realize token-adaptive routing for MoE, where different tokens are permitted to select a various number of experts. AdaMoE makes minimal modifications to the vanilla MoE with top-k routing -- it simply introduces a fixed number of null experts, which do not consume any FLOPs, to the expert set and increases the value of k. AdaMoE does not force each token to occupy a fixed number of null experts but ensures the average usage of the null experts with a load-balancing loss, leading to an adaptive number of null/true experts used by each token. AdaMoE exhibits a strong resemblance to MoEs with expert choice routing while allowing for trivial auto-regressive modeling. AdaMoE is easy to implement and can be effectively applied to pre-trained (MoE-)LLMs. Extensive studies show that AdaMoE can reduce average expert load (FLOPs) while achieving superior performance. For example, on the ARC-C dataset, applying our method to fine-tuning Mixtral-8x7B can reduce FLOPs by 14.5% while increasing accuracy by 1.69%.", "year": 2024, "publicationdate": "2024-06-19", "externalids": {"DOI": "10.48550/arXiv.2406.13233"}, "doi_lower": "10.48550/arxiv.2406.13233"}
{"paper_id": 269982525, "title": "Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models", "author_names": ["Yongxin Guo", "Zhenglin Cheng", "Xiaoying Tang", "Tao Lin"], "venue": "International Conference on Learning Representations", "abstract": "The Sparse Mixture of Experts (SMoE) has been widely employed to enhance the efficiency of training and inference for Transformer-based foundational models, yielding promising results.However, the performance of SMoE heavily depends on the choice of hyper-parameters, such as the number of experts and the number of experts to be activated (referred to as top-k), resulting in significant computational overhead due to the extensive model training by searching over various hyper-parameter configurations. As a remedy, we introduce the Dynamic Mixture of Experts (DynMoE) technique. DynMoE incorporates (1) a novel gating method that enables each token to automatically determine the number of experts to activate. (2) An adaptive process automatically adjusts the number of experts during training. Extensive numerical results across Vision, Language, and Vision-Language tasks demonstrate the effectiveness of our approach to achieve competitive performance compared to GMoE for vision and language tasks, and MoE-LLaVA for vision-language tasks, while maintaining efficiency by activating fewer parameters. Our code is available at https://github.com/LINs-lab/DynMoE.", "year": 2024, "publicationdate": "2024-05-23", "externalids": {"DOI": "10.48550/arXiv.2405.14297"}, "doi_lower": "10.48550/arxiv.2405.14297"}
{"paper_id": 270560556, "title": "Flextron: Many-in-One Flexible Large Language Model", "author_names": ["Ruisi Cai", "Saurav Muralidharan", "Greg Heinrich", "Hongxu Yin", "Zhangyang Wang", "Jan Kautz", "Pavlo Molchanov"], "venue": "International Conference on Machine Learning", "abstract": "Training modern LLMs is extremely resource intensive, and customizing them for various deployment scenarios characterized by limited compute and memory resources through repeated training is impractical. In this paper, we introduce Flextron, a network architecture and post-training model optimization framework supporting flexible model deployment. The Flextron architecture utilizes a nested elastic structure to rapidly adapt to specific user-defined latency and accuracy targets during inference with no additional fine-tuning required. It is also input-adaptive, and can automatically route tokens through its sub-networks for improved performance and efficiency. We present a sample-efficient training method and associated routing algorithms for systematically transforming an existing trained LLM into a Flextron model. We evaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate superior performance over multiple end-to-end trained variants and other state-of-the-art elastic networks, all with a single pretraining run that consumes a mere 7.63% tokens compared to original pretraining.", "year": 2024, "publicationdate": "2024-06-11", "externalids": {"DOI": "10.48550/arXiv.2406.10260"}, "doi_lower": "10.48550/arxiv.2406.10260"}
{"paper_id": 235367626, "title": "Hash Layers For Large Sparse Models", "author_names": ["Stephen Roller", "Sainbayar Sukhbaatar", "Arthur Szlam", "J. Weston"], "venue": "Neural Information Processing Systems", "abstract": "We investigate the training of sparse layers that use different parameters for different inputs based on hashing in large Transformer models. Specifically, we modify the feedforward layer to hash to different sets of weights depending on the current token, over all tokens in the sequence. We show that this procedure either outperforms or is competitive with learning-to-route mixture-of-expert methods such as Switch Transformers and BASE Layers, while requiring no routing parameters or extra terms in the objective function such as a load balancing loss, and no sophisticated assignment algorithm. We study the performance of different hashing techniques, hash sizes and input features, and show that balanced and random hashes focused on the most local features work best, compared to either learning clusters or using longer-range context. We show our approach works well both on large language modeling and dialogue tasks, and on downstream fine-tuning tasks.", "year": 2021, "publicationdate": "2021-06-08", "externalids": {}, "doi_lower": null}
{"paper_id": 238531645, "title": "Taming Sparsely Activated Transformer with Stochastic Experts", "author_names": ["Simiao Zuo", "Xiaodong Liu", "Jian Jiao", "Young Jin Kim", "Hany Hassan", "Ruofei Zhang", "T. Zhao", "Jianfeng Gao"], "venue": "International Conference on Learning Representations", "abstract": "Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can easily scale to have outrageously large amounts of parameters without significant increase in computational cost. However, SAMs are reported to be parameter inefficient such that larger models do not always lead to better performance. While most on-going research focuses on improving SAMs models by exploring methods of routing inputs to experts, our analysis reveals that such research might not lead to the solution we expect, i.e., the commonly-used routing methods based on gating mechanisms do not work better than randomly routing inputs to experts. In this paper, we propose a new expert-based model, THOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models, such as the Switch Transformer, experts in THOR are randomly activated for each input during training and inference. THOR models are trained using a consistency regularized loss, where experts learn not only from training data but also from other experts as teachers, such that all the experts make consistent predictions. We validate the effectiveness of THOR on machine translation tasks. Results show that THOR models are more parameter efficient in that they significantly outperform the Transformer and MoE models across various settings. For example, in multilingual translation, THOR outperforms the Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as that of a state-of-the-art MoE model that is 18 times larger. Our code is publicly available at: https://github.com/microsoft/Stochastic-Mixture-of-Experts.", "year": 2021, "publicationdate": "2021-10-08", "externalids": {}, "doi_lower": null}
{"paper_id": 236976189, "title": "DEMix Layers: Disentangling Domains for Modular Language Modeling", "author_names": ["Suchin Gururangan", "Michael Lewis", "Ari Holtzman", "Noah A. Smith", "Luke Zettlemoyer"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer includes a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added, or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity (especially for out-of-domain data), increase training efficiency, and enable rapid adaptation. Mixing experts during inference, using a parameter-free weighted ensemble, enables better generalization to heterogeneous or unseen domains. We also show it is possible to add experts to adapt to new domains without forgetting older ones, and remove experts to restrict access to unwanted domains. Overall, these results demonstrate benefits of domain modularity in language models.", "year": 2021, "publicationdate": "2021-08-11", "externalids": {"DOI": "10.18653/v1/2022.naacl-main.407"}, "doi_lower": "10.18653/v1/2022.naacl-main.407"}
{"paper_id": 224814118, "title": "Beyond English-Centric Multilingual Machine Translation", "author_names": ["Angela Fan", "Shruti Bhosale", "Holger Schwenk", "Zhiyi Ma", "Ahmed El-Kishky", "Siddharth Goyal", "Mandeep Baines", "Onur Çelebi", "Guillaume Wenzek", "Vishrav Chaudhary", "Naman Goyal", "Tom Birch", "Vitaliy Liptchinsky", "Sergey Edunov", "Edouard Grave", "Michael Auli", "Armand Joulin"], "venue": "Journal of machine learning research", "abstract": "Existing work in translation demonstrated the potential of massively multilingual machine translation by training a single model able to translate between any pair of languages. However, much of this work is English-Centric by training only on data which was translated from or to English. While this is supported by large sources of training data, it does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. We build and open source a training dataset that covers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively to the best single systems of WMT. We open-source our scripts so that others may reproduce the data, evaluation, and final M2M-100 model.", "year": 2020, "publicationdate": "2020-10-21", "externalids": {}, "doi_lower": null}
{"paper_id": 247011948, "title": "Mixture-of-Experts with Expert Choice Routing", "author_names": ["Yan-Quan Zhou", "Tao Lei", "Han-Chu Liu", "Nan Du", "Yanping Huang", "Vincent Zhao", "Andrew M. Dai", "Zhifeng Chen", "Quoc V. Le", "J. Laudon"], "venue": "Neural Information Processing Systems", "abstract": "Sparsely-activated Mixture-of-experts (MoE) models allow the number of parameters to greatly increase while keeping the amount of computation for a given token or a given sample unchanged. However, a poor expert routing strategy (e.g. one resulting in load imbalance) can cause certain experts to be under-trained, leading to an expert being under or over-specialized. Prior work allocates a fixed number of experts to each token using a top-k function regardless of the relative importance of different tokens. To address this, we propose a heterogeneous mixture-of-experts employing an expert choice method. Instead of letting tokens select the top-k experts, we have experts selecting the top-k tokens. As a result, each token can be routed to a variable number of experts and each expert can have a fixed bucket size. We systematically study pre-training speedups using the same computational resources of the Switch Transformer top-1 and GShard top-2 gating of prior work and find that our method improves training convergence time by more than 2x. For the same computational cost, our method demonstrates higher performance in fine-tuning 11 selected tasks in the GLUE and SuperGLUE benchmarks. For a smaller activation cost, our method outperforms the T5 dense model in 7 out of the 11 tasks.", "year": 2022, "publicationdate": "2022-02-18", "externalids": {}, "doi_lower": null}
{"paper_id": 258999501, "title": "Brainformers: Trading Simplicity for Efficiency", "author_names": ["Yan-Quan Zhou", "Nan Du", "Yanping Huang", "Daiyi Peng", "Chang Lan", "Da Huang", "Siamak Shakeri", "David R. So", "Andrew M. Dai", "Yifeng Lu", "Zhifeng Chen", "Quoc V. Le", "Claire Cui", "J.H.J. Laundon", "J. Dean"], "venue": "International Conference on Machine Learning", "abstract": "Transformers are central to recent successes in natural language processing and computer vision. Transformers have a mostly uniform backbone where layers alternate between feed-forward and self-attention in order to build a deep network. Here we investigate this design choice and find that more complex blocks that have different permutations of layer primitives can be more efficient. Using this insight, we develop a complex block, named Brainformer, that consists of a diverse sets of layers such as sparsely gated feed-forward layers, dense feed-forward layers, attention layers, and various forms of layer normalization and activation functions. Brainformer consistently outperforms the state-of-the-art dense and sparse Transformers, in terms of both quality and efficiency. A Brainformer model with 8 billion activated parameters per token demonstrates 2x faster training convergence and 5x faster step time compared to its GLaM counterpart. In downstream task evaluation, Brainformer also demonstrates a 3% higher SuperGLUE score with fine-tuning compared to GLaM with a similar number of activated parameters. Finally, Brainformer largely outperforms a Primer dense model derived with NAS with similar computation per token on fewshot evaluations.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {"DOI": "10.48550/arXiv.2306.00008"}, "doi_lower": "10.48550/arxiv.2306.00008"}
{"paper_id": 265457202, "title": "HOPE: A Memory-Based and Composition-Aware Framework for Zero-Shot Learning with Hopfield Network and Soft Mixture of Experts", "author_names": ["Do Huu Dat", "P. Mao", "Tien Hoang Nguyen", "W. Buntine", "Bennamoun"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "Compositional Zero-Shot Learning (CZSL) has emerged as an essential paradigm in machine learning, aiming to overcome the constraints of traditional zero-shot learning by incorporating compositional thinking into its method-ology. Conventional zero-shot learning has difficulty managing unfamiliar combinations of seen and unseen classes because it depends on pre-defined class embeddings. In contrast, Compositional Zero-Shot Learning leverages the inherent hierarchies and structural connections among classes, creating new class representations by combining at-tributes, components, or other semantic elements. In our paper, we propose a novel framework that for the first time combines the Modern Hopfield Network with a Mixture of Experts (HOPE) to classify the compositions of previously unseen objects. Specifically, the Modern Hopfield Network creates a memory that stores label prototypes and identifies relevant labels for a given input image. Subsequently, the Mixture of Expert models integrates the image with the appropriate prototype to produce the final composition classi-fication. Our approach achieves SOTA performance on sev-eral benchmarks, including MIT-States and UT-Zappos. We also examine how each component contributes to improved generalization.", "year": 2023, "publicationdate": "2023-11-23", "externalids": {"DOI": "10.1109/WACV61041.2025.00115"}, "doi_lower": "10.1109/wacv61041.2025.00115"}
{"paper_id": 264145896, "title": "Merging Experts into One: Improving Computational Efficiency of Mixture of Experts", "author_names": ["Shwai He", "Run-Ze Fan", "Liang Ding", "Li Shen", "Tianyi Zhou", "D. Tao"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Scaling the size of language models usually leads to remarkable advancements in NLP tasks. But it often comes with a price of growing computational cost. Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a small subset of parameters (e.g., one expert) for each input, its computation escalates significantly if increasing the number of activated experts, limiting its practical utility. Can we retain the advantages of adding more experts without substantially increasing the computational costs? In this paper, we first demonstrate the superiority of selecting multiple experts and then propose a computation-efficient approach called \\textbf{\\texttt{Merging Experts into One}} (MEO), which reduces the computation cost to that of a single expert. Extensive experiments show that MEO significantly improves computational efficiency, e.g., FLOPS drops from 72.0G of vanilla MoE to 28.6G (MEO). Moreover, we propose a token-level attention block that further enhances the efficiency and performance of token-level MEO, e.g., 83.3\\% (MEO) vs. 82.6\\% (vanilla MoE) average score on the GLUE benchmark. Our code will be released upon acceptance. Code will be released at: \\url{https://github.com/Shwai-He/MEO}.", "year": 2023, "publicationdate": "2023-10-15", "externalids": {"DOI": "10.48550/arXiv.2310.09832"}, "doi_lower": "10.48550/arxiv.2310.09832"}
{"paper_id": 258967629, "title": "Emergent Modularity in Pre-trained Transformers", "author_names": ["Zhengyan Zhang", "Zhiyuan Zeng", "Yankai Lin", "Chaojun Xiao", "Xiaozhi Wang", "Xu Han", "Zhiyuan Liu", "Ruobing Xie", "Maosong Sun", "Jie Zhou"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore finding a structure that groups neurons into modules by function, and each module works for its corresponding function. Given the enormous amount of possible structures, we focus on Mixture-of-Experts as a promising candidate, which partitions neurons into experts and usually activates different experts for different inputs. Experimental results show that there are functional experts, where clustered are the neurons specialized in a certain function. Moreover, perturbing the activations of functional experts significantly affects the corresponding function. Finally, we study how modularity emerges during pre-training, and find that the modular structure is stabilized at the early stage, which is faster than neuron stabilization. It suggests that Transformers first construct the modular structure and then learn fine-grained neuron functions. Our code and data are available at https://github.com/THUNLP/modularity-analysis.", "year": 2023, "publicationdate": "2023-05-28", "externalids": {"DOI": "10.48550/arXiv.2305.18390"}, "doi_lower": "10.48550/arxiv.2305.18390"}
{"paper_id": 259095471, "title": "Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks", "author_names": ["Mohammed Nowaz Rabbani Chowdhury", "Shuai Zhang", "M. Wang", "Sijia Liu", "Pin-Yu Chen"], "venue": "International Conference on Machine Learning", "abstract": "In deep learning, mixture-of-experts (MoE) activates one or few experts (sub-networks) on a per-sample or per-token basis, resulting in significant computation reduction. The recently proposed \\underline{p}atch-level routing in \\underline{MoE} (pMoE) divides each input into $n$ patches (or tokens) and sends $l$ patches ($l\\ll n$) to each expert through prioritized routing. pMoE has demonstrated great empirical success in reducing training and inference costs while maintaining test accuracy. However, the theoretical explanation of pMoE and the general MoE remains elusive. Focusing on a supervised classification task using a mixture of two-layer convolutional neural networks (CNNs), we show for the first time that pMoE provably reduces the required number of training samples to achieve desirable generalization (referred to as the sample complexity) by a factor in the polynomial order of $n/l$, and outperforms its single-expert counterpart of the same or even larger capacity. The advantage results from the discriminative routing property, which is justified in both theory and practice that pMoE routers can filter label-irrelevant patches and route similar class-discriminative patches to the same expert. Our experimental results on MNIST, CIFAR-10, and CelebA support our theoretical findings on pMoE's generalization and show that pMoE can avoid learning spurious correlations.", "year": 2023, "publicationdate": "2023-06-07", "externalids": {"DOI": "10.48550/arXiv.2306.04073"}, "doi_lower": "10.48550/arxiv.2306.04073"}
{"paper_id": 261049747, "title": "Robust Mixture-of-Expert Training for Convolutional Neural Networks", "author_names": ["Yihua Zhang", "Ruisi Cai", "Tianlong Chen", "Guanhua Zhang", "Huan Zhang", "Pin-Yu Chen", "Shiyu Chang", "Zhangyang Wang", "Sijia Liu"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Sparsely-gated Mixture of Expert (MoE), an emerging deep model architecture, has demonstrated a great promise to enable high-accuracy and ultra-efficient model inference. Despite the growing popularity of MoE, little work investigated its potential to advance convolutional neural networks (CNNs), especially in the plane of adversarial robustness. Since the lack of robustness has become one of the main hurdles for CNNs, in this paper we ask: How to adversarially robustify a CNN-based MoE model? Can we robustly train it like an ordinary CNN model? Our pilot study shows that the conventional adversarial training (AT) mechanism (developed for vanilla CNNs) no longer remains effective to robustify an MoE-CNN. To better understand this phenomenon, we dissect the robustness of an MoE-CNN into two dimensions: Robustness of routers (i.e., gating functions to select data-specific experts) and robustness of experts (i.e., the router-guided pathways defined by the subnetworks of the backbone CNN). Our analyses show that routers and experts are hard to adapt to each other in the vanilla AT. Thus, we propose a new router-expert alternating Adversarial training framework for MoE, termed AdvMoE. The effectiveness of our proposal is justified across 4 commonly-used CNN model architectures over 4 benchmark datasets. We find that AdvMoE achieves 1% ~ 4% adversarial robustness improvement over the original dense CNN, and enjoys the efficiency merit of sparsity-gated MoE, leading to more than 50% inference cost reduction. Codes are available at https://github.com/OPTML-Group/Robust-MoE-CNN.", "year": 2023, "publicationdate": "2023-08-19", "externalids": {"DOI": "10.1109/ICCV51070.2023.00015"}, "doi_lower": "10.1109/iccv51070.2023.00015"}
{"paper_id": 251320183, "title": "Towards Understanding Mixture of Experts in Deep Learning", "author_names": ["Zixiang Chen", "Yihe Deng", "Yue Wu", "Quanquan Gu", "Yuan-Fang Li"], "venue": "arXiv.org", "abstract": "The Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by a router, has achieved great success in deep learning. However, the understanding of such architecture remains elusive. In this paper, we formally study how the MoE layer improves the performance of neural network learning and why the mixture model will not collapse into a single model. Our empirical results suggest that the cluster structure of the underlying problem and the non-linearity of the expert are pivotal to the success of MoE. To further understand this, we consider a challenging classification problem with intrinsic cluster structures, which is hard to learn using a single expert. Yet with the MoE layer, by choosing the experts as two-layer nonlinear convolutional neural networks (CNNs), we show that the problem can be learned successfully. Furthermore, our theory shows that the router can learn the cluster-center features, which helps divide the input complex problem into simpler linear classification sub-problems that individual experts can conquer. To our knowledge, this is the first result towards formally understanding the mechanism of the MoE layer for deep learning.", "year": 2022, "publicationdate": "2022-08-04", "externalids": {"DOI": "10.48550/arXiv.2208.02813"}, "doi_lower": "10.48550/arxiv.2208.02813"}
{"paper_id": 256461199, "title": "Efficient Large Scale Language Modeling with Mixtures of Experts", "author_names": ["Xi Victoria Lin", "Todor Mihaylov", "Mikel Artetxe", "Tianlu Wang", "Shuohui Chen", "Daniel Simig", "Myle Ott", "Naman Goyal", "Shruti Bhosale", "Jingfei Du", "Ramakanth Pasunuru", "Sam Shleifer", "Punit Singh Koura", "Vishrav Chaudhary", "Brian O'Horo", "Jeff Wang", "Luke Zettlemoyer", "Zornitsa Kozareva", "Mona T. Diab", "Ves Stoyanov", "Xian Li"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 282139488, "title": "Retrofitting Small Multilingual Models for Retrieval: Matching 7B Performance with 300M Parameters", "author_names": ["Lifu Tu", "Yingbo Zhou", "Semih Yavuz"], "venue": "arXiv.org", "abstract": "Training effective multilingual embedding models presents unique challenges due to the diversity of languages and task objectives. Although small multilingual models (<1 B parameters) perform well on multilingual tasks generally, they consistently lag behind larger models (>1 B) in the most prevalent use case: retrieval. This raises a critical question: Can smaller models be retrofitted specifically for retrieval tasks to enhance their performance? In this work, we investigate key factors that influence the effectiveness of multilingual embeddings, focusing on training data scale, negative sampling strategies, and data diversity. We find that while increasing the scale of training data yields initial performance gains, these improvements quickly plateau - indicating diminishing returns. Incorporating hard negatives proves essential for consistently improving retrieval accuracy. Furthermore, our analysis reveals that task diversity in the training data contributes more significantly to performance than language diversity alone. As a result, we develop a compact (approximately 300M) multilingual model that achieves retrieval performance comparable to or even surpassing current strong 7B models.", "year": 2025, "publicationdate": "2025-10-16", "externalids": {"DOI": "10.48550/arXiv.2510.14274"}, "doi_lower": "10.48550/arxiv.2510.14274"}
{"paper_id": 271038610, "title": "Mixture of A Million Experts", "author_names": ["Xu Owen He"], "venue": "arXiv.org", "abstract": "The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.", "year": 2024, "publicationdate": "2024-07-04", "externalids": {"DOI": "10.48550/arXiv.2407.04153"}, "doi_lower": "10.48550/arxiv.2407.04153"}
{"paper_id": 267311517, "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models", "author_names": ["Bin Lin", "Zhenyu Tang", "Yang Ye", "Jiaxi Cui", "Bin Zhu", "Peng Jin", "Jinfa Huang", "Junwu Zhang", "Munan Ning", "Li Yuan"], "venue": "arXiv.org", "abstract": "Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs) effectively improves downstream task performances. However, existing scaling methods enable all model parameters to be active for each token in the calculation, which brings massive training and inferring costs. In this work, we propose a simple yet effective training strategy MoE-Tuning for LVLMs. This strategy innovatively addresses the common issue of performance degradation in multi-modal sparsity learning, consequently constructing a sparse model with an outrageous number of parameters but a constant computational cost. Furthermore, we present the MoE-LLaVA, a MoE-based sparse LVLM architecture, which uniquely activates only the top-k experts through routers during deployment, keeping the remaining experts inactive. Extensive experiments show the significant performance of MoE-LLaVA in a variety of visual understanding and object hallucination benchmarks. Remarkably, with only approximately 3B sparsely activated parameters, MoE-LLaVA demonstrates performance comparable to the LLaVA-1.5-7B on various visual understanding datasets and even surpasses the LLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to establish a baseline for sparse LVLMs and provide valuable insights for future research in developing more efficient and effective multi-modal learning systems. Code is released at https://github.com/PKU-YuanGroup/MoE-LLaVA.", "year": 2024, "publicationdate": "2024-01-29", "externalids": {"DOI": "10.48550/arXiv.2401.15947"}, "doi_lower": "10.48550/arxiv.2401.15947"}
{"paper_id": 2239473, "title": "Deep Sparse Rectifier Neural Networks", "author_names": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics", "abstract": null, "year": 2011, "publicationdate": "2011-06-14", "externalids": {}, "doi_lower": null}
{"paper_id": 211096588, "title": "GLU Variants Improve Transformer", "author_names": ["Noam Shazeer"], "venue": "arXiv.org", "abstract": "Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.", "year": 2020, "publicationdate": "2020-02-12", "externalids": {}, "doi_lower": null}
{"paper_id": 269004450, "title": "Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts", "author_names": ["Weilin Cai", "Juyong Jiang", "Le Qin", "Junwei Cui", "Sunghun Kim", "Jiayi Huang"], "venue": "arXiv.org", "abstract": "Expert parallelism has emerged as a key strategy for distributing the computational workload of sparsely-gated mixture-of-experts (MoE) models across multiple devices, enabling the processing of increasingly large-scale models. However, the All-to-All communication inherent to expert parallelism poses a significant bottleneck, limiting the efficiency of MoE models. Although existing optimization methods partially mitigate this issue, they remain constrained by the sequential dependency between communication and computation operations. To address this challenge, we propose ScMoE, a novel shortcut-connected MoE architecture integrated with an overlapping parallelization strategy. ScMoE decouples communication from its conventional sequential ordering, enabling up to 100% overlap with computation. Compared to the prevalent top-2 MoE baseline, ScMoE achieves speedups of 1.49 times in training and 1.82 times in inference. Moreover, our experiments and analyses indicate that ScMoE not only achieves comparable but in some instances surpasses the model quality of existing approaches.", "year": 2024, "publicationdate": "2024-04-07", "externalids": {"DOI": "10.48550/arXiv.2404.05019"}, "doi_lower": "10.48550/arxiv.2404.05019"}
{"paper_id": 268554647, "title": "HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts", "author_names": ["Hao Zhao", "Zihan Qiu", "Huijia Wu", "Zili Wang", "Zhaofeng He", "Jie Fu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multiple datasets and backbones establish that HyperMoE significantly outperforms existing MoE methods under identical conditions concerning the number of experts.", "year": 2024, "publicationdate": "2024-02-20", "externalids": {"DOI": "10.18653/v1/2024.acl-long.571"}, "doi_lower": "10.18653/v1/2024.acl-long.571"}
{"paper_id": 259108831, "title": "Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models’ Memories", "author_names": ["Shizhe Diao", "Tianyang Xu", "Ruijia Xu", "Jiawei Wang", "T. Zhang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Pre-trained language models (PLMs) demonstrate excellent abilities to understand texts in the generic domain while struggling in a specific domain. Although continued pre-training on a large domain-specific corpus is effective, it is costly to tune all the parameters on the domain. In this paper, we investigate whether we can adapt PLMs both effectively and efficiently by only tuning a few parameters. Specifically, we decouple the feed-forward networks (FFNs) of the Transformer architecture into two parts: the original pre-trained FFNs to maintain the old-domain knowledge and our novel domain-specific adapters to inject domain-specific knowledge in parallel.Then we adopt a mixture-of-adapters gate to fuse the knowledge from different domain adapters dynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a two-stage adapter-tuning strategy that leverages both unlabeled data and labeled data to help the domain adaptation: i) domain-specific adapter on unlabeled data; followed by ii) the task-specific adapter on labeled data. MixDA can be seamlessly plugged into the pretraining-finetuning paradigm and our experiments demonstrate that MixDA achieves superior performance on in-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), and knowledge-intensive tasks (KILT).Further analyses demonstrate the reliability, scalability, and efficiency of our method.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.48550/arXiv.2306.05406"}, "doi_lower": "10.48550/arxiv.2306.05406"}
{"paper_id": 269302398, "title": "MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA based Mixture of Experts", "author_names": ["Dengchun Li", "Yingzi Ma", "Naizheng Wang", "Zhiyuan Cheng", "Lei Duan", "Jie Zuo", "Cal Yang", "Mingjie Tang"], "venue": "arXiv.org", "abstract": "Fine-tuning Large Language Models (LLMs) is a common practice to adapt pre-trained models for specific applications. While methods like LoRA have effectively addressed GPU memory constraints during fine-tuning, their performance often falls short, especially in multi-task scenarios. In contrast, Mixture-of-Expert (MoE) models, such as Mixtral 8x7B, demonstrate remarkable performance in multi-task learning scenarios while maintaining a reduced parameter count. However, the resource requirements of these MoEs remain challenging, particularly for consumer-grade GPUs with less than 24GB memory. To tackle these challenges, we propose MixLoRA, an approach to construct a resource-efficient sparse MoE model based on LoRA. MixLoRA inserts multiple LoRA-based experts within the feed-forward network block of a frozen pre-trained dense model and employs a commonly used top-k router. Unlike other LoRA-based MoE methods, MixLoRA enhances model performance by utilizing independent attention-layer LoRA adapters. Additionally, an auxiliary load balance loss is employed to address the imbalance problem of the router. Our evaluations show that MixLoRA improves about 9% accuracy compared to state-of-the-art PEFT methods in multi-task learning scenarios. We also propose a new high-throughput framework to alleviate the computation and memory bottlenecks during the training and inference of MOE models. This framework reduces GPU memory consumption by 40% and token computation latency by 30% during both training and inference.", "year": 2024, "publicationdate": "2024-04-22", "externalids": {"DOI": "10.48550/arXiv.2404.15159"}, "doi_lower": "10.48550/arxiv.2404.15159"}
{"paper_id": 267312176, "title": "LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs", "author_names": ["Shaoxiang Chen", "Zequn Jie", "Lin Ma"], "venue": "arXiv.org", "abstract": "Instruction finetuning on a variety of image-text instruction data is the key to obtaining a versatile Multimodal Large Language Model (MLLM), and different configurations of the instruction data can lead to finetuned models with different capabilities. However, we have discovered that data conflicts are inevitable when mixing instruction data from distinct domains, which can result in performance drops for tasks of a specific domain. To address this issue, we propose to apply an efficient Mixture of Experts (MoE) design, which is a sparse Mixture of LoRA Experts (MoLE) for instruction finetuning MLLMs. Within the Transformer layers, we extend the popular Low-Rank Adaption (LoRA) method by creating a set of LoRA experts specifically for the MLP layer, and route each token to the top-1 expert based on a routing function, allowing adaptive choices for tokens from different domains. Since the LoRA experts are sparsely activated, the training and inference cost are kept roughly constant compared to the original LoRA method. By replacing the plain-LoRA of LLaVA-1.5 with our MoE design, our final model is named LLaVA-MoLE. Extensive experiments proved that LLaVA-MoLE effectively mitigates the data conflict issue when mixing multiple distinct instruction datasets with various configurations, and achieves consistent performance gains over the strong plain-LoRA baselines. Most importantly, on the mixed datasets, LLaVA-MoLE can even outperform the plain-LoRA baseline trained with twice the samples.", "year": 2024, "publicationdate": "2024-01-29", "externalids": {"DOI": "10.48550/arXiv.2401.16160"}, "doi_lower": "10.48550/arxiv.2401.16160"}
{"paper_id": 238857301, "title": "UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning", "author_names": ["Yuning Mao", "Lambert Mathias", "Rui Hou", "Amjad Almahairi", "Hao Ma", "Jiawei Han", "Wen-tau Yih", "Madian Khabsa"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Recent parameter-efficient language model tuning (PELT) methods manage to match the performance of fine-tuning with much fewer trainable parameters and perform especially well when training data is limited. However, different PELT methods may perform rather differently on the same task, making it nontrivial to select the most appropriate method for a specific task, especially considering the fast-growing number of new PELT methods and tasks. In light of model diversity and the difficulty of model selection, we propose a unified framework, UniPELT, which incorporates different PELT methods as submodules and learns to activate the ones that best suit the current data or task setup via gating mechanism. On the GLUE benchmark, UniPELT consistently achieves 1 4% gains compared to the best individual PELT method that it incorporates and even outperforms fine-tuning under different setups. Moreover, UniPELT generally surpasses the upper bound that takes the best performance of all its submodules used individually on each task, indicating that a mixture of multiple PELT methods may be inherently more effective than single methods.", "year": 2021, "publicationdate": "2021-10-14", "externalids": {"DOI": "10.18653/v1/2022.acl-long.433"}, "doi_lower": "10.18653/v1/2022.acl-long.433"}
{"paper_id": 267636601, "title": "Higher Layers Need More LoRA Experts", "author_names": ["Chongyang Gao", "Kezhen Chen", "Jinmeng Rao", "Baochen Sun", "Ruibo Liu", "Daiyi Peng", "Yawen Zhang", "Xiaoyuan Guo", "Jie Yang", "V. Subrahmanian"], "venue": "arXiv.org", "abstract": "Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on Large Language Models, but their impact on model performance remains limited. Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages. Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy. Does this statement also apply to parameter-efficient MoE? In this paper, we introduce a novel parameter-efficient MoE method, \\textit{\\textbf{M}oE-L\\textbf{o}RA with \\textbf{L}ayer-wise Expert \\textbf{A}llocation (MoLA)} for Transformer-based models, where each model layer has the flexibility to employ a varying number of LoRA experts. We investigate several architectures with varying layer-wise expert configurations. Experiments on six well-known NLP and commonsense QA benchmarks demonstrate that MoLA achieves equal or superior performance compared to all baselines. We find that allocating more LoRA experts to higher layers further enhances the effectiveness of models with a certain number of experts in total. With much fewer parameters, this allocation strategy outperforms the setting with the same number of experts in every layer. This work can be widely used as a plug-and-play parameter-efficient tuning approach for various applications. The code is available at https://github.com/GCYZSL/MoLA.", "year": 2024, "publicationdate": "2024-02-13", "externalids": {"DOI": "10.48550/arXiv.2402.08562"}, "doi_lower": "10.48550/arxiv.2402.08562"}
{"paper_id": 269149321, "title": "T-REX: Mixture-of-Rank-One-Experts with Semantic-aware Intuition for Multi-task Large Language Model Finetuning", "author_names": ["Rongyu Zhang", "Yijiang Liu", "Huanrui Yang", "Shenli Zheng", "Dan Wang", "Yuan Du", "Li Du", "Shanghang Zhang"], "venue": "", "abstract": "Large language models (LLMs) encounter significant adaptation challenges in diverse multitask finetuning. Mixture-of-experts (MoE) provides a promising solution with a dynamic architecture, enabling effective task decoupling. However, scaling up the number of MoE experts incurs substantial parameter and computational overheads and suffers from limited performance gain due to naive routing mechanisms. In this paper, we design a novel framework, mix\\underline{\\textbf{T}}ure\\underline{\\textbf{-}}of-\\underline{\\textbf{R}}ank-on\\underline{\\textbf{E}}-e\\underline{\\textbf{X}}perts (\\texttt{T-REX}), which leverages the combination of ultra-low rank experts to construct LoRA weights on pretrained LLMs. The rank-1 experts enable a mix-and-match mechanism to quadratically expand the vector subspace of experts with linear parameter overheads, achieving approximate error reduction with optimal efficiency. In addition, T-REX offers implicit guidance to the router, leveraging the inherent semantic clustering of training embeddings as prior knowledge, enabling optimized feature allocation across experts for a smoother convergence. Extensive theoretical and empirical results demonstrate that T-REX achieves superior efficiency and generalizability across diverse tasks. Compared with other LoRA-based methods, T-REX achieves up to 1.78\\% mean accuracy improvement with around 30\\%-40\\% less trainable parameters across 14 public datasets. \\href{https://github.com/RoyZry98/T-REX-Pytorch}{Code} is available.", "year": 2024, "publicationdate": "2024-04-13", "externalids": {}, "doi_lower": null}
{"paper_id": 269982348, "title": "MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models", "author_names": ["Jingwei Xu", "Junyu Lai", "Yunpeng Huang"], "venue": "International Conference on Learning Representations", "abstract": "The pretrain+fine-tune paradigm is foundational for deploying large language models (LLMs) across various downstream applications. Within this framework, Low-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning (PEFT), producing numerous reusable task-specific LoRA adapters. However, this approach requires explicit task intention selection, posing challenges for autonomous task sensing and switching during inference with multiple existing LoRA adapters embedded in a single LLM. In this work, we introduce MeteoRA (Multiple-tasks embedded LoRA), a scalable and efficient framework that reuses multiple task-specific LoRA adapters into the base LLM via a full-mode Mixture-of-Experts (MoE) architecture. This framework also includes novel MoE forward acceleration strategies to address the efficiency challenges of traditional MoE implementations. Our evaluation, using the LlaMA2-13B and LlaMA3-8B base models equipped with 28 existing LoRA adapters through MeteoRA, demonstrates equivalent performance with the traditional PEFT method. Moreover, the LLM equipped with MeteoRA achieves superior performance in handling composite tasks, effectively solving ten sequential problems in a single inference pass, thereby demonstrating the framework's enhanced capability for timely adapter switching.", "year": 2024, "publicationdate": "2024-05-19", "externalids": {"DOI": "10.48550/arXiv.2405.13053"}, "doi_lower": "10.48550/arxiv.2405.13053"}
{"paper_id": 248266614, "title": "Residual Mixture of Experts", "author_names": ["Lemeng Wu", "Mengchen Liu", "Yinpeng Chen", "Dongdong Chen", "Xiyang Dai", "Lu Yuan"], "venue": "arXiv.org", "abstract": "Mixture of Experts (MoE) is able to scale up vision transformers effectively. However, it requires prohibiting computation resources to train a large MoE transformer. In this paper, we propose Residual Mixture of Experts (RMoE), an efficient training pipeline for MoE vision transformers on downstream tasks, such as segmentation and detection. RMoE achieves comparable results with the upper-bound MoE training, while only introducing minor additional training cost than the lower-bound non-MoE training pipelines. The efficiency is supported by our key observation: the weights of an MoE transformer can be factored into an input-independent core and an input-dependent residual. Compared with the weight core, the weight residual can be efficiently trained with much less computation resource, e.g., finetuning on the downstream data. We show that, compared with the current MoE training pipeline, we get comparable results while saving over 30% training cost. When compared with state-of-the-art non- MoE transformers, such as Swin-T / CvT-13 / Swin-L, we get +1.1 / 0.9 / 1.0 mIoU gain on ADE20K segmentation and +1.4 / 1.6 / 0.6 AP gain on MS-COCO object detection task with less than 3% additional training cost.", "year": 2022, "publicationdate": "2022-04-20", "externalids": {"DOI": "10.48550/arXiv.2204.09636"}, "doi_lower": "10.48550/arxiv.2204.09636"}
{"paper_id": 239009754, "title": "Tricks for Training Sparse Translation Models", "author_names": ["Dheeru Dua", "Shruti Bhosale", "Vedanuj Goswami", "James Cross", "M. Lewis", "Angela Fan"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Multi-task learning with an unbalanced data distribution skews model learning towards high resource tasks, especially when model capacity is fixed and fully shared across all tasks. Sparse scaling architectures, such as BASELayers, provide flexible mechanisms for different tasks to have a variable number of parameters, which can be useful to counterbalance skewed data distributions. We find that that sparse architectures for multilingual machine translation can perform poorly out of the box and propose two straightforward techniques to mitigate this — a temperature heating mechanism and dense pre-training. Overall, these methods improve performance on two multilingual translation benchmarks compared to standard BASELayers and Dense scaling baselines, and in combination, more than 2x model convergence speed.", "year": 2021, "publicationdate": "2021-10-15", "externalids": {"DOI": "10.18653/v1/2022.naacl-main.244"}, "doi_lower": "10.18653/v1/2022.naacl-main.244"}
{"paper_id": 257353502, "title": "Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers", "author_names": ["Tianlong Chen", "Zhenyu (Allen) Zhang", "Ajay Jaiswal", "Shiwei Liu", "Zhangyang Wang"], "venue": "International Conference on Learning Representations", "abstract": "Despite their remarkable achievement, gigantic transformers encounter significant drawbacks, including exorbitant computational and memory footprints during training, as well as severe collapse evidenced by a high degree of parameter redundancy. Sparsely-activated Mixture-of-Experts (SMoEs) have shown promise to mitigate the issue of training efficiency, yet they are prone to (1) redundant experts due to representational collapse; and (2) poor expert scalability for inference and downstream fine-tuning, primarily due to overfitting of the learned routing policy to the number of activated experts during training. As recent research efforts are predominantly focused on improving routing policies to encourage expert specializations, this work focuses on exploring the overlooked scalability bottleneck of SMoEs and leveraging it to effectively scale dense transformers. To this end, we propose a new plug-and-play training framework, SMoE-Dropout, to enable scaling transformers to better accuracy in their full capacity without collapse. Specifically, SMoE-Dropout consists of a randomly initialized and fixed router network to activate experts and gradually increases the activated expert number as training progresses over time. Transformers trained by SMoE-Dropout naturally exhibit a self-slimmable property subject to resource availability, offering smooth and consistent performance boosts with an increase in activated experts during inference or fine-tuning. Our extensive experiments demonstrate the superior performance and substantial computation savings of SMoE-Dropout, compared to dense training baselines with equivalent parameter counts. In particular, our trained BERT outperforms its densely trained counterpart with consistent improvements of {1.03%, 0.78%, 1.09%} on challenging reasoning tasks {ASDiv-A, MAWPS, SVAMP}, respectively.", "year": 2023, "publicationdate": "2023-03-02", "externalids": {"DOI": "10.48550/arXiv.2303.01610"}, "doi_lower": "10.48550/arxiv.2303.01610"}
{"paper_id": 248227273, "title": "MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation", "author_names": ["Simiao Zuo", "Qingru Zhang", "Chen Liang", "Pengcheng He", "T. Zhao", "Weizhu Chen"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Pre-trained language models have demonstrated superior performance in various natural language processing tasks. However, these models usually contain hundreds of millions of parameters, which limits their practicality because of latency requirements in real-world applications. Existing methods train small compressed models via knowledge distillation. However, performance of these small models drops significantly compared with the pre-trained models due to their reduced model capacity. We propose MoEBERT, which uses a Mixture-of-Experts structure to increase model capacity and inference speed. We initialize MoEBERT by adapting the feed-forward neural networks in a pre-trained model into multiple experts. As such, representation power of the pre-trained model is largely retained. During inference, only one of the experts is activated, such that speed can be improved. We also propose a layer-wise distillation method to train MoEBERT. We validate the efficiency and efficacy of MoEBERT on natural language understanding and question answering tasks. Results show that the proposed method outperforms existing task-specific distillation algorithms. For example, our method outperforms previous approaches by over 2% on the MNLI (mismatched) dataset. Our code is publicly available at https://github.com/SimiaoZuo/MoEBERT.", "year": 2022, "publicationdate": "2022-04-15", "externalids": {"DOI": "10.48550/arXiv.2204.07675"}, "doi_lower": "10.48550/arxiv.2204.07675"}
{"paper_id": 264172340, "title": "Unlocking Emergent Modularity in Large Language Models", "author_names": ["Zihan Qiu", "Zeyu Huang", "Jie Fu"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Modular Neural Networks (MNNs) demonstrate various advantages over monolithic models.Existing MNNs are generally \\textit{explicit}: their modular architectures are pre-defined, with individual modules expected to implement distinct functions.Recent works reveal that there exists \\textit{implicit} modularity in standard pre-trained transformers, namely \\textit{Emergent Modularity}.They indicate that such modular structures spontaneously exhibit during the early pre-training phase.Despite the benefits of modularity, most Language Models (LMs) are still treated as monolithic models in the pre-train and fine-tune paradigm, with their emergent modularity locked and underutilized.In this work, focusing on unlocking the emergent modularity in LMs, we showcase that standard LMs could be fine-tuned as their Mixture-of-Expert (MoEs) counterparts without introducing any extra parameters. Such MoEs are derived from emergent modularity and are referred to as Emergent MoEs (EMoE).Our experiments demonstrate that fine-tuning EMoE effectively improves downstream in-domain and out-of-domain generalization compared with vanilla fine-tuning.Our analysis and ablation studies further illustrate that it is robust to various configurations and can scale up to Large Language Models (i.e., Llama2-7B and Llama-30B). Code is available at https://github.com/qiuzh20/EMoE.", "year": 2023, "publicationdate": "2023-10-17", "externalids": {"DOI": "10.18653/v1/2024.naacl-long.144"}, "doi_lower": "10.18653/v1/2024.naacl-long.144"}
{"paper_id": 268725342, "title": "Demystifying Softmax Gating in Gaussian Mixture of Experts", "author_names": ["Huy Nguyen", "TrungTin Nguyen", "Nhat Ho"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.03288"}, "doi_lower": "10.48550/arxiv.2305.03288"}
{"paper_id": 251371375, "title": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models", "author_names": ["Margaret Li", "Suchin Gururangan", "Tim Dettmers", "M. Lewis", "Tim Althoff", "Noah A. Smith", "Luke Zettlemoyer"], "venue": "arXiv.org", "abstract": "We present Branch-Train-Merge (BTM), a communication-efficient algorithm for embarrassingly parallel training of large language models (LLMs). We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs. BTM learns a set of independent expert LMs (ELMs), each specialized to a different textual domain, such as scientific or legal text. These ELMs can be added and removed to update data coverage, ensembled to generalize to new domains, or averaged to collapse back to a single LM for efficient inference. New ELMs are learned by branching from (mixtures of) ELMs in the current set, further training the parameters on data for the new domain, and then merging the resulting model back into the set for future use. Experiments show that BTM improves in- and out-of-domain perplexities as compared to GPT-style Transformer LMs, when controlling for training cost. Through extensive analysis, we show that these results are robust to different ELM initialization schemes, but require expert domain specialization; LM ensembles with random data splits do not perform well. We also present a study of scaling BTM into a new corpus of 64 domains (192B whitespace-separated tokens in total); the resulting LM (22.4B total parameters) performs as well as a Transformer LM trained with 2.5 times more compute. These gains grow with the number of domains, suggesting more aggressive parallelism could be used to efficiently train larger models in future work.", "year": 2022, "publicationdate": "2022-08-05", "externalids": {"DOI": "10.48550/arXiv.2208.03306"}, "doi_lower": "10.48550/arxiv.2208.03306"}
{"paper_id": 263608332, "title": "Fusing Models with Complementary Expertise", "author_names": ["Hongyi Wang", "Felipe Maia Polo", "Yuekai Sun", "Souvik Kundu", "Eric P. Xing", "M. Yurochkin"], "venue": "International Conference on Learning Representations", "abstract": "Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the\"frugal\"setting where it is desired to reduce the number of expert model evaluations at test time. Our implementation is publicly available at https://github.com/hwang595/FoE-ICLR2024.", "year": 2023, "publicationdate": "2023-10-02", "externalids": {"DOI": "10.48550/arXiv.2310.01542"}, "doi_lower": "10.48550/arxiv.2310.01542"}
{"paper_id": 232335691, "title": "FastMoE: A Fast Mixture-of-Expert Training System", "author_names": ["Jiaao He", "J. Qiu", "Aohan Zeng", "Zhilin Yang", "Jidong Zhai", "Jie Tang"], "venue": "arXiv.org", "abstract": "Mixture-of-Expert (MoE) presents a strong potential in enlarging the size of language model to trillions of parameters. However, training trillion-scale MoE requires algorithm and system co-design for a well-tuned high performance distributed training system. Unfortunately, the only existing platform that meets the requirements strongly depends on Google's hardware (TPU) and software (Mesh Tensorflow) stack, and is not open and available to the public, especially GPU and PyTorch communities. In this paper, we present FastMoE, a distributed MoE training system based on PyTorch with common accelerators. The system provides a hierarchical interface for both flexible model design and easy adaption to different applications, such as Transformer-XL and Megatron-LM. Different from direct implementation of MoE models using PyTorch, the training speed is highly optimized in FastMoE by sophisticated high-performance acceleration skills. The system supports placing different experts on multiple GPUs across multiple nodes, enabling enlarging the number of experts linearly against the number of GPUs. The source of FastMoE is available at https://github.com/laekov/fastmoe under Apache-2 license.", "year": 2021, "publicationdate": "2021-03-24", "externalids": {}, "doi_lower": null}
{"paper_id": 248965412, "title": "MoESys: A Distributed and Efficient Mixture-of-Experts Training and Inference System for Internet Services", "author_names": ["Liang Shen", "Zhihua Wu", "Weibao Gong", "Hongxiang Hao", "Yangfan Bai", "Huachao Wu", "Xinxuan Wu", "Haoyi Xiong", "Dianhai Yu", "Yanjun Ma"], "venue": "IEEE Transactions on Services Computing", "abstract": "While modern internet services, such as chatbots, search engines, and online advertising, demand the use of large-scale deep neural networks (DNNs), distributed training and inference over heterogeneous computing systems are desired to facilitate these DNN models. Mixture-of-Experts (MoE) is one the most common strategies to lower the cost of training subject to the overall size of models/data through gating and parallelism in a divide-and-conquer fashion. While DeepSpeed Rasley et al. 2020 has made efforts in carrying out large-scale MoE training over heterogeneous infrastructures, the efficiency of training and inference could be further improved from several system aspects, including load balancing, communication/computation efficiency, and memory footprint limits. In this work, we present a novel MoESys that boosts efficiency in both large-scale training and inference. Specifically, in the training procedure, the proposed MoESys adopts an Elastic MoE training strategy with 2D prefetch and Fusion communication over Hierarchical storage, so as to enjoy efficient parallelisms. For scalable inference in a single node, especially when the model size is larger than GPU memory, MoESys builds the CPU-GPU memory jointly into a ring of sections to load the model, and executes the computation tasks across the memory sections in a round-robin manner for efficient inference. We carried out extensive experiments to evaluate MoESys, where MoESys successfully trains a Unified Feature Optimization Zhang et al. 2021 (UFO) model with a Sparsely-Gated Mixture-of-Experts model of 12B parameters in 8 days on 48 A100 GPU cards. The comparison against the state-of-the-art shows that MoESys outperformed DeepSpeed with 33% higher throughput (tokens per second) in training and 13% higher throughput in inference in general. Particularly, under unbalanced MoE Tasks, e.g., UFO, MoESys achieved 64% higher throughput with 18% lower memory footprints.", "year": 2022, "publicationdate": "2022-05-20", "externalids": {"DOI": "10.1109/TSC.2024.3399654"}, "doi_lower": "10.1109/tsc.2024.3399654"}
{"paper_id": 247765599, "title": "FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models", "author_names": ["Jiaao He", "Jidong Zhai", "Tiago Antunes", "Haojie Wang", "Fuwen Luo", "Shangfeng Shi", "Qingwen Li"], "venue": "ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming", "abstract": "The current trend in deep learning is to scale models to extremely large sizes with the objective of increasing their accuracy. Mixture-of-Expert (MoE) is the most popular pre-trained model that makes feasible the training of models with parameters beyond trillion-scale. Thanks to the dynamic activation of experts, i.e., shallow layers specialized in certain domains, it allows for sparse training of bigger models, removing the linearity between model size and computation. However, different from traditional deep learning models, it draws huge challenges to the efficiency of these training systems, including dynamic load imbalance, inefficient synchronous execution mode, and congested all-to-all communication. To address these challenges, we first propose a performance model that can both accurately predict the latency of different operations of a specific training task, and intuitively analyze its end-to-end performance via a novel roofline-like model. Then, guided by this model, we invent a dynamic shadowing approach to cope with load imbalance, and a smart fine-grained schedule that splits different operations and executes them concurrently. We design a congestion-avoiding expert selection strategy that relieves network congestion for the lower latency of iterations, when modification of expert selection is allowed. We implement and integrate the above optimizations as a general system, FasterMoE, empowering efficient distributed MoE model training. FasterMoE is evaluated on different cluster systems using up to 64 GPUs. It achieves 1.37X - 17.87X speedup compared with state-of-the-art systems for large models, including ZeRO, GShard, and BASE Layer. Source code of FasterMoE is now available at https://github.com/thu-pacman/FasterMoE.", "year": 2022, "publicationdate": "2022-03-28", "externalids": {"DOI": "10.1145/3503221.3508418"}, "doi_lower": "10.1145/3503221.3508418"}
{"paper_id": 258686146, "title": "A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training", "author_names": ["Siddharth Singh", "Olatunji Ruwase", "A. A. Awan", "Samyam Rajbhandari", "Yuxiong He", "A. Bhatele"], "venue": "International Conference on Supercomputing", "abstract": "Mixture-of-Experts (MoE) is a neural network architecture that adds sparsely activated expert blocks to a base model, increasing the number of parameters without impacting computational costs. However, current distributed deep learning frameworks are limited in their ability to train high-quality MoE models with large base models. In this work, we present DeepSpeed-TED, a novel, three-dimensional, hybrid parallel algorithm that combines data, tensor, and expert parallelism to enable the training of MoE models with 4--8× larger base models than the current state-of-the-art. We also describe memory optimizations in the optimizer step, and communication optimizations that eliminate unnecessary data movement. We implement our approach in DeepSpeed and achieve speedups of 26% over a baseline (i.e. without our communication optimizations) when training a 40 billion parameter MoE model (6.7 billion base model with 16 experts) on 128 V100 GPUs.", "year": 2023, "publicationdate": "2023-03-11", "externalids": {"DOI": "10.1145/3577193.3593704"}, "doi_lower": "10.1145/3577193.3593704"}
{"paper_id": 247762354, "title": "HetuMoE: An Efficient Trillion-scale Mixture-of-Expert Distributed Training System", "author_names": ["Xiaonan Nie", "Pinxue Zhao", "Xupeng Miao", "Bin Cui"], "venue": "arXiv.org", "abstract": "As giant dense models advance quality but require large-scale expensive GPU clusters for training, the sparsely gated Mixture-of-Experts (MoE), a kind of conditional computation architecture, are proposed to scale models while keeping the computation constant. Speciﬁcally, the input data is routed by a gate network and only activates a part of the expert network. Existing MoE training systems only support part of mainstream MoE models (e.g. Top k) training under expensive high-bandwidth GPU clusters. In this paper, we present HetuMoE, a high-performance large-scale sparse MoE training system built on Hetu. HetuMoE provides multiple gating strategies and efﬁcient GPU kernel implementations. To further improve the training efﬁciency on commodity GPU clusters (e.g, with only 1 NiC), we introduce the hierarchical AllToAll communication that combines hierarchical networks and aggregating messages. Compared with existing state-of-the-art MoE systems, HetuMoE obtains at least 15% speedup. Speciﬁcally, HetuMoE outperforms DeepSpeed-MoE up to 8 . 1 × under the switch gate with a batch size of 32. The code is available at: https://github.com/PKU-DAIR/Hetu .", "year": 2022, "publicationdate": "2022-03-28", "externalids": {"DOI": "10.48550/arXiv.2203.14685"}, "doi_lower": "10.48550/arxiv.2203.14685"}
{"paper_id": 258048524, "title": "FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement", "author_names": ["Xiaonan Nie", "Xupeng Miao", "Zilong Wang", "Zichao Yang", "Jilong Xue", "Lingxiao Ma", "Gang-Ming Cao", "Bin Cui"], "venue": "Proc. ACM Manag. Data", "abstract": "With the increasing data volume, there is a trend of using large-scale pre-trained models to store the knowledge into an enormous number of model parameters. The training of these models is composed of lots of dense algebras, requiring a huge amount of hardware resources. Recently, sparsely-gated Mixture-of-Experts (MoEs) are becoming more popular and have demonstrated impressive pretraining scalability in various downstream tasks. However, such a sparse conditional computation may not be effective as expected in practical systems due to the routing imbalance and fluctuation problems. Generally, MoEs are becoming a new data analytics paradigm in the data life cycle and suffering from unique challenges at scales, complexities, and granularities never before possible. In this paper, we propose a novel DNN training framework, FlexMoE, which systematically and transparently address the inefficiency caused by dynamic dataflow. We first present an empirical analysis on the problems and opportunities of training MoE models, which motivates us to overcome the routing imbalance and fluctuation problems by a dynamic expert management and device placement mechanism. Then we introduce a novel scheduling module over the existing DNN runtime to monitor the data flow, make the scheduling plans, and dynamically adjust the model-to-hardware mapping guided by the real-time data traffic. A simple but efficient heuristic algorithm is exploited to dynamically optimize the device placement during training. We have conducted experiments on both NLP models (e.g., BERT and GPT) and vision models (e.g., Swin). And results show FlexMoE can achieve superior performance compared with existing systems on real-world workloads --- FlexMoE outperforms DeepSpeed by 1.70x on average and up to 2.10x, and outperforms FasterMoE by 1.30x on average and up to 1.45x.", "year": 2023, "publicationdate": "2023-04-08", "externalids": {"DOI": "10.1145/3588964"}, "doi_lower": "10.1145/3588964"}
{"paper_id": 259858759, "title": "SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization", "author_names": ["Mingshu Zhai", "Jiaao He", "Zixuan Ma", "Zan Zong", "Runqing Zhang", "Jidong Zhai"], "venue": "USENIX Annual Technical Conference", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 254069783, "title": "MegaBlocks: Efficient Sparse Training with Mixture-of-Experts", "author_names": ["Trevor Gale", "D. Narayanan", "C. Young", "M. Zaharia"], "venue": "Conference on Machine Learning and Systems", "abstract": "We present MegaBlocks, a system for efficient Mixture-of-Experts (MoE) training on GPUs. Our system is motivated by the limitations of current frameworks, which restrict the dynamic routing in MoE layers to satisfy the constraints of existing software and hardware. These formulations force a tradeoff between model quality and hardware efficiency, as users must choose between dropping tokens from the computation or wasting computation and memory on padding. To address these limitations, we reformulate MoE computation in terms of block-sparse operations and develop new block-sparse GPU kernels that efficiently handle the dynamism present in MoEs. Our approach never drops tokens and maps efficiently to modern hardware, enabling end-to-end training speedups of up to 40% over MoEs trained with the state-of-the-art Tutel library and 2.4x over DNNs trained with the highly-optimized Megatron-LM framework.", "year": 2022, "publicationdate": "2022-11-29", "externalids": {"DOI": "10.48550/arXiv.2211.15841"}, "doi_lower": "10.48550/arxiv.2211.15841"}
{"paper_id": 268379441, "title": "Scattered Mixture-of-Experts Implementation", "author_names": ["Shawn Tan", "Yikang Shen", "Rameswar Panda", "Aaron C. Courville"], "venue": "arXiv.org", "abstract": "We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE) on GPUs. ScatterMoE builds upon existing implementations, and overcoming some of the limitations to improve inference and training speed, and memory footprint. This implementation achieves this by avoiding padding and making excessive copies of the input. We introduce ParallelLinear, the main component we use to build our implementation and the various kernels used to speed up the operation. We benchmark our implementation against Megablocks, and show that it enables a higher throughput and lower memory footprint. We also show how ParallelLinear enables extension of the Mixture-of-Experts concept by demonstrating with an implementation of Mixture of Attention.", "year": 2024, "publicationdate": "2024-03-13", "externalids": {"DOI": "10.48550/arXiv.2403.08245"}, "doi_lower": "10.48550/arxiv.2403.08245"}
{"paper_id": 256274793, "title": "PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation", "author_names": ["Ningxin Zheng", "Huiqiang Jiang", "Quan Zhang", "Zhenhua Han", "Yuqing Yang", "Lingxiao Ma", "Fan Yang", "Lili Qiu", "Mao Yang", "Lidong Zhou"], "venue": "Symposium on Operating Systems Principles", "abstract": "Dynamic sparsity, where the sparsity patterns are unknown until runtime, poses a significant challenge to deep learning. The state-of-the-art sparsity-aware deep learning solutions are restricted to pre-defined, static sparsity patterns due to significant overheads associated with preprocessing. Efficient execution of dynamic sparse computation often faces the misalignment between the GPU-friendly tile configuration for efficient execution and the sparsity-aware tile shape that minimizes coverage wastes (non-zero values in tensor). In this paper, we propose PIT, a deep-learning compiler for dynamic sparsity. PIT proposes a novel tiling mechanism that leverages Permutation Invariant Transformation (PIT), a mathematically proven property, to transform multiple sparsely located micro-tiles into a GPU-efficient dense tile without changing the computation results, thus achieving both high GPU utilization and low coverage waste. Given a model, PIT first finds feasible PIT rules for all its operators and generates efficient GPU kernels accordingly. At runtime, with the novel SRead and SWrite primitives, PIT rules can be executed extremely fast to support dynamic sparsity in an online manner. Extensive evaluation on diverse models shows that PIT can accelerate dynamic sparsity computation by up to 5.9x (average 2.43x) over state-of-the-art compilers.", "year": 2023, "publicationdate": "2023-01-26", "externalids": {"DOI": "10.1145/3600006.3613139"}, "doi_lower": "10.1145/3600006.3613139"}
{"paper_id": 267028699, "title": "Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference", "author_names": ["Jinghan Yao", "Quentin G. Anthony", "A. Shafi", "H. Subramoni", "D. Panda"], "venue": "IEEE International Parallel and Distributed Processing Symposium", "abstract": "In the realm of large language models (LLMs) like the Generative Pre-trained Transformer (GPT), the Mixture of Experts (MoE) paradigm has emerged as a powerful technique for enhancing model expressiveness and accuracy. However, the deployment of GPT MoE models for parallel inference on distributed systems presents significant challenges, primarily due to the extensive Alltoall communication required for expert routing and aggregation. This communication bottleneck exacerbates the already complex computational landscape, hindering the efficient utilization of high-performance computing resources. In this paper, we propose a lightweight optimization technique called ExFlow, to largely accelerate the inference of these MoE models. We take a new perspective on alleviating the communication overhead by exploiting the inter-layer expert affinity. Unlike previous methods, our solution can be directly applied to pre-trained MoE models without any fine-tuning or accuracy degradation. By proposing a context-coherent expert parallelism on distributed systems, our ExFlow design only uses one Alltoall communication to deliver the same functionality while previous methods all require two Alltoalls. By carefully examining the conditional probability in tokens’ routing across multiple layers, we proved that pre-trained GPT MoE models implicitly exhibit a strong inter-layer expert affinity. We then design an efficient integer programming model to precisely capture such features and show that by properly placing the experts on corresponding GPUs, we can reduce up to 67% of tokens’ cross-GPU routing latency on various hardware configurations and topologies. Our solution beats the cutting-edge Deepspeed-MoE in GPT MoE models with experts from 8 to 64, with up to 2.2x improvement in inference throughput. To the best of our knowledge, this is the first work in leveraging inter-layer expert affinity to accelerate the inference of GPT MoE models. We further provide a detailed study of how the model implicitly acquires this expert affinity at the very early training stage and how this affinity evolves and stabilizes during training.", "year": 2024, "publicationdate": "2024-01-16", "externalids": {"DOI": "10.1109/IPDPS57955.2024.00086"}, "doi_lower": "10.1109/ipdps57955.2024.00086"}
{"paper_id": 257038272, "title": "TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training", "author_names": ["Chang-Qin Chen", "Min Li", "Zhihua Wu", "Dianhai Yu", "Chao Yang"], "venue": "Neural Information Processing Systems", "abstract": "Sparsely gated Mixture-of-Expert (MoE) has demonstrated its effectiveness in scaling up deep neural networks to an extreme scale. Despite that numerous efforts have been made to improve the performance of MoE from the model design or system optimization perspective, existing MoE dispatch patterns are still not able to fully exploit the underlying heterogeneous network environments. In this paper, we propose TA-MoE, a topology-aware routing strategy for large-scale MoE trainging, from a model-system co-design perspective, which can dynamically adjust the MoE dispatch pattern according to the network topology. Based on communication modeling, we abstract the dispatch problem into an optimization objective and obtain the approximate dispatch pattern under different topologies. On top of that, we design a topology-aware auxiliary loss, which can adaptively route the data to fit in the underlying topology without sacrificing the model accuracy. Experiments show that TA-MoE can substantially outperform its counterparts on various hardware and model configurations, with roughly 1.01x-1.61x, 1.01x-4.77x, 1.25x-1.54x improvements over the popular DeepSpeed-MoE, FastMoE and FasterMoE.", "year": 2023, "publicationdate": "2023-02-20", "externalids": {"DOI": "10.48550/arXiv.2302.09915"}, "doi_lower": "10.48550/arxiv.2302.09915"}
{"paper_id": 269024443, "title": "MPMoE: Memory Efficient MoE for Pre-Trained Models With Adaptive Pipeline Parallelism", "author_names": ["Zheng Zhang", "Yaqi Xia", "Hulin Wang", "Donglin Yang", "Chuang Hu", "Xiaobo Zhou", "Dazhao Cheng"], "venue": "IEEE Transactions on Parallel and Distributed Systems", "abstract": "In recent years, the Mixture-of-Experts (MoE) technique has gained widespread popularity as a means to scale pre-trained models to exceptionally large sizes. Dynamic activation of experts allows for conditional computation, increasing the number of parameters of neural networks, which is critical for absorbing the vast amounts of knowledge available in many deep learning areas. However, despite the existing system and algorithm optimizations, there are significant challenges to be tackled when it comes to the inefficiencies of communication and memory consumption. In this paper, we present the design and implementation of MPMoE, a high-performance library that accelerates MoE training with adaptive and memory-efficient pipeline parallelism. Inspired by that the MoE training procedure can be divided into multiple independent sub-stages. We design a pipeline parallelism method for reducing communication latency by overlapping with computation operations. Further, we analyze the memory footprint breakdown of MoE training and identify that activations and temporary buffers are the primary contributors to the overall memory footprint. Toward memory efficiency, we propose memory reuse strategies to reduce memory requirements by eliminating memory redundancies. Finally, to optimize pipeline granularity and memory reuse strategies jointly, we propose a profile-based algorithm and a performance model to determine the configurations of MPMoE at runtime. We implement MPMoE upon PyTorch and evaluate it with common MoE models in two physical clusters, including 64 NVIDIA A100 GPU cards and 16 NVIDIA V100 GPU cards. Compared with the state-of-art approach, MPMoE achieves up to 2.3× speedup while reducing more than 30% memory footprint for training large models.", "year": 2024, "publicationdate": "2024-06-01", "externalids": {"DOI": "10.1109/TPDS.2024.3385639"}, "doi_lower": "10.1109/tpds.2024.3385639"}
{"paper_id": 269456940, "title": "Lancet: Accelerating Mixture-of-Experts Training via Whole Graph Computation-Communication Overlapping", "author_names": ["Chenyu Jiang", "Ye Tian", "Zhen Jia", "Shuai Zheng", "Chuan Wu", "Yida Wang"], "venue": "Conference on Machine Learning and Systems", "abstract": "The Mixture-of-Expert (MoE) technique plays a crucial role in expanding the size of DNN model parameters. However, it faces the challenge of extended all-to-all communication latency during the training process. Existing methods attempt to mitigate this issue by overlapping all-to-all with expert computation. Yet, these methods frequently fall short of achieving sufficient overlap, consequently restricting the potential for performance enhancements. In our study, we extend the scope of this challenge by considering overlap at the broader training graph level. During the forward pass, we enable non-MoE computations to overlap with all-to-all through careful partitioning and pipelining. In the backward pass, we achieve overlap with all-to-all by scheduling gradient weight computations. We implement these techniques in Lancet, a system using compiler-based optimization to automatically enhance MoE model training. Our extensive evaluation reveals that Lancet significantly reduces the time devoted to non-overlapping communication, by as much as 77%. Moreover, it achieves a notable end-to-end speedup of up to 1.3 times when compared to the state-of-the-art solutions.", "year": 2024, "publicationdate": "2024-04-30", "externalids": {"DOI": "10.48550/arXiv.2404.19429"}, "doi_lower": "10.48550/arxiv.2404.19429"}
{"paper_id": 261390644, "title": "PipeMoE: Accelerating Mixture-of-Experts through Adaptive Pipelining", "author_names": ["S. Shi", "Xinglin Pan", "X. Chu", "Bo Li"], "venue": "IEEE Conference on Computer Communications", "abstract": "Large models have attracted much attention in the AI area. The sparsely activated mixture-of-experts (MoE) technique pushes the model size to a trillion-level with a sub-linear increase of computations as an MoE layer can be equipped with many separate experts, but only one or two experts need to be trained for each input data. However, the feature of dynamically activating experts of MoE introduces extensive communications in distributed training. In this work, we propose PipeMoE to adaptively pipeline the communications and computations in MoE to maximally hide the communication time. Specifically, we first identify the root reason why a higher pipeline degree does not always achieve better performance in training MoE models. Then we formulate an optimization problem that aims to minimize the training iteration time. To solve this problem, we build performance models for computation and communication tasks in MoE and develop an optimal solution to determine the pipeline degree such that the iteration time is minimal. We conduct extensive experiments with 174 typical MoE layers and two real-world NLP models on a 64-GPU cluster. Experimental results show that our PipeMoE almost always chooses the best pipeline degree and outperforms state-of-the-art MoE training systems by 5%-77% in training time.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {"DOI": "10.1109/INFOCOM53939.2023.10228874"}, "doi_lower": "10.1109/infocom53939.2023.10228874"}
{"paper_id": 269243508, "title": "ScheMoE: An Extensible Mixture-of-Experts Distributed Training System with Tasks Scheduling", "author_names": ["S. Shi", "Xinglin Pan", "Qiang Wang", "Chengjian Liu", "Xiaozhe Ren", "Zhongzhe Hu", "Yu Yang", "Bo Li", "Xiaowen Chu"], "venue": "European Conference on Computer Systems", "abstract": "In recent years, large-scale models can be easily scaled to trillions of parameters with sparsely activated mixture-of-experts (MoE), which significantly improves the model quality while only requiring a sub-linear increase in computational costs. However, MoE layers require the input data to be dynamically routed to a particular GPU for computing during distributed training. The highly dynamic property of data routing and high communication costs in MoE make the training system low scaling efficiency on GPU clusters. In this work, we propose an extensible and efficient MoE training system, ScheMoE, which is equipped with several features. 1) ScheMoE provides a generic scheduling framework that allows the communication and computation tasks in training MoE models to be scheduled in an optimal way. 2) ScheMoE integrates our proposed novel all-to-all collective which better utilizes intra- and inter-connect bandwidths. 3) ScheMoE supports easy extensions of customized all-to-all collectives and data compression approaches while enjoying our scheduling algorithm. Extensive experiments are conducted on a 32-GPU cluster and the results show that ScheMoE outperforms existing state-of-the-art MoE systems, Tutel and Faster-MoE, by 9%-30%.", "year": 2024, "publicationdate": "2024-04-22", "externalids": {"DOI": "10.1145/3627703.3650083"}, "doi_lower": "10.1145/3627703.3650083"}
{"paper_id": 258615489, "title": "Optimizing Distributed ML Communication with Fused Computation-Collective Operations", "author_names": ["Kishore Punniyamurthy", "Khaled Hamidouche", "Bradford M. Beckmann"], "venue": "International Conference for High Performance Computing, Networking, Storage and Analysis", "abstract": "Machine learning models are distributed across multiple nodes using numerous parallelism strategies. The resulting collective communication is often on the critical path due to a lack of independent coarse-grain computation kernels available to execute. In this work, we propose fusing computation with its subsequent collective communication and leverage GPUs’ massive parallelism, along with GPU-initiated communication, to overlap communication and computation. Specifically threadblocks/workgroups (WGs) immediately communicate their results to remote GPUs after completing their computation, while other WGs within the same kernel perform computation. We developed three prototype fused operators (embedding+All-toAll, GEMV+AllReduce, and GEMM+All-to-All) to address the communication overheads in DLRM, Transformers and MoE model architectures. We expose fused kernels as new PyTorch operators, as well as extend the Triton framework to demonstrate their practicality. Our evaluations show our approach effectively overlaps communication with computations, subsequently reducing their combined execution time achieving $\\mathbf{1 2 \\%}$ - $\\mathbf{3 1 \\%}$ lower execution time across all three operators.", "year": 2023, "publicationdate": "2023-05-11", "externalids": {"DOI": "10.1109/SC41406.2024.00094"}, "doi_lower": "10.1109/sc41406.2024.00094"}
{"paper_id": 261076133, "title": "Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference", "author_names": ["Ranggi Hwang", "Jianyu Wei", "Shijie Cao", "Changho Hwang", "Xiaohu Tang", "Ting Cao", "Mao Yang"], "venue": "International Symposium on Computer Architecture", "abstract": "Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-ofExperts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE’s high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE’s memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system codesign. Pre-gated MoE employs our novel pre-gating function which alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE is able to improve performance, reduce GPU memory consumption, while also maintaining the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.", "year": 2023, "publicationdate": "2023-08-23", "externalids": {"DOI": "10.1109/ISCA59077.2024.00078"}, "doi_lower": "10.1109/isca59077.2024.00078"}
{"paper_id": 283081399, "title": "EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models", "author_names": ["Rongjie Yi", "Liwei Guo", "Shiyun Wei", "Ao Zhou", "Shangguang Wang", "Mengwei Xu"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2308.14352"}, "doi_lower": "10.48550/arxiv.2308.14352"}
{"paper_id": 269983704, "title": "MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability", "author_names": ["Yanrui Du", "Sendong Zhao", "Danyang Zhao", "Ming Ma", "Yuhan Chen", "Liangyu Huo", "Qing Yang", "Dongliang Xu", "Bing Qin"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) are increasingly deployed in various applications. As their usage grows, concerns regarding their safety are rising, especially in maintaining harmless responses when faced with malicious instructions. Many defense strategies have been developed to enhance the safety of LLMs. However, our research finds that existing defense strategies lead LLMs to predominantly adopt a rejection-oriented stance, thereby diminishing the usability of their responses to benign instructions. To solve this problem, we introduce the MoGU framework, designed to enhance LLMs' safety while preserving their usability. Our MoGU framework transforms the base LLM into two variants: the usable LLM and the safe LLM, and further employs dynamic routing to balance their contribution. When encountering malicious instructions, the router will assign a higher weight to the safe LLM to ensure that responses are harmless. Conversely, for benign instructions, the router prioritizes the usable LLM, facilitating usable and helpful responses. On various open-sourced LLMs, we compare multiple defense strategies to verify the superiority of our MoGU framework. Besides, our analysis provides key insights into the effectiveness of MoGU and verifies that our designed routing mechanism can effectively balance the contribution of each variant by assigning weights. Our work released the safer Llama2, Vicuna, Falcon, Dolphin, and Baichuan2.", "year": 2024, "publicationdate": "2024-05-23", "externalids": {"DOI": "10.48550/arXiv.2405.14488"}, "doi_lower": "10.48550/arxiv.2405.14488"}
{"paper_id": 221784966, "title": "Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations", "author_names": ["Hongyan Tang", "Junning Liu", "Ming Zhao", "Xudong Gong"], "venue": "ACM Conference on Recommender Systems", "abstract": "Multi-task learning (MTL) has been successfully applied to many recommendation applications. However, MTL models often suffer from performance degeneration with negative transfer due to the complex and competing task correlation in real-world recommender systems. Moreover, through extensive experiments across SOTA MTL models, we have observed an interesting seesaw phenomenon that performance of one task is often improved by hurting the performance of some other tasks. To address these issues, we propose a Progressive Layered Extraction (PLE) model with a novel sharing structure design. PLE separates shared components and task-specific components explicitly and adopts a progressive routing mechanism to extract and separate deeper semantic knowledge gradually, improving efficiency of joint representation learning and information routing across tasks in a general setup. We apply PLE to both complicatedly correlated and normally correlated tasks, ranging from two-task cases to multi-task cases on a real-world Tencent video recommendation dataset with 1 billion samples, and results show that PLE outperforms state-of-the-art MTL models significantly under different task correlations and task-group size. Furthermore, online evaluation of PLE on a large-scale content recommendation platform at Tencent manifests 2.23% increase in view-count and 1.84% increase in watch time compared to SOTA MTL models, which is a significant improvement and demonstrates the effectiveness of PLE. Finally, extensive offline experiments on public benchmark datasets demonstrate that PLE can be applied to a variety of scenarios besides recommendations to eliminate the seesaw phenomenon. PLE now has been deployed to the online video recommender system in Tencent successfully.", "year": 2020, "publicationdate": "2020-09-22", "externalids": {"DOI": "10.1145/3383313.3412236"}, "doi_lower": "10.1145/3383313.3412236"}
{"paper_id": 248863193, "title": "AdaMCT: Adaptive Mixture of CNN-Transformer for Sequential Recommendation", "author_names": ["Juyong Jiang", "Jae Boum Kim", "Yingtao Luo", "Kai Zhang", "Sunghun Kim"], "venue": "International Conference on Information and Knowledge Management", "abstract": "Sequential recommendation (SR) aims to model users' dynamic preferences from a series of interactions. A pivotal challenge in user modeling for SR lies in the inherent variability of user preferences. An effective SR model is expected to capture both the long-term and short-term preferences exhibited by users, wherein the former can offer a comprehensive understanding of stable interests that impact the latter. To more effectively capture such information, we incorporate locality inductive bias into the Transformer by amalgamating its global attention mechanism with a local convolutional filter, and adaptively ascertain the mixing importance on a personalized basis through layer-aware adaptive mixture units, termed as AdaMCT. Moreover, as users may repeatedly browse potential purchases, it is expected to consider multiple relevant items concurrently in long-/short-term preferences modeling. Given that softmax-based attention may promote unimodal activation, we propose the Squeeze-Excitation Attention (with sigmoid activation) into SR models to capture multiple pertinent items (keys) simultaneously. Extensive experiments on three widely employed benchmarks substantiate the effectiveness and efficiency of our proposed approach. Source code is available at https://github.com/juyongjiang/AdaMCT.", "year": 2022, "publicationdate": "2022-05-18", "externalids": {"DOI": "10.1145/3583780.3614773"}, "doi_lower": "10.1145/3583780.3614773"}
{"paper_id": 269449211, "title": "M3oE: Multi-Domain Multi-Task Mixture-of Experts Recommendation Framework", "author_names": ["Zijian Zhang", "Shuchang Liu", "Jiaao Yu", "Qingpeng Cai", "Xiangyu Zhao", "Chunxu Zhang", "Ziru Liu", "Qidong Liu", "Hongwei Zhao", "Lantao Hu", "Peng Jiang", "Kun Gai"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Multi-domain recommendation and multi-task recommendation have demonstrated their effectiveness in leveraging common information from different domains and objectives for comprehensive user modeling. Nonetheless, the practical recommendation usually faces multiple domains and tasks simultaneously, which cannot be well-addressed by current methods. To this end, we introduce M3oE, an adaptive Multi-domain Multi-task Mixture-of-Experts recommendation framework. M3oE integrates multi-domain information, maps knowledge across domains and tasks, and optimizes multiple objectives. We leverage three mixture-of-experts modules to learn common, domain-aspect, and task-aspect user preferences respectively to address the complex dependencies among multiple domains and tasks in a disentangled manner. Additionally, we design a two-level fusion mechanism for precise control over feature extraction and fusion across diverse domains and tasks. The framework's adaptability is further enhanced by applying AutoML technique, which allows dynamic structure optimization. To the best of the authors' knowledge, our M3oE is the first effort to solve multi-domain multi-task recommendation self-adaptively. Extensive experiments on two benchmark datasets against diverse baselines demonstrate M3oE's superior performance. The implementation code is available to ensure reproducibility.", "year": 2024, "publicationdate": "2024-04-29", "externalids": {"DOI": "10.1145/3626772.3657686"}, "doi_lower": "10.1145/3626772.3657686"}
{"paper_id": 249394802, "title": "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts", "author_names": ["Basil Mustafa", "C. Riquelme", "J. Puigcerver", "Rodolphe Jenatton", "N. Houlsby"], "venue": "Neural Information Processing Systems", "abstract": "Large sparsely-activated models have obtained excellent performance in multiple domains. However, such models are typically trained on a single modality at a time. We present the Language-Image MoE, LIMoE, a sparse mixture of experts model capable of multimodal learning. LIMoE accepts both images and text simultaneously, while being trained using a contrastive loss. MoEs are a natural fit for a multimodal backbone, since expert layers can learn an appropriate partitioning of modalities. However, new challenges arise; in particular, training stability and balanced expert utilization, for which we propose an entropy-based regularization scheme. Across multiple scales, we demonstrate remarkable performance improvement over dense models of equivalent computational cost. LIMoE-L/16 trained comparably to CLIP-L/14 achieves 78.6% zero-shot ImageNet accuracy (vs. 76.2%), and when further scaled to H/14 (with additional data) it achieves 84.1%, comparable to state-of-the-art methods which use larger custom per-modality backbones and pre-training schemes. We analyse the quantitative and qualitative behavior of LIMoE, and demonstrate phenomena such as differing treatment of the modalities and the organic emergence of modality-specific experts.", "year": 2022, "publicationdate": "2022-06-06", "externalids": {"DOI": "10.48550/arXiv.2206.02770"}, "doi_lower": "10.48550/arxiv.2206.02770"}
{"paper_id": 257496100, "title": "Scaling Vision-Language Models with Sparse Mixture of Experts", "author_names": ["Sheng Shen", "Z. Yao", "Chunyuan Li", "Trevor Darrell", "K. Keutzer", "Yuxiong He"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The field of natural language processing (NLP) has made significant strides in recent years, particularly in the development of large-scale vision-language models (VLMs). These models aim to bridge the gap between text and visual information, enabling a more comprehensive understanding of multimedia data. However, as these models become larger and more complex, they also become more challenging to train and deploy. One approach to addressing this challenge is the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the model into smaller, specialized sub-models that can jointly solve a task. In this paper, we explore the effectiveness of MoE in scaling vision-language models, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost. Our research offers valuable insights into stabilizing the training of MoE models, understanding the impact of MoE on model interpretability, and balancing the trade-offs between compute performance when scaling VLMs. We hope our work will inspire further research into the use of MoE for scaling large-scale vision-language models and other multimodal machine learning applications.", "year": 2023, "publicationdate": "2023-03-13", "externalids": {"DOI": "10.48550/arXiv.2303.07226"}, "doi_lower": "10.48550/arxiv.2303.07226"}
{"paper_id": 258865469, "title": "PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts", "author_names": ["Yunshui Li", "Binyuan Hui", "Zhichao Yin", "Min Yang", "Fei Huang", "Yongbin Li"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Perceiving multi-modal information and fulfilling dialogues with humans is a long-term goal of artificial intelligence. Pre-training is commonly regarded as an effective approach for multi-modal dialogue. However, due to the limited availability of multi-modal dialogue data, there is still scarce research on multi-modal dialogue pre-training. Yet another intriguing challenge emerges from the encompassing nature of multi-modal dialogue, which involves various modalities and tasks. Moreover, new forms of tasks may arise at unpredictable points in the future. Hence, it is essential for designed multi-modal dialogue models to possess sufficient flexibility to adapt to such scenarios. This paper proposes PaCE, a unified, structured, compositional multi-modal dialogue pre-training framework. It utilizes a combination of several fundamental experts to accommodate multiple dialogue-related tasks and can be pre-trained using limited dialogue and extensive non-dialogue multi-modal data. Furthermore, we propose a progressive training method where old experts from the past can assist new experts, facilitating the expansion of their capabilities. Experimental results demonstrate that PaCE achieves state-of-the-art results on eight multi-modal dialog benchmarks.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.14839"}, "doi_lower": "10.48550/arxiv.2305.14839"}
{"paper_id": 269921303, "title": "Uni-MoE: Scaling Unified Multimodal LLMs With Mixture of Experts", "author_names": ["Yunxin Li", "Shenyuan Jiang", "Baotian Hu", "Longyue Wang", "Wanqi Zhong", "Wenhan Luo", "Lin Ma", "Min Zhang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) underscore the significance of scalable models and data to boost performance, yet this often incurs substantial computational costs. Although the Mixture of Experts (MoE) architecture has been employed to scale large language or visual-language models efficiently, these efforts typically involve fewer experts and limited modalities. To address this, our work presents the pioneering attempt to develop a unified MLLM with the MoE architecture, named Uni-MoE that can handle a wide array of modalities. Specifically, it features modality-specific encoders with connectors for a unified multimodal representation. We also implement a sparse MoE architecture within the LLMs to enable efficient training and inference through modality-level data parallelism and expert-level model parallelism. To enhance the multi-expert collaboration and generalization, we present a progressive training strategy: 1) Cross-modality alignment using various connectors with different cross-modality data, 2) Training modality-specific experts with cross-modality instruction data to activate experts’ preferences, and 3) Tuning the whole Uni-MoE framework utilizing Low-Rank Adaptation (LoRA) on mixed multimodal instruction data. We evaluate the instruction-tuned Uni-MoE on a comprehensive set of multimodal datasets. The extensive experimental results demonstrate Uni-MoE's principal advantage of significantly reducing performance bias in handling mixed multimodal datasets, alongside improved multi-expert collaboration and generalization.", "year": 2024, "publicationdate": "2024-05-18", "externalids": {"DOI": "10.1109/TPAMI.2025.3532688"}, "doi_lower": "10.1109/tpami.2025.3532688"}
{"paper_id": 274023923, "title": "MM1: Methods, Analysis and Insights from Multimodal LLM Pre-training", "author_names": ["Brandon McKinzie", "Zhe Gan", "J. Fauconnier", "Sam Dodge", "Bowen Zhang", "Philipp Dufter", "Dhruti Shah", "Xianzhi Du", "Futang Peng", "Anton Belyi", "Haotian Zhang", "Karanjeet Singh", "Doug Kang", "Hongyu Hè", "Max Schwarzer", "Tom Gunter", "Xiang Kong", "Aonan Zhang", "Jianyu Wang", "Chong Wang", "Nan Du", "Tao Lei", "Sam Wiseman", "Mark Lee", "Zirui Wang", "Ruoming Pang", "Peter Grasch", "Alexander Toshev", "Yinfei Yang"], "venue": "European Conference on Computer Vision", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.1007/978-3-031-73397-0_18"}, "doi_lower": "10.1007/978-3-031-73397-0_18"}
{"paper_id": 259950998, "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "author_names": ["Hugo Touvron", "Louis Martin", "Kevin R. Stone", "Peter Albert", "Amjad Almahairi", "Yasmine Babaei", "Niko-lay Bashlykov", "Soumya Batra", "Prajjwal Bhargava", "Shruti Bhosale", "D. Bikel", "Lukas Blecher", "Cris-tian Cantón Ferrer", "Moya Chen", "Guillem Cucurull", "David Esiobu", "Jude Fernandes", "J. Fu", "Wenyin Fu", "Brian Fuller", "Cynthia Gao", "Vedanuj Goswami", "Naman Goyal", "A. Hartshorn", "Saghar Hosseini", "Rui Hou", "Hakan Inan", "Marcin Kardas", "Viktor Kerkez", "Madian Khabsa", "Isabel M. Kloumann", "A. Korenev", "Punit Singh Koura", "M. Lachaux", "Thibaut Lavril", "Jenya Lee", "Diana Liskovich", "Yinghai Lu", "Yuning Mao", "Xavier Martinet", "Todor Mihaylov", "Pushkar Mishra", "Igor Molybog", "Yixin Nie", "Andrew Poulton", "J. Reizenstein", "Rashi Rungta", "Kalyan Saladi", "A. Schelten", "Ruan Silva", "Eric Michael Smith", "R. Subramanian", "Xia Tan", "Binh Tang", "Ross Taylor", "Adina Williams", "Jian Xiang Kuan", "Puxin Xu", "Zhengxu Yan", "Iliyan Zarov", "Yuchen Zhang", "Angela Fan", "M. Kambadur", "Sharan Narang", "Aur'elien Rodriguez", "Robert Stojnic", "Sergey Edunov", "Thomas Scialom"], "venue": "arXiv.org", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "year": 2023, "publicationdate": "2023-07-18", "externalids": {}, "doi_lower": null}
{"paper_id": 258989607, "title": "ChatGPT and large language models in gastroenterology", "author_names": ["Prateek Sharma", "S. Parasa"], "venue": "Nature reviews: Gastroenterology & hepatology", "abstract": null, "year": 2023, "publicationdate": "2023-05-30", "externalids": {"DOI": "10.1038/s41575-023-00799-8"}, "doi_lower": "10.1038/s41575-023-00799-8"}
{"paper_id": 266818336, "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism", "author_names": ["DeepSeek-AI Xiao Bi", "Deli Chen", "Guanting Chen", "Shanhuang Chen", "Damai Dai", "C. Deng", "Honghui Ding", "Kai Dong", "Qiushi Du", "Zhe Fu", "Huazuo Gao", "Kaige Gao", "Wenjun Gao", "Ruiqi Ge", "Kang Guan", "Daya Guo", "Jianzhong Guo", "Guangbo Hao", "Zhewen Hao", "Ying He", "Wen-Hui Hu", "Panpan Huang", "Erhang Li", "Guowei Li", "Jiashi Li", "Yao Li", "Y. K. Li", "W. Liang", "Fangyun Lin", "A. Liu", "Bo Liu (Benjamin Liu)", "Wen Liu", "Xiaodong Liu", "Xin Liu", "Yiyuan Liu", "Haoyu Lu", "Shanghao Lu", "Fuli Luo", "Shirong Ma", "X. Nie", "Tian Pei", "Yishi Piao", "Junjie Qiu", "Hui Qu", "Tongzheng Ren", "Z. Ren", "C. Ruan", "Zhangli Sha", "Zhihong Shao", "Jun-Mei Song", "Xuecheng Su", "Jingxiang Sun", "Yaofeng Sun", "Min Tang", "Bing-Li Wang", "Peiyi Wang", "Shiyu Wang", "Yaohui Wang", "Yongji Wang", "Tong Wu", "Yu Wu", "Xin Xie", "Zhenda Xie", "Ziwei Xie", "Yi Xiong", "Hanwei Xu", "R. X. Xu", "Yanhong Xu", "Dejian Yang", "Yu-mei You", "Shuiping Yu", "Xin-yuan Yu", "Bo Zhang", "Haowei Zhang", "Lecong Zhang", "Liyue Zhang", "Mingchuan Zhang", "Minghu Zhang", "Wentao Zhang", "Yichao Zhang", "Chenggang Zhao", "Yao Zhao", "Shangyan Zhou", "Shunfeng Zhou", "Qihao Zhu", "Yuheng Zou"], "venue": "arXiv.org", "abstract": "The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.", "year": 2024, "publicationdate": "2024-01-05", "externalids": {}, "doi_lower": null}
{"paper_id": 152273371, "title": "Introducing JMSSays: Introducing JMSSays", "author_names": ["R. Delbridge", "R. Suddaby", "Bill Harley"], "venue": "", "abstract": null, "year": 2016, "publicationdate": "2016-03-01", "externalids": {"DOI": "10.1111/JOMS.12174"}, "doi_lower": "10.1111/joms.12174"}
{"paper_id": 7047554, "title": "Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks", "author_names": ["Andrew S. Davis", "I. Arel"], "venue": "International Conference on Learning Representations", "abstract": "Scalability properties of deep neural networks raise key research questions, particularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced by Bengio, et. al., where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity activation can be efficiently obtained. For networks using rectified-linear hidden units, this implies that the computation of a hidden unit with an estimated negative pre-nonlinearity can be ommitted altogether, as its value will become zero when nonlinearity is applied. For sparse neural networks, this can result in considerable speed gains. Experimental results using the MNIST and SVHN data sets with a fully-connected deep neural network demonstrate the performance robustness of the proposed scheme with respect to the error introduced by the conditional computation process.", "year": 2013, "publicationdate": "2013-12-16", "externalids": {}, "doi_lower": null}
{"paper_id": 818973, "title": "Dynamic Capacity Networks", "author_names": ["Amjad Almahairi", "Nicolas Ballas", "Tim Cooijmans", "Yin Zheng", "H. Larochelle", "Aaron C. Courville"], "venue": "International Conference on Machine Learning", "abstract": "We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity sub-networks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies input regions for which the DCN's output is most sensitive and to which we should devote more capacity. We focus our empirical evaluation on the Cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar or even better performance.", "year": 2015, "publicationdate": "2015-11-24", "externalids": {}, "doi_lower": null}
{"paper_id": 16049527, "title": "Conditional Computation in Neural Networks for faster models", "author_names": ["Emmanuel Bengio", "Pierre-Luc Bacon", "Joelle Pineau", "Doina Precup"], "venue": "arXiv.org", "abstract": "Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive. The conditional computation approach has been proposed to tackle this problem (Bengio et al., 2013; Davis & Arel, 2013). It operates by selectively activating only parts of the network at a time. In this paper, we use reinforcement learning as a tool to optimize conditional computation policies. More specifically, we cast the problem of learning activation-dependent policies for dropping out blocks of units as a reinforcement learning problem. We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy. We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy. We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation.", "year": 2015, "publicationdate": "2015-11-19", "externalids": {}, "doi_lower": null}
{"paper_id": 22014305, "title": "Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning", "author_names": ["C. Rosenbaum", "Tim Klinger", "M. Riemer"], "venue": "International Conference on Learning Representations", "abstract": "Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network - for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we obtain cross-stitch performance levels with an 85% reduction in training time.", "year": 2017, "publicationdate": "2017-11-03", "externalids": {}, "doi_lower": null}
{"paper_id": 139103965, "title": "Routing Networks and the Challenges of Modular and Compositional Computation", "author_names": ["C. Rosenbaum", "Ignacio Cases", "M. Riemer", "Tim Klinger"], "venue": "arXiv.org", "abstract": "Compositionality is a key strategy for addressing combinatorial complexity and the curse of dimensionality. Recent work has shown that compositional solutions can be learned and offer substantial gains across a variety of domains, including multi-task learning, language modeling, visual question answering, machine comprehension, and others. However, such models present unique challenges during training when both the module parameters and their composition must be learned jointly. In this paper, we identify several of these issues and analyze their underlying causes. Our discussion focuses on routing networks, a general approach to this problem, and examines empirically the interplay of these challenges and a variety of design decisions. In particular, we consider the effect of how the algorithm decides on module composition, how the algorithm updates the modules, and if the algorithm uses regularization.", "year": 2019, "publicationdate": "2019-04-29", "externalids": {}, "doi_lower": null}
{"paper_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "author_names": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "venue": "Journal of machine learning research", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.", "year": 2019, "publicationdate": "2019-10-23", "externalids": {}, "doi_lower": null}
{"paper_id": 259138847, "title": "The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers", "author_names": ["Zong-xiao Li", "Chong You", "Srinadh Bhojanapalli", "Daliang Li", "A. Rawat", "Sashank J. Reddi", "Kenneth Q Ye", "Felix Chern", "Felix X. Yu", "Ruiqi Guo", "Surinder Kumar"], "venue": "International Conference on Learning Representations", "abstract": "This paper studies the curious phenomenon for machine learning models with Transformer architectures that their activation maps are sparse. By activation map we refer to the intermediate output of the multi-layer perceptrons (MLPs) after a ReLU activation function, and by sparse we mean that on average very few entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each input to MLP. Moreover, larger Transformers with more layers and wider MLP hidden dimensions are sparser as measured by the percentage of nonzero entries. Through extensive experiments we demonstrate that the emergence of sparsity is a prevalent phenomenon that occurs for both natural language processing and vision tasks, on both training and evaluation data, for Transformers of various configurations, at layers of all depth levels, as well as for other architectures including MLP-mixers and 2-layer MLPs. We show that sparsity also emerges using training datasets with random labels, or with random inputs, or with infinite amount of data, demonstrating that sparsity is not a result of a specific family of datasets. We discuss how sparsity immediately implies a way to significantly reduce the FLOP count and improve efficiency for Transformers. Moreover, we demonstrate perhaps surprisingly that enforcing an even sparser activation via Top-k thresholding with a small value of k brings a collection of desired but missing properties for Transformers, namely less sensitivity to noisy training data, more robustness to input corruptions, and better calibration for their prediction confidence.", "year": 2022, "publicationdate": "2022-10-12", "externalids": {}, "doi_lower": null}
{"paper_id": 6869636, "title": "Hard Mixtures of Experts for Large Scale Weakly Supervised Vision", "author_names": ["Sam Gross", "Marc'Aurelio Ranzato", "Arthur Szlam"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Training convolutional networks (CNNs) that fit on a single GPU with minibatch stochastic gradient descent has become effective in practice. However, there is still no effective method for training large networks that do not fit in the memory of a few GPU cards, or for parallelizing CNN training. In this work we show that a simple hard mixture of experts model can be efficiently trained to good effect on large scale hashtag (multilabel) prediction tasks. Mixture of experts models are not new [7, 3], but in the past, researchers have had to devise sophisticated methods to deal with data fragmentation. We show empirically that modern weakly supervised data sets are large enough to support naive partitioning schemes where each data point is assigned to a single expert. Because the experts are independent, training them in parallel is easy, and evaluation is cheap for the size of the model. Furthermore, we show that we can use a single decoding layer for all the experts, allowing a unified feature embedding space. We demonstrate that it is feasible (and in fact relatively painless) to train far larger models than could be practically trained with standard CNN architectures, and that the extra capacity can be well used on current datasets.", "year": 2017, "publicationdate": "2017-04-20", "externalids": {"DOI": "10.1109/CVPR.2017.540"}, "doi_lower": "10.1109/cvpr.2017.540"}
{"paper_id": 235458009, "title": "LoRA: Low-Rank Adaptation of Large Language Models", "author_names": ["J. E. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Weizhu Chen"], "venue": "International Conference on Learning Representations", "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.", "year": 2021, "publicationdate": "2021-06-17", "externalids": {}, "doi_lower": null}
{"paper_id": 272366674, "title": "OLMoE: Open Mixture-of-Experts Language Models", "author_names": ["Niklas Muennighoff", "Luca Soldaini", "Dirk Groeneveld", "Kyle Lo", "Jacob Daniel Morrison", "Sewon Min", "Weijia Shi", "Pete Walsh", "Oyvind Tafjord", "Nathan Lambert", "Yuling Gu", "Shane Arora", "Akshita Bhagia", "Dustin Schwenk", "David Wadden", "Alexander Wettig", "Binyuan Hui", "Tim Dettmers", "Douwe Kiela", "Ali Farhadi", "Noah A. Smith", "Pang Wei Koh", "Amanpreet Singh", "Hanna Hajishirzi"], "venue": "arXiv.org", "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.", "year": 2024, "publicationdate": "2024-09-03", "externalids": {"DOI": "10.48550/arXiv.2409.02060"}, "doi_lower": "10.48550/arxiv.2409.02060"}
{"paper_id": 275118643, "title": "DeepSeek-V3 Technical Report", "author_names": ["DeepSeek-AI", "A. Liu", "Bei Feng", "Bing Xue", "Bing-Li Wang", "Bochao Wu", "Chengda Lu", "Chenggang Zhao", "C. Deng", "Chenyu Zhang", "C. Ruan", "Damai Dai", "Daya Guo", "Dejian Yang", "Deli Chen", "Dong-Li Ji", "Erhang Li", "Fangyun Lin", "Fucong Dai", "Fuli Luo", "Guangbo Hao", "Guanting Chen", "Guowei Li", "H. Zhang", "Han Bao", "Hanwei Xu", "Haocheng Wang", "Haowei Zhang", "Honghui Ding", "Huajian Xin", "Huazuo Gao", "Hui Li", "Hui Qu", "J. Cai", "Jian Liang", "Jianzhong Guo", "J. Ni", "Jiashi Li", "Jiawei Wang", "Jin Chen", "JingChang Chen", "Jingyang Yuan", "Junjie Qiu", "Junlong Li", "Jun-Mei Song", "Kai Dong", "Kai Hu", "Kaige Gao", "Kang Guan", "Kexin Huang", "K. Yu", "Lean Wang", "Lecong Zhang", "Lei Xu", "Leyi Xia", "Liang Zhao", "Litong Wang", "Liyue Zhang", "Meng Li", "Miaojun Wang", "Mingchuan Zhang", "Minghua Zhang", "Minghui Tang", "Mingming Li", "Ning Tian", "Panpan Huang", "Peiyi Wang", "Peng Zhang", "Qiancheng Wang", "Qihao Zhu", "Qinyu Chen", "Qiushi Du", "R. J. Chen", "R. Jin", "Ruiqi Ge", "Ruisong Zhang", "Ruizhe Pan", "Runji Wang", "R. Xu", "Ruoyu Zhang", "Ruyi Chen", "S. S. Li", "Shanghao Lu", "Shangyan Zhou", "Shanhuang Chen", "Shao-Ping Wu", "Shengfeng Ye", "Shirong Ma", "Shiyu Wang", "Shuang Zhou", "Shuiping Yu", "Shunfeng Zhou", "Shuting Pan", "T. Wang", "Tao Yun", "Tian Pei", "T. Sun", "W. Xiao", "Wangding Zeng", "Wanjia Zhao", "Wei An", "Wen Liu", "W. Liang", "Wenjun Gao", "Wen-xuan Yu", "Wentao Zhang", "X. Q. Li", "Xiangyu Jin", "Xianzu Wang", "Xiaoling Bi", "Xiaodong Liu", "Xiaohan Wang", "Xi-Cheng Shen", "Xiaokang Chen", "Xiaokang Zhang", "Xiaosha Chen", "X. Nie", "Xiaowen Sun", "Xiaoxiang Wang", "Xin Cheng", "Xin Liu", "Xin Xie", "Xingchao Liu", "Xingkai Yu", "Xinnan Song", "Xinxia Shan", "Xinyi Zhou", "Xinyu Yang", "Xinyuan Li", "Xuecheng Su", "Xuheng Lin", "Y. K. Li", "Y. Q. Wang", "Y. X. Wei", "Y. X. Zhu", "Yang Zhang", "Yanhong Xu", "Yanping Huang", "Yao Li", "Yao Zhao", "Yaofeng Sun", "Yao Li", "Yaohui Wang", "Yi Yu", "Yi Zheng", "Yichao Zhang", "Yifan Shi", "Yi Xiong", "Ying He", "Ying Tang", "Yishi Piao", "Yisong Wang", "Yixuan Tan", "Yi-Bing Ma", "Yiyuan Liu", "Yongqiang Guo", "Yu Wu", "Y. Ou", "Yuchen Zhu", "Yuduan Wang", "Yue Gong", "Yuheng Zou", "Yujia He", "Y. Zha", "Yunfan Xiong", "Yunxiang Ma", "Yuting Yan", "Yu-Wei Luo", "Yu-mei You", "Yuxuan Liu", "Yuyang Zhou", "Z. F. Wu", "Z. Ren", "Z. Ren", "Zhangli Sha", "Zhe Fu", "Zhean Xu", "Zhen Huang", "Zhen Zhang", "Zhenda Xie", "Zhen-guo Zhang", "Zhewen Hao", "Zhibin Gou", "Zhicheng Ma", "Zhigang Yan", "Zhihong Shao", "Zhipeng Xu", "Zhiyu Wu", "Zhongyu Zhang", "Zhuoshu Li", "Zihui Gu", "Zijia Zhu", "Zijun Liu", "Zi-An Li", "Ziwei Xie", "Ziyang Song", "Ziyi Gao", "Zizheng Pan"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 195886317, "title": "Large Memory Layers with Product Keys", "author_names": ["Guillaume Lample", "Alexandre Sablayrolles", "Marc'Aurelio Ranzato", "Ludovic Denoyer", "H. Jégou"], "venue": "Neural Information Processing Systems", "abstract": "This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.", "year": 2019, "publicationdate": "2019-07-10", "externalids": {}, "doi_lower": null}
{"paper_id": 265551773, "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "author_names": ["Albert Gu", "Tri Dao"], "venue": "arXiv.org", "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.", "year": 2023, "publicationdate": "2023-12-01", "externalids": {}, "doi_lower": null}
{"paper_id": 221516475, "title": "Measuring Massive Multitask Language Understanding", "author_names": ["Dan Hendrycks", "Collin Burns", "Steven Basart", "Andy Zou", "Mantas Mazeika", "D. Song", "J. Steinhardt"], "venue": "International Conference on Learning Representations", "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.", "year": 2020, "publicationdate": "2020-09-07", "externalids": {}, "doi_lower": null}
{"paper_id": 239998651, "title": "Training Verifiers to Solve Math Word Problems", "author_names": ["K. Cobbe", "Vineet Kosaraju", "Mo Bavarian", "Mark Chen", "Heewoo Jun", "Lukasz Kaiser", "Matthias Plappert", "Jerry Tworek", "Jacob Hilton", "Reiichiro Nakano", "Christopher Hesse", "John Schulman"], "venue": "arXiv.org", "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.", "year": 2021, "publicationdate": "2021-10-27", "externalids": {}, "doi_lower": null}
{"paper_id": 232134851, "title": "Measuring Mathematical Problem Solving With the MATH Dataset", "author_names": ["Dan Hendrycks", "Collin Burns", "Saurav Kadavath", "Akul Arora", "Steven Basart", "Eric Tang", "D. Song", "J. Steinhardt"], "venue": "NeurIPS Datasets and Benchmarks", "abstract": "Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.", "year": 2021, "publicationdate": "2021-03-05", "externalids": {}, "doi_lower": null}
{"paper_id": 235755472, "title": "Evaluating Large Language Models Trained on Code", "author_names": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Pondé", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "Scott Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mo Bavarian", "Clemens Winter", "P. Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "Jan Leike", "Josh Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "I. Sutskever", "Wojciech Zaremba"], "venue": "arXiv.org", "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.", "year": 2021, "publicationdate": "2021-07-07", "externalids": {}, "doi_lower": null}
{"paper_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "author_names": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/N19-1423"}, "doi_lower": "10.18653/v1/n19-1423"}
{"paper_id": 113405151, "title": "Root Mean Square Layer Normalization", "author_names": ["Biao Zhang", "Rico Sennrich"], "venue": "Neural Information Processing Systems", "abstract": "Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network, e.g. RNN in particular. In this paper, we hypothesize that re-centering invariance in LayerNorm is dispensable and propose root mean square layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm. We also present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of the summed inputs without breaking the above properties. Extensive experiments on several tasks using diverse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7%~64% on different models. Source code is available at https://github.com/bzhangGo/rmsnorm.", "year": 2019, "publicationdate": "2019-10-16", "externalids": {"DOI": "10.5167/UZH-177483"}, "doi_lower": "10.5167/uzh-177483"}
{"paper_id": 233307138, "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "author_names": ["Jianlin Su", "Yu Lu", "Shengfeng Pan", "Bo Wen", "Yunfeng Liu"], "venue": "Neurocomputing", "abstract": null, "year": 2021, "publicationdate": "2021-04-20", "externalids": {"DOI": "10.1016/j.neucom.2023.127063"}, "doi_lower": "10.1016/j.neucom.2023.127063"}
{"paper_id": 268553763, "title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey", "author_names": ["Zeyu Han", "Chao Gao", "Jinyang Liu", "Jeff Zhang", "Sai Qian Zhang"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adjusting the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large model to adapt it to a specific task or domain while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large-scale language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to providing an extensive survey from an algorithmic standpoint, we also examine various real-world system designs to investigate the implementation costs associated with different PEFT approaches. This survey serves as a valuable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed ......", "year": 2024, "publicationdate": "2024-03-21", "externalids": {"DOI": "10.48550/arXiv.2403.14608"}, "doi_lower": "10.48550/arxiv.2403.14608"}
{"paper_id": 257771591, "title": "Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning", "author_names": ["Vladislav Lialin", "Vijeta Deshpande", "Anna Rumshisky"], "venue": "arXiv.org", "abstract": "This paper presents a systematic overview of parameter-efficient fine-tuning methods, covering over 50 papers published between early 2019 and mid-2024. These methods aim to address the challenges of fine-tuning large language models by training only a small subset of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a specific focus on real-life efficiency in fine-tuning multibillion-scale language models. We also conduct an extensive head-to-head experimental comparison of 15 diverse PEFT methods, evaluating their performance and efficiency on models up to 11B parameters. Our findings reveal that methods previously shown to surpass a strong LoRA baseline face difficulties in resource-constrained settings, where hyperparameter optimization is limited and the network is fine-tuned only for a few epochs. Finally, we provide a set of practical recommendations for using PEFT methods and outline potential future research directions.", "year": 2023, "publicationdate": "2023-03-28", "externalids": {}, "doi_lower": null}
{"paper_id": 248693283, "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning", "author_names": ["Haokun Liu", "Derek Tam", "Mohammed Muqeeth", "Jay Mohta", "Tenghao Huang", "Mohit Bansal", "Colin Raffel"], "venue": "Neural Information Processing Systems", "abstract": "Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA)$^3$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6% absolute. All of the code used in our experiments is publicly available.", "year": 2022, "publicationdate": "2022-05-11", "externalids": {"DOI": "10.48550/arXiv.2205.05638"}, "doi_lower": "10.48550/arxiv.2205.05638"}
{"paper_id": 269588500, "title": "A Case Study of Instruction Tuning with Mixture of Parameter-Efficient Experts", "author_names": ["O. Ostapenko", "Lucas Caccia", "Zhan Su", "Nicolas Le Roux", "Laurent Charlin", "Alessandro Sordoni"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 230433941, "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "author_names": ["Xiang Lisa Li", "Percy Liang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.", "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2021.acl-long.353"}, "doi_lower": "10.18653/v1/2021.acl-long.353"}
{"paper_id": 257353502, "title": "Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers", "author_names": ["Tianlong Chen", "Zhenyu (Allen) Zhang", "Ajay Jaiswal", "Shiwei Liu", "Zhangyang Wang"], "venue": "International Conference on Learning Representations", "abstract": "Despite their remarkable achievement, gigantic transformers encounter significant drawbacks, including exorbitant computational and memory footprints during training, as well as severe collapse evidenced by a high degree of parameter redundancy. Sparsely-activated Mixture-of-Experts (SMoEs) have shown promise to mitigate the issue of training efficiency, yet they are prone to (1) redundant experts due to representational collapse; and (2) poor expert scalability for inference and downstream fine-tuning, primarily due to overfitting of the learned routing policy to the number of activated experts during training. As recent research efforts are predominantly focused on improving routing policies to encourage expert specializations, this work focuses on exploring the overlooked scalability bottleneck of SMoEs and leveraging it to effectively scale dense transformers. To this end, we propose a new plug-and-play training framework, SMoE-Dropout, to enable scaling transformers to better accuracy in their full capacity without collapse. Specifically, SMoE-Dropout consists of a randomly initialized and fixed router network to activate experts and gradually increases the activated expert number as training progresses over time. Transformers trained by SMoE-Dropout naturally exhibit a self-slimmable property subject to resource availability, offering smooth and consistent performance boosts with an increase in activated experts during inference or fine-tuning. Our extensive experiments demonstrate the superior performance and substantial computation savings of SMoE-Dropout, compared to dense training baselines with equivalent parameter counts. In particular, our trained BERT outperforms its densely trained counterpart with consistent improvements of {1.03%, 0.78%, 1.09%} on challenging reasoning tasks {ASDiv-A, MAWPS, SVAMP}, respectively.", "year": 2023, "publicationdate": "2023-03-02", "externalids": {"DOI": "10.48550/arXiv.2303.01610"}, "doi_lower": "10.48550/arxiv.2303.01610"}
{"paper_id": 4704285, "title": "Overcoming catastrophic forgetting in neural networks", "author_names": ["J. Kirkpatrick", "Razvan Pascanu", "Neil C. Rabinowitz", "J. Veness", "Guillaume Desjardins", "Andrei A. Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "A. Grabska-Barwinska", "D. Hassabis", "C. Clopath", "D. Kumaran", "R. Hadsell"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "abstract": "Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.", "year": 2016, "publicationdate": "2016-12-02", "externalids": {"DOI": "10.1073/pnas.1611835114"}, "doi_lower": "10.1073/pnas.1611835114"}
{"paper_id": 244486551, "title": "Deep Learning on Supercomputers", "author_names": ["Zhao Zhang", "V. Codreanu", "Ian T. Foster"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 269617042, "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models", "author_names": ["Samyam Rajbhandari", "Jeff Rasley", "Olatunji Ruwase", "Yuxiong He"], "venue": "International Conference for High Performance Computing, Networking, Storage and Analysis", "abstract": "Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today’s hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8. 3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create Turing-NLG, the world’s largest language model at the time (17B parameters) with record breaking accuracy.", "year": 2019, "publicationdate": "2019-10-04", "externalids": {"DOI": "10.1109/SC41405.2020.00024"}, "doi_lower": "10.1109/sc41405.2020.00024"}
{"paper_id": 233289729, "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning", "author_names": ["Samyam Rajbhandari", "Olatunji Ruwase", "Jeff Rasley", "Shaden Smith", "Yuxiong He"], "venue": "International Conference for High Performance Computing, Networking, Storage and Analysis", "abstract": "In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model. In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs (40% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed 11DeepSpeed (https://www.deepspeed.ai/) is a deep learning optimization library designed to make distributed training easy, efficient, and effective. DeepSpeed has been extensively adopted by the DL community..", "year": 2021, "publicationdate": "2021-04-16", "externalids": {"DOI": "10.1145/3458817.3476205"}, "doi_lower": "10.1145/3458817.3476205"}
{"paper_id": 247289671, "title": "BaGuaLu: targeting brain scale pretrained models with over 37 million cores", "author_names": ["Zixuan Ma", "Jiaao He", "J. Qiu", "Huan Cao", "Yuanwei Wang", "Zhenbo Sun", "Liyan Zheng", "Haojie Wang", "Shizhi Tang", "Tianyu Zheng", "Junyang Lin", "Guanyu Feng", "Zeqiang Huang", "Jie Gao", "Aohan Zeng", "Jianwei Zhang", "Runxin Zhong", "Tianhui Shi", "Shangkun Liu", "Weimin Zheng", "Jie Tang", "Hongxia Yang", "Xin Liu", "Jidong Zhai", "Wenguang Chen"], "venue": "ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming", "abstract": "Large-scale pretrained AI models have shown state-of-the-art accuracy in a series of important applications. As the size of pretrained AI models grows dramatically each year in an effort to achieve higher accuracy, training such models requires massive computing and memory capabilities, which accelerates the convergence of AI and HPC. However, there are still gaps in deploying AI applications on HPC systems, which need application and system co-design based on specific hardware features. To this end, this paper proposes BaGuaLu1, the first work targeting training brain scale models on an entire exascale supercomputer, the New Generation Sunway Supercomputer. By combining hardware-specific intra-node optimization and hybrid parallel strategies, BaGuaLu enables decent performance and scalability on unprecedentedly large models. The evaluation shows that BaGuaLu can train 14.5-trillion-parameter models with a performance of over 1 EFLOPS using mixed-precision and has the capability to train 174-trillion-parameter models, which rivals the number of synapses in a human brain.", "year": 2022, "publicationdate": "2022-03-28", "externalids": {"DOI": "10.1145/3503221.3508417"}, "doi_lower": "10.1145/3503221.3508417"}
{"paper_id": 280572161, "title": "The ASPLOS 2025 / EuroSys 2025 Contest on Intra-Operator Parallelism for Distributed Deep Learning", "author_names": ["Michael D. Moffitt", "Pratik Fegade"], "venue": "Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3", "abstract": "A chief enabler of large-scale deep learning is the distribution of computation across multiple interconnected hardware accelerators. In order to unlock the maximum possible performance, a compiler must first select a reasonable strategy to parallelize a model's operations. Since neural network architectures admit multiple flavors of parallelism, determining the proper strategy for each instruction is a critical (albeit non-trivial) task. To solicit new ideas toward solving this challenging combinatorial optimization problem, we organized the ASPLOS 2025 / EuroSys 2025 Contest on Intra-Operator Parallelism for Distributed Deep Learning, a multi-month competition focused on advancing the state-of-the-art for model partitioning algorithms. In this paper, we offer a retrospective of this event, including the basic problem formulation, key challenges & opportunities, our new benchmark suite, and the quality of submissions received.", "year": 2025, "publicationdate": "2025-08-06", "externalids": {"DOI": "10.1145/3676642.3736399"}, "doi_lower": "10.1145/3676642.3736399"}
{"paper_id": 246411325, "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model", "author_names": ["Shaden Smith", "M. Patwary", "Brandon Norick", "P. LeGresley", "Samyam Rajbhandari", "J. Casper", "Zhun Liu", "Shrimai Prabhumoye", "George Zerveas", "V. Korthikanti", "Elton Zhang", "R. Child", "Reza Yazdani Aminabadi", "J. Bernauer", "Xia Song", "M. Shoeybi", "Yuxiong He", "Michael Houston", "Saurabh Tiwary", "Bryan Catanzaro"], "venue": "arXiv.org", "abstract": "Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 236635565, "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM", "author_names": ["D. Narayanan", "M. Shoeybi", "J. Casper", "P. LeGresley", "M. Patwary", "V. Korthikanti", "Dmitri Vainbrand", "Prethvi Kashinkunti", "J. Bernauer", "Bryan Catanzaro", "Amar Phanishayee", "M. Zaharia"], "venue": "International Conference for High Performance Computing, Networking, Storage and Analysis", "abstract": "Large language models have led to state-of-the-art accuracies across several tasks. However, training these models efficiently is challenging because: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to scaling issues at thousands of GPUs. In this paper, we show how tensor, pipeline, and data parallelism can be composed to scale to thousands of GPUs. We propose a novel interleaved pipelining schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs (per-GPU throughput of 52% of theoretical peak).", "year": 2021, "publicationdate": "2021-04-09", "externalids": {"DOI": "10.1145/3458817.3476209"}, "doi_lower": "10.1145/3458817.3476209"}
{"paper_id": 202488191, "title": "PipeDream: generalized pipeline parallelism for DNN training", "author_names": ["D. Narayanan", "A. Harlap", "Amar Phanishayee", "Vivek Seshadri", "Nikhil R. Devanur", "G. Ganger", "Phillip B. Gibbons", "M. Zaharia"], "venue": "Symposium on Operating Systems Principles", "abstract": "DNN training is extremely time-consuming, necessitating efficient multi-accelerator parallelization. Current approaches to parallelizing training primarily use intra-batch parallelization, where a single iteration of training is split over the available workers, but suffer from diminishing returns at higher worker counts. We present PipeDream, a system that adds inter-batch pipelining to intra-batch parallelism to further improve parallel training throughput, helping to better overlap computation with communication and reduce the amount of communication when possible. Unlike traditional pipelining, DNN training is bi-directional, where a forward pass through the computation graph is followed by a backward pass that uses state and intermediate data computed during the forward pass. Naïve pipelining can thus result in mismatches in state versions used in the forward and backward passes, or excessive pipeline flushes and lower hardware efficiency. To address these challenges, PipeDream versions model parameters for numerically correct gradient computations, and schedules forward and backward passes of different minibatches concurrently on different workers with minimal pipeline stalls. PipeDream also automatically partitions DNN layers among workers to balance work and minimize communication. Extensive experimentation with a range of DNN tasks, models, and hardware configurations shows that PipeDream trains models to high accuracy up to 5.3X faster than commonly used intra-batch parallelism techniques.", "year": 2019, "publicationdate": "2019-10-27", "externalids": {"DOI": "10.1145/3341301.3359646"}, "doi_lower": "10.1145/3341301.3359646"}
{"paper_id": 267060979, "title": "Zero Bubble Pipeline Parallelism", "author_names": ["Penghui Qi", "Xinyi Wan", "Guangxing Huang", "Min Lin"], "venue": "arXiv.org", "abstract": "Pipeline parallelism is one of the key components for large-scale distributed training, yet its efficiency suffers from pipeline bubbles which were deemed inevitable. In this work, we introduce a scheduling strategy that, to our knowledge, is the first to successfully achieve zero pipeline bubbles under synchronous training semantics. The key idea behind this improvement is to split the backward computation into two parts, one that computes gradient for the input and another that computes for the parameters. Based on this idea, we handcraft novel pipeline schedules that significantly outperform the baseline methods. We further develop an algorithm that automatically finds an optimal schedule based on specific model configuration and memory limit. Additionally, to truly achieve zero bubble, we introduce a novel technique to bypass synchronizations during the optimizer step. Experimental evaluations show that our method outperforms the 1F1B schedule up to 23% in throughput under a similar memory limit. This number can be further pushed to 31% when the memory constraint is relaxed. We believe our results mark a major step forward in harnessing the true potential of pipeline parallelism. We open sourced our implementation based on the popular Megatron-LM repository on https://github.com/sail-sg/zero-bubble-pipeline-parallelism.", "year": 2023, "publicationdate": "2023-11-30", "externalids": {"DOI": "10.48550/arXiv.2401.10241"}, "doi_lower": "10.48550/arxiv.2401.10241"}
{"paper_id": 246017095, "title": "Sequence Parallelism: Long Sequence Training from System Perspective", "author_names": ["Shenggui Li", "Fuzhao Xue", "Yongbin Li", "Yang You"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Transformer achieves promising results on various tasks. However, self-attention suffers from quadratic memory requirements with respect to the sequence length. Existing work focuses on reducing time and space complexity from an algorithm perspective. In this work, we propose sequence parallelism, a memory-efficient parallelism to solve this issue from system perspective instead. Our approach is compatible with most existing parallelisms (e.g., data, pipeline, and tensor parallelism), which means our sequence parallelism makes 4D parallelism possible. More importantly, we no longer require a single device to hold the whole sequence. Besides, using efficient attention with linear complexity, our sequence parallelism enables us to train transformer with infinite long sequence. Specifically, we split the input sequence into multiple chunks and feed each chunk into its corresponding device (i.e., GPU). To compute the attention output, we integrated ring-style communication with self-attention calculation and proposed Ring Self-Attention (RSA). Experiments show that sequence parallelism performs well when scaling with batch size and sequence length. Compared with tensor parallelism, our approach achieved 13.7\\times and 3.0\\times maximum batch size and sequence length respectively when scaling up to 64 NVIDIA P100 GPUs. With efficient attention, sequence can handle sequence with over 114K tokens, which is over 27\\times longer than existing efficient attention works holding the whole sequence on a single device.", "year": 2021, "publicationdate": "2021-05-26", "externalids": {"DOI": "10.18653/v1/2023.acl-long.134"}, "doi_lower": "10.18653/v1/2023.acl-long.134"}
{"paper_id": 248693351, "title": "Reducing Activation Recomputation in Large Transformer Models", "author_names": ["V. Korthikanti", "J. Casper", "Sangkug Lym", "Lawrence C. McAfee", "M. Andersch", "M. Shoeybi", "Bryan Catanzaro"], "venue": "Conference on Machine Learning and Systems", "abstract": "Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron.", "year": 2022, "publicationdate": "2022-05-10", "externalids": {"DOI": "10.48550/arXiv.2205.05198"}, "doi_lower": "10.48550/arxiv.2205.05198"}
{"paper_id": 225039882, "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "author_names": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "M. Minderer", "G. Heigold", "S. Gelly", "Jakob Uszkoreit", "N. Houlsby"], "venue": "International Conference on Learning Representations", "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.", "year": 2020, "publicationdate": "2020-10-22", "externalids": {}, "doi_lower": null}
{"paper_id": 244814987, "title": "A survey of recommender systems with multi-objective optimization", "author_names": ["Yong Zheng", "D. Wang"], "venue": "Neurocomputing", "abstract": null, "year": 2021, "publicationdate": "2021-12-01", "externalids": {"DOI": "10.1016/j.neucom.2021.11.041"}, "doi_lower": "10.1016/j.neucom.2021.11.041"}
{"paper_id": 352650, "title": "Multimodal Deep Learning", "author_names": ["Jiquan Ngiam", "A. Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "A. Ng"], "venue": "International Conference on Machine Learning", "abstract": "Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning.", "year": 2011, "publicationdate": "2011-06-28", "externalids": {"DOI": "10.48550/arXiv.2301.04856"}, "doi_lower": "10.48550/arxiv.2301.04856"}
{"paper_id": 10137425, "title": "Multimodal Machine Learning: A Survey and Taxonomy", "author_names": ["T. Baltrušaitis", "Chaitanya Ahuja", "Louis-philippe Morency"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.", "year": 2017, "publicationdate": "2017-05-26", "externalids": {"DOI": "10.1109/TPAMI.2018.2798607"}, "doi_lower": "10.1109/tpami.2018.2798607"}
{"paper_id": 238639167, "title": "Multimodal research in vision and language: A review of current and emerging trends", "author_names": ["Shagun Uppal", "Sarthak Bhagat", "Devamanyu Hazarika", "Navonil Majumder", "Soujanya Poria", "Roger Zimmermann", "Amir Zadeh"], "venue": "Information Fusion", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.1016/j.inffus.2021.07.009"}, "doi_lower": "10.1016/j.inffus.2021.07.009"}
{"paper_id": 202734445, "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA", "author_names": ["Luowei Zhou", "Hamid Palangi", "Lei Zhang", "Houdong Hu", "Jason J. Corso", "Jianfeng Gao"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "This paper presents a unified Vision-Language Pre-training (VLP) model. The model is unified in that (1) it can be fine-tuned for either vision-language generation (e.g., image captioning) or understanding (e.g., visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The unified VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the first reported model that achieves state-of-the-art results on both vision-language generation and understanding tasks, as disparate as image captioning and visual question answering, across three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and the pre-trained models are available at https://github.com/LuoweiZhou/VLP.", "year": 2019, "publicationdate": "2019-09-24", "externalids": {"DOI": "10.1609/AAAI.V34I07.7005"}, "doi_lower": "10.1609/aaai.v34i07.7005"}
{"paper_id": 249151871, "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "author_names": ["Tri Dao", "Daniel Y. Fu", "Stefano Ermon", "A. Rudra", "Christopher R'e"], "venue": "Neural Information Processing Systems", "abstract": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).", "year": 2022, "publicationdate": "2022-05-27", "externalids": {}, "doi_lower": null}
{"paper_id": 259342096, "title": "Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models", "author_names": ["Sheng Shen", "Le Hou", "Yan-Quan Zhou", "Nan Du", "S. Longpre", "Jason Wei", "Hyung Won Chung", "Barret Zoph", "W. Fedus", "Xinyun Chen", "Tu Vu", "Yuexin Wu", "Wuyang Chen", "Albert Webson", "Yunxuan Li", "Vincent Zhao", "Hongkun Yu", "K. Keutzer", "T. Darrell", "Denny Zhou"], "venue": "International Conference on Learning Representations", "abstract": "Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be utilized to add learnable parameters to Large Language Models (LLMs) without increasing inference cost. Instruction tuning is a technique for training LLMs to follow instructions. We advocate combining these two approaches, as we find that MoE models benefit more from instruction tuning than dense models. In particular, we conduct empirical studies across three experimental setups: (i) Direct finetuning on individual downstream tasks devoid of instruction tuning; (ii) Instructiontuning followed by in-context few-shot or zero-shot generalization on downstream tasks; and (iii) Instruction tuning supplemented by further finetuning on individual downstream tasks. In the first scenario, MoE models overall underperform dense models of identical computational capacity. This narrative, however, dramatically changes with the introduction of instruction tuning (second and third scenario), used independently or in conjunction with task-specific finetuning. Our most powerful model, FLAN-MOE-32B, surpasses the performance of FLAN-PALM-62B on four benchmark tasks, while using only a third of the FLOPs. The advancements embodied byFLAN-MOE inspire a reevaluation of the design principles of large-scale, high-performance language models in the framework of task-agnostic learning.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {}, "doi_lower": null}
