{
  "survey": "This survey explores the Mixture of Experts (MoE) framework's role in optimizing large language models (LLMs), focusing on enhancing performance and efficiency through innovative architectures and training methodologies. MoE paradigms are pivotal in advancing LLMs by leveraging sparse architectures that activate expert networks selectively, optimizing computational resources and improving adaptability. The survey investigates transformations of dense models, multi-domain integrations, and expert specialization to address computational challenges. Highlighted are frameworks like Skywork-MoE and approaches such as M3oE and Omni-SMoLA, which enhance recommendation accuracy and generalist performance. The scope includes scalability, resource allocation, and efficiency strategies, emphasizing innovations like GLaM, MeteoRA, and Sparse Universal Transformers in handling memory constraints and parallelization. Challenges such as training instability, computational overhead, and generalization limitations are discussed, citing needs for robust benchmarks. Future directions suggest refining expert selection mechanisms, optimizing resource allocation, and addressing ethical considerations, contributing to the efficient deployment of AI systems. MoE models, with their scalable solutions and adaptability across domains, are integral to progressing artificial intelligence research and its practical applications.\n\nIntroduction Significance of Mixture of Experts in Large Language Models The Mixture of Experts (MoE) paradigm is essential for enhancing the performance and efficiency of large language models. By utilizing sparse architectures, MoE models selectively activate expert networks based on specific input requirements, optimizing computational resources and improving adaptability [1]. This selective activation significantly reduces computational demands compared to dense models, which require extensive parameters for similar performance levels [2]. MoE effectively mitigates the high computational, memory, and storage costs associated with few-shot in-context learning (ICL), providing scalable solutions for executing unseen tasks with pre-trained language models [3]. The introduction of MoE architectures has been crucial for scaling model size without a proportional increase in computational requirements, thereby enhancing accessibility and performance in language processing tasks [4]. MoE techniques are vital for optimizing long sequence transformer models, addressing inefficiencies in processing extended sequences through IO-aware attention algorithms, thus improving both performance and efficiency [5]. Furthermore, MoE frameworks advance large-scale vision-language models that integrate text and visual information, highlighting their importance in enhancing model performance in natural language processing tasks [6]. The conditional computation approach effectively addresses training and evaluation inefficiencies in deep learning models due to high computational costs, underscoring MoE's role in enhancing neural network performance [7]. MoE models excel at capturing complex, nonlinear relationships, a task challenging for traditional methods limited by rigid parametric constraints. This capability is crucial for improving the performance of large language models, facilitating more accurate and nuanced language understanding [8]. Additionally, MoE techniques enhance multi-domain and multi-task recommendations, refining user modeling and recommendation accuracy [9]. The Grouped-Query Attention (GQA) method illustrates how MoE can improve decoder inference speed while maintaining quality, further emphasizing its significance in boosting neural network performance [10]. In summary, the Mixture of Experts framework is integral to optimizing large language models, offering scalable and efficient solutions to the computational challenges posed by modern neural network architectures. Innovative resource allocation strategies employed by MoE architectures enhance the efficiency and performance of language models, making them indispensable in contemporary artificial intelligence research [11]. Objectives of the Survey This survey aims to investigate the role of Mixture of Experts (MoE) in enhancing the performance and efficiency of large language models through innovative architectures and training methodologies. It explores methods for constructing MoE models from existing dense models, such as transforming the LLaMA-2 7B model to address data and stability challenges [12]. The integration of MoE with multi-domain and multi-task frameworks, exemplified by approaches like M3oE, is examined for its role in improving recommendation systems through expert specialization [9]. The survey focuses on optimizing resource utilization and performance, as demonstrated by models like Skywork-MoE, which employ novel training techniques to achieve these goals [13]. Additionally, it evaluates scalable frameworks such as MeteoRA, which efficiently manage multiple task-specific LoRA adapters within a comprehensive MoE architecture [14]. The Omni-SMoLA architecture, utilizing a Soft Mixture of Experts to enhance generalist performance without significant increases in model size, is also discussed [15]. Furthermore, the survey explores MoE’s potential in enhancing the safety and usability of large language models in response to benign instructions, ensuring responsible deployment [16]. Frameworks like PaCE, which unify multiple experts for diverse dialogue-related tasks, highlight the versatility and adaptability of MoE approaches [17]. Through these objectives, the survey aims to provide a comprehensive understanding of the advancements, challenges, and future directions of Mixture of Experts (MoE) in large language models. It examines cost-effectiveness trade-offs, routing mechanisms, and scalability strategies, contributing to the development of more efficient and adaptable neural network architectures. Key findings include Context-Independent Specialization and Early Routing Learning, alongside proposed strategies for mitigating routing issues to enhance MoE-based language model designs. The potential of MoE techniques in scaling vision-language models and multitask multilingual models is also explored, offering insights into improving training stability, model interpretability, and computational performance. Ultimately, the survey encourages further research into MoE applications across multimodal machine learning [18,19,6,20]. Scope of the Paper This survey provides a comprehensive analysis of Mixture of Experts (MoE) techniques within large language models, emphasizing their ability to enhance model performance and efficiency across various domains. It includes a detailed examination of architectures such as GLaM, which illustrate the potential of MoE to increase model capacity while reducing training costs [21]. The impact of model scaling and few-shot learning performance across diverse tasks, as demonstrated by the PaLM model, is also explored [22]. A significant aspect of the survey involves the integration of hierarchical control and branch selection mechanisms, as proposed in the MoLE approach, to improve LoRA fusion performance [23]. The survey reviews over 50 papers published between early 2019 and mid-2024, focusing on parameter-efficient fine-tuning methods and efficiency in fine-tuning large models [24]. It critically analyzes the restrictive nature of constant top-k routing in existing MoE methods, which limits adaptive token processing [25]. The introduction of Sparse Mixture of Experts (SMoE) and dynamic halting mechanisms to improve the efficiency of Universal Transformers is included [26]. The challenges of training large transformer models due to memory limitations and inefficiencies in parallelization techniques are also addressed [27]. The integration of multiple LoRA adapters into a single LLM and the exploration of MoE strategies for enhancing efficiency and performance are key topics within the scope of this survey [14]. The survey further investigates innovative solutions for training and deploying complex models, particularly through sparse mixture-of-experts techniques [6]. The high computational and memory demands of large language models and the challenges faced by conventional MoE architectures are critical issues explored in this survey [4]. Through these focused areas, the survey aims to provide a comprehensive understanding of the current landscape and future directions of Mixture of Experts in large language models. Structure of the Survey The survey is systematically organized into key sections to facilitate a thorough exploration of Mixture of Experts (MoE) in large language models. The introductory section establishes the significance of MoE techniques in enhancing model performance and efficiency, outlining the survey’s objectives and scope. Following this, the Background and Definitions section provides foundational knowledge on large language models, neural networks, and expert models, offering clear definitions and tracing the historical development of these concepts. The Mixture of Experts Techniques section delves into various methodologies employed in large language models, including routing strategies and expert selection mechanisms, while comparing sparse and dense approaches [26]. This is succeeded by the Applications in Language Model Optimization section, which examines the practical applications of MoE techniques in optimizing language models, highlighting efficiency strategies and case studies in multilingual and multimodal tasks [6]. The survey then addresses Challenges and Limitations, identifying key issues such as training instability, computational overhead, and benchmarking limitations [27]. The Future Directions section suggests potential research avenues, exploring emerging techniques, architectural innovations, and ethical considerations [15]. Finally, the Conclusion synthesizes the survey's findings, reflecting on MoE's importance in advancing large language models and the anticipated impact of future research. This structured approach ensures a coherent narrative aligned with the survey's objectives, providing a comprehensive understanding of the advancements and challenges in Mixture of Experts (MoE) within large language models. It delves into various innovative techniques such as sparsely-gated MoE for scaling vision-language models, computational efficiency improvements through methods like Merging Experts into One (MEO), and the integration of HyperMoE for enhanced knowledge transfer among experts. Additionally, it addresses issues like representation collapse and explores modular multi-task learning frameworks, offering insights into stabilizing training, improving model interpretability, and balancing computational trade-offs in scaling large-scale models [28,29,30,6,31].The following sections are organized as shown in . Background and Definitions Overview of Large Language Models and Neural Networks Large language models (LLMs) are pivotal in natural language processing, leveraging advanced neural network architectures, particularly transformers, to efficiently manage input sequences through self-attention mechanisms, addressing memory and sequence length challenges [5]. Traditional views often treat LLMs as monolithic, potentially neglecting their structural capabilities [10]. Scaling neural networks is essential for improving model quality, especially in data-intensive applications [32], yet training large models demands substantial computational resources and complex refactoring, limiting accessibility to those with significant infrastructure [27]. Techniques like ILMP enable efficient training of large transformers by distributing layers across multiple GPUs [33]. Activation functions are crucial for model performance, with traditional functions like ReLU and ELU prompting exploration of alternatives better suited for LLM computations [10]. While transformer architectures have potential beyond NLP, fields like computer vision remain dominated by convolutional neural networks (CNNs) [5]. LLMs demonstrate adaptability across various linguistic contexts, trained on diverse datasets, including multiple languages and coding tasks, extending to specialized applications like dialogue systems [32]. In vision-language tasks, models often struggle to produce coherent outputs from visual inputs, indicating a need for improved integration of visual and textual data [33]. LLMs represent the convergence of advanced neural architectures and computational techniques, significantly advancing NLP and impacting areas like code generation and machine translation. They employ transfer learning to excel in various tasks, showcasing remarkable few-shot learning capabilities, as seen in models like GPT-3, revealing emergent abilities with increased model scale. This convergence has led to breakthroughs in software development, language understanding, and translation, while raising ethical and environmental concerns [34,35,36,37,38]. Their capacity to process complex sequences and adapt to diverse tasks positions them as essential tools in the evolution of artificial intelligence. Defining Mixture of Experts and Expert Models The Mixture of Experts (MoE) framework enhances computational efficiency and performance through dynamic computation, partitioning model parameters into subsets known as experts, selectively activated based on input requirements [1]. Gating mechanisms are pivotal, routing input data to suitable experts to optimize resource allocation and minimize computational overhead [7]. This approach is particularly beneficial for large language models, increasing model capacity without proportional computational demands [4]. Expert models within the MoE framework specialize in complex tasks via modular multi-task learning, exemplified by the Deep Mixture of Experts (DMOE) architecture, integrating multiple expert layers with gating mechanisms for efficient processing [1]. Dirichlet process mixtures support this by non-parametrically modeling joint distributions, managing nonlinear relationships [8]. MoE models face challenges like routing inefficiencies and underutilization of inactive experts during inference, hindering performance [4]. Token-adaptive routing methods, like AdaMoE, address these by introducing null experts and adjusting active expert numbers based on token requirements, enhancing efficiency [7]. The Lory architecture offers a fully-differentiable MoE framework for autoregressive language model pre-training, incorporating techniques for efficient expert merging and data batching [11]. The MoE framework signifies a transformative approach in neural network design, offering scalable solutions to enhance performance and resource allocation. Through innovative architectures and training methodologies, MoE models revolutionize large language models by providing sparsely activated structures that increase model size and accuracy without proportional computational costs. These models utilize multi-dimensional parallelism and heterogeneous memory technologies to scale efficiently to trillions of parameters, enabling deployment of larger models on the same hardware. Advanced training methods, such as expert pruning and improved sample efficiency, further enhance inference time and performance. MoE models achieve state-of-the-art results in tasks like machine translation and multilingual natural language generation, with models trained on billions of parameters across multiple languages. The open-source DeepSpeed library facilitates efficient MoE training, explored through the OpenMoE initiative, demonstrating favorable cost-effectiveness compared to dense models and identifying insights into routing mechanisms to inform future MoE designs [20,19]. Language Model Optimization Techniques Optimizing language models is crucial for enhancing efficiency and performance, particularly given computational and memory constraints inherent in their deployment. Techniques like Zero Bubble Pipeline Parallelism eliminate pipeline bubbles during synchronous training, optimizing throughput and reducing latency [39], significantly improving LLM training efficiency. Effective management of Key-Value (KV) cache memory is critical, as inefficient handling can lead to substantial memory waste and hinder batch processing [40]. Addressing these issues is vital for improving scalability and responsiveness, especially in real-time applications. The MoE framework presents unique optimization opportunities through innovative training methodologies. StableMoE employs a two-stage training approach to establish stable routing strategies, mitigating fluctuations in expert selection [41], essential for consistent model performance. Sparse architectures, such as those in the PanGu-Σ model, exemplify optimization techniques using Random Routed Experts to distribute tasks in a trillion-parameter framework [42], enhancing training efficiency and generalization across linguistic contexts. Integrating low-rank matrices, as proposed in the LoRA technique, facilitates efficient task-specific fine-tuning of pre-trained models [43], enabling modifications without extensive retraining. Benchmarking efforts, like OpenMoE, provide insights into MoE-based LLM performance, focusing on routing mechanisms' impact [20], crucial for evaluating optimization techniques and guiding future innovations. Language model optimization encompasses strategies aimed at improving computational efficiency, memory management, and model adaptability, enabling LLMs to address real-world language processing challenges more effectively [44]. Historical Development and Evolution The historical development of large language models (LLMs) and the Mixture of Experts (MoE) framework reflects significant advancements in computational techniques and model architectures. Transformer architectures, foundational to LLMs, initially struggled with activation sparsity, crucial for optimizing computational efficiency [45]. Early models focused on monolithic structures, overlooking modularity potential [22], driving evolution toward more adaptable architectures. Emergent properties highlight models' ability to exhibit behaviors not explicitly programmed, evident in MoE frameworks where dynamic expert selection enhances adaptability [15]. MoE's development faced challenges like costs associated with replicating and storing expert models, necessitating innovative management approaches [15]. Sparse expert models, integral to MoE, evolved from dense, resource-intensive models to architectures optimizing efficiency through selective activation [30]. MoE layers' ability to learn complex tasks exemplifies advancements, despite representation collapse issues reducing diversity [30]. Language model evaluation benchmarks illustrate progress, addressing complexities and resource demands of training models with hundreds of billions of parameters [22], leading to comprehensive frameworks capturing capabilities and limitations [46]. In multimodal tasks, benchmarks may inadequately assess real-world AI performance, particularly regarding factual accuracy and behavioral expectations [47]. The evolution of LLMs and MoE represents a continuous journey of innovation, enhancing performance and sample efficiency in neural architectures. Emergent abilities suggest further scaling could expand capabilities in AI. Sparse expert models, like MoE and Switch Transformers, lead to efficient architectures decoupling parameter count from computational requirements, advancing domains like NLP, computer vision, and speech recognition. These developments collectively set the stage for future breakthroughs in AI [48,36]. The exploration of Mixture of Experts (MoE) techniques has gained significant attention in recent years, particularly due to their potential for enhancing model performance through specialized routing strategies. As depicted in , this figure illustrates the hierarchical categorization of MoE techniques, focusing on routing strategies, the distinction between sparse and dense models, and innovative architectures. It highlights the key methods, benefits, and challenges associated with each category, showcasing the advancements in neural network design and performance optimization. By examining these categories, we can better understand how MoE frameworks can be effectively implemented to leverage their strengths while addressing inherent limitations. Mixture of Experts Techniques Routing Strategies and Expert Selection Routing strategies and expert selection are pivotal in optimizing Mixture of Experts (MoE) models. These mechanisms allocate input data across expert networks, enhancing resource utilization. The DMOE method exemplifies this by structuring gating and expert networks, training the gating network to map inputs to expert distributions, and combining outputs based on predictions [1]. Similarly, the DS-MoE framework employs dense training with sparse inference, boosting parameter efficiency and reducing computational costs [2]. Innovative routing strategies include the Dirichlet Process Mixture Model for Nonlinear Classification, which uses linear model mixtures to capture nonlinear relationships, offering robust routing for MoE architectures [8]. The GQA method introduces intermediate key-value heads, balancing speed and quality in attention mechanisms for enhanced routing efficiency [10]. The M3oE approach uses three mixture-of-experts modules to learn user preferences across common, domain-aspect, and task-aspect dimensions, showcasing sophisticated expert selection [9]. Conditional computation selectively activates network parts based on learned policies, improving efficiency [7]. The Lory architecture employs causal segment routing for expert merging, presenting a distinct approach [11]. These strategies are essential for enhancing MoE model performance and scalability. They address routing fluctuations and load imbalances that can hinder training efficiency. By refining token-to-expert assignments and stabilizing routing processes, these strategies enable effective management of complex tasks, improve convergence speed, and maintain cost-effectiveness compared to dense models. They facilitate sophisticated MoE architectures leveraging intrinsic cluster structures and nonlinear networks, leading to superior performance across applications [18,41,49,20]. By activating the most suitable experts for specific tasks, these strategies enhance overall performance and scalability of large language models, enabling efficient handling of complex linguistic tasks. Sparse vs. Dense Mixture of Experts Sparse and dense Mixture of Experts (MoE) architectures offer distinct strategies for optimizing neural network performance and resource efficiency. Sparse MoE models activate a subset of experts during inference, optimizing computational resources while maintaining high performance. The GLaM model exemplifies this by increasing capacity without a linear resource consumption increase, demonstrating scalability and efficiency of sparse architectures [21]. The Sparse Universal Transformer (SUT) enhances computational efficiency through selective activation [26]. Techniques like SCMoE introduce a training-free strategy utilizing unchosen experts, improving reasoning capabilities while maintaining computational efficiency [50]. The OLMoE framework emphasizes specialization and efficiency, contrasting previous benchmarks through extensive training data and novel routing strategies [51]. Pre-gated MoE addresses dynamic activation challenges of sparse experts, enhancing adaptability [4]. Dense models utilize all parameters for every input, potentially reducing overfitting risk and improving inference speed. The Lory architecture allows efficient expert merging while preserving autoregressive properties, enhancing pre-training performance [11]. Dense models often require proportional computational cost increases for scalability, highlighting sparse approaches' advantages in resource-limited scenarios [14]. While dense models use all parameters for every input, sparse MoE architectures offer resource-efficient solutions by dynamically activating specific experts, enhancing performance and optimizing computational resource allocation. This adaptability makes sparse models suitable for large-scale language processing tasks. Routing networks making adaptive decisions on function block selection exemplify innovative strategies in sparse MoE models for superior performance [6]. Innovative Techniques and Architectures Innovative techniques and architectures within the Mixture of Experts (MoE) framework have advanced scalability and efficiency of large language models, transforming neural network design. The Mamba architecture offers a hardware-aware parallel algorithm in recurrent mode, enabling selective information propagation and enhancing computational performance [52]. DS-MoE employs dense computation across all experts during training and sparse computation during inference, balancing efficiency and performance [2]. FlashAttention optimizes models by reducing High Bandwidth Memory (HBM) accesses compared to standard attention mechanisms, enhancing performance across SRAM sizes [5]. These architectures demonstrate MoE framework's dynamic evolution, offering scalable solutions to challenges posed by large language models. MoE methodologies enhance neural networks' capabilities in tackling complex linguistic and multimodal tasks. MoE models, characterized by sparse activation and sublinear compute costs relative to parameters, enable substantial growth in model size and accuracy without proportional computational resource increases. Recent innovations focus on scaling MoE models to trillions of parameters, improving training efficiency, and reducing inference time through expert pruning strategies. MoE architectures have been applied to develop unified Multimodal Large Language Models (MLLMs) that handle diverse modalities efficiently by employing modality-specific encoders and sparse architectures. These advancements enhance multi-expert collaboration, generalization, and reduce performance bias across multimodal datasets. Exploring emergent modularity in language models reveals MoE architectures can be fine-tuned to enhance generalization without adding extra parameters. These developments underscore MoE's potential to revolutionize neural network performance in complex, multi-task environments [53,29,19,18,54]. Applications in Language Model Optimization Efficiency and Resource Allocation Strategies Efficiency and resource allocation are fundamental in optimizing large language models (LLMs), focusing on reducing computational overhead while maximizing resource utilization. The Mixture of Experts (MoE) framework dynamically allocates expert resources based on token relevance, enhancing training efficiency and performance across diverse tasks. Techniques like Dynamic Capacity Networks exemplify this approach by significantly reducing computational costs while maintaining or improving performance compared to traditional methods [55]. Innovative models such as LIMoE utilize the MoE framework to enhance multimodal learning, allowing expert layers to specialize in different modalities, thus optimizing resource allocation [56]. The BASE layer simplifies the training process, facilitating better resource allocation for large language models [57]. The MoLE approach combines LoRAs to boost performance across various tasks, highlighting the effectiveness of adaptive strategies in resource optimization [23]. AdaMoE implements adaptive token routing, dynamically allocating resources to the appropriate number of experts, optimizing resource usage, and improving model accuracy [25]. The DS-MoE method employs dense computation during training and sparse computation during inference, enhancing parameter efficiency while reducing computational costs [2]. The Pre-gated MoE approach improves performance and reduces memory requirements, enabling cost-effective deployment of large-scale LLMs on a single GPU [4]. DeepSpeed techniques enhance the training of long sequence models by maintaining constant communication volume as sequence length increases, thus improving resource allocation [33]. The GQA method enhances inference efficiency without significant loss in output quality, demonstrating effective resource allocation in language models [10]. Additionally, M3oE improves efficiency in user recommendations by adaptively learning preferences across multiple domains [9]. The DMOE framework illustrates the advantages of scaling with the input space's complexity while efficiently utilizing computational resources, enabling faster training and inference [1]. Collectively, these strategies empower LLMs to achieve superior performance and scalability, effectively addressing real-world language processing challenges. Case Studies in Multilingual and Multimodal Tasks The application of MoE techniques in multilingual and multimodal tasks underscores their versatility in addressing complex language processing challenges. Multilingual models, such as the PaLM architecture, demonstrate the ability of MoE frameworks to scale efficiently across diverse linguistic contexts, enhancing few-shot learning performance in various language tasks [22]. Hierarchical control mechanisms further optimize LoRA fusion performance, improving model adaptability across domains [23]. In multimodal tasks, the LIMoE architecture leverages expert layers specializing in different modalities, optimizing resource allocation and enhancing performance when processing visual and textual data [56]. This approach addresses challenges faced by existing vision-language models, which often struggle with coherent output from visual inputs [33]. The integration of MoE techniques facilitates a nuanced understanding and generation of multimodal content, thereby improving output quality. Case studies in multilingual applications highlight the effectiveness of token-adaptive routing methods, such as AdaMoE, which dynamically allocate resources based on token requirements, optimizing model accuracy and efficiency [25]. This adaptability is crucial for managing diverse linguistic inputs in multilingual tasks, ensuring effective processing and generation across contexts. The MeteoRA framework exemplifies efficient management of multiple task-specific LoRA adapters, showcasing MoE architectures' potential to enhance performance in multilingual and multimodal scenarios [14]. Innovative routing strategies and expert selection mechanisms enable these models to address complexities in processing diverse language and modality inputs, achieving state-of-the-art performance in real-world applications. The application of MoE techniques in multilingual and multimodal tasks emphasizes their capacity to enhance language model performance and efficiency across varied domains. Leveraging innovative architectures and adaptive resource allocation strategies, MoE models significantly improve large language models' capabilities, efficiently scaling to handle complex linguistic and multimodal challenges. These sparsely activated models offer sublinear compute costs relative to their parameters, enabling substantial growth in model size with improved accuracy while maintaining a lower computational budget. MoE models address system and modeling challenges through multi-dimensional parallelism and heterogeneous memory technologies, allowing for the development of models up to eight times larger on the same hardware. Furthermore, MoE architectures facilitate efficient training and inference across diverse modalities, enhancing multilingual task performance and reducing bias in mixed multimodal datasets. Open-source initiatives like OpenMoE and Uni-MoE demonstrate MoE models' cost-effectiveness and scalability, providing insights into routing mechanisms and expert collaboration for refining deployment in large-scale vision-language models and other multimodal applications [53,20,6,19]. State-of-the-Art Performance Achievements The MoE framework has been crucial in achieving state-of-the-art performance across various natural language processing and vision-language tasks, underscoring its efficacy in optimizing model efficiency and accuracy. Notably, the OLMoE model has outperformed existing models, setting new benchmarks in language modeling research [51]. This achievement exemplifies the transformative impact of MoE architectures on enhancing language model capabilities. In few-shot learning, the (IA)$^3$ method has achieved superior performance while significantly reducing computational costs compared to traditional in-context learning (ICL) approaches [3]. This method highlights the efficiency gains possible through MoE techniques, enabling models to perform complex tasks with reduced resource consumption. The Lory architecture further showcases state-of-the-art performance, achieving up to a 13.9\\ These achievements underscore the transformative potential of MoE techniques in establishing new standards for performance and efficiency in artificial intelligence. By leveraging innovative architectures and adaptive resource allocation strategies, MoE models revolutionize language and vision processing capabilities, offering scalable solutions with sublinear compute costs relative to their parameters. These sparsely activated models enable significant increases in size and accuracy while minimizing computational demands. Recent advancements have overcome system and modeling challenges, allowing MoE models to scale efficiently to trillions of parameters through multi-dimensional parallelism and heterogeneous memory technologies. This progress has led to state-of-the-art performance in machine translation and multilingual natural language generation tasks, as well as enhanced vision-language models that integrate text and visual information. By effectively managing compute resources and improving inference time, MoE models drive significant advancements in multimedia data understanding and inspire further research in multimodal machine learning applications [6,19]. Challenges and Limitations Training Instability and Learning Challenges Deploying Mixture of Experts (MoE) models involves navigating training instability and learning challenges due to complex expert network management across tasks. Optimizing gating mechanisms is crucial, as suboptimal tuning can degrade model performance, highlighted by DMOE limitations [1]. Latency in transferring activated experts from CPU to GPU memory presents significant performance overheads, impacting scalability, especially in real-time applications [4]. Sparse MoE approaches face additional training instability due to gating mechanism complexities, affecting computational optimization [6]. The MeteoRA framework illustrates challenges in efficiently switching between LoRA adapters during inference, restricting adaptability to varied inputs [14]. Methods like DeepSpeed-Ulysses require specific hardware configurations, imposing constraints in resource-limited environments [33]. Optimizing training to avoid overfitting and maximize computational resource utilization remains difficult [32], demanding precise hyperparameter tuning to balance efficiency and accuracy. Addressing these obstacles involves refining gating mechanisms, optimizing resource use, and enhancing MoE adaptability. Advances in parameter-efficient fine-tuning, sparse scaling architectures, and transfer learning can improve stability and efficiency, supporting scalable neural networks handling multibillion-scale parameters, enhancing multilingual translation, and achieving state-of-the-art performance across NLP tasks. Insights into scaling laws and emergent abilities of large models further guide optimal resource use and unlock new capabilities [35,24,36,32,58]. Computational Overhead and Resource Limitations Mixture of Experts (MoE) models face substantial computational overhead and resource constraints in large language frameworks, impeding scalability. HetuMoE’s dependence on high-bandwidth infrastructure restricts its applicability in resource-constrained environments [59]. Managing complex expert structures introduces significant overhead, complicating integration [60]. The resource-intense nature of training large models limits accessibility [43], and sparse implementations suffer from high memory usage and inefficient data padding [60]. Activation functions like GELU constrain performance in resource-limited settings [61], while suboptimal data partitioning adds performance burdens due to expert management overhead [62]. Although the Dynamic Capacity Network method reduces costs, it introduces complexity via gradient-based attention, heightening computational burdens [55]. Current benchmarks fail to fully evaluate large sparse models’ challenges [63], and specific compute budget reliance hampers generalization [64]. The BASE method’s use of linear assignments adds overhead, affecting scalability [57]. Methods like Omni-SMoLA risk overhead and expert performance issues in specialized tasks [15], and Sparse Universal Transformer inference performance faces degradations [26]. PaCE method efficiency can be dataset-dependent [17], while DS-MoE’s parameter tuning during inference requires precision [2]. M3oE scalability faces concerns with large datasets or diverse domains [9], and nonlinear classification aligns with these complexity challenges [8]. GQA method reliance on multi-head checkpoints may limit application to specific architectures [10], and conditional computation faces reinforcement learning implementation complexities [7]. Recent advances, like the MEO technique, have optimized computational efficiency by reducing expert activation, significantly lowering FLOPS from 72.0G to 28.6G [31]. Innovations such as HyperMoE utilize Hypernetworks to balance expert knowledge and sparsity, promoting efficient and scalable MoE architectures [29]. Overcoming these challenges enhances large language models’ capabilities, ensuring efficient deployment in complex linguistic and multimodal tasks. Benchmarking and Generalization Limitations Benchmarking and generalization limitations significantly hinder the evaluation and application of Mixture of Experts (MoE) models in language frameworks. Reliance on specific datasets may not reflect real-world complexity, compromising model generalizability [47]. Task variability complicates MoE model generalization, evidenced by challenges in cross-architecture findings [65]. Comprehensive evaluation frameworks are necessary for accurate assessment across linguistic and multimodal tasks, considering MoE models' emergent properties which may be inconsistently replicable [65]. Table illustrates the variety and scope of benchmarks utilized to evaluate the performance and generalization limitations of Mixture of Experts models within different domains and task formats. Benchmarks often overlook MoE models' unique challenges, affecting performance evaluation. Absence of standardized metrics considering expert selection and routing complexities limits meaningful model comparisons. MoE models' ability to surpass dense feedforward networks by efficiently managing computational costs and memory is restricted by the lack of comprehensive evaluation frameworks, hindering optimization and full potential exploration [66,18]. Addressing these limitations requires developing robust methodologies to evaluate MoE models’ diverse capabilities accurately. By refining benchmarking approaches to enhance comprehensiveness and applicability, effective deployment in real-world processing challenges is ensured. This enables nuanced analysis of academic and professional understanding across varied tasks, identifying enhancement areas. Innovations like HyperMoE and OpenMoE offer insights into optimizing expert selection and routing while scalable training techniques support larger, efficient models. Advances present promising paths to achieve expert-level accuracy in language models, balance computational trade-offs, and enhance interpretability, paving the way for state-of-the-art performance in multilingual and multimodal applications [67,29,19,6,20]. Future Directions Emerging Techniques and Their Impact Emerging techniques within the Mixture of Experts (MoE) framework promise to enhance the scalability and adaptability of large language models across diverse domains. Key advancements include optimizing gating mechanisms for multimodal tasks, which boost model versatility and performance in complex scenarios [6]. The DS-MoE framework's refined activation mechanisms suggest broad applicability across various model types [2]. The M3oE framework exemplifies how optimization techniques can improve adaptability, particularly in complex recommendation scenarios, thereby increasing utility across domains [9]. Insights into scaling laws guide future model training and design, ensuring efficient neural network scaling [32]. Optimizing computational aspects of the Dirichlet Process Mixture Model and FlashAttention for different architectures presents significant directions for enhancing efficiency and applicability [8,5]. Future research should focus on improving gating network designs and integrating DMOE with other architectures, underscoring advancements in expert selection mechanisms [1]. Optimizing GQA for efficiency and adaptability suggests emerging techniques with profound impacts on the MoE field [10]. The Pre-gated MoE system offers opportunities for further optimization of pre-gating functions and adaptation to other architectures, with broader real-world applications being a key exploration area [4]. The reinforcement learning framework offers optimization avenues with applicability to various neural network types, promising significant advancements in MoE methodologies [7]. Refining techniques within the Lory architecture and exploring fully-differentiable MoE models could yield substantial advancements in MoE capabilities [11]. Continuous innovation positions MoE architectures to significantly influence AI research and applications. Architectural Innovations and Scalability Architectural innovations in the Mixture of Experts (MoE) framework are crucial for addressing scalability challenges and enhancing large language models' efficiency. Scalable architectures like Omni-SMoLA demonstrate MoE systems' ability to achieve superior performance without significant model size increases, optimizing resource allocation and adaptability [15]. Sparse architectures, exemplified by the GLaM model, increase model capacity while minimizing computational costs, providing scalable solutions for extensive language processing tasks [21]. The MeteoRA framework supports scalability by managing multiple task-specific LoRA adapters within a comprehensive MoE architecture, emphasizing innovative design's role in enhancing model performance across applications [14]. Dynamic routing strategies, such as those in AdaMoE, optimize scalability by adjusting the number of active experts based on token requirements, enhancing resource efficiency and accuracy [25]. The DS-MoE method balances computational efficiency and scalability by utilizing dense computation during training and sparse computation during inference, serving as a viable alternative to dense and traditional sparse models [2]. Pre-gated MoE systems exemplify scalability strategies by optimizing performance and reducing memory requirements, enabling cost-effective large-scale LLM deployment on a single GPU [4]. FlashAttention techniques optimize performance by reducing High Bandwidth Memory accesses, enhancing scalability across SRAM sizes [5]. Architectural innovations in the MoE framework enhance large language models' efficiency and adaptability. Strategic design and resource optimization address catastrophic forgetting, enable sequential learning across tasks, and facilitate multimodal feature learning, ensuring effective deployment in complex linguistic and multimodal tasks, including language translation and audio-visual speech classification. Emergent modularity in pre-trained transformers advances functional specialization and neuron grouping, enhancing neural network adaptability and intelligence [68,69,65]. Transfer Learning and Cross-Domain Applications Integrating Mixture of Experts (MoE) techniques in transfer learning and cross-domain applications enhances large language models' adaptability and performance across varied tasks and domains. Future research should optimize expert management within MoE frameworks to validate their effectiveness across broader applications, improving applicability in transfer learning contexts [70]. This optimization ensures MoE models can generalize their capabilities to new domains, facilitating seamless transitions between linguistic and modality tasks. The scalability of MoE architectures, such as AdaMoE, across different model architectures and tasks underscores their potential for transfer learning and cross-domain applications [25]. Exploring scalability can unlock MoE frameworks' adaptability to diverse domains, enhancing utility in real-world applications. Examining emergent modularity in other model architectures could optimize modular components' activation, supporting MoE models' transferability across contexts [65]. Future work may optimize the BASE layer for different neural network architectures, relevant to transfer learning and cross-domain applications [57]. This optimization could facilitate efficient MoE model adaptation to new tasks, leveraging existing knowledge to improve performance in unfamiliar domains. Refining the MoE-finetuning process and extending frameworks like (IA)$^3$ to more diverse domains could enhance MoE models' adaptability, supporting deployment across scenarios [3]. Expanding benchmarks to include diverse tasks and datasets, while investigating proposed architectures' scalability, is promising for future research [71]. Incorporating varied applications and refining evaluation aspects, benchmarks can comprehensively assess MoE models' capabilities in transfer learning scenarios. Understanding sparse models' strengths and limitations through robust benchmarking is crucial for advancing their application in transfer learning and cross-domain contexts [21]. MoE's role in transfer learning and cross-domain applications is pivotal for enhancing large language models' versatility and adaptability. Improving expert selection processes, scaling architectures, and refining benchmarks can significantly advance neural network capabilities. These enhancements facilitate training models with trillions of parameters, yielding substantial accuracy gains while maintaining low computational costs. Multi-dimensional parallelism and heterogeneous memory technologies allow model creation up to eight times larger on the same hardware. Novel training methods, such as expert pruning and parameter-efficient expert retrieval, improve sample and inference time efficiency, enabling effective neural network deployment in diverse tasks and achieving state-of-the-art performance in machine translation and multilingual natural language generation across languages [66,19]. Ethical Considerations and Real-World Applications Deploying Mixture of Experts (MoE) models in real-world applications requires examining ethical considerations, especially in large language models (LLMs). Emerging trends highlight the need to assess MoE techniques' ethical implications, including privacy, bias, and transparency in AI systems [37]. MoE frameworks' integration in language models raises concerns about potential biases in expert selection mechanisms, which may inadvertently reinforce existing prejudices if not carefully managed. The environmental impact of training large-scale MoE models is a critical ethical consideration, given the substantial computational resources required for their development and deployment. Strategies to mitigate these impacts, such as optimizing resource allocation and enhancing model efficiency, are essential for ensuring MoE architectures' sustainable advancement [37]. Continuous LLM improvement through innovative MoE techniques underscores the necessity for responsible AI practices, ensuring these models align with ethical standards and societal values. In practical applications, MoE models offer transformative potential across domains, including software development, where they enhance code generation and optimization processes [37]. MoE architectures' adaptability to diverse tasks highlights their utility in real-world scenarios, facilitating advancements in fields such as healthcare, finance, and education. Leveraging MoE models' dynamic and scalable nature, industries can achieve more efficient and accurate solutions to complex problems, driving progress in AI applications. Addressing ethical considerations and exploring Mixture of Experts models' real-world applications is crucial for ensuring responsible and impactful deployment. Prioritizing ethical practices while harnessing MoE architectures' advanced capabilities, such as HyperMoE and ExFlow, can significantly enhance AI systems' performance and efficiency. HyperMoE addresses balancing expert knowledge and sparsity by integrating knowledge transfer among experts, improving model capacity without compromising selection sparsity. Meanwhile, ExFlow optimizes inference in large language models by exploiting inter-layer expert affinity, reducing communication overhead and increasing throughput. These innovations enable AI systems' development that is innovative and better aligned with societal needs [29,72]. Conclusion The exploration of Mixture of Experts (MoE) within large language models underscores their pivotal role in enhancing both efficiency and performance across diverse applications. Techniques like Zero Bubble Pipeline Parallelism demonstrate significant strides in computational efficiency, achieving notable improvements over traditional methods. Concurrently, frameworks such as StableMoE address critical challenges like routing fluctuations, highlighting the architectural advancements in expert selection processes. Sparse expert models have shown marked improvements in tasks spanning natural language processing and computer vision, paving the way for innovative research directions. The integration of delta tuning emerges as a promising approach, offering a nuanced understanding of pre-trained models and deep neural networks. Current benchmarks, while revealing the capabilities of existing language models, also identify gaps that necessitate further enhancement to reach expert-level performance. The effectiveness of systems like HetuMoE, which achieve substantial speedups, underscores the value of MoE in optimizing large-scale models. Moreover, the development of open-source models such as Llama 2 provides a significant impetus for innovation, offering alternatives to proprietary systems. The BTX model exemplifies an optimal balance between accuracy and efficiency, reinforcing the critical contributions of MoE techniques to the progression of artificial intelligence.",
  "reference": {
    "1": "1312.4314v3",
    "2": "2404.05567v1",
    "3": "2205.05638v2",
    "4": "2308.12066v3",
    "5": "2205.14135v2",
    "6": "2303.07226v1",
    "7": "1511.06297v2",
    "8": "math/0703292v1",
    "9": "2404.18465v3",
    "10": "2305.13245v3",
    "11": "2405.03133v2",
    "12": "2406.16554v1",
    "13": "2406.06563v1",
    "14": "2405.13053v3",
    "15": "2312.00968v2",
    "16": "2405.14488v1",
    "17": "2305.14839v2",
    "18": "2208.02813v1",
    "19": "2109.10465v1",
    "20": "2402.01739v2",
    "21": "2112.06905v2",
    "22": "2204.02311v5",
    "23": "2404.13628v1",
    "24": "2303.15647v2",
    "25": "2406.13233v2",
    "26": "2310.07096v1",
    "27": "1909.08053v4",
    "28": "2212.08066v1",
    "29": "2402.12656v4",
    "30": "2204.09179v3",
    "31": "2310.09832v3",
    "32": "2001.08361v1",
    "33": "2309.14509v2",
    "34": "2005.14165v4",
    "35": "1910.10683v4",
    "36": "2206.07682v2",
    "37": "2406.00515v2",
    "38": "2207.04672v3",
    "39": "2401.10241v1",
    "40": "2309.06180v1",
    "41": "2204.08396v1",
    "42": "2303.10845v1",
    "43": "2106.09685v2",
    "44": "2310.19341v1",
    "45": "2210.06313v2",
    "46": "2110.14168v2",
    "47": "2303.08774v6",
    "48": "2209.01667v1",
    "49": "2202.09368v2",
    "50": "2405.14507v2",
    "51": "2409.02060v2",
    "52": "2312.00752v2",
    "53": "2405.11273v1",
    "54": "2310.10908v2",
    "55": "1511.07838v7",
    "56": "2206.02770v1",
    "57": "2103.16716v1",
    "58": "2110.08246v1",
    "59": "2203.14685v3",
    "60": "2312.12379v5",
    "61": "1606.08415v5",
    "62": "1704.06363v1",
    "63": "2202.08906v2",
    "64": "2203.15556v1",
    "65": "2305.18390v2",
    "66": "2407.04153v1",
    "67": "2009.03300v3",
    "68": "2301.04856v1",
    "69": "1612.00796v2",
    "70": "2108.05036v2",
    "71": "2401.04088v1",
    "72": "2401.08383v2"
  },
  "chooseref": {
    "1": "2303.06318v2",
    "2": "2406.00515v2",
    "3": "2209.01667v1",
    "4": "2205.12410v2",
    "5": "2406.13233v2",
    "6": "2010.11929v2",
    "7": "1706.03762v7",
    "8": "2103.16716v1",
    "9": "2110.03742v1",
    "10": "2010.11125v1",
    "11": "2306.00008v2",
    "12": "2208.03306v1",
    "13": "1511.06297v2",
    "14": "2108.05036v2",
    "15": "2405.04434v5",
    "16": "2401.06066v1",
    "17": "2309.14509v2",
    "18": "2203.06904v2",
    "19": "2404.05567v1",
    "20": "1502.02843v3",
    "21": "1511.07838v7",
    "22": "2405.14297v4",
    "23": "2104.04473v5",
    "24": "2309.06180v1",
    "25": "2206.07682v2",
    "26": "2305.18390v2",
    "27": "1308.3432v1",
    "28": "2107.03374v2",
    "29": "2112.14397v2",
    "30": "1611.06194v2",
    "31": "2308.06093v2",
    "32": "2401.08383v2",
    "33": "1910.10683v4",
    "34": "2103.13262v1",
    "35": "2205.05638v2",
    "36": "2205.14135v2",
    "37": "2304.03946v1",
    "38": "2406.10260v2",
    "39": "2308.00951v2",
    "40": "2310.01542v2",
    "41": "2305.13245v3",
    "42": "1606.08415v5",
    "43": "1506.03478v2",
    "44": "2112.06905v2",
    "45": "2002.05202v1",
    "46": "2107.11817v3",
    "47": "2303.08774v6",
    "48": "2006.16668v1",
    "49": "1704.06363v1",
    "50": "2106.04426v3",
    "51": "2203.14685v3",
    "52": "2402.08562v1",
    "53": "2404.01954v2",
    "54": "2402.12656v4",
    "55": "2403.19887v2",
    "56": "2406.16554v1",
    "57": "2404.19429v1",
    "58": "2005.14165v4",
    "59": "1907.05242v2",
    "60": "1312.4314v3",
    "61": "2109.01134v6",
    "62": "2307.09288v2",
    "63": "2401.16160v2",
    "64": "2106.09685v2",
    "65": "2405.03133v2",
    "66": "1312.4461v4",
    "67": "2404.18465v3",
    "68": "2312.00752v2",
    "69": "2009.03300v3",
    "70": "2103.03874v2",
    "71": "2211.15841v1",
    "72": "1909.08053v4",
    "73": "2310.09832v3",
    "74": "2405.13053v3",
    "75": "2304.10592v2",
    "76": "2404.15159v3",
    "77": "2401.04088v1",
    "78": "2210.05144v1",
    "79": "2404.13628v1",
    "80": "2407.04153v1",
    "81": "2312.12379v5",
    "82": "2404.02258v1",
    "83": "2202.09368v2",
    "84": "2110.01786v3",
    "85": "2212.08066v1",
    "86": "2401.15947v5",
    "87": "2405.14488v1",
    "88": "2206.02770v1",
    "89": "2301.04856v1",
    "90": "1705.09406v2",
    "91": "2207.04672v3",
    "92": "math/0703292v1",
    "93": "2409.02060v2",
    "94": "2312.00968v2",
    "95": "2204.09179v3",
    "96": "2201.10890v4",
    "97": "2402.01739v2",
    "98": "1612.00796v2",
    "99": "2305.14839v2",
    "100": "2211.05528v4",
    "101": "2204.02311v5",
    "102": "2303.10845v1",
    "103": "2301.10936v2",
    "104": "2308.12066v3",
    "105": "2101.00190v1",
    "106": "2309.05444v1",
    "107": "2308.10110v1",
    "108": "1910.07467v1",
    "109": "1904.12774v1",
    "110": "1711.01239v2",
    "111": "1411.4413v2",
    "112": "2109.10465v1",
    "113": "2303.15647v2",
    "114": "2001.08361v1",
    "115": "2106.05974v1",
    "116": "2303.07226v1",
    "117": "2403.08245v2",
    "118": "2105.13120v3",
    "119": "2311.09179v1",
    "120": "2406.06563v1",
    "121": "2310.19341v1",
    "122": "2306.03745v2",
    "123": "2303.01610v1",
    "124": "2310.07096v1",
    "125": "2212.05055v2",
    "126": "2202.08906v2",
    "127": "2204.08396v1",
    "128": "2103.14030v2",
    "129": "2101.03961v3",
    "130": "2110.04260v3",
    "131": "2210.06313v2",
    "132": "2208.02813v1",
    "133": "2203.15556v1",
    "134": "2110.14168v2",
    "135": "2110.08246v1",
    "136": "2206.03382v2",
    "137": "2405.14507v2",
    "138": "2405.11273v1",
    "139": "2206.04674v2",
    "140": "2110.07577v3",
    "141": "2202.01169v2",
    "142": "1909.11059v3",
    "143": "2310.10908v2",
    "144": "2201.11990v3",
    "145": "2405.17976v2",
    "146": "2401.10241v1",
    "147": "1904.01038v1",
    "148": "2101.06840v1"
  }
}