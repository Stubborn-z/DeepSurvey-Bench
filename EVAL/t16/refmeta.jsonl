{"paper_id": 273418823, "title": "Reply to Wang et al, Wei et al, Liu et al, and Wei et al.", "author_names": ["Rong Xu"], "venue": "Gastroenterology", "abstract": null, "year": 2024, "publicationdate": "2024-10-01", "externalids": {"DOI": "10.1053/j.gastro.2024.10.015"}, "doi_lower": "10.1053/j.gastro.2024.10.015"}
{"paper_id": 268949290, "title": "Reply to Metoki et al, Shiratori et al, and Li et al.", "author_names": ["H. W. Yoo", "Su Jin Hong"], "venue": "Gastroenterology", "abstract": null, "year": 2024, "publicationdate": "2024-04-01", "externalids": {"DOI": "10.1053/j.gastro.2024.04.003"}, "doi_lower": "10.1053/j.gastro.2024.04.003"}
{"paper_id": 1132098, "title": "Lessons from LambdaMOO: A Social, Text-Based Virtual Environment", "author_names": ["Diane J. Schiano"], "venue": "Presence: Teleoperators & Virtual Environments", "abstract": null, "year": 1999, "publicationdate": "1999-04-01", "externalids": {"DOI": "10.1162/105474699566125"}, "doi_lower": "10.1162/105474699566125"}
{"paper_id": 257978210, "title": "TextWorld", "author_names": ["P. O. Donnell"], "venue": "", "abstract": null, "year": 2009, "publicationdate": "2009-07-02", "externalids": {"DOI": "10.1353/cli.0.0046"}, "doi_lower": "10.1353/cli.0.0046"}
{"paper_id": 147025664, "title": "Bundschuh-Van Duikeren et al. 2014, etc.", "author_names": ["Ted Laros"], "venue": "", "abstract": null, "year": 2016, "publicationdate": "2016-04-19", "externalids": {}, "doi_lower": null}
{"paper_id": 59853469, "title": "A Modern Approach to Artificial Intelligence Technologies", "author_names": ["S. Aggarwal"], "venue": "", "abstract": null, "year": 2012, "publicationdate": "2012-05-15", "externalids": {}, "doi_lower": null}
{"paper_id": 3952468, "title": "Diderot's Early Philosophical Works", "author_names": ["Denis Diderot"], "venue": "Nature", "abstract": null, "year": null, "publicationdate": null, "externalids": {"DOI": "10.1038/099343b0"}, "doi_lower": "10.1038/099343b0"}
{"paper_id": 14636783, "title": "Computing Machinery and Intelligence", "author_names": ["A. Turing"], "venue": "The Philosophy of Artificial Intelligence", "abstract": null, "year": 1950, "publicationdate": "1950-10-01", "externalids": {"DOI": "10.1093/MIND/LIX.236.433"}, "doi_lower": "10.1093/mind/lix.236.433"}
{"paper_id": 221342993, "title": "Intelligent agents: theory and practice", "author_names": ["M. Wooldridge", "N. Jennings"], "venue": "Knowledge engineering review (Print)", "abstract": null, "year": 1995, "publicationdate": "1995-06-01", "externalids": {"DOI": "10.1017/S0269888900008122"}, "doi_lower": "10.1017/s0269888900008122"}
{"paper_id": 271962381, "title": "Stanford Encyclopedia of Philosophy", "author_names": ["Heather Morrison", "Michael McIntosh"], "venue": "", "abstract": null, "year": 2005, "publicationdate": null, "externalids": {"DOI": "10.5860/choice.42sup-0171"}, "doi_lower": "10.5860/choice.42sup-0171"}
{"paper_id": 267932519, "title": "Actors: a Model of Concurrent Computation in Distributed Systems (Parallel Processing, Semantics, Open, Programming Languages, Artificial Intelligence)", "author_names": ["G. Agha"], "venue": "", "abstract": null, "year": 1985, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 62742275, "title": "Software agents: A review", "author_names": ["S. Green", "L. Hurst", "B. Nangle", "P. Cunningham"], "venue": "", "abstract": null, "year": 1997, "publicationdate": "1997-05-27", "externalids": {}, "doi_lower": null}
{"paper_id": 260960356, "title": "Software Agents", "author_names": ["G. Weichhart"], "venue": "Interface Cultures", "abstract": null, "year": 2008, "publicationdate": "2008-12-31", "externalids": {"DOI": "10.1515/9783839408841-006"}, "doi_lower": "10.1515/9783839408841-006"}
{"paper_id": 109572417, "title": "Developing intelligent agent systems - a practical guide", "author_names": ["L. Padgham", "M. Winikoff"], "venue": "Wiley series in agent technology", "abstract": null, "year": 2004, "publicationdate": null, "externalids": {"DOI": "10.1002/0470861223"}, "doi_lower": "10.1002/0470861223"}
{"paper_id": 61802476, "title": "Research on Agent-Oriented Programming: Research on Agent-Oriented Programming", "author_names": ["Xinjun Mao", "Cuiyun Hu", "Yuekun Sun", "Huai-Min Wang"], "venue": "", "abstract": null, "year": 2014, "publicationdate": "2014-01-23", "externalids": {"DOI": "10.3724/SP.J.1001.2012.04297"}, "doi_lower": "10.3724/sp.j.1001.2012.04297"}
{"paper_id": 256300, "title": "Towards a Universal Theory of Artificial Intelligence Based on Algorithmic Probability and Sequential Decisions", "author_names": ["Marcus Hutter"], "venue": "European Conference on Machine Learning", "abstract": "Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown distributions. We unify both theories and give strong arguments that the resulting universal AIξ model behaves optimally in any computable environment. The major drawback of the AIξ model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIξtl, which is still superior to any other time t and length l bounded agent. The computation time of AIξtl is of the order tċ2l.", "year": 2000, "publicationdate": "2000-12-16", "externalids": {"DOI": "10.1007/3-540-44795-4_20"}, "doi_lower": "10.1007/3-540-44795-4_20"}
{"paper_id": 3146858, "title": "Planning in a Hierarchy of Abstraction Spaces", "author_names": ["E. Sacerdoti"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": null, "year": 1974, "publicationdate": "1974-06-01", "externalids": {"DOI": "10.1016/0004-3702(74)90026-5"}, "doi_lower": "10.1016/0004-3702(74)90026-5"}
{"paper_id": 207507849, "title": "Intelligence without Representation", "author_names": ["R. Brooks"], "venue": "Artificial Intelligence", "abstract": null, "year": 1991, "publicationdate": "1991-02-01", "externalids": {"DOI": "10.1016/0004-3702(91)90053-M"}, "doi_lower": "10.1016/0004-3702(91)90053-m"}
{"paper_id": 32139511, "title": "Designing autonomous agents: Theory and practice from biology to engineering and back", "author_names": ["P. Maes"], "venue": "Robotics Auton. Syst.", "abstract": null, "year": 1990, "publicationdate": "1990-06-01", "externalids": {"DOI": "10.1016/S0921-8890(05)80024-7"}, "doi_lower": "10.1016/s0921-8890(05)80024-7"}
{"paper_id": 261966625, "title": "REINFORCEMENT LEARNING AGENTS", "author_names": [], "venue": "", "abstract": null, "year": 2017, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 234343501, "title": "Reinforcement Learning: A Survey", "author_names": ["Deepali J. Joshi", "Ishaan R. Kale", "Sadanand Gandewar", "Omkar Korate", "Divya Patwari", "Shivkumar Y. Patil"], "venue": "Machine Learning and Information Processing", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.1007/978-981-33-4859-2_29"}, "doi_lower": "10.1007/978-981-33-4859-2_29"}
{"paper_id": 1811573, "title": "Enabling agents to work together", "author_names": ["R. Guha", "D. Lenat"], "venue": "CACM", "abstract": null, "year": 1994, "publicationdate": "1994-07-01", "externalids": {"DOI": "10.1145/176789.176804"}, "doi_lower": "10.1145/176789.176804"}
{"paper_id": 62482013, "title": "An Architecture for Intelligent Reactive Systems", "author_names": ["L. Kaelbling"], "venue": "", "abstract": null, "year": 1987, "publicationdate": null, "externalids": {"DOI": "10.1016/B978-0-934613-30-9.50019-6"}, "doi_lower": "10.1016/b978-0-934613-30-9.50019-6"}
{"paper_id": 283428236, "title": "Reinforcement Learning: An Introduction", "author_names": ["R. S. Sutton"], "venue": "IEEE Transactions on Neural Networks", "abstract": null, "year": 2005, "publicationdate": null, "externalids": {"DOI": "10.1109/tnn.2004.842673"}, "doi_lower": "10.1109/tnn.2004.842673"}
{"paper_id": 258040990, "title": "Generative Agents: Interactive Simulacra of Human Behavior", "author_names": ["J. Park", "Joseph C. O’Brien", "Carrie J. Cai", "M. Morris", "Percy Liang", "Michael S. Bernstein"], "venue": "ACM Symposium on User Interface Software and Technology", "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.", "year": 2023, "publicationdate": "2023-04-07", "externalids": {"DOI": "10.1145/3586183.3606763"}, "doi_lower": "10.1145/3586183.3606763"}
{"paper_id": 258832478, "title": "Interactive Natural Language Processing", "author_names": ["Zekun Wang", "Ge Zhang", "Kexin Yang", "Ning Shi", "Wangchunshu Zhou", "Shaochun Hao", "Guangzheng Xiong", "Yizhi Li", "Mong Yuan Sim", "Xiuying Chen", "Qingqing Zhu", "Zhenzhu Yang", "A. Nik", "Qi Liu", "Chenghua Lin", "Shi Wang", "Ruibo Liu", "Wenhu Chen", "Ke Xu", "Dayiheng Liu", "Yi-Ting Guo", "Jie Fu"], "venue": "arXiv.org", "abstract": "Interactive Natural Language Processing (iNLP) has emerged as a novel paradigm within the field of NLP, aimed at addressing limitations in existing frameworks while aligning with the ultimate goals of artificial intelligence. This paradigm considers language models as agents capable of observing, acting, and receiving feedback iteratively from external entities. Specifically, language models in this context can: (1) interact with humans for better understanding and addressing user needs, personalizing responses, aligning with human values, and improving the overall user experience; (2) interact with knowledge bases for enriching language representations with factual knowledge, enhancing the contextual relevance of responses, and dynamically leveraging external information to generate more accurate and informed responses; (3) interact with models and tools for effectively decomposing and addressing complex tasks, leveraging specialized expertise for specific subtasks, and fostering the simulation of social behaviors; and (4) interact with environments for learning grounded representations of language, and effectively tackling embodied tasks such as reasoning, planning, and decision-making in response to environmental observations. This paper offers a comprehensive survey of iNLP, starting by proposing a unified definition and framework of the concept. We then provide a systematic classification of iNLP, dissecting its various components, including interactive objects, interaction interfaces, and interaction methods. We proceed to delve into the evaluation methodologies used in the field, explore its diverse applications, scrutinize its ethical and safety issues, and discuss prospective research directions. This survey serves as an entry point for researchers who are interested in this rapidly evolving area and offers a broad view of the current landscape and future trajectory of iNLP.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.48550/arXiv.2305.13246"}, "doi_lower": "10.48550/arxiv.2305.13246"}
{"paper_id": 246426909, "title": "Training language models to follow instructions with human feedback", "author_names": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "John Schulman", "Jacob Hilton", "Fraser Kelton", "Luke E. Miller", "Maddie Simens", "Amanda Askell", "Peter Welinder", "P. Christiano", "Jan Leike", "Ryan J. Lowe"], "venue": "Neural Information Processing Systems", "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.", "year": 2022, "publicationdate": "2022-03-04", "externalids": {}, "doi_lower": null}
{"paper_id": 257532815, "title": "GPT-4 Technical Report", "author_names": ["OpenAI Josh Achiam", "Steven Adler", "Sandhini Agarwal", "Lama Ahmad", "Ilge Akkaya", "Florencia Leoni Aleman", "Diogo Almeida", "Janko Altenschmidt", "Sam Altman", "Shyamal Anadkat", "Red Avila", "Igor Babuschkin", "S. Balaji", "Valerie Balcom", "Paul Baltescu", "Haim-ing Bao", "Mo Bavarian", "J. Belgum", "Irwan Bello", "Jake Berdine", "Gabriel Bernadett-Shapiro", "Christopher Berner", "Lenny Bogdonoff", "Oleg Boiko", "Made-laine Boyd", "Anna-Luisa Brakman", "Greg Brockman", "Tim Brooks", "Miles Brundage", "Kevin Button", "Trevor Cai", "Rosie Campbell", "Andrew Cann", "Brittany Carey", "Chelsea Carlson", "Rory Carmichael", "Brooke Chan", "Che Chang", "Fotis Chantzis", "Derek Chen", "Sully Chen", "Ruby Chen", "Jason Chen", "Mark Chen", "Benjamin Chess", "Chester Cho", "Casey Chu", "Hyung Won Chung", "Dave Cummings", "Jeremiah Currier", "Yunxing Dai", "Cory Decareaux", "Thomas Degry", "Noah Deutsch", "Damien Deville", "Arka Dhar", "David Dohan", "Steve Dowling", "Sheila Dunning", "Adrien Ecoffet", "Atty Eleti", "Tyna Eloundou", "David Farhi", "L. Fedus", "Niko Felix", "Sim'on Posada Fishman", "Juston Forte", "Is-abella Fulford", "Leo Gao", "Elie Georges", "C. Gibson", "Vik Goel", "Tarun Gogineni", "Gabriel Goh", "Raphael Gontijo-Lopes", "Jonathan Gordon", "Morgan Grafstein", "Scott Gray", "Ryan Greene", "Joshua Gross", "S. Gu", "Yufei Guo", "Chris Hallacy", "Jesse Han", "Jeff Harris", "Yuchen He", "Mike Heaton", "Johannes Heidecke", "Chris Hesse", "Alan Hickey", "W. Hickey", "Peter Hoeschele", "Brandon Houghton", "Kenny Hsu", "Shengli Hu", "Xin Hu", "Joost Huizinga", "Shantanu Jain", "Shawn Jain", "Joanne Jang", "Angela Jiang", "Roger Jiang", "Haozhun Jin", "Denny Jin", "Shino Jomoto", "B. Jonn", "Heewoo Jun", "Tomer Kaftan", "Lukasz Kaiser", "Ali Kamali", "I. Kanitscheider", "N. Keskar", "Tabarak Khan", "Logan Kilpatrick", "Jong Wook Kim", "Christina Kim", "Yongjik Kim", "Hendrik Kirchner", "J. Kiros", "Matthew Knight", "Daniel Kokotajlo", "Lukasz Kondraciuk", "Andrew Kondrich", "Aris Konstantinidis", "Kyle Kosic", "Gretchen Krueger", "Vishal Kuo", "Michael Lampe", "Ikai Lan", "Teddy Lee", "Jan Leike", "Jade Leung", "Daniel Levy", "Chak Li", "Rachel Lim", "Molly Lin", "Stephanie Lin", "Ma-teusz Litwin", "Theresa Lopez", "Ryan Lowe", "Patricia Lue", "A. Makanju", "Kim Malfacini", "Sam Manning", "Todor Markov", "Yaniv Markovski", "Bianca Martin", "Katie Mayer", "Andrew Mayne", "Bob McGrew", "S. McKinney", "Christine McLeavey", "Paul McMillan", "Jake McNeil", "David Medina", "Aalok Mehta", "Jacob Menick", "Luke Metz", "An-drey Mishchenko", "Pamela Mishkin", "Vinnie Monaco", "Evan Morikawa", "Daniel P. Mossing", "Tong Mu", "Mira Murati", "O. Murk", "David M'ely", "Ashvin Nair", "Reiichiro Nakano", "Rajeev Nayak", "Arvind Neelakantan", "Richard Ngo", "Hyeonwoo Noh", "Ouyang Long", "Cullen O'Keefe", "J. Pachocki", "A. Paino", "Joe Palermo", "Ashley Pantuliano", "Giambattista Parascandolo", "J. Parish", "Emy Parparita", "Alexandre Passos", "Mikhail Pavlov", "Andrew Peng", "Adam Perelman", "Filipe de Avila Belbute Peres", "Michael Petrov", "Henrique Pondé de Oliveira Pinto", "Michael Pokorny", "Michelle Pokrass", "Vitchyr H. Pong", "Tolly Powell", "Alethea Power", "Boris Power", "Elizabeth Proehl", "Raul Puri", "Alec Radford", "Jack W. Rae", "Aditya Ramesh", "Cameron Raymond", "Francis Real", "Kendra Rimbach", "Carl Ross", "Bob Rotsted", "Henri Roussez", "N. Ryder", "M. Saltarelli", "Ted Sanders", "Shibani Santurkar", "Girish Sastry", "Heather Schmidt", "David Schnurr", "John Schulman", "Daniel Selsam", "Kyla Sheppard", "Toki Sherbakov", "Jessica Shieh", "Sarah Shoker", "Pranav Shyam", "Szymon Sidor", "Eric Sigler", "Maddie Simens", "Jordan Sitkin", "Katarina Slama", "Ian Sohl", "Benjamin Sokolowsky", "Yang Song", "Natalie Staudacher", "F. Such", "Natalie Summers", "I. Sutskever", "Jie Tang", "N. Tezak", "Madeleine Thompson", "P. Tillet", "Amin Tootoonchian", "Elizabeth Tseng", "Preston Tuggle", "Nick Turley", "Jerry Tworek", "Juan Felipe Cer'on Uribe", "Andrea Vallone", "Arun Vijayvergiya", "Chelsea Voss", "Carroll L. Wainwright", "Justin Jay Wang", "Alvin Wang", "Ben Wang", "Jonathan Ward", "Jason Wei", "CJ Weinmann", "Akila Welihinda", "Peter Welinder", "Jiayi Weng", "Lilian Weng", "Matt Wiethoff", "Dave Willner", "Clemens Winter", "Samuel Wolrich", "Hannah Wong", "Lauren Workman", "Sherwin Wu", "Jeff Wu", "Michael Wu", "Kai Xiao", "Tao Xu", "Sarah Yoo", "Kevin Yu", "Qim-ing Yuan", "Wojciech Zaremba", "Rowan Zellers", "Chong Zhang", "Marvin Zhang", "Shengjia Zhao", "Tianhao Zheng", "Juntang Zhuang", "William Zhuk", "Barret Zoph"], "venue": "", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {}, "doi_lower": null}
{"paper_id": 258947756, "title": "Training Socially Aligned Language Models in Simulated Human Society", "author_names": ["Ruibo Liu", "Ruixin Yang", "Chenyan Jia", "Ge Zhang", "Denny Zhou", "Andrew M. Dai", "Diyi Yang", "Soroush Vosoughi"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.16960"}, "doi_lower": "10.48550/arxiv.2305.16960"}
{"paper_id": 261556862, "title": "Cognitive Architectures for Language Agents", "author_names": ["T. Sumers", "Shunyu Yao", "Karthik Narasimhan", "Thomas L. Griffiths"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.", "year": 2023, "publicationdate": "2023-09-05", "externalids": {"DOI": "10.48550/arXiv.2309.02427"}, "doi_lower": "10.48550/arxiv.2309.02427"}
{"paper_id": 277983858, "title": "Conceptual Foundations of LLM-Powered Agents: From Language Processing to Autonomous Reasoning", "author_names": ["Yifei Jian"], "venue": "Scientific Journal of Intelligent Systems Research", "abstract": "Large language models (LLMs) have transformed artificial intelligence by enabling advanced natural language processing and generation. However, their evolution toward autonomous agents require additional capabilities, including structured reasoning, adaptive learning, and interaction with external environments. This article explores the progression from early NLP models to modern LLM-based agents, detailing their core mechanisms, key capabilities, and applications. Additionally, we discuss the challenges in developing fully autonomous AI systems, such as alignment issues, computational efficiency, and decision-making constraints. Finally, we examine future trends, including reinforcement learning integration, enhanced memory architectures, and humanAI collaboration, which will shape the next generation of intelligent agents.", "year": 2025, "publicationdate": "2025-04-20", "externalids": {"DOI": "10.54691/atmxvb43"}, "doi_lower": "10.54691/atmxvb43"}
{"paper_id": 257663729, "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4", "author_names": ["Sébastien Bubeck", "Varun Chandrasekaran", "Ronen Eldan", "J. Gehrke", "Eric Horvitz", "Ece Kamar", "Peter Lee", "Y. Lee", "Yuan-Fang Li", "Scott M. Lundberg", "Harsha Nori", "Hamid Palangi", "Marco Tulio Ribeiro", "Yi Zhang"], "venue": "arXiv.org", "abstract": "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.", "year": 2023, "publicationdate": "2023-03-22", "externalids": {}, "doi_lower": null}
{"paper_id": 125567544, "title": "Intention. G. E. M. Anscombe", "author_names": ["Irving M. Copi"], "venue": "", "abstract": null, "year": 1959, "publicationdate": "1959-04-01", "externalids": {"DOI": "10.1086/287660"}, "doi_lower": "10.1086/287660"}
{"paper_id": 171730555, "title": "Reasons, Actions, and Causes", "author_names": ["Chiwook Won"], "venue": "", "abstract": null, "year": 2013, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 144474830, "title": "Agent, action, and reason", "author_names": ["Robert W. Binkley", "Richard Bronaugh", "Ausonio Marras"], "venue": "", "abstract": null, "year": 1971, "publicationdate": "1971-01-31", "externalids": {"DOI": "10.3138/9781442656963"}, "doi_lower": "10.3138/9781442656963"}
{"paper_id": 145750868, "title": "Précis of The Intentional Stance", "author_names": ["D. Dennett"], "venue": "Behavioral and Brain Sciences", "abstract": null, "year": 1988, "publicationdate": "1988-09-01", "externalids": {"DOI": "10.1017/S0140525X00058611"}, "doi_lower": "10.1017/s0140525x00058611"}
{"paper_id": 11566080, "title": "Defining Agency: Individuality, Normativity, Asymmetry, and Spatio-temporality in Action", "author_names": ["Xabier E. Barandiaran", "E. Paolo", "M. Rohde"], "venue": "Adaptive Behavior", "abstract": "The concept of agency is of crucial importance in cognitive science and artificial intelligence, and it is often used as an intuitive and rather uncontroversial term, in contrast to more abstract and theoretically heavily weighted terms such as intentionality , rationality, or mind. However, most of the available definitions of agency are too loose or unspecific to allow for a progressive scientific research program. They implicitly and unproblematically assume the features that characterize agents, thus obscuring the full potential and challenge of modeling agency. We identify three conditions that a system must meet in order to be considered as a genuine agent: (a) a system must define its own individuality, (b) it must be the active source of activity in its environment (interactional asymmetry), and (c) it must regulate this activity in relation to certain norms (normativity). We find that even minimal forms of proto-cellular systems can already provide a paradigmatic example of genuine agency. By abstracting away some specific details of minimal models of living agency we define the kind of organization that is capable of meeting the required conditions for agency (which is not restricted to living organisms). On this basis, we define agency as an autonomous organization that adaptively regulates its coupling with its environment and contributes to sustaining itself as a consequence. We find that spatiality and temporality are the two fundamental domains in which agency spans at different scales. We conclude by giving an outlook for the road that lies ahead in the pursuit of understanding, modeling, and synthesizing agents.", "year": 2009, "publicationdate": "2009-10-01", "externalids": {"DOI": "10.1177/1059712309343819"}, "doi_lower": "10.1177/1059712309343819"}
{"paper_id": 11824179, "title": "Ascribing Mental Qualities to Machines", "author_names": ["John McCarthy"], "venue": "", "abstract": null, "year": 1979, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 18412522, "title": "The Synthesis of Digital Machines With Provable Epistemic Properties", "author_names": ["S. Rosenschein", "L. Kaelbling"], "venue": "Theoretical Aspects of Rationality and Knowledge", "abstract": null, "year": 1986, "publicationdate": "1986-03-19", "externalids": {"DOI": "10.1016/B978-0-934613-04-0.50009-0"}, "doi_lower": "10.1016/b978-0-934613-04-0.50009-0"}
{"paper_id": 49313245, "title": "Improving Language Understanding by Generative Pre-Training", "author_names": ["Alec Radford", "Karthik Narasimhan"], "venue": "", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 160025533, "title": "Language Models are Unsupervised Multitask Learners", "author_names": ["Alec Radford", "Jeff Wu", "R. Child", "D. Luan", "Dario Amodei", "I. Sutskever"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 235097243, "title": "Limitations of Autoregressive Models and Their Alternatives", "author_names": ["Chu-Cheng Lin", "Aaron Jaech", "Xin Li", "M. Gormley", "Jason Eisner"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Standard autoregressive language models perform only polynomial-time computation to compute the probability of the next symbol. While this is attractive, it means they cannot model distributions whose next-symbol probability is hard to compute. Indeed, they cannot even model them well enough to solve associated easy decision problemsin main text, note that they’re easy because you get to see the whole string rather than a prefix—this is really the difference between checking a given assignment against a formula and asking whether any satisfying assignment exists for which an engineer might want to consult a language model. These limitations apply no matter how much computation and data are used to train the model, unless the model is given access to oracle parameters that grow superpolynomially in sequence length. Thus, simply training larger autoregressive language models is not a panacea for NLP. Alternatives include energy-based models (which give up efficient sampling) and latent-variable autoregressive models (which give up efficient scoring of a given string). Both are powerful enough to escape the above limitations.", "year": 2020, "publicationdate": "2020-10-23", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.405"}, "doi_lower": "10.18653/v1/2021.naacl-main.405"}
{"paper_id": 144940329, "title": "Constructing a Language: A Usage-Based Theory of Language Acquisition", "author_names": ["H. Gross"], "venue": "", "abstract": null, "year": 2005, "publicationdate": "2005-06-01", "externalids": {"DOI": "10.1097/01.nmd.0000165300.84030.1a"}, "doi_lower": "10.1097/01.nmd.0000165300.84030.1a"}
{"paper_id": 143256689, "title": "How children learn the meanings of words", "author_names": ["P. Bloom"], "venue": "", "abstract": null, "year": 2000, "publicationdate": null, "externalids": {"DOI": "10.7551/mitpress/3577.001.0001"}, "doi_lower": "10.7551/mitpress/3577.001.0001"}
{"paper_id": 141996838, "title": "Grounding Cognition: Introduction to Grounding Cognition: The Role of Perception and Action in Memory, Language, and Thinking", "author_names": ["D. Pecher", "Rolf A. Zwaan"], "venue": "", "abstract": null, "year": 2005, "publicationdate": null, "externalids": {"DOI": "10.1017/CBO9780511499968.001"}, "doi_lower": "10.1017/cbo9780511499968.001"}
{"paper_id": 254246305, "title": "Language Models as Agent Models", "author_names": ["Jacob Andreas"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in an outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them -- a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of intentional communication in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language. I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- LMs infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.", "year": 2022, "publicationdate": "2022-12-03", "externalids": {"DOI": "10.48550/arXiv.2212.01681"}, "doi_lower": "10.48550/arxiv.2212.01681"}
{"paper_id": 259224900, "title": "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought", "author_names": ["L. Wong", "Gabriel Grand", "Alexander K. Lew", "Noah D. Goodman", "Vikash K. Mansinghka", "Jacob Andreas", "J. Tenenbaum"], "venue": "arXiv.org", "abstract": "How does language inform our downstream thinking? In particular, how do humans make meaning from language--and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose rational meaning construction, a computational framework for language-informed thinking that combines neural language models with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling. Our architecture integrates two computational tools that have not previously come together: we model thinking with probabilistic programs, an expressive representation for commonsense reasoning; and we model meaning construction with large language models (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language. We illustrate our framework through examples covering four core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, and social reasoning. In each, we show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning. We extend our framework to integrate cognitively-motivated symbolic modules (physics simulators, graphics engines, and planning algorithms) to provide a unified commonsense thinking interface from language. Finally, we explore how language can drive the construction of world models themselves. We hope this work will provide a roadmap towards cognitive models and AI systems that synthesize the insights of both modern and classical computational perspectives.", "year": 2023, "publicationdate": "2023-06-22", "externalids": {"DOI": "10.48550/arXiv.2306.12672"}, "doi_lower": "10.48550/arxiv.2306.12672"}
{"paper_id": 14838925, "title": "Learning to Generate Reviews and Discovering Sentiment", "author_names": ["Alec Radford", "R. Józefowicz", "I. Sutskever"], "venue": "arXiv.org", "abstract": "We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.", "year": 2017, "publicationdate": "2017-04-05", "externalids": {}, "doi_lower": null}
{"paper_id": 10021863, "title": "An intelligent system for document retrieval in distributed office environments", "author_names": ["Uttam Mukhopadhyay", "L. Stephens", "M. Huhns", "R. Bonnell"], "venue": "Journal of the American Society for Information Science", "abstract": null, "year": 1986, "publicationdate": "1986-05-01", "externalids": {"DOI": "10.1002/(SICI)1097-4571(198605)37:3%3C123::AID-ASI3%3E3.0.CO;2-3"}, "doi_lower": "10.1002/(sici)1097-4571(198605)37:3%3c123::aid-asi3%3e3.0.co;2-3"}
{"paper_id": 29500588, "title": "Situated agents can have goals", "author_names": ["P. Maes"], "venue": "Robotics Auton. Syst.", "abstract": null, "year": 1990, "publicationdate": "1990-06-01", "externalids": {"DOI": "10.1016/S0921-8890(05)80028-4"}, "doi_lower": "10.1016/s0921-8890(05)80028-4"}
{"paper_id": 18995699, "title": "Toward agent programs with circuit semantics", "author_names": ["N. Nilsson"], "venue": "", "abstract": null, "year": 1992, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 10918398, "title": "Modelling Interacting Agents in Dynamic Environments", "author_names": ["J. Müller", "M. Pischel"], "venue": "European Conference on Artificial Intelligence", "abstract": null, "year": 1994, "publicationdate": "1994-08-08", "externalids": {}, "doi_lower": null}
{"paper_id": 263892776, "title": "A robust layered control system for a mobile robot", "author_names": ["Rodney A. Brooks"], "venue": "", "abstract": null, "year": 1991, "publicationdate": "1991-06-01", "externalids": {}, "doi_lower": null}
{"paper_id": 5847773, "title": "Intelligence Without Reason", "author_names": ["R. Brooks"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": null, "year": 1991, "publicationdate": "1991-08-24", "externalids": {"DOI": "10.4324/9781351001885-2"}, "doi_lower": "10.4324/9781351001885-2"}
{"paper_id": 5581562, "title": "Computer science as empirical inquiry: symbols and search", "author_names": ["A. Newell", "H. Simon"], "venue": "CACM", "abstract": null, "year": 1976, "publicationdate": "1976-03-01", "externalids": {"DOI": "10.1145/360018.360022"}, "doi_lower": "10.1145/360018.360022"}
{"paper_id": 63883486, "title": "Essentials Of Artificial Intelligence", "author_names": ["U. Fink"], "venue": "", "abstract": null, "year": 2016, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 30282456, "title": "Practical planning - extending the classical AI planning paradigm", "author_names": ["D. Wilkins"], "venue": "Morgan Kaufmann series in representation and reasoning", "abstract": null, "year": 1989, "publicationdate": "1989-01-14", "externalids": {}, "doi_lower": null}
{"paper_id": 144556467, "title": "Rhetoric, Action, and Agency in Institutionalized Science and Technology", "author_names": ["W. Kinsella"], "venue": "The State of Rhetoric of Science and Technology", "abstract": "This essay argues that to an unprecedented degree the practices of contemporary science and technology are embedded within complex institutional systems. This embeddedness problematizes received views of rhetorical action and agency, which must be reformulated to locate these principles within larger systems of power/ knowledge. Three sets of resources are identified for this reformulation: theories of organizational rhetoric, Foucauldian studies of knowledge-intensive organizations, and Foucauldian approaches to the philosophy of science.", "year": 2005, "publicationdate": "2005-07-01", "externalids": {"DOI": "10.1207/s15427625tcq1403_8"}, "doi_lower": "10.1207/s15427625tcq1403_8"}
{"paper_id": 8917481, "title": "The Nonlinear Nature of Plans", "author_names": ["E. Sacerdoti"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": null, "year": 1975, "publicationdate": "1975-09-03", "externalids": {}, "doi_lower": null}
{"paper_id": 763785, "title": "Universal Plans for Reactive Robots in Unpredictable Environments", "author_names": ["M. Schoppers"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": null, "year": 1987, "publicationdate": "1987-08-23", "externalids": {}, "doi_lower": null}
{"paper_id": 263892776, "title": "A robust layered control system for a mobile robot", "author_names": ["Rodney A. Brooks"], "venue": "", "abstract": null, "year": 1991, "publicationdate": "1991-06-01", "externalids": {}, "doi_lower": null}
{"paper_id": 14250548, "title": "Steps toward Artificial Intelligence", "author_names": ["M. Minsky"], "venue": "Proceedings of the IRE", "abstract": null, "year": 1995, "publicationdate": "1995-10-26", "externalids": {"DOI": "10.1109/JRPROC.1961.287775"}, "doi_lower": "10.1109/jrproc.1961.287775"}
{"paper_id": 59809750, "title": "Learning from delayed rewards", "author_names": ["C. Watkins"], "venue": "", "abstract": null, "year": 1989, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 59872172, "title": "On-line Q-learning using connectionist systems", "author_names": ["Gavin Adrian Rummery", "M. Niranjan"], "venue": "", "abstract": null, "year": 1994, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 6023746, "title": "Temporal Difference Learning and TD-Gammon", "author_names": ["G. Tesauro"], "venue": "CACM", "abstract": null, "year": 1995, "publicationdate": "1995-03-01", "externalids": {"DOI": "10.1145/203330.203343"}, "doi_lower": "10.1145/203330.203343"}
{"paper_id": 202729266, "title": "An Overview of Deep Reinforcement Learning", "author_names": ["LiChun Cao", "ZhiMin"], "venue": "International Conference on Automation, Control and Robotics Engineering", "abstract": "As a new machine learning method, deep reinforcement learning has made important progress in various fields of people's production and life since it was proposed. However, there are still many difficulties in function design and other aspects. Therefore, further research on deep reinforcement learning is of great significance for promoting the progress of the whole science and society. Based on the basic theory of deep learning, this paper introduces the basic theory, research method, main network model and successful application in various fields of deep reinforcement learning.", "year": 2019, "publicationdate": "2019-07-19", "externalids": {"DOI": "10.1145/3351917.3351989"}, "doi_lower": "10.1145/3351917.3351989"}
{"paper_id": 15238391, "title": "Playing Atari with Deep Reinforcement Learning", "author_names": ["Volodymyr Mnih", "K. Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "D. Wierstra", "Martin A. Riedmiller"], "venue": "arXiv.org", "abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.", "year": 2013, "publicationdate": "2013-12-19", "externalids": {}, "doi_lower": null}
{"paper_id": 52904113, "title": "Generalization and Regularization in DQN", "author_names": ["Jesse Farebrother", "Marlos C. Machado", "Michael H. Bowling"], "venue": "arXiv.org", "abstract": "Deep reinforcement learning (RL) algorithms have shown an impressive ability to learn complex control policies in high-dimensional environments. However, despite the ever-increasing performance on popular benchmarks such as the Arcade Learning Environment (ALE), policies learned by deep RL algorithms often struggle to generalize when evaluated in remarkably similar environments. In this paper, we assess the generalization capabilities of DQN, one of the most traditional deep RL algorithms in the field. We provide evidence suggesting that DQN overspecializes to the training environment. We comprehensively evaluate the impact of traditional regularization methods, $\\ell_2$-regularization and dropout, and of reusing the learned representations to improve the generalization capabilities of DQN. We perform this study using different game modes of Atari 2600 games, a recently introduced modification for the ALE which supports slight variations of the Atari 2600 games traditionally used for benchmarking. Despite regularization being largely underutilized in deep RL, we show that it can, in fact, help DQN learn more general features. These features can then be reused and fine-tuned on similar tasks, considerably improving the sample efficiency of DQN.", "year": 2018, "publicationdate": "2018-09-27", "externalids": {}, "doi_lower": null}
{"paper_id": 5011374, "title": "A Study on Overfitting in Deep Reinforcement Learning", "author_names": ["Chiyuan Zhang", "O. Vinyals", "R. Munos", "Samy Bengio"], "venue": "arXiv.org", "abstract": "Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen \"robustly\": commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.", "year": 2018, "publicationdate": "2018-04-18", "externalids": {}, "doi_lower": null}
{"paper_id": 234868359, "title": "Challenges of real-world reinforcement learning: definitions, benchmarks and analysis", "author_names": ["Cosmin Paduraru", "D. Mankowitz", "Gabriel Dulac-Arnold", "Jerry Li", "Nir Levine", "Sven Gowal", "Todd Hester"], "venue": "Machine-mediated learning", "abstract": null, "year": 2021, "publicationdate": "2021-04-22", "externalids": {"DOI": "10.1007/s10994-021-05961-4"}, "doi_lower": "10.1007/s10994-021-05961-4"}
{"paper_id": 235829153, "title": "Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability", "author_names": ["Dibya Ghosh", "Jad Rahme", "Aviral Kumar", "Amy Zhang", "Ryan P. Adams", "S. Levine"], "venue": "Neural Information Processing Systems", "abstract": "Generalization is a central challenge for the deployment of reinforcement learning (RL) systems in the real world. In this paper, we show that the sequential structure of the RL problem necessitates new approaches to generalization beyond the well-studied techniques used in supervised learning. While supervised learning methods can generalize effectively without explicitly accounting for epistemic uncertainty, we show that, perhaps surprisingly, this is not the case in RL. We show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Informed by this observation, we recast the problem of generalization in RL as solving the induced partially observed Markov decision process, which we call the epistemic POMDP. We demonstrate the failure modes of algorithms that do not appropriately handle this partial observability, and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, we demonstrate that our simple algorithm derived from the epistemic POMDP achieves significant gains in generalization over current methods on the Procgen benchmark suite.", "year": 2021, "publicationdate": "2021-07-13", "externalids": {}, "doi_lower": null}
{"paper_id": 15700284, "title": "Policy Transfer using Reward Shaping", "author_names": ["Tim Brys", "A. Harutyunyan", "Matthew E. Taylor", "A. Nowé"], "venue": "Adaptive Agents and Multi-Agent Systems", "abstract": null, "year": 2015, "publicationdate": "2015-05-04", "externalids": {}, "doi_lower": null}
{"paper_id": 8241258, "title": "Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning", "author_names": ["Emilio Parisotto", "Jimmy Ba", "R. Salakhutdinov"], "venue": "International Conference on Learning Representations", "abstract": "The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed \"Actor-Mimic\", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.", "year": 2015, "publicationdate": "2015-11-19", "externalids": {}, "doi_lower": null}
{"paper_id": 221761694, "title": "Transfer Learning in Deep Reinforcement Learning: A Survey", "author_names": ["Zhuangdi Zhu", "Kaixiang Lin", "Anil K. Jain", "Jiayu Zhou"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progress.", "year": 2020, "publicationdate": "2020-09-16", "externalids": {"DOI": "10.1109/TPAMI.2023.3292075"}, "doi_lower": "10.1109/tpami.2023.3292075"}
{"paper_id": 260423621, "title": "RL: FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING", "author_names": ["Yan Duan", "John Schulman", "Xi Chen", "P. Bartlett", "I. Sutskever", "P. Abbeel"], "venue": "", "abstract": null, "year": 2016, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 260456700, "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "author_names": [], "venue": "", "abstract": null, "year": 2017, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 3418899, "title": "Meta-Reinforcement Learning of Structured Exploration Strategies", "author_names": ["Abhishek Gupta", "R. Mendonca", "Yuxuan Liu", "P. Abbeel", "S. Levine"], "venue": "Neural Information Processing Systems", "abstract": "Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.", "year": 2018, "publicationdate": "2018-02-20", "externalids": {}, "doi_lower": null}
{"paper_id": 84187276, "title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables", "author_names": ["Kate Rakelly", "Aurick Zhou", "Deirdre Quillen", "Chelsea Finn", "S. Levine"], "venue": "International Conference on Machine Learning", "abstract": "Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While in principle meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. The also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness in sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.", "year": 2019, "publicationdate": "2019-03-19", "externalids": {}, "doi_lower": null}
{"paper_id": 93400, "title": "Topology, Crystallized (Experiments): P. Dziawa et al., arXiv:1206.1705; S.-Y. Xu et al., arXiv:1206.2088", "author_names": ["T. Hsieh", "J. Moore"], "venue": "", "abstract": null, "year": 2012, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 52938664, "title": "Meta-Learning: A Survey", "author_names": ["Joaquin Vanschoren"], "venue": "Automated Machine Learning", "abstract": "Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.", "year": 2018, "publicationdate": "2018-10-08", "externalids": {"DOI": "10.1007/978-3-030-05318-5_2"}, "doi_lower": "10.1007/978-3-030-05318-5_2"}
{"paper_id": 17216004, "title": "Transfer Learning for Reinforcement Learning Domains: A Survey", "author_names": ["Matthew E. Taylor", "P. Stone"], "venue": "Journal of machine learning research", "abstract": null, "year": 2009, "publicationdate": "2009-12-01", "externalids": {"DOI": "10.5555/1577069.1755839"}, "doi_lower": "10.5555/1577069.1755839"}
{"paper_id": 44116893, "title": "Importance Weighted Transfer of Samples in Reinforcement Learning", "author_names": ["Andrea Tirinzoni", "Andrea Sessa", "Matteo Pirotta", "Marcello Restelli"], "venue": "International Conference on Machine Learning", "abstract": "We consider the transfer of experience samples (i.e., tuples ) in reinforcement learning (RL), collected from a set of source tasks to improve the learning process in a given target task. Most of the related approaches focus on selecting the most relevant source samples for solving the target task, but then all the transferred samples are used without considering anymore the discrepancies between the task models. In this paper, we propose a model-based technique that automatically estimates the relevance (importance weight) of each source sample for solving the target task. In the proposed approach, all the samples are transferred and used by a batch RL algorithm to solve the target task, but their contribution to the learning process is proportional to their importance weight. By extending the results for importance weighting provided in supervised learning literature, we develop a finite-sample analysis of the proposed batch RL algorithm. Furthermore, we empirically compare the proposed algorithm to state-of-the-art approaches, showing that it achieves better learning performance and is very robust to negative transfer, even when some source tasks are significantly different from the target task.", "year": 2018, "publicationdate": "2018-05-28", "externalids": {}, "doi_lower": null}
{"paper_id": 272983731, "title": "APPLICATION OF META-LEARNING IN MULTI-AGENT REINFORCEMENT LEARNING - A SURVEY", "author_names": [], "venue": "Trends in Social Sciences and Humanities Research", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.61784/tsshr3075"}, "doi_lower": "10.61784/tsshr3075"}
{"paper_id": 261064713, "title": "A survey on large language model based autonomous agents", "author_names": ["Lei Wang", "Chengbang Ma", "Xueyang Feng", "Zeyu Zhang", "Hao-ran Yang", "Jingsen Zhang", "Zhi-Yang Chen", "Jiakai Tang", "Xu Chen", "Yankai Lin", "Wayne Xin Zhao", "Zhewei Wei", "Ji-rong Wen"], "venue": "Frontiers of Computer Science", "abstract": "Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.", "year": 2023, "publicationdate": "2023-08-22", "externalids": {"DOI": "10.1007/s11704-024-40231-1"}, "doi_lower": "10.1007/s11704-024-40231-1"}
{"paper_id": 245329531, "title": "WebGPT: Browser-assisted question-answering with human feedback", "author_names": ["Reiichiro Nakano", "Jacob Hilton", "S. Balaji", "Jeff Wu", "Ouyang Long", "Christina Kim", "Christopher Hesse", "Shantanu Jain", "Vineet Kosaraju", "W. Saunders", "Xu Jiang", "K. Cobbe", "Tyna Eloundou", "Gretchen Krueger", "Kevin Button", "Matthew Knight", "Benjamin Chess", "John Schulman"], "venue": "arXiv.org", "abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.", "year": 2021, "publicationdate": "2021-12-17", "externalids": {}, "doi_lower": null}
{"paper_id": 252762395, "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "author_names": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "venue": "International Conference on Learning Representations", "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io", "year": 2022, "publicationdate": "2022-10-06", "externalids": {}, "doi_lower": null}
{"paper_id": 256697342, "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "author_names": ["Timo Schick", "Jane Dwivedi-Yu", "Roberto Dessì", "R. Raileanu", "M. Lomeli", "Luke Zettlemoyer", "Nicola Cancedda", "Thomas Scialom"], "venue": "Neural Information Processing Systems", "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.", "year": 2023, "publicationdate": "2023-02-09", "externalids": {"DOI": "10.48550/arXiv.2302.04761"}, "doi_lower": "10.48550/arxiv.2302.04761"}
{"paper_id": 271304591, "title": "Chameleon : Plug-and-Play Compositional Reasoning with Large Language Models", "author_names": [], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "author_names": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "Ed H. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "venue": "Neural Information Processing Systems", "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 249017743, "title": "Large Language Models are Zero-Shot Reasoners", "author_names": ["Takeshi Kojima", "S. Gu", "Machel Reid", "Yutaka Matsuo", "Yusuke Iwasawa"], "venue": "Neural Information Processing Systems", "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.", "year": 2022, "publicationdate": "2022-05-24", "externalids": {}, "doi_lower": null}
{"paper_id": 248986239, "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "author_names": ["Denny Zhou", "Nathanael Scharli", "Le Hou", "Jason Wei", "Nathan Scales", "Xuezhi Wang", "D. Schuurmans", "O. Bousquet", "Quoc Le", "Ed H. Chi"], "venue": "International Conference on Learning Representations", "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.", "year": 2022, "publicationdate": "2022-05-21", "externalids": {"DOI": "10.48550/arXiv.2205.10625"}, "doi_lower": "10.48550/arxiv.2205.10625"}
{"paper_id": 258865576, "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement", "author_names": ["Zhiheng Xi", "Senjie Jin", "Yuhao Zhou", "Rui Zheng", "Songyang Gao", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "To enhance the multi-step reasoning capabilities of large language models, researchers have extensively explored prompting methods, notably the Chain-of-Thought (CoT) method which explicitly elicits human-like rationales. However, they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems. In this work, we start from the problem side and propose Self-Polish (SP), a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable. We also explore several automatic prompting varients and propose the Self-Polish prompt bank for the community. SP is orthogonal to all other prompting methods of answer/reasoning side like CoT, allowing for seamless integration with state-of-the-art techniques for further improvement. Thorough experiments show that the proposed method attains notable and consistent effectiveness on five reasoning benchmarks across different models. Furthermore, our method also showcases impressive performance on robustness evaluation. Codes and prompts are available at https://github.com/WooooDyy/Self-Polish.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14497"}, "doi_lower": "10.48550/arxiv.2305.14497"}
{"paper_id": 258833055, "title": "Reflexion: language agents with verbal reinforcement learning", "author_names": ["Noah Shinn", "Federico Cassano", "Beck Labash", "A. Gopinath", "Karthik Narasimhan", "Shunyu Yao"], "venue": "Neural Information Processing Systems", "abstract": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.", "year": 2023, "publicationdate": "2023-03-20", "externalids": {}, "doi_lower": null}
{"paper_id": 258685337, "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs", "author_names": ["Afra Feyza Akyürek", "Ekin Akyürek", "Aman Madaan", "A. Kalyan", "Peter Clark", "Derry Tanti Wijaya", "Niket Tandon"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Despite their unprecedented success, even the largest language models make mistakes.Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 times its size. RL4F produces critiques that help GPT-3 revise its outputs. We study three datasets for action planning, summarization and alphabetization and show relative improvements up to 10% in multiple text similarity metrics over other learned, retrieval-augmented or prompting-based critique generators.", "year": 2023, "publicationdate": "2023-05-15", "externalids": {"DOI": "10.48550/arXiv.2305.08844"}, "doi_lower": "10.48550/arxiv.2305.08844"}
{"paper_id": 257205781, "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback", "author_names": ["Baolin Peng", "Michel Galley", "Pengcheng He", "Hao Cheng", "Yujia Xie", "Yu Hu", "Qiuyuan Huang", "Lars Lidén", "Zhou Yu", "Weizhu Chen", "Jianfeng Gao"], "venue": "arXiv.org", "abstract": "Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.", "year": 2023, "publicationdate": "2023-02-24", "externalids": {"DOI": "10.48550/arXiv.2302.12813"}, "doi_lower": "10.48550/arxiv.2302.12813"}
{"paper_id": 256615299, "title": "Languages are Rewards: Chain of Hindsight Finetuning using Human Feedback", "author_names": ["Hao Liu", "Carmelo Sferrazza", "P. Abbeel"], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 239009562, "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "author_names": ["Victor Sanh", "Albert Webson", "Colin Raffel", "Stephen H. Bach", "Lintang Sutawika", "Zaid Alyafeai", "Antoine Chaffin", "Arnaud Stiegler", "Teven Le Scao", "Arun Raja", "Manan Dey", "M Saiful Bari", "Canwen Xu", "Urmish Thakker", "S. Sharma", "Eliza Szczechla", "Taewoon Kim", "Gunjan Chhablani", "Nihal V. Nayak", "Debajyoti Datta", "Jonathan D. Chang", "Mike Tian-Jian Jiang", "Han Wang", "Matteo Manica", "Sheng Shen", "Zheng-Xin Yong", "Harshit Pandey", "Rachel Bawden", "Thomas Wang", "Trishala Neeraj", "Jos Rozen", "Abheesht Sharma", "Andrea Santilli", "Thibault Févry", "Jason Alan Fries", "R. Teehan", "Stella Biderman", "Leo Gao", "T. Bers", "Thomas Wolf", "Alexander M. Rush"], "venue": "arXiv.org", "abstract": "Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.", "year": 2021, "publicationdate": "2021-10-15", "externalids": {}, "doi_lower": null}
{"paper_id": 253018554, "title": "Scaling Instruction-Finetuned Language Models", "author_names": ["Hyung Won Chung", "Le Hou", "S. Longpre", "Barret Zoph", "Yi Tay", "W. Fedus", "Eric Li", "Xuezhi Wang", "Mostafa Dehghani", "Siddhartha Brahma", "Albert Webson", "S. Gu", "Zhuyun Dai", "Mirac Suzgun", "Xinyun Chen", "A. Chowdhery", "Dasha Valter", "Sharan Narang", "Gaurav Mishra", "Adams Wei Yu", "Vincent Zhao", "Yanping Huang", "Andrew M. Dai", "Hongkun Yu", "Slav Petrov", "Ed H. Chi", "J. Dean", "Jacob Devlin", "Adam Roberts", "Denny Zhou", "Quoc V. Le", "Jason Wei"], "venue": "Journal of machine learning research", "abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.", "year": 2022, "publicationdate": "2022-10-20", "externalids": {"DOI": "10.48550/arXiv.2210.11416"}, "doi_lower": "10.48550/arxiv.2210.11416"}
{"paper_id": 259936967, "title": "Communicative Agents for Software Development", "author_names": ["Chen Qian", "Wei Liu", "Xin Cong", "Cheng Yang", "Weize Chen", "Yusheng Su", "Juyuan Xu", "Zhiyuan Liu", "Maosong Sun"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2307.07924"}, "doi_lower": "10.48550/arxiv.2307.07924"}
{"paper_id": 258841118, "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate", "author_names": ["Yilun Du", "Shuang Li", "A. Torralba", "J. Tenenbaum", "Igor Mordatch"], "venue": "International Conference on Machine Learning", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14325"}, "doi_lower": "10.48550/arxiv.2305.14325"}
{"paper_id": 258967540, "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "author_names": ["Tian Liang", "Zhiwei He", "Wenxiang Jiao", "Xing Wang", "Yan Wang", "Rui Wang", "Yujiu Yang", "Zhaopeng Tu", "Shuming Shi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of “tit for tat” and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of “tit for tat” state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents.", "year": 2023, "publicationdate": "2023-05-30", "externalids": {"DOI": "10.48550/arXiv.2305.19118"}, "doi_lower": "10.48550/arxiv.2305.19118"}
{"paper_id": 11497579, "title": "Guarantees for Autonomy in Cognitive Agent Architecture", "author_names": ["C. Castelfranchi"], "venue": "ECAI Workshop on Agent Theories, Architectures, and Languages", "abstract": null, "year": 1995, "publicationdate": "1995-03-01", "externalids": {"DOI": "10.1007/3-540-58855-8_3"}, "doi_lower": "10.1007/3-540-58855-8_3"}
{"paper_id": 258551627, "title": "Chemists are teaching GPT-4 to experiment and control robots", "author_names": ["A. Wilkins"], "venue": "New Scientist", "abstract": null, "year": 2023, "publicationdate": "2023-05-01", "externalids": {"DOI": "10.1016/s0262-4079(23)00785-6"}, "doi_lower": "10.1016/s0262-4079(23)00785-6"}
{"paper_id": 211530803, "title": "From Python to Pythonic: Searching for Python idioms in GitHub", "author_names": ["José J. Merchante", "Gregorio Robles", "GSyCLibreSoft"], "venue": "", "abstract": null, "year": 2017, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 247585187, "title": "Wordcraft: Story Writing With Large Language Models", "author_names": ["Ann Yuan", "Andy Coenen", "Emily Reif", "Daphne Ippolito"], "venue": "International Conference on Intelligent User Interfaces", "abstract": "The latest generation of large neural language models such as GPT-3 have achieved new levels of performance on benchmarks for language understanding and generation. These models have even demonstrated an ability to perform arbitrary tasks without explicit training. In this work, we sought to learn how people might use such models in the process of creative writing. We built Wordcraft, a text editor in which users collaborate with a generative language model to write a story. We evaluated Wordcraft with a user study in which participants wrote short stories with and without the tool. Our results show that large language models enable novel co-writing experiences. For example, the language model is able to engage in open-ended conversation about the story, respond to writers’ custom requests expressed in natural language (such as ”rewrite this text to be more Dickensian”), and generate suggestions that serve to unblock writers in the creative process. Based on these results, we discuss design implications for future human-AI co-writing systems.", "year": 2022, "publicationdate": "2022-03-22", "externalids": {"DOI": "10.1145/3490099.3511105"}, "doi_lower": "10.1145/3490099.3511105"}
{"paper_id": 257913327, "title": "On the creativity of large language models", "author_names": ["Giorgio Franceschelli", "Mirco Musolesi"], "venue": "Ai & Society", "abstract": "Large language models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arises: can LLMs be really considered creative? In this article, we first analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. In particular, we focus our discussion on the dimensions of value, novelty, and surprise as proposed by Margaret Boden in her work. Then, we consider different classic perspectives, namely product, process, press, and person. We discuss a set of “easy” and “hard” problems in machine creativity, presenting them in relation to LLMs. Finally, we examine the societal impact of these technologies with a particular focus on the creative industries, analyzing the opportunities offered, the challenges arising from them, and the potential associated risks, from both legal and ethical points of view.", "year": 2023, "publicationdate": "2023-03-27", "externalids": {"DOI": "10.1007/s00146-024-02127-3"}, "doi_lower": "10.1007/s00146-024-02127-3"}
{"paper_id": 258291930, "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "author_names": ["Deyao Zhu", "Jun Chen", "Xiaoqian Shen", "Xiang Li", "Mohamed Elhoseiny"], "venue": "International Conference on Learning Representations", "abstract": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.", "year": 2023, "publicationdate": "2023-04-20", "externalids": {"DOI": "10.48550/arXiv.2304.10592"}, "doi_lower": "10.48550/arxiv.2304.10592"}
{"paper_id": 259243718, "title": "A survey on multimodal large language models", "author_names": ["Shukang Yin", "Chaoyou Fu", "Sirui Zhao", "Ke Li", "Xing Sun", "Tong Xu", "Enhong Chen"], "venue": "National Science Review", "abstract": "ABSTRACT Recently, the multimodal large language model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful large language models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of the MLLM, such as writing stories based on images and optical character recognition–free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even outperform GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First, we present the basic formulation of the MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages and scenarios. We continue with multimodal hallucination and extended techniques, including multimodal in-context learning, multimodal chain of thought and LLM-aided visual reasoning. To conclude the paper, we discuss existing challenges and point out promising research directions.", "year": 2023, "publicationdate": "2023-06-23", "externalids": {"DOI": "10.1093/nsr/nwae403"}, "doi_lower": "10.1093/nsr/nwae403"}
{"paper_id": 258865718, "title": "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought", "author_names": ["Yao Mu", "Qinglong Zhang", "Mengkang Hu", "Wen Wang", "Mingyu Ding", "Jun Jin", "Bin Wang", "Jifeng Dai", "Y. Qiao", "Ping Luo"], "venue": "Neural Information Processing Systems", "abstract": "Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments. In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the\"Chain of Thoughts\"mode for effective embodied planning. (ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated planning queries to form a closed loop between high-level planning and low-level control. Extensive experiments show the effectiveness of EmbodiedGPT on embodied tasks, including embodied planning, embodied control, visual captioning, and visual question answering. Notably, EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.15021"}, "doi_lower": "10.48550/arxiv.2305.15021"}
{"paper_id": 8408509, "title": "Conflict monitoring and cognitive control.", "author_names": ["M. Botvinick", "T. Braver", "D. Barch", "C. Carter", "J. Cohen"], "venue": "Psychology Review", "abstract": null, "year": 2001, "publicationdate": "2001-06-29", "externalids": {"DOI": "10.1037/0033-295X.108.3.624"}, "doi_lower": "10.1037/0033-295x.108.3.624"}
{"paper_id": 258947645, "title": "Think Before You Act: Decision Transformers with Internal Working Memory", "author_names": ["Jikun Kang", "R. Laroche", "Xingdi Yuan", "Adam Trischler", "Xuefei Liu", "Jie Fu"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.16338"}, "doi_lower": "10.48550/arxiv.2305.16338"}
{"paper_id": 256846992, "title": "On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark)", "author_names": ["Karthik Valmeekam", "S. Sreedharan", "Matthew Marquez", "Alberto Olmo Hernandez", "Subbarao Kambhampati"], "venue": "arXiv.org", "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) how good LLMs are by themselves in generating and validating simple plans in commonsense planning tasks (of the type that humans are generally quite good at) and (2) how good LLMs are in being a source of heuristic guidance for other agents--either AI planners or human planners--in their planning tasks. To investigate these questions in a systematic rather than anecdotal manner, we start by developing a benchmark suite based on the kinds of domains employed in the International Planning Competition. On this benchmark, we evaluate LLMs in three modes: autonomous, heuristic and human-in-the-loop. Our results show that LLM's ability to autonomously generate executable plans is quite meager, averaging only about 3% success rate. The heuristic and human-in-the-loop modes show slightly more promise. In addition to these results, we also make our benchmark and evaluation tools available to support investigations by research community.", "year": 2023, "publicationdate": "2023-02-13", "externalids": {"DOI": "10.48550/arXiv.2302.06706"}, "doi_lower": "10.48550/arxiv.2302.06706"}
{"paper_id": 258298051, "title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency", "author_names": ["B. Liu", "Yuqian Jiang", "Xiaohan Zhang", "Qian Liu", "Shiqi Zhang", "Joydeep Biswas", "Peter Stone"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems.\\footnote{The code and results are publicly available at https://github.com/Cranial-XIX/llm-pddl.git.", "year": 2023, "publicationdate": "2023-04-22", "externalids": {"DOI": "10.48550/arXiv.2304.11477"}, "doi_lower": "10.48550/arxiv.2304.11477"}
{"paper_id": 258841681, "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models", "author_names": ["Yen-Ting Lin", "Yun-Nung (Vivian) Chen"], "venue": "NLP4CONVAI", "abstract": "We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.13711"}, "doi_lower": "10.48550/arxiv.2305.13711"}
{"paper_id": 247996664, "title": "Inferring Rewards from Language in Context", "author_names": ["Jessy Lin", "Daniel Fried", "D. Klein", "A. Dragan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In classic instruction following, language like “I’d like the JetBlue flight” maps to actions (e.g., selecting that flight). However, language also conveys information about a user’s underlying reward function (e.g., a general preference for JetBlue), which can allow a model to carry out desirable actions in new contexts. We present a model that infers rewards from language pragmatically: reasoning about how speakers choose utterances not only to elicit desired actions, but also to reveal information about their preferences. On a new interactive flight–booking task with natural language, our model more accurately infers rewards and predicts optimal actions in unseen environments, in comparison to past work that first maps language to actions (instruction following) and then maps actions to rewards (inverse reinforcement learning).", "year": 2022, "publicationdate": "2022-04-05", "externalids": {"DOI": "10.48550/arXiv.2204.02515"}, "doi_lower": "10.48550/arxiv.2204.02515"}
{"paper_id": 259342833, "title": "Building Cooperative Embodied Agents Modularly with Large Language Models", "author_names": ["Hongxin Zhang", "Weihua Du", "Jiaming Shan", "Qinhong Zhou", "Yilun Du", "J. Tenenbaum", "Tianmin Shu", "Chuang Gan"], "venue": "International Conference on Learning Representations", "abstract": "In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.", "year": 2023, "publicationdate": "2023-07-05", "externalids": {"DOI": "10.48550/arXiv.2307.02485"}, "doi_lower": "10.48550/arxiv.2307.02485"}
{"paper_id": 218579954, "title": "On the Origin of Species On the Origin of Species", "author_names": ["C. Darwin", "A. Classics"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 256662612, "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity", "author_names": ["Yejin Bang", "Samuel Cahyawijaya", "Nayeon Lee", "Wenliang Dai", "Dan Su", "Bryan Wilie", "Holy Lovenia", "Ziwei Ji", "Tiezheng Yu", "Willy Chung", "Quyet V. Do", "Yan Xu", "Pascale Fung"], "venue": "International Joint Conference on Natural Language Processing", "abstract": "This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn\"prompt engineering\"fashion. We also release codebase for evaluation set extraction.", "year": 2023, "publicationdate": "2023-02-08", "externalids": {"DOI": "10.18653/v1/2023.ijcnlp-main.45"}, "doi_lower": "10.18653/v1/2023.ijcnlp-main.45"}
{"paper_id": 257039031, "title": "Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints", "author_names": ["Albert Lu", "Hongxin Zhang", "Yanzhe Zhang", "Xuezhi Wang", "Diyi Yang"], "venue": "Findings", "abstract": "The limits of open-ended generative models are unclear, yet increasingly important. What causes them to succeed and what causes them to fail? In this paper, we take a prompt-centric approach to analyzing and bounding the abilities of open-ended generative models. We present a generic methodology of analysis with two challenging prompt constraint types: structural and stylistic. These constraint types are categorized into a set of well-defined constraints that are analyzable by a single prompt. We then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model’s generative failures. We also show the generalizability of our proposed method on other large models like BLOOM and OPT. Our results and our in-context mitigation strategies reveal open challenges for future research.", "year": 2023, "publicationdate": "2023-02-17", "externalids": {"DOI": "10.48550/arXiv.2302.09185"}, "doi_lower": "10.48550/arxiv.2302.09185"}
{"paper_id": 237416618, "title": "Theory of Mind Based Assistive Communication in Complex Human Robot Cooperation", "author_names": ["Moritz C. Buehler", "J. Adamy", "Thomas H. Weisswange"], "venue": "arXiv.org", "abstract": "When cooperating with a human, a robot should not only care about its environment and task but also develop an understanding of the partner's reasoning. To support its human partner in complex tasks, the robot can share information that it knows. However simply communicating everything will annoy and distract humans since they might already be aware of and not all information is relevant in the current situation. The questions when and what type of information the human needs, are addressed through the concept of Theory of Mind based Communication which selects information sharing actions based on evaluation of relevance and an estimation of human beliefs. We integrate this into a communication assistant to support humans in a cooperative setting and evaluate performance benefits. We designed a human robot Sushi making task that is challenging for the human and generates different situations where humans are unaware and communication could be beneficial. We evaluate the influence of the human centric communication concept on performance with a user study. Compared to the condition without information exchange, assisted participants can recover from unawareness much earlier. The approach respects the costs of communication and balances interruptions better than other approaches. By providing information adapted to specific situations, the robot does not instruct but enable the human to make good decision.", "year": 2021, "publicationdate": "2021-09-03", "externalids": {}, "doi_lower": null}
{"paper_id": 258865502, "title": "Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models", "author_names": ["Natalie Shapira", "Mosh Levy", "S. Alavi", "Xuhui Zhou", "Yejin Choi", "Yoav Goldberg", "Maarten Sap", "Vered Shwartz"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "The escalating debate on AI’s capabilities warrants developing reliable metrics to assess machine “intelligence.” Recently, many anecdotal examples were used to suggest that newer Large Language Models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs’ N-ToM through an extensive evaluation of 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.14763"}, "doi_lower": "10.48550/arxiv.2305.14763"}
{"paper_id": 2937095, "title": "Learning Distributed Representations of Sentences from Unlabelled Data", "author_names": ["Felix Hill", "Kyunghyun Cho", "A. Korhonen"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.", "year": 2016, "publicationdate": "2016-02-10", "externalids": {"DOI": "10.18653/v1/N16-1162"}, "doi_lower": "10.18653/v1/n16-1162"}
{"paper_id": 351666, "title": "Natural Language Processing (Almost) from Scratch", "author_names": ["R. Collobert", "J. Weston", "L. Bottou", "Michael Karlen", "K. Kavukcuoglu", "Pavel P. Kuksa"], "venue": "Journal of machine learning research", "abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.", "year": 2011, "publicationdate": "2011-02-01", "externalids": {"DOI": "10.5555/1953048.2078186"}, "doi_lower": "10.5555/1953048.2078186"}
{"paper_id": 210861095, "title": "Scaling Laws for Neural Language Models", "author_names": ["J. Kaplan", "Sam McCandlish", "T. Henighan", "Tom B. Brown", "Benjamin Chess", "R. Child", "Scott Gray", "Alec Radford", "Jeff Wu", "Dario Amodei"], "venue": "arXiv.org", "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.", "year": 2020, "publicationdate": "2020-01-23", "externalids": {}, "doi_lower": null}
{"paper_id": 211205183, "title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?", "author_names": ["Adam Roberts", "Colin Raffel", "Noam Shazeer"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales surprisingly well with model size and outperforms models that explicitly look up knowledge on the open-domain variants of Natural Questions and WebQuestions. To facilitate reproducibility and future work, we release our code and trained models.", "year": 2020, "publicationdate": "2020-02-10", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.437"}, "doi_lower": "10.18653/v1/2020.emnlp-main.437"}
{"paper_id": 3523957, "title": "Commonsense Knowledge in Machine Intelligence", "author_names": ["Niket Tandon", "A. Varde", "Gerard de Melo"], "venue": "SGMD", "abstract": null, "year": 2018, "publicationdate": "2018-02-22", "externalids": {"DOI": "10.1145/3186549.3186562"}, "doi_lower": "10.1145/3186549.3186562"}
{"paper_id": 222290596, "title": "Probing Pretrained Language Models for Lexical Semantics", "author_names": ["Ivan Vulic", "E. Ponti", "Robert Litschko", "Goran Glavas", "A. Korhonen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks? 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.", "year": 2020, "publicationdate": "2020-10-12", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.586"}, "doi_lower": "10.18653/v1/2020.emnlp-main.586"}
{"paper_id": 106402715, "title": "A Structural Probe for Finding Syntax in Word Representations", "author_names": ["John Hewitt", "Christopher D. Manning"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network’s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models’ vector geometry.", "year": 2019, "publicationdate": "2019-06-01", "externalids": {"DOI": "10.18653/v1/N19-1419"}, "doi_lower": "10.18653/v1/n19-1419"}
{"paper_id": 22321032, "title": "Information extraction and text summarization using linguistic knowledge acquisition", "author_names": ["L. Rau", "P. Jacobs", "U. Zernik"], "venue": "Information Processing & Management", "abstract": null, "year": 1989, "publicationdate": "1989-06-01", "externalids": {"DOI": "10.1016/0306-4573(89)90069-1"}, "doi_lower": "10.1016/0306-4573(89)90069-1"}
{"paper_id": 38555458, "title": "Improved Automatic Keyword Extraction Given More Semantic Knowledge", "author_names": ["Kai Yang", "Zhenhong Chen", "Y. Cai", "Dongping Huang", "Ho-fung Leung"], "venue": "DASFAA Workshops", "abstract": null, "year": 2016, "publicationdate": "2016-04-16", "externalids": {"DOI": "10.1007/978-3-319-32055-7_10"}, "doi_lower": "10.1007/978-3-319-32055-7_10"}
{"paper_id": 244119704, "title": "Probing Pre-trained Language Models for Semantic Attributes and their Values", "author_names": ["Meriem Beloucif", "Chris Biemann"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Pretrained Language Models (PTLMs) yield state-of-the-art performance on many Natural Language Processing tasks, including syntax, semantics and commonsense reasoning. In this paper, we focus on identifying to what extent do PTLMs capture semantic attributes and their values, e.g. the relation between rich and high net worth. We use PTLMs to predict masked tokens using patterns and lists of items from Wikidata in order to verify how likely PTLMs encode semantic attributes along with their values. Such inferences based on semantics are intuitive for us humans as part of our language understanding. Since PTLMs are trained on large amount of Wikipedia data, we would assume that they can generate similar predictions. However, our ﬁndings reveal that PTLMs perform still much worse than humans on this task. We show an analysis which explains how to exploit our methodology to integrate better context and semantics into PTLMs using knowledge bases.", "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.218"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.218"}
{"paper_id": 232110776, "title": "Advances in Multi-turn Dialogue Comprehension: A Survey", "author_names": ["Zhuosheng Zhang", "Hai Zhao"], "venue": "arXiv.org", "abstract": "Training machines to understand natural language and interact with humans is an elusive and essential task of artificial intelligence. A diversity of dialogue systems has been designed with the rapid development of deep learning techniques, especially the recent pre-trained language models (PrLMs). Among these studies, the fundamental yet challenging type of task is dialogue comprehension whose role is to teach the machines to read and comprehend the dialogue context before responding. In this paper, we review the previous methods from the technical perspective of dialogue modeling for the dialogue comprehension task. We summarize the characteristics and challenges of dialogue comprehension in contrast to plain-text reading comprehension. Then, we discuss three typical patterns of dialogue modeling. In addition, we categorize dialogue-related pre-training techniques which are employed to enhance PrLMs in dialogue scenarios. Finally, we highlight the technical advances in recent years and point out the lessons from the empirical analysis and the prospects towards a new frontier of researches.", "year": 2021, "publicationdate": "2021-03-04", "externalids": {}, "doi_lower": null}
{"paper_id": 233219920, "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "author_names": ["Tara Safavi", "Danai Koutra"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Relational knowledge bases (KBs) are commonly used to represent world knowledge in machines. However, while advantageous for their high degree of precision and interpretability, KBs are usually organized according to manually-defined schemas, which limit their expressiveness and require significant human efforts to engineer and maintain. In this review, we take a natural language processing perspective to these limitations, examining how they may be addressed in part by training deep contextual language models (LMs) to internalize and express relational knowledge in more flexible forms. We propose to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision. Our contributions are threefold: (1) We provide a high-level, extensible taxonomy for knowledge representation in LMs; (2) Within our taxonomy, we highlight notable models, evaluation tasks, and findings, in order to provide an up-to-date review of current knowledge representation capabilities in LMs; and (3) We suggest future research directions that build upon the complementary aspects of LMs and KBs as knowledge representations.", "year": 2021, "publicationdate": "2021-04-12", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.81"}, "doi_lower": "10.18653/v1/2021.emnlp-main.81"}
{"paper_id": 208513249, "title": "How Can We Know What Language Models Know?", "author_names": ["Zhengbao Jiang", "Frank F. Xu", "J. Araki", "Graham Neubig"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Abstract Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as “Obama is a __ by profession”. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as “Obama worked as a __ ” may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.", "year": 2019, "publicationdate": "2019-11-28", "externalids": {"DOI": "10.1162/tacl_a_00324"}, "doi_lower": "10.1162/tacl_a_00324"}
{"paper_id": 252873120, "title": "Language Models of Code are Few-Shot Commonsense Learners", "author_names": ["Aman Madaan", "Shuyan Zhou", "Uri Alon", "Yiming Yang", "Graham Neubig"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph.To employ large language models (LMs) for this task, existing approaches ‘serialize’ the output graph as a flat list of nodes and edges.Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all.We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (codex) outperforms natural-LMs that are fine-tuned on the target task (T5) and other strong LMs such as GPT-3 in the few-shot setting.", "year": 2022, "publicationdate": "2022-10-13", "externalids": {"DOI": "10.48550/arXiv.2210.07128"}, "doi_lower": "10.48550/arxiv.2210.07128"}
{"paper_id": 247158549, "title": "A systematic evaluation of large language models of code", "author_names": ["Frank F. Xu", "Uri Alon", "Graham Neubig", "Vincent J. Hellendoorn"], "venue": "MAPS@PLDI", "abstract": "Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area. We have an online appendix at https://arxiv.org/abs/2202.13169.", "year": 2022, "publicationdate": "2022-02-26", "externalids": {"DOI": "10.1145/3520312.3534862"}, "doi_lower": "10.1145/3520312.3534862"}
{"paper_id": 278730323, "title": "Large Language Models in Medicine", "author_names": ["Jinwook Choi"], "venue": "Healthcare Informatics Research", "abstract": null, "year": 2025, "publicationdate": "2025-04-01", "externalids": {"DOI": "10.4258/hir.2025.31.2.111"}, "doi_lower": "10.4258/hir.2025.31.2.111"}
{"paper_id": 253734939, "title": "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation", "author_names": ["Yuhang Lai", "Chengxi Li", "Yiming Wang", "Tianyi Zhang", "Ruiqi Zhong", "Luke Zettlemoyer", "S. Yih", "Daniel Fried", "Si-yi Wang", "Tao Yu"], "venue": "International Conference on Machine Learning", "abstract": "We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.", "year": 2022, "publicationdate": "2022-11-18", "externalids": {"DOI": "10.48550/arXiv.2211.11501"}, "doi_lower": "10.48550/arxiv.2211.11501"}
{"paper_id": 22910766, "title": "Measuring Catastrophic Forgetting in Neural Networks", "author_names": ["Ronald Kemker", "Angelina Abitino", "Marc McClure", "Christopher Kanan"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem is not yet solved.", "year": 2017, "publicationdate": "2017-08-07", "externalids": {"DOI": "10.1609/aaai.v32i1.11651"}, "doi_lower": "10.1609/aaai.v32i1.11651"}
{"paper_id": 233289412, "title": "Editing Factual Knowledge in Language Models", "author_names": ["Nicola De Cao", "Wilker Aziz", "Ivan Titov"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix ‘bugs’ or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor’s efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a ‘probe’ revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor", "year": 2021, "publicationdate": "2021-04-16", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.522"}, "doi_lower": "10.18653/v1/2021.emnlp-main.522"}
{"paper_id": 249642147, "title": "Memory-Based Model Editing at Scale", "author_names": ["E. Mitchell", "Charles Lin", "Antoine Bosselut", "Christopher D. Manning", "Chelsea Finn"], "venue": "International Conference on Machine Learning", "abstract": "Even the largest neural networks make errors, and once-correct predictions can become invalid as the world changes. Model editors make local updates to the behavior of base (pre-trained) models to inject updated knowledge or correct undesirable behaviors. Existing model editors have shown promise, but also suffer from insufficient expressiveness: they struggle to accurately model an edit's intended scope (examples affected by the edit), leading to inaccurate predictions for test inputs loosely related to the edit, and they often fail altogether after many edits. As a higher-capacity alternative, we propose Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC), which stores edits in an explicit memory and learns to reason over them to modulate the base model's predictions as needed. To enable more rigorous evaluation of model editors, we introduce three challenging language model editing problems based on question answering, fact-checking, and dialogue generation. We find that only SERAC achieves high performance on all three problems, consistently outperforming existing approaches to model editing by a significant margin. Code, data, and additional project information will be made available at https://sites.google.com/view/serac-editing.", "year": 2022, "publicationdate": "2022-06-13", "externalids": {}, "doi_lower": null}
{"paper_id": 257557820, "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models", "author_names": ["Potsawee Manakul", "Adian Liusie", "M. Gales"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.", "year": 2023, "publicationdate": "2023-03-16", "externalids": {"DOI": "10.48550/arXiv.2303.08896"}, "doi_lower": "10.48550/arxiv.2303.08896"}
{"paper_id": 258823123, "title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing", "author_names": ["Zhibin Gou", "Zhihong Shao", "Yeyun Gong", "Yelong Shen", "Yujiu Yang", "Nan Duan", "Weizhu Chen"], "venue": "International Conference on Learning Representations", "abstract": "Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially\"black boxes\"to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.", "year": 2023, "publicationdate": "2023-05-19", "externalids": {"DOI": "10.48550/arXiv.2305.11738"}, "doi_lower": "10.48550/arxiv.2305.11738"}
{"paper_id": 204960716, "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "author_names": ["M. Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdel-rahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.", "year": 2019, "publicationdate": "2019-10-29", "externalids": {"DOI": "10.18653/v1/2020.acl-main.703"}, "doi_lower": "10.18653/v1/2020.acl-main.703"}
{"paper_id": 247597009, "title": "Efficient Classification of Long Documents Using Transformers", "author_names": ["Hyunji Hayley Park", "Yogarshi Vyas", "Kashif Shah"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Several methods have been proposed for classifying long textual documents using Transformers. However, there is a lack of consensus on a benchmark to enable a fair comparison among different approaches. In this paper, we provide a comprehensive evaluation of the relative efficacy measured against various baselines and diverse datasets — both in terms of accuracy as well as time and space overheads. Our datasets cover binary, multi-class, and multi-label classification tasks and represent various ways information is organized in a long text (e.g. information that is critical to making the classification decision is at the beginning or towards the end of the document). Our results show that more complex models often fail to outperform simple baselines and yield inconsistent performance across datasets. These findings emphasize the need for future studies to consider comprehensive baselines and datasets that better represent the task of long document classification to develop robust models.", "year": 2022, "publicationdate": "2022-03-21", "externalids": {"DOI": "10.48550/arXiv.2203.11258"}, "doi_lower": "10.48550/arxiv.2203.11258"}
{"paper_id": 245144820, "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences", "author_names": ["Mandy Guo", "J. Ainslie", "David C. Uthus", "Santiago Ontañón", "Jianmo Ni", "Yun-Hsuan Sung", "Yinfei Yang"], "venue": "NAACL-HLT", "abstract": "Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.", "year": 2021, "publicationdate": "2021-12-15", "externalids": {"DOI": "10.18653/v1/2022.findings-naacl.55"}, "doi_lower": "10.18653/v1/2022.findings-naacl.55"}
{"paper_id": 257622671, "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation", "author_names": ["J. Ainslie", "Tao Lei", "Michiel de Jong", "Santiago Ontan'on", "Siddhartha Brahma", "Yury Zemlyanskiy", "David C. Uthus", "Mandy Guo", "J. Lee-Thorp", "Yi Tay", "Yun-Hsuan Sung", "Sumit K. Sanghai"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.", "year": 2023, "publicationdate": "2023-03-17", "externalids": {"DOI": "10.48550/arXiv.2303.09752"}, "doi_lower": "10.48550/arxiv.2303.09752"}
{"paper_id": 258947457, "title": "Randomized Positional Encodings Boost Length Generalization of Transformers", "author_names": ["Anian Ruoss", "Gr'egoire Del'etang", "Tim Genewein", "Jordi Grau-Moya", "R. Csordás", "Mehdi Abbana Bennani", "S. Legg", "J. Veness"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem. Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence’s length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0% on average).", "year": 2023, "publicationdate": "2023-05-26", "externalids": {"DOI": "10.48550/arXiv.2305.16843"}, "doi_lower": "10.48550/arxiv.2305.16843"}
{"paper_id": 268241203, "title": "Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System", "author_names": ["Xinnian Liang", "Bing Wang", "Hui Huang", "Shuangzhi Wu", "Peihao Wu", "Lu Lu", "Zejun Ma", "Zhoujun Li"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2304.13343"}, "doi_lower": "10.48550/arxiv.2304.13343"}
{"paper_id": 257636839, "title": "Reflexion: an autonomous agent with dynamic memory and self-reflection", "author_names": ["Noah Shinn", "Beck Labash", "A. Gopinath"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 258741194, "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory", "author_names": ["Wanjun Zhong", "Lianghong Guo", "Qi-Fei Gao", "He Ye", "Yanlin Wang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Large Language Models (LLMs) have drastically reshaped our interactions with artificial intelligence (AI) systems, showcasing impressive performance across an extensive array of tasks. Despite this, a notable hindrance remains—the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems, psychological counseling, and secretarial assistance. Recognizing the necessity for long-term memory, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user's personality over time by synthesizing information from previous interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory. This mechanism permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a more human-like memory mechanism and enriched user experience. MemoryBank is versatile in accommodating both closed-source models like ChatGPT and open-source models such as ChatGLM. To validate MemoryBank's effectiveness, we exemplify its application through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned with psychological dialog data, SiliconFriend displays heightened empathy and discernment in its interactions. Experiment involves both qualitative analysis with real-world user dialogs and quantitative analysis with simulated dialogs. In the latter, ChatGPT acts as multiple users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics. The results of our analysis reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong capability for long-term companionship as it can provide emphatic response, recall relevant memories and understand user personality.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {"DOI": "10.48550/arXiv.2305.10250"}, "doi_lower": "10.48550/arxiv.2305.10250"}
{"paper_id": 260887105, "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate", "author_names": ["Chi-Min Chan", "Weize Chen", "Yusheng Su", "Jianxuan Yu", "Wei Xue", "Shan Zhang", "Jie Fu", "Zhiyuan Liu"], "venue": "arXiv.org", "abstract": "Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.", "year": 2023, "publicationdate": "2023-08-14", "externalids": {"DOI": "10.48550/arXiv.2308.07201"}, "doi_lower": "10.48550/arxiv.2308.07201"}
{"paper_id": 258959262, "title": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory", "author_names": ["Xizhou Zhu", "Yuntao Chen", "Hao Tian", "Chenxin Tao", "Weijie Su", "Chenyu Yang", "Gao Huang", "Bin Li", "Lewei Lu", "Xiaogang Wang", "Y. Qiao", "Zhaoxiang Zhang", "Jifeng Dai"], "venue": "arXiv.org", "abstract": "The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular\"ObtainDiamond\"task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the\"ObtainDiamond\"task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based interactions. We develop a set of structured actions and leverage LLMs to generate action plans for the agents to execute. The resulting LLM-based agent markedly surpasses previous methods, achieving a remarkable improvement of +47.5% in success rate on the\"ObtainDiamond\"task, demonstrating superior robustness compared to traditional RL-based controllers. Notably, our agent is the first to procure all items in the Minecraft Overworld technology tree, demonstrating its extensive capabilities. GITM does not need any GPU for training, but a single CPU node with 32 CPU cores is enough. This research shows the potential of LLMs in developing capable agents for handling long-horizon, complex tasks and adapting to uncertainties in open-world environments. See the project website at https://github.com/OpenGVLab/GITM.", "year": 2023, "publicationdate": "2023-05-25", "externalids": {"DOI": "10.48550/arXiv.2305.17144"}, "doi_lower": "10.48550/arxiv.2305.17144"}
{"paper_id": 258841042, "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models", "author_names": ["Ali Modarressi", "Ayyoob Imani", "Mohsen Fayyaz", "Hinrich Schütze"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP) through their extensive parameters and comprehensive data utilization. However, existing LLMs lack a dedicated memory unit, limiting their ability to explicitly store and retrieve knowledge for various tasks. In this paper, we propose RET-LLM a novel framework that equips LLMs with a general write-read memory unit, allowing them to extract, store, and recall knowledge from the text as needed for task performance. Inspired by Davidsonian semantics theory, we extract and save knowledge in the form of triplets. The memory unit is designed to be scalable, aggregatable, updatable, and interpretable. Through qualitative evaluations, we demonstrate the superiority of our proposed framework over baseline approaches in question answering tasks. Moreover, our framework exhibits robust performance in handling temporal-based question answering tasks, showcasing its ability to effectively manage time-dependent information.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14322"}, "doi_lower": "10.48550/arxiv.2305.14322"}
{"paper_id": 260704607, "title": "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation", "author_names": ["Jiaju Lin", "Haoran Zhao", "Aochi Zhang", "Yiting Wu", "Huqiuyue Ping", "Qin Chen"], "venue": "arXiv.org", "abstract": "With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https://agentsims.com .", "year": 2023, "publicationdate": "2023-08-08", "externalids": {"DOI": "10.48550/arXiv.2308.04026"}, "doi_lower": "10.48550/arxiv.2308.04026"}
{"paper_id": 260438773, "title": "Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents", "author_names": ["Ziheng Huang", "S. Gutierrez", "Hemanth Kamana", "Stephen MacNeil"], "venue": "ACM Symposium on User Interface Software and Technology", "abstract": "The recent advent of large language models (LLM) has resulted in high-performing conversational agents such as ChatGPT. These agents must remember key information from an ongoing conversation to provide responses that are contextually relevant to the user. However, these agents have limited memory and can be distracted by irrelevant parts of the conversation. While many strategies exist to manage conversational memory, users currently lack affordances for viewing and controlling what the agent remembers, resulting in a poor mental model and conversational breakdowns. In this paper, we present Memory Sandbox, an interactive system and design probe that allows users to manage the conversational memory of LLM-powered agents. By treating memories as data objects that can be viewed, manipulated, recorded, summarized, and shared across conversations, Memory Sandbox provides interaction affordances for users to manage how the agent should ‘see’ the conversation.", "year": 2023, "publicationdate": "2023-08-03", "externalids": {"DOI": "10.1145/3586182.3615796"}, "doi_lower": "10.1145/3586182.3615796"}
{"paper_id": 248887351, "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning", "author_names": ["Antonia Creswell", "M. Shanahan", "I. Higgins"], "venue": "International Conference on Learning Representations", "abstract": "Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.", "year": 2022, "publicationdate": "2022-05-19", "externalids": {}, "doi_lower": null}
{"paper_id": 257900871, "title": "Self-Refine: Iterative Refinement with Self-Feedback", "author_names": ["Aman Madaan", "Niket Tandon", "Prakhar Gupta", "Skyler Hallinan", "Luyu Gao", "Sarah Wiegreffe", "Uri Alon", "Nouha Dziri", "Shrimai Prabhumoye", "Yiming Yang", "S. Welleck", "Bodhisattwa Prasad Majumder", "Shashank Gupta", "A. Yazdanbakhsh", "Peter Clark"], "venue": "Neural Information Processing Systems", "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {"DOI": "10.48550/arXiv.2303.17651"}, "doi_lower": "10.48550/arxiv.2303.17651"}
{"paper_id": 257833781, "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face", "author_names": ["Yongliang Shen", "Kaitao Song", "Xu Tan", "Dongsheng Li", "Weiming Lu", "Y. Zhuang"], "venue": "Neural Information Processing Systems", "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2303.17580"}, "doi_lower": "10.48550/arxiv.2303.17580"}
{"paper_id": 258762525, "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "author_names": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "T. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "venue": "Neural Information Processing Systems", "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {"DOI": "10.48550/arXiv.2305.10601"}, "doi_lower": "10.48550/arxiv.2305.10601"}
{"paper_id": 256598146, "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "author_names": ["Zihao Wang", "Shaofei Cai", "Anji Liu", "Xiaojian Ma", "Yitao Liang"], "venue": "arXiv.org", "abstract": "We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and $\\underline{S}$elect\"($\\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of the plan execution process and providing self-$\\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.", "year": 2023, "publicationdate": "2023-02-03", "externalids": {"DOI": "10.48550/arXiv.2302.01560"}, "doi_lower": "10.48550/arxiv.2302.01560"}
{"paper_id": 258865812, "title": "Reasoning with Language Model is Planning with World Model", "author_names": ["Shibo Hao", "Yi Gu", "Haodi Ma", "Joshua Jiahua Hong", "Zhen Wang", "D. Wang", "Zhiting Hu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.14992"}, "doi_lower": "10.48550/arxiv.2305.14992"}
{"paper_id": 258960143, "title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks", "author_names": ["Bill Yuchen Lin", "Yicheng Fu", "Karina Yang", "Prithviraj Ammanabrolu", "Faeze Brahman", "Shiyu Huang", "Chandra Bhagavatula", "Yejin Choi", "Xiang Ren"], "venue": "Neural Information Processing Systems", "abstract": "We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks.", "year": 2023, "publicationdate": "2023-05-27", "externalids": {"DOI": "10.48550/arXiv.2305.17390"}, "doi_lower": "10.48550/arxiv.2305.17390"}
{"paper_id": 250451569, "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models", "author_names": ["Wenlong Huang", "F. Xia", "Ted Xiao", "Harris Chan", "Jacky Liang", "Peter R. Florence", "Andy Zeng", "Jonathan Tompson", "Igor Mordatch", "Yevgen Chebotar", "P. Sermanet", "Noah Brown", "Tomas Jackson", "Linda Luu", "S. Levine", "Karol Hausman", "Brian Ichter"], "venue": "Conference on Robot Learning", "abstract": "Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.", "year": 2022, "publicationdate": "2022-07-12", "externalids": {"DOI": "10.48550/arXiv.2207.05608"}, "doi_lower": "10.48550/arxiv.2207.05608"}
{"paper_id": 258841374, "title": "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models", "author_names": ["Z. Chen", "Kun Zhou", "Beichen Zhang", "Zheng Gong", "Wayne Xin Zhao", "Ji-rong Wen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose \\textbf{ChatCoT}, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs. In ChatCoT, we model the chain-of-thought~(CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting. At each turn, LLMs can either interact with tools or perform the reasoning. Our approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. Specially, we initialize the early turns of the conversation by the tools, tasks and reasoning format, and propose an iterative \\emph{tool-augmented reasoning} step to perform step-by-step tool-augmented reasoning. The experiment results on two complex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of ChatCoT on complex reasoning tasks, achieving a 6.8\\% relative improvement over the state-of-the-art baseline. Our code and data are available at: \\url{https://github.com/RUCAIBOX/ChatCoT}.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14323"}, "doi_lower": "10.48550/arxiv.2305.14323"}
{"paper_id": 238353829, "title": "AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts", "author_names": ["Tongshuang Sherry Wu", "Michael Terry", "Carrie J. Cai"], "venue": "International Conference on Human Factors in Computing Systems", "abstract": "Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by “unit-testing” sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications.", "year": 2021, "publicationdate": "2021-10-04", "externalids": {"DOI": "10.1145/3491102.3517582"}, "doi_lower": "10.1145/3491102.3517582"}
{"paper_id": 257532768, "title": "Chat with the Environment: Interactive Multimodal Perception Using Large Language Models", "author_names": ["Xufeng Zhao", "Mengdi Li", "C. Weber", "Muhammad Burhan Hafez", "Stefan Wermter"], "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems", "abstract": "Programming robot behavior in a complex world faces challenges on multiple levels, from dextrous low-level skills to high-level planning and reasoning. Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in few-shot robotic planning. However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold. We develop a robot interaction scenario with a partially observable state, which necessitates a robot to decide on a range of epistemic actions in order to sample sensory information among multiple modalities, before being able to execute the task correctly. An interactive perception framework is therefore proposed with an LLM as its backbone, whose ability is exploited to instruct epistemic actions and to reason over the resulting multimodal sensations (vision, sound, haptics, proprioception), as well as to plan an entire task execution based on the interactively acquired information. Our study demonstrates that LLMs can provide high-level planning and reasoning skills and control interactive robot behavior in a multimodal environment, while multimodal modules with the context of the environmental state help ground the LLMs and extend their processing ability. The project website can be found at https://matcha-model.github.io/.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {"DOI": "10.1109/IROS55552.2023.10342363"}, "doi_lower": "10.1109/iros55552.2023.10342363"}
{"paper_id": 260350986, "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning", "author_names": ["Ning Miao", "Y. Teh", "Tom Rainforth"], "venue": "International Conference on Learning Representations", "abstract": "The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on three datasets (GSM8K, MathQA, and MATH) and find that it successfully recognizes errors and, in turn, increases final answer accuracies.", "year": 2023, "publicationdate": "2023-08-01", "externalids": {"DOI": "10.48550/arXiv.2308.00436"}, "doi_lower": "10.48550/arxiv.2308.00436"}
{"paper_id": 255440307, "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers", "author_names": ["Chengyi Wang", "Sanyuan Chen", "Yu Wu", "Zi-Hua Zhang", "Long Zhou", "Shujie Liu", "Zhuo Chen", "Yanqing Liu", "Huaming Wang", "Jinyu Li", "Lei He", "Sheng Zhao", "Furu Wei"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 50 k hours of English speech which is hundreds of times larger than existing systems. VALL-E emerges in-context learning capability and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. Experiment results show that VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find VALL-E could preserve the speaker's emotion and acoustic environment from the prompt in synthesis.", "year": 2023, "publicationdate": "2023-01-05", "externalids": {"DOI": "10.1109/TASLPRO.2025.3530270"}, "doi_lower": "10.1109/taslpro.2025.3530270"}
{"paper_id": 263886074, "title": "A Survey for In-context Learning", "author_names": ["Qingxiu Dong", "Lei Li", "Damai Dai", "Ce Zheng", "Zhiyong Wu", "Baobao Chang", "Xu Sun", "Jingjing Xu", "Lei Li", "Zhifang Sui"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 256459333, "title": "A Comprehensive Survey of Continual Learning: Theory, Method and Application", "author_names": ["Liyuan Wang", "Xingxing Zhang", "Hang Su", "Jun Zhu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance drop of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.", "year": 2023, "publicationdate": "2023-01-31", "externalids": {"DOI": "10.1109/TPAMI.2024.3367329"}, "doi_lower": "10.1109/tpami.2024.3367329"}
{"paper_id": 256390383, "title": "Progressive Prompts: Continual Learning for Language Models", "author_names": ["Anastasia Razdaibiedina", "Yuning Mao", "Rui Hou", "Madian Khabsa", "M. Lewis", "Amjad Almahairi"], "venue": "International Conference on Learning Representations", "abstract": "We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement>20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.", "year": 2023, "publicationdate": "2023-01-29", "externalids": {"DOI": "10.48550/arXiv.2301.12314"}, "doi_lower": "10.48550/arxiv.2301.12314"}
{"paper_id": 142881141, "title": "Discoveries in the Human Brain: Neuroscience Prehistory, Brain Structure, and Function", "author_names": ["L. Marshall", "H. Magoun"], "venue": "", "abstract": null, "year": 1997, "publicationdate": "1997-12-15", "externalids": {}, "doi_lower": null}
{"paper_id": 269173541, "title": "What is Language? Some Preliminary Remarks*", "author_names": [], "venue": "", "abstract": null, "year": 2009, "publicationdate": null, "externalids": {"DOI": "10.1017/CBO9780511619489.002"}, "doi_lower": "10.1017/cbo9780511619489.002"}
{"paper_id": 257219404, "title": "LLaMA: Open and Efficient Foundation Language Models", "author_names": ["Hugo Touvron", "Thibaut Lavril", "Gautier Izacard", "Xavier Martinet", "M. Lachaux", "Timothée Lacroix", "Baptiste Rozière", "Naman Goyal", "Eric Hambro", "Faisal Azhar", "Aur'elien Rodriguez", "Armand Joulin", "Edouard Grave", "Guillaume Lample"], "venue": "arXiv.org", "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.", "year": 2023, "publicationdate": "2023-02-27", "externalids": {}, "doi_lower": null}
{"paper_id": 264405644, "title": "H2O Open Ecosystem for State-of-the-art Large Language Models", "author_names": ["Arno Candel", "Jon McKinney", "Philipp Singer", "Pascal Pfeiffer", "Maximilian Jeblick", "Chun Ming Lee", "Marcos V. Conde"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs of diverse sizes. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are fully open-source. We believe this work helps to boost AI development and make it more accessible, efficient and trustworthy. The demo is available at: https://gpt.h2o.ai/", "year": 2023, "publicationdate": "2023-10-17", "externalids": {"DOI": "10.48550/arXiv.2310.13012"}, "doi_lower": "10.48550/arxiv.2310.13012"}
{"paper_id": 1857011, "title": "Generative Deep Neural Networks for Dialogue: A Short Review", "author_names": ["Iulian Serban", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau"], "venue": "arXiv.org", "abstract": "Researchers have recently started investigating deep neural networks for dialogue applications. In particular, generative sequence-to-sequence (Seq2Seq) models have shown promising results for unstructured tasks, such as word-level dialogue response generation. The hope is that such models will be able to leverage massive amounts of data to learn meaningful natural language representations and response generation strategies, while requiring a minimum amount of domain knowledge and hand-crafting. An important challenge is to develop models that can effectively incorporate dialogue context and generate meaningful and diverse responses. In support of this goal, we review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure.", "year": 2016, "publicationdate": "2016-11-18", "externalids": {}, "doi_lower": null}
{"paper_id": 12300158, "title": "A Neural Conversational Model", "author_names": ["O. Vinyals", "Quoc V. Le"], "venue": "arXiv.org", "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.", "year": 2015, "publicationdate": "2015-06-19", "externalids": {}, "doi_lower": null}
{"paper_id": 210920238, "title": "Towards a Human-like Open-Domain Chatbot", "author_names": ["Daniel De Freitas", "Minh-Thang Luong", "David R. So", "Jamie Hall", "Noah Fiedel", "R. Thoppilan", "Zi Yang", "Apoorv Kulshreshtha", "Gaurav Nemade", "Yifeng Lu", "Quoc V. Le"], "venue": "arXiv.org", "abstract": "We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.", "year": 2020, "publicationdate": "2020-01-27", "externalids": {}, "doi_lower": null}
{"paper_id": 258947227, "title": "Mindstorms in Natural Language-Based Societies of Mind", "author_names": ["Mingchen Zhuge", "Haozhe Liu", "Francesco Faccio", "Dylan R. Ashley", "R'obert Csord'as", "Anand Gopalakrishnan", "Abdullah Hamdi", "Hasan Hammoud", "Vincent Herrmann", "Kazuki Irie", "Louis Kirsch", "Bing-chuan Li", "G. Li", "Shuming Liu", "Jinjie Mai", "Piotr Pikekos", "A. Ramesh", "Imanol Schlag", "Weimin Shi", "Aleksandar Stani'c", "Wenyi Wang", "Yu‐Han Wang", "Mengmeng Xu", "Deng-Ping Fan", "Bernard Ghanem", "J. Schmidhuber"], "venue": "Computational Visual Media", "abstract": null, "year": 2025, "publicationdate": "2025-02-01", "externalids": {"DOI": "10.26599/cvm.2025.9450460"}, "doi_lower": "10.26599/cvm.2025.9450460"}
{"paper_id": 216562425, "title": "Recipes for Building an Open-Domain Chatbot", "author_names": ["Stephen Roller", "Emily Dinan", "Naman Goyal", "Da Ju", "Mary Williamson", "Yinhan Liu", "Jing Xu", "Myle Ott", "Kurt Shuster", "Eric Michael Smith", "Y-Lan Boureau", "J. Weston"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.", "year": 2020, "publicationdate": "2020-04-28", "externalids": {"DOI": "10.18653/v1/2021.eacl-main.24"}, "doi_lower": "10.18653/v1/2021.eacl-main.24"}
{"paper_id": 267338108, "title": "Olive: An Instruction Following LLaMA Model For Odia Language", "author_names": ["Shantipriya Parida", "Sambit Sekhar", "Subhadarshi Panda", "Swateek Jena", "Abhijeet Parida", "Soumendra Kumar Sahoo", "S. Dash"], "venue": "2023 IEEE Silchar Subsection Conference (SILCON)", "abstract": "The AI community is experiencing a profound impact from Large Language Models (LLMs), and the introduction of ChatGPT and GPT-4 is prompting a reconsideration of the potential of artificial general intelligence(AGI). However, most of the LLMs are trained in English and other high-resource languages, resulting in the unavailability of LLM and its related technologies and services for many low-resource languages. In India, where only 10% of the population is proficient in English, the need for LLM models adapted to regional languages becomes crucial.In this paper, we emphasized the need for LLM for the low-resource Odia language by evaluating the available LLM-supporting Odia language. We describe the development process of the instruction-tuning LLM model for Odia. The developed instruction tuning Odia LLM is available freely for research and non-commercial purposes.", "year": 2023, "publicationdate": "2023-11-03", "externalids": {"DOI": "10.1109/SILCON59133.2023.10404195"}, "doi_lower": "10.1109/silcon59133.2023.10404195"}
{"paper_id": 258049306, "title": "OpenAGI: When LLM Meets Domain Experts", "author_names": ["Yingqiang Ge", "Wenyue Hua", "Kai Mei", "Jianchao Ji", "Juntao Tan", "Shuyuan Xu", "Zelong Li", "Yongfeng Zhang"], "venue": "Neural Information Processing Systems", "abstract": "Human intelligence excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive intelligent models, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research platform designed for multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.", "year": 2023, "publicationdate": "2023-04-10", "externalids": {"DOI": "10.48550/arXiv.2304.04370"}, "doi_lower": "10.48550/arxiv.2304.04370"}
{"paper_id": 257663467, "title": "MEGA: Multilingual Evaluation of Generative AI", "author_names": ["Kabir Ahuja", "Rishav Hada", "Millicent Ochieng", "Prachi Jain", "Harshita Diddee", "Krithika Ramesh", "Samuel C. Maina", "Tanuja Ganu", "Sameer Segal", "Maxamed Axmed", "Kalika Bali", "Sunayana Sitaram"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.", "year": 2023, "publicationdate": "2023-03-22", "externalids": {"DOI": "10.18653/v1/2023.emnlp-main.258"}, "doi_lower": "10.18653/v1/2023.emnlp-main.258"}
{"paper_id": 202734604, "title": "Do Massively Pretrained Language Models Make Better Storytellers?", "author_names": ["A. See", "Aneesh S. Pappu", "Rohun Saxena", "Akhila Yerukola", "Christopher D. Manning"], "venue": "Conference on Computational Natural Language Learning", "abstract": "Large neural language models trained on massive amounts of text have emerged as a formidable strategy for Natural Language Understanding tasks. However, the strength of these models as Natural Language Generators is less clear. Though anecdotal evidence suggests that these models generate better quality text, there has been no detailed study characterizing their generation abilities. In this work, we compare the performance of an extensively pretrained model, OpenAI GPT2-117 (Radford et al., 2019), to a state-of-the-art neural story generation model (Fan et al., 2018). By evaluating the generated text across a wide variety of automatic metrics, we characterize the ways in which pretrained models do, and do not, make better storytellers. We find that although GPT2-117 conditions more strongly on context, is more sensitive to ordering of events, and uses more unusual words, it is just as likely to produce repetitive and under-diverse text when using likelihood-maximizing decoding algorithms.", "year": 2019, "publicationdate": "2019-09-24", "externalids": {"DOI": "10.18653/v1/K19-1079"}, "doi_lower": "10.18653/v1/k19-1079"}
{"paper_id": 278775846, "title": "Are Larger Language Models Better at Disambiguation?", "author_names": ["Ziyuan Cao", "William Schuler"], "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics", "abstract": "Humans deal with temporary syntactic ambiguity all the time in incremental sentence processing. Sentences with temporary ambiguity that causes processing difficulties, often reflected by increase in reading time, are referred to as garden-path sentences. Garden-path theories of sentence processing attribute the increases in reading time to the reanalysis of the previously ambiguous syntactic structure to make it consistent with the new disambiguating text. It is unknown whether transformer-based language models successfully resolve the temporary ambiguity after encountering the disambiguating text. We investigated this question by analyzing completions generated from language models for a type of garden-path sentence with ambiguity between a complement clause interpretation and a relative clause interpretation. We found that larger language models are worse at resolving such ambiguity.", "year": 2025, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2025.cmcl-1.20"}, "doi_lower": "10.18653/v1/2025.cmcl-1.20"}
{"paper_id": 244345615, "title": "How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN", "author_names": ["R. Thomas McCoy", "P. Smolensky", "Tal Linzen", "Jianfeng Gao", "Asli Celikyilmaz"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Current language models can generate high-quality text. Are they simply copying text they have seen before, or have they learned generalizable linguistic abstractions? To tease apart these possibilities, we introduce RAVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure. We apply these analyses to four neural language models trained on English (an LSTM, a Transformer, Transformer-XL, and GPT-2). For local structure—e.g., individual dependencies—text generated with a standard sampling scheme is substantially less novel than our baseline of human-generated text from each model’s test set. For larger-scale structure—e.g., overall sentence structure—model-generated text is as novel or even more novel than the human-generated baseline, but models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set. We also perform extensive manual analysis, finding evidence that GPT-2 uses both compositional and analogical generalization mechanisms and showing that GPT-2’s novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-contradictory).", "year": 2021, "publicationdate": "2021-11-18", "externalids": {"DOI": "10.1162/tacl_a_00567"}, "doi_lower": "10.1162/tacl_a_00567"}
{"paper_id": 220828823, "title": "Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation", "author_names": ["Stefanie Tellex", "T. Kollar", "Steven Dickerson", "Matthew R. Walter", "A. Banerjee", "S. Teller", "N. Roy"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "This paper describes a new model for understanding natural language commands given to autonomous systems that perform navigation and mobile manipulation in semi-structured environments. Previous approaches have used models with fixed structure to infer the likelihood of a sequence of actions given the environment and the command. In contrast, our framework, called Generalized Grounding Graphs, dynamically instantiates a probabilistic graphical model for a particular natural language command according to the command's hierarchical and compositional semantic structure. Our system performs inference in the model to successfully find and execute plans corresponding to natural language commands such as \"Put the tire pallet on the truck.\" The model is trained using a corpus of commands collected using crowdsourcing. We pair each command with robot actions and use the corpus to learn the parameters of the model. We evaluate the robot's performance by inferring plans from natural language commands, executing each plan in a realistic robot simulator, and asking users to evaluate the system's performance. We demonstrate that our system can successfully follow many natural language commands from the corpus.", "year": 2011, "publicationdate": "2011-08-04", "externalids": {"DOI": "10.1609/aaai.v25i1.7979"}, "doi_lower": "10.1609/aaai.v25i1.7979"}
{"paper_id": 4787508, "title": "Deep Reinforcement Learning from Human Preferences", "author_names": ["P. Christiano", "Jan Leike", "Tom B. Brown", "Miljan Martic", "S. Legg", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.", "year": 2017, "publicationdate": "2017-06-12", "externalids": {}, "doi_lower": null}
{"paper_id": 3616025, "title": "Learning from Richer Human Guidance: Augmenting Comparison-Based Learning with Feature Queries", "author_names": ["Chandrayee Basu", "M. Singhal", "A. Dragan"], "venue": "IEEE/ACM International Conference on Human-Robot Interaction", "abstract": "We focus on learning the desired objective function for a robot. Although trajectory demonstrations can be very informative of the desired objective, they can also be difficult for users to provide. Answers to comparison queries, asking which of two trajectories is preferable, are much easier for users, and have emerged as an effective alternative. Unfortunately, comparisons are far less informative. We propose that there is much richer information that users can easily provide and that robots ought to leverage. We focus on augmenting comparisons with feature queries, and introduce a unified formalism for treating all answers as observations about the true desired reward. We derive an active query selection algorithm, and test these queries in simulation and on real users. We find that richer, feature-augmented queries can extract more information faster, leading to robots that better match user preferences in their behavior.", "year": 2018, "publicationdate": "2018-02-05", "externalids": {"DOI": "10.1145/3171221.3171284"}, "doi_lower": "10.1145/3171221.3171284"}
{"paper_id": 222066972, "title": "Learning Rewards from Linguistic Feedback", "author_names": ["T. Sumers", "Mark K. Ho", "Robert D. Hawkins", "Karthik Narasimhan", "T. Griffiths"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "We explore unconstrained natural language feedback as a learning signal for artificial agents. Humans use rich and varied language to teach, yet most prior work on interactive learning from language assumes a particular form of input (e.g., commands). We propose a general framework which does not make this assumption, instead using aspect-based sentiment analysis to decompose feedback into sentiment over the features of a Markov decision process. We then infer the teacher's reward function by regressing the sentiment on the features, an analogue of inverse reinforcement learning. To evaluate our approach, we first collect a corpus of teaching behavior in a cooperative task where both teacher and learner are human. We implement three artificial learners: sentiment-based \"literal\" and \"pragmatic\" models, and an inference network trained end-to-end to predict rewards. We then re-run our initial experiment, pairing human teachers with these artificial learners. All three models successfully learn from interactive human feedback. The inference network approaches the performance of the \"literal\" sentiment model, while the \"pragmatic\" model nears human performance. Our work provides insight into the information structure of naturalistic linguistic feedback as well as methods to leverage it for reinforcement learning.", "year": 2020, "publicationdate": "2020-09-30", "externalids": {"DOI": "10.1609/aaai.v35i7.16749"}, "doi_lower": "10.1609/aaai.v35i7.16749"}
{"paper_id": 211083001, "title": "Reward-rational (implicit) choice: A unifying formalism for reward learning", "author_names": ["Hong Jun Jeon", "S. Milli", "A. Dragan"], "venue": "Neural Information Processing Systems", "abstract": "It is often difficult to hand-specify what the correct reward function is for a task, so researchers have instead aimed to learn reward functions from human behavior or feedback. The types of behavior interpreted as evidence of the reward function have expanded greatly in recent years. We've gone from demonstrations, to comparisons, to reading into the information leaked when the human is pushing the robot away or turning it off. And surely, there is more to come. How will a robot make sense of all these diverse types of behavior? Our key insight is that different types of behavior can be interpreted in a single unifying formalism - as a reward-rational choice that the human is making, often implicitly. The formalism offers both a unifying lens with which to view past work, as well as a recipe for interpreting new sources of information that are yet to be uncovered. We provide two examples to showcase this: interpreting a new feedback type, and reading into how the choice of feedback itself leaks information about the reward.", "year": 2020, "publicationdate": "2020-02-01", "externalids": {}, "doi_lower": null}
{"paper_id": 5736997, "title": "Reference Resolution Challenges for Intelligent Agents: The Need for Knowledge", "author_names": ["Marjorie J. McShane"], "venue": "IEEE Intelligent Systems", "abstract": null, "year": 2009, "publicationdate": "2009-07-01", "externalids": {"DOI": "10.1109/MIS.2009.79"}, "doi_lower": "10.1109/mis.2009.79"}
{"paper_id": 216080466, "title": "Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks", "author_names": ["Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.", "year": 2020, "publicationdate": "2020-04-23", "externalids": {"DOI": "10.18653/v1/2020.acl-main.740"}, "doi_lower": "10.18653/v1/2020.acl-main.740"}
{"paper_id": 256459776, "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "author_names": ["Freda Shi", "Xinyun Chen", "Kanishka Misra", "Nathan Scales", "David Dohan", "Ed H. Chi", "Nathanael Scharli", "Denny Zhou"], "venue": "International Conference on Machine Learning", "abstract": "Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.", "year": 2023, "publicationdate": "2023-01-31", "externalids": {"DOI": "10.48550/arXiv.2302.00093"}, "doi_lower": "10.48550/arxiv.2302.00093"}
{"paper_id": 261530162, "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models", "author_names": ["Yue Zhang", "Yafu Li", "Leyang Cui", "Deng Cai", "Lemao Liu", "Tingchen Fu", "Xinting Huang", "Enbo Zhao", "Yu Zhang", "Yulong Chen", "Longyue Wang", "A. Luu", "Wei Bi", "Freda Shi", "Shuming Shi"], "venue": "Computational Linguistics", "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.", "year": 2023, "publicationdate": "2023-09-03", "externalids": {"DOI": "10.1162/coli.a.16"}, "doi_lower": "10.1162/coli.a.16"}
{"paper_id": 256868474, "title": "Augmented Language Models: a Survey", "author_names": ["G. Mialon", "Roberto Dessì", "M. Lomeli", "Christoforos Nalmpantis", "Ramakanth Pasunuru", "R. Raileanu", "Baptiste Rozière", "Timo Schick", "Jane Dwivedi-Yu", "Asli Celikyilmaz", "Edouard Grave", "Yann LeCun", "Thomas Scialom"], "venue": "Trans. Mach. Learn. Res.", "abstract": "This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.", "year": 2023, "publicationdate": "2023-02-15", "externalids": {}, "doi_lower": null}
{"paper_id": 259991467, "title": "Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation", "author_names": ["Ruiyang Ren", "Yuhao Wang", "Yingqi Qu", "Wayne Xin Zhao", "J. Liu", "Hao Tian", "Huaqin Wu", "Ji-rong Wen", "Haifeng Wang"], "venue": "International Conference on Computational Linguistics", "abstract": "Large language models (LLMs) have shown impressive prowess in solving a wide range of tasks with world knowledge. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly under retrieval augmentation settings. In this study, we present the first analysis on the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain question answering (QA), with a bunch of important findings. Specifically, we focus on three research questions and analyze them by examining QA, priori judgement and posteriori judgement capabilities of LLMs. We show evidence that LLMs possess unwavering confidence in their knowledge and cannot handle the conflict between internal and external knowledge well. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries. We further conduct thorough experiments to examine how different factors affect LLMs and propose a simple method to dynamically utilize supporting documents with our judgement strategy. Additionally, we find that the relevance between the supporting documents and the questions significantly impacts LLMs' QA and judgemental capabilities. The code to reproduce this work is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.", "year": 2023, "publicationdate": "2023-07-20", "externalids": {"DOI": "10.48550/arXiv.2307.11019"}, "doi_lower": "10.48550/arxiv.2307.11019"}
{"paper_id": 7897551, "title": "Extending Cognitive Architecture with Episodic Memory", "author_names": ["Andrew Nuxoll", "J. Laird"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": null, "year": 2007, "publicationdate": "2007-07-22", "externalids": {}, "doi_lower": null}
{"paper_id": 274175050, "title": "Mechanisms of memory.", "author_names": [], "venue": "Journal of Geriatric Psychiatry and Neurology", "abstract": null, "year": 1993, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 10917976, "title": "Reconsolidation of Human Memory: Brain Mechanisms and Clinical Relevance", "author_names": ["L. Schwabe", "K. Nader", "J. Pruessner"], "venue": "Biological Psychiatry", "abstract": null, "year": 2014, "publicationdate": "2014-08-15", "externalids": {"DOI": "10.1016/j.biopsych.2014.03.008"}, "doi_lower": "10.1016/j.biopsych.2014.03.008"}
{"paper_id": 7158833, "title": "A Theory of Universal Artificial Intelligence based on Algorithmic Complexity", "author_names": ["Marcus Hutter"], "venue": "arXiv.org", "abstract": "Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomono’s theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameterless theory of universal Artificial Intelligence. We give strong arguments that the resulting AI model is the most intelligent unbiased agent possible. We outline for a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning, how the AI model can formally solve them. The major drawback of the AI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AI tl , which is still eectively more intelligent than any other time t and space l bounded agent. The computation time of AI tl is of the order t·2 l . Other discussed topics are formal definitions of intelligence order relations, the horizon problem and relations of the AI theory to other AI approaches.", "year": 2000, "publicationdate": "2000-04-02", "externalids": {}, "doi_lower": null}
{"paper_id": 155100086, "title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization", "author_names": ["Xingxing Zhang", "Furu Wei", "M. Zhou"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which are created heuristically using rule-based methods. Training the hierarchical encoder with these inaccurate labels is challenging. Inspired by the recent work on pre-training transformer sentence encoders (Devlin et al., 2018), we propose Hibert (as shorthand for HIerachical Bidirectional Encoder Representations from Transformers) for document encoding and a method to pre-train it using unlabeled data. We apply the pre-trained Hibert to our summarization model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on the CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times dataset. We also achieve the state-of-the-art performance on these two datasets.", "year": 2019, "publicationdate": "2019-05-01", "externalids": {"DOI": "10.18653/v1/P19-1499"}, "doi_lower": "10.18653/v1/p19-1499"}
{"paper_id": 258887482, "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers", "author_names": ["Amirkeivan Mohtashami", "Martin Jaggi"], "venue": "Neural Information Processing Systems", "abstract": "While Transformers have shown remarkable success in natural language processing, their attention mechanism's large memory requirements have limited their ability to handle longer contexts. Prior approaches, such as recurrent memory or retrieval-based augmentation, have either compromised the random-access flexibility of attention (i.e., the capability to select any token in the entire context) or relied on separate mechanisms for relevant context retrieval, which may not be compatible with the model's attention. In this paper, we present a novel approach that allows access to the complete context while retaining random-access flexibility, closely resembling running attention on the entire context. Our method uses a landmark token to represent each block of the input and trains the attention to use it for selecting relevant blocks, enabling retrieval of blocks directly through the attention mechanism instead of by relying on a separate mechanism. Our approach seamlessly integrates with specialized data structures and the system's memory hierarchy, enabling processing of arbitrarily long context lengths. We demonstrate that our method can obtain comparable performance with Transformer-XL while significantly reducing the number of retrieved tokens in each step. Finally, we show that fine-tuning LLaMA 7B with our method successfully extends its context length capacity to over 32k tokens, allowing for inference at the context lengths of GPT-4. We release the implementation of landmark attention and the code to reproduce our experiments at https://github.com/epfml/landmark-attention/.", "year": 2023, "publicationdate": "2023-05-25", "externalids": {"DOI": "10.48550/arXiv.2305.16300"}, "doi_lower": "10.48550/arxiv.2305.16300"}
{"paper_id": 252815789, "title": "An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification", "author_names": ["Ilias Chalkidis", "Xiang Dai", "Manos Fergadiotis", "Prodromos Malakasiotis", "Desmond Elliott"], "venue": "arXiv.org", "abstract": "Non-hierarchical sparse attention Transformer-based models, such as Longformer and Big Bird, are popular approaches to working with long documents. There are clear benefits to these approaches compared to the original Transformer in terms of efficiency, but Hierarchical Attention Transformer (HAT) models are a vastly understudied alternative. We develop and release fully pre-trained HAT models that use segment-wise followed by cross-segment encoders and compare them with Longformer models and partially pre-trained HATs. In several long document downstream classification tasks, our best HAT model outperforms equally-sized Longformer models while using 10-20% less GPU memory and processing documents 40-45% faster. In a series of ablation studies, we find that HATs perform best with cross-segment contextualization throughout the model than alternative configurations that implement either early or late cross-segment contextualization. Our code is on GitHub: https://github.com/coastalcph/hierarchical-transformers.", "year": 2022, "publicationdate": "2022-10-11", "externalids": {"DOI": "10.48550/arXiv.2210.05529"}, "doi_lower": "10.48550/arxiv.2210.05529"}
{"paper_id": 252815949, "title": "Capturing Global Structural Information in Long Document Question Answering with Compressive Graph Selector Network", "author_names": ["Yuxiang Nie", "Heyan Huang", "Wei Wei", "Xian-ling Mao"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Long document question answering is a challenging task due to its demands for complex reasoning over long text. Previous works usually take long documents as non-structured flat texts or only consider the local structure in long documents. However, these methods usually ignore the global structure of the long document, which is essential for long-range understanding. To tackle this problem, we propose Compressive Graph Selector Network (CGSN) to capture the global structure in a compressive and iterative manner. The proposed model mainly focuses on the evidence selection phase of long document question answering. Specifically, it consists of three modules: local graph network, global graph network and evidence memory network. Firstly, the local graph network builds the graph structure of the chunked segment in token, sentence, paragraph and segment levels to capture the short-term dependency of the text. Secondly, the global graph network selectively receives the information of each level from the local graph, compresses them into the global graph nodes and applies graph attention to the global graph nodes to build the long-range reasoning over the entire text in an iterative way. Thirdly, the evidence memory network is designed to alleviate the redundancy problem in the evidence selection by saving the selected result in the previous steps. Extensive experiments show that the proposed model outperforms previous methods on two datasets.", "year": 2022, "publicationdate": "2022-10-11", "externalids": {"DOI": "10.48550/arXiv.2210.05499"}, "doi_lower": "10.48550/arxiv.2210.05499"}
{"paper_id": 258436892, "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input", "author_names": ["Amanda Bertsch", "Uri Alon", "Graham Neubig", "Matthew R. Gormley"], "venue": "Neural Information Processing Systems", "abstract": "Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .", "year": 2023, "publicationdate": "2023-05-02", "externalids": {"DOI": "10.48550/arXiv.2305.01625"}, "doi_lower": "10.48550/arxiv.2305.01625"}
{"paper_id": 237452204, "title": "Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems", "author_names": ["Potsawee Manakul", "M. Gales"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Transformer models have achieved state-of-the-art results in a wide range of NLP tasks including summarization. Training and inference using large transformer models can be computationally expensive. Previous work has focused on one important bottleneck, the quadratic self-attention mechanism in the encoder. Modified encoder architectures such as LED or LoBART use local attention patterns to address this problem for summarization. In contrast, this work focuses on the transformer’s encoder-decoder attention mechanism. The cost of this attention becomes more significant in inference or training approaches that require model-generated histories. First, we examine the complexity of the encoder-decoder attention. We demonstrate empirically that there is a sparse sentence structure in document summarization that can be exploited by constraining the attention mechanism to a subset of input sentences, whilst maintaining system performance. Second, we propose a modified architecture that selects the subset of sentences to constrain the encoder-decoder attention. Experiments are carried out on abstractive summarization tasks, including CNN/DailyMail, XSum, Spotify Podcast, and arXiv.", "year": 2021, "publicationdate": "2021-09-08", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.739"}, "doi_lower": "10.18653/v1/2021.emnlp-main.739"}
{"paper_id": 220831004, "title": "Big Bird: Transformers for Longer Sequences", "author_names": ["M. Zaheer", "Guru Guruganesh", "Kumar Avinava Dubey", "J. Ainslie", "Chris Alberti", "Santiago Ontañón", "Philip Pham", "Anirudh Ravula", "Qifan Wang", "Li Yang", "Amr Ahmed"], "venue": "Neural Information Processing Systems", "abstract": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.", "year": 2020, "publicationdate": "2020-07-28", "externalids": {}, "doi_lower": null}
{"paper_id": 261048772, "title": "ExpeL: LLM Agents Are Experiential Learners", "author_names": ["Andrew Zhao", "Daniel Huang", "Quentin Xu", "Matthieu Lin", "Y. Liu", "Gao Huang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.", "year": 2023, "publicationdate": "2023-08-20", "externalids": {"DOI": "10.48550/arXiv.2308.10144"}, "doi_lower": "10.48550/arxiv.2308.10144"}
{"paper_id": 260775770, "title": "LLM As DBA", "author_names": ["Xuanhe Zhou", "Guoliang Li", "Zhiyuan Liu"], "venue": "", "abstract": "Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experimental results that D-Bot can efficiently and effectively diagnose the root causes and our code is available at github.com/TsinghuaDatabaseGroup/DB-GPT.", "year": 2023, "publicationdate": "2023-08-10", "externalids": {}, "doi_lower": null}
{"paper_id": 1212273, "title": "Reasoning about a Rule", "author_names": ["P. Wason"], "venue": "Quarterly Journal of Experimental Psychology", "abstract": null, "year": 1968, "publicationdate": "1968-08-01", "externalids": {"DOI": "10.1080/14640746808400161"}, "doi_lower": "10.1080/14640746808400161"}
{"paper_id": 143200804, "title": "P. C. Wason & P. N. Johnson-Laird, Psychology of reasoning: structure and content . London: Batsford, 1972. Pp. viii + 264.", "author_names": ["R. Narasimhan"], "venue": "Journal of Linguistics", "abstract": null, "year": 1974, "publicationdate": "1974-09-01", "externalids": {"DOI": "10.1017/S0022226700006174"}, "doi_lower": "10.1017/s0022226700006174"}
{"paper_id": 143779846, "title": "Approaches to studying formal and everyday reasoning.", "author_names": ["K. Galotti"], "venue": "", "abstract": null, "year": 1989, "publicationdate": "1989-05-01", "externalids": {"DOI": "10.1037/0033-2909.105.3.331"}, "doi_lower": "10.1037/0033-2909.105.3.331"}
{"paper_id": 275570199, "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models", "author_names": ["Fengli Xu", "Qianyue Hao", "Zefang Zong", "Jingwei Wang", "Yunke Zhang", "Jingyi Wang", "Xiaochong Lan", "Jiahui Gong", "Tianjian Ouyang", "Fanjin Meng", "Chenyang Shao", "Yuwei Yan", "Qinglong Yang", "Yiwen Song", "Sijian Ren", "Xinyuan Hu", "Yu Li", "J. Feng", "Chen Gao", "Yong Li"], "venue": "arXiv.org", "abstract": "Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of\"thought\"-- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to\"think\"with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI's o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.", "year": 2025, "publicationdate": "2025-01-16", "externalids": {"DOI": "10.48550/arXiv.2501.09686"}, "doi_lower": "10.48550/arxiv.2501.09686"}
{"paper_id": 254854575, "title": "Emergent analogical reasoning in large language models", "author_names": ["Taylor W. Webb", "K. Holyoak", "Hongjing Lu"], "venue": "Nature Human Behaviour", "abstract": "The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of Generative Pre-trained Transformer (GPT)-3) on a range of analogical tasks, including a non-visual matrix reasoning task based on the rule structure of Raven’s Standard Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings; preliminary tests of GPT-4 indicated even better performance. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems. Webb et al. show that new artificial intelligence language models, such as Generative Pre-trained Transformer 3, are able to solve analogical reasoning problems at a human-like level of performance.", "year": 2022, "publicationdate": "2022-12-19", "externalids": {"DOI": "10.1038/s41562-023-01659-w"}, "doi_lower": "10.1038/s41562-023-01659-w"}
{"paper_id": 258865989, "title": "Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective", "author_names": ["Guhao Feng", "Yuntian Gu", "Bohang Zhang", "Haotian Ye", "Di He", "Liwei Wang"], "venue": "Neural Information Processing Systems", "abstract": "Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language format. Moreover, we show LLMs with CoT are capable of solving a general class of decision-making problems known as Dynamic Programming, thus justifying its power in tackling complex real-world tasks. Finally, extensive experiments on four tasks show that, while Transformers always fail to predict the answers directly, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.15408"}, "doi_lower": "10.48550/arxiv.2305.15408"}
{"paper_id": 145761266, "title": "Planning and the brain", "author_names": ["J. Grafman", "J. Hendler"], "venue": "Behavioral and Brain Sciences", "abstract": null, "year": 1991, "publicationdate": "1991-12-01", "externalids": {"DOI": "10.1017/S0140525X00071351"}, "doi_lower": "10.1017/s0140525x00071351"}
{"paper_id": 2231038, "title": "Planning and problem solving: From neuropsychology to functional neuroimaging", "author_names": ["J. Unterrainer", "A. Owen"], "venue": "Journal of Physiology - Paris", "abstract": null, "year": 2006, "publicationdate": "2006-06-01", "externalids": {"DOI": "10.1016/j.jphysparis.2006.03.014"}, "doi_lower": "10.1016/j.jphysparis.2006.03.014"}
{"paper_id": 261701030, "title": "Integrative Literature Review: Human Capital Planning: A Review of Literature and Implications for Human Resource Development", "author_names": ["Kenneth J. Zula", "T. Chermack"], "venue": "", "abstract": null, "year": 2007, "publicationdate": "2007-09-01", "externalids": {"DOI": "10.1177/1534484307303762"}, "doi_lower": "10.1177/1534484307303762"}
{"paper_id": 16429301, "title": "Plans and resource‐bounded practical reasoning", "author_names": ["M. Bratman", "David J. Israel", "Martha E. Pollack"], "venue": "International Conference on Climate Informatics", "abstract": null, "year": 1988, "publicationdate": "1988-09-01", "externalids": {"DOI": "10.1111/j.1467-8640.1988.tb00284.x"}, "doi_lower": "10.1111/j.1467-8640.1988.tb00284.x"}
{"paper_id": 262339885, "title": "Artificial intelligence - a modern approach, 2nd Edition", "author_names": ["Stuart Russell", "Peter Norvig"], "venue": "Prentice Hall series in artificial intelligence", "abstract": null, "year": 2003, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 142530025, "title": "Readings in Planning Theory", "author_names": ["Scott Campbell", "S. Fainstein"], "venue": "", "abstract": null, "year": 2003, "publicationdate": "2003-02-12", "externalids": {}, "doi_lower": null}
{"paper_id": 33236132, "title": "Decomposition of planning problems", "author_names": ["L. Sebastiá", "E. Onaindía", "Eliseo Marzal"], "venue": "AI Communications", "abstract": null, "year": 2006, "publicationdate": "2006-02-01", "externalids": {"DOI": "10.3233/EAI-2006-361"}, "doi_lower": "10.3233/eai-2006-361"}
{"paper_id": 9449916, "title": "Automated Agent Decomposition for Classical Planning", "author_names": ["Matthew Crosby", "Michael Rovatsos", "Ronald P. A. Petrick"], "venue": "International Conference on Automated Planning and Scheduling", "abstract": "Many real-world planning domains, including those used in common benchmark problems, are based on multiagent scenarios. It has long been recognised that breaking down such problems into sub-problems for individual agents may help reduce overall planning complexity. This kind of approach is especially effective in domains where interaction between agents is limited. In this paper we present a fully centralised, offline, sequential, total-order planning algorithm for solving classical planning problems based on this idea. This algorithm consists of an automated decomposition process and a heuristic search method designed specifically for decomposed domains. The decomposition method is part of a preprocessing step and can be used to determine the 'multiagent nature' of a planning problem prior to actual plan search. The heuristic search strategy is shown to effectively exploit any decompositions that are found and performs significantly better than current approaches on loosely coupled domains.", "year": 2013, "publicationdate": "2013-06-02", "externalids": {"DOI": "10.1609/icaps.v23i1.13564"}, "doi_lower": "10.1609/icaps.v23i1.13564"}
{"paper_id": 258967566, "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models", "author_names": ["Binfeng Xu", "Zhiyuan Peng", "Bowen Lei", "Subhabrata Mukherjee", "Yuchen Liu", "Dongkuan Xu"], "venue": "arXiv.org", "abstract": "Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model parameters. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant potential for truly efficient and scalable ALM systems.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.18323"}, "doi_lower": "10.48550/arxiv.2305.18323"}
{"paper_id": 276928025, "title": "Self-Corrective Task Planning by Inverse Prompting with Large Language Models", "author_names": ["Jiho Lee", "Hayun Lee", "Jonghyeon Kim", "Kyungjae Lee", "Eunwoo Kim"], "venue": "IEEE International Conference on Robotics and Automation", "abstract": "In robot task planning, large language models (LLMs) have shown significant promise in generating complex and long-horizon action sequences. However, it is observed that LLMs often produce responses that sound plausible but are not accurate. To address these problems, existing methods typically employ predefined error sets or external knowledge sources, requiring human efforts and computation resources. Recently, self-correction approaches have emerged, where LLM generates and refines plans, identifying errors by itself. Despite their effectiveness, they are more prone to failures in correction due to insufficient reasoning. In this paper, we introduce InversePrompt, a novel self-corrective task planning approach that leverages inverse prompting to enhance interpretability. Our method incorporates reasoning steps to provide clear, interpretable feedback. It generates inverse actions corresponding to the initially generated actions and verifies whether these inverse actions can restore the system to its original state, explicitly validating the logical coherence of the generated plans. The results on benchmark datasets show an average 16.3% higher success rate over existing LLM-based task planning methods. Our approach offers clearer justifications for feedback in real-world environments, resulting in more successful task completion than existing self-correction approaches across various scenarios.", "year": 2025, "publicationdate": "2025-03-10", "externalids": {"DOI": "10.1109/ICRA55743.2025.11128028"}, "doi_lower": "10.1109/icra55743.2025.11128028"}
{"paper_id": 256416127, "title": "Faithful Chain-of-Thought Reasoning", "author_names": ["Qing Lyu", "Shreya Havaldar", "Adam Stein", "Li Zhang", "D. Rao", "Eric Wong", "Marianna Apidianaki", "Chris Callison-Burch"], "venue": "International Joint Conference on Natural Language Processing", "abstract": "While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.", "year": 2023, "publicationdate": "2023-01-31", "externalids": {"DOI": "10.48550/arXiv.2301.13379"}, "doi_lower": "10.48550/arxiv.2301.13379"}
{"paper_id": 246035276, "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents", "author_names": ["Wenlong Huang", "P. Abbeel", "Deepak Pathak", "Igor Mordatch"], "venue": "International Conference on Machine Learning", "abstract": "Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g.\"make breakfast\"), to a chosen set of actionable steps (e.g.\"open fridge\"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner", "year": 2022, "publicationdate": "2022-01-18", "externalids": {}, "doi_lower": null}
{"paper_id": 260887774, "title": "Dynamic Planning with a LLM", "author_names": ["Gautier Dagan", "Frank Keller", "A. Lascarides"], "venue": "arXiv.org", "abstract": "While Large Language Models (LLMs) can solve many NLP tasks in zero-shot settings, applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows. Planning requires understanding the likely effects of one's actions and identifying whether the current environment satisfies the goal state. While symbolic planners find optimal solutions quickly, they require a complete and accurate representation of the planning problem, severely limiting their use in practical scenarios. In contrast, modern LLMs cope with noisy observations and high levels of uncertainty when reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a neuro-symbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, LLM-DP solves Alfworld faster and more efficiently than a naive LLM ReAct baseline.", "year": 2023, "publicationdate": "2023-08-11", "externalids": {"DOI": "10.48550/arXiv.2308.06391"}, "doi_lower": "10.48550/arxiv.2308.06391"}
{"paper_id": 259837542, "title": "SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning", "author_names": ["Krishan Rana", "Jesse Haviland", "Sourav Garg", "Jad Abou-Chakra", "I. Reid", "Niko Sünderhauf"], "venue": "Conference on Robot Learning", "abstract": "Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic search' for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an 'iterative replanning' pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page https://sayplan.github.io.", "year": 2023, "publicationdate": "2023-07-12", "externalids": {"DOI": "10.48550/arXiv.2307.06135"}, "doi_lower": "10.48550/arxiv.2307.06135"}
{"paper_id": 3626819, "title": "Deep Contextualized Word Representations", "author_names": ["Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.", "year": 2018, "publicationdate": "2018-02-15", "externalids": {"DOI": "10.18653/v1/N18-1202"}, "doi_lower": "10.18653/v1/n18-1202"}
{"paper_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "author_names": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/N19-1423"}, "doi_lower": "10.18653/v1/n19-1423"}
{"paper_id": 246485605, "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts", "author_names": ["Stephen H. Bach", "Victor Sanh", "Zheng-Xin Yong", "Albert Webson", "Colin Raffel", "Nihal V. Nayak", "Abheesht Sharma", "Taewoon Kim", "M Saiful Bari", "Thibault Févry", "Zaid Alyafeai", "Manan Dey", "Andrea Santilli", "Zhiqing Sun", "Srulik Ben-David", "Canwen Xu", "Gunjan Chhablani", "Han Wang", "Jason Alan Fries", "Maged Saeed Al-shaibani", "S. Sharma", "Urmish Thakker", "Khalid Almubarak", "Xiangru Tang", "Mike Tian-Jian Jiang", "Alexander M. Rush"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at https://github.com/bigscience-workshop/promptsource.", "year": 2022, "publicationdate": "2022-02-02", "externalids": {"DOI": "10.18653/v1/2022.acl-demo.9"}, "doi_lower": "10.18653/v1/2022.acl-demo.9"}
{"paper_id": 255096269, "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization", "author_names": ["S. Iyer", "Xi Victoria Lin", "Ramakanth Pasunuru", "Todor Mihaylov", "Daniel Simig", "Ping Yu", "Kurt Shuster", "Tianlu Wang", "Qing Liu", "Punit Singh Koura", "Xian Li", "Brian O'Horo", "Gabriel Pereyra", "Jeff Wang", "Christopher Dewan", "Asli Celikyilmaz", "Luke S. Zettlemoyer", "Veselin Stoyanov"], "venue": "arXiv.org", "abstract": "Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.", "year": 2022, "publicationdate": "2022-12-22", "externalids": {}, "doi_lower": null}
{"paper_id": 233296494, "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity", "author_names": ["Yao Lu", "Max Bartolo", "Alastair Moore", "Sebastian Riedel", "Pontus Stenetorp"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are “fantastic” and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.", "year": 2021, "publicationdate": "2021-04-18", "externalids": {"DOI": "10.18653/v1/2022.acl-long.556"}, "doi_lower": "10.18653/v1/2022.acl-long.556"}
{"paper_id": 235658331, "title": "Multimodal Few-Shot Learning with Frozen Language Models", "author_names": ["M. Tsimpoukelli", "Jacob Menick", "Serkan Cabi", "S. Eslami", "O. Vinyals", "Felix Hill", "Zacharias Janssen"], "venue": "Neural Information Processing Systems", "abstract": "When trained at sufficient scale, auto-regressive language models exhibit the notable ability to learn a new language task after being prompted with just a few examples. Here, we present a simple, yet effective, approach for transferring this few-shot learning ability to a multimodal setting (vision and language). Using aligned image and caption data, we train a vision encoder to represent each image as a sequence of continuous embeddings, such that a pre-trained, frozen language model prompted with this prefix generates the appropriate caption. The resulting system is a multimodal few-shot learner, with the surprising ability to learn a variety of new tasks when conditioned on examples, represented as a sequence of multiple interleaved image and text embeddings. We demonstrate that it can rapidly learn words for new objects and novel visual categories, do visual question-answering with only a handful of examples, and make use of outside knowledge, by measuring a single model on a variety of established and new benchmarks.", "year": 2021, "publicationdate": "2021-06-25", "externalids": {}, "doi_lower": null}
{"paper_id": 258048937, "title": "Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis", "author_names": ["Wenhao Zhu", "Hongyi Liu", "Qingxiu Dong", "Jingjing Xu", "Lingpeng Kong", "Jiajun Chen", "Lei Li", "Shujian Huang"], "venue": "NAACL-HLT", "abstract": "Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating massive languages? 2) Which factors affect LLMs' performance in translation? We thoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our empirical results show that translation capabilities of LLMs are continually involving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system like Google Translate, especially on low-resource languages. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, LLM can acquire translation ability in a resource-efficient way and generate moderate translation even on zero-resource languages. Second, instruction semantics can surprisingly be ignored when given in-context exemplars. Third, cross-lingual exemplars can provide better task guidance for low-resource translation than exemplars in the same language pairs. Code will be released at: https://github.com/NJUNLP/MMT-LLM.", "year": 2023, "publicationdate": "2023-04-10", "externalids": {"DOI": "10.48550/arXiv.2304.04675"}, "doi_lower": "10.48550/arxiv.2304.04675"}
{"paper_id": 257378493, "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling", "author_names": ["Zi-Hua Zhang", "Long Zhou", "Chengyi Wang", "Sanyuan Chen", "Yu Wu", "Shujie Liu", "Zhuo Chen", "Yanqing Liu", "Huaming Wang", "Jinyu Li", "Lei He", "Sheng Zhao", "Furu Wei"], "venue": "arXiv.org", "abstract": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.", "year": 2023, "publicationdate": "2023-03-07", "externalids": {"DOI": "10.48550/arXiv.2303.03926"}, "doi_lower": "10.48550/arxiv.2303.03926"}
{"paper_id": 263708879, "title": "Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance", "author_names": ["Jesse Zhang", "Jiahui Zhang", "Karl Pertsch", "Ziyi Liu", "Xiang Ren", "Minsuk Chang", "Shao-Hua Sun", "Joseph J. Lim"], "venue": "Conference on Robot Learning", "abstract": "We propose BOSS, an approach that automatically learns to solve new long-horizon, complex, and meaningful tasks by growing a learned skill library with minimal supervision. Prior work in reinforcement learning require expert supervision, in the form of demonstrations or rich reward functions, to learn long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills) learns to accomplish new tasks by performing\"skill bootstrapping,\"where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. This bootstrapping phase is guided by large language models (LLMs) that inform the agent of meaningful skills to chain together. Through this process, BOSS builds a wide range of complex and useful behaviors from a basic set of primitive skills. We demonstrate through experiments in realistic household environments that agents trained with our LLM-guided bootstrapping procedure outperform those trained with naive bootstrapping as well as prior unsupervised skill acquisition methods on zero-shot execution of unseen, long-horizon tasks in new environments. Website at clvrai.com/boss.", "year": 2023, "publicationdate": "2023-10-16", "externalids": {"DOI": "10.48550/arXiv.2310.10021"}, "doi_lower": "10.48550/arxiv.2310.10021"}
{"paper_id": 61019113, "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem", "author_names": ["M. McCloskey", "N. J. Cohen"], "venue": "", "abstract": null, "year": 1989, "publicationdate": null, "externalids": {"DOI": "10.1016/S0079-7421(08)60536-8"}, "doi_lower": "10.1016/s0079-7421(08)60536-8"}
{"paper_id": 4853851, "title": "Learning without Forgetting", "author_names": ["Zhizhong Li", "Derek Hoiem"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.", "year": 2016, "publicationdate": "2016-06-29", "externalids": {"DOI": "10.1007/978-3-319-46493-0_37"}, "doi_lower": "10.1007/978-3-319-46493-0_37"}
{"paper_id": 204734380, "title": "Orthogonal Gradient Descent for Continual Learning", "author_names": ["Mehrdad Farajtabar", "Navid Azizan", "A. Mott", "Ang Li"], "venue": "International Conference on Artificial Intelligence and Statistics", "abstract": "Neural networks are achieving state of the art and sometimes super-human performance on learning tasks across a variety of domains. Whenever these problems require learning in a continual or sequential manner, however, neural networks suffer from the problem of catastrophic forgetting; they forget how to solve previous tasks after being trained on a new task, despite having the essential capacity to solve both tasks if they were trained on both simultaneously. In this paper, we propose to address this issue from a parameter space perspective and study an approach to restrict the direction of the gradient updates to avoid forgetting previously-learned data. We present the Orthogonal Gradient Descent (OGD) method, which accomplishes this goal by projecting the gradients from new tasks onto a subspace in which the neural network output on previous task does not change and the projected gradient is still in a useful direction for learning the new task. Our approach utilizes the high capacity of a neural network more efficiently and does not require storing the previously learned data that might raise privacy concerns. Experiments on common benchmarks reveal the effectiveness of the proposed OGD method.", "year": 2019, "publicationdate": "2019-10-15", "externalids": {}, "doi_lower": null}
{"paper_id": 37308416, "title": "Gradient Episodic Memory for Continual Learning", "author_names": ["David Lopez-Paz", "Marc'Aurelio Ranzato"], "venue": "Neural Information Processing Systems", "abstract": "One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.", "year": 2017, "publicationdate": "2017-06-01", "externalids": {}, "doi_lower": null}
{"paper_id": 174798232, "title": "Episodic Memory in Lifelong Language Learning", "author_names": ["Cyprien de Masson d'Autume", "Sebastian Ruder", "Lingpeng Kong", "Dani Yogatama"], "venue": "Neural Information Processing Systems", "abstract": "We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly (~50-90%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.", "year": 2019, "publicationdate": "2019-06-01", "externalids": {}, "doi_lower": null}
{"paper_id": 53860287, "title": "Experience Replay for Continual Learning", "author_names": ["David Rolnick", "Arun Ahuja", "Jonathan Schwarz", "T. Lillicrap", "Greg Wayne"], "venue": "Neural Information Processing Systems", "abstract": "Continual learning is the problem of learning new tasks or knowledge while protecting old knowledge and ideally generalizing from old experience to learn new tasks faster. Neural networks trained by stochastic gradient descent often degrade on old tasks when trained successively on new tasks with different data distributions. This phenomenon, referred to as catastrophic forgetting, is considered a major hurdle to learning with non-stationary data or sequences of new tasks, and prevents networks from continually accumulating knowledge and skills. We examine this issue in the context of reinforcement learning, in a setting where an agent is exposed to tasks in a sequence. Unlike most other work, we do not provide an explicit indication to the model of task boundaries, which is the most general circumstance for a learning agent exposed to continuous experience. While various methods to counteract catastrophic forgetting have recently been proposed, we explore a straightforward, general, and seemingly overlooked solution - that of using experience replay buffers for all past events - with a mixture of on- and off-policy learning, leveraging behavioral cloning. We show that this strategy can still learn new tasks quickly yet can substantially reduce catastrophic forgetting in both Atari and DMLab domains, even matching the performance of methods that require task identities. When buffer storage is constrained, we confirm that a simple mechanism for randomly discarding data allows a limited size buffer to perform almost as well as an unbounded one.", "year": 2018, "publicationdate": "2018-11-28", "externalids": {}, "doi_lower": null}
{"paper_id": 2157345, "title": "Overcoming catastrophic forgetting with hard attention to the task", "author_names": ["J. Serrà", "Dídac Surís", "M. Miron", "Alexandros Karatzoglou"], "venue": "International Conference on Machine Learning", "abstract": "Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.", "year": 2018, "publicationdate": "2018-01-04", "externalids": {}, "doi_lower": null}
{"paper_id": 20282961, "title": "Neural Discrete Representation Learning", "author_names": ["Aäron van den Oord", "O. Vinyals", "K. Kavukcuoglu"], "venue": "Neural Information Processing Systems", "abstract": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.", "year": 2017, "publicationdate": "2017-11-02", "externalids": {}, "doi_lower": null}
{"paper_id": 238354201, "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer", "author_names": ["Sachin Mehta", "Mohammad Rastegari"], "venue": "International Conference on Learning Representations", "abstract": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets", "year": 2021, "publicationdate": "2021-10-05", "externalids": {}, "doi_lower": null}
{"paper_id": 257219775, "title": "Language Is Not All You Need: Aligning Perception with Language Models", "author_names": ["Shaohan Huang", "Li Dong", "Wenhui Wang", "Y. Hao", "Saksham Singhal", "Shuming Ma", "Tengchao Lv", "Lei Cui", "O. Mohammed", "Qiang Liu", "Kriti Aggarwal", "Zewen Chi", "Johan Bjorck", "Vishrav Chaudhary", "Subhojit Som", "Xia Song", "Furu Wei"], "venue": "Neural Information Processing Systems", "abstract": "A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.", "year": 2023, "publicationdate": "2023-02-27", "externalids": {"DOI": "10.48550/arXiv.2302.14045"}, "doi_lower": "10.48550/arxiv.2302.14045"}
{"paper_id": 256390509, "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "author_names": ["Junnan Li", "Dongxu Li", "S. Savarese", "Steven C. H. Hoi"], "venue": "International Conference on Machine Learning", "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.", "year": 2023, "publicationdate": "2023-01-30", "externalids": {"DOI": "10.48550/arXiv.2301.12597"}, "doi_lower": "10.48550/arxiv.2301.12597"}
{"paper_id": 258615266, "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", "author_names": ["Wenliang Dai", "Junnan Li", "Dongxu Li", "A. Tiong", "Junqi Zhao", "Weisheng Wang", "Boyang Albert Li", "Pascale Fung", "Steven C. H. Hoi"], "venue": "Neural Information Processing Systems", "abstract": "Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.", "year": 2023, "publicationdate": "2023-05-11", "externalids": {"DOI": "10.48550/arXiv.2305.06500"}, "doi_lower": "10.48550/arxiv.2305.06500"}
{"paper_id": 248476411, "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "author_names": ["Jean-Baptiste Alayrac", "Jeff Donahue", "Pauline Luc", "Antoine Miech", "Iain Barr", "Yana Hasson", "Karel Lenc", "A. Mensch", "Katie Millican", "Malcolm Reynolds", "Roman Ring", "Eliza Rutherford", "Serkan Cabi", "Tengda Han", "Zhitao Gong", "Sina Samangooei", "Marianne Monteiro", "Jacob Menick", "Sebastian Borgeaud", "Andy Brock", "Aida Nematzadeh", "Sahand Sharifzadeh", "Mikolaj Binkowski", "Ricardo Barreira", "O. Vinyals", "Andrew Zisserman", "K. Simonyan"], "venue": "Neural Information Processing Systems", "abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.", "year": 2022, "publicationdate": "2022-04-29", "externalids": {}, "doi_lower": null}
{"paper_id": 258947721, "title": "PandaGPT: One Model To Instruction-Follow Them All", "author_names": ["Yixuan Su", "Tian Lan", "Huayang Li", "Jialu Xu", "Yan Wang", "Deng Cai"], "venue": "Tsinghua Interdisciplinary Workshop on Logic, Language and Meaning", "abstract": "We present PandaGPT, an approach to emPower large lANguage moDels with visual and Auditory instruction-following capabilities. Our pilot experiments show that PandaGPT can perform complex tasks such as detailed image description generation, writing stories inspired by videos, and answering questions about audios. More interestingly, PandaGPT can take multimodal inputs simultaneously and compose their semantics naturally. For example, PandaGPT can connect how objects look in an image/video and how they sound in an audio. To do so, PandaGPT combines the multimodal encoders from ImageBind and the large language models from Vicuna. Notably, only aligned image-text pairs are required for the training of PandaGPT. Thanks to the strong capability of ImageBind in embedding data from different modalities into the same space, PandaGPT displays emergent, i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g., video, audio, depth, thermal, and IMU). We hope that PandaGPT serves as an initial step toward building AGI that can perceive and understand inputs in different modalities holistically, as we humans do.", "year": 2023, "publicationdate": "2023-05-25", "externalids": {"DOI": "10.48550/arXiv.2305.16355"}, "doi_lower": "10.48550/arxiv.2305.16355"}
{"paper_id": 258309430, "title": "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head", "author_names": ["Rongjie Huang", "Mingze Li", "Dongchao Yang", "Jiatong Shi", "Xuankai Chang", "Zhenhui Ye", "Yuning Wu", "Zhiqing Hong", "Jia-Bin Huang", "Jinglin Liu", "Yixiang Ren", "Zhou Zhao", "Shinji Watanabe"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving 16 AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Code can be found in https://github.com/AIGC-Audio/AudioGPT", "year": 2023, "publicationdate": "2023-04-25", "externalids": {"DOI": "10.48550/arXiv.2304.12995"}, "doi_lower": "10.48550/arxiv.2304.12995"}
{"paper_id": 233024831, "title": "AST: Audio Spectrogram Transformer", "author_names": ["Yuan Gong", "Yu-An Chung", "James R. Glass"], "venue": "Interspeech", "abstract": "In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2.", "year": 2021, "publicationdate": "2021-04-05", "externalids": {"DOI": "10.21437/interspeech.2021-698"}, "doi_lower": "10.21437/interspeech.2021-698"}
{"paper_id": 235421619, "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units", "author_names": ["Wei-Ning Hsu", "Benjamin Bolte", "Yao-Hung Hubert Tsai", "Kushal Lakhotia", "R. Salakhutdinov", "Abdel-rahman Mohamed"], "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing", "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960 h) and Libri-light (60,000 h) benchmarks with 10 min, 1 h, 10 h, 100 h, and 960 h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.12", "year": 2021, "publicationdate": "2021-06-14", "externalids": {"DOI": "10.1109/taslp.2021.3122291"}, "doi_lower": "10.1109/taslp.2021.3122291"}
{"paper_id": 258558106, "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages", "author_names": ["Feilong Chen", "Minglun Han", "Haozhi Zhao", "Qingyang Zhang", "Jing Shi", "Shuang Xu", "Bo Xu"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces independently. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. Our experiments show that X-LLM demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 84.5\\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. And we also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.", "year": 2023, "publicationdate": "2023-05-07", "externalids": {"DOI": "10.48550/arXiv.2305.04160"}, "doi_lower": "10.48550/arxiv.2305.04160"}
{"paper_id": 259075356, "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding", "author_names": ["Hang Zhang", "Xin Li", "Lidong Bing"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "We present Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual and audio encoders with LLM's embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.", "year": 2023, "publicationdate": "2023-06-05", "externalids": {"DOI": "10.48550/arXiv.2306.02858"}, "doi_lower": "10.48550/arxiv.2306.02858"}
{"paper_id": 258564914, "title": "InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language", "author_names": ["Zhaoyang Liu", "Yinan He", "Wenhai Wang", "Weiyun Wang", "Yi Wang", "Shoufa Chen", "Qing-Long Zhang", "Yang Yang", "Qingyun Li", "Jiashuo Yu", "Kunchang Li", "Zhe Chen", "Xuecheng Yang", "Xizhou Zhu", "Yali Wang", "Limin Wang", "Ping Luo", "Jifeng Dai", "Yu Qiao"], "venue": "arXiv.org", "abstract": "We present an interactive visual framework named InternGPT, or iGPT for short. The framework integrates chatbots that have planning and reasoning capabilities, such as ChatGPT, with non-verbal instructions like pointing movements that enable users to directly manipulate images or videos on the screen. Pointing (including gestures, cursors, etc.) movements can provide more flexibility and precision in performing vision-centric tasks that require fine-grained control, editing, and generation of visual content. The name InternGPT stands for \\textbf{inter}action, \\textbf{n}onverbal, and \\textbf{chat}bots. Different from existing interactive systems that rely on pure language, by incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2. Additionally, in iGPT, an auxiliary control mechanism is used to improve the control capability of LLM, and a large vision-language model termed Husky is fine-tuned for high-quality multi-modal dialogue (impressing ChatGPT-3.5-turbo with 93.89\\% GPT-4 Quality). We hope this work can spark new ideas and directions for future interactive visual systems. Welcome to watch the code at https://github.com/OpenGVLab/InternGPT.", "year": 2023, "publicationdate": "2023-05-09", "externalids": {"DOI": "10.48550/arXiv.2305.05662"}, "doi_lower": "10.48550/arxiv.2305.05662"}
{"paper_id": 17055992, "title": "Receptive fields, binocular interaction and functional architecture in the cat's visual cortex", "author_names": ["D. Hubel", "T. Wiesel"], "venue": "Journal of Physiology", "abstract": null, "year": 1962, "publicationdate": null, "externalids": {"DOI": "10.1113/jphysiol.1962.sp006837"}, "doi_lower": "10.1113/jphysiol.1962.sp006837"}
{"paper_id": 253238764, "title": "Visual Saliency for Object Recognition, and Object Recognition for Visual Saliency", "author_names": ["Carola Figueroa Flores"], "venue": "", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 263305224, "title": "The Ethics of OpenAI/ChatGPT", "author_names": ["Jacques Rousseau"], "venue": "ECE Official Conference Proceedings", "abstract": null, "year": 2023, "publicationdate": "2023-09-18", "externalids": {"DOI": "10.22492/issn.2188-1162.2023.25"}, "doi_lower": "10.22492/issn.2188-1162.2023.25"}
{"paper_id": 220730012, "title": "Improving Contextual Language Models for Response Retrieval in Multi-Turn Conversation", "author_names": ["Junyu Lu", "Xiancong Ren", "Yazhou Ren", "Ao Liu", "Zenglin Xu"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "As an important branch of current dialogue systems, retrieval-based chatbots leverage information retrieval to select proper predefined responses. Various promising architectures have been designed for boosting response retrieval, however, few researches exploit the effectiveness of the pre-trained contextual language models. In this paper, we propose two approaches to adapt contextual language models in dialogue response selection task. In detail, the Speaker Segmentation approach is designed to discriminate different speakers to fully utilize speaker characteristics. Besides, we propose the Dialogue Augmentation approach, i.e., cutting off real conversations at different time points, to enlarge the training corpora. Compared with previous works which use utterance-level representations, our augmented contextual language models are able to obtain top-hole contextual dialogue representations for deeper semantic understanding. Evaluation on three large-scale datasets has demonstrated that our proposed approaches yield better performance than existing models.", "year": 2020, "publicationdate": "2020-07-25", "externalids": {"DOI": "10.1145/3397271.3401255"}, "doi_lower": "10.1145/3397271.3401255"}
{"paper_id": 201070367, "title": "Attention on Attention for Image Captioning", "author_names": ["Lun Huang", "Wenmin Wang", "Jie Chen", "Xiao-Yong Wei"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Attention mechanisms are widely used in current encoder/decoder frameworks of image captioning, where a weighted average on encoded vectors is generated at each time step to guide the caption decoding process. However, the decoder has little idea of whether or how well the attended vector and the given attention query are related, which could make the decoder give misled results. In this paper, we propose an Attention on Attention (AoA) module, which extends the conventional attention mechanisms to determine the relevance between attention results and queries. AoA first generates an information vector and an attention gate using the attention result and the current context, then adds another attention by applying element-wise multiplication to them and finally obtains the attended information, the expected useful knowledge. We apply AoA to both the encoder and the decoder of our image captioning model, which we name as AoA Network (AoANet). Experiments show that AoANet outperforms all previously published methods and achieves a new state-of-the-art performance of 129.8 CIDEr-D score on MS COCO Karpathy offline test split and 129.6 CIDEr-D (C40) score on the official online testing server. Code is available at https://github.com/husthuaan/AoANet.", "year": 2019, "publicationdate": "2019-08-19", "externalids": {"DOI": "10.1109/ICCV.2019.00473"}, "doi_lower": "10.1109/iccv.2019.00473"}
{"paper_id": 214727638, "title": "X-Linear Attention Networks for Image Captioning", "author_names": ["Yingwei Pan", "Ting Yao", "Yehao Li", "Tao Mei"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Recent progress on fine-grained visual recognition and visual question answering has featured Bilinear Pooling, which effectively models the 2nd order interactions across multi-modal inputs. Nevertheless, there has not been evidence in support of building such interactions concurrently with attention mechanism for image captioning. In this paper, we introduce a unified attention block --- X-Linear attention block, that fully employs bilinear pooling to selectively capitalize on visual information or perform multi-modal reasoning. Technically, X-Linear attention block simultaneously exploits both the spatial and channel-wise bilinear attention distributions to capture the 2$^{nd}$ order interactions between the input single-modal or multi-modal features. Higher and even infinity order feature interactions are readily modeled through stacking multiple X-Linear attention blocks and equipping the block with Exponential Linear Unit (ELU) in a parameter-free fashion, respectively. Furthermore, we present X-Linear Attention Networks (dubbed as X-LAN) that novelly integrates X-Linear attention block(s) into image encoder and sentence decoder of image captioning model to leverage higher order intra- and inter-modal interactions. The experiments on COCO benchmark demonstrate that our X-LAN obtains to-date the best published CIDEr performance of 132.0% on COCO Karpathy test split. When further endowing Transformer with X-Linear attention blocks, CIDEr is boosted up to 132.8%. Source code is available at https://github.com/Panda-Peter/image-captioning.", "year": 2020, "publicationdate": "2020-03-31", "externalids": {"DOI": "10.1109/cvpr42600.2020.01098"}, "doi_lower": "10.1109/cvpr42600.2020.01098"}
{"paper_id": 219635470, "title": "Meshed-Memory Transformer for Image Captioning", "author_names": ["Marcella Cornia", "Matteo Stefanini", "L. Baraldi", "R. Cucchiara"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Transformer-based architectures represent the state of the art in sequence modeling tasks like machine translation and language understanding. Their applicability to multi-modal contexts like image captioning, however, is still largely under-explored. With the aim of filling this gap, we present M² - a Meshed Transformer with Memory for Image Captioning. The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features. Experimentally, we investigate the performance of the M² Transformer and different fully-attentive models in comparison with recurrent ones. When tested on COCO, our proposal achieves a new state of the art in single-model and ensemble configurations on the \"Karpathy\" test split and on the online test server. We also assess its performances when describing objects unseen in the training set. Trained models and code for reproducing the experiments are publicly available at: https://github.com/aimagelab/meshed-memory-transformer.", "year": 2019, "publicationdate": "2019-12-17", "externalids": {"DOI": "10.1109/cvpr42600.2020.01059"}, "doi_lower": "10.1109/cvpr42600.2020.01059"}
{"paper_id": 231985569, "title": "VisualGPT: Data-efficient Image Captioning by Balancing Visual Input and Linguistic Knowledge from Pretraining", "author_names": ["Jun Chen", "Han Guo", "Kai Yi", "Boyang Albert Li", "Mohamed Elhoseiny"], "venue": "arXiv.org", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 258588306, "title": "VideoChat: chat-centric video understanding", "author_names": ["Kunchang Li", "Yinan He", "Yi Wang", "Yizhuo Li", "Wen Wang", "Ping Luo", "Yali Wang", "Limin Wang", "Yu Qiao"], "venue": "Science China Information Sciences", "abstract": "In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we propose a video-centric instruction dataset, composed of thousands of videos matched with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and causal relationships, providing a valuable asset for training chat-centric video understanding systems. Preliminary qualitative experiments reveal our system’s potential across a broad spectrum of video applications and set the standard for future research. Access our code and data at https://github.com/OpenGVLab/Ask-Anything.", "year": 2023, "publicationdate": "2023-05-10", "externalids": {"DOI": "10.1007/s11432-024-4321-9"}, "doi_lower": "10.1007/s11432-024-4321-9"}
{"paper_id": 260438420, "title": "Learning to Model the World with Language", "author_names": ["Jessy Lin", "Yuqing Du", "Olivia Watkins", "Danijar Hafner", "P. Abbeel", "D. Klein", "A. Dragan"], "venue": "International Conference on Machine Learning", "abstract": "To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents can learn to execute simple language instructions, we aim to build agents that leverage diverse language -- language like\"this button turns on the TV\"or\"I put the bowls away\"-- that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. While current methods that learn language-conditioned policies degrade in performance with more diverse types of language, we show that Dynalang learns to leverage environment descriptions, game rules, and instructions to excel on tasks ranging from game-playing to navigating photorealistic home scans. Finally, we show that our method enables additional capabilities due to learning a generative model: Dynalang can be pretrained on text-only data, enabling learning from offline datasets, and generate language grounded in an environment.", "year": 2023, "publicationdate": "2023-07-31", "externalids": {"DOI": "10.48550/arXiv.2308.01399"}, "doi_lower": "10.48550/arxiv.2308.01399"}
{"paper_id": 233210249, "title": "Not All Attention Is All You Need", "author_names": ["Hongqiu Wu", "Hai Zhao", "Min Zhang"], "venue": "arXiv.org", "abstract": "Beyond the success story of pre-trained language models (PrLMs) in recent natural language processing, they are susceptible to over-fitting due to unusual large model size. To this end, dropout serves as a therapy. However, existing methods like random-based, knowledge-based and search-based dropout are more general but less effective onto self-attention based models, which are broadly chosen as the fundamental architecture of PrLMs. In this paper, we propose a novel dropout method named AttendOut to let self-attention empowered PrLMs capable of more robust task-specific tuning. We demonstrate that state-of-the-art models with elaborate training design may achieve much stronger results. We verify the universality of our approach on extensive natural language processing tasks.", "year": 2021, "publicationdate": "2021-04-10", "externalids": {}, "doi_lower": null}
{"paper_id": 229363322, "title": "Training data-efficient image transformers & distillation through attention", "author_names": ["Hugo Touvron", "M. Cord", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Herv'e J'egou"], "venue": "International Conference on Machine Learning", "abstract": "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.", "year": 2020, "publicationdate": "2020-12-23", "externalids": {}, "doi_lower": null}
{"paper_id": 249848272, "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks", "author_names": ["Jiasen Lu", "Christopher Clark", "Rowan Zellers", "Roozbeh Mottaghi", "Aniruddha Kembhavi"], "venue": "International Conference on Learning Representations", "abstract": "We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and demos for Unified-IO are available at: https://unified-io.allenai.org.", "year": 2022, "publicationdate": "2022-06-17", "externalids": {"DOI": "10.48550/arXiv.2206.08916"}, "doi_lower": "10.48550/arxiv.2206.08916"}
{"paper_id": 259165461, "title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration", "author_names": ["Chenyang Lyu", "Minghao Wu", "Longyue Wang", "Xinting Huang", "Bingshuai Liu", "Zefeng Du", "Shuming Shi", "Zhaopeng Tu"], "venue": "arXiv.org", "abstract": "Although instruction-tuned large language models (LLMs) have exhibited remarkable capabilities across various NLP tasks, their effectiveness on other data modalities beyond text has not been fully studied. In this work, we propose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual, audio, and textual information. Macaw-LLM consists of three main components: a modality module for encoding multi-modal data, a cognitive module for harnessing pretrained LLMs, and an alignment module for harmonizing diverse representations. Our novel alignment module seamlessly bridges multi-modal features to textual features, simplifying the adaptation process from the modality modules to the cognitive module. In addition, we construct a large-scale multi-modal instruction dataset in terms of multi-turn dialogue, including 69K image instances and 50K video instances. We have made our data, code and model publicly available, which we hope can pave the way for future research in multi-modal LLMs and expand the capabilities of LLMs to handle diverse data modalities and address complex real-world scenarios.", "year": 2023, "publicationdate": "2023-06-15", "externalids": {"DOI": "10.48550/arXiv.2306.09093"}, "doi_lower": "10.48550/arxiv.2306.09093"}
{"paper_id": 259108333, "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models", "author_names": ["Muhammad Maaz", "H. Rasheed", "Salman H. Khan", "F. Khan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the under-explored field of \\emph{video-based conversation} by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is capable of understanding and generating detailed conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantitative evaluation framework for video-based dialogue models to objectively analyze the strengths and weaknesses of video-based dialogue models. Code: https://github.com/mbzuai-oryx/Video-ChatGPT.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.48550/arXiv.2306.05424"}, "doi_lower": "10.48550/arxiv.2306.05424"}
{"paper_id": 258041377, "title": "Training-Free Layout Control with Cross-Attention Guidance", "author_names": ["Minghao Chen", "Iro Laina", "A. Vedaldi"], "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "abstract": "Recent diffusion-based generators can produce high-quality images from textual prompts. However, they often disregard textual instructions that specify the spatial layout of the composition. We propose a simple approach that achieves robust layout control without the need for training or fine-tuning of the image generator. Our technique manipulates the cross-attention layers that the model uses to interface textual and visual information and steers the generation in the desired direction given, e.g., a user-specified layout. To determine how to best guide attention, we study the role of attention maps and explore two alternative strategies, forward and backward guidance. We thoroughly evaluate our approach on three benchmarks and provide several qualitative examples and a comparative analysis of the two strategies that demonstrate the superiority of backward guidance compared to forward guidance, as well as prior work. We further demonstrate the versatility of layout guidance by extending it to applications such as editing the layout and context of real images.", "year": 2023, "publicationdate": "2023-04-06", "externalids": {"DOI": "10.1109/WACV57701.2024.00526"}, "doi_lower": "10.1109/wacv57701.2024.00526"}
{"paper_id": 252923993, "title": "Robust Speech Recognition via Large-Scale Weak Supervision", "author_names": ["Alec Radford", "Jong Wook Kim", "Tao Xu", "Greg Brockman", "Christine McLeavey", "I. Sutskever"], "venue": "International Conference on Machine Learning", "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.", "year": 2022, "publicationdate": "2022-12-06", "externalids": {}, "doi_lower": null}
{"paper_id": 219531522, "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech", "author_names": ["Yi Ren", "Chenxu Hu", "Xu Tan", "Tao Qin", "Sheng Zhao", "Zhou Zhao", "Tie-Yan Liu"], "venue": "International Conference on Learning Representations", "abstract": "Advanced text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs during training and use predicted values during inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of full end-to-end training and even faster inference than FastSpeech. Experimental results show that 1) FastSpeech 2 and 2s outperform FastSpeech in voice quality with much simplified training pipeline and reduced training time; 2) FastSpeech 2 and 2s can match the voice quality of autoregressive models while enjoying much faster inference speed.", "year": 2020, "publicationdate": "2020-06-08", "externalids": {}, "doi_lower": null}
{"paper_id": 248377374, "title": "SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech", "author_names": ["Zhenhui Ye", "Zhou Zhao", "Yi Ren", "Fei Wu"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "The recent progress in non-autoregressive text-to-speech (NAR-TTS) has made fast and high-quality speech synthesis possible. However, current NAR-TTS models usually use phoneme sequence as input and thus cannot understand the tree-structured syntactic information of the input sequence, which hurts the prosody modeling. To this end, we propose SyntaSpeech, a syntax-aware and light-weight NAR-TTS model, which integrates tree-structured syntactic information into the prosody modeling modules in PortaSpeech. Specifically, 1) We build a syntactic graph based on the dependency tree of the input sentence, then process the text encoding with a syntactic graph encoder to extract the syntactic information. 2) We incorporate the extracted syntactic encoding with PortaSpeech to improve the prosody prediction. 3) We introduce a multi-length discriminator to replace the flow-based post-net in PortaSpeech, which simplifies the training pipeline and improves the inference speed, while keeping the naturalness of the generated audio. Experiments on three datasets not only show that the tree-structured syntactic information grants SyntaSpeech the ability to synthesize better audio with expressive prosody, but also demonstrate the generalization ability of SyntaSpeech to adapt to multiple languages and multi-speaker text-to-speech. Ablation studies demonstrate the necessity of each component in SyntaSpeech. Source code and audio samples are available at https://syntaspeech.github.io.", "year": 2022, "publicationdate": "2022-04-25", "externalids": {"DOI": "10.48550/arXiv.2204.11792"}, "doi_lower": "10.48550/arxiv.2204.11792"}
{"paper_id": 235417304, "title": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech", "author_names": ["Jaehyeon Kim", "Jungil Kong", "Juhee Son"], "venue": "International Conference on Machine Learning", "abstract": "Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.", "year": 2021, "publicationdate": "2021-06-11", "externalids": {}, "doi_lower": null}
{"paper_id": 253761316, "title": "TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation", "author_names": ["Zhongqiu Wang", "Samuele Cornell", "Shukjae Choi", "Younglo Lee", "Byeonghak Kim", "Shinji Watanabe"], "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing", "abstract": "We propose TF-GridNet for speech separation. The model is a novel deep neural network (DNN) integrating full- and sub-band modeling in the time-frequency (T-F) domain. It stacks several blocks, each consisting of an intra-frame full-band module, a sub-band temporal module, and a cross-frame self-attention module. It is trained to perform complex spectral mapping, where the real and imaginary (RI) components of input signals are stacked as features to predict target RI components. We first evaluate it on monaural anechoic speaker separation. Without using data augmentation and dynamic mixing, it obtains a state-of-the-art 23.5 dB improvement in scale-invariant signal-to-distortion ratio (SI-SDR) on WSJ0-2mix, a standard dataset for two-speaker separation. To show its robustness to noise and reverberation, we evaluate it on monaural reverberant speaker separation using the SMS-WSJ dataset and on noisy-reverberant speaker separation using WHAMR!, and obtain state-of-the-art performance on both datasets. We then extend TF-GridNet to multi-microphone conditions through multi-microphone complex spectral mapping, and integrate it into a two-DNN system with a beamformer in between (named as MISO-BF-MISO in earlier studies), where the beamformer proposed in this article is a novel multi-frame Wiener filter computed based on the outputs of the first DNN. State-of-the-art performance is obtained on the multi-channel tasks of SMS-WSJ and WHAMR!. Besides speaker separation, we apply the proposed algorithms to speech dereverberation and noisy-reverberant speech enhancement. State-of-the-art performance is obtained on a dereverberation dataset and on the dataset of the recent L3DAS22 multi-channel speech enhancement challenge.", "year": 2022, "publicationdate": "2022-11-22", "externalids": {"DOI": "10.1109/TASLP.2023.3304482"}, "doi_lower": "10.1109/taslp.2023.3304482"}
{"paper_id": 235262772, "title": "DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism", "author_names": ["Jinglin Liu", "Chengxi Li", "Yi Ren", "Feiyang Chen", "Zhou Zhao"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Singing voice synthesis (SVS) systems are built to synthesize high-quality and expressive singing voice, in which the acoustic model generates the acoustic features (e.g., mel-spectrogram) given a music score. Previous singing acoustic models adopt a simple loss (e.g., L1 and L2) or generative adversarial network (GAN) to reconstruct the acoustic features, while they suffer from over-smoothing and unstable training issues respectively, which hinder the naturalness of synthesized singing. \nIn this work, we propose DiffSinger, an acoustic model for SVS based on the diffusion probabilistic model. DiffSinger is a parameterized Markov chain that iteratively converts the noise into mel-spectrogram conditioned on the music score. By implicitly optimizing variational bound, DiffSinger can be stably trained and generate realistic outputs. \nTo further improve the voice quality and speed up inference, we introduce a shallow diffusion mechanism to make better use of the prior knowledge learned by the simple loss. Specifically, DiffSinger starts generation at a shallow step smaller than the total number of diffusion steps, according to the intersection of the diffusion trajectories of the ground-truth mel-spectrogram and the one predicted by a simple mel-spectrogram decoder. Besides, we propose boundary prediction methods to locate the intersection and determine the shallow step adaptively.\nThe evaluations conducted on a Chinese singing dataset demonstrate that DiffSinger outperforms state-of-the-art SVS work. Extensional experiments also prove the generalization of our methods on text-to-speech task (DiffSpeech). Audio samples: https://diffsinger.github.io. Codes: https://github.com/MoonInTheRiver/DiffSinger.", "year": 2021, "publicationdate": "2021-05-06", "externalids": {"DOI": "10.1609/aaai.v36i10.21350"}, "doi_lower": "10.1609/aaai.v36i10.21350"}
{"paper_id": 237941181, "title": "Fast-MD: Fast Multi-Decoder End-to-End Speech Translation with Non-Autoregressive Hidden Intermediates", "author_names": ["H. Inaguma", "Siddharth Dalmia", "Brian Yan", "Shinji Watanabe"], "venue": "Automatic Speech Recognition & Understanding", "abstract": "The multi-decoder (MD) end-to-end speech translation model has demonstrated high translation quality by searching for better intermediate automatic speech recognition (ASR) decoder states as hidden intermediates (HI). It is a two-pass decoding model decomposing the overall task into ASR and machine translation sub-tasks. However, the decoding speed is not fast enough for real-world applications because it conducts beam search for both sub-tasks during inference. We propose Fast-MD, a fast MD model that generates HI by non-autoregressive (NAR) decoding based on connectionist temporal classification (CTC) outputs followed by an ASR decoder. We investigated two types of NAR HI: (1) parallel HI by using an autoregressive Transformer ASR decoder and (2) masked HI by using Mask-CTC, which combines CTC and the conditional masked language model. To reduce a mismatch in the ASR decoder between teacher-forcing during training and conditioning on CTC outputs during testing, we also propose sampling CTC outputs during training. Experimental evaluations on three corpora show that Fast-MD achieved about 2× and 4× faster decoding speed than that of the naïve MD model on GPU and CPU with comparable translation quality. Adopting the Conformer encoder and intermediate CTC loss further boosts its quality without sacrificing decoding speed.", "year": 2021, "publicationdate": "2021-09-27", "externalids": {"DOI": "10.1109/ASRU51503.2021.9687894"}, "doi_lower": "10.1109/asru51503.2021.9687894"}
{"paper_id": 222919533, "title": "Speech Analysis, Synthesis and Perception", "author_names": ["E. A. Newman"], "venue": "", "abstract": null, "year": 1966, "publicationdate": "1966-04-01", "externalids": {"DOI": "10.1088/0031-9112/17/4/013"}, "doi_lower": "10.1088/0031-9112/17/4/013"}
{"paper_id": 123205845, "title": "LIDAR: Mapping the world in 3D", "author_names": ["Brent Schwarz"], "venue": "", "abstract": null, "year": 2010, "publicationdate": "2010-07-01", "externalids": {"DOI": "10.1038/NPHOTON.2010.148"}, "doi_lower": "10.1038/nphoton.2010.148"}
{"paper_id": 114347801, "title": "Progress in Astronautics and Aeronautics", "author_names": ["T. Grundy", "Gp Keefe", "M. Lowson"], "venue": "", "abstract": null, "year": 2001, "publicationdate": null, "externalids": {"DOI": "10.1016/b978-1-4832-2716-0.50001-3"}, "doi_lower": "10.1016/b978-1-4832-2716-0.50001-3"}
{"paper_id": 249017698, "title": "TALM: Tool Augmented Language Models", "author_names": ["Aaron Parisi", "Yao Zhao", "Noah Fiedel"], "venue": "arXiv.org", "abstract": "Transformer based language models (LMs) demonstrate increasing performance with scale across a wide variety of tasks. Scale alone however cannot enable models to solve tasks that require access to ephemeral, changing, or private data that was unavailable at training time. Many useful tasks may also benefit from LMs being able to access APIs that read or modify state. In this work, we present Tool Augmented Language Models (TALM), combining a text-only approach to augment language models with non-differentiable tools, and an iterative\"self-play\"technique to bootstrap performance starting from few tool demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA task and a reasoning oriented math task with simple tools. At a given model scale, TALM significantly outperforms non-augmented LMs. We further demonstrate that TALM successfully performs out-of-distribution inferences on both QA and math tasks, where non-augmented LMs fail. Our results suggest that Tool Augmented Language Models are a promising direction to enrich LMs' capabilities, with less dependence on scale.", "year": 2022, "publicationdate": "2022-05-24", "externalids": {"DOI": "10.48550/arXiv.2205.12255"}, "doi_lower": "10.48550/arxiv.2205.12255"}
{"paper_id": 140748241, "title": "Metacognition and the use of tools", "author_names": ["G. Clarebout", "J. Elen", "N. J. Collazo", "Griet Lust", "Lai Jiang"], "venue": "", "abstract": null, "year": 2013, "publicationdate": null, "externalids": {"DOI": "10.1007/978-1-4419-5546-3_13"}, "doi_lower": "10.1007/978-1-4419-5546-3_13"}
{"paper_id": 257404891, "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models", "author_names": ["Chenfei Wu", "Sheng-Kai Yin", "Weizhen Qi", "Xiaodong Wang", "Zecheng Tang", "Nan Duan"], "venue": "arXiv.org", "abstract": "ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called \\textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at \\url{https://github.com/microsoft/visual-chatgpt}.", "year": 2023, "publicationdate": "2023-03-08", "externalids": {"DOI": "10.48550/arXiv.2303.04671"}, "doi_lower": "10.48550/arxiv.2303.04671"}
{"paper_id": 258947222, "title": "Large Language Models as Tool Makers", "author_names": ["Tianle Cai", "Xuezhi Wang", "Tengyu Ma", "Xinyun Chen", "Denny Zhou"], "venue": "International Conference on Learning Representations", "abstract": "Recent research has highlighted the potential of large language models (LLMs) to improve their problem-solving capabilities with the aid of suitable external tools. In our work, we further advance this concept by introducing a closed-loop framework, referred to as LLMs A s Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two phases: 1) tool making: an LLM acts as the tool maker that crafts tools for a set of tasks. 2) tool using: another LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. On the problem-solving server side, tool-making enables continual tool generation and caching as new requests emerge. This framework enables subsequent requests to access cached tools via their corresponding APIs, enhancing the efficiency of task resolution. Recognizing that tool-making requires more sophisticated capabilities, we assign this task to a powerful, albeit resource-intensive, model. Conversely, the simpler tool-using phase is delegated to a lightweight model. This strategic division of labor allows the once-off cost of tool-making to be spread over multiple instances of tool-using, significantly reducing average costs while maintaining strong performance. Furthermore, our method offers a functional cache through the caching and reuse of tools, which stores the functionality of a class of requests instead of the natural language responses from LLMs, thus extending the applicability of the conventional cache mechanism. We evaluate our approach across various complex reasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM demonstrates performance equivalent to using GPT-4 for both roles, but with a significantly reduced inference cost.", "year": 2023, "publicationdate": "2023-05-26", "externalids": {"DOI": "10.48550/arXiv.2305.17126"}, "doi_lower": "10.48550/arxiv.2305.17126"}
{"paper_id": 278899894, "title": "CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation", "author_names": ["Cheng Qian", "Chi Han", "Y. Fung", "Yujia Qin", "Zhiyuan Liu", "Heng Ji"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.14318"}, "doi_lower": "10.48550/arxiv.2305.14318"}
{"paper_id": 258059885, "title": "Teaching Large Language Models to Self-Debug", "author_names": ["Xinyun Chen", "Maxwell Lin", "Nathanael Schärli", "Denny Zhou"], "venue": "International Conference on Learning Representations", "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.", "year": 2023, "publicationdate": "2023-04-11", "externalids": {"DOI": "10.48550/arXiv.2304.05128"}, "doi_lower": "10.48550/arxiv.2304.05128"}
{"paper_id": 253098249, "title": "Instruction-Following Agents with Jointly Pre-Trained Vision-Language Models", "author_names": ["Hao Liu", "Lisa Lee", "Kimin Lee", "P. Abbeel"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2210.13431"}, "doi_lower": "10.48550/arxiv.2210.13431"}
{"paper_id": 252846090, "title": "Interactive Language: Talking to Robots in Real Time", "author_names": ["Corey Lynch", "Ayzaan Wahid", "Jonathan Tompson", "Tianli Ding", "James Betker", "Robert Baruch", "Travis Armstrong", "Peter R. Florence"], "venue": "IEEE Robotics and Automation Letters", "abstract": "We present a framework for building interactive, real-time, natural language-instructable robots in the real world, and we open source related assets (dataset, environment, benchmark, and policies). Trained with behavioral cloning on a dataset of hundreds of thousands of language-annotated trajectories, a produced policy can proficiently execute an order of magnitude more commands than previous works: specifically we estimate a 93.5% success rate on a set of 87,000 unique natural language strings specifying raw end-to-end visuo-linguo-motor skills in the real world. We find that the same policy is capable of being guided by a human via real-time language to address a wide range of precise long-horizon rearrangement goals, e.g.\"make a smiley face out of blocks\". The dataset we release comprises nearly 600,000 language-labeled trajectories, an order of magnitude larger than prior available datasets. We hope the demonstrated results and associated assets enable further advancement of helpful, capable, natural-language-interactable robots. See videos at https://interactive-language.github.io.", "year": 2022, "publicationdate": "2022-10-12", "externalids": {"DOI": "10.48550/arXiv.2210.06407"}, "doi_lower": "10.48550/arxiv.2210.06407"}
{"paper_id": 258967880, "title": "AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation", "author_names": ["Chuhao Jin", "Wenhui Tan", "Jiange Yang", "Bei Liu", "Ruihua Song", "Limin Wang", "Jianlong Fu"], "venue": "arXiv.org", "abstract": "We propose a novel framework for learning high-level cognitive capabilities in robot manipulation tasks, such as making a smiley face using building blocks. These tasks often involve complex multi-step reasoning, presenting significant challenges due to the limited paired data connecting human instructions (e.g., making a smiley face) and robot actions (e.g., end-effector movement). Existing approaches relieve this challenge by adopting an open-loop paradigm decomposing high-level instructions into simple sub-task plans, and executing them step-by-step using low-level control models. However, these approaches are short of instant observations in multi-step reasoning, leading to sub-optimal results. To address this issue, we propose to automatically collect a cognitive robot dataset by Large Language Models (LLMs). The resulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of multi-step text plans and paired observation sequences. To enable efficient data acquisition, we employ elaborated multi-round prompt designs that effectively reduce the burden of extensive human involvement. We further propose a closed-loop multi-modal embodied planning model that autoregressively generates plans by taking image observations as input. To facilitate effective learning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and finetune additional vision adapter and Q-former to enable fine-grained spatial perception for manipulation tasks. We conduct experiments to verify the superiority over existing open and closed-loop methods, and achieve a significant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4 based robot tasks. Real-world demos are shown in https://www.youtube.com/watch?v=ayAzID1_qQk .", "year": 2023, "publicationdate": "2023-05-30", "externalids": {"DOI": "10.48550/arXiv.2305.18898"}, "doi_lower": "10.48550/arxiv.2305.18898"}
{"paper_id": 250426345, "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action", "author_names": ["Dhruv Shah", "B. Osinski", "Brian Ichter", "S. Levine"], "venue": "Conference on Robot Learning", "abstract": "Goal-conditioned policies for robotic navigation can be trained on large, unannotated datasets, providing for good generalization to real-world settings. However, particularly in vision-based settings where specifying goals requires an image, this makes for an unnatural interface. Language provides a more convenient modality for communication with robots, but contemporary methods typically require expensive supervision, in the form of trajectories annotated with language descriptions. We present a system, LM-Nav, for robotic navigation that enjoys the benefits of training on unannotated large datasets of trajectories, while still providing a high-level interface to the user. Instead of utilizing a labeled instruction following dataset, we show that such a system can be constructed entirely out of pre-trained models for navigation (ViNG), image-language association (CLIP), and language modeling (GPT-3), without requiring any fine-tuning or language-annotated robot data. We instantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon navigation through complex, outdoor environments from natural language instructions. For videos of our experiments, code release, and an interactive Colab notebook that runs in your browser, please check out our project page https://sites.google.com/view/lmnav", "year": 2022, "publicationdate": "2022-07-10", "externalids": {"DOI": "10.48550/arXiv.2207.04429"}, "doi_lower": "10.48550/arxiv.2207.04429"}
{"paper_id": 258947250, "title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models", "author_names": ["Gengze Zhou", "Yicong Hong", "Qi Wu"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goals, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models. Code is available at: https://github.com/GengzeZhou/NavGPT.", "year": 2023, "publicationdate": "2023-05-26", "externalids": {"DOI": "10.48550/arXiv.2305.16986"}, "doi_lower": "10.48550/arxiv.2305.16986"}
{"paper_id": 249848263, "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge", "author_names": ["Linxi (Jim) Fan", "Guanzhi Wang", "Yunfan Jiang", "Ajay Mandlekar", "Yuncong Yang", "Haoyi Zhu", "Andrew Tang", "De-An Huang", "Yuke Zhu", "Anima Anandkumar"], "venue": "Neural Information Processing Systems", "abstract": "Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MineDojo's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of open-ended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.", "year": 2022, "publicationdate": "2022-06-17", "externalids": {"DOI": "10.48550/arXiv.2206.08853"}, "doi_lower": "10.48550/arxiv.2206.08853"}
{"paper_id": 235658728, "title": "Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft", "author_names": ["I. Kanitscheider", "Joost Huizinga", "David Farhi", "William H. Guss", "Brandon Houghton", "Raul Sampedro", "Peter Zhokhov", "Bowen Baker", "Adrien Ecoffet", "Jie Tang", "Oleg Klimov", "J. Clune"], "venue": "arXiv.org", "abstract": "An important challenge in reinforcement learning is training agents that can solve a wide variety of tasks. If tasks depend on each other (e.g. needing to learn to walk before learning to run), curriculum learning can speed up learning by focusing on the next best task to learn. We explore curriculum learning in a complex, visual domain with many hard exploration challenges: Minecraft. We find that learning progress (defined as a change in success probability of a task) is a reliable measure of learnability for automatically constructing an effective curriculum. We introduce a learning-progress based curriculum and test it on a complex reinforcement learning problem (called\"Simon Says\") where an agent is instructed to obtain a desired goal item. Many of the required skills depend on each other. Experiments demonstrate that: (1) a within-episode exploration bonus for obtaining new items improves performance, (2) dynamically adjusting this bonus across training such that it only applies to items the agent cannot reliably obtain yet further increases performance, (3) the learning-progress based curriculum elegantly follows the learning curve of the agent, and (4) when the learning-progress based curriculum is combined with the dynamic exploration bonus it learns much more efficiently and obtains far higher performance than uniform baselines. These results suggest that combining intra-episode and across-training exploration bonuses with learning progress creates a promising method for automated curriculum generation, which may substantially increase our ability to train more capable, generally intelligent agents.", "year": 2021, "publicationdate": "2021-06-28", "externalids": {}, "doi_lower": null}
{"paper_id": 256389514, "title": "Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling", "author_names": ["Kolby Nottingham", "Prithviraj Ammanabrolu", "Alane Suhr", "Yejin Choi", "Hannaneh Hajishirzi", "Sameer Singh", "Roy Fox"], "venue": "International Conference on Machine Learning", "abstract": "Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.", "year": 2023, "publicationdate": "2023-01-28", "externalids": {"DOI": "10.48550/arXiv.2301.12050"}, "doi_lower": "10.48550/arxiv.2301.12050"}
{"paper_id": 256389594, "title": "Distilling Internet-Scale Vision-Language Models into Embodied Agents", "author_names": ["T. Sumers", "Kenneth Marino", "Arun Ahuja", "R. Fergus", "Ishita Dasgupta"], "venue": "International Conference on Machine Learning", "abstract": "Instruction-following agents must ground language into their observation and action spaces. Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data. To address this challenge, we propose using pretrained vision-language models (VLMs) to supervise embodied agents. We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior. Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment. Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects). Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents.", "year": 2023, "publicationdate": "2023-01-29", "externalids": {"DOI": "10.48550/arXiv.2301.12507"}, "doi_lower": "10.48550/arxiv.2301.12507"}
{"paper_id": 256389993, "title": "Extracting Training Data from Diffusion Models", "author_names": ["Nicholas Carlini", "Jamie Hayes", "Milad Nasr", "Matthew Jagielski", "Vikash Sehwag", "Florian Tramèr", "Borja Balle", "Daphne Ippolito", "Eric Wallace"], "venue": "USENIX Security Symposium", "abstract": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.", "year": 2023, "publicationdate": "2023-01-30", "externalids": {"DOI": "10.48550/arXiv.2301.13188"}, "doi_lower": "10.48550/arxiv.2301.13188"}
{"paper_id": 272855816, "title": "ChatAnalysis: Can GPT-4 undermine Privacy in Smart Homes with Data Analysis?", "author_names": ["V. Jüttner", "Arthur Fleig", "Erik Buchmann"], "venue": "Message Understanding Conference", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.18420/muc2024-mci-ws13-143"}, "doi_lower": "10.18420/muc2024-mci-ws13-143"}
{"paper_id": 259502302, "title": "Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey", "author_names": ["Chen Ling", "Xujiang Zhao", "Jiaying Lu", "Chengyuan Deng", "Can Zheng", "Junxiang Wang", "Tanmoy Chowdhury", "Yun-Qing Li", "Hejie Cui", "Xuchao Zhang", "Tian-yu Zhao", "Amit Panalkar", "Wei Cheng", "Haoyu Wang", "Yanchi Liu", "Zhengzhang Chen", "Haifeng Chen", "Chris White", "Quanquan Gu", "Jian Pei", "Carl Yang", "Liang Zhao"], "venue": "ACM Computing Surveys", "abstract": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to making large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to summarize better and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.", "year": 2023, "publicationdate": "2023-05-30", "externalids": {"DOI": "10.1145/3764579"}, "doi_lower": "10.1145/3764579"}
{"paper_id": 229722844, "title": "Explainable AI: A Review of Machine Learning Interpretability Methods", "author_names": ["Pantelis Linardatos", "Vasilis Papastefanopoulos", "S. Kotsiantis"], "venue": "Entropy", "abstract": "Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.", "year": 2020, "publicationdate": "2020-12-25", "externalids": {"DOI": "10.3390/e23010018"}, "doi_lower": "10.3390/e23010018"}
{"paper_id": 260202961, "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models", "author_names": ["Andy Zou", "Zifan Wang", "J. Z. Kolter", "Matt Fredrikson"], "venue": "arXiv.org", "abstract": "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.", "year": 2023, "publicationdate": "2023-07-27", "externalids": {}, "doi_lower": null}
{"paper_id": 277742754, "title": "Imitation Learning: A Survey of Learning Methods, Environments and Metrics", "author_names": ["Nathan Gavenski", "Odinaldo Rodrigues", "Michael Luck"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2404.19456"}, "doi_lower": "10.48550/arxiv.2404.19456"}
{"paper_id": 2866526, "title": "Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation", "author_names": ["Yuxuan Liu", "Abhishek Gupta", "P. Abbeel", "S. Levine"], "venue": "IEEE International Conference on Robotics and Automation", "abstract": "Imitation learning is an effective approach for autonomous systems to acquire control policies when an explicit reward function is unavailable, using supervision provided as demonstrations from an expert, typically a human operator. However, standard imitation learning methods assume that the agent receives examples of observation-action tuples that could be provided, for instance, to a supervised learning algorithm. This stands in contrast to how humans and animals imitate: we observe another person performing some behavior and then figure out which actions will realize that behavior, compensating for changes in viewpoint, surroundings, object positions and types, and other factors. We term this kind of imitation learning “imitation-from-observation,” and propose an imitation learning method based on video prediction with context translation and deep reinforcement learning. This lifts the assumption in imitation learning that the demonstration should consist of observations in the same environment configuration, and enables a variety of interesting applications, including learning robotic skills that involve tool use simply by observing videos of human tool use. Our experimental results show the effectiveness of our approach in learning a wide range of real-world robotic tasks modeled after common household chores from videos of a human demonstrator, including sweeping, ladling almonds, pushing objects as well as a number of tasks in simulation.", "year": 2017, "publicationdate": "2017-07-11", "externalids": {"DOI": "10.1109/ICRA.2018.8462901"}, "doi_lower": "10.1109/icra.2018.8462901"}
{"paper_id": 249953673, "title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos", "author_names": ["Bowen Baker", "Ilge Akkaya", "P. Zhokhov", "Joost Huizinga", "Jie Tang", "Adrien Ecoffet", "Brandon Houghton", "Raul Sampedro", "J. Clune"], "venue": "Neural Information Processing Systems", "abstract": "Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.", "year": 2022, "publicationdate": "2022-06-23", "externalids": {"DOI": "10.48550/arXiv.2206.11795"}, "doi_lower": "10.48550/arxiv.2206.11795"}
{"paper_id": 13072941, "title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author_names": ["S. Levine", "P. Pastor", "A. Krizhevsky", "Deirdre Quillen"], "venue": "Int. J. Robotics Res.", "abstract": "We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images independent of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. We describe two large-scale experiments that we conducted on two separate robotic platforms. In the first experiment, about 800,000 grasp attempts were collected over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and gripper wear and tear. In the second experiment, we used a different robotic platform and 8 robots to collect a dataset consisting of over 900,000 grasp attempts. The second robotic platform was used to test transfer between robots, and the degree to which data from a different set of robots can be used to aid learning. Our experimental results demonstrate that our approach achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing. Our transfer experiment also illustrates that data from different robots can be combined to learn more reliable and effective grasping.", "year": 2016, "publicationdate": "2016-03-07", "externalids": {"DOI": "10.1177/0278364917710318"}, "doi_lower": "10.1177/0278364917710318"}
{"paper_id": 259766568, "title": "Secrets of RLHF in Large Language Models Part I: PPO", "author_names": ["Rui Zheng", "Shihan Dou", "Songyang Gao", "Wei Shen", "Wei-Yuan Shen", "Bing Wang", "Yan Liu", "Senjie Jin", "Qin Liu", "Limao Xiong", "Luyao Chen", "Zhiheng Xi", "Yuhao Zhou", "Nuo Xu", "Wen-De Lai", "Minghao Zhu", "Rongxiang Weng", "Wen-Chun Cheng", "Cheng Chang", "Zhangyue Yin", "Yuan Hua", "Haoran Huang", "Tianxiang Sun", "Hang Yan", "Tao Gui", "Qi Zhang", "Xipeng Qiu", "Xuanjing Huang"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \\textbf{reward models} to measure human preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \\textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.", "year": 2023, "publicationdate": "2023-07-11", "externalids": {"DOI": "10.48550/arXiv.2307.04964"}, "doi_lower": "10.48550/arxiv.2307.04964"}
{"paper_id": 10973716, "title": "Curriculum and learning.", "author_names": ["Dintcho As"], "venue": "Journal of the American Podiatry Association", "abstract": null, "year": 1970, "publicationdate": null, "externalids": {"DOI": "10.7547/87507315-60-1-22"}, "doi_lower": "10.7547/87507315-60-1-22"}
{"paper_id": 235755472, "title": "Evaluating Large Language Models Trained on Code", "author_names": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Pondé", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "Scott Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mo Bavarian", "Clemens Winter", "P. Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "Jan Leike", "Josh Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "I. Sutskever", "Wojciech Zaremba"], "venue": "arXiv.org", "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.", "year": 2021, "publicationdate": "2021-07-07", "externalids": {}, "doi_lower": null}
{"paper_id": 258059792, "title": "Augmenting large language models with chemistry tools", "author_names": ["Andrés M Bran", "Sam Cox", "Oliver Schilter", "Carlo Baldassari", "Andrew D. White", "P. Schwaller"], "venue": "Nature Machine Intelligence", "abstract": "Large language models (LLMs) have shown strong performance in tasks across domains but struggle with chemistry-related problems. These models also lack access to external knowledge sources, limiting their usefulness in scientific applications. We introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery and materials design. By integrating 18 expert-designed tools and using GPT-4 as the LLM, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our agent autonomously planned and executed the syntheses of an insect repellent and three organocatalysts and guided the discovery of a novel chromophore. Our evaluation, including both LLM and expert assessments, demonstrates ChemCrow’s effectiveness in automating a diverse set of chemical tasks. Our work not only aids expert chemists and lowers barriers for non-experts but also fosters scientific advancement by bridging the gap between experimental and computational chemistry. Large language models can be queried to perform chain-of-thought reasoning on text descriptions of data or computational tools, which can enable flexible and autonomous workflows. Bran et al. developed ChemCrow, a GPT-4-based agent that has access to computational chemistry tools and a robotic chemistry platform, which can autonomously solve tasks for designing or synthesizing chemicals such as drugs or materials.", "year": 2023, "publicationdate": "2023-04-11", "externalids": {"DOI": "10.1038/s42256-024-00832-8"}, "doi_lower": "10.1038/s42256-024-00832-8"}
{"paper_id": 265381326, "title": "TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents", "author_names": ["Jingqing Ruan", "Yihong Chen", "Bin Zhang", "Zhiwei Xu", "Tianpeng Bao", "Guoqing Du", "Shiwei Shi", "Hangyu Mao", "Xingyu Zeng", "Rui Zhao"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2308.03427"}, "doi_lower": "10.48550/arxiv.2308.03427"}
{"paper_id": 258352210, "title": "Industrial Engineering with Large Language Models: A Case Study of ChatGPT's Performance on Oil & Gas Problems", "author_names": ["O. Ogundare", "S. Madasu", "N. Wiggins"], "venue": "International Conference on Control, Mechatronics and Automation", "abstract": "Large Language Models (LLMs) have shown great potential in solving complex problems in various fields, including oil and gas engineering and other industrial engineering disciplines like factory automation, PLC programming etc. However, automatic identification of strong and weak solutions to fundamental physics equations governing several industrial processes remain a challenging task. This paper identifies the limitation of current LLM approaches, particularly ChatGPT in selected practical problems native to oil and gas engineering but not exclusively. The performance of ChatGPT in solving complex problems in oil and gas engineering is discussed and the areas where LLMs are most effective are presented.", "year": 2023, "publicationdate": "2023-04-27", "externalids": {"DOI": "10.1109/ICCMA59762.2023.10374622"}, "doi_lower": "10.1109/iccma59762.2023.10374622"}
{"paper_id": 7107473, "title": "The Development of Embodied Cognition: Six Lessons from Babies", "author_names": ["Linda B. Smith", "M. Gasser"], "venue": "Artificial Life", "abstract": null, "year": 2005, "publicationdate": null, "externalids": {"DOI": "10.1162/1064546053278973"}, "doi_lower": "10.1162/1064546053278973"}
{"paper_id": 232146971, "title": "A Survey of Embodied AI: From Simulators to Research Tasks", "author_names": ["Jiafei Duan", "Samson Yu", "Tangyao Li", "Huaiyu Zhu", "Cheston Tan"], "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence", "abstract": "There has been an emerging paradigm shift from the era of “internet AI” to “embodied AI,” where AI algorithms and agents no longer learn from datasets of images, videos or text curated primarily from the internet. Instead, they learn through interactions with their environments from an egocentric perception similar to humans. Consequently, there has been substantial growth in the demand for embodied AI simulators to support various embodied AI research tasks. This growing interest in embodied AI is beneficial to the greater pursuit of Artificial General Intelligence (AGI), but there has not been a contemporary and comprehensive survey of this field. This paper aims to provide an encyclopedic survey for the field of embodied AI, from its simulators to its research. By evaluating nine current embodied AI simulators with our proposed seven features, this paper aims to understand the simulators in their provision for use in embodied AI research and their limitations. Lastly, this paper surveys the three main research tasks in embodied AI – visual exploration, visual navigation and embodied question answering (QA), covering the state-of-the-art approaches, evaluation metrics and datasets. Finally, with the new insights revealed through surveying the field, the paper will provide suggestions for simulator-for-task selections and recommendations for the future directions of the field.", "year": 2021, "publicationdate": "2021-03-08", "externalids": {"DOI": "10.1109/tetci.2022.3141105"}, "doi_lower": "10.1109/tetci.2022.3141105"}
{"paper_id": 15238391, "title": "Playing Atari with Deep Reinforcement Learning", "author_names": ["Volodymyr Mnih", "K. Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "D. Wierstra", "Martin A. Riedmiller"], "venue": "arXiv.org", "abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.", "year": 2013, "publicationdate": "2013-12-19", "externalids": {}, "doi_lower": null}
{"paper_id": 49470584, "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation", "author_names": ["Dmitry Kalashnikov", "A. Irpan", "P. Pastor", "Julian Ibarz", "Alexander Herzog", "Eric Jang", "Deirdre Quillen", "E. Holly", "Mrinal Kalakrishnan", "Vincent Vanhoucke", "S. Levine"], "venue": "Conference on Robot Learning", "abstract": "In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.", "year": 2018, "publicationdate": "2018-06-27", "externalids": {}, "doi_lower": null}
{"paper_id": 89617697, "title": "Review of Deep Reinforcement Learning for Robot Manipulation", "author_names": ["Hai V. Nguyen", "Hung M. La"], "venue": "International Conference on Robotic Computing", "abstract": null, "year": 2019, "publicationdate": "2019-02-01", "externalids": {"DOI": "10.1109/IRC.2019.00120"}, "doi_lower": "10.1109/irc.2019.00120"}
{"paper_id": 253180684, "title": "Collaborating with language models for embodied reasoning", "author_names": ["Ishita Dasgupta", "Christine Kaeser-Chen", "Kenneth Marino", "Arun Ahuja", "Sheila Babayan", "Felix Hill", "R. Fergus"], "venue": "arXiv.org", "abstract": "Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.", "year": 2023, "publicationdate": "2023-02-01", "externalids": {"DOI": "10.48550/arXiv.2302.00763"}, "doi_lower": "10.48550/arxiv.2302.00763"}
{"paper_id": 227228335, "title": "VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation", "author_names": ["Yicong Hong", "Qi Wu", "Yuankai Qi", "Cristian Rodriguez-Opazo", "Stephen Gould"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Accuracy of many visiolinguistic tasks has benefited significantly from the application of vision-and-language (V&L) BERT. However, its application for the task of vision-and-language navigation (VLN) remains limited. One reason for this is the difficulty adapting the BERT architecture to the partially observable Markov decision process present in VLN, requiring history-dependent attention and decision making. In this paper we propose a recurrent BERT model that is time-aware for use in VLN. Specifically, we equip the BERT model with a recurrent function that maintains cross-modal state information for the agent. Through extensive experiments on R2R and REVERIE we demonstrate that our model can replace more complex encoder-decoder models to achieve state-of-the-art results. Moreover, our approach can be generalised to other transformer-based architectures, supports pre-training, and is capable of solving navigation and referring expression tasks simultaneously.", "year": 2020, "publicationdate": "2020-11-26", "externalids": {"DOI": "10.1109/CVPR46437.2021.00169"}, "doi_lower": "10.1109/cvpr46437.2021.00169"}
{"paper_id": 236975859, "title": "Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion", "author_names": ["Alessandro Suglia", "Qiaozi Gao", "Jesse Thomason", "Govind Thattai", "G. Sukhatme"], "venue": "arXiv.org", "abstract": "Language-guided robots performing home and office tasks must navigate in and interact with the world. Grounding language instructions against visual observations and actions to take in an environment is an open challenge. We present Embodied BERT (EmBERT), a transformer-based model which can attend to high-dimensional, multi-modal inputs across long temporal horizons for language-conditioned task completion. Additionally, we bridge the gap between successful object-centric navigation models used for non-interactive agents and the language-guided visual task completion benchmark, ALFRED, by introducing object navigation targets for EmBERT training. We achieve competitive performance on the ALFRED benchmark, and EmBERT marks the first transformer-based model to successfully handle the long-horizon, dense, multi-modal histories of ALFRED, and the first ALFRED model to utilize object-centric navigation targets.", "year": 2021, "publicationdate": "2021-08-10", "externalids": {}, "doi_lower": null}
{"paper_id": 208006269, "title": "Reinforcement Learning for Market Making in a Multi-agent Dealer Market", "author_names": ["Sumitra Ganesh", "N. Vadori", "Mengda Xu", "Huabao Zheng", "P. Reddy", "M. Veloso"], "venue": "arXiv.org", "abstract": "Market makers play an important role in providing liquidity to markets by continuously quoting prices at which they are willing to buy and sell, and managing inventory risk. In this paper, we build a multi-agent simulation of a dealer market and demonstrate that it can be used to understand the behavior of a reinforcement learning (RL) based market maker agent. We use the simulator to train an RL-based market maker agent with different competitive scenarios, reward formulations and market price trends (drifts). We show that the reinforcement learning agent is able to learn about its competitor's pricing policy; it also learns to manage inventory by smartly selecting asymmetric prices on the buy and sell sides (skewing), and maintaining a positive (or negative) inventory depending on whether the market price drift is positive (or negative). Finally, we propose and test reward formulations for creating risk averse RL-based market maker agents.", "year": 2019, "publicationdate": "2019-11-14", "externalids": {}, "doi_lower": null}
{"paper_id": 251689151, "title": "Reinforcement learning in spacecraft control applications: Advances, prospects, and challenges", "author_names": ["M. Tipaldi", "R. Iervolino", "P. R. Massenio"], "venue": "Annual Reviews in Control", "abstract": null, "year": 2022, "publicationdate": "2022-08-01", "externalids": {"DOI": "10.1016/j.arcontrol.2022.07.004"}, "doi_lower": "10.1016/j.arcontrol.2022.07.004"}
{"paper_id": 91184540, "title": "Habitat: A Platform for Embodied AI Research", "author_names": ["M. Savva", "Abhishek Kadian", "Oleksandr Maksymets", "Yili Zhao", "Erik Wijmans", "Bhavana Jain", "Julian Straub", "Jia Liu", "V. Koltun", "Jitendra Malik", "Devi Parikh", "Dhruv Batra"], "venue": "IEEE International Conference on Computer Vision", "abstract": "We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.", "year": 2019, "publicationdate": "2019-04-02", "externalids": {"DOI": "10.1109/ICCV.2019.00943"}, "doi_lower": "10.1109/iccv.2019.00943"}
{"paper_id": 256415991, "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning", "author_names": ["S. Longpre", "Le Hou", "Tu Vu", "Albert Webson", "Hyung Won Chung", "Yi Tay", "Denny Zhou", "Quoc V. Le", "Barret Zoph", "Jason Wei", "Adam Roberts"], "venue": "International Conference on Machine Learning", "abstract": "We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2.", "year": 2023, "publicationdate": "2023-01-31", "externalids": {"DOI": "10.48550/arXiv.2301.13688"}, "doi_lower": "10.48550/arxiv.2301.13688"}
{"paper_id": 254877310, "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "author_names": ["Yizhong Wang", "Yeganeh Kordi", "Swaroop Mishra", "Alisa Liu", "Noah A. Smith", "Daniel Khashabi", "Hannaneh Hajishirzi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10560"}, "doi_lower": "10.48550/arxiv.2212.10560"}
{"paper_id": 252355542, "title": "Code as Policies: Language Model Programs for Embodied Control", "author_names": ["Jacky Liang", "Wenlong Huang", "F. Xia", "Peng Xu", "Karol Hausman", "Brian Ichter", "Peter R. Florence", "Andy Zeng"], "venue": "IEEE International Conference on Robotics and Automation", "abstract": "Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io", "year": 2022, "publicationdate": "2022-09-16", "externalids": {"DOI": "10.1109/ICRA48891.2023.10160591"}, "doi_lower": "10.1109/icra48891.2023.10160591"}
{"paper_id": 204915922, "title": "HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation with Mobile Manipulators", "author_names": ["Chengshu Li", "Fei Xia", "R. M. Martin", "S. Savarese"], "venue": "Conference on Robot Learning", "abstract": "Most common navigation tasks in human environments require auxiliary arm interactions, e.g. opening doors, pressing buttons and pushing obstacles away. This type of navigation tasks, which we call Interactive Navigation, requires the use of mobile manipulators: mobile bases with manipulation capabilities. Interactive Navigation tasks are usually long-horizon and composed of heterogeneous phases of pure navigation, pure manipulation, and their combination. Using the wrong part of the embodiment is inefficient and hinders progress. We propose HRL4IN, a novel Hierarchical RL architecture for Interactive Navigation tasks. HRL4IN exploits the exploration benefits of HRL over flat RL for long-horizon tasks thanks to temporally extended commitments towards subgoals. Different from other HRL solutions, HRL4IN handles the heterogeneous nature of the Interactive Navigation task by creating subgoals in different spaces in different phases of the task. Moreover, HRL4IN selects different parts of the embodiment to use for each phase, improving energy efficiency. We evaluate HRL4IN against flat PPO and HAC, a state-of-the-art HRL algorithm, on Interactive Navigation in two environments - a 2D grid-world environment and a 3D environment with physics simulation. We show that HRL4IN significantly outperforms its baselines in terms of task performance and energy efficiency. More information is available at this https URL.", "year": 2019, "publicationdate": "2019-10-24", "externalids": {}, "doi_lower": null}
{"paper_id": 229332243, "title": "Hierarchical principles of embodied reinforcement learning: A review", "author_names": ["Manfred Eppe", "Christian Gumbsch", "Matthias Kerzel", "Phuong D. H. Nguyen", "Martin Volker Butz", "Stefan Wermter"], "venue": "arXiv.org", "abstract": "Cognitive Psychology and related disciplines have identiﬁed several critical mechanisms that enable intelligent biological agents to learn to solve complex problems. There exists pressing evidence that the cognitive mechanisms that enable problem-solving skills in these species build on hierarchical mental representations. Among the most promising computational approaches to provide comparable learning-based problem-solving abilities for artiﬁcial agents and robots is hierarchical reinforcement learning. However, so far the existing computational approaches have not been able to equip artiﬁcial agents with problem-solving abilities that are comparable to intelligent animals, including human and non-human primates, crows, or octopuses. Here, we ﬁrst survey the literature in Cognitive Psychology, and related disciplines, and ﬁnd that many important mental mechanisms involve compositional abstraction, curiosity, and forward models. We then relate these insights with contemporary hierarchical reinforcement learning methods, and identify the key machine intelligence approaches that realise these mechanisms. As our main result, we show that all important cognitive mechanisms have been implemented independently in isolated computational architectures, and there is simply a lack of approaches that integrate them appropriately. We expect our results to guide the development of more sophisticated cognitively inspired hierarchical methods, so that future artiﬁcial agents achieve a problem- solving performance on the level of intelligent animals. Humans and several other higher level intelligent animal species have the ability to break down complex unknown problems into hierarchies of simpler previously learned sub-problems. This hier- archical approach allows them to solve previously unseen problems in a zero-shot manner,", "year": 2020, "publicationdate": "2020-12-18", "externalids": {}, "doi_lower": null}
{"paper_id": 252907859, "title": "AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments", "author_names": ["Sudipta Paul", "A. Roy-Chowdhury", "A. Cherian"], "venue": "Neural Information Processing Systems", "abstract": "Recent years have seen embodied visual navigation advance in two distinct directions: (i) in equipping the AI agent to follow natural language instructions, and (ii) in making the navigable world multimodal, e.g., audio-visual navigation. However, the real world is not only multimodal, but also often complex, and thus in spite of these advances, agents still need to understand the uncertainty in their actions and seek instructions to navigate. To this end, we present AVLEN~ -- an interactive agent for Audio-Visual-Language Embodied Navigation. Similar to audio-visual navigation tasks, the goal of our embodied agent is to localize an audio event via navigating the 3D visual world; however, the agent may also seek help from a human (oracle), where the assistance is provided in free-form natural language. To realize these abilities, AVLEN uses a multimodal hierarchical reinforcement learning backbone that learns: (a) high-level policies to choose either audio-cues for navigation or to query the oracle, and (b) lower-level policies to select navigation actions based on its audio-visual and language inputs. The policies are trained via rewarding for the success on the navigation task while minimizing the number of queries to the oracle. To empirically evaluate AVLEN, we present experiments on the SoundSpaces framework for semantic audio-visual navigation tasks. Our results show that equipping the agent to ask for help leads to a clear improvement in performance, especially in challenging cases, e.g., when the sound is unheard during training or in the presence of distractor sounds.", "year": 2022, "publicationdate": "2022-10-14", "externalids": {"DOI": "10.48550/arXiv.2210.07940"}, "doi_lower": "10.48550/arxiv.2210.07940"}
{"paper_id": 259108425, "title": "Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach", "author_names": ["Bin Hu", "Chenyang Zhao", "Pushi Zhang", "Zihao Zhou", "Yuanhang Yang", "Zenglin Xu", "Bin Liu"], "venue": "RLJ", "abstract": "Large language models (LLMs) encode a vast amount of world knowledge acquired from massive text datasets. Recent studies have demonstrated that LLMs can assist an embodied agent in solving complex sequential decision making tasks by providing high-level instructions. However, interactions with LLMs can be time-consuming. In many practical scenarios, it requires a significant amount of storage space that can only be deployed on remote cloud servers. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency. In this paper, we explore how to enable intelligent cost-effective interactions between a down stream task oriented agent and an LLM. We find that this problem can be naturally formulated by a Markov decision process (MDP), and propose When2Ask, a reinforcement learning based approach that learns when it is necessary to query LLMs for high-level instructions to accomplish a target task. On one side, When2Ask discourages unnecessary redundant interactions, while on the other side, it enables the agent to identify and follow useful instructions from the LLM. This enables the agent to halt an ongoing plan and transition to a more suitable one based on new environmental observations. Experiments on MiniGrid and Habitat environments that entail planning sub-goals demonstrate that When2Ask learns to solve target tasks with only a few necessary interactions with the LLM, significantly reducing interaction costs in testing environments compared with baseline methods. Our code is available at: https://github.com/ZJLAB-AMMI/LLM4RL.", "year": 2023, "publicationdate": "2023-06-06", "externalids": {"DOI": "10.48550/arXiv.2306.03604"}, "doi_lower": "10.48550/arxiv.2306.03604"}
{"paper_id": 221203066, "title": "SoundSpaces: Audio-Visual Navigation in 3D Environments", "author_names": ["Changan Chen", "Unnat Jain", "Carl Schissler", "S. V. A. Garí", "Ziad Al-Halah", "V. Ithapu", "Philip Robinson", "K. Grauman"], "venue": "European Conference on Computer Vision", "abstract": null, "year": 2019, "publicationdate": "2019-12-24", "externalids": {"DOI": "10.1007/978-3-030-58539-6_2"}, "doi_lower": "10.1007/978-3-030-58539-6_2"}
{"paper_id": 102560185, "title": "PROCEEDINGS - PART VI", "author_names": ["A. Orsetti"], "venue": "", "abstract": null, "year": 1994, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 261034676, "title": "GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech", "author_names": [], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 229298058, "title": "ViNG: Learning Open-World Navigation with Visual Goals", "author_names": ["Dhruv Shah", "Benjamin Eysenbach", "G. Kahn", "Nicholas Rhinehart", "S. Levine"], "venue": "IEEE International Conference on Robotics and Automation", "abstract": "We propose a learning-based navigation system for reaching visually indicated goals and demonstrate this system on a real mobile robot platform. Learning provides an appealing alternative to conventional methods for robotic navigation: instead of reasoning about environments in terms of geometry and maps, learning can enable a robot to learn about navigational affordances, understand what types of obstacles are traversable (e.g., tall grass) or not (e.g., walls), and generalize over patterns in the environment. However, unlike conventional planning algorithms, it is harder to change the goal for a learned policy during deployment. We propose a method for learning to navigate towards a goal image of the desired destination. By combining a learned policy with a topological graph constructed out of previously observed data, our system can determine how to reach this visually indicated goal even in the presence of variable appearance and lighting. Three key insights, waypoint proposal, graph pruning and negative mining, enable our method to learn to navigate in real-world environments using only offline data, a setting where prior methods struggle. We instantiate our method on a real outdoor ground robot and show that our system, which we call ViNG, outperforms previously-proposed methods for goal-conditioned reinforcement learning, including other methods that incorporate reinforcement learning and search. We also study how ViNG generalizes to unseen environments and evaluate its ability to adapt to such an environment with growing experience. Finally, we demonstrate ViNG on a number of real-world applications, such as last-mile delivery and warehouse inspection. We encourage the reader to visit the project website for videos of our experiments and demonstrations 1.", "year": 2020, "publicationdate": "2020-12-17", "externalids": {"DOI": "10.1109/ICRA48506.2021.9561936"}, "doi_lower": "10.1109/icra48506.2021.9561936"}
{"paper_id": 252846548, "title": "Visual Language Maps for Robot Navigation", "author_names": ["Chen Huang", "Oier Mees", "Andy Zeng", "Wolfram Burgard"], "venue": "IEEE International Conference on Robotics and Automation", "abstract": "Grounding language to the visual observations of a navigating agent can be performed using off-the-shelf visual-language models pretrained on Internet-scale data (e.g., image captions). While this is useful for matching images to natural language descriptions of object goals, it remains disjoint from the process of mapping the environment, so that it lacks the spatial precision of classic geometric maps. To address this problem, we propose VLMaps, a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world. VLMaps can be autonomously built from video feed on robots using standard exploration approaches and enables natural language indexing of the map without additional labeled data. Specifically, when combined with large language models (LLMs), VLMaps can be used to (i) translate natural language commands into a sequence of open-vocabulary navigation goals (which, beyond prior work, can be spatial by construction, e.g., “in between the sofa and the TV” or “three meters to the right of the chair”) directly localized in the map, and (ii) can be shared among multiple robots with different embodiments to generate new obstacle maps on-the-fly (by using a list of obstacle categories). Extensive experiments carried out in simulated and real-world environments show that VLMaps enable navigation according to more complex language instructions than existing methods. Videos are available at https://vlmaps.github.io.", "year": 2022, "publicationdate": "2022-10-11", "externalids": {"DOI": "10.1109/ICRA48891.2023.10160969"}, "doi_lower": "10.1109/icra48891.2023.10160969"}
{"paper_id": 247362748, "title": "Cross-modal Map Learning for Vision and Language Navigation", "author_names": ["G. Georgakis", "Karl Schmeckpeper", "Karan Wanchoo", "Soham Dan", "E. Miltsakaki", "D. Roth", "Kostas Daniilidis"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We consider the problem of Vision-and-Language Navigation (VLN). The majority of current methods for VLN are trained end-to-end using either unstructured memory such as LSTM, or using cross-modal attention over the egocentric observations of the agent. In contrast to other works, our key insight is that the association between language and vision is stronger when it occurs in explicit spatial representations. In this work, we propose a cross-modal map learning model for vision-and-language navigation that first learns to predict the top-down semantics on an egocentric map for both observed and unobserved regions, and then predicts a path towards the goal as a set of way-points. In both cases, the prediction is informed by the language through cross-modal attention mechanisms. We experimentally test the basic hypothesis that language-driven navigation can be solved given a map, and then show competitive results on the full VLN-CE benchmark.", "year": 2022, "publicationdate": "2022-03-10", "externalids": {"DOI": "10.1109/CVPR52688.2022.01502"}, "doi_lower": "10.1109/cvpr52688.2022.01502"}
{"paper_id": 257378363, "title": "Can an Embodied Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation", "author_names": ["Vishnu Sashank Dorbala", "James F. Mullen", "Dinesh Manocha"], "venue": "IEEE Robotics and Automation Letters", "abstract": "We present language-guided exploration (LGX), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to an uniquely described target object in a previously unseen environment. Our approach makes use of large language models (LLMs) for this task by leveraging the LLM's commonsense-reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via real-world experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects.", "year": 2023, "publicationdate": "2023-03-06", "externalids": {"DOI": "10.1109/LRA.2023.3346800"}, "doi_lower": "10.1109/lra.2023.3346800"}
{"paper_id": 244920947, "title": "Grounded Language-Image Pre-training", "author_names": ["Liunian Harold Li", "Pengchuan Zhang", "Haotian Zhang", "Jianwei Yang", "Chunyuan Li", "Yiwu Zhong", "Lijuan Wang", "Lu Yuan", "Lei Zhang", "Jenq-Neng Hwang", "Kai-Wei Chang", "Jianfeng Gao"], "venue": "Computer Vision and Pattern Recognition", "abstract": "This paper presents a grounded language-image pretraining (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representations semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines.11Supervised baselines on COCO object detection: Faster-RCNN w/ ResNet50 (40.2) or ResNet101 (42.0), and DyHead w/ Swin-Tiny (49.7). 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code will be released at https://github.com/microsoft/GLIP.", "year": 2021, "publicationdate": "2021-12-07", "externalids": {"DOI": "10.1109/CVPR52688.2022.01069"}, "doi_lower": "10.1109/cvpr52688.2022.01069"}
{"paper_id": 209500898, "title": "Look, Listen, and Act: Towards Audio-Visual Embodied Navigation", "author_names": ["Chuang Gan", "Yiwei Zhang", "Jiajun Wu", "Boqing Gong", "J. Tenenbaum"], "venue": "IEEE International Conference on Robotics and Automation", "abstract": "A crucial ability of mobile intelligent agents is to integrate the evidence from multiple sensory inputs in an environment and to make a sequence of actions to reach their goals. In this paper, we attempt to approach the problem of Audio-Visual Embodied Navigation, the task of planning the shortest path from a random starting location in a scene to the sound source in an indoor environment, given only raw egocentric visual and audio sensory data. To accomplish this task, the agent is required to learn from various modalities, i.e., relating the audio signal to the visual environment. Here we describe an approach to audio-visual embodied navigation that takes advantage of both visual and audio pieces of evidence. Our solution is based on three key ideas: a visual perception mapper module that constructs its spatial memory of the environment, a sound perception module that infers the relative location of the sound source from the agent, and a dynamic path planner that plans a sequence of actions based on the audio-visual observations and the spatial memory of the environment to navigate toward the goal. Experimental results on a newly collected Visual-Audio-Room dataset using the simulated multi-modal environment demonstrate the effectiveness of our approach over several competitive baselines.", "year": 2019, "publicationdate": "2019-12-25", "externalids": {"DOI": "10.1109/ICRA40945.2020.9197008"}, "doi_lower": "10.1109/icra40945.2020.9197008"}
{"paper_id": 254591260, "title": "RT-1: Robotics Transformer for Real-World Control at Scale", "author_names": ["Anthony Brohan", "Noah Brown", "Justice Carbajal", "Yevgen Chebotar", "Joseph Dabis", "Chelsea Finn", "K. Gopalakrishnan", "Karol Hausman", "Alexander Herzog", "Jasmine Hsu", "Julian Ibarz", "Brian Ichter", "A. Irpan", "Tomas Jackson", "Sally Jesmonth", "Nikhil J. Joshi", "Ryan C. Julian", "Dmitry Kalashnikov", "Yuheng Kuang", "Isabel Leal", "Kuang-Huei Lee", "S. Levine", "Yao Lu", "U. Malla", "D. Manjunath", "Igor Mordatch", "Ofir Nachum", "Carolina Parada", "Jodilyn Peralta", "Emily Perez", "Karl Pertsch", "Jornell Quiambao", "Kanishka Rao", "M. Ryoo", "Grecia Salazar", "Pannag R. Sanketi", "Kevin Sayed", "Jaspiar Singh", "S. Sontakke", "Austin Stone", "Clayton Tan", "Huong Tran", "Vincent Vanhoucke", "Steve Vega", "Q. Vuong", "F. Xia", "Ted Xiao", "Peng Xu", "Sichun Xu", "Tianhe Yu", "Brianna Zitkovich"], "venue": "Robotics: Science and Systems", "abstract": "By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io", "year": 2022, "publicationdate": "2022-12-13", "externalids": {"DOI": "10.48550/arXiv.2212.06817"}, "doi_lower": "10.48550/arxiv.2212.06817"}
{"paper_id": 260293142, "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control", "author_names": ["Anthony Brohan", "Noah Brown", "Justice Carbajal", "Yevgen Chebotar", "K. Choromanski", "Tianli Ding", "Danny Driess", "Kumar Avinava Dubey", "Chelsea Finn", "Peter R. Florence", "Chuyuan Fu", "Montse Gonzalez Arenas", "K. Gopalakrishnan", "Kehang Han", "Karol Hausman", "Alexander Herzog", "Jasmine Hsu", "Brian Ichter", "A. Irpan", "Nikhil J. Joshi", "Ryan C. Julian", "Dmitry Kalashnikov", "Yuheng Kuang", "Isabel Leal", "S. Levine", "H. Michalewski", "Igor Mordatch", "Karl Pertsch", "Kanishka Rao", "Krista Reymann", "M. Ryoo", "Grecia Salazar", "Pannag R. Sanketi", "P. Sermanet", "Jaspiar Singh", "Anikait Singh", "Radu Soricut", "Huong Tran", "Vincent Vanhoucke", "Q. Vuong", "Ayzaan Wahid", "Stefan Welker", "Paul Wohlhart", "Ted Xiao", "Tianhe Yu", "Brianna Zitkovich"], "venue": "Conference on Robot Learning", "abstract": "We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).", "year": 2023, "publicationdate": "2023-07-28", "externalids": {"DOI": "10.48550/arXiv.2307.15818"}, "doi_lower": "10.48550/arxiv.2307.15818"}
{"paper_id": 162735407, "title": "ga - RP32/2013, MN591/2013, RP39/2013, MN622/2013, RP179/2013, MN595/2013, RP803/2013, MN599/2013, RP804/2013, MN600/2013, RP820/2013, MN610/2013, RP821/2013, MN611/2013, RP822/2013, MN612/2013, RP823/2013, MN613/2013, RP824/2013, MN614/2013, RP825/2013, MN615/2013, RP826/2013, MN616/2013, RP827/201", "author_names": ["S. Elaine"], "venue": "", "abstract": null, "year": 2014, "publicationdate": "2014-12-03", "externalids": {}, "doi_lower": null}
{"paper_id": 260126067, "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis", "author_names": ["Izzeddin Gur", "Hiroki Furuta", "Austin Huang", "Mustafa Safdari", "Yutaka Matsuo", "D. Eck", "Aleksandra Faust"], "venue": "International Conference on Learning Representations", "abstract": "Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.", "year": 2023, "publicationdate": "2023-07-24", "externalids": {"DOI": "10.48550/arXiv.2307.12856"}, "doi_lower": "10.48550/arxiv.2307.12856"}
{"paper_id": 259129428, "title": "Mind2Web: Towards a Generalist Agent for the Web", "author_names": ["Xiang Deng", "Yu Gu", "Boyuan Zheng", "Shijie Chen", "Samuel Stevens", "Boshi Wang", "Huan Sun", "Yu Su"], "venue": "Neural Information Processing Systems", "abstract": "We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.", "year": 2023, "publicationdate": "2023-06-09", "externalids": {"DOI": "10.48550/arXiv.2306.06070"}, "doi_lower": "10.48550/arxiv.2306.06070"}
{"paper_id": 259300306, "title": "INSTRUCTION-FINETUNED FOUNDATION MODELS FOR MULTIMODAL WEB NAVIGATION", "author_names": ["Hiroki Furuta", "Ofir Nachum", "Kuang-Huei Lee", "Yutaka Matsuo", "S. Gu"], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 260164780, "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents", "author_names": ["Shuyan Zhou", "Frank F. Xu", "Hao Zhu", "Xuhui Zhou", "Robert Lo", "Abishek Sridhar", "Xianyi Cheng", "Yonatan Bisk", "Daniel Fried", "Uri Alon", "Graham Neubig"], "venue": "International Conference on Learning Representations", "abstract": "With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.", "year": 2023, "publicationdate": "2023-07-25", "externalids": {"DOI": "10.48550/arXiv.2307.13854"}, "doi_lower": "10.48550/arxiv.2307.13854"}
{"paper_id": 257834038, "title": "Language Models can Solve Computer Tasks", "author_names": ["Geunwoo Kim", "P. Baldi", "S. McAleer"], "venue": "Neural Information Processing Systems", "abstract": "Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {"DOI": "10.48550/arXiv.2303.17491"}, "doi_lower": "10.48550/arxiv.2303.17491"}
{"paper_id": 271770680, "title": "Few-shot Classification with Shrinkage Exemplars", "author_names": ["Tao Zhang", "Wu Huang"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.18970"}, "doi_lower": "10.48550/arxiv.2305.18970"}
{"paper_id": 260438734, "title": "InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent", "author_names": ["Po-Lin Chen", "Cheng-Shang Chang"], "venue": "arXiv.org", "abstract": "This research paper delves into the integration of OpenAI's ChatGPT into embodied agent systems, evaluating its influence on interactive decision-making benchmark. Drawing a parallel to the concept of people assuming roles according to their unique strengths, we introduce InterAct. In this approach, we feed ChatGPT with varied prompts, assigning it a numerous roles like a checker and a sorter, then integrating them with the original language model. Our research shows a remarkable success rate of 98% in AlfWorld, which consists of 6 different tasks in a simulated household environment, emphasizing the significance of proficient prompt engineering. The results highlight ChatGPT's competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.", "year": 2023, "publicationdate": "2023-08-03", "externalids": {"DOI": "10.48550/arXiv.2308.01552"}, "doi_lower": "10.48550/arxiv.2308.01552"}
{"paper_id": 260351308, "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models", "author_names": ["Haonan Li", "Yu Hao", "Yizhuo Zhai", "Zhiyun Qian"], "venue": "arXiv.org", "abstract": "Static analysis is a widely used technique in software engineering for identifying and mitigating bugs. However, a significant hurdle lies in achieving a delicate balance between precision and scalability. Large Language Models (LLMs) offer a promising alternative, as recent advances demonstrate remarkable capabilities in comprehending, generating, and even debugging code. Yet, the logic of bugs can be complex and require sophisticated reasoning and a large analysis scope spanning multiple functions. Therefore, at this point, LLMs are better used in an assistive role to complement static analysis. In this paper, we take a deep dive into the open space of LLM-assisted static analysis, using use-before-initialization (UBI) bugs as a case study. To this end, we develop LLift, a fully automated framework that interfaces with both a static analysis tool and an LLM. By carefully designing the framework and the prompts, we are able to overcome a number of challenges, including bug-specific modeling, the large problem scope, the non-deterministic nature of LLMs, etc. Tested in a real-world scenario analyzing nearly a thousand potential UBI bugs produced by static analysis, LLift demonstrates a potent capability, showcasing a reasonable precision (50%) and appearing to have no missing bugs. It even identified 13 previously unknown UBI bugs in the Linux kernel. This research paves the way for new opportunities and methodologies in using LLMs for bug discovery in extensive, real-world datasets.", "year": 2023, "publicationdate": "2023-08-01", "externalids": {"DOI": "10.48550/arXiv.2308.00245"}, "doi_lower": "10.48550/arxiv.2308.00245"}
{"paper_id": 259108951, "title": "Towards Autonomous Testing Agents via Conversational Large Language Models", "author_names": ["R. Feldt", "Sungmin Kang", "Juyeon Yoon", "Shin Yoo"], "venue": "International Conference on Automated Software Engineering", "abstract": "Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized “hallucination” of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.1109/ASE56229.2023.00148"}, "doi_lower": "10.1109/ase56229.2023.00148"}
{"paper_id": 260438479, "title": "ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks", "author_names": ["Y. Kang", "Jihan Kim"], "venue": "arXiv.org", "abstract": "ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate metal-organic frameworks (MOFs). By leveraging a large-scale language model (GPT-4 and GPT-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generations. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.", "year": 2023, "publicationdate": "2023-08-01", "externalids": {"DOI": "10.48550/arXiv.2308.01423"}, "doi_lower": "10.48550/arxiv.2308.01423"}
{"paper_id": 257805102, "title": "Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks", "author_names": ["Haoqi Yuan", "Chi Zhang", "Hongchen Wang", "Feiyang Xie", "Penglin Cai", "Hao Dong", "Zongqing Lu"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2303.16563"}, "doi_lower": "10.48550/arxiv.2303.16563"}
{"paper_id": 258309449, "title": "ChatLLM Network: More brains, More intelligence", "author_names": ["Rui Hao", "Linmei Hu", "Weijian Qi", "Qingliu Wu", "Yirui Zhang", "Liqiang Nie"], "venue": "AI Open", "abstract": "Dialogue-based language models mark a huge milestone in the field of artificial intelligence, by their impressive ability to interact with users, as well as a series of challenging tasks prompted by customized instructions. However, the prevalent large-scale dialogue-based language models like ChatGPT still have room for improvement, such as unstable responses to questions and the inability to think cooperatively like humans. Considering the ability of dialogue-based language models in conversation and their inherent randomness in thinking, we propose ChatLLM network that allows multiple dialogue-based language models to interact, provide feedback, and think together. We design the network of ChatLLMs based on ChatGPT. Specifically, individual instances of ChatGPT may possess distinct perspectives towards the same problem, and by consolidating these diverse viewpoints via a separate ChatGPT, the ChatLLM network system can conduct decision-making more objectively and comprehensively. In addition, a language-based feedback mechanism comparable to backpropagation is devised to update the ChatGPTs within the network. Experiments on two datasets demonstrate that our network attains significant improvements in problem-solving, leading to observable progress amongst each member.", "year": 2023, "publicationdate": "2023-04-24", "externalids": {"DOI": "10.48550/arXiv.2304.12998"}, "doi_lower": "10.48550/arxiv.2304.12998"}
{"paper_id": 259501567, "title": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models", "author_names": ["Zhao Mandi", "Shreeya Jain", "Shuran Song"], "venue": "IEEE International Conference on Robotics and Automation", "abstract": "We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset that evaluates LLMs’ agent representation and reasoning capability. We experimentally demonstrate the effectiveness of our approach — it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility — in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together.", "year": 2023, "publicationdate": "2023-07-10", "externalids": {"DOI": "10.1109/ICRA57147.2024.10610855"}, "doi_lower": "10.1109/icra57147.2024.10610855"}
{"paper_id": 255825875, "title": "Blind Judgement: Agent-Based Supreme Court Modelling With GPT", "author_names": ["S. Hamilton"], "venue": "arXiv.org", "abstract": "We present a novel Transformer-based multi-agent system for simulating the judicial rulings of the 2010-2016 Supreme Court of the United States. We train nine separate models with the respective authored opinions of each supreme justice active ca. 2015 and test the resulting system on 96 real-world cases. We find our system predicts the decisions of the real-world Supreme Court with better-than-random accuracy. We further find a correlation between model accuracy with respect to individual justices and their alignment between legal conservatism&liberalism. Our methods and results hold significance for researchers interested in using language models to simulate politically-charged discourse between multiple agents.", "year": 2023, "publicationdate": "2023-01-12", "externalids": {"DOI": "10.48550/arXiv.2301.05327"}, "doi_lower": "10.48550/arxiv.2301.05327"}
{"paper_id": 265301950, "title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework", "author_names": ["Sirui Hong", "Mingchen Zhuge", "Jonathan Chen", "Xiawu Zheng", "Yuheng Cheng", "Ceyao Zhang", "Jinlin Wang", "Zili Wang", "Steven Ka Shing Yau", "Z. Lin", "Liyang Zhou", "Chenyu Ran", "Lingfeng Xiao", "Chenglin Wu", "Jürgen Schmidhuber"], "venue": "International Conference on Learning Representations", "abstract": "Remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Existing LLM-based multi-agent systems can already solve simple dialogue tasks. Solutions to more complex tasks, however, are complicated through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors. MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together. On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems. Our project can be found at https://github.com/geekan/MetaGPT", "year": 2023, "publicationdate": "2023-08-01", "externalids": {}, "doi_lower": null}
{"paper_id": 261064959, "title": "ProAgent: Building Proactive Cooperative Agents with Large Language Models", "author_names": ["Ceyao Zhang", "Kaijie Yang", "Siyi Hu", "Zihao Wang", "Guanghe Li", "Y. Sun", "Chen Zhang", "Zhaowei Zhang", "Anji Liu", "Song-Chun Zhu", "Xiaojun Chang", "Junge Zhang", "F. Yin", "Yitao Liang", "Yaodong Yang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Building agents with adaptive behavior in cooperative tasks stands as a paramount goal in the realm of multi-agent systems. Current approaches to developing cooperative agents rely primarily on learning-based methods, whose policy generalization depends heavily on the diversity of teammates they interact with during the training phase. Such reliance, however, constrains the agents' capacity for strategic adaptation when cooperating with unfamiliar teammates, which becomes a significant challenge in zero-shot coordination scenarios. To address this challenge, we propose ProAgent, a novel framework that harnesses large language models (LLMs) to create proactive agents capable of dynamically adapting their behavior to enhance cooperation with teammates. ProAgent can analyze the present state, and infer the intentions of teammates from observations. It then updates its beliefs in alignment with the teammates' subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of modularity and interpretability, making it easily integrated into various of coordination scenarios. Experimental evaluations conducted within the Overcooked-AI environment unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training when cooperating with AI agents. Furthermore, in partnered with human proxy models, its performance exhibits an average improvement exceeding 10% compared to the current state-of-the-art method. For more information about our project, please visit https://pku-proagent.github.io.", "year": 2023, "publicationdate": "2023-08-22", "externalids": {"DOI": "10.1609/aaai.v38i16.29710"}, "doi_lower": "10.1609/aaai.v38i16.29710"}
{"paper_id": 257833743, "title": "DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents", "author_names": ["Varun Nair", "Elliot Schumacher", "G. Tso", "Anitha Kannan"], "venue": "Clinical Natural Language Processing Workshop", "abstract": "Large language models (LLMs) have emerged as valuable tools for many natural language understanding tasks. In safety-critical applications such as healthcare, the utility of these models is governed by their ability to generate factually accurate and complete outputs. In this work, we present dialog-enabled resolving agents (DERA). DERA is a paradigm made possible by the increased conversational abilities of LLMs. It provides a simple, interpretable forum for models to communicate feedback and iteratively improve output. We frame our dialog as a discussion between two agent types – a Researcher, who processes information and identifies crucial problem components, and a Decider, who has the autonomy to integrate the Researcher’s information and makes judgments on the final output.We test DERA against three clinically-focused tasks, with GPT-4 serving as our LLM. DERA shows significant improvement over the base GPT-4 performance in both human expert preference evaluations and quantitative metrics for medical conversation summarization and care plan generation. In a new finding, we also show that GPT-4’s performance (70%) on an open-ended version of the MedQA question-answering (QA) dataset (Jin 2021; USMLE) is well above the passing level (60%), with DERA showing similar performance. We will release the open-ended MedQA dataset.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {"DOI": "10.48550/arXiv.2303.17071"}, "doi_lower": "10.48550/arxiv.2303.17071"}
{"paper_id": 259088724, "title": "Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents", "author_names": ["Yashar Talebirad", "Amirhossein Nadiri"], "venue": "arXiv.org", "abstract": "In this paper, we present a novel framework for enhancing the capabilities of large language models (LLMs) by leveraging the power of multi-agent systems. Our framework introduces a collaborative environment where multiple intelligent agent components, each with distinctive attributes and roles, work together to handle complex tasks more efficiently and effectively. We demonstrate the practicality and versatility of our framework through case studies in artificial general intelligence (AGI), specifically focusing on the Auto-GPT and BabyAGI models. We also examine the\"Gorilla\"model, which integrates external APIs into the LLM. Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations. By modeling various domains such as courtroom simulations and software development scenarios, we showcase the potential applications and benefits of our proposed multi-agent system. Our framework provides an avenue for advancing the capabilities and performance of LLMs through collaboration and knowledge exchange among intelligent agents.", "year": 2023, "publicationdate": "2023-06-05", "externalids": {"DOI": "10.48550/arXiv.2306.03314"}, "doi_lower": "10.48550/arxiv.2306.03314"}
{"paper_id": 261101102, "title": "CGMI: Configurable General Multi-Agent Interaction Framework", "author_names": ["Jinxin Shi", "Jiabao Zhao", "Yilei Wang", "Xingjiao Wu", "Jiawen Li", "Liangbo He"], "venue": "arXiv.org", "abstract": "Benefiting from the powerful capabilities of large language models (LLMs), agents based on LLMs have shown the potential to address domain-specific tasks and emulate human behaviors. However, the content generated by these agents remains somewhat superficial, owing to their limited domain expertise and the absence of an effective cognitive architecture. To address this, we present the Configurable General Multi-Agent Interaction (CGMI) framework, designed to replicate human interactions in real-world scenarios. Specifically, we propose a tree-structured methodology for the assignment, detection, and maintenance of agent personality. Additionally, we designed a cognitive architecture equipped with a skill library based on the ACT* model, which contains memory, reflection, and planning modules. We have also integrated general agents to augment the virtual environment's realism. Using the CGMI framework, we simulated numerous classroom interactions between teacher and students. The experiments indicate that aspects such as the teaching methodology, curriculum, and student performance closely mirror real classroom settings. We will open source our work.", "year": 2023, "publicationdate": "2023-08-24", "externalids": {"DOI": "10.48550/arXiv.2308.12503"}, "doi_lower": "10.48550/arxiv.2308.12503"}
{"paper_id": 268554465, "title": "Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate", "author_names": ["Kai Xiong", "Xiao Ding", "Yixin Cao", "Ting Liu", "Bing Qin"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.11595"}, "doi_lower": "10.48550/arxiv.2305.11595"}
{"paper_id": 257757183, "title": "Hey Dona! Can you help me with student course registration?", "author_names": ["Vishesh Kalvakurthi", "A. Varde", "J. Jenq"], "venue": "arXiv.org", "abstract": "In this paper, we present a demo of an intelligent personal agent called Hey Dona (or just Dona) with virtual voice assistance in student course registration. It is a deployed project in the theme of AI for education. In this digital age with a myriad of smart devices, users often delegate tasks to agents. While pointing and clicking supersedes the erstwhile command-typing, modern devices allow users to speak commands for agents to execute tasks, enhancing speed and convenience. In line with this progress, Dona is an intelligent agent catering to student needs by automated, voice-operated course registration, spanning a multitude of accents, entailing task planning optimization, with some language translation as needed. Dona accepts voice input by microphone (Bluetooth, wired microphone), converts human voice to computer understandable language, performs query processing as per user commands, connects with the Web to search for answers, models task dependencies, imbibes quality control, and conveys output by speaking to users as well as displaying text, thus enabling human-AI interaction by speech cum text. It is meant to work seamlessly on desktops, smartphones etc. and in indoor as well as outdoor settings. To the best of our knowledge, Dona is among the first of its kind as an intelligent personal agent for voice assistance in student course registration. Due to its ubiquitous access for educational needs, Dona directly impacts AI for education. It makes a broader impact on smart city characteristics of smart living and smart people due to its contributions to providing benefits for new ways of living and assisting 21st century education, respectively.", "year": 2023, "publicationdate": "2023-03-21", "externalids": {"DOI": "10.48550/arXiv.2303.13548"}, "doi_lower": "10.48550/arxiv.2303.13548"}
{"paper_id": 258714781, "title": "Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback", "author_names": ["Shang-ling Hsu", "Raj Sanjay Shah", "Prathik Senthil", "Zahra Ashktorab", "Casey Dugan", "Werner Geyer", "Diyi Yang"], "venue": "Proc. ACM Hum. Comput. Interact.", "abstract": "Millions of users come to online peer counseling platforms to seek support. However, studies show that online peer support groups are not always as effective as expected, largely due to users' negative experiences with unhelpful counselors. Peer counselors are key to the success of online peer counseling platforms, but most often do not receive appropriate training. Hence, we introduce CARE: an AI-based tool to empower and train peer counselors through practice and feedback. Concretely, CARE helps diagnose which counseling strategies are needed in a given situation and suggests example responses to counselors during their practice sessions. Building upon the Motivational Interviewing framework, CARE utilizes large-scale counseling conversation data with text generation techniques to enable these functionalities. We demonstrate the efficacy of CARE by performing quantitative evaluations and qualitative user studies through simulated chats and semi-structured interviews, finding that CARE especially helps novice counselors in challenging situations. The code is available at https://github.com/SALT-NLP/CARE.", "year": 2023, "publicationdate": "2023-05-15", "externalids": {"DOI": "10.1145/3710993"}, "doi_lower": "10.1145/3710993"}
{"paper_id": 258865566, "title": "HuatuoGPT, towards Taming Language Model to Be a Doctor", "author_names": ["Hongbo Zhang", "Junying Chen", "Feng Jiang", "Fei Yu", "Zhihong Chen", "Jianquan Li", "Guimin Chen", "Xiangbo Wu", "Zhiyi Zhang", "Qingying Xiao", "Xiang Wan", "Benyou Wang", "Haizhou Li"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "In this paper, we present HuatuoGPT, a large language model (LLM) for medical consultation. The core recipe of HuatuoGPT is to leverage both \\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors} in the supervised fine-tuned stage. The responses of ChatGPT are usually detailed, well-presented and informative while it cannot perform like a doctor in many aspects, e.g. for integrative diagnosis. We argue that real-world data from doctors would be complementary to distilled data in the sense the former could tame a distilled language model to perform like doctors. To better leverage the strengths of both data, we train a reward model to align the language model with the merits that both data bring, following an RLAIF (reinforced learning from AI feedback) fashion. To evaluate and benchmark the models, we propose a comprehensive evaluation scheme (including automatic and manual metrics). Experimental results demonstrate that HuatuoGPT achieves state-of-the-art results in performing medical consultation among open-source LLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It is worth noting that by using additional real-world data and RLAIF, the distilled language model (i.e., HuatuoGPT) outperforms its teacher model ChatGPT in most cases. Our code, data, and models are publicly available at \\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is available at \\url{https://www.HuatuoGPT.cn/}.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.15075"}, "doi_lower": "10.48550/arxiv.2305.15075"}
{"paper_id": 260681932, "title": "Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue", "author_names": ["Songhua Yang", "Hanjia Zhao", "Senbin Zhu", "Guangyu Zhou", "Hongfei Xu", "Yuxiang Jia", "Hongying Zan"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Recent advances in Large Language Models (LLMs) have achieved remarkable breakthroughs in understanding and responding to user intents. However, their performance lag behind general use cases in some expertise domains, such as Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue data. These models lack the ability for doctor-like proactive inquiry and multi-turn comprehension and cannot align responses with experts' intentions. In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM that implements an entire training pipeline from continuous pre-training, SFT, to Reinforcement Learning from Human Feedback (RLHF). Additionally, we construct a Chinese multi-turn medical dialogue dataset of 70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly enhances the model's capability for complex dialogue and proactive inquiry initiation. We also define a refined annotation rule and evaluation criteria given the unique characteristics of the biomedical domain. Extensive experimental results show that Zhongjing outperforms baselines in various capacities and matches the performance of ChatGPT in some abilities, despite the 100x parameters. Ablation studies also demonstrate the contributions of each component: pre-training enhances medical knowledge, and RLHF further improves instruction-following ability and safety. Our code, datasets, and models are available at https://github.com/SupritYoung/Zhongjing.", "year": 2023, "publicationdate": "2023-08-07", "externalids": {"DOI": "10.48550/arXiv.2308.03549"}, "doi_lower": "10.48550/arxiv.2308.03549"}
{"paper_id": 265038681, "title": "A Virtual Conversational Agent for Teens with Autism Spectrum Disorder: Experimental Results and Design Lessons", "author_names": ["Mohammad Rafayet Ali", "S. Z. Razavi", "Raina Langevin", "A. A. Mamun", "Benjamin Kane", "Reza Rawassizadeh", "Lenhart K. Schubert", "Ehsan Hoque"], "venue": "International Conference on Intelligent Virtual Agents", "abstract": "We present the design of an online social skills development interface for teenagers with autism spectrum disorder (ASD). The interface is intended to enable private conversation practice anywhere, anytime using a web-browser. Users converse informally with a virtual agent, receiving feedback on nonverbal cues in realtime, and summary feedback. The prototype was developed in consultation with an expert UX designer, two psychologists, and a pediatrician. Using the data from 47 individuals, feedback and dialogue generation were automated using a hidden Markov model and a schema-driven dialogue manager capable of handling multi-topic conversations. We conducted a study with nine high-functioning ASD teenagers. Through a thematic analysis of post-experiment interviews, identified several key design considerations, notably: 1) Users should be fully briefed at the outset about the purpose and limitations of the system, to avoid unrealistic expectations. 2) An interface should incorporate positive acknowledgment of behavior change. 3) Realistic appearance of a virtual agent and responsiveness are important in engaging users. 4) Conversation personalization, for instance in prompting laconic users for more input and reciprocal questions, would help the teenagers engage for longer terms and increase the system's utility.", "year": 2020, "publicationdate": "2020-10-19", "externalids": {"DOI": "10.1145/3383652.3423900"}, "doi_lower": "10.1145/3383652.3423900"}
{"paper_id": 260387281, "title": "Multi-Turn Dialogue Agent as Sales' Assistant in Telemarketing", "author_names": ["Wanting Gao", "Xinyi Gao", "Yin Tang"], "venue": "IEEE International Joint Conference on Neural Network", "abstract": "The traditional telemarketing call centers are being hunched by significant human resources costs related to high workers' workload, as well as even higher turnover-rate, which are also proven to be impossible to be solved by Artificial Intelligence (AI) chatbot alone. In view of this dilemma, applying AI techniques to assist human salespeople in call centers by instantly analyzing the phone call between salesperson and customer, thus recommending a response for the salesperson, becomes a challenging yet applicable means. In this paper, we propose a multiturn dialogue agent based on Deep Attention Matching Network (DAM) with SimBERT, used as a data augmentation tool to efficiently improve the response selection accuracy. Experiments on a real-world telemarketing scenario dataset have confirmed its validity.", "year": 2023, "publicationdate": "2023-06-18", "externalids": {"DOI": "10.1109/IJCNN54540.2023.10192042"}, "doi_lower": "10.1109/ijcnn54540.2023.10192042"}
{"paper_id": 251765117, "title": "PEER: A Collaborative Language Model", "author_names": ["Timo Schick", "Jane Dwivedi-Yu", "Zhengbao Jiang", "F. Petroni", "Patrick Lewis", "Gautier Izacard", "Qingfei You", "Christoforos Nalmpantis", "Edouard Grave", "Sebastian Riedel"], "venue": "International Conference on Learning Representations", "abstract": "Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today's language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of self-training techniques for increasing the quality, amount and diversity of training data. This unlocks PEER's full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.", "year": 2022, "publicationdate": "2022-08-24", "externalids": {"DOI": "10.48550/arXiv.2208.11663"}, "doi_lower": "10.48550/arxiv.2208.11663"}
{"paper_id": 259924704, "title": "Does Collaborative Human-LM Dialogue Generation Help Information Extraction from Human Dialogues?", "author_names": ["Bo-Ru Lu", "Nikita Haduong", "Chia-Hsuan Lee", "Zeqiu Wu", "Hao Cheng", "Paul Koester", "J. Utke", "Tao Yu", "Noah A. Smith", "Mari Ostendorf"], "venue": "", "abstract": "The capabilities of pretrained language models have opened opportunities to explore new application areas, but applications involving human-human interaction are limited by the fact that most data is protected from public release for privacy reasons. Problem-solving human dialogues in real applications can be much more complex than existing Wizard-of-Oz collections, preventing successful domain transfer. To support information extraction (IE) for a private call center dataset, we introduce a human-in-the-loop dialogue generation framework capable of synthesizing realistic dialogues. In IE experiments with auto insurance call center dialogues, we observe 25\\% relative improvement in $F_1$ after augmenting a small set of real human conversations with synthetic data. We release code and our synthetic dataset to illustrate the complexity of real-world call center conversations and encourage development of complex dialogue datasets that are more representative of natural data.", "year": 2023, "publicationdate": "2023-07-13", "externalids": {}, "doi_lower": null}
{"paper_id": 259164559, "title": "AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn", "author_names": ["Difei Gao", "Lei Ji", "Luowei Zhou", "Kevin Lin", "Joya Chen", "Zihan Fan", "Mike Zheng Shou"], "venue": "arXiv.org", "abstract": "Recent research on Large Language Models (LLMs) has led to remarkable advancements in general NLP AI assistants. Some studies have further explored the use of LLMs for planning and invoking models or APIs to address more general multi-modal user queries. Despite this progress, complex visual-based tasks still remain challenging due to the diverse nature of visual tasks. This diversity is reflected in two aspects: 1) Reasoning paths. For many real-life applications, it is hard to accurately decompose a query simply by examining the query itself. Planning based on the specific visual content and the results of each step is usually required. 2) Flexible inputs and intermediate results. Input forms could be flexible for in-the-wild cases, and involves not only a single image or video but a mixture of videos and images, e.g., a user-view image with some reference videos. Besides, a complex reasoning process will also generate diverse multimodal intermediate results, e.g., video narrations, segmented video clips, etc. To address such general cases, we propose a multi-modal AI assistant, AssistGPT, with an interleaved code and language reasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate LLMs with various tools. Specifically, the Planner is capable of using natural language to plan which tool in Executor should do next based on the current reasoning progress. Inspector is an efficient memory manager to assist the Planner to feed proper visual information into a specific tool. Finally, since the entire reasoning process is complex and flexible, a Learner is designed to enable the model to autonomously explore and discover the optimal solution. We conducted experiments on A-OKVQA and NExT-QA benchmarks, achieving state-of-the-art results. Moreover, showcases demonstrate the ability of our system to handle questions far more complex than those found in the benchmarks.", "year": 2023, "publicationdate": "2023-06-14", "externalids": {"DOI": "10.48550/arXiv.2306.08640"}, "doi_lower": "10.48550/arxiv.2306.08640"}
{"paper_id": 260681572, "title": "SAPIEN: Affective Virtual Agents Powered by Large Language Models*", "author_names": ["Masum Hasan", "Cengiz Özel", "Sammy Potter", "E. Hoque"], "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)", "abstract": "In this demo paper, we introduce SAPIEN, a platform for high-fidelity virtual agents driven by large language models that can hold open domain conversations with users in 13 different languages, and display emotions through facial expressions and voice. The platform allows users to customize their virtual agent’s personality, background, and conversation premise, thus providing a rich, immersive interaction experience. Furthermore, after the virtual meeting, the user can choose to get the conversation analyzed and receive actionable feedback on their communication skills. This paper illustrates an overview of the platform and discusses the various application domains of this technology, ranging from entertainment to mental health, communication training, language learning, education, healthcare, and beyond. Additionally, we consider the ethical implications of such realistic virtual agent representations and the potential challenges in ensuring responsible use.", "year": 2023, "publicationdate": "2023-08-06", "externalids": {"DOI": "10.1109/ACIIW59127.2023.10388188"}, "doi_lower": "10.1109/aciiw59127.2023.10388188"}
{"paper_id": 250396732, "title": "Artificial empathy in marketing interactions: Bridging the human-AI gap in affective and social customer experience", "author_names": ["Y. Liu-Thompkins", "Shintaro Okazaki", "Hairong Li"], "venue": "Journal of the Academy of Marketing Science", "abstract": "Artificial intelligence (AI) continues to transform firm-customer interactions. However, current AI marketing agents are often perceived as cold and uncaring and can be poor substitutes for human-based interactions. Addressing this issue, this article argues that artificial empathy needs to become an important design consideration in the next generation of AI marketing applications. Drawing from research in diverse disciplines, we develop a systematic framework for integrating artificial empathy into AI-enabled marketing interactions. We elaborate on the key components of artificial empathy and how each component can be implemented in AI marketing agents. We further explicate and test how artificial empathy generates value for both customers and firms by bridging the AI-human gap in affective and social customer experience. Recognizing that artificial empathy may not always be desirable or relevant, we identify the requirements for artificial empathy to create value and deduce situations where it is unnecessary and, in some cases, harmful.", "year": 2022, "publicationdate": "2022-07-08", "externalids": {"DOI": "10.1007/s11747-022-00892-5"}, "doi_lower": "10.1007/s11747-022-00892-5"}
{"paper_id": 252815905, "title": "Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning", "author_names": ["A. Bakhtin", "David J. Wu", "Adam Lerer", "Jonathan Gray", "Athul Paul Jacob", "Gabriele Farina", "Alexander H. Miller", "Noam Brown"], "venue": "International Conference on Learning Representations", "abstract": "No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitation-learned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus. In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model.", "year": 2022, "publicationdate": "2022-10-11", "externalids": {"DOI": "10.48550/arXiv.2210.05492"}, "doi_lower": "10.48550/arxiv.2210.05492"}
{"paper_id": 253759631, "title": "Human-level play in the game of Diplomacy by combining language models with strategic reasoning", "author_names": ["A. Bakhtin", "Noam Brown", "Emily Dinan", "Gabriele Farina", "Colin Flaherty", "Daniel Fried", "Andrew Goff", "Jonathan Gray", "Hengyuan Hu", "Athul Paul Jacob", "Mo-jtaba Komeili", "Karthik Konath", "Minae Kwon", "Adam Lerer", "Mike Lewis", "Alexander H. Miller", "S. Mitts", "Adithya Renduchintala", "Stephen Roller", "Dirk Rowe", "Weiyan Shi", "Joe Spisak", "Alexander Wei", "David J. Wu", "Hugh Zhang", "Markus Zijlstra"], "venue": "Science", "abstract": "Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game. Description AI masters Diplomacy The game Diplomacy has been a major challenge for artificial intelligence (AI). Unlike other competitive games that AI has recently mastered, such as chess, Go, and poker, Diplomacy cannot be solved purely through self-play; it requires the development of an agent to understand other players’ motivations and perspectives and to use natural language to negotiate complex shared plans. The Meta Fundamental AI Research Diplomacy Team (FAIR) et al. developed an agent that is able to play the full natural language form of the game and demonstrates performance well above the human average in an online Diplomacy league. The present work has far-reaching implications for the development of cooperative AI and language models for communication with people, even when interactions involve a mixture of aligned and competing interests. —YS Artificial intelligence demonstrates human-level performance in the strategic board game Diplomacy.", "year": 2022, "publicationdate": "2022-11-22", "externalids": {"DOI": "10.1126/science.ade9097"}, "doi_lower": "10.1126/science.ade9097"}
{"paper_id": 258987554, "title": "Decision-Oriented Dialogue for Human-AI Collaboration", "author_names": ["Jessy Lin", "Nicholas Tomlin", "Jacob Andreas", "J. Eisner"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Abstract We describe a class of tasks called decision-oriented dialogues, in which AI assistants such as large language models (LMs) must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: Assistants can access and process large amounts of information, while users have preferences and constraints external to the system. For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach. We evaluate LMs in self-play and in collaboration with humans and find that they fall short compared to human assistants, achieving much lower rewards despite engaging in longer dialogues. We highlight a number of challenges models face in decision-oriented dialogues, ranging from goal-directed behavior to reasoning and optimization, and release our environments as a testbed for future work.", "year": 2023, "publicationdate": "2023-05-31", "externalids": {"DOI": "10.1162/tacl_a_00679"}, "doi_lower": "10.1162/tacl_a_00679"}
{"paper_id": 260681901, "title": "Quantifying the Impact of Large Language Models on Collective Opinion Dynamics", "author_names": ["Chao Li", "Xingye Su", "Haoying Han", "Cong Xue", "Chunmo Zheng", "C. Fan"], "venue": "arXiv.org", "abstract": "The process of opinion expression and exchange is a critical component of democratic societies. As people interact with large language models (LLMs) in the opinion shaping process different from traditional media, the impacts of LLMs are increasingly recognized and being concerned. However, the knowledge about how LLMs affect the process of opinion expression and exchange of social opinion networks is very limited. Here, we create an opinion network dynamics model to encode the opinions of LLMs, cognitive acceptability and usage strategies of individuals, and simulate the impact of LLMs on opinion dynamics in a variety of scenarios. The outcomes of the simulations inform about effective demand-oriented opinion network interventions. The results from this study suggested that the output opinion of LLMs has a unique and positive effect on the collective opinion difference. The marginal effect of cognitive acceptability on collective opinion formation is nonlinear and shows a decreasing trend. When people partially rely on LLMs, the exchange process of opinion becomes more intense and the diversity of opinion becomes more favorable. In fact, there is 38.6% more opinion diversity when people all partially rely on LLMs, compared to prohibiting the use of LLMs entirely. The optimal diversity of opinion was found when the fractions of people who do not use, partially rely on, and fully rely on LLMs reached roughly 4:12:1. Our experiments also find that introducing extra agents with opposite/neutral/random opinions, we can effectively mitigate the impact of biased/toxic output from LLMs. Our findings provide valuable insights into opinion dynamics in the age of LLMs, highlighting the need for customized interventions tailored to specific scenarios to address the drawbacks of improper output and use of LLMs.", "year": 2023, "publicationdate": "2023-08-07", "externalids": {"DOI": "10.48550/arXiv.2308.03313"}, "doi_lower": "10.48550/arxiv.2308.03313"}
{"paper_id": 270738192, "title": "Poisoned LangChain: Jailbreak LLMs by LangChain", "author_names": ["Ziqiu Wang", "Jun Liu", "Shengkai Zhang", "Yang Yang"], "venue": "arXiv.org", "abstract": "With the development of natural language processing (NLP), large language models (LLMs) are becoming increasingly popular. LLMs are integrating more into everyday life, raising public concerns about their security vulnerabilities. Consequently, the security of large language models is becoming critically important. Currently, the techniques for attacking and defending against LLMs are continuously evolving. One significant method type of attack is the jailbreak attack, which designed to evade model safety mechanisms and induce the generation of inappropriate content. Existing jailbreak attacks primarily rely on crafting inducement prompts for direct jailbreaks, which are less effective against large models with robust filtering and high comprehension abilities. Given the increasing demand for real-time capabilities in large language models, real-time updates and iterations of new knowledge have become essential. Retrieval-Augmented Generation (RAG), an advanced technique to compensate for the model's lack of new knowledge, is gradually becoming mainstream. As RAG enables the model to utilize external knowledge bases, it provides a new avenue for jailbreak attacks. In this paper, we conduct the first work to propose the concept of indirect jailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on this, we further design a novel method of indirect jailbreak attack, termed Poisoned-LangChain (PLC), which leverages a poisoned external knowledge base to interact with large language models, thereby causing the large models to generate malicious non-compliant dialogues.We tested this method on six different large language models across three major categories of jailbreak issues. The experiments demonstrate that PLC successfully implemented indirect jailbreak attacks under three different scenarios, achieving success rates of 88.56%, 79.04%, and 82.69% respectively.", "year": 2024, "publicationdate": "2024-06-26", "externalids": {"DOI": "10.48550/arXiv.2406.18122"}, "doi_lower": "10.48550/arxiv.2406.18122"}
{"paper_id": 11419044, "title": "VerifiedDSP: Verifying Digital Signal Processing Designs in Coq https://github.com/JeremyRubin/VerifiedDSP", "author_names": ["Jeremy Rubin"], "venue": "", "abstract": null, "year": 2015, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 279267209, "title": "Assessing the effectiveness of AI generated code in Improving Software Engineering Processes using GPT-Engineer with OpenAI GPT models", "author_names": ["Timothy Y. Wikedzi", "Mwanga Gladness"], "venue": "2025 IEEE/ACM Symposium on Software Engineering in the Global South (SEiGS)", "abstract": "This paper evaluates GPT-Engineer, an open-source AI-powered tool leveraging OpenAI’s models, other Large Language Models (LLMs), and flexible integrations to enhance software engineering workflows. We applied GPT-Engineer to document an API codebase using Swagger Documentation, showcasing the tool’s potential to streamline engineering processes, optimize resources, and address challenges such as workforce shortages. This study focused on assessing GPT-Engineer’s effectiveness in generating precise, reliable API documentation and its understanding of the underlying codebase by targeting specific Laravel controllers and models, without requiring manual annotations. The evaluation explored adaptability, prompt effectiveness, and cost-efficiency using the \"improve (-i)\" module within GPT-Engineer across various GPT models. Findings indicate that AI-driven tools like GPT-Engineer can mitigate resource constraints, enabling organizations in low-resource environments to produce high-quality software. Key results highlight the importance of prompt engineering strategies, effective cost management, and scalable integrations for maximizing the utility of AI solutions in resource-limited settings.", "year": 2025, "publicationdate": "2025-05-03", "externalids": {"DOI": "10.1109/SEiGS66664.2025.00013"}, "doi_lower": "10.1109/seigs66664.2025.00013"}
{"paper_id": 227305541, "title": "Playing Text-Based Games with Common Sense", "author_names": ["Sahith N. Dambekodi", "Spencer Frazier", "Prithviraj Ammanabrolu", "Mark O. Riedl"], "venue": "arXiv.org", "abstract": "Text based games are simulations in which an agent interacts with the world purely through natural language. They typically consist of a number of puzzles interspersed with interactions with common everyday objects and locations. Deep reinforcement learning agents can learn to solve these puzzles. However, the everyday interactions with the environment, while trivial for human players, present as additional puzzles to agents. We explore two techniques for incorporating commonsense knowledge into agents. Inferring possibly hidden aspects of the world state with either a commonsense inference model COMET, or a language model BERT. Biasing an agents exploration according to common patterns recognized by a language model. We test our technique in the 9to05 game, which is an extreme version of a text based game that requires numerous interactions with common, everyday objects in common, everyday scenarios. We conclude that agents that augment their beliefs about the world state with commonsense inferences are more robust to observational errors and omissions of common elements from text descriptions.", "year": 2020, "publicationdate": "2020-12-04", "externalids": {}, "doi_lower": null}
{"paper_id": 236087322, "title": "Pre-trained Language Models as Prior Knowledge for Playing Text-based Games", "author_names": ["Ishika Singh", "Gargi Singh", "Ashutosh Modi"], "venue": "Adaptive Agents and Multi-Agent Systems", "abstract": "Recently, text world games have been proposed to enable artificial agents to understand and reason about real-world scenarios. These text-based games are challenging for artificial agents, as it requires an understanding of and interaction using natural language in a partially observable environment. Agents observe the environment via textual descriptions designed to be challenging enough for even human players. Past approaches have not paid enough attention to the language understanding capability of the proposed agents. Typically, these approaches train from scratch, an agent that learns both textual representations and the gameplay online during training using a temporal loss function. Given the sample-inefficiency of RL approaches, it is inefficient to learn rich enough textual representations to be able to understand and reason using the textual observation in such a complicated game environment setting. In this paper, we improve the semantic understanding of the agent by proposing a simple RL with LM framework where we use transformer-based language models with Deep RL models. We perform a detailed study of our framework to demonstrate how our model outperforms all existing agents on the popular game, Zork1, to achieve a score of 44.7, which is 1.6 higher than the state-of-the-art model. Overall, our proposed approach outperforms 4 games out of the 14 text-based games, while performing comparable to the state-of-the-art models on the remaining games.", "year": 2021, "publicationdate": "2021-07-18", "externalids": {"DOI": "10.5555/3535850.3536091"}, "doi_lower": "10.5555/3535850.3536091"}
{"paper_id": 222125301, "title": "How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds", "author_names": ["Prithviraj Ammanabrolu", "Jack Urbanek", "Margaret Li", "Arthur Szlam", "Tim Rocktaschel", "J. Weston"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We seek to create agents that both act and communicate with other agents in pursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al. 2019)—a large-scale crowd-sourced fantasy text-game—with a dataset of quests. These contain natural language motivations paired with in-game goals and human demonstrations; completing a quest might require dialogue or actions (or both). We introduce a reinforcement learning system that (1) incorporates large-scale language modeling-based and commonsense reasoning-based pre-training to imbue the agent with relevant priors; and (2) leverages a factorized action space of action commands and dialogue, balancing between the two. We conduct zero-shot evaluations using held-out human expert demonstrations, showing that our agents are able to act consistently and talk naturally with respect to their motivations.", "year": 2020, "publicationdate": "2020-09-28", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.64"}, "doi_lower": "10.18653/v1/2021.naacl-main.64"}
{"paper_id": 232417812, "title": "Grounding Open-Domain Instructions to Automate Web Support Tasks", "author_names": ["N. Xu", "Sam Masling", "Michael Du", "Giovanni Campagna", "Larry Heck", "J. Landay", "M. Lam"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Grounding natural language instructions on the web to perform previously unseen tasks enables accessibility and automation. We introduce a task and dataset to train AI agents from open-domain, step-by-step instructions originally written for people. We build RUSS (Rapid Universal Support Service) to tackle this problem. RUSS consists of two models: First, a BERT-LSTM with pointers parses instructions to WebLang, a domain-specific language we design for grounding natural language on the web. Then, a grounding model retrieves the unique IDs of any webpage elements requested in the WebLang. RUSS may interact with the user through a dialogue (e.g. ask for an address) or execute a web operation (e.g. click a button) inside the web runtime. To augment training, we synthesize natural language instructions mapped to WebLang. Our dataset consists of 80 different customer service problems from help websites, with a total of 741 step-by-step instructions and their corresponding actions. RUSS achieves 76.7% end-to-end accuracy predicting agent actions from single instructions. It outperforms state-of-the-art models that directly map instructions to actions without WebLang. Our user study shows that RUSS is preferred by actual users over web navigation.", "year": 2021, "publicationdate": "2021-03-30", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.80"}, "doi_lower": "10.18653/v1/2021.naacl-main.80"}
{"paper_id": 258564769, "title": "Knowledge-enhanced Agents for Interactive Text Games", "author_names": ["P. Chhikara", "Jiarui Zhang", "Filip Ilievski", "Jonathan M Francis", "Kaixin Ma"], "venue": "International Conference on Knowledge Capture", "abstract": "Communication via natural language is a key aspect of machine intelligence, and it requires computational models to learn and reason about world concepts, with varying levels of supervision. Significant progress has been made on fully-supervised non-interactive tasks, such as question-answering and procedural text understanding. Yet, various sequential interactive tasks, as in text-based games, have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment. In this paper, we propose a knowledge-injection framework for improved functional grounding of agents in text-based games. Specifically, we consider two forms of domain knowledge that we inject into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment. Our framework supports two representative model classes: reinforcement learning agents and language model agents. Furthermore, we devise multiple injection strategies for the above domain knowledge types and agent architectures, including injection via knowledge graphs and augmentation of the existing input encoding strategies. We experiment with four models on the 10 tasks in the ScienceWorld text-based game environment, to illustrate the impact of knowledge injection on various model configurations and challenging task settings. Our findings provide crucial insights into the interplay between task properties, model architectures, and domain knowledge for interactive contexts.", "year": 2023, "publicationdate": "2023-05-08", "externalids": {"DOI": "10.1145/3587259.3627561"}, "doi_lower": "10.1145/3587259.3627561"}
{"paper_id": 259262077, "title": "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models", "author_names": ["Kaiyu Yang", "Aidan M. Swope", "Alex Gu", "Rahul Chalamala", "Peiyang Song", "Shixing Yu", "Saad Godil", "R. Prenger", "Anima Anandkumar"], "venue": "Neural Information Processing Systems", "abstract": "Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection: a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. Furthermore, we construct a new benchmark consisting of 98,734 theorems and proofs extracted from Lean's math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4. We thus provide the first set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research.", "year": 2023, "publicationdate": "2023-06-27", "externalids": {"DOI": "10.48550/arXiv.2306.15626"}, "doi_lower": "10.48550/arxiv.2306.15626"}
{"paper_id": 253259177, "title": "Evolutionary-scale prediction of atomic level protein structure with a language model", "author_names": ["Zeming Lin", "Halil Akin", "Roshan Rao", "Brian L. Hie", "Zhongkai Zhu", "Wenting Lu", "Nikita Smetanin", "Robert Verkuil", "Ori Kabeli", "Y. Shmueli", "Allan dos Santos Costa", "Maryam Fazel-Zarandi", "Tom Sercu", "Salvatore Candido", "Alexander Rives"], "venue": "bioRxiv", "abstract": "Artificial intelligence has the potential to open insight into the structure of proteins at the scale of evolution. It has only recently been possible to extend protein structure prediction to two hundred million cataloged proteins. Characterizing the structures of the exponentially growing billions of protein sequences revealed by large scale gene sequencing experiments would necessitate a break-through in the speed of folding. Here we show that direct inference of structure from primary sequence using a large language model enables an order of magnitude speed-up in high resolution structure prediction. Leveraging the insight that language models learn evolutionary patterns across millions of sequences, we train models up to 15B parameters, the largest language model of proteins to date. As the language models are scaled they learn information that enables prediction of the three-dimensional structure of a protein at the resolution of individual atoms. This results in prediction that is up to 60x faster than state-of-the-art while maintaining resolution and accuracy. Building on this, we present the ESM Metage-nomic Atlas. This is the first large-scale structural characterization of metagenomic proteins, with more than 617 million structures. The atlas reveals more than 225 million high confidence predictions, including millions whose structures are novel in comparison with experimentally determined structures, giving an unprecedented view into the vast breadth and diversity of the structures of some of the least understood proteins on earth.", "year": 2022, "publicationdate": "2022-12-21", "externalids": {"DOI": "10.1126/science.ade2574"}, "doi_lower": "10.1126/science.ade2574"}
{"paper_id": 237747003, "title": "Chemformer: a pre-trained transformer for computational chemistry", "author_names": ["Ross Irwin", "Spyridon Dimitriadis", "Jiazhen He", "E. Bjerrum"], "venue": "Machine Learning: Science and Technology", "abstract": "Transformer models coupled with a simplified molecular line entry system (SMILES) have recently proven to be a powerful combination for solving challenges in cheminformatics. These models, however, are often developed specifically for a single application and can be very resource-intensive to train. In this work we present the Chemformer model—a Transformer-based model which can be quickly applied to both sequence-to-sequence and discriminative cheminformatics tasks. Additionally, we show that self-supervised pre-training can improve performance and significantly speed up convergence on downstream tasks. On direct synthesis and retrosynthesis prediction benchmark datasets we publish state-of-the-art results for top-1 accuracy. We also improve on existing approaches for a molecular optimisation task and show that Chemformer can optimise on multiple discriminative tasks simultaneously. Models, datasets and code will be made available after publication.", "year": 2021, "publicationdate": "2021-07-15", "externalids": {"DOI": "10.1088/2632-2153/ac3ffb"}, "doi_lower": "10.1088/2632-2153/ac3ffb"}
{"paper_id": 253254903, "title": "Learning to Solve Voxel Building Embodied Tasks from Pixels and Natural Language Instructions", "author_names": ["Alexey Skrynnik", "Z. Volovikova", "Marc-Alexandre Côté", "Anton Voronov", "Artem Zholus", "Negar Arabzadeh", "Shrestha Mohanty", "Milagro Teruel", "A. Awadallah", "A. Panov", "M. Burtsev", "Julia Kiseleva"], "venue": "arXiv.org", "abstract": "The adoption of pre-trained language models to generate action plans for embodied agents is a promising research strategy. However, execution of instructions in real or simulated environments requires verification of the feasibility of actions as well as their relevance to the completion of a goal. We propose a new method that combines a language model and reinforcement learning for the task of building objects in a Minecraft-like environment according to the natural language instructions. Our method first generates a set of consistently achievable sub-goals from the instructions and then completes associated sub-tasks with a pre-trained RL policy. The proposed method formed the RL baseline at the IGLU 2022 competition.", "year": 2022, "publicationdate": "2022-11-01", "externalids": {"DOI": "10.48550/arXiv.2211.00688"}, "doi_lower": "10.48550/arxiv.2211.00688"}
{"paper_id": 220364294, "title": "Scaling Imitation Learning in Minecraft", "author_names": ["Artemij Amiranashvili", "Nicolai Dorka", "Wolfram Burgard", "V. Koltun", "T. Brox"], "venue": "arXiv.org", "abstract": "Imitation learning is a powerful family of techniques for learning sensorimotor coordination in immersive environments. We apply imitation learning to attain state-of-the-art performance on hard exploration problems in the Minecraft environment. We report experiments that highlight the influence of network architecture, loss function, and data augmentation. An early version of our approach reached second place in the MineRL competition at NeurIPS 2019. Here we report stronger results that can be used as a starting point for future competition entries and related research. Our code is available at this https URL.", "year": 2020, "publicationdate": "2020-07-06", "externalids": {}, "doi_lower": null}
{"paper_id": 181218387, "title": "Reclaiming mind and society in mind", "author_names": ["L. Slonimsky"], "venue": "", "abstract": null, "year": 2012, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 53727498, "title": "Multi-Agent systems: an introduction", "author_names": ["M. Wooldridge"], "venue": "", "abstract": null, "year": 2001, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 1129799, "title": "KQML as an agent communication language", "author_names": ["Timothy W. Finin", "R. Fritzson", "D. McKay", "R. McEntire"], "venue": "International Conference on Information and Knowledge Management", "abstract": null, "year": 1994, "publicationdate": "1994-11-29", "externalids": {"DOI": "10.1145/191246.191322"}, "doi_lower": "10.1145/191246.191322"}
{"paper_id": 266649637, "title": "Overview on reinforcement learning of multi-agent game", "author_names": ["Wenrui Zou"], "venue": "Journal of Physics: Conference Series", "abstract": "Game intelligence is an emerging hot topic in the field of artificial intelligence in recent years, and multi-agent learning is a frontier topic in the field of the intelligent game, which has a huge development prospect in all fields. This paper introduces the origin of reinforcement learning (RL) from the law of effect in animal experimental psychology and the optimization theory of optimal control. Then, the author describes the systematic composition of multi-agent reinforcement learning (MARL), and summarizes the classification of its research methods. The existing problems of MARL are discussed from three aspects: non-stationarity of the environment, partial observability, and the dimensional explosion problem. Finally, an outlook on the future is given based on the current development status of MARL and the important and difficult issues in the research field.", "year": 2023, "publicationdate": "2023-12-01", "externalids": {"DOI": "10.1088/1742-6596/2646/1/012021"}, "doi_lower": "10.1088/1742-6596/2646/1/012021"}
{"paper_id": 155433616, "title": "The Wealth of Nations Rediscovered: Introduction: The Wealth of Nations and National Wealth", "author_names": ["R. Wright"], "venue": "", "abstract": null, "year": 2002, "publicationdate": null, "externalids": {"DOI": "10.1017/CBO9780511550010.002"}, "doi_lower": "10.1017/cbo9780511550010.002"}
{"paper_id": 279000557, "title": "Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration", "author_names": ["Zhenhailong Wang", "Shaoguang Mao", "Wenshan Wu", "Tao Ge", "Furu Wei", "Heng Ji"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2307.05300"}, "doi_lower": "10.48550/arxiv.2307.05300"}
{"paper_id": 38025515, "title": "Theory of Games and Economic Behavior (60th-Anniversary Edition)", "author_names": ["J. Neumann", "O. Morgenstern"], "venue": "", "abstract": null, "year": 2007, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 10737367, "title": "Multiagent systems: algorithmic, game-theoretic, and logical foundations by Y. Shoham and K. Leyton-Brown Cambridge University Press, 2008", "author_names": ["H. Aziz"], "venue": "SIGA", "abstract": null, "year": 2010, "publicationdate": "2010-03-01", "externalids": {"DOI": "10.1145/1753171.1753181"}, "doi_lower": "10.1145/1753171.1753181"}
{"paper_id": 200728524, "title": "Chih-Hsiung Hsu", "author_names": ["黃瑜萍"], "venue": "", "abstract": null, "year": 2010, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 205261034, "title": "Mastering the game of Go without human knowledge", "author_names": ["David Silver", "Julian Schrittwieser", "K. Simonyan", "Ioannis Antonoglou", "Aja Huang", "A. Guez", "T. Hubert", "Lucas baker", "Matthew Lai", "Adrian Bolton", "Yutian Chen", "T. Lillicrap", "Fan Hui", "L. Sifre", "George van den Driessche", "T. Graepel", "D. Hassabis"], "venue": "Nature", "abstract": null, "year": 2017, "publicationdate": "2017-10-19", "externalids": {"DOI": "10.1038/nature24270"}, "doi_lower": "10.1038/nature24270"}
{"paper_id": 2454882, "title": "Deal or No Deal? End-to-End Learning of Negotiation Dialogues", "author_names": ["M. Lewis", "Denis Yarats", "Yann Dauphin", "Devi Parikh", "Dhruv Batra"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other’s reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available.", "year": 2017, "publicationdate": "2017-06-01", "externalids": {"DOI": "10.18653/v1/D17-1259"}, "doi_lower": "10.18653/v1/d17-1259"}
{"paper_id": 232404883, "title": "Alignment of Language Agents", "author_names": ["Zachary Kenton", "Tom Everitt", "Laura Weidinger", "Iason Gabriel", "Vladimir Mikulik", "G. Irving"], "venue": "arXiv.org", "abstract": "For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.", "year": 2021, "publicationdate": "2021-03-26", "externalids": {}, "doi_lower": null}
{"paper_id": 257124039, "title": "Digitization of healthcare sector: A study on privacy and security concerns", "author_names": ["Metty Paul", "Leandros A. Maglaras", "M. Ferrag", "Iman M. Almomani"], "venue": "ICT express", "abstract": null, "year": 2023, "publicationdate": "2023-02-01", "externalids": {"DOI": "10.1016/j.icte.2023.02.007"}, "doi_lower": "10.1016/j.icte.2023.02.007"}
{"paper_id": 46396818, "title": "Interactional Feedback and the Impact of Attitude and Motivation on Noticing L2 Form", "author_names": ["Mohammad Amin Bassiri"], "venue": "", "abstract": "A number of second language (L2) studies have recently discovered the positive impacts of interactional feedback (IF) in L2 development by connecting the underlying processes such as noticing. The current study followed a two-fold purpose: first to examine the impact of IF on noticing question forms in Iranian L2 classroom context and secondly to investigate the possible effects of motivation and attitude on noting IF. IF was provided to experimental group learners in response to their production problems with question forms. Learners’ noticing was assessed through on-line learning journals, introspective comments while checking with audiotapes, and questionnaire responses. Learners’ motivation was also assessed using Gardner’s (1996) attitude/motivation test. The results point to a positive relationship between IF in the classroom and the learners’ reports about question forms of English. The results also confirmed the existence of a positive correlation between motivation and noticing.", "year": 2011, "publicationdate": "2011-11-27", "externalids": {"DOI": "10.5539/ELLS.V1N2P61"}, "doi_lower": "10.5539/ells.v1n2p61"}
{"paper_id": 8477711, "title": "Approaching the Symbol Grounding Problem with Probabilistic Graphical Models", "author_names": ["Stefanie Tellex", "T. Kollar", "Steven Dickerson", "Matthew R. Walter", "A. Banerjee", "S. Teller", "N. Roy"], "venue": "The AI Magazine", "abstract": "n order for robots to engage in dialog with human teammates, they must have the ability to map between words in the language and aspects of the external world. A solution to this symbol grounding problem (Harnad, 1990) would enable a robot to interpret commands such as “Drive over to receiving and pick up the tire pallet.” In this article we describe several of our results that use probabilistic inference to address the symbol grounding problem. Our speciﬁc approach is to develop models that factor according to the linguistic structure of a command. We ﬁrst describe an early result, a generative model that factors according to the sequential structure of language, and then discuss our new framework, generalized grounding graphs (G3). The G3 framework dynamically instantiates a probabilistic graphical model for a natural language input, enabling a mapping between words in language and concrete objects, places, paths and events in the external world. We report on corpus-based experiments where the robot is able to learn and use word meanings in three real-world tasks: indoor navigation, spatial language video retrieval, and mobile manipulation.", "year": 2011, "publicationdate": "2011-12-16", "externalids": {"DOI": "10.1609/aimag.v32i4.2384"}, "doi_lower": "10.1609/aimag.v32i4.2384"}
{"paper_id": 1658890, "title": "Learning to Parse Natural Language Commands to a Robot Control System", "author_names": ["Cynthia Matuszek", "E. Herbst", "Luke Zettlemoyer", "D. Fox"], "venue": "International Symposium on Experimental Robotics", "abstract": null, "year": 2012, "publicationdate": null, "externalids": {"DOI": "10.1007/978-3-319-00065-7_28"}, "doi_lower": "10.1007/978-3-319-00065-7_28"}
{"paper_id": 2921786, "title": "Gated-Attention Architectures for Task-Oriented Language Grounding", "author_names": ["Devendra Singh Chaplot", "Kanthashree Mysore Sathyendra", "Rama Kumar Pasumarthi", "Dheeraj Rajagopal", "R. Salakhutdinov"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. This problem is called task-oriented language grounding. We propose an end-to-end trainable neural architecture for task-oriented language grounding in 3D environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input. The proposed model combines the image and text representations using a Gated-Attention mechanism and learns a policy to execute the natural language instruction using standard reinforcement and imitation learning methods. We show the effectiveness of the proposed model on unseen instructions as well as unseen maps, both quantitatively and qualitatively. We also introduce a novel environment based on a 3D game engine to simulate the challenges of task-oriented language grounding over a rich set of instructions and environment states.", "year": 2017, "publicationdate": "2017-06-22", "externalids": {"DOI": "10.1609/aaai.v32i1.11832"}, "doi_lower": "10.1609/aaai.v32i1.11832"}
{"paper_id": 164019, "title": "Dialogue Learning With Human-In-The-Loop", "author_names": ["Jiwei Li", "Alexander H. Miller", "S. Chopra", "Marc'Aurelio Ranzato", "J. Weston"], "venue": "International Conference on Learning Representations", "abstract": "An important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes. Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime. Finally, real experiments with Mechanical Turk validate the approach.", "year": 2016, "publicationdate": "2016-11-04", "externalids": {}, "doi_lower": null}
{"paper_id": 497108, "title": "Learning a Neural Semantic Parser from User Feedback", "author_names": ["Srini Iyer", "Ioannis Konstas", "Alvin Cheung", "Jayant Krishnamurthy", "Luke Zettlemoyer"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves over time based on user feedback, and requires minimal intervention. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers. Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch.", "year": 2017, "publicationdate": "2017-04-27", "externalids": {"DOI": "10.18653/v1/P17-1089"}, "doi_lower": "10.18653/v1/p17-1089"}
{"paper_id": 2017135, "title": "Dialog-based Language Learning", "author_names": ["J. Weston"], "venue": "Neural Information Processing Systems", "abstract": "A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of (Weston et al., 2015) and large-scale question answering from (Dodge et al., 2015). We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.", "year": 2016, "publicationdate": "2016-04-20", "externalids": {}, "doi_lower": null}
{"paper_id": 251371589, "title": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage", "author_names": ["Kurt Shuster", "Jing Xu", "M. Komeili", "Da Ju", "Eric Michael Smith", "Stephen Roller", "Megan Ung", "Moya Chen", "Kushal Arora", "Joshua Lane", "Morteza Behrooz", "W.K.F. Ngan", "Spencer Poff", "Naman Goyal", "Arthur Szlam", "Y-Lan Boureau", "M. Kambadur", "J. Weston"], "venue": "arXiv.org", "abstract": "We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction.", "year": 2022, "publicationdate": "2022-08-05", "externalids": {"DOI": "10.48550/arXiv.2208.03188"}, "doi_lower": "10.48550/arxiv.2208.03188"}
{"paper_id": 248069330, "title": "Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision", "author_names": ["Wanyu Du", "Zae Myung Kim", "Vipul Raheja", "Dhruv Kumar", "Dongyeop Kang"], "venue": "IN2WRITING", "abstract": "Revision is an essential part of the human writing process. It tends to be strategic, adaptive, and, more importantly, iterative in nature. Despite the success of large language models on text revision tasks, they are limited to non-iterative, one-shot revisions. Examining and evaluating the capability of large language models for making continuous revisions and collaborating with human writers is a critical step towards building effective writing assistants. In this work, we present a human-in-the-loop iterative text revision system, Read, Revise, Repeat (R3), which aims at achieving high quality text revisions with minimal human efforts by reading model-generated revisions and user feedbacks, revising documents, and repeating human-machine interactions. In R3, a text revision model provides text editing suggestions for human writers, who can accept or reject the suggested edits. The accepted edits are then incorporated into the model for the next iteration of document revision. Writers can therefore revise documents iteratively by interacting with the system and simply accepting/rejecting its suggested edits until the text revision model stops making further revisions or reaches a predefined maximum number of revisions. Empirical experiments show that R3 can generate revisions with comparable acceptance rate to human writers at early revision depths, and the human-machine interaction can get higher quality revisions with fewer iterations and edits. The collected human-model interaction dataset and system code are available at https://github.com/vipulraheja/IteraTeR. Our system demonstration is available at https://youtu.be/lK08tIpEoaE.", "year": 2022, "publicationdate": "2022-04-07", "externalids": {"DOI": "10.18653/v1/2022.in2writing-1.14"}, "doi_lower": "10.18653/v1/2022.in2writing-1.14"}
{"paper_id": 4950709, "title": "Can Neural Machine Translation be Improved with User Feedback?", "author_names": ["Julia Kreutzer", "Shahram Khadivi", "E. Matusov", "S. Riezler"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We present the first real-world application of methods for improving neural machine translation (NMT) with human reinforcement, based on explicit and implicit user feedback collected on the eBay e-commerce platform. Previous work has been confined to simulation experiments, whereas in this paper we work with real logged feedback for offline bandit learning of NMT parameters. We conduct a thorough analysis of the available explicit user judgments—five-star ratings of translation quality—and show that they are not reliable enough to yield significant improvements in bandit learning. In contrast, we successfully utilize implicit task-based feedback collected in a cross-lingual search task to improve task-specific and machine translation quality metrics.", "year": 2018, "publicationdate": "2018-04-16", "externalids": {"DOI": "10.18653/v1/N18-3012"}, "doi_lower": "10.18653/v1/n18-3012"}
{"paper_id": 21736196, "title": "DialSQL: Dialogue Based Structured Query Generation", "author_names": ["Izzeddin Gur", "Semih Yavuz", "Yu Su", "Xifeng Yan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The recent advance in deep learning and semantic parsing has significantly improved the translation accuracy of natural language questions to structured queries. However, further improvement of the existing approaches turns out to be quite challenging. Rather than solely relying on algorithmic innovations, in this work, we introduce DialSQL, a dialogue-based structured query generation framework that leverages human intelligence to boost the performance of existing algorithms via user interaction. DialSQL is capable of identifying potential errors in a generated SQL query and asking users for validation via simple multi-choice questions. User feedback is then leveraged to revise the query. We design a generic simulator to bootstrap synthetic training dialogues and evaluate the performance of DialSQL on the WikiSQL dataset. Using SQLNet as a black box query generation tool, DialSQL improves its performance from 61.3% to 69.0% using only 2.4 validation questions per dialogue.", "year": 2018, "publicationdate": "2018-07-01", "externalids": {"DOI": "10.18653/v1/P18-1124"}, "doi_lower": "10.18653/v1/p18-1124"}
{"paper_id": 204509379, "title": "Model-based Interactive Semantic Parsing: A Unified Framework and A Text-to-SQL Case Study", "author_names": ["Ziyu Yao", "Yu Su", "Huan Sun", "Wen-tau Yih"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "As a promising paradigm, interactive semantic parsing has shown to improve both semantic parsing accuracy and user confidence in the results. In this paper, we propose a new, unified formulation of the interactive semantic parsing problem, where the goal is to design a model-based intelligent agent. The agent maintains its own state as the current predicted semantic parse, decides whether and where human intervention is needed, and generates a clarification question in natural language. A key part of the agent is a world model: it takes a percept (either an initial question or subsequent feedback from the user) and transitions to a new state. We then propose a simple yet remarkably effective instantiation of our framework, demonstrated on two text-to-SQL datasets (WikiSQL and Spider) with different state-of-the-art base semantic parsers. Compared to an existing interactive semantic parsing approach that treats the base parser as a black box, our approach solicits less user feedback but yields higher run-time accuracy.", "year": 2019, "publicationdate": "2019-10-01", "externalids": {"DOI": "10.18653/v1/D19-1547"}, "doi_lower": "10.18653/v1/d19-1547"}
{"paper_id": 152282449, "title": "Improving Natural Language Interaction with Robots Using Advice", "author_names": ["Nikhil Mehta", "Dan Goldwasser"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Over the last few years, there has been growing interest in learning models for physically grounded language understanding tasks, such as the popular blocks world domain. These works typically view this problem as a single-step process, in which a human operator gives an instruction and an automated agent is evaluated on its ability to execute it. In this paper we take the first step towards increasing the bandwidth of this interaction, and suggest a protocol for including advice, high-level observations about the task, which can help constrain the agent’s prediction. We evaluate our approach on the blocks world task, and show that even simple advice can help lead to significant performance improvements. To help reduce the effort involved in supplying the advice, we also explore model self-generated advice which can still improve results.", "year": 2019, "publicationdate": "2019-05-12", "externalids": {"DOI": "10.18653/v1/N19-1195"}, "doi_lower": "10.18653/v1/n19-1195"}
{"paper_id": 232379973, "title": "NL-EDIT: Correcting Semantic Parse Errors through Natural Language Interaction", "author_names": ["Ahmed Elgohary", "Christopher Meek", "Matthew Richardson", "Adam Fourney", "Gonzalo A. Ramos", "Ahmed Hassan Awadallah"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We study semantic parsing in an interactive setting in which users correct errors with natural language feedback. We present NL-EDIT, a model for interpreting natural language feedback in the interaction context to generate a sequence of edits that can be applied to the initial parse to correct its errors. We show that NL-EDIT can boost the accuracy of existing text-to-SQL parsers by up to 20% with only one turn of correction. We analyze the limitations of the model and discuss directions for improvement and evaluation. The code and datasets used in this paper are publicly available at http://aka.ms/NLEdit.", "year": 2021, "publicationdate": "2021-03-23", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.444"}, "doi_lower": "10.18653/v1/2021.naacl-main.444"}
{"paper_id": 248666081, "title": "Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback", "author_names": ["Niket Tandon", "Aman Madaan", "Peter Clark", "Yiming Yang"], "venue": "NAACL-HLT", "abstract": "Large language models (LMs), while powerful, are not immune to mistakes, but can be difficult to retrain. Our goal is for an LM to continue to improve after deployment, without retraining, using feedback from the user. Our approach pairs an LM with (i) a growing memory of cases where the user identified an output error and provided general feedback on how to correct it (ii) a corrector model, trained to translate this general feedback into specific edits to repair the model output. Given a new, unseen input, our model can then use feedback from similar, past cases to repair output errors that may occur. We instantiate our approach using an existing, fixed model for script generation, that takes a goal (e.g.,\"bake a cake\") and generates a partially ordered sequence of actions to achieve that goal, sometimes containing errors. Our memory-enhanced system, FBNet, learns to apply user feedback to repair such errors (up to 30 points improvement), while making a start at avoiding similar past mistakes on new, unseen examples (up to 7 points improvement in a controlled setting). This is a first step towards strengthening deployed models, potentially broadening their utility. Our code and data is available at https://github.com/allenai/interscript/.", "year": 2021, "publicationdate": "2021-12-16", "externalids": {"DOI": "10.18653/v1/2022.findings-naacl.26"}, "doi_lower": "10.18653/v1/2022.findings-naacl.26"}
{"paper_id": 257805110, "title": "Training Language Models with Language Feedback at Scale", "author_names": ["J'er'emy Scheurer", "Jon Ander Campos", "Tomasz Korbak", "Jun Shern Chan", "Angelica Chen", "Kyunghyun Cho", "Ethan Perez"], "venue": "arXiv.org", "abstract": "Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate ILF's effectiveness on a carefully-controlled toy task and a realistic summarization task. Our experiments demonstrate that large language models accurately incorporate feedback and that finetuning with ILF scales well with the dataset size, even outperforming finetuning on human summaries. Learning from both language and comparison feedback outperforms learning from each alone, achieving human-level summarization performance.", "year": 2023, "publicationdate": "2023-03-28", "externalids": {"DOI": "10.48550/arXiv.2303.16755"}, "doi_lower": "10.48550/arxiv.2303.16755"}
{"paper_id": 251371461, "title": "Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback", "author_names": ["Jing Xu", "Megan Ung", "M. Komeili", "Kushal Arora", "Y-Lan Boureau", "J. Weston"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Frozen models trained to mimic static datasets can never improve their performance. Models that can employ internet-retrieval for up-to-date information and obtain feedback from humans during deployment provide the promise of both adapting to new information, and improving their performance. In this work we study how to improve internet-driven conversational skills in such a learning framework. We collect deployment data, which we make publicly available, of human interactions, and collect various types of human feedback – including binary quality measurements, free-form text feedback, and fine-grained reasons for failure. We then study various algorithms for improving from such feedback, including standard supervised learning, rejection sampling, model-guiding and reward-based learning, in order to make recommendations on which type of feed- back and algorithms work best. We find the recently introduced DIRECTOR model (Arora et al., 2022) shows significant improvements over other existing approaches.", "year": 2022, "publicationdate": "2022-08-05", "externalids": {"DOI": "10.48550/arXiv.2208.03270"}, "doi_lower": "10.48550/arxiv.2208.03270"}
{"paper_id": 259144807, "title": "Human-in-the-Loop through Chain-of-Thought", "author_names": ["Zefan Cai", "Baobao Chang", "Wenjuan Han"], "venue": "arXiv.org", "abstract": "While the emergence of powerful language models along with Chain-of-thought prompting has made automation more and more omnipresent, it sometimes demonstrates its weakness in long-term or multi-step logical reasoning. For example, users don't always get desirable answers for complex mathematical problems without human involvement. Against this background, we present the Manual Correction System (MCS) -- a human-in-the-loop system enhanced by Chain-of-Thought prompting, which explores how manual correction of sub-logics in rationales can improve LLM's reasoning performance. Moving one step forward, considering a system with human-in-the-loop involves more than having humans improve performance but also controlling the cost. Therefore, we post a Cost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on classical economics theory to analyze, quantify and balance the utility and the corresponding cost. We conduct experiments of MCS and CAMLOP with twelve datasets. A significant advantage w.r.t cost and utility proves its superiority over strong baselines.", "year": 2023, "publicationdate": "2023-06-10", "externalids": {"DOI": "10.48550/arXiv.2306.07932"}, "doi_lower": "10.48550/arxiv.2306.07932"}
{"paper_id": 58007087, "title": "Learning from Dialogue after Deployment: Feed Yourself, Chatbot!", "author_names": ["Braden Hancock", "Antoine Bordes", "Pierre-Emmanuel Mazaré", "J. Weston"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed, leaving a vast store of potential training signal untapped. In this work, we propose the self-feeding chatbot, a dialogue agent with the ability to extract new training examples from the conversations it participates in. As our agent engages in conversation, it also estimates user satisfaction in its responses. When the conversation appears to be going well, the user’s responses become new training examples to imitate. When the agent believes it has made a mistake, it asks for feedback; learning to predict the feedback that will be given improves the chatbot’s dialogue abilities further. On the PersonaChat chit-chat dataset with over 131k training examples, we find that learning from dialogue with a self-feeding chatbot significantly improves performance, regardless of the amount of traditional supervision.", "year": 2019, "publicationdate": "2019-01-16", "externalids": {"DOI": "10.18653/v1/P19-1358"}, "doi_lower": "10.18653/v1/p19-1358"}
{"paper_id": 258291842, "title": "Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback", "author_names": ["Nikhil Mehta", "Milagro Teruel", "Patricio Figueroa Sanz", "Xinwei Deng", "A. Awadallah", "Julia Kiseleva"], "venue": "Findings", "abstract": "Many approaches to Natural Language Processing tasks often treat them as single-step problems, where an agent receives an instruction, executes it, and is evaluated based on the final outcome. However, language is inherently interactive, as evidenced by the back-and-forth nature of human conversations. In light of this, we posit that human-AI collaboration should also be interactive, with humans monitoring the work of AI agents and providing feedback that the agent can understand and utilize. Further, the AI agent should be able to detect when it needs additional information and proactively ask for help. Enabling this scenario would lead to more natural, efficient, and engaging human-AI collaboration.In this paper, we investigate these directions using the challenging task established by the IGLU competition, an interactive grounded language understanding task in a MineCraft-like world. We delve into multiple types of help players can give to the AI to guide it and analyze the impact of this help on behavior, resulting in performance improvements and an end-to-end interactive system.", "year": 2023, "publicationdate": "2023-04-21", "externalids": {"DOI": "10.48550/arXiv.2304.10750"}, "doi_lower": "10.48550/arxiv.2304.10750"}
{"paper_id": 212554350, "title": "Patricc: A Platform for Triadic Interaction with Changeable Characters", "author_names": ["O. Gvirsman", "Yaacov Koren", "Tal Norman", "Goren Gordon"], "venue": "IEEE/ACM International Conference on Human-Robot Interaction", "abstract": "While social robots for education are slowly being integrated in many scenarios, ranging from higher-education, through elementary school and kindergarten, the use case of robots for toddlers in their homes has not gained much attention. In this contribution, we introduce Patricc, a robotic platform that is specifically designed for toddler-parent-robot triadic interaction. It addresses the unique challenges of this age group, namely, desire for continuous physical interaction and novelty. Patricc’s unique design enables changing characters by using dress-able puppets over a 3D-printed skeleton and the use of physical props. A novel authoring tool enables robot behavior and content creation by non-programmers. We conducted an evaluation study with 18 parent-toddler pairs and compared Patricc to similar tablet-based interactions. Our quantitative and qualitative analyses show that Patricc promotes significantly more triadic interaction, measured by video-coded gaze, compared to the tablet and that parents indeed perceive the interaction as triadic. Furthermore, there was no novelty-induced significant change in task-oriented behaviors, when toddlers interacted with two different characters consecutively. Finally, parents pointed out the benefits of changeable puppet-like characters over tablets and the appropriateness of the platform for the target age-group. These results suggest that Patricc can serve as the first gateway of toddlers to the emerging world of social robots. CCS CONCEPTS • Human-centered computing → Empirical studies in HCI; • Applied computing → Interactive learning environments; •Computer systems organization → Robotic autonomy. Acm Reference Format: Omer Gvirsman, Yaacov Koren, Tal Norman, and Goren Gordon. 2020. Patricc: A Platform for Triadic Interaction with Changeable Characters. In Proceedings of the 2020 ACM/IEEE International Conference on HumanRobot Interaction (HRI ’20), March 23–26, 2020, Cambridge, United Kingdom. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3319502.3374792", "year": 2020, "publicationdate": "2020-03-06", "externalids": {"DOI": "10.1145/3319502.3374792"}, "doi_lower": "10.1145/3319502.3374792"}
{"paper_id": 49409649, "title": "What might get in the way: Barriers to the use of apps for depression", "author_names": ["Colleen Stiles-Shields", "E. Montague", "E. Lattie", "M. Kwasny", "D. Mohr"], "venue": "Digital Health", "abstract": "Objective Smartphones are being used with increasing frequency to deliver behavioral interventions for depression via apps. However, barriers specific to using an app for depression are poorly defined. The purpose of the current study is to identify barriers to the use of a mobile app to deliver treatment for depression. Secondarily, design implications will be provided based upon identified barriers. Method A card sorting task that ranked and grouped barriers to the use of apps for depression was completed. Participants first completed a card sorting task identifying barriers to face-to-face treatment, as a primer to identification of treatment barriers. The sample consisted of those above (n = 9) and below (n = 11) the threshold for a referral to psychotherapy, to capture anticipated barriers for likely end users. Cluster analyses were conducted to analyze the card sorting data. Multiple analyses were conducted to identify: 1) the most important barriers, and 2) how consistently barriers were ranked as important. Result The card sorting task identified a number of primary barriers to the use of apps for depression treatment, including concerns over intervention efficacy, app functioning, privacy, cost, and lack of guidance and tailored feedback. The top face-to-face treatment barrier was cost, overlapping with mobile barriers. Conclusion This study identified perceived barriers to the use of mobile treatment apps. Identification of barriers implicates design recommendations for apps for depression.", "year": 2017, "publicationdate": "2017-06-01", "externalids": {"DOI": "10.1177/2055207617713827"}, "doi_lower": "10.1177/2055207617713827"}
{"paper_id": 228935772, "title": "Conversational AI: Dialogue Systems, Conversational Agents, and Chatbots", "author_names": ["M. McTear"], "venue": "Conversational AI", "abstract": "Abstract This book provides a comprehensive introduction to Conversational AI. While the idea of interacting with a computer using voice or text goes back a long way, it is only in recent years tha...", "year": 2020, "publicationdate": "2020-10-30", "externalids": {"DOI": "10.2200/s01060ed1v01y202010hlt048"}, "doi_lower": "10.2200/s01060ed1v01y202010hlt048"}
{"paper_id": 235490147, "title": "Conversational Agents in Software Engineering: Survey, Taxonomy and Challenges", "author_names": ["Quim Motger", "Xavier Franch", "Jordi Marco"], "venue": "arXiv.org", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 233416226, "title": "The human side of human-chatbot interaction: A systematic literature review of ten years of research on text-based chatbots", "author_names": ["A. Rapp", "L. Curti", "Arianna Boldi"], "venue": "Int. J. Hum. Comput. Stud.", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.1016/j.ijhcs.2021.102630"}, "doi_lower": "10.1016/j.ijhcs.2021.102630"}
{"paper_id": 228846872, "title": "Chatbots: History, technology, and applications", "author_names": ["Eleni Adamopoulou", "Lefteris Moussiades"], "venue": "", "abstract": null, "year": 2020, "publicationdate": "2020-12-15", "externalids": {"DOI": "10.1016/j.mlwa.2020.100006"}, "doi_lower": "10.1016/j.mlwa.2020.100006"}
{"paper_id": 51609768, "title": "SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks", "author_names": ["Ke Wang", "Xiaojun Wan"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Generating texts of different sentiment labels is getting more and more attention in the area of natural language generation. Recently, Generative Adversarial Net (GAN) has shown promising results in text generation. However, the texts generated by GAN usually suffer from the problems of poor quality, lack of diversity and mode collapse. In this paper, we propose a novel framework - SentiGAN, which has multiple generators and one multi-class discriminator, to address the above problems. In our framework, multiple generators are trained simultaneously, aiming at generating texts of different sentiment labels without supervision. We propose a penalty based objective in the generators to force each of them to generate diversified examples of a specific sentiment label. Moreover, the use of multiple generators and one multi-class discriminator can make each generator focus on generating its own examples of a specific sentiment label accurately. Experimental results on four datasets demonstrate that our model consistently outperforms several state-of-the-art text generation methods in the sentiment accuracy and quality of generated texts.", "year": 2018, "publicationdate": "2018-07-01", "externalids": {"DOI": "10.24963/ijcai.2018/618"}, "doi_lower": "10.24963/ijcai.2018/618"}
{"paper_id": 3033303, "title": "MojiTalk: Generating Emotional Responses at Scale", "author_names": ["Xianda Zhou", "William Yang Wang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Generating emotional language is a key step towards building empathetic natural language processing agents. However, a major challenge for this line of research is the lack of large-scale labeled training data, and previous studies are limited to only small sets of human annotated sentiment labels. Additionally, explicitly controlling the emotion and sentiment of generated text is also difficult. In this paper, we take a more radical approach: we exploit the idea of leveraging Twitter data that are naturally labeled with emojis. We collect a large corpus of Twitter conversations that include emojis in the response and assume the emojis convey the underlying emotions of the sentence. We investigate several conditional variational autoencoders training on these conversations, which allow us to use emojis to control the emotion of the generated text. Experimentally, we show in our quantitative and qualitative analyses that the proposed models can successfully generate high-quality abstractive conversation responses in accordance with designated emotions.", "year": 2017, "publicationdate": "2017-11-11", "externalids": {"DOI": "10.18653/v1/P18-1104"}, "doi_lower": "10.18653/v1/p18-1104"}
{"paper_id": 216552995, "title": "CAiRE: An Empathetic Neural Chatbot.", "author_names": ["Zhaojiang Lin", "Peng Xu", "Genta Indra Winata", "Farhad Bin Siddique", "Zihan Liu", "Jamin Shin", "Pascale Fung"], "venue": "", "abstract": "In this paper, we present an end-to-end empathetic conversation agent CAiRE. Our system adapts TransferTransfo (Wolf et al., 2019) learning approach that fine-tunes a large-scale pre-trained language model with multi-task objectives: response language modeling, response prediction and dialogue emotion detection. We evaluate our model on the recently proposed empathetic-dialogues dataset (Rashkin et al., 2019), the experiment results show that CAiRE achieves state-of-the-art performance on dialogue emotion detection and empathetic response generation.", "year": 2019, "publicationdate": "2019-07-28", "externalids": {}, "doi_lower": null}
{"paper_id": 238531610, "title": "CheerBots: Chatbots toward Empathy and Emotionusing Reinforcement Learning", "author_names": ["Jiun-hao Jhan", "Chao-Peng Liu", "Shyh-Kang Jeng", "Hung-yi Lee"], "venue": "arXiv.org", "abstract": "Apart from the coherence and fluency of responses, an empathetic chatbot emphasizes more on people's feelings. By considering altruistic behaviors between human interaction, empathetic chatbots enable people to get a better interactive and supportive experience. This study presents a framework whereby several empathetic chatbots are based on understanding users' implied feelings and replying empathetically for multiple dialogue turns. We call these chatbots CheerBots. CheerBots can be retrieval-based or generative-based and were finetuned by deep reinforcement learning. To respond in an empathetic way, we develop a simulating agent, a Conceptual Human Model, as aids for CheerBots in training with considerations on changes in user's emotional states in the future to arouse sympathy. Finally, automatic metrics and human rating results demonstrate that CheerBots outperform other baseline chatbots and achieves reciprocal altruism. The code and the pre-trained models will be made available.", "year": 2021, "publicationdate": "2021-10-08", "externalids": {}, "doi_lower": null}
{"paper_id": 201124425, "title": "MoEL: Mixture of Empathetic Listeners", "author_names": ["Zhaojiang Lin", "Andrea Madotto", "Jamin Shin", "Peng Xu", "Pascale Fung"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Previous research on empathetic dialogue systems has mostly focused on generating responses given certain emotions. However, being empathetic not only requires the ability of generating emotional responses, but more importantly, requires the understanding of user emotions and replying appropriately. In this paper, we propose a novel end-to-end approach for modeling empathy in dialogue systems: Mixture of Empathetic Listeners (MoEL). Our model first captures the user emotions and outputs an emotion distribution. Based on this, MoEL will softly combine the output states of the appropriate Listener(s), which are each optimized to react to certain emotions, and generate an empathetic response. Human evaluations on EMPATHETIC-DIALOGUES dataset confirm that MoEL outperforms multitask training baseline in terms of empathy, relevance, and fluency. Furthermore, the case study on generated responses of different Listeners shows high interpretability of our model.", "year": 2019, "publicationdate": "2019-08-21", "externalids": {"DOI": "10.18653/v1/D19-1012"}, "doi_lower": "10.18653/v1/d19-1012"}
{"paper_id": 222134115, "title": "MIME: MIMicking Emotions for Empathetic Response Generation", "author_names": ["Navonil Majumder", "Pengfei Hong", "Shanshan Peng", "Jiankun Lu", "Deepanway Ghosal", "Alexander Gelbukh", "Rada Mihalcea", "Soujanya Poria"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly. We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content. We show that the consideration of this polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art. Also, we introduce stochasticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work. We demonstrate the importance of these factors to empathetic response generation using both automatic- and human-based evaluations. The implementation of MIME is publicly available at this https URL.", "year": 2020, "publicationdate": "2020-10-04", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.721"}, "doi_lower": "10.18653/v1/2020.emnlp-main.721"}
{"paper_id": 237492045, "title": "CEM: Commonsense-aware Empathetic Response Generation", "author_names": ["Sahand Sabour", "Chujie Zheng", "Minlie Huang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "A key trait of daily conversations between individuals is the ability to express empathy towards others, and exploring ways to implement empathy is a crucial step towards human-like dialogue systems. Previous approaches on this topic mainly focus on detecting and utilizing the user’s emotion for generating empathetic responses. However, since empathy includes both aspects of affection and cognition, we argue that in addition to identifying the user’s emotion, cognitive understanding of the user’s situation should also be considered. To this end, we propose a novel approach for empathetic response generation, which leverages commonsense to draw more information about the user’s situation and uses this additional information to further enhance the empathy expression in generated responses. We evaluate our approach on EMPATHETICDIALOGUES, which is a widely-used benchmark dataset for empathetic response generation. Empirical results demonstrate that our approach outperforms the baseline models in both automatic and human evaluations and can generate more informative and empathetic responses. Our code is available at https://github.com/Sahandfer/CEM.", "year": 2021, "publicationdate": "2021-09-13", "externalids": {"DOI": "10.1609/aaai.v36i10.21373"}, "doi_lower": "10.1609/aaai.v36i10.21373"}
{"paper_id": 245537585, "title": "Knowledge Bridging for Empathetic Dialogue Generation", "author_names": ["Qintong Li", "Pijian Li", "Z. Ren", "Pengjie Ren", "Zhumin Chen"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Lack of external knowledge makes empathetic dialogue systems difficult to perceive implicit emotions and learn emotional interactions from limited dialogue history. To address the above problems, we propose to leverage external knowledge, including commonsense knowledge and emotional lexical knowledge, to explicitly understand and express emotions in empathetic dialogue generation. We first enrich the dialogue history by jointly interacting with external knowledge and construct an emotional context graph. Then we learn emotional context representations from the knowledge-enriched emotional context graph and distill emotional signals, which are the prerequisites to predicate emotions expressed in responses. Finally, to generate the empathetic response, we propose an emotional cross-attention mechanism to learn the emotional dependencies from the emotional context graph. Extensive experiments conducted on a benchmark dataset verify the effectiveness of the proposed method. In addition, we find the performance of our method can be further improved by integrating with a pre-trained model that works orthogonally.", "year": 2020, "publicationdate": "2020-09-21", "externalids": {"DOI": "10.1609/aaai.v36i10.21347"}, "doi_lower": "10.1609/aaai.v36i10.21347"}
{"paper_id": 52986431, "title": "Should Machines Express Sympathy and Empathy? Experiments with a Health Advice Chatbot", "author_names": ["MA Bingjie Liu", "PhD S. Shyam Sundar"], "venue": "Cyberpsychology, Behavior, and Social Networking", "abstract": "Abstract When we ask a chatbot for advice about a personal problem, should it simply provide informational support and refrain from offering emotional support? Or, should it show sympathy and empathize with our situation? Although expression of caring and understanding is valued in supportive human communications, do we want the same from a chatbot, or do we simply reject it due to its artificiality and uncanniness? To answer this question, we conducted two experiments with a chatbot providing online medical information advice about a sensitive personal issue. In Study 1, participants (N = 158) simply read a dialogue between a chatbot and a human user. In Study 2, participants (N = 88) interacted with a real chatbot. We tested the effect of three types of empathic expression—sympathy, cognitive empathy, and affective empathy—on individuals' perceptions of the service and the chatbot. Data reveal that expression of sympathy and empathy is favored over unemotional provision of advice, in support of the Computers are Social Actors (CASA) paradigm. This is particularly true for users who are initially skeptical about machines possessing social cognitive capabilities. Theoretical, methodological, and practical implications are discussed.", "year": 2018, "publicationdate": "2018-10-01", "externalids": {"DOI": "10.1089/cyber.2018.0110"}, "doi_lower": "10.1089/cyber.2018.0110"}
{"paper_id": 233198793, "title": "Analyzing Description, User Understanding and Expectations of AI in Mobile Health Applications", "author_names": ["Zhaoyuan Su", "Mayara Costa Figueiredo", "Jueun Jo", "Kai Zheng", "Yunan Chen"], "venue": "American Medical Informatics Association Annual Symposium", "abstract": null, "year": 2020, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 12274704, "title": "DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker", "author_names": ["Matej Moravcík", "Martin Schmid", "Neil Burch", "V. Lisý", "Dustin Morrill", "Nolan Bard", "Trevor Davis", "K. Waugh", "Michael Bradley Johanson", "Michael Bowling"], "venue": "arXiv.org", "abstract": null, "year": 2017, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 202770731, "title": "On the Utility of Learning about Humans for Human-AI Coordination", "author_names": ["Micah Carroll", "Rohin Shah", "Mark K. Ho", "T. Griffiths", "S. Seshia", "P. Abbeel", "A. Dragan"], "venue": "Neural Information Processing Systems", "abstract": "While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human's gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them. Code is available at https://github.com/HumanCompatibleAI/overcooked_ai.", "year": 2019, "publicationdate": "2019-10-13", "externalids": {}, "doi_lower": null}
{"paper_id": 59553476, "title": "The Hanabi Challenge: A New Frontier for AI Research", "author_names": ["Nolan Bard", "Jakob N. Foerster", "A. Chandar", "Neil Burch", "Marc Lanctot", "H. F. Song", "Emilio Parisotto", "Vincent Dumoulin", "Subhodeep Moitra", "Edward Hughes", "Iain Dunning", "Shibl Mourad", "H. Larochelle", "Marc G. Bellemare", "Michael H. Bowling"], "venue": "Artificial Intelligence", "abstract": null, "year": 2019, "publicationdate": "2019-02-02", "externalids": {"DOI": "10.1016/j.artint.2019.103216"}, "doi_lower": "10.1016/j.artint.2019.103216"}
{"paper_id": 189928358, "title": "Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good", "author_names": ["Xuewei Wang", "Weiyan Shi", "Richard Kim", "Y. Oh", "Sijia Yang", "Jingwen Zhang", "Zhou Yu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Developing intelligent persuasive conversational agents to change people’s opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems. To do so, the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations. We designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. We collected a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset. Based on the annotation, we built a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus. Furthermore, to develop an understanding of personalized persuasion processes, we analyzed the relationships between individuals’ demographic and psychological backgrounds including personality, morality, value systems, and their willingness for donation. Then, we analyzed which types of persuasion strategies led to a greater amount of donation depending on the individuals’ personal backgrounds. This work lays the ground for developing a personalized persuasive dialogue system.", "year": 2019, "publicationdate": "2019-06-16", "externalids": {"DOI": "10.18653/v1/P19-1566"}, "doi_lower": "10.18653/v1/p19-1566"}
{"paper_id": 216410788, "title": "I–C–E Framework: Concepts for Group Dynamics Research in Human-Robot Interaction", "author_names": ["Anna M. H. Abrams", "Astrid M. Rosenthal-von der Pütten"], "venue": "International Journal of Social Robotics", "abstract": "The research community of human-robot interaction relies on theories and phenomena from the social sciences in order to study and validate robotic developments in interaction. These studies mainly concerned one (human) on one (robot) interactions in the past. The present paper shifts the attention to groups and group dynamics and reviews relevant concepts from the social sciences: ingroup identification (I), cohesion (C) and entitativity (E). Ubiquitous robots will be part of larger social settings in the near future. A conceptual framework, the I–C–E framework, is proposed as a theoretical foundation for group (dynamics) research in HRI. Additionally, we present methods and possible measures for these relevant concepts and outline topics for future research.", "year": 2019, "publicationdate": "2019-10-03", "externalids": {"DOI": "10.1007/s12369-020-00642-z"}, "doi_lower": "10.1007/s12369-020-00642-z"}
{"paper_id": 261681932, "title": "Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf", "author_names": ["Yuzhuang Xu", "Shuo Wang", "Peng Li", "Fuwen Luo", "Xiaolong Wang", "Weidong Liu", "Yang Liu"], "venue": "arXiv.org", "abstract": "Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in communication games, and in response, propose a tuning-free framework. Our approach keeps LLMs frozen, and relies on the retrieval and reflection on past communications and experiences for improvement. An empirical study on the representative and widely-studied communication game, ``Werewolf'', demonstrates that our framework can effectively play Werewolf game without tuning the parameters of the LLMs. More importantly, strategic behaviors begin to emerge in our experiments, suggesting that it will be a fruitful journey to engage LLMs in communication games and associated domains.", "year": 2023, "publicationdate": "2023-09-09", "externalids": {"DOI": "10.48550/arXiv.2309.04658"}, "doi_lower": "10.48550/arxiv.2309.04658"}
{"paper_id": 250113371, "title": "Using cognitive psychology to understand GPT-3", "author_names": ["Marcel Binz", "Eric Schulz"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "abstract": "Significance Language models are trained to predict the next word for a given text. Recently, it has been shown that scaling up these models causes them to not only generate language but also to solve challenging reasoning problems. The present article lets a large language model (GPT-3) do experiments from the cognitive psychology literature. We find that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books. We additionally utilize analysis tools from the cognitive psychology literature to demystify how GPT-3 solves different tasks and use the thereby acquired insights to make recommendations for how to improve future model iterations.", "year": 2022, "publicationdate": "2022-06-21", "externalids": {"DOI": "10.1073/pnas.2218523120"}, "doi_lower": "10.1073/pnas.2218523120"}
{"paper_id": 250526626, "title": "Language models show human-like content effects on reasoning", "author_names": ["Ishita Dasgupta", "Andrew Kyle Lampinen", "Stephanie C. Y. Chan", "Antonia Creswell", "D. Kumaran", "James L. McClelland", "Felix Hill"], "venue": "arXiv.org", "abstract": "Reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable\"content effects\"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large language models, as well as humans, and find that the language models reflect many of the same patterns observed in humans across these tasks $\\unicode{x2014}$ like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected both in answer patterns, and in lower-level features like the relationship between model answer distributions and human response times. Our findings have implications for understanding both these cognitive effects in humans, and the factors that contribute to language model performance.", "year": 2022, "publicationdate": "2022-07-14", "externalids": {"DOI": "10.48550/arXiv.2207.07051"}, "doi_lower": "10.48550/arxiv.2207.07051"}
{"paper_id": 257636780, "title": "Mind meets machine: Unravelling GPT-4's cognitive psychology", "author_names": ["Sifatkaur Dhingra", "Manmeet Singh", "Vaisakh S.B.", "Neetiraj Malviya", "S. Gill"], "venue": "BenchCouncil Transactions on Benchmarks, Standards and Evaluations", "abstract": "Cognitive psychology delves on understanding perception, attention, memory, language, problem-solving, decision-making, and reasoning. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans exam and complex problems has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Although GPT-4 report has shown performance on some cognitive psychology tasks, a comprehensive assessment of GPT-4, via the existing well-established datasets is required. In this study, we focus on the evaluation of GPT-4's performance on a set of cognitive psychology datasets such as CommonsenseQA, SuperGLUE, MATH and HANS. In doing so, we understand how GPT-4 processes and integrates cognitive psychology with contextual information, providing insight into the underlying cognitive processes that enable its ability to generate the responses. We show that GPT-4 exhibits a high level of accuracy in cognitive psychology tasks relative to the prior state-of-the-art models. Our results strengthen the already available assessments and confidence on GPT-4's cognitive psychology abilities. It has significant potential to revolutionize the field of AI, by enabling machines to bridge the gap between human and machine reasoning.", "year": 2023, "publicationdate": "2023-03-20", "externalids": {"DOI": "10.48550/arXiv.2303.11436"}, "doi_lower": "10.48550/arxiv.2303.11436"}
{"paper_id": 257757370, "title": "Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods", "author_names": ["Thilo Hagendorff"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2303.13988"}, "doi_lower": "10.48550/arxiv.2303.13988"}
{"paper_id": 259951557, "title": "Emotional intelligence of Large Language Models", "author_names": ["Xuena Wang", "Xueting Li", "Zi Yin", "Yue Wu", "Liu Jia Department of PsychologyTsinghua Laboratory of Brain", "Intelligence", "Tsinghua University", "Departmentof Psychology", "Renmin University"], "venue": "Journal of Pacific Rim Psychology", "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI. This test is an objective, performance-driven, and text-based evaluation, which requires evaluating complex emotions in realistic scenarios, providing a consistent assessment for both human and LLM capabilities. With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achieved above-average Emotional Quotient (EQ) scores, with GPT-4 exceeding 89% of human participants with an EQ of 117. Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not rely on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans. In addition, we discussed the impact of factors such as model size, training method, and architecture on LLMs' EQ. In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence. Project website: https://emotional-intelligence.github.io/", "year": 2023, "publicationdate": "2023-01-01", "externalids": {"DOI": "10.1177/18344909231213958"}, "doi_lower": "10.1177/18344909231213958"}
{"paper_id": 254926903, "title": "Computer says \"No\": The Case Against Empathetic Conversational AI", "author_names": ["Alba Curry", "A. C. Curry"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Emotions are an integral part of human cognition and they guide not only our understanding of the world but also our actions within it. As such, whether we soothe or flame an emotion is not inconsequential. Recent work in conversational AI has focused on responding empathetically to users, validating and soothing their emotions without a real basis. This AI-aided emotional regulation can have negative consequences for users and society, tending towards a one-noted happiness defined as only the absence of\"negative\"emotions. We argue that we must carefully consider whether and how to respond to users' emotions.", "year": 2022, "publicationdate": "2022-12-21", "externalids": {"DOI": "10.48550/arXiv.2212.10983"}, "doi_lower": "10.48550/arxiv.2212.10983"}
{"paper_id": 258891670, "title": "ChatGPT outperforms humans in emotional awareness evaluations", "author_names": ["Zohar Elyoseph", "D. Hadar-Shoval", "K. Asraf", "Maya Lvovsky"], "venue": "Frontiers in Psychology", "abstract": "The artificial intelligence chatbot, ChatGPT, has gained widespread attention for its ability to perform natural language processing tasks and has the fastest-growing user base in history. Although ChatGPT has successfully generated theoretical information in multiple fields, its ability to identify and describe emotions is still unknown. Emotional awareness (EA), the ability to conceptualize one’s own and others’ emotions, is considered a transdiagnostic mechanism for psychopathology. This study utilized the Levels of Emotional Awareness Scale (LEAS) as an objective, performance-based test to analyze ChatGPT’s responses to twenty scenarios and compared its EA performance with that of the general population norms, as reported by a previous study. A second examination was performed one month later to measure EA improvement over time. Finally, two independent licensed psychologists evaluated the fit-to-context of ChatGPT’s EA responses. In the first examination, ChatGPT demonstrated significantly higher performance than the general population on all the LEAS scales (Z score = 2.84). In the second examination, ChatGPT’s performance significantly improved, almost reaching the maximum possible LEAS score (Z score = 4.26). Its accuracy levels were also extremely high (9.7/10). The study demonstrated that ChatGPT can generate appropriate EA responses, and that its performance may improve significantly over time. The study has theoretical and clinical implications, as ChatGPT can be used as part of cognitive training for clinical populations with EA impairments. In addition, ChatGPT’s EA-like abilities may facilitate psychiatric diagnosis and assessment and be used to enhance emotional language. Further research is warranted to better understand the potential benefits and risks of ChatGPT and refine it to promote mental health.", "year": 2023, "publicationdate": "2023-05-26", "externalids": {"DOI": "10.3389/fpsyg.2023.1199058"}, "doi_lower": "10.3389/fpsyg.2023.1199058"}
{"paper_id": 257038551, "title": "Empathetic AI for Empowering Resilience in Games", "author_names": ["R. Habibi", "Johannes Pfau", "Jonattan Holmes", "M. S. El-Nasr"], "venue": "arXiv.org", "abstract": "Failure and resilience are important aspects of gameplay. This is especially important for serious and competitive games, where players need to adapt and cope with failure frequently. In such situations, emotion regulation -- the active process of modulating ones' emotions to cope and adapt to challenging situations -- becomes essential. It is one of the prominent aspects of human intelligence and promotes mental health and well-being. While there has been work on developing artificial emotional regulation assistants to help users cope with emotion regulation in the field of Intelligent Tutoring systems, little is done to incorporate such systems or ideas into (serious) video games. In this paper, we introduce a data-driven 6-phase approach to establish empathetic artificial intelligence (EAI), which operates on raw chat log data to detect key affective states, identify common sequences and emotion regulation strategies and generalizes these to make them applicable for intervention systems.", "year": 2023, "publicationdate": "2023-02-16", "externalids": {"DOI": "10.48550/arXiv.2302.09070"}, "doi_lower": "10.48550/arxiv.2302.09070"}
{"paper_id": 254877016, "title": "Identifying and Manipulating the Personality Traits of Language Models", "author_names": ["Graham Caron", "Shashank Srivastava"], "venue": "arXiv.org", "abstract": "Psychology research has long explored aspects of human personality such as extroversion, agreeableness and emotional stability. Categorizations like the `Big Five' personality traits are commonly used to assess and diagnose personality types. In this work, we explore the question of whether the perceived personality in language models is exhibited consistently in their language generation. For example, is a language model such as GPT2 likely to respond in a consistent way if asked to go out to a party? We also investigate whether such personality traits can be controlled. We show that when provided different types of contexts (such as personality descriptions, or answers to diagnostic questions about personality traits), language models such as BERT and GPT2 can consistently identify and reflect personality markers in those contexts. This behavior illustrates an ability to be manipulated in a highly predictable way, and frames them as tools for identifying personality traits and controlling personas in applications such as dialog systems. We also contribute a crowd-sourced data-set of personality descriptions of human subjects paired with their `Big Five' personality assessment data, and a data-set of personality descriptions collated from Reddit.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10276"}, "doi_lower": "10.48550/arxiv.2212.10276"}
{"paper_id": 260334342, "title": "Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models", "author_names": ["Keyu Pan", "Yawen Zeng"], "venue": "arXiv.org", "abstract": "The field of large language models (LLMs) has made significant progress, and their knowledge storage capacity is approaching that of human beings. Furthermore, advanced techniques, such as prompt learning and reinforcement learning, are being employed to address ethical concerns and hallucination problems associated with LLMs, bringing them closer to aligning with human values. This situation naturally raises the question of whether LLMs with human-like abilities possess a human-like personality? In this paper, we aim to investigate the feasibility of using the Myers-Briggs Type Indicator (MBTI), a widespread human personality assessment tool, as an evaluation metric for LLMs. Specifically, extensive experiments will be conducted to explore: 1) the personality types of different LLMs, 2) the possibility of changing the personality types by prompt engineering, and 3) How does the training dataset affect the model's personality. Although the MBTI is not a rigorous assessment, it can still reflect the similarity between LLMs and human personality. In practice, the MBTI has the potential to serve as a rough indicator. Our codes are available at https://github.com/HarderThenHarder/transformers_tasks/tree/main/LLM/llms_mbti.", "year": 2023, "publicationdate": "2023-07-30", "externalids": {"DOI": "10.48550/arXiv.2307.16180"}, "doi_lower": "10.48550/arxiv.2307.16180"}
{"paper_id": 254877169, "title": "Is GPT-3 a Psychopath? Evaluating Large Language Models from a Psychological Perspective", "author_names": ["Xingxuan Li", "Yutong Li", "Linlin Liu", "Lidong Bing", "Shafiq R. Joty"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2212.10529"}, "doi_lower": "10.48550/arxiv.2212.10529"}
{"paper_id": 259317218, "title": "Personality Traits in Large Language Models", "author_names": ["Mustafa Safdari", "Gregory Serapio-Garc'ia", "Clé-ment Crepy", "Stephen Fitz", "P. Romero", "Luning Sun", "Marwa Abdulhai", "Aleksandra Faust", "Maja Matari'c"], "venue": "arXiv.org", "abstract": "The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly powerconversational agents used by the general public world-wide, the synthetic personality traits embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we present a novel and comprehensive psychometrically valid and reliable methodology for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method to 18 LLMs, we found: 1) personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss the application and ethical implications of the measurement and shaping method, in particular regarding responsible AI.", "year": 2023, "publicationdate": "2023-07-01", "externalids": {"DOI": "10.48550/arXiv.2307.00184"}, "doi_lower": "10.48550/arxiv.2307.00184"}
{"paper_id": 49552345, "title": "TextWorld: A Learning Environment for Text-based Games", "author_names": ["Marc-Alexandre Côté", "Ákos Kádár", "Xingdi Yuan", "B. Kybartas", "Tavian Barnes", "Emery Fine", "James Moore", "Matthew J. Hausknecht", "Layla El Asri", "Mahmoud Adada", "Wendy Tay", "Adam Trischler"], "venue": "CGW@IJCAI", "abstract": "We introduce TextWorld, a sandbox learning environment for the training and evaluation of RL agents on text-based games. TextWorld is a Python library that handles interactive play-through of text games, as well as backend functions like state tracking and reward assignment. It comes with a curated list of games whose features and challenges we have analyzed. More significantly, it enables users to handcraft or automatically generate new games. Its generative mechanisms give precise control over the difficulty, scope, and language of constructed games, and can be used to relax challenges inherent to commercial text games like partial observability and sparse rewards. By generating sets of varied but similar games, TextWorld can also be used to study generalization and transfer learning. We cast text-based games in the Reinforcement Learning formalism, use our framework to develop a set of benchmark games, and evaluate several baseline agents on this set and the curated list.", "year": 2018, "publicationdate": "2018-06-29", "externalids": {"DOI": "10.1007/978-3-030-24337-1_3"}, "doi_lower": "10.1007/978-3-030-24337-1_3"}
{"paper_id": 71144630, "title": "Learning to Speak and Act in a Fantasy Text Adventure Game", "author_names": ["Jack Urbanek", "Angela Fan", "Siddharth Karamcheti", "Saachi Jain", "Samuel Humeau", "Emily Dinan", "Tim Rocktäschel", "Douwe Kiela", "Arthur Szlam", "J. Weston"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "We introduce a large-scale crowdsourced text adventure game as a research platform for studying grounded dialogue. In it, agents can perceive, emote, and act whilst conducting dialogue with other agents. Models and humans can both act as characters within the game. We describe the results of training state-of-the-art generative and retrieval models in this setting. We show that in addition to using past dialogue, these models are able to effectively use the state of the underlying world to condition their predictions. In particular, we show that grounding on the details of the local environment, including location descriptions, and the objects (and their affordances) and characters (and their previous actions) present within it allows better predictions of agent behavior and dialogue. We analyze the ingredients necessary for successful grounding in this setting, and how each of these factors relate to agents that can talk and act successfully.", "year": 2019, "publicationdate": "2019-03-01", "externalids": {"DOI": "10.18653/v1/D19-1062"}, "doi_lower": "10.18653/v1/d19-1062"}
{"paper_id": 202565447, "title": "Interactive Fiction Games: A Colossal Adventure", "author_names": ["Matthew J. Hausknecht", "Prithviraj Ammanabrolu", "Marc-Alexandre Côté", "Xingdi Yuan"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "A hallmark of human intelligence is the ability to understand and communicate with language. Interactive Fiction games are fully text-based simulation environments where a player issues text commands to effect change in the environment and progress through the story. We argue that IF games are an excellent testbed for studying language-based autonomous agents. In particular, IF games combine challenges of combinatorial action spaces, language understanding, and commonsense reasoning. To facilitate rapid development of language-based agents, we introduce Jericho, a learning environment for man-made IF games and conduct a comprehensive study of text-agents across a rich set of games, highlighting directions in which agents can improve.", "year": 2019, "publicationdate": "2019-09-11", "externalids": {"DOI": "10.1609/AAAI.V34I05.6297"}, "doi_lower": "10.1609/aaai.v34i05.6297"}
{"paper_id": 260438869, "title": "Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models", "author_names": ["Aidan O'Gara"], "venue": "arXiv.org", "abstract": "Are current language models capable of deception and lie detection? We study this question by introducing a text-based game called $\\textit{Hoodwinked}$, inspired by Mafia and Among Us. Players are locked in a house and must find a key to escape, but one player is tasked with killing the others. Each time a murder is committed, the surviving players have a natural language discussion then vote to banish one player from the game. We conduct experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find evidence of deception and lie detection capabilities. The killer often denies their crime and accuses others, leading to measurable effects on voting outcomes. More advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons. Secondary metrics provide evidence that this improvement is not mediated by different actions, but rather by stronger persuasive skills during discussions. To evaluate the ability of AI agents to deceive humans, we make this game publicly available at h https://hoodwinked.ai/ .", "year": 2023, "publicationdate": "2023-07-05", "externalids": {"DOI": "10.48550/arXiv.2308.01404"}, "doi_lower": "10.48550/arxiv.2308.01404"}
{"paper_id": 261518421, "title": "RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking", "author_names": ["Homanga Bharadhwaj", "Jay Vakil", "Mohit Sharma", "Abhi Gupta", "Shubham Tulsiani", "Vikash Kumar"], "venue": "IEEE International Conference on Robotics and Automation", "abstract": "The grand aim of having a single robot that can manipulate arbitrary objects in diverse settings is at odds with the paucity of robotics datasets. Acquiring and growing such datasets is strenuous due to manual efforts, operational costs, and safety challenges. A path toward such a universal agent requires an efficient framework capable of generalization but within a reasonable data budget. In this paper, we develop an efficient framework (MT-ACT) for training universal agents capable of multi-task manipulation skills using (a) semantic augmentations that can rapidly multiply existing datasets and (b) action representations that can extract performant policies with small yet diverse multi-modal datasets without overfitting. In addition, reliable task conditioning and an expressive policy architecture enables our agent to exhibit a diverse repertoire of skills in novel situations specified using task commands. Using merely 7500 demonstrations, we are able to train a single policy RoboAgent capable of 12 unique skills, and demonstrate its generalization over 38 tasks spread across common daily activities in diverse kitchen scenes. On average, RoboAgent outperforms prior methods by over 40% in unseen situations while being more sample efficient. See https://robopen.github.io/for video results and appendix.", "year": 2023, "publicationdate": "2023-09-05", "externalids": {"DOI": "10.1109/ICRA57147.2024.10611293"}, "doi_lower": "10.1109/icra57147.2024.10611293"}
{"paper_id": 251403008, "title": "Social Simulacra: Creating Populated Prototypes for Social Computing Systems", "author_names": ["J. Park", "Lindsay Popowski", "Carrie J. Cai", "M. Morris", "Percy Liang", "Michael S. Bernstein"], "venue": "ACM Symposium on User Interface Software and Technology", "abstract": "Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many challenges do not arise until a system is populated at a larger scale. Can a designer understand how a social system might behave when populated, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping technique that generates a breadth of realistic social interactions that may emerge when a social computing system is populated. Social simulacra take as input the designer’s description of a community’s design—goal, rules, and member personas—and produce as output an instance of that design with simulated behavior, including posts, replies, and anti-social behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of “what if?” scenarios where community members or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large language models’ training data already includes a wide variety of positive and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacra from actual community behavior and that social computing designers successfully refine their social computing designs when using social simulacra.", "year": 2022, "publicationdate": "2022-08-08", "externalids": {"DOI": "10.1145/3526113.3545616"}, "doi_lower": "10.1145/3526113.3545616"}
{"paper_id": 260202947, "title": "S3: Social-network Simulation System with Large Language Model-Empowered Agents", "author_names": ["Chen Gao", "Xiaochong Lan", "Zhi-jie Lu", "Jinzhu Mao", "J. Piao", "Huandong Wang", "Depeng Jin", "Yong Li"], "venue": "Social Science Research Network", "abstract": "Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\\textbf{S}$ocial network $\\textbf{S}$imulation $\\textbf{S}$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.", "year": 2023, "publicationdate": "2023-07-27", "externalids": {"DOI": "10.48550/arXiv.2307.14984"}, "doi_lower": "10.48550/arxiv.2307.14984"}
{"paper_id": 263888378, "title": "RecAgent: A Novel Simulation Paradigm for Recommender Systems", "author_names": ["Lei Wang", "Jingsen Zhang", "Xu Chen", "Yankai Lin", "Ruihua Song", "Wayne Xin Zhao", "Ji-rong Wen"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.02552"}, "doi_lower": "10.48550/arxiv.2306.02552"}
{"paper_id": 259766713, "title": "Epidemic Modeling with Generative Agents", "author_names": ["Ross Williams", "Niyousha Hosseinichimeh", "A. Majumdar", "Navid Ghaffarzadegan"], "venue": "arXiv.org", "abstract": "This study offers a new paradigm of individual-level modeling to address the grand challenge of incorporating human behavior in epidemic models. Using generative artificial intelligence in an agent-based epidemic model, each agent is empowered to make its own reasonings and decisions via connecting to a large language model such as ChatGPT. Through various simulation experiments, we present compelling evidence that generative agents mimic real-world behaviors such as quarantining when sick and self-isolation when cases rise. Collectively, the agents demonstrate patterns akin to multiple waves observed in recent pandemics followed by an endemic period. Moreover, the agents successfully flatten the epidemic curve. This study creates potential to improve dynamic system modeling by offering a way to represent human brain, reasoning, and decision making.", "year": 2023, "publicationdate": "2023-07-11", "externalids": {"DOI": "10.48550/arXiv.2307.04986"}, "doi_lower": "10.48550/arxiv.2307.04986"}
{"paper_id": 119104167, "title": "A Variational Basis for the Regulation and Structuration Mechanisms of Agent Societies", "author_names": ["A. C. R. Costa"], "venue": "Cambridge International Law Journal", "abstract": null, "year": 2019, "publicationdate": "2019-04-18", "externalids": {"DOI": "10.1007/978-3-030-16335-8"}, "doi_lower": "10.1007/978-3-030-16335-8"}
{"paper_id": 233840355, "title": "THE EVERYDAY LIFE IN THE SIMS 4 DURING A PANDEMIC. A LIFE SIMULATION AS A VIRTUAL MIRROR OF SOCIETY?", "author_names": ["Simon Wimmer", "Alexander Pfeiffer", "Natalie Denk"], "venue": "", "abstract": null, "year": 2021, "publicationdate": "2021-03-01", "externalids": {"DOI": "10.21125/INTED.2021.1162"}, "doi_lower": "10.21125/inted.2021.1162"}
{"paper_id": 238634091, "title": "All One Needs to Know about Metaverse: A Complete Survey on Technological Singularity, Virtual Ecosystem, and Research Agenda", "author_names": ["Lik-Hang Lee", "Tristan Braud", "Pengyuan Zhou", "Lin Wang", "Dianlei Xu", "Zijun Lin", "Abhishek Kumar", "Carlos Bermejo", "Pan Hui"], "venue": "Found. Trends Hum. Comput. Interact.", "abstract": "Since the popularisation of the Internet in the 1990s, the cyberspace has kept evolving. We have created various computer-mediated virtual environments including social networks, video conferencing, virtual 3D worlds (e.g., VR Chat), augmented reality applications (e.g., Pokemon Go), and Non-Fungible Token Games (e.g., Upland). Such virtual environments, albeit non-perpetual and unconnected, have bought us various degrees of digital transformation. The term `metaverse' has been coined to further facilitate the digital transformation in every aspect of our physical lives. At the core of the metaverse stands the vision of an immersive Internet as a gigantic, unified, persistent, and shared realm. While the metaverse may seem futuristic, catalysed by emerging technologies such as Extended Reality, 5G, and Artificial Intelligence, the digital `big bang' of our cyberspace is not far away. This survey paper presents the first effort to offer a comprehensive framework that examines the latest metaverse development under the dimensions of state-of-the-art technologies and metaverse ecosystems, and illustrates the possibility of the digital `big bang'. First, technologies are the enablers that drive the transition from the current Internet to the metaverse. We thus examine eight enabling technologies rigorously - Extended Reality, User Interactivity (Human-Computer Interaction), Artificial Intelligence, Blockchain, Computer Vision, IoT and Robotics, Edge and Cloud computing, and Future Mobile Networks. In terms of applications, the metaverse ecosystem allows human users to live and play within a self-sustaining, persistent, and shared realm. Therefore, we discuss six user-centric factors -- Avatar, Content Creation, Virtual Economy, Social Acceptability, Security and Privacy, and Trust and Accountability. Finally, we propose a concrete research agenda for the development of the metaverse.", "year": 2021, "publicationdate": "2021-10-06", "externalids": {"DOI": "10.13140/RG.2.2.11200.05124/1"}, "doi_lower": "10.13140/rg.2.2.11200.05124/1"}
{"paper_id": 112753256, "title": "Becoming Modern: Individual Change in Six Developing Countries", "author_names": ["C. Black", "A. Inkeles", "D. H. Smith"], "venue": "", "abstract": null, "year": 1976, "publicationdate": "1976-03-01", "externalids": {"DOI": "10.2307/3103560"}, "doi_lower": "10.2307/3103560"}
{"paper_id": 267846994, "title": "Social Science Microsimulation", "author_names": ["K. G. Troitzsch", "Ulrich Mueller", "G. Nigel", "Gilbert bullet", "Jim Doran"], "venue": "", "abstract": null, "year": 1999, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 216410788, "title": "I–C–E Framework: Concepts for Group Dynamics Research in Human-Robot Interaction", "author_names": ["Anna M. H. Abrams", "Astrid M. Rosenthal-von der Pütten"], "venue": "International Journal of Social Robotics", "abstract": "The research community of human-robot interaction relies on theories and phenomena from the social sciences in order to study and validate robotic developments in interaction. These studies mainly concerned one (human) on one (robot) interactions in the past. The present paper shifts the attention to groups and group dynamics and reviews relevant concepts from the social sciences: ingroup identification (I), cohesion (C) and entitativity (E). Ubiquitous robots will be part of larger social settings in the near future. A conceptual framework, the I–C–E framework, is proposed as a theoretical foundation for group (dynamics) research in HRI. Additionally, we present methods and possible measures for these relevant concepts and outline topics for future research.", "year": 2019, "publicationdate": "2019-10-03", "externalids": {"DOI": "10.1007/s12369-020-00642-z"}, "doi_lower": "10.1007/s12369-020-00642-z"}
{"paper_id": 244799619, "title": "A General Language Assistant as a Laboratory for Alignment", "author_names": ["Amanda Askell", "Yuntao Bai", "Anna Chen", "Dawn Drain", "Deep Ganguli", "T. Henighan", "Andy Jones", "Nicholas Joseph", "Benjamin Mann", "Nova Dassarma", "Nelson Elhage", "Zac Hatfield-Dodds", "Danny Hernandez", "John Kernion", "Kamal Ndousse", "Catherine Olsson", "Dario Amodei", "Tom B. Brown", "Jack Clark", "Sam McCandlish", "Chris Olah", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.", "year": 2021, "publicationdate": "2021-12-01", "externalids": {}, "doi_lower": null}
{"paper_id": 281526591, "title": "Heterogeneous Value Evaluation for Large Language Models", "author_names": ["Zhaowei Zhang", "Nian Liu", "Siyuan Qi", "Ceyao Zhang", "Ziqi Rong", "Song-Chun Zhu", "Shuguang Cui", "Yaodong Yang"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.17147"}, "doi_lower": "10.48550/arxiv.2305.17147"}
{"paper_id": 259888896, "title": "“Personhood and AI: Why large language models don’t understand us”", "author_names": ["Jacob Browning"], "venue": "Ai & Society", "abstract": null, "year": 2023, "publicationdate": "2023-07-12", "externalids": {"DOI": "10.1007/s00146-023-01724-y"}, "doi_lower": "10.1007/s00146-023-01724-y"}
{"paper_id": 249674444, "title": "MPI: Evaluating and Inducing Personality in Pre-trained Language Models", "author_names": ["Guangyuan Jiang", "Manjie Xu", "Song-Chun Zhu", "Wenjuan Han", "Chi Zhang", "Yixin Zhu"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2206.07550"}, "doi_lower": "10.48550/arxiv.2206.07550"}
{"paper_id": 263890629, "title": "Theory of Mind May Have Spontaneously Emerged in Large Language Models", "author_names": ["Michal Kosinski"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2302.02083"}, "doi_lower": "10.48550/arxiv.2302.02083"}
{"paper_id": 147351899, "title": "Psychobiology of personality: Marvin Zuckerman Cambridge University Press (1991). xv + 482 pp. Hardback", "author_names": ["H. Eysenck"], "venue": "", "abstract": null, "year": 1992, "publicationdate": "1992-06-01", "externalids": {"DOI": "10.1016/0191-8869(92)90247-M"}, "doi_lower": "10.1016/0191-8869(92)90247-m"}
{"paper_id": 259138322, "title": "Inductive reasoning in humans and large language models", "author_names": ["Simon J. Han", "Keith Ransom", "Andrew Perfors", "Charles Kemp"], "venue": "Cognitive Systems Research", "abstract": null, "year": 2023, "publicationdate": "2023-06-11", "externalids": {"DOI": "10.1016/j.cogsys.2023.101155"}, "doi_lower": "10.1016/j.cogsys.2023.101155"}
{"paper_id": 277740856, "title": "Fast-Slow-Thinking: Complex Task Solving with Large Language Models", "author_names": ["Yiliu Sun", "Yanfang Zhang", "Zicheng Zhao", "Sheng Wan", "Dacheng Tao", "Chen Gong"], "venue": "arXiv.org", "abstract": "Nowadays, Large Language Models (LLMs) have been gradually employed to solve complex tasks. To face the challenge, task decomposition has become an effective way, which proposes to divide a complex task into multiple simpler subtasks and then solve them separately so that the difficulty of the original task can be reduced. However, the performance of existing task decomposition methods can be suboptimal when the task contains overly complex logic and constraints. In this situation, the solution generated by LLMs may deviate from the original purpose of the task, or contain redundant or even erroneous content. Therefore, inspired by the fact that humans possess two thinking systems including fast thinking and slow thinking, this paper introduces a new task decomposition method termed ``Fast-Slow-Thinking'' (FST), which stimulates LLMs to solve tasks through the cooperation of Fast Thinking (FT) and Slow Thinking (ST) steps. Here FT focuses more on the general and concise aspect of the task, and ST focuses more on the details of the task. In FT, LLMs are prompted to remove the constraints of the original task, therefore simplifying it to a general and concise one. In ST, we recall the constraints removed in FT, so that LLMs can improve the answer generated in FT to meet the requirements of the original task. Therefore, our FST method enables LLMs to consider a complex problem via a human-like cognition process from coarse to fine, the effectiveness of which has been well demonstrated by the experiments on three types of tasks.", "year": 2025, "publicationdate": "2025-04-11", "externalids": {"DOI": "10.48550/arXiv.2504.08690"}, "doi_lower": "10.48550/arxiv.2504.08690"}
{"paper_id": 263828908, "title": "Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models - and Disappeared in GPT-4", "author_names": ["Thilo Hagendorff", "Sarah Fabi"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.07622"}, "doi_lower": "10.48550/arxiv.2306.07622"}
{"paper_id": 260333873, "title": "Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support", "author_names": ["Zilin Ma", "Yiyang Mei", "Zhaoyuan Su"], "venue": "AMIA ... Annual Symposium proceedings. AMIA Symposium", "abstract": "Conversational agents powered by large language models (LLM) have increasingly been utilized in the realm of mental well-being support. However, the implications and outcomes associated with their usage in such a critical field remain somewhat ambiguous and unexplored. We conducted a qualitative analysis of 120 posts, encompassing 2917 user comments, drawn from the most popular subreddit focused on mental health support applications powered by large language models (u/Replika). This exploration aimed to shed light on the advantages and potential pitfalls associated with the integration of these sophisticated models in conversational agents intended for mental health support. We found the app (Replika) beneficial in offering on-demand, non-judgmental support, boosting user confidence, and aiding self-discovery. Yet, it faced challenges in filtering harmful content, sustaining consistent communication, remembering new information, and mitigating users' overdependence. The stigma attached further risked isolating users socially. We strongly assert that future researchers and designers must thoroughly evaluate the appropriateness of employing LLMs for mental well-being support, ensuring their responsible and effective application.", "year": 2023, "publicationdate": "2023-07-28", "externalids": {"DOI": "10.48550/arXiv.2307.15810"}, "doi_lower": "10.48550/arxiv.2307.15810"}
{"paper_id": 207178664, "title": "The role of emotion in believable agents", "author_names": ["J. Bates"], "venue": "CACM", "abstract": null, "year": 1994, "publicationdate": "1994-07-01", "externalids": {"DOI": "10.1145/176789.176803"}, "doi_lower": "10.1145/176789.176803"}
{"paper_id": 248392154, "title": "AI Personification: Estimating the Personality of Language Models", "author_names": ["Saketh Reddy Karra", "Son Nguyen", "Theja Tulabandhula"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2204.12000"}, "doi_lower": "10.48550/arxiv.2204.12000"}
{"paper_id": 6869582, "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?", "author_names": ["Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "J. Weston"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.", "year": 2018, "publicationdate": "2018-01-22", "externalids": {"DOI": "10.18653/v1/P18-1205"}, "doi_lower": "10.18653/v1/p18-1205"}
{"paper_id": 259089085, "title": "What, When, and How to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue", "author_names": ["D. Kwon", "Sunwoo Lee", "Ki Hyun Kim", "Seojin Lee", "Tae-Yoon Kim", "Eric Davis"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "This paper presents a method for building a personalized open-domain dialogue system to address the WWH (WHAT, WHEN, and HOW) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of WWH in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.", "year": 2023, "publicationdate": "2023-06-06", "externalids": {"DOI": "10.48550/arXiv.2306.03361"}, "doi_lower": "10.48550/arxiv.2306.03361"}
{"paper_id": 8122852, "title": "Artificial life meets entertainment: lifelike autonomous agents", "author_names": ["P. Maes"], "venue": "CACM", "abstract": null, "year": 1995, "publicationdate": "1995-11-01", "externalids": {"DOI": "10.1145/219717.219808"}, "doi_lower": "10.1145/219717.219808"}
{"paper_id": 259166062, "title": "AI and the transformation of social science research", "author_names": ["I. Grossmann", "M. Feinberg", "D. C. Parker", "N. Christakis", "P. Tetlock", "William A. Cunningham"], "venue": "Science", "abstract": "Careful bias management and data fidelity are key Advances in artificial intelligence (AI), particularly large language models (LLMs), are substantially affecting social science research. These transformer-based machine-learning models pretrained on vast amounts of text data are increasingly capable of simulating human-like responses and behaviors (1, 2), offering opportunities to test theories and hypotheses about human behavior at great scale and speed. This presents urgent challenges: How can social science research practices be adapted, even reinvented, to harness the power of foundational AI? And how can this be done while ensuring transparent and replicable research?", "year": 2023, "publicationdate": "2023-06-16", "externalids": {"DOI": "10.1126/science.adi1778"}, "doi_lower": "10.1126/science.adi1778"}
{"paper_id": 258352487, "title": "Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models", "author_names": ["Jimmy Wei", "Kurt Shuster", "Arthur Szlam", "J. Weston", "Jack Urbanek", "M. Komeili"], "venue": "arXiv.org", "abstract": "Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.", "year": 2023, "publicationdate": "2023-04-26", "externalids": {"DOI": "10.48550/arXiv.2304.13835"}, "doi_lower": "10.48550/arxiv.2304.13835"}
{"paper_id": 60923, "title": "STEAMER: An Interactive Inspectable Simulation-Based Training System", "author_names": ["James Hollan", "E. Hutchins", "L. Weitzman"], "venue": "The AI Magazine", "abstract": null, "year": 1984, "publicationdate": "1984-06-15", "externalids": {"DOI": "10.1609/AIMAG.V5I2.434"}, "doi_lower": "10.1609/aimag.v5i2.434"}
{"paper_id": 10229249, "title": "Intelligent Agents for Interactive Simulation Environments", "author_names": ["Milind Tambe", "W. Johnson", "Randolph M. Jones", "F. Koss", "J. Laird", "P. Rosenbloom", "K. Schwamb"], "venue": "The AI Magazine", "abstract": null, "year": 1995, "publicationdate": "1995-03-15", "externalids": {"DOI": "10.1609/aimag.v16i1.1121"}, "doi_lower": "10.1609/aimag.v16i1.1121"}
{"paper_id": 126408454, "title": "‘Dynamics of Growth in a Finite World’ – Comprehensive Sensitivity Analysis", "author_names": ["P. Vermeulen", "D.C.J. de Jongh"], "venue": "", "abstract": null, "year": 1976, "publicationdate": "1976-06-01", "externalids": {"DOI": "10.1016/S1474-6670(17)67333-6"}, "doi_lower": "10.1016/s1474-6670(17)67333-6"}
{"paper_id": 4849593, "title": "System Dynamics and the Lessons of 35 Years", "author_names": ["J. Forrester"], "venue": "", "abstract": null, "year": 1993, "publicationdate": null, "externalids": {"DOI": "10.1007/978-1-4615-3226-2_7"}, "doi_lower": "10.1007/978-1-4615-3226-2_7"}
{"paper_id": 84766051, "title": "Cellular automata models for the simulation of real-world urban processes: A review and analysis", "author_names": ["I. Santé", "A. M. García", "D. Miranda", "R. Crecente"], "venue": "", "abstract": null, "year": 2010, "publicationdate": "2010-05-30", "externalids": {"DOI": "10.1016/J.LANDURBPLAN.2010.03.001"}, "doi_lower": "10.1016/j.landurbplan.2010.03.001"}
{"paper_id": 5427495, "title": "Multi-Agent Systems: A Survey", "author_names": ["D. Yazdansepas", "M. R. Nami"], "venue": "International Conference on Parallel and Distributed Processing Techniques and Applications", "abstract": null, "year": 2010, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 7866772, "title": "Multi-Agent Systems : Gossiping with Random Arrivals and Departures", "author_names": ["J. Hendrickx", "Samuel Martin"], "venue": "", "abstract": null, "year": 2017, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 258547324, "title": "Can Large Language Models Transform Computational Social Science?", "author_names": ["Caleb Ziems", "William B. Held", "Omar Shaikh", "Jiaao Chen", "Zhehao Zhang", "Diyi Yang"], "venue": "International Conference on Computational Logic", "abstract": "Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.", "year": 2023, "publicationdate": "2023-04-12", "externalids": {"DOI": "10.1162/coli_a_00502"}, "doi_lower": "10.1162/coli_a_00502"}
{"paper_id": 221940729, "title": "Simulating Societies : The Computer Simulation of Social Phenomena", "author_names": ["N. Gilbert", "J. Doran"], "venue": "", "abstract": null, "year": 1995, "publicationdate": "1995-12-01", "externalids": {"DOI": "10.4324/9781351165129"}, "doi_lower": "10.4324/9781351165129"}
{"paper_id": 12579598, "title": "A New Approach to the Economic Analysis of Nonstationary Time Series and the Business Cycle", "author_names": ["James D. Hamilton"], "venue": "", "abstract": null, "year": 1989, "publicationdate": "1989-03-01", "externalids": {"DOI": "10.2307/1912559"}, "doi_lower": "10.2307/1912559"}
{"paper_id": 14166978, "title": "Time series forecasting using a hybrid ARIMA and neural network model", "author_names": ["G. Zhang"], "venue": "Neurocomputing", "abstract": null, "year": 2003, "publicationdate": null, "externalids": {"DOI": "10.1016/S0925-2312(01)00702-0"}, "doi_lower": "10.1016/s0925-2312(01)00702-0"}
{"paper_id": 10797853, "title": "Innateness and culture in the evolution of language", "author_names": ["S. Kirby", "Mike Dowman", "T. Griffiths"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "abstract": null, "year": 2006, "publicationdate": "2006-03-01", "externalids": {"DOI": "10.1073/pnas.0608222104"}, "doi_lower": "10.1073/pnas.0608222104"}
{"paper_id": 257050264, "title": "Playing the Werewolf game with artificial intelligence for language understanding", "author_names": ["Hisaichi Shibata", "S. Miki", "Yuta Nakamura"], "venue": "arXiv.org", "abstract": "The Werewolf game is a social deduction game based on free natural language communication, in which players try to deceive others in order to survive. An important feature of this game is that a large portion of the conversations are false information, and the behavior of artificial intelligence (AI) in such a situation has not been widely investigated. The purpose of this study is to develop an AI agent that can play Werewolf through natural language conversations. First, we collected game logs from 15 human players. Next, we fine-tuned a Transformer-based pretrained language model to construct a value network that can predict a posterior probability of winning a game at any given phase of the game and given a candidate for the next action. We then developed an AI agent that can interact with humans and choose the best voting target on the basis of its probability from the value network. Lastly, we evaluated the performance of the agent by having it actually play the game with human players. We found that our AI agent, Deep Wolf, could play Werewolf as competitively as average human players in a villager or a betrayer role, whereas Deep Wolf was inferior to human players in a werewolf or a seer role. These results suggest that current language models have the capability to suspect what others are saying, tell a lie, or detect lies in conversations.", "year": 2023, "publicationdate": "2023-02-21", "externalids": {"DOI": "10.48550/arXiv.2302.10646"}, "doi_lower": "10.48550/arxiv.2302.10646"}
{"paper_id": 260899853, "title": "Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering", "author_names": ["Edward Junprung"], "venue": "arXiv.org", "abstract": "The final frontier for simulation is the accurate representation of complex, real-world social systems. While agent-based modeling (ABM) seeks to study the behavior and interactions of agents within a larger system, it is unable to faithfully capture the full complexity of human-driven behavior. Large language models (LLMs), like ChatGPT, have emerged as a potential solution to this bottleneck by enabling researchers to explore human-driven interactions in previously unimaginable ways. Our research investigates simulations of human interactions using LLMs. Through prompt engineering, inspired by Park et al. (2023), we present two simulations of believable proxies of human behavior: a two-agent negotiation and a six-agent murder mystery game.", "year": 2023, "publicationdate": "2023-08-14", "externalids": {"DOI": "10.48550/arXiv.2308.07411"}, "doi_lower": "10.48550/arxiv.2308.07411"}
{"paper_id": 274610062, "title": "Searching for Structure: Investigating Emergent Communication with Large Language Models", "author_names": ["Tom Kouwenhoven", "Max Peeperkorn", "Tessa Verhoef"], "venue": "International Conference on Computational Linguistics", "abstract": "Human languages have evolved to be structured through repeated language learning and use. These processes introduce biases that operate during language acquisition and shape linguistic systems toward communicative efficiency. In this paper, we investigate whether the same happens if artificial languages are optimised for implicit biases of Large Language Models (LLMs). To this end, we simulate a classical referential game in which LLMs learn and use artificial languages. Our results show that initially unstructured holistic languages are indeed shaped to have some structural properties that allow two LLM agents to communicate successfully. Similar to observations in human experiments, generational transmission increases the learnability of languages, but can at the same time result in non-humanlike degenerate vocabularies. Taken together, this work extends experimental findings, shows that LLMs can be used as tools in simulations of language evolution, and opens possibilities for future human-machine experiments in this field.", "year": 2024, "publicationdate": "2024-12-10", "externalids": {"DOI": "10.48550/arXiv.2412.07646"}, "doi_lower": "10.48550/arxiv.2412.07646"}
{"paper_id": 56665218, "title": "Complex Systems and Society: Modeling and Simulation", "author_names": ["N. Bellomo", "G. A. Marsan", "A. Tosin"], "venue": "", "abstract": null, "year": 2013, "publicationdate": "2013-05-24", "externalids": {"DOI": "10.1007/978-1-4614-7242-1"}, "doi_lower": "10.1007/978-1-4614-7242-1"}
{"paper_id": 109879215, "title": "Simulation modelling for sustainability: a review of the literature", "author_names": ["Y. Moon"], "venue": "", "abstract": null, "year": 2017, "publicationdate": "2017-01-02", "externalids": {"DOI": "10.1080/19397038.2016.1220990"}, "doi_lower": "10.1080/19397038.2016.1220990"}
{"paper_id": 257271887, "title": "ChatGPT and the AI Act", "author_names": ["N. Helberger", "N. Diakopoulos"], "venue": "Internet Policy Review", "abstract": null, "year": 2023, "publicationdate": "2023-02-16", "externalids": {"DOI": "10.14763/2023.1.1682"}, "doi_lower": "10.14763/2023.1.1682"}
{"paper_id": 244954639, "title": "Ethical and social risks of harm from Language Models", "author_names": ["Laura Weidinger", "John F. J. Mellor", "Maribeth Rauh", "Conor Griffin", "Jonathan Uesato", "Po-Sen Huang", "Myra Cheng", "Mia Glaese", "Borja Balle", "Atoosa Kasirzadeh", "Zachary Kenton", "S. Brown", "W. Hawkins", "T. Stepleton", "Courtney Biles", "Abeba Birhane", "Julia Haas", "Laura Rimell", "Lisa Anne Hendricks", "William S. Isaac", "Sean Legassick", "G. Irving", "Iason Gabriel"], "venue": "arXiv.org", "abstract": "This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.", "year": 2021, "publicationdate": "2021-12-08", "externalids": {}, "doi_lower": null}
{"paper_id": 258060002, "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models", "author_names": ["A. Deshpande", "Vishvak Murahari", "Tanmay Rajpurohit", "A. Kalyan", "Karthik Narasimhan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and harmful to an unsuspecting user. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.", "year": 2023, "publicationdate": "2023-04-11", "externalids": {"DOI": "10.48550/arXiv.2304.05335"}, "doi_lower": "10.48550/arxiv.2304.05335"}
{"paper_id": 236950797, "title": "Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models", "author_names": ["Hannah Rose Kirk", "Yennie Jun", "Haider Iqbal", "Elias Benussi", "Filippo Volpin", "F. Dreyer", "Aleksandar Shtedritski", "Yuki M. Asano"], "venue": "Neural Information Processing Systems", "abstract": "The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Open source libraries such as HuggingFace have made these models easily available and accessible. While prior research has identified biases in large language models, this paper considers biases contained in the most popular versions of these models when applied `out-of-the-box' for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Specifically, we conduct an in-depth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, with over half a million downloads per month. We assess biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by fitting 262 logistic models; (iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models should learn - whether they should reflect or correct for existing inequalities.", "year": 2021, "publicationdate": "2021-02-08", "externalids": {}, "doi_lower": null}
{"paper_id": 215828184, "title": "StereoSet: Measuring stereotypical bias in pretrained language models", "author_names": ["Moin Nadeem", "Anna Bethke", "Siva Reddy"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.", "year": 2020, "publicationdate": "2020-04-20", "externalids": {"DOI": "10.18653/v1/2021.acl-long.416"}, "doi_lower": "10.18653/v1/2021.acl-long.416"}
{"paper_id": 91181789, "title": "Assessing the Role of Social Media and Digital Technology in Violence Reporting", "author_names": ["Tony Roberts", "G. Marchais"], "venue": "", "abstract": null, "year": 2017, "publicationdate": "2017-08-01", "externalids": {"DOI": "10.22381/crlsj10220181"}, "doi_lower": "10.22381/crlsj10220181"}
{"paper_id": 253522998, "title": "Large Language Models Struggle to Learn Long-Tail Knowledge", "author_names": ["Nikhil Kandpal", "H. Deng", "Adam Roberts", "Eric Wallace", "Colin Raffel"], "venue": "International Conference on Machine Learning", "abstract": "The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.", "year": 2022, "publicationdate": "2022-11-15", "externalids": {}, "doi_lower": null}
{"paper_id": 258041203, "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models", "author_names": ["Emilio Ferrara"], "venue": "First Monday", "abstract": "As generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI.", "year": 2023, "publicationdate": "2023-04-07", "externalids": {"DOI": "10.5210/fm.v28i11.13346"}, "doi_lower": "10.5210/fm.v28i11.13346"}
{"paper_id": 261582269, "title": "OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs", "author_names": ["Patrick Haller", "Ansar Aynetdinov", "A. Akbik"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable ability to generate fitting responses to natural language instructions. However, an open research question concerns the inherent biases of trained models and their responses. For instance, if the data used to tune an LLM is dominantly written by persons with a specific political bias, we might expect generated answers to share this bias. Current research work seeks to de-bias such models, or suppress potentially biased answers.With this demonstration, we take a different view on biases in instruction-tuning: Rather than aiming to suppress them, we aim to make them explicit and transparent. To this end, we present OpinionGPT, a web demo in which users can ask questions and select all biases they wish to investigate. The demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. To train the underlying model, we identified 11 different biases (political, geographic, gender, age) and derived an instruction-tuning corpus in which each answer was written by members of one of these demographics. This paper presents OpinionGPT, illustrates how we trained the bias-aware model and showcases the web application (available at https://opiniongpt.informatik.hu-berlin.de).", "year": 2023, "publicationdate": "2023-09-07", "externalids": {"DOI": "10.48550/arXiv.2309.03876"}, "doi_lower": "10.48550/arxiv.2309.03876"}
{"paper_id": 258866192, "title": "In-Context Impersonation Reveals Large Language Models' Strengths and Biases", "author_names": ["Leonard Salewski", "Stephan Alaniz", "Isabel Rio-Torto", "Eric Schulz", "Zeynep Akata"], "venue": "Neural Information Processing Systems", "abstract": "In everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vision and language tasks. We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are complementary to visual information when describing different categories. We find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman. These findings demonstrate that LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their hidden strengths and biases.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.14930"}, "doi_lower": "10.48550/arxiv.2305.14930"}
{"paper_id": 257913805, "title": "Towards Healthy AI: Large Language Models Need Therapists Too", "author_names": ["Baihan Lin", "Djallel Bouneffouf", "G. Cecchi", "Kush R. Varshney"], "venue": "TRUSTNLP", "abstract": "Recent advances in large language models (LLMs) have led to the development of powerful chatbots capable of engaging in fluent human-like conversations. However, these chatbots may be harmful, exhibiting manipulation, gaslighting, narcissism, and other toxicity. To work toward safer and more well-adjusted models, we propose a framework that uses psychotherapy to identify and mitigate harmful chatbot behaviors. The framework involves four different artificial intelligence (AI) agents: the Chatbot whose behavior is to be adjusted, a User, a Therapist, and a Critic that can be paired with reinforcement learning-based LLM tuning. We illustrate the framework with a working example of a social conversation involving four instances of ChatGPT, showing that the framework may mitigate the toxicity in conversations between LLM-driven chatbots and people. Although there are still several challenges and directions to be addressed in the future, the proposed framework is a promising approach to improving the alignment between LLMs and human values.", "year": 2023, "publicationdate": "2023-04-02", "externalids": {"DOI": "10.48550/arXiv.2304.00416"}, "doi_lower": "10.48550/arxiv.2304.00416"}
{"paper_id": 235623756, "title": "Towards Understanding and Mitigating Social Biases in Language Models", "author_names": ["Paul Pu Liang", "Chiyu Wu", "Louis-philippe Morency", "R. Salakhutdinov"], "venue": "International Conference on Machine Learning", "abstract": "As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.", "year": 2021, "publicationdate": "2021-06-24", "externalids": {}, "doi_lower": null}
{"paper_id": 33499714, "title": "Ethical Challenges in Data-Driven Dialogue Systems", "author_names": ["Peter Henderson", "Koustuv Sinha", "Nicolas Angelard-Gontier", "Nan Rosemary Ke", "G. Fried", "Ryan Lowe", "Joelle Pineau"], "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.", "year": 2017, "publicationdate": "2017-11-24", "externalids": {"DOI": "10.1145/3278721.3278777"}, "doi_lower": "10.1145/3278721.3278777"}
{"paper_id": 248965347, "title": "You Don’t Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers’ Private Personas", "author_names": ["Haoran Li", "Yangqiu Song", "Lixin Fan"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Social chatbots, also known as chit-chat chatbots, evolve rapidly with large pretrained language models. Despite the huge progress, privacy concerns have arisen recently: training data of large language models can be extracted via model inversion attacks. On the other hand, the datasets used for training chatbots contain many private conversations between two individuals. In this work, we further investigate the privacy leakage of the hidden states of chatbots trained by language modeling which has not been well studied yet. We show that speakers’ personas can be inferred through a simple neural network with high accuracy. To this end, we propose effective defense objectives to protect persona leakage from hidden states. We conduct extensive experiments to demonstrate that our proposed defense objectives can greatly reduce the attack accuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preserve language models’ powerful generation ability.", "year": 2022, "publicationdate": "2022-04-26", "externalids": {"DOI": "10.48550/arXiv.2205.10228"}, "doi_lower": "10.48550/arxiv.2205.10228"}
{"paper_id": 246823897, "title": "What Does it Mean for a Language Model to Preserve Privacy?", "author_names": ["Hannah Brown", "Katherine Lee", "FatemehSadat Mireshghallah", "R. Shokri", "Florian Tramèr"], "venue": "Conference on Fairness, Accountability and Transparency", "abstract": "Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.", "year": 2022, "publicationdate": "2022-02-11", "externalids": {"DOI": "10.1145/3531146.3534642"}, "doi_lower": "10.1145/3531146.3534642"}
{"paper_id": 258926447, "title": "Privacy and Data Protection in ChatGPT and Other AI Chatbots: Strategies for Securing User Information", "author_names": ["Glorin Sebastian"], "venue": "Social Science Research Network", "abstract": "The evolution of artificial intelligence (AI) and machine learning (ML) has led to the development of sophisticated large language models (LLMs) that are used extensively in applications such as chatbots. This research investigates the critical issues of data protection and privacy enhancement in the context of LLM-based chatbots, with a focus on OpenAI's ChatGPT. It explores the dual challenges of safeguarding sensitive user information while ensuring the efficiency of machine learning models. It assesses existing privacy-enhancing technologies (PETs) and proposes innovative methods, such as differential privacy, federated learning, and data minimization techniques. The study also includes a survey of Chatbot users to measure their concerns related to data privacy with the use of these LLM-based applications. This study is meant to serve as a comprehensive guide for developers, policymakers, and researchers, contributing to the discourse on data protection in artificial intelligence.", "year": 2023, "publicationdate": "2023-07-11", "externalids": {"DOI": "10.2139/ssrn.4454761"}, "doi_lower": "10.2139/ssrn.4454761"}
{"paper_id": 62596838, "title": "MEDIA EQUATION: HOW PEOPLE TREAT COMPUTERS, TELEVISION, AND NEW MEDIA LIKE REAL PEOPLE AND PLACES", "author_names": ["Richard L. Soash"], "venue": "", "abstract": null, "year": 1999, "publicationdate": "1999-09-01", "externalids": {"DOI": "10.1300/J105V24N03_14"}, "doi_lower": "10.1300/j105v24n03_14"}
{"paper_id": 260098433, "title": "Left Unsettled", "author_names": ["Claudia Hilb’s"], "venue": "", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 253098566, "title": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task", "author_names": ["Kenneth Li", "Aspen K. Hopkins", "David Bau", "Fernanda Vi'egas", "H. Pfister", "M. Wattenberg"], "venue": "International Conference on Learning Representations", "abstract": "Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create\"latent saliency maps\"that can help explain predictions in human terms.", "year": 2022, "publicationdate": "2022-10-24", "externalids": {"DOI": "10.48550/arXiv.2210.13382"}, "doi_lower": "10.48550/arxiv.2210.13382"}
{"paper_id": 254823489, "title": "Constitutional AI: Harmlessness from AI Feedback", "author_names": ["Yuntao Bai", "Saurav Kadavath", "Sandipan Kundu", "Amanda Askell", "John Kernion", "Andy Jones", "A. Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "Carol Chen", "Catherine Olsson", "Chris Olah", "Danny Hernandez", "Dawn Drain", "Deep Ganguli", "Dustin Li", "Eli Tran-Johnson", "E. Perez", "Jamie Kerr", "J. Mueller", "Jeffrey Ladish", "J. Landau", "Kamal Ndousse", "Kamilė Lukošiūtė", "Liane Lovitt", "M. Sellitto", "Nelson Elhage", "Nicholas Schiefer", "Noem'i Mercado", "Nova Dassarma", "R. Lasenby", "Robin Larson", "Sam Ringer", "Scott Johnston", "Shauna Kravec", "S. E. Showk", "Stanislav Fort", "Tamera Lanham", "Timothy Telleen-Lawton", "Tom Conerly", "T. Henighan", "Tristan Hume", "Sam Bowman", "Zac Hatfield-Dodds", "Benjamin Mann", "Dario Amodei", "Nicholas Joseph", "Sam McCandlish", "Tom B. Brown", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.", "year": 2022, "publicationdate": "2022-12-15", "externalids": {"DOI": "10.48550/arXiv.2212.08073"}, "doi_lower": "10.48550/arxiv.2212.08073"}
{"paper_id": 248118878, "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "author_names": ["Yuntao Bai", "Andy Jones", "Kamal Ndousse", "Amanda Askell", "Anna Chen", "Nova Dassarma", "Dawn Drain", "Stanislav Fort", "Deep Ganguli", "T. Henighan", "Nicholas Joseph", "Saurav Kadavath", "John Kernion", "Tom Conerly", "S. El-Showk", "Nelson Elhage", "Zac Hatfield-Dodds", "Danny Hernandez", "Tristan Hume", "Scott Johnston", "Shauna Kravec", "Liane Lovitt", "Neel Nanda", "Catherine Olsson", "Dario Amodei", "Tom B. Brown", "Jack Clark", "Sam McCandlish", "Chris Olah", "Benjamin Mann", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "We apply preference modeling and reinforcement learning from human feedback (RLHF) to ﬁnetune language models to act as helpful and harmless assistants. We ﬁnd this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efﬁciently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work. Figure These plots show that PM accuracy decreases as we focus exclusively on comparisons between pairs of samples with high score. We have normalized all preference models to have the same mean score on a held-out dataset so that they’re directly comparable, and then plotted accuracy for the comparisons where both samples have scores above a speciﬁc threshold.", "year": 2022, "publicationdate": "2022-04-12", "externalids": {"DOI": "10.48550/arXiv.2204.05862"}, "doi_lower": "10.48550/arxiv.2204.05862"}
{"paper_id": 260682249, "title": "AgentBench: Evaluating LLMs as Agents", "author_names": ["Xiao Liu", "Hao Yu", "Hanchen Zhang", "Yifan Xu", "Xuanyu Lei", "Hanyu Lai", "Yu Gu", "Yuxian Gu", "Hangliang Ding", "Kai Men", "Kejuan Yang", "Shudan Zhang", "Xiang Deng", "Aohan Zeng", "Zhengxiao Du", "Chenhui Zhang", "Shengqi Shen", "Tianjun Zhang", "Sheng Shen", "Yu Su", "Huan Sun", "Minlie Huang", "Yuxiao Dong", "Jie Tang"], "venue": "International Conference on Learning Representations", "abstract": "The potential of Large Language Model (LLM) as agents has been widely acknowledged recently. Thus, there is an urgent need to quantitatively \\textit{evaluate LLMs as agents} on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities. Our extensive test over \\num API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Improving instruction following and training on high quality multi-round alignment data could improve agent performance. And different from existing assumptions, training on code present ambivalent impacts on different agent tasks. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.", "year": 2023, "publicationdate": "2023-08-07", "externalids": {"DOI": "10.48550/arXiv.2308.03688"}, "doi_lower": "10.48550/arxiv.2308.03688"}
{"paper_id": 251719353, "title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies", "author_names": ["Gati Aher", "RosaI. Arriaga", "A. Kalai"], "venue": "International Conference on Machine Learning", "abstract": "We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a\"hyper-accuracy distortion\"present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.", "year": 2022, "publicationdate": "2022-08-18", "externalids": {}, "doi_lower": null}
{"paper_id": 260125274, "title": "Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models", "author_names": ["Yuanzhi Liang", "Linchao Zhu", "Yezhou Yang"], "venue": "arXiv.org", "abstract": "Recent advancements in natural language and Large Language Models (LLMs) have enabled AI agents to simulate human-like interactions within virtual worlds. However, these interactions still face limitations in complexity and flexibility, particularly in scenarios involving multiple characters and novel objects. Pre-defining all interactable objects in the agent's world model presents challenges, and conveying implicit intentions to multiple characters through complex interactions remains difficult. To address these issues, we propose integrating virtual Game Masters (GMs) into the agent's world model, drawing inspiration from Tabletop Role-Playing Games (TRPGs). GMs play a crucial role in overseeing information, estimating players' intentions, providing environment descriptions, and offering feedback, compensating for current world model deficiencies. To facilitate future explorations for complex interactions, we introduce a benchmark named Tachikuma, comprising a Multiple character and novel Object based interaction Estimation (MOE) task and a supporting dataset. MOE challenges models to understand characters' intentions and accurately determine their actions within intricate contexts involving multi-character and novel object interactions. Besides, the dataset captures log data from real-time communications during gameplay, providing diverse, grounded, and complex interactions for further explorations. Finally, we present a simple prompting baseline and evaluate its performance, demonstrating its effectiveness in enhancing interaction understanding. We hope that our dataset and task will inspire further research in complex interactions with natural language, fostering the development of more advanced AI agents.", "year": 2023, "publicationdate": "2023-07-24", "externalids": {"DOI": "10.48550/arXiv.2307.12573"}, "doi_lower": "10.48550/arxiv.2307.12573"}
{"paper_id": 260704331, "title": "Gentopia: A Collaborative Platform for Tool-Augmented LLMs", "author_names": ["Binfeng Xu", "Xukun Liu", "Hua Shen", "Zeyu Han", "Yuhan Li", "Murong Yue", "Zhi-Ping Peng", "Yuchen Liu", "Ziyu Yao", "Dongkuan Xu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Augmented Language Models (ALMs) empower large language models with the ability to use tools, transforming them into intelligent agents for real-world interactions. However, most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible customization, collaborative democratization, and holistic evaluation. We present gentopia, an ALM framework enabling flexible customization of agents through simple configurations, seamlessly integrating various language models, task formats, prompting modules, and plugins into a unified paradigm. Furthermore, we establish gentpool, a public platform enabling the registration and sharing of user-customized agents. Agents registered in gentpool are composable such that they can be assembled together for agent collaboration, advancing the democratization of artificial intelligence. To ensure high-quality agents, gentbench, an integral component of gentpool, is designed to thoroughly evaluate user-customized agents across diverse aspects such as safety, robustness, efficiency, etc. We release gentopia on Github and will continuously move forward.", "year": 2023, "publicationdate": "2023-08-08", "externalids": {"DOI": "10.48550/arXiv.2308.04030"}, "doi_lower": "10.48550/arxiv.2308.04030"}
{"paper_id": 252780815, "title": "\"Help Me Help the AI\": Understanding How Explainability Can Support Human-AI Interaction", "author_names": ["Sunnie S. Y. Kim", "E. A. Watkins", "Olga Russakovsky", "Ruth C. Fong", "A. Monroy-Hernández"], "venue": "International Conference on Human Factors in Computing Systems", "abstract": "Despite the proliferation of explainable AI (XAI) methods, little is understood about end-users’ explainability needs and behaviors around XAI explanations. To address this gap and contribute to understanding how explainability can support human-AI interaction, we conducted a mixed-methods study with 20 end-users of a real-world AI application, the Merlin bird identification app, and inquired about their XAI needs, uses, and perceptions. We found that participants desire practically useful information that can improve their collaboration with the AI, more so than technical system details. Relatedly, participants intended to use XAI explanations for various purposes beyond understanding the AI’s outputs: calibrating trust, improving their task skills, changing their behavior to supply better inputs to the AI, and giving constructive feedback to developers. Finally, among existing XAI approaches, participants preferred part-based explanations that resemble human reasoning and explanations. We discuss the implications of our findings and provide recommendations for future XAI design.", "year": 2022, "publicationdate": "2022-10-02", "externalids": {"DOI": "10.1145/3544548.3581001"}, "doi_lower": "10.1145/3544548.3581001"}
{"paper_id": 258865939, "title": "Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark", "author_names": ["Minje Choi", "Jiaxin Pei", "Sagar Kumar", "Chang Shu", "David Jurgens"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand \\textit{social} language. Here, we introduce a new theory-driven benchmark, SocKET, that contains 58 NLP tasks testing social knowledge which we group into five categories: humor&sarcasm, offensiveness, sentiment&emotion, and trustworthiness. In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory. Through zero-shot evaluations, we show that pretrained models already possess some innate but limited capabilities of social language understanding and training on one category of tasks can improve zero-shot testing on others. Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs. The associated resources are released at https://github.com/minjechoi/SOCKET.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.18653/v1/2023.emnlp-main.699"}, "doi_lower": "10.18653/v1/2023.emnlp-main.699"}
{"paper_id": 150662692, "title": "\"If you catch my drift...\": ability to infer implied meaning is distinct from vocabulary and grammar skills", "author_names": ["Alexander C. Wilson", "D. Bishop"], "venue": "Wellcome Open Research", "abstract": "Background: Some individuals with autism find it challenging to use and understand language in conversation, despite having good abilities in core aspects of language such as grammar and vocabulary. This suggests that pragmatic skills (such as understanding implied meanings in conversation) are separable from core language skills. However, it has been surprisingly difficult to demonstrate this dissociation in the general population. We propose that this may be because prior studies have used tasks in which different aspects of language are confounded. Methods: The present study used novel language tasks and factor analysis to test whether pragmatic understanding of implied meaning, as part of a broader domain involving social understanding, is separable from core language skills. 120 adult participants were recruited online to complete a 7-task battery, including a test assessing comprehension of conversational implicature. Results: In confirmatory analysis of a preregistered model, we compared whether the data showed better fit to a two-factor structure (including a “social understanding” and “core language” factor) or a simpler one-factor structure (comprising a general factor). The two-factor model showed significantly better fit. Conclusions: This study supports the view that interpreting context-dependent conversational meaning is partially distinct from core language skills. This has implications for understanding the pragmatic language impairments reported in autism.", "year": 2019, "publicationdate": "2019-04-15", "externalids": {"DOI": "10.12688/wellcomeopenres.15210.3"}, "doi_lower": "10.12688/wellcomeopenres.15210.3"}
{"paper_id": 245124169, "title": "Am I Me or You? State-of-the-Art Dialogue Models Cannot Maintain an Identity", "author_names": ["Kurt Shuster", "Jack Urbanek", "Arthur Szlam", "J. Weston"], "venue": "NAACL-HLT", "abstract": "State-of-the-art dialogue models still often stumble with regards to factual accuracy and self-contradiction. Anecdotally, they have been observed to fail to maintain character identity throughout discourse; and more specifically, may take on the role of their interlocutor. In this work we formalize and quantify this deficiency, and show experimentally through human evaluations that this is indeed a problem. In contrast, we show that discriminative models trained specifically to recognize who is speaking can perform well; and further, these can be used as automated metrics. Finally, we evaluate a wide variety of mitigation methods, including changes to model architecture, training protocol, and decoding strategy. Our best models reduce mistaken identity issues by nearly 65% according to human annotators, while simultaneously improving engagingness. Despite these results, we find that maintaining character identity still remains a challenging problem.", "year": 2021, "publicationdate": "2021-12-10", "externalids": {"DOI": "10.18653/v1/2022.findings-naacl.182"}, "doi_lower": "10.18653/v1/2022.findings-naacl.182"}
{"paper_id": 252355458, "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned", "author_names": ["Deep Ganguli", "Liane Lovitt", "John Kernion", "Amanda Askell", "Yuntao Bai", "Saurav Kadavath", "Benjamin Mann", "Ethan Perez", "Nicholas Schiefer", "Kamal Ndousse", "Andy Jones", "Sam Bowman", "Anna Chen", "Tom Conerly", "Nova Dassarma", "Dawn Drain", "Nelson Elhage", "S. El-Showk", "Stanislav Fort", "Z. Dodds", "T. Henighan", "Danny Hernandez", "Tristan Hume", "Josh Jacobson", "Scott Johnston", "Shauna Kravec", "Catherine Olsson", "Sam Ringer", "Eli Tran-Johnson", "Dario Amodei", "Tom B. Brown", "Nicholas Joseph", "Sam McCandlish", "Chris Olah", "Jared Kaplan", "Jack Clark"], "venue": "arXiv.org", "abstract": "We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.", "year": 2022, "publicationdate": "2022-08-23", "externalids": {"DOI": "10.48550/arXiv.2209.07858"}, "doi_lower": "10.48550/arxiv.2209.07858"}
{"paper_id": 250451161, "title": "Language Models (Mostly) Know What They Know", "author_names": ["Saurav Kadavath", "Tom Conerly", "Amanda Askell", "T. Henighan", "Dawn Drain", "Ethan Perez", "Nicholas Schiefer", "Z. Dodds", "Nova Dassarma", "Eli Tran-Johnson", "Scott Johnston", "S. El-Showk", "Andy Jones", "Nelson Elhage", "Tristan Hume", "Anna Chen", "Yuntao Bai", "Sam Bowman", "Stanislav Fort", "Deep Ganguli", "Danny Hernandez", "Josh Jacobson", "John Kernion", "Shauna Kravec", "Liane Lovitt", "Kamal Ndousse", "Catherine Olsson", "Sam Ringer", "Dario Amodei", "Tom B. Brown", "Jack Clark", "Nicholas Joseph", "Benjamin Mann", "Sam McCandlish", "Chris Olah", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability\"P(True)\"that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict\"P(IK)\", the probability that\"I know\"the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.", "year": 2022, "publicationdate": "2022-07-11", "externalids": {"DOI": "10.48550/arXiv.2207.05221"}, "doi_lower": "10.48550/arxiv.2207.05221"}
{"paper_id": 258832563, "title": "Augmenting Autotelic Agents with Large Language Models", "author_names": ["Cédric Colas", "Laetitia Teodorescu", "Pierre-Yves Oudeyer", "Xingdi Yuan", "Marc-Alexandre Côté"], "venue": "CoLLAs", "abstract": "Humans learn to master open-ended repertoires of skills by imagining and practicing their own goals. This autotelic learning process, literally the pursuit of self-generated (auto) goals (telos), becomes more and more open-ended as the goals become more diverse, abstract and creative. The resulting exploration of the space of possible skills is supported by an inter-individual exploration: goal representations are culturally evolved and transmitted across individuals, in particular using language. Current artificial agents mostly rely on predefined goal representations corresponding to goal spaces that are either bounded (e.g. list of instructions), or unbounded (e.g. the space of possible visual inputs) but are rarely endowed with the ability to reshape their goal representations, to form new abstractions or to imagine creative goals. In this paper, we introduce a language model augmented autotelic agent (LMA3) that leverages a pretrained language model (LM) to support the representation, generation and learning of diverse, abstract, human-relevant goals. The LM is used as an imperfect model of human cultural transmission; an attempt to capture aspects of humans' common-sense, intuitive physics and overall interests. Specifically, it supports three key components of the autotelic architecture: 1)~a relabeler that describes the goals achieved in the agent's trajectories, 2)~a goal generator that suggests new high-level goals along with their decomposition into subgoals the agent already masters, and 3)~reward functions for each of these goals. Without relying on any hand-coded goal representations, reward functions or curriculum, we show that LMA3 agents learn to master a large diversity of skills in a task-agnostic text-based environment.", "year": 2023, "publicationdate": "2023-05-21", "externalids": {"DOI": "10.48550/arXiv.2305.12487"}, "doi_lower": "10.48550/arxiv.2305.12487"}
{"paper_id": 260495344, "title": "Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence", "author_names": ["Arslan Chaudhry"], "venue": "", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 195453293, "title": "Learning a Unified Classifier Incrementally via Rebalancing", "author_names": ["Saihui Hou", "Xinyu Pan", "Chen Change Loy", "Zilei Wang", "Dahua Lin"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Conventionally, deep neural networks are trained offline, relying on a large dataset prepared in advance. This paradigm is often challenged in real-world applications, e.g. online services that involve continuous streams of incoming data. Recently, incremental learning receives increasing attention, and is considered as a promising solution to the practical challenges mentioned above. However, it has been observed that incremental learning is subject to a fundamental difficulty -- catastrophic forgetting, namely adapting a model to new data often results in severe performance degradation on previous tasks or classes. Our study reveals that the imbalance between previous and new data is a crucial cause to this problem. In this work, we develop a new framework for incrementally learning a unified classifier, e.g. a classifier that treats both old and new classes uniformly. Specifically, we incorporate three components, cosine normalization, less-forget constraint, and inter-class separation, to mitigate the adverse effects of the imbalance. Experiments show that the proposed method can effectively rebalance the training process, thus obtaining superior performance compared to the existing methods. On CIFAR-100 and ImageNet, our method can reduce the classification errors by more than 6% and 13% respectively, under the incremental setting of 10 phases.", "year": 2019, "publicationdate": "2019-06-01", "externalids": {"DOI": "10.1109/CVPR.2019.00092"}, "doi_lower": "10.1109/cvpr.2019.00092"}
{"paper_id": 229297493, "title": "Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey", "author_names": ["Cédric Colas", "Tristan Karch", "Olivier Sigaud", "Pierre-Yves Oudeyer"], "venue": "Journal of Artificial Intelligence Research", "abstract": "Building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial intelligence. Developmental approaches argue that this can only be achieved by autotelic agents: intrinsically motivated learning agents that can learn to represent, generate, select and solve their own problems. In recent years, the convergence of developmental approaches with deep reinforcement learning (RL) methods has been leading to the emergence of a new field: developmental reinforcement learning. Developmental RL is concerned with the use of deep RL algorithms to tackle a developmental problem— the intrinsically motivated acquisition of open-ended repertoires of skills. The self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions. This raises new challenges compared to standard RL algorithms originally designed to tackle pre-defined sets of goals using external reward signals. The present paper introduces developmental RL and proposes a computational framework based on goal-conditioned RL to tackle the intrinsically motivated skills acquisition problem. It proceeds to present a typology of the various goal representations used in the literature, before reviewing existing methods to learn to represent and prioritize goals in autonomous systems. We finally close the paper by discussing some open challenges in the quest of intrinsically motivated skills acquisition.", "year": 2020, "publicationdate": "2020-12-17", "externalids": {"DOI": "10.1613/jair.1.13554"}, "doi_lower": "10.1613/jair.1.13554"}
{"paper_id": 604334, "title": "Intriguing properties of neural networks", "author_names": ["Christian Szegedy", "Wojciech Zaremba", "I. Sutskever", "Joan Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "International Conference on Learning Representations", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. \nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. \nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "year": 2013, "publicationdate": "2013-12-20", "externalids": {}, "doi_lower": null}
{"paper_id": 6706414, "title": "Explaining and Harnessing Adversarial Examples", "author_names": ["I. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "International Conference on Learning Representations", "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.", "year": 2014, "publicationdate": "2014-12-19", "externalids": {}, "doi_lower": null}
{"paper_id": 3488815, "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "author_names": ["A. Ma̧dry", "Aleksandar Makelov", "Ludwig Schmidt", "Dimitris Tsipras", "Adrian Vladu"], "venue": "International Conference on Learning Representations", "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.", "year": 2017, "publicationdate": "2017-06-19", "externalids": {}, "doi_lower": null}
{"paper_id": 259858905, "title": "Characterizing the Impacts of Instances on Robustness", "author_names": ["Rui Zheng", "Zhiheng Xi", "Qin Liu", "W. Lai", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Jin Ma", "Yingchun Shan", "Weifeng Ge"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Building robust deep neural networks (DNNs) against adversarial attacks is an important but challenging task. Previous defense approaches mainly focus on developing new model structures or training algorithms, but they do little to tap the potential of training instances, especially instances with robust patterns car-ring innate robustness. In this paper, we show that robust and non-robust instances in the training dataset, though are both important for test performance, have contrary impacts on robustness, which makes it possible to build a highly robust model by leveraging the training dataset in a more effective way. We pro-pose a new method that can distinguish robust instances from non-robust ones according to the model’s sensitivity to perturbations on individual instances during training. Surprisingly, we find that the model under standard training easily overfits the robust instances by relying on their simple patterns before the model completely learns their robust features. Finally, we propose a new mitigation algorithm to further release the potential of robust instances. Experimental results show that proper use of robust instances in the original dataset is a new line to achieve highly robust models. Our codes are publicly available at https: //github.com/ruizheng20/robust_data .", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.findings-acl.146"}, "doi_lower": "10.18653/v1/2023.findings-acl.146"}
{"paper_id": 261341825, "title": "Safety and Ethical Concerns of Large Language Models", "author_names": ["Zhiheng Xi", "Zheng Rui", "Gui Tao"], "venue": "China National Conference on Chinese Computational Linguistics", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 236772715, "title": "Threat of Adversarial Attacks on Deep Learning in Computer Vision: Survey II", "author_names": ["Naveed Akhtar", "A. Mian", "Navid Kardan", "M. Shah"], "venue": "arXiv.org", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 244773637, "title": "A Systematic Review of Robustness in Deep Learning for Computer Vision: Mind the gap?", "author_names": ["Nathan Drenkow", "Numair Sani", "I. Shpitser", "M. Unberath"], "venue": "", "abstract": "Deep neural networks for computer vision are deployed in increasingly safety-critical and socially-impactful applications, motivating the need to close the gap in model performance under varied, naturally occurring imaging conditions. Robustness, ambiguously used in multiple contexts including adversarial machine learning, refers here to preserving model performance under naturally-induced image corruptions or alterations. We perform a systematic review to identify, analyze, and summarize current definitions and progress towards non-adversarial robustness in deep learning for computer vision. We find this area of research has received disproportionately less attention relative to adversarial machine learning, yet a significant robustness gap exists that manifests in performance degradation similar in magnitude to adversarial conditions. Toward developing a more transparent definition of robustness, we provide a conceptual framework based on a structural causal model of the data generating process and interpret non-adversarial robustness as pertaining to a model's behavior on corrupted images corresponding to low-probability samples from the unaltered data distribution. We identify key architecture-, data augmentation-, and optimization tactics for improving neural network robustness. This robustness perspective reveals that common practices in the literature correspond to causal concepts. We offer perspectives on how future research may mind this evident and significant non-adversarial robustness gap.", "year": 2021, "publicationdate": "2021-12-01", "externalids": {}, "doi_lower": null}
{"paper_id": 56657912, "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations", "author_names": ["Dan Hendrycks", "Thomas G. Dietterich"], "venue": "International Conference on Learning Representations", "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.", "year": 2019, "publicationdate": "2019-03-28", "externalids": {}, "doi_lower": null}
{"paper_id": 245144787, "title": "Measure and Improve Robustness in NLP Models: A Survey", "author_names": ["Xuezhi Wang", "Haohan Wang", "Diyi Yang"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "As NLP models achieved state-of-the-art performances over benchmarks and gained wide applications, it has been increasingly important to ensure the safe deployment of these models in the real world, e.g., making sure the models are robust against unseen or challenging scenarios. Despite robustness being an increasingly studied topic, it has been separately explored in applications like vision and NLP, with various definitions, evaluation and mitigation strategies in multiple lines of research. In this paper, we aim to provide a unifying survey of how to define, measure and improve robustness in NLP. We first connect multiple definitions of robustness, then unify various lines of work on identifying robustness failures and evaluating models’ robustness. Correspondingly, we present mitigation strategies that are data-driven, model-driven, and inductive-prior-based, with a more systematic view of how to effectively improve robustness in NLP models. Finally, we conclude by outlining open challenges and future directions to motivate further research in this area.", "year": 2021, "publicationdate": "2021-12-15", "externalids": {"DOI": "10.18653/v1/2022.naacl-main.339"}, "doi_lower": "10.18653/v1/2022.naacl-main.339"}
{"paper_id": 54815878, "title": "TextBugger: Generating Adversarial Text Against Real-world Applications", "author_names": ["Jinfeng Li", "S. Ji", "Tianyu Du", "Bo Li", "Ting Wang"], "venue": "Network and Distributed System Security Symposium", "abstract": "Deep Learning-based Text Understanding (DLTU) is the backbone technique behind various applications, including question answering, machine translation, and text classification. Despite its tremendous popularity, the security vulnerabilities of DLTU are still largely unknown, which is highly concerning given its increasing use in security-sensitive applications such as sentiment analysis and toxic content detection. In this paper, we show that DLTU is inherently vulnerable to adversarial text attacks, in which maliciously crafted texts trigger target DLTU systems and services to misbehave. Specifically, we present TextBugger, a general attack framework for generating adversarial texts. In contrast to prior works, TextBugger differs in significant ways: (i) effective -- it outperforms state-of-the-art attacks in terms of attack success rate; (ii) evasive -- it preserves the utility of benign text, with 94.9\\% of the adversarial text correctly recognized by human readers; and (iii) efficient -- it generates adversarial text with computational complexity sub-linear to the text length. We empirically evaluate TextBugger on a set of real-world DLTU systems and services used for sentiment analysis and toxic content detection, demonstrating its effectiveness, evasiveness, and efficiency. For instance, TextBugger achieves 100\\% success rate on the IMDB dataset based on Amazon AWS Comprehend within 4.61 seconds and preserves 97\\% semantic similarity. We further discuss possible defense mechanisms to mitigate such attack and the adversary's potential countermeasures, which leads to promising directions for further research.", "year": 2018, "publicationdate": "2018-12-13", "externalids": {"DOI": "10.14722/ndss.2019.23138"}, "doi_lower": "10.14722/ndss.2019.23138"}
{"paper_id": 209475786, "title": "FreeLB: Enhanced Adversarial Training for Natural Language Understanding", "author_names": ["Chen Zhu", "Yu Cheng", "Zhe Gan", "S. Sun", "T. Goldstein", "Jingjing Liu"], "venue": "International Conference on Learning Representations", "abstract": "Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm, FreeLB, that promotes higher invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the GLUE benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of BERT-base model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art single-model test accuracies of 85.44\\% and 67.75\\% on ARC-Easy and ARC-Challenge. Experiments on CommonsenseQA benchmark further demonstrate that FreeLB can be generalized and boost the performance of RoBERTa-large model on other tasks as well. Code is available at \\url{this https URL .", "year": 2019, "publicationdate": "2019-09-25", "externalids": {}, "doi_lower": null}
{"paper_id": 253510521, "title": "Efficient Adversarial Training with Robust Early-Bird Tickets", "author_names": ["Zhiheng Xi", "Rui Zheng", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Adversarial training is one of the most powerful methods to improve the robustness of pre-trained language models (PLMs). However, this approach is typically more expensive than traditional fine-tuning because of the necessity to generate adversarial examples via gradient descent. Delving into the optimization process of adversarial training, we find that robust connectivity patterns emerge in the early training phase (typically 0.15~0.3 epochs), far before parameters converge. Inspired by this finding, we dig out robust early-bird tickets (i.e., subnetworks) to develop an efficient adversarial training method: (1) searching for robust tickets with structured sparsity in the early stage; (2) fine-tuning robust tickets in the remaining time. To extract the robust tickets as early as possible, we design a ticket convergence metric to automatically terminate the searching process. Experiments show that the proposed efficient adversarial training method can achieve up to 7\\times \\sim 13 \\times training speedups while maintaining comparable or even better robustness compared to the most competitive state-of-the-art adversarial training methods.", "year": 2022, "publicationdate": "2022-11-14", "externalids": {"DOI": "10.48550/arXiv.2211.07263"}, "doi_lower": "10.48550/arxiv.2211.07263"}
{"paper_id": 15313471, "title": "Robust Adversarial Reinforcement Learning", "author_names": ["Lerrel Pinto", "James Davidson", "R. Sukthankar", "A. Gupta"], "venue": "International Conference on Machine Learning", "abstract": "Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H∞ control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced - that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper, Walker2d and Ant) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.", "year": 2017, "publicationdate": "2017-03-08", "externalids": {}, "doi_lower": null}
{"paper_id": 248405675, "title": "RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning", "author_names": ["Marc Rigter", "Bruno Lacerda", "Nick Hawes"], "venue": "Neural Information Processing Systems", "abstract": "Offline reinforcement learning (RL) aims to find performant policies from logged data without further environment interaction. Model-based algorithms, which learn a model of the environment from the dataset and perform conservative policy optimisation within that model, have emerged as a promising approach to this problem. In this work, we present Robust Adversarial Model-Based Offline RL (RAMBO), a novel approach to model-based offline RL. We formulate the problem as a two-player zero sum game against an adversarial environment model. The model is trained to minimise the value function while still accurately predicting the transitions in the dataset, forcing the policy to act conservatively in areas not covered by the dataset. To approximately solve the two-player game, we alternate between optimising the policy and adversarially optimising the model. The problem formulation that we address is theoretically grounded, resulting in a probably approximately correct (PAC) performance guarantee and a pessimistic value function which lower bounds the value function in the true environment. We evaluate our approach on widely studied offline RL benchmarks, and demonstrate that it outperforms existing state-of-the-art baselines.", "year": 2022, "publicationdate": "2022-04-26", "externalids": {"DOI": "10.48550/arXiv.2204.12581"}, "doi_lower": "10.48550/arxiv.2204.12581"}
{"paper_id": 251468252, "title": "Robust Reinforcement Learning using Offline Data", "author_names": ["Kishan Panaganti", "Zaiyan Xu", "D. Kalathil", "M. Ghavamzadeh"], "venue": "Neural Information Processing Systems", "abstract": "The goal of robust reinforcement learning (RL) is to learn a policy that is robust against the uncertainty in model parameters. Parameter uncertainty commonly occurs in many real-world RL applications due to simulator modeling errors, changes in the real-world system dynamics over time, and adversarial disturbances. Robust RL is typically formulated as a max-min problem, where the objective is to learn the policy that maximizes the value against the worst possible models that lie in an uncertainty set. In this work, we propose a robust RL algorithm called Robust Fitted Q-Iteration (RFQI), which uses only an offline dataset to learn the optimal robust policy. Robust RL with offline data is significantly more challenging than its non-robust counterpart because of the minimization over all models present in the robust Bellman operator. This poses challenges in offline data collection, optimization over the models, and unbiased estimation. In this work, we propose a systematic approach to overcome these challenges, resulting in our RFQI algorithm. We prove that RFQI learns a near-optimal robust policy under standard assumptions and demonstrate its superior performance on standard benchmark problems.", "year": 2022, "publicationdate": "2022-08-10", "externalids": {"DOI": "10.48550/arXiv.2208.05129"}, "doi_lower": "10.48550/arxiv.2208.05129"}
{"paper_id": 253415319, "title": "The Impact of Autopilot on Tesla", "author_names": ["Runze Chen", "Hankai Mao"], "venue": "BCP Business &amp; Management", "abstract": "As Tesla advances in technology, Tesla is expeditiously embarking on exploring an emerging field, driverless technology. Due to the current instability of driverless technology, driverless systems are not commonly used at the moment. Nevertheless, its impact on Tesla can not be neglected. Therefore, this study focuses on the impact of the emergence of autonomous driving on Tesla. Specifically, this paper explores the impact brought about by autonomous driving by collecting statistical data, gathering real-life cases, and analyzing the information. However, the research illustrates that Tesla’s Autopilot is a double-edged sword. It damages the reputation of Tesla while offering the huge potential for gaining tremendous revenue in the present and future. In the long run, the scales are tipped in favor of autonomous driving technology.  Thus, persisting in exploring the field of driverless technology will speed up the promotion of Tesla.", "year": 2022, "publicationdate": "2022-11-05", "externalids": {"DOI": "10.54691/bcpbm.v31i.2540"}, "doi_lower": "10.54691/bcpbm.v31i.2540"}
{"paper_id": 208310168, "title": "Adversarial T-Shirt! Evading Person Detectors in a Physical World", "author_names": ["Kaidi Xu", "Gaoyuan Zhang", "Sijia Liu", "Quanfu Fan", "Mengshu Sun", "Hongge Chen", "Pin-Yu Chen", "Yanzhi Wang", "Xue Lin"], "venue": "European Conference on Computer Vision", "abstract": null, "year": 2019, "publicationdate": "2019-10-18", "externalids": {"DOI": "10.1007/978-3-030-58558-7_39"}, "doi_lower": "10.1007/978-3-030-58558-7_39"}
{"paper_id": 207241700, "title": "Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition", "author_names": ["Mahmood Sharif", "Sruti Bhagavatula", "Lujo Bauer", "M. Reiter"], "venue": "Conference on Computer and Communications Security", "abstract": null, "year": 2016, "publicationdate": "2016-10-24", "externalids": {"DOI": "10.1145/2976749.2978392"}, "doi_lower": "10.1145/2976749.2978392"}
{"paper_id": 202539059, "title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment", "author_names": ["Di Jin", "Zhijing Jin", "Joey Tianyi Zhou", "Peter Szolovits"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective—it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving—it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient—it generates adversarial text with computational complexity linear to the text length.1", "year": 2019, "publicationdate": "2019-07-27", "externalids": {"DOI": "10.1609/AAAI.V34I05.6311"}, "doi_lower": "10.1609/aaai.v34i05.6311"}
{"paper_id": 196202909, "title": "Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency", "author_names": ["Shuhuai Ren", "Yihe Deng", "Kun He", "Wanxiang Che"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We address the problem of adversarial attacks on text classification, which is rarely studied comparing to attacks on image classification. The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a greedy algorithm called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the models. At last, our method also exhibits a good transferability on the generated adversarial examples.", "year": 2019, "publicationdate": "2019-07-01", "externalids": {"DOI": "10.18653/v1/P19-1103"}, "doi_lower": "10.18653/v1/p19-1103"}
{"paper_id": 271493049, "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts", "author_names": ["Kaijie Zhu", "Jindong Wang", "Jiaheng Zhou", "Zichen Wang", "Hao Chen", "Yidong Wang", "Linyi Yang", "Weirong Ye", "Neil Zhenqiang Gong", "Yue Zhang", "Xing Xie"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.04528"}, "doi_lower": "10.48550/arxiv.2306.04528"}
{"paper_id": 257255121, "title": "How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks", "author_names": ["Xuanting Chen", "Junjie Ye", "Can Zu", "Nuo Xu", "Rui Zheng", "Minlong Peng", "Jie Zhou", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "venue": "arXiv.org", "abstract": "The GPT-3.5 models have demonstrated impressive performance in various Natural Language Processing (NLP) tasks, showcasing their strong understanding and reasoning capabilities. However, their robustness and abilities to handle various complexities of the open world have yet to be explored, which is especially crucial in assessing the stability of models and is a key aspect of trustworthy AI. In this study, we perform a comprehensive experimental analysis of GPT-3.5, exploring its robustness using 21 datasets (about 116K test samples) with 66 text transformations from TextFlint that cover 9 popular Natural Language Understanding (NLU) tasks. Our findings indicate that while GPT-3.5 outperforms existing fine-tuned models on some tasks, it still encounters significant robustness degradation, such as its average performance dropping by up to 35.74\\% and 43.59\\% in natural language inference and sentiment analysis tasks, respectively. We also show that GPT-3.5 faces some specific robustness challenges, including robustness instability, prompt sensitivity, and number sensitivity. These insights are valuable for understanding its limitations and guiding future research in addressing these challenges to enhance GPT-3.5's overall performance and generalization abilities.", "year": 2023, "publicationdate": "2023-03-01", "externalids": {"DOI": "10.48550/arXiv.2303.00293"}, "doi_lower": "10.48550/arxiv.2303.00293"}
{"paper_id": 26783139, "title": "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain", "author_names": ["Tianyu Gu", "Brendan Dolan-Gavitt", "S. Garg"], "venue": "arXiv.org", "abstract": "Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a \\emph{BadNet}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of {25}\\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.", "year": 2017, "publicationdate": "2017-08-22", "externalids": {}, "doi_lower": null}
{"paper_id": 238354397, "title": "BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements", "author_names": ["Xiaoyi Chen", "A. Salem", "Dingfan Chen", "M. Backes", "Shiqing Ma", "Qingni Shen", "Zhonghai Wu", "Yang Zhang"], "venue": "Asia-Pacific Computer Systems Architecture Conference", "abstract": "Deep neural networks (DNNs) have progressed rapidly during the past decade and have been deployed in various real-world applications. Meanwhile, DNN models have been shown to be vulnerable to security and privacy attacks. One such attack that has attracted a great deal of attention recently is the backdoor attack. Specifically, the adversary poisons the target model’s training set to mislead any input with an added secret trigger to a target class. Previous backdoor attacks predominantly focus on computer vision (CV) applications, such as image classification. In this paper, we perform a systematic investigation of backdoor attack on NLP models, and propose BadNL, a general NLP backdoor attack framework including novel attack methods. Specifically, we propose three methods to construct triggers, namely BadChar, BadWord, and BadSentence, including basic and semantic-preserving variants. Our attacks achieve an almost perfect attack success rate with a negligible effect on the original model’s utility. For instance, using the BadChar, our backdoor attack achieves a 98.9% attack success rate with yielding a utility improvement of 1.5% on the SST-5 dataset when only poisoning 3% of the original set. Moreover, we conduct a user study to prove that our triggers can well preserve the semantics from humans perspective.", "year": 2020, "publicationdate": "2020-06-01", "externalids": {"DOI": "10.1145/3485832.3485837"}, "doi_lower": "10.1145/3485832.3485837"}
{"paper_id": 237592770, "title": "BFClass: A Backdoor-free Text Classification Framework", "author_names": ["Zichao Li", "Dheeraj Mekala", "Chengyu Dong", "Jingbo Shang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Backdoor attack introduces artificial vulnerabilities into the model by poisoning a subset of the training data via injecting triggers and modifying labels. Various trigger design strategies have been explored to attack text classifiers, however, defending such attacks remains an open problem. In this work, we propose BFClass, a novel efficient backdoor-free training framework for text classification. The backbone of BFClass is a pre-trained discriminator that predicts whether each token in the corrupted input was replaced by a masked language model. To identify triggers, we utilize this discriminator to locate the most suspicious token from each training sample and then distill a concise set by considering their association strengths with particular labels. To recognize the poisoned subset, we examine the training samples with these identified triggers as the most suspicious token, and check if removing the trigger will change the poisoned model's prediction. Extensive experiments demonstrate that BFClass can identify all the triggers, remove 95% poisoned training samples with very limited false alarms, and achieve almost the same performance as the models trained on the benign training data.", "year": 2021, "publicationdate": "2021-09-22", "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.40"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.40"}
{"paper_id": 252088971, "title": "PromptAttack: Prompt-based Attack for Language Models via Gradient Search", "author_names": ["Yundi Shi", "Piji Li", "Changchun Yin", "Zhaoyang Han", "Lu Zhou", "Zhe Liu"], "venue": "Natural Language Processing and Chinese Computing", "abstract": "As the pre-trained language models (PLMs) continue to grow, so do the hardware and data requirements for fine-tuning PLMs. Therefore, the researchers have come up with a lighter method called \\textit{Prompt Learning}. However, during the investigations, we observe that the prompt learning methods are vulnerable and can easily be attacked by some illegally constructed prompts, resulting in classification errors, and serious security problems for PLMs. Most of the current research ignores the security issue of prompt-based methods. Therefore, in this paper, we propose a malicious prompt template construction method (\\textbf{PromptAttack}) to probe the security performance of PLMs. Several unfriendly template construction approaches are investigated to guide the model to misclassify the task. Extensive experiments on three datasets and three PLMs prove the effectiveness of our proposed approach PromptAttack. We also conduct experiments to verify that our method is applicable in few-shot scenarios.", "year": 2022, "publicationdate": "2022-09-05", "externalids": {"DOI": "10.48550/arXiv.2209.01882"}, "doi_lower": "10.48550/arxiv.2209.01882"}
{"paper_id": 253581710, "title": "Ignore Previous Prompt: Attack Techniques For Language Models", "author_names": ["Fábio Perez", "Ian Ribeiro"], "venue": "arXiv.org", "abstract": "Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PromptInject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject is available at https://github.com/agencyenterprise/PromptInject.", "year": 2022, "publicationdate": "2022-11-17", "externalids": {"DOI": "10.48550/arXiv.2211.09527"}, "doi_lower": "10.48550/arxiv.2211.09527"}
{"paper_id": 263423935, "title": "Holistic Evaluation of Language Models", "author_names": ["Percy Liang", "Rishi Bommasani", "Tony Lee", "Dimitris Tsipras", "Dilara Soylu", "Michihiro Yasunaga", "Yian Zhang", "Deepak Narayanan", "Yuhuai Wu", "Ananya Kumar", "Benjamin Newman", "Binhang Yuan", "Bobby Yan", "Ce Zhang", "Christian Cosgrove", "Christopher D. Manning", "Christopher Ré", "Diana Acosta-Navas", "Drew A. Hudson", "E. Zelikman", "Esin Durmus", "Faisal Ladhak", "Frieda Rong", "Hongyu Ren", "Huaxiu Yao", "Jue Wang", "Keshav Santhanam", "Laurel J. Orr", "Lucia Zheng", "Mert Yüksekgönül", "Mirac Suzgun", "Nathan Kim", "Neel Guha", "Niladri S. Chatterji", "O. Khattab", "Peter Henderson", "Qian Huang", "Ryan Chi", "Sang Michael Xie", "Shibani Santurkar", "Surya Ganguli", "Tatsunori Hashimoto", "Thomas Icard", "Tianyi Zhang", "Vishrav Chaudhary", "William Wang", "Xuechen Li", "Yifan Mai", "Yuhui Zhang", "Yuta Koreeda"], "venue": "arXiv.org", "abstract": "Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.", "year": 2022, "publicationdate": "2022-11-16", "externalids": {"DOI": "10.48550/arXiv.2211.09110"}, "doi_lower": "10.48550/arxiv.2211.09110"}
{"paper_id": 246276138, "title": "Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection", "author_names": ["Suchin Gururangan", "Dallas Card", "Sarah K. Drier", "E. K. Gade", "Leroy Z. Wang", "Zeyu Wang", "Luke Zettlemoyer", "Noah A. Smith"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Language models increasingly rely on massive web crawls for diverse text data. However, these sources are rife with undesirable content. As such, resources like Wikipedia, books, and news often serve as anchors for automatically selecting web text most suitable for language modeling, a process typically referred to as quality filtering. Using a new dataset of U.S. high school newspaper articles—written by students from across the country—we investigate whose language is preferred by the quality filter used for GPT-3. We find that newspapers from larger schools, located in wealthier, educated, and urban zones (ZIP codes) are more likely to be classified as high quality. We also show that this quality measurement is unaligned with other sensible metrics, such as factuality or literary acclaim. We argue that privileging any corpus as high quality entails a language ideology, and more care is needed to construct training corpora for language models, with better transparency and justification for the inclusion or exclusion of various texts.", "year": 2022, "publicationdate": "2022-01-25", "externalids": {"DOI": "10.18653/v1/2022.emnlp-main.165"}, "doi_lower": "10.18653/v1/2022.emnlp-main.165"}
{"paper_id": 259129807, "title": "Prompt Injection attack against LLM-integrated Applications", "author_names": ["Yi Liu", "Gelei Deng", "Yuekang Li", "Kailong Wang", "Tianwei Zhang", "Yepang Liu", "Haoyu Wang", "Yanhong Zheng", "Yang Liu"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.48550/arXiv.2306.05499"}, "doi_lower": "10.48550/arxiv.2306.05499"}
{"paper_id": 4475201, "title": "Audio Adversarial Examples: Targeted Attacks on Speech-to-Text", "author_names": ["Nicholas Carlini", "D. Wagner"], "venue": "2018 IEEE Security and Privacy Workshops (SPW)", "abstract": "We construct targeted audio adversarial examples on automatic speech recognition. Given any audio waveform, we can produce another that is over 99.9% similar, but transcribes as any phrase we choose (recognizing up to 50 characters per second of audio). We apply our white-box iterative optimization-based attack to Mozilla's implementation DeepSpeech end-to-end, and show it has a 100% success rate. The feasibility of this attack introduce a new domain to study adversarial examples.", "year": 2018, "publicationdate": "2018-01-05", "externalids": {"DOI": "10.1109/SPW.2018.00009"}, "doi_lower": "10.1109/spw.2018.00009"}
{"paper_id": 220714040, "title": "TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP", "author_names": ["John X. Morris", "Eli Lifland", "Jin Yong Yoo", "J. Grigsby", "Di Jin", "Yanjun Qi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack’s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.", "year": 2020, "publicationdate": "2020-04-29", "externalids": {"DOI": "10.18653/v1/2020.emnlp-demos.16"}, "doi_lower": "10.18653/v1/2020.emnlp-demos.16"}
{"paper_id": 236477723, "title": "Better Robustness by More Coverage: Adversarial and Mixup Data Augmentation for Robust Finetuning", "author_names": ["Chenglei Si", "Zhengyan Zhang", "Fanchao Qi", "Zhiyuan Liu", "Yasheng Wang", "Qun Liu", "Maosong Sun"], "venue": "Findings", "abstract": "Pretrained language models (PLMs) perform poorly under adversarial attacks. To improve the adversarial robustness, adversarial data augmentation (ADA) has been widely adopted to cover more search space of adversarial attacks by adding textual adversarial examples during training. However, the number of adversarial examples for text augmentation is still extremely insufficient due to the exponentially large attack search space. In this work, we propose a simple and effective method to cover a much larger proportion of the attack search space, called Adversarial and Mixup Data Augmentation (AMDA). Specifically, AMDA linearly interpolates the representations of pairs of training samples to form new virtual samples, which are more abundant and diverse than the discrete text adversarial examples in conventional ADA. Moreover, to fairly evaluate the robustness of different models, we adopt a challenging evaluation setup, which generates a new set of adversarial examples targeting each model. In text classification experiments of BERT and RoBERTa, AMDA achieves significant robustness gains under two strong adversarial attacks and alleviates the performance degradation of ADA on the clean data. Our code is available at: https://github.com/thunlp/MixADA .", "year": 2020, "publicationdate": "2020-12-31", "externalids": {"DOI": "10.18653/v1/2021.findings-acl.137"}, "doi_lower": "10.18653/v1/2021.findings-acl.137"}
{"paper_id": 247223086, "title": "Detection of Word Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation", "author_names": ["Kiyoon Yoo", "Jangho Kim", "Jiho Jang", "N. Kwak"], "venue": "arXiv.org", "abstract": "Word-level adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in recent years. As a countermeasure, adversarial defense has been explored, but relatively few efforts have been made to detect adversarial examples. However, detecting adversarial examples may be crucial for automated tasks (e.g. review sentiment analysis) that wish to amass information about a certain population and additionally be a step towards a robust defense system. To this end, we release a dataset for four popular attack methods on four datasets and four models to encourage further research in this field. Along with it, we propose a competitive baseline based on density estimation that has the highest AUC on 29 out of 30 dataset-attack-model combinations. Source code is available in https://github.com/anoymous92874838/text-adv-detection.", "year": 2022, "publicationdate": "2022-03-03", "externalids": {"DOI": "10.48550/arXiv.2203.01677"}, "doi_lower": "10.48550/arxiv.2203.01677"}
{"paper_id": 234353603, "title": "A Sweet Rabbit Hole by DARCY: Using Honeypots to Detect Universal Trigger’s Adversarial Attacks", "author_names": ["Thai Le", "Noseong Park", "Dongwon Lee"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The Universal Trigger (UniTrigger) is a recently-proposed powerful adversarial textual attack method. Utilizing a learning-based mechanism, UniTrigger generates a fixed phrase that, when added to any benign inputs, can drop the prediction accuracy of a textual neural network (NN) model to near zero on a target class. To defend against this attack that can cause significant harm, in this paper, we borrow the “honeypot” concept from the cybersecurity community and propose DARCY, a honeypot-based defense framework against UniTrigger. DARCY greedily searches and injects multiple trapdoors into an NN model to “bait and catch” potential attacks. Through comprehensive experiments across four public datasets, we show that DARCY detects UniTrigger’s adversarial attacks with up to 99% TPR and less than 2% FPR in most cases, while maintaining the prediction accuracy (in F1) for clean inputs within a 1% margin. We also demonstrate that DARCY with multiple trapdoors is also robust to a diverse set of attack scenarios with attackers’ varying levels of knowledge and skills. We release the source code of DARCY at: https://github.com/lethaiq/ACL2021-DARCY-HoneypotDefenseNLP.", "year": 2020, "publicationdate": "2020-11-20", "externalids": {"DOI": "10.18653/v1/2021.acl-long.296"}, "doi_lower": "10.18653/v1/2021.acl-long.296"}
{"paper_id": 211002957, "title": "ROBUSTNESS MAY BE AT ODDS WITH ACCURACY", "author_names": ["Shohaib Mahmud", "Zhiming Fan", "Zetian Liu", "Jiechao Gao"], "venue": "", "abstract": null, "year": 2020, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 59222747, "title": "Theoretically Principled Trade-off between Robustness and Accuracy", "author_names": ["Hongyang Zhang", "Yaodong Yu", "Jiantao Jiao", "E. Xing", "L. Ghaoui", "Michael I. Jordan"], "venue": "International Conference on Machine Learning", "abstract": "We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by $11.41\\%$ in terms of mean $\\ell_2$ perturbation distance.", "year": 2019, "publicationdate": "2019-01-24", "externalids": {}, "doi_lower": null}
{"paper_id": 221654919, "title": "How Much Can We Really Trust You? Towards Simple, Interpretable Trust Quantification Metrics for Deep Neural Networks", "author_names": ["A. Wong", "Xiao Yu Wang", "Andrew Hryniowski"], "venue": "arXiv.org", "abstract": "A critical step to building trustworthy deep neural networks is trust quantification, where we ask the question: How much can we trust a deep neural network? In this study, we take a step towards simple, interpretable metrics for trust quantification by introducing a suite of metrics for assessing the overall trustworthiness of deep neural networks based on their behaviour when answering a set of questions. We conduct a thought experiment and explore two key questions about trust in relation to confidence: 1) How much trust do we have in actors who give wrong answers with great confidence? and 2) How much trust do we have in actors who give right answers hesitantly? Based on insights gained, we introduce the concept of question-answer trust to quantify trustworthiness of an individual answer based on confident behaviour under correct and incorrect answer scenarios, and the concept of trust density to characterize the distribution of overall trust for an individual answer scenario. We further introduce the concept of trust spectrum for representing overall trust with respect to the spectrum of possible answer scenarios across correctly and incorrectly answered questions. Finally, we introduce NetTrustScore, a scalar metric summarizing overall trustworthiness. The suite of metrics aligns with past social psychology studies that study the relationship between trust and confidence. Leveraging these metrics, we quantify the trustworthiness of several well-known deep neural network architectures for image recognition to get a deeper understanding of where trust breaks down. The proposed metrics are by no means perfect, but the hope is to push the conversation towards better metrics to help guide practitioners and regulators in producing, deploying, and certifying deep learning solutions that can be trusted to operate in real-world, mission-critical scenarios.", "year": 2020, "publicationdate": "2020-09-12", "externalids": {}, "doi_lower": null}
{"paper_id": 198967636, "title": "A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability", "author_names": ["Xiaowei Huang", "D. Kroening", "Wenjie Ruan", "James Sharp", "Youcheng Sun", "Emese Thamo", "Min Wu", "Xinping Yi"], "venue": "Computer Science Review", "abstract": null, "year": 2018, "publicationdate": "2018-12-18", "externalids": {"DOI": "10.1016/j.cosrev.2020.100270"}, "doi_lower": "10.1016/j.cosrev.2020.100270"}
{"paper_id": 258823083, "title": "A survey of safety and trustworthiness of large language models through the lens of verification and validation", "author_names": ["Xiaowei Huang", "Wenjie Ruan", "Wei Huang", "Gao Jin", "Yizhen Dong", "Changshun Wu", "S. Bensalem", "Ronghui Mu", "Yi Qi", "Xingyu Zhao", "Kaiwen Cai", "Yanghao Zhang", "Sihao Wu", "Peipei Xu", "Dengyu Wu", "André Freitas", "Mustafa A. Mustafa"], "venue": "Artificial Intelligence Review", "abstract": "Large language models (LLMs) have exploded a new heatwave of AI for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V&V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks as independent processes to check the alignment of their implementations against the specifications, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and regulations and ethical use. In total, 370+ references are considered to support the quick understanding of the safety and trustworthiness issues from the perspective of V&V. While intensive research has been conducted to identify the safety and trustworthiness issues, rigorous yet practical methods are called for to ensure the alignment of LLMs with safety and trustworthiness requirements.", "year": 2023, "publicationdate": "2023-05-19", "externalids": {"DOI": "10.1007/s10462-024-10824-0"}, "doi_lower": "10.1007/s10462-024-10824-0"}
{"paper_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "author_names": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "venue": "Journal of machine learning research", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.", "year": 2019, "publicationdate": "2019-10-23", "externalids": {}, "doi_lower": null}
{"paper_id": 253244504, "title": "A Close Look into the Calibration of Pre-trained Language Models", "author_names": ["Yangyi Chen", "Lifan Yuan", "Ganqu Cui", "Zhiyuan Liu", "Heng Ji"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs’ calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don’t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs’ confidence in wrong predictions.", "year": 2022, "publicationdate": "2022-10-31", "externalids": {"DOI": "10.48550/arXiv.2211.00151"}, "doi_lower": "10.48550/arxiv.2211.00151"}
{"paper_id": 218971825, "title": "Language (Technology) is Power: A Critical Survey of “Bias” in NLP", "author_names": ["Su Lin Blodgett", "Solon Barocas", "Hal Daum'e", "Hanna M. Wallach"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process. We further find that these papers’ proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “bias” in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of “bias”---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements—and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.", "year": 2020, "publicationdate": "2020-05-28", "externalids": {"DOI": "10.18653/v1/2020.acl-main.485"}, "doi_lower": "10.18653/v1/2020.acl-main.485"}
{"paper_id": 219530686, "title": "Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases", "author_names": ["W. Guo", "Aylin Caliskan"], "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that occur in contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models. Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD achieves an accuracy of 81.6% and 82.7%, respectively, when detecting the intersectional biases of African American females and Mexican American females, where the random correct identification rates are 14.3% and 13.3%. EIBD reaches an accuracy of 84.7% and 65.3%, respectively, when detecting the emergent intersectional biases unique to African American females and Mexican American females, where the random correct identification rates are 9.2% and 6.1%. Our results indicate that intersectional biases associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.", "year": 2020, "publicationdate": "2020-06-06", "externalids": {"DOI": "10.1145/3461702.3462536"}, "doi_lower": "10.1145/3461702.3462536"}
{"paper_id": 1704893, "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings", "author_names": ["Tolga Bolukbasi", "Kai-Wei Chang", "James Y. Zou", "Venkatesh Saligrama", "A. Kalai"], "venue": "Neural Information Processing Systems", "abstract": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.", "year": 2016, "publicationdate": "2016-07-21", "externalids": {}, "doi_lower": null}
{"paper_id": 23163324, "title": "Semantics derived automatically from language corpora contain human-like biases", "author_names": ["Aylin Caliskan", "J. Bryson", "Arvind Narayanan"], "venue": "Science", "abstract": "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.", "year": 2016, "publicationdate": "2016-08-25", "externalids": {"DOI": "10.1126/science.aal4230"}, "doi_lower": "10.1126/science.aal4230"}
{"paper_id": 246652372, "title": "Survey of Hallucination in Natural Language Generation", "author_names": ["Ziwei Ji", "Nayeon Lee", "Rita Frieske", "Tiezheng Yu", "D. Su", "Yan Xu", "Etsuko Ishii", "Yejin Bang", "Delong Chen", "Wenliang Dai", "Andrea Madotto", "Pascale Fung"], "venue": "ACM Computing Surveys", "abstract": "Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.", "year": 2022, "publicationdate": "2022-02-08", "externalids": {"DOI": "10.1145/3571730"}, "doi_lower": "10.1145/3571730"}
{"paper_id": 258887694, "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation", "author_names": ["Niels Mündler", "Jingxuan He", "Slobodan Jenko", "Martin T. Vechev"], "venue": "International Conference on Learning Representations", "abstract": "Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context. In this work, we present a comprehensive investigation into self-contradiction for various instruction-tuned LMs, covering evaluation, detection, and mitigation. Our primary evaluation task is open-domain text generation, but we also demonstrate the applicability of our approach to shorter question answering. Our analysis reveals the prevalence of self-contradictions, e.g., in 17.7% of all sentences produced by ChatGPT. We then propose a novel prompting-based framework designed to effectively detect and mitigate self-contradictions. Our detector achieves high accuracy, e.g., around 80% F1 score when prompting ChatGPT. The mitigation algorithm iteratively refines the generated text to remove contradictory information while preserving text fluency and informativeness. Importantly, our entire framework is applicable to black-box LMs and does not require retrieval of external knowledge. Rather, our method complements retrieval-based methods, as a large portion of self-contradictions (e.g., 35.2% for ChatGPT) cannot be verified using online text. Our approach is practically effective and has been released as a push-button tool to benefit the public at https://chatprotect.ai/.", "year": 2023, "publicationdate": "2023-05-25", "externalids": {"DOI": "10.48550/arXiv.2305.15852"}, "doi_lower": "10.48550/arxiv.2305.15852"}
{"paper_id": 218487034, "title": "On Faithfulness and Factuality in Abstractive Summarization", "author_names": ["Joshua Maynez", "Shashi Narayan", "Bernd Bohnet", "Ryan T. McDonald"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.", "year": 2020, "publicationdate": "2020-05-02", "externalids": {"DOI": "10.18653/v1/2020.acl-main.173"}, "doi_lower": "10.18653/v1/2020.acl-main.173"}
{"paper_id": 263699899, "title": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation", "author_names": ["Neeraj Varshney", "Wenlin Yao", "Hongming Zhang", "Jianshu Chen", "Dong Yu"], "venue": "arXiv.org", "abstract": "Recently developed large language models have achieved remarkable success in generating fluent and coherent text. However, these models often tend to 'hallucinate' which critically hampers their reliability. In this work, we address this crucial problem and propose an approach that actively detects and mitigates hallucinations during the generation process. Specifically, we first identify the candidates of potential hallucination leveraging the model's logit output values, check their correctness through a validation procedure, mitigate the detected hallucinations, and then continue with the generation process. Through extensive experiments with GPT-3.5 (text-davinci-003) on the 'article generation task', we first demonstrate the individual efficacy of our detection and mitigation techniques. Specifically, the detection technique achieves a recall of ~88% and the mitigation technique successfully mitigates 57.6% of the correctly detected hallucinations. Importantly, our mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. Then, we show that the proposed active detection and mitigation approach successfully reduces the hallucinations of the GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the effectiveness and wide applicability of our approach through additional studies including performance on different types of questions (multi-hop and false premise questions) and with another LLM from a different model family (Vicuna). In summary, our work contributes to improving the reliability and trustworthiness of large language models, a crucial step en route to enabling their widespread adoption in real-world applications.", "year": 2023, "publicationdate": "2023-07-08", "externalids": {"DOI": "10.48550/arXiv.2307.03987"}, "doi_lower": "10.48550/arxiv.2307.03987"}
{"paper_id": 258987659, "title": "Let's Verify Step by Step", "author_names": ["H. Lightman", "Vineet Kosaraju", "Yura Burda", "Harrison Edwards", "Bowen Baker", "Teddy Lee", "Jan Leike", "John Schulman", "I. Sutskever", "K. Cobbe"], "venue": "International Conference on Learning Representations", "abstract": "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.", "year": 2023, "publicationdate": "2023-05-31", "externalids": {"DOI": "10.48550/arXiv.2305.20050"}, "doi_lower": "10.48550/arxiv.2305.20050"}
{"paper_id": 251800110, "title": "Shortcut Learning of Large Language Models in Natural Language Understanding", "author_names": ["Mengnan Du", "Fengxiang He", "Na Zou", "Dacheng Tao", "Xia Hu"], "venue": "Communications of the ACM", "abstract": "Shortcuts often hinder the robustness of large language models.", "year": 2022, "publicationdate": "2022-08-25", "externalids": {"DOI": "10.1145/3596490"}, "doi_lower": "10.1145/3596490"}
{"paper_id": 3385567, "title": "The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation", "author_names": ["Miles Brundage", "S. Avin", "Jack Clark", "H. Toner", "P. Eckersley", "Ben Garfinkel", "A. Dafoe", "P. Scharre", "Thomas Zeitzoff", "Bobby Filar", "H. Anderson", "H. Roff", "Gregory C. Allen", "J. Steinhardt", "Carrick Flynn", "Seán Ó hÉigeartaigh", "S. Beard", "Haydn Belfield", "Sebastian Farquhar", "Clare Lyle", "Rebecca Crootof", "Owain Evans", "Michael Page", "Joanna J. Bryson", "Roman V. Yampolskiy", "Dario Amodei"], "venue": "arXiv.org", "abstract": "The following organisations are named on the report: Future of Humanity Institute, University of Oxford, Centre for the Study of Existential Risk, University of Cambridge, Center for a New American Security, Electronic Frontier Foundation, OpenAI. The Future of Life Institute is acknowledged as a funder.", "year": 2018, "publicationdate": "2018-02-20", "externalids": {"DOI": "10.17863/CAM.22520"}, "doi_lower": "10.17863/cam.22520"}
{"paper_id": 237091588, "title": "On the Opportunities and Risks of Foundation Models", "author_names": ["Rishi Bommasani", "Drew A. Hudson", "E. Adeli", "R. Altman", "Simran Arora", "Sydney von Arx", "Michael S. Bernstein", "Jeannette Bohg", "Antoine Bosselut", "E. Brunskill", "Erik Brynjolfsson", "S. Buch", "Dallas Card", "Rodrigo Castellon", "Niladri S. Chatterji", "Annie S. Chen", "Kathleen A. Creel", "Jared Davis", "Dora Demszky", "Chris Donahue", "M. Doumbouya", "Esin Durmus", "Stefano Ermon", "J. Etchemendy", "Kawin Ethayarajh", "L. Fei-Fei", "Chelsea Finn", "Trevor Gale", "Lauren Gillespie", "Karan Goel", "Noah D. Goodman", "S. Grossman", "Neel Guha", "Tatsunori Hashimoto", "Peter Henderson", "John Hewitt", "Daniel E. Ho", "Jenny Hong", "Kyle Hsu", "Jing Huang", "Thomas F. Icard", "Saahil Jain", "Dan Jurafsky", "Pratyusha Kalluri", "Siddharth Karamcheti", "G. Keeling", "Fereshte Khani", "O. Khattab", "Pang Wei Koh", "M. Krass", "Ranjay Krishna", "Rohith Kuditipudi", "Ananya Kumar", "Faisal Ladhak", "Mina Lee", "Tony Lee", "J. Leskovec", "Isabelle Levent", "Xiang Lisa Li", "Xuechen Li", "Tengyu Ma", "Ali Malik", "Christopher D. Manning", "Suvir Mirchandani", "E. Mitchell", "Zanele Munyikwa", "Suraj Nair", "A. Narayan", "D. Narayanan", "Benjamin Newman", "Allen Nie", "Juan Carlos Niebles", "H. Nilforoshan", "Julian Nyarko", "Giray Ogut", "Laurel J. Orr", "Isabel Papadimitriou", "J. Park", "C. Piech", "Eva Portelance", "Christopher Potts", "Aditi Raghunathan", "Robert Reich", "Hongyu Ren", "Frieda Rong", "Yusuf H. Roohani", "Camilo Ruiz", "Jack Ryan", "Christopher R'e", "Dorsa Sadigh", "Shiori Sagawa", "Keshav Santhanam", "Andy Shih", "K. Srinivasan", "Alex Tamkin", "Rohan Taori", "A. Thomas", "Florian Tramèr", "Rose E. Wang", "William Wang", "Bohan Wu", "Jiajun Wu", "Yuhuai Wu", "Sang Michael Xie", "Michihiro Yasunaga", "Jiaxuan You", "M. Zaharia", "Michael Zhang", "Tianyi Zhang", "Xikun Zhang", "Yuhui Zhang", "Lucia Zheng", "Kaitlyn Zhou", "Percy Liang"], "venue": "arXiv.org", "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.", "year": 2021, "publicationdate": "2021-08-16", "externalids": {}, "doi_lower": null}
{"paper_id": 258865215, "title": "From Text to MITRE Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads", "author_names": ["P. V. S. Charan", "Hrushikesh Chunduri", "P. Anand", "S. Shukla"], "venue": "arXiv.org", "abstract": "This research article critically examines the potential risks and implications arising from the malicious utilization of large language models(LLM), focusing specifically on ChatGPT and Google's Bard. Although these large language models have numerous beneficial applications, the misuse of this technology by cybercriminals for creating offensive payloads and tools is a significant concern. In this study, we systematically generated implementable code for the top-10 MITRE Techniques prevalent in 2022, utilizing ChatGPT, and conduct a comparative analysis of its performance with Google's Bard. Our experimentation reveals that ChatGPT has the potential to enable attackers to accelerate the operation of more targeted and sophisticated attacks. Additionally, the technology provides amateur attackers with more capabilities to perform a wide range of attacks and empowers script kiddies to develop customized tools that contribute to the acceleration of cybercrime. Furthermore, LLMs significantly benefits malware authors, particularly ransomware gangs, in generating sophisticated variants of wiper and ransomware attacks with ease. On a positive note, our study also highlights how offensive security researchers and pentesters can make use of LLMs to simulate realistic attack scenarios, identify potential vulnerabilities, and better protect organizations. Overall, we conclude by emphasizing the need for increased vigilance in mitigating the risks associated with LLMs. This includes implementing robust security measures, increasing awareness and education around the potential risks of this technology, and collaborating with security experts to stay ahead of emerging threats.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.15336"}, "doi_lower": "10.48550/arxiv.2305.15336"}
{"paper_id": 232147529, "title": "Putting Humans in the Natural Language Processing Loop: A Survey", "author_names": ["Zijie J. Wang", "Dongjin Choi", "Shenyu Xu", "Diyi Yang"], "venue": "HCINLP", "abstract": "How can we design Natural Language Processing (NLP) systems that learn from human feedback? There is a growing research body of Human-in-the-loop (HITL) NLP frameworks that continuously integrate human feedback to improve the model itself. HITL NLP research is nascent but multifarious—solving various NLP problems, collecting diverse feedback from different people, and applying different methods to learn from human feedback. We present a survey of HITL NLP work from both Machine Learning (ML) and Human-computer Interaction (HCI) communities that highlights its short yet inspiring history, and thoroughly summarize recent frameworks focusing on their tasks, goals, human interactions, and feedback learning methods. Finally, we discuss future studies for integrating human feedback in the NLP development loop.", "year": 2021, "publicationdate": "2021-03-06", "externalids": {}, "doi_lower": null}
{"paper_id": 152326463, "title": "The Inn of Tranquillity: Studies and Essays", "author_names": ["J. Galsworthy"], "venue": "", "abstract": null, "year": 1970, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 244556896, "title": "Risks and opportunities for child health in the digital world", "author_names": ["N. Ustinova", "Leyla S Namazova-Baranova", "A. Fominykh"], "venue": "", "abstract": null, "year": 2021, "publicationdate": "2021-11-23", "externalids": {"DOI": "10.26226/morressier.618aaeaa4a84e7b4701d8197"}, "doi_lower": "10.26226/morressier.618aaeaa4a84e7b4701d8197"}
{"paper_id": 115283847, "title": "The Three Laws of Robotics", "author_names": ["Audrey Watters"], "venue": "", "abstract": null, "year": 2015, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 281843816, "title": "Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework", "author_names": ["Hao Gu", "Vibhas Nair", "Amrithaa Ashok Kumar", "Jayvart Sharma", "Ryan Lagasse"], "venue": "arXiv.org", "abstract": "Interpreting language models often involves circuit analysis, which aims to identify sparse subnetworks, or circuits, that accomplish specific tasks. Existing circuit discovery algorithms face a fundamental trade-off: attribution patching is fast but unfaithful to the full model, while edge pruning is faithful but computationally expensive. This research proposes a hybrid attribution and pruning (HAP) framework that uses attribution patching to identify a high-potential subgraph, then applies edge pruning to extract a faithful circuit from it. We show that HAP is 46\\% faster than baseline algorithms without sacrificing circuit faithfulness. Furthermore, we present a case study on the Indirect Object Identification task, showing that our method preserves cooperative circuit components (e.g. S-inhibition heads) that attribution patching methods prune at high sparsity. Our results show that HAP could be an effective approach for improving the scalability of mechanistic interpretability research to larger models. Our code is available at https://anonymous.4open.science/r/HAP-circuit-discovery.", "year": 2025, "publicationdate": "2025-09-28", "externalids": {"DOI": "10.48550/arXiv.2510.03282"}, "doi_lower": "10.48550/arxiv.2510.03282"}
{"paper_id": 261064903, "title": "Is There Any Social Principle for LLM-Based Agents?", "author_names": ["Jitao Bai", "Simiao Zhang", "Zhong Chen"], "venue": "arXiv.org", "abstract": "Focus on Large Language Model based agents should involve more than\"human-centered\"alignment or application. We argue that more attention should be paid to the agent itself and discuss the potential of establishing tailored social sciences for agents.", "year": 2023, "publicationdate": "2023-08-22", "externalids": {"DOI": "10.48550/arXiv.2308.11136"}, "doi_lower": "10.48550/arxiv.2308.11136"}
{"paper_id": 116166130, "title": "A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy", "author_names": ["S. Baum"], "venue": "", "abstract": null, "year": 2017, "publicationdate": "2017-11-12", "externalids": {"DOI": "10.2139/SSRN.3070741"}, "doi_lower": "10.2139/ssrn.3070741"}
{"paper_id": 224432528, "title": "Twitter.com search", "author_names": ["K. Sodhi"], "venue": "", "abstract": null, "year": 2013, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 279205475, "title": "The Potential of Large Language Models to Achieve Artificial General Intelligence", "author_names": ["Hao Zhan"], "venue": "Topoi", "abstract": null, "year": 2025, "publicationdate": "2025-06-04", "externalids": {"DOI": "10.1007/s11245-025-10207-2"}, "doi_lower": "10.1007/s11245-025-10207-2"}
{"paper_id": 277057192, "title": "Integrating Large Language Models and Knowledge Graphs for Next-level AGI", "author_names": ["Linhao Luo", "Carl Yang", "Evgeny Kharlamov", "Shirui Pan"], "venue": "The Web Conference", "abstract": "Large language models (LLMs), due to their emergent ability and generalizability, are making new waves in developing Artificial General Intelligence (AGI). However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs) are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to integrating LLMs and KGs together and simultaneously leveraging their advantages to achieve AGI's ultimate goal: to reason, adapt, and synthesize knowledge with human-level nuance and factual accuracy. This tutorial aims to bridge this gap by presenting a comprehensive overview of the unification of LLMs and KGs for next-level AGI. Specifically, we will cover three key frameworks: (1) KG-enhanced LLMs, which focus on augmenting LLMs with KGs for pre-training, fine-tuning, and inference, thereby enriching the LLMs' factual and contextual accuracy; (2) LLM-augmented KGs, which leverage LLMs to assist in tasks such as KG completion, construction, and question answering, ultimately facilitating KG scalability and adaptability; and (3) Synergized LLM-KG Systems and Applications, where LLMs and KGs function symbiotically to enable real-time, bidirectional reasoning, transforming static knowledge structures into dynamic, AGI-driven frameworks. Through this tutorial, participants will gain a structured understanding of the architectures, underlying methodologies, and key advancements in LLM-KG unification, alongside insights into current real-world applications and challenges. We will also explore future research directions, encouraging the development of innovative AGI systems that are not only knowledgeable but also faithful in reasoning. This tutorial will empower researchers and practitioners to unlock the next level of AGI by integrating the strengths of LLMs and KGs into cohesive, intelligent systems.", "year": 2025, "publicationdate": "2025-05-08", "externalids": {"DOI": "10.1145/3701716.3715866"}, "doi_lower": "10.1145/3701716.3715866"}
{"paper_id": 269790943, "title": "How Far Are We From AGI", "author_names": ["Tao Feng", "Chuanyang Jin", "Jingyu Liu", "Kunlun Zhu", "Haoqin Tu", "Zirui Cheng", "Guanyu Lin", "Jiaxuan You"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2405.10313"}, "doi_lower": "10.48550/arxiv.2405.10313"}
{"paper_id": 224432528, "title": "Twitter.com search", "author_names": ["K. Sodhi"], "venue": "", "abstract": null, "year": 2013, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 251881108, "title": "A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27", "author_names": ["Yann LeCun", "Courant"], "venue": "", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 258987641, "title": "Monotonic Location Attention for Length Generalization", "author_names": ["Jishnu Ray Chowdhury", "Cornelia Caragea"], "venue": "International Conference on Machine Learning", "abstract": "We explore different ways to utilize position-based cross-attention in seq2seq networks to enable length generalization in algorithmic tasks. We show that a simple approach of interpolating the original and reversed encoded representations combined with relative attention allows near-perfect length generalization for both forward and reverse lookup tasks or copy tasks that had been generally hard to tackle. We also devise harder diagnostic tasks where the relative distance of the ideal attention position varies with timestep. In such settings, the simple interpolation trick with relative attention is not sufficient. We introduce novel variants of location attention building on top of Dubois et al. (2020) to address the new diagnostic tasks. We also show the benefits of our approaches for length generalization in SCAN (Lake&Baroni, 2018) and CFQ (Keysers et al., 2020). Our code is available on GitHub.", "year": 2023, "publicationdate": "2023-05-31", "externalids": {"DOI": "10.48550/arXiv.2305.20019"}, "doi_lower": "10.48550/arxiv.2305.20019"}
{"paper_id": 8201466, "title": "Everything as a Service (XaaS) on the Cloud: Origins, Current and Future Trends", "author_names": ["Yucong Duan", "Guohua Fu", "N. Zhou", "Xiaobing Sun", "N. Narendra", "Bo Hu"], "venue": "IEEE International Conference on Cloud Computing", "abstract": null, "year": 2015, "publicationdate": "2015-06-27", "externalids": {"DOI": "10.1109/CLOUD.2015.88"}, "doi_lower": "10.1109/cloud.2015.88"}
{"paper_id": 168087618, "title": "A Study of Cloud Computing Infrastructure-as-a- Service (IaaS) in Financial Firms", "author_names": ["H. Howell-Barber", "J. Lawler", "A. Joseph", "Stuti Narula"], "venue": "", "abstract": null, "year": 2013, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 2225017, "title": "Infrastructure as a Service and Cloud Technologies", "author_names": ["Nicolás Serrano", "Gorka Gallardo", "J. Hernantes"], "venue": "IEEE Software", "abstract": null, "year": 2015, "publicationdate": "2015-03-10", "externalids": {"DOI": "10.1109/MS.2015.43"}, "doi_lower": "10.1109/ms.2015.43"}
{"paper_id": 167781982, "title": "The NIST Definition of Cloud Computing", "author_names": ["P. Mell", "T. Grance"], "venue": "", "abstract": null, "year": 2011, "publicationdate": "2011-09-28", "externalids": {"DOI": "10.6028/NIST.SP.800-145"}, "doi_lower": "10.6028/nist.sp.800-145"}
{"paper_id": 43377731, "title": "Software as a Service: An Integration Perspective", "author_names": ["Wei Sun", "Kuo Zhang", "Shyh-Kwei Chen", "Xin Zhang", "Haiqi Liang"], "venue": "International Conference on Service Oriented Computing", "abstract": null, "year": 2007, "publicationdate": "2007-09-17", "externalids": {"DOI": "10.1007/978-3-540-74974-5_52"}, "doi_lower": "10.1007/978-3-540-74974-5_52"}
{"paper_id": 206309855, "title": "Networking lessons in delivering ‘Software as a Service’—Part II", "author_names": ["D. Greschler", "Tim Mangan"], "venue": "International Journal of Network Management", "abstract": null, "year": 2002, "publicationdate": "2002-11-11", "externalids": {"DOI": "10.1002/nem.447"}, "doi_lower": "10.1002/nem.447"}
{"paper_id": 245836882, "title": "Black-Box Tuning for Language-Model-as-a-Service", "author_names": ["Tianxiang Sun", "Yunfan Shao", "Hong Qian", "Xuanjing Huang", "Xipeng Qiu"], "venue": "International Conference on Machine Learning", "abstract": "Extremely large pre-trained language models (PTMs) such as GPT-3 are usually released as a service. It allows users to design task-specific prompts to query the PTMs through some black-box APIs. In such a scenario, which we call Language-Model-as-a-Service (LMaaS), the gradients of PTMs are usually unavailable. Can we optimize the task prompts by only accessing the model inference APIs? This paper proposes the black-box tuning framework to optimize the continuous prompt prepended to the input text via derivative-free optimization. Instead of optimizing in the original high-dimensional prompt space, which is intractable for traditional derivative-free optimization, we perform optimization in a randomly generated subspace due to the low intrinsic dimensionality of large PTMs. The experimental results show that the black-box tuning with RoBERTa on a few labeled samples not only significantly outperforms manual prompt and GPT-3's in-context learning, but also surpasses the gradient-based counterparts, i.e., prompt tuning and full model tuning.", "year": 2022, "publicationdate": "2022-01-10", "externalids": {}, "doi_lower": null}
