{"paper_id": 1100302, "title": "Wizard of Oz Method for Learning Dialog Agents", "author_names": ["Masayuki Okamoto", "Yeonsoo Yang", "T. Ishida"], "venue": "International Workshop on Cooperative Information Agents", "abstract": null, "year": 2001, "publicationdate": "2001-09-06", "externalids": {"DOI": "10.1007/3-540-44799-7_3"}, "doi_lower": "10.1007/3-540-44799-7_3"}
{"paper_id": 253522998, "title": "Large Language Models Struggle to Learn Long-Tail Knowledge", "author_names": ["Nikhil Kandpal", "H. Deng", "Adam Roberts", "Eric Wallace", "Colin Raffel"], "venue": "International Conference on Machine Learning", "abstract": "The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.", "year": 2022, "publicationdate": "2022-11-15", "externalids": {}, "doi_lower": null}
{"paper_id": 261530162, "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models", "author_names": ["Yue Zhang", "Yafu Li", "Leyang Cui", "Deng Cai", "Lemao Liu", "Tingchen Fu", "Xinting Huang", "Enbo Zhao", "Yu Zhang", "Yulong Chen", "Longyue Wang", "A. Luu", "Wei Bi", "Freda Shi", "Shuming Shi"], "venue": "Computational Linguistics", "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.", "year": 2023, "publicationdate": "2023-09-03", "externalids": {"DOI": "10.1162/coli.a.16"}, "doi_lower": "10.1162/coli.a.16"}
{"paper_id": 264817661, "title": "GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval", "author_names": ["Daman Arora", "Anush Kini", "Sayak Ray Chowdhury", "Nagarajan Natarajan", "Gaurav Sinha", "Amit Sharma"], "venue": "arXiv.org", "abstract": "Given a query and a document corpus, the information retrieval (IR) task is to output a ranked list of relevant documents. Combining large language models (LLMs) with embedding-based retrieval models, recent work shows promising results on the zero-shot retrieval problem, i.e., no access to labeled data from the target domain. Two such popular paradigms are generation-augmented retrieval or GAR (generate additional context for the query and then retrieve), and retrieval-augmented generation or RAG (retrieve relevant documents as context and then generate answers). The success of these paradigms hinges on (i) high-recall retrieval models, which are difficult to obtain in the zero-shot setting, and (ii) high-precision (re-)ranking models which typically need a good initialization. In this work, we propose a novel GAR-meets-RAG recurrence formulation that overcomes the challenges of existing paradigms. Our method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in the zero-shot setting. A key design principle is that the rewrite-retrieval stages improve the recall of the system and a final re-ranking stage improves the precision. We conduct extensive experiments on zero-shot passage retrieval benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in the BEIR benchmark, outperforming previous best results in Recall@100 and nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the previous best.", "year": 2023, "publicationdate": "2023-10-31", "externalids": {}, "doi_lower": null}
{"paper_id": 245219136, "title": "Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks", "author_names": ["Akari Asai", "Matt Gardner", "Hannaneh Hajishirzi"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Retrieval-augmented generation models have shown state-of-the-art performance across many knowledge-intensive NLP tasks such as open-domain question answering and fact verification. These models are trained to generate a final output given retrieved passages that can be irrelevant to an input query, leading to learning spurious cues or memorization. This work introduces a method to incorporate evidentiality of passages—whether a passage contains correct evidence to support the output—into training the generator. We introduce a multi-task learning framework to jointly generate the final output and predict the evidentiality of each passage. Furthermore, we introduce a new task-agnostic method for obtaining high-quality silver evidentiality labels, addressing the issues of gold evidentiality labels being unavailable in most domains. Our experiments on five datasets across three knowledge-intensive tasks show that our new evidentiality-guided generator significantly outperforms its direct counterpart on all of them, and advances the state of the art on three of them. Our analysis shows that multi-task learning and silver evidentiality mining play key roles. Our code is available at https://github.com/AkariAsai/evidentiality_qa", "year": 2021, "publicationdate": "2021-12-16", "externalids": {"DOI": "10.18653/v1/2022.naacl-main.162"}, "doi_lower": "10.18653/v1/2022.naacl-main.162"}
{"paper_id": 244954723, "title": "Improving language models by retrieving from trillions of tokens", "author_names": ["Sebastian Borgeaud", "A. Mensch", "Jordan Hoffmann", "Trevor Cai", "Eliza Rutherford", "Katie Millican", "George van den Driessche", "Jean-Baptiste Lespiau", "Bogdan Damoc", "Aidan Clark", "Diego de Las Casas", "Aurelia Guy", "Jacob Menick", "Roman Ring", "T. Hennigan", "Saffron Huang", "Lorenzo Maggiore", "Chris Jones", "Albin Cassirer", "Andy Brock", "Michela Paganini", "G. Irving", "O. Vinyals", "Simon Osindero", "K. Simonyan", "Jack W. Rae", "Erich Elsen", "L. Sifre"], "venue": "International Conference on Machine Learning", "abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.", "year": 2021, "publicationdate": "2021-12-08", "externalids": {}, "doi_lower": null}
{"paper_id": 246426909, "title": "Training language models to follow instructions with human feedback", "author_names": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "John Schulman", "Jacob Hilton", "Fraser Kelton", "Luke E. Miller", "Maddie Simens", "Amanda Askell", "Peter Welinder", "P. Christiano", "Jan Leike", "Ryan J. Lowe"], "venue": "Neural Information Processing Systems", "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.", "year": 2022, "publicationdate": "2022-03-04", "externalids": {}, "doi_lower": null}
{"paper_id": 266163878, "title": "Query Rewriting in Retrieval-Augmented Large Language Models", "author_names": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.emnlp-main.322"}, "doi_lower": "10.18653/v1/2023.emnlp-main.322"}
{"paper_id": 277821981, "title": "Comparative Analysis of Advanced RAG Techniques using Mahabharata", "author_names": ["Ganesh Vaidyanathan K", "Varun M S", "Shauryadeepsinh Gajendrasinh Raolji", "Bhaskarjyoti Das"], "venue": "2025 IEEE 17th International Conference on Computer Research and Development (ICCRD)", "abstract": "Retrieval Augmented Generation (RAG) is a hot topic and one of the most compute-effective context-extending techniques in the LLM industry. Unfortunately, RAG also faces certain challenges, which make it vulnerable to mistakes. To overcome these challenges, recent studies have come up with numerous methods, which are collectively referred to as advanced RAG. In our study, we test out the effectiveness of individual Advanced RAG methods and compare them when used with a variety of LLMs. We perform this comparative study using the Mahabharata, the Indian Epic, as the external knowledge base. Examining how Advanced RAG techniques can be used in practical applications is this paper’s primary contribution. We then test the efficacy of the techniques using answer-independent evaluation metrics. We conclude by tabulating the performance metrics recorded and provide insights on the results obtained.", "year": 2025, "publicationdate": "2025-01-17", "externalids": {"DOI": "10.1109/ICCRD64588.2025.10962837"}, "doi_lower": "10.1109/iccrd64588.2025.10962837"}
{"paper_id": 265042961, "title": "Large Language Model based Long-tail Query Rewriting in Taobao Search", "author_names": ["Wenjun Peng", "Guiyang Li", "Yue Jiang", "Zilong Wang", "Dan Ou", "Xiaoyi Zeng", "Tongxu", "Enhong Chen"], "venue": "The Web Conference", "abstract": "In the realm of e-commerce search, the significance of semantic matching cannot be overstated, as it directly impacts both user experience and company revenue. Along this line, query rewriting, serving as an important technique to bridge the semantic gaps inherent in the semantic matching process, has attached wide attention from the industry and academia. However, existing query rewriting methods often struggle to effectively optimize long-tail queries and alleviate the phenomenon of \"few recall'' caused by semantic gap. In this paper, we present BEQUE, a comprehensive framework that Bridges the sE mantic gap for long-tail QUE ries. In detail, BEQUE comprises three stages: multi-instruction supervised fine tuning (SFT), offline feedback, and objective alignment. We first construct a rewriting dataset based on rejection sampling and auxiliary tasks mixing to fine-tune our large language model (LLM) in a supervised fashion. Subsequently, with the well-trained LLM, we employ beam search to generate multiple candidate rewrites, and feed them into Taobao offline system to obtain the partial order. Leveraging the partial order of rewrites, we introduce a contrastive learning method to highlight the distinctions between rewrites, and align the model with the Taobao online objectives. Offline experiments prove the effectiveness of our method in bridging semantic gap. Online A/B tests reveal that our method can significantly boost gross merchandise volume (GMV), number of transaction (#Trans) and unique visitor (UV) for long-tail queries. BEQUE has been deployed on Taobao, one of most popular online shopping platforms in China, since October 2023.", "year": 2023, "publicationdate": "2023-11-07", "externalids": {"DOI": "10.1145/3589335.3648298"}, "doi_lower": "10.1145/3589335.3648298"}
{"paper_id": 263830368, "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "author_names": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "E. Chi", "Quoc V. Le", "Denny Zhou"], "venue": "International Conference on Learning Representations", "abstract": "We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7% and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.", "year": 2023, "publicationdate": "2023-10-09", "externalids": {"DOI": "10.48550/arXiv.2310.06117"}, "doi_lower": "10.48550/arxiv.2310.06117"}
{"paper_id": 254877046, "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "author_names": ["Luyu Gao", "Xueguang Ma", "Jimmy J. Lin", "Jamie Callan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "While dense retrieval has been shown to be effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance labels are available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings (HyDE). Given a query, HyDE first zero-shot prompts an instruction-following language model (e.g., InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is “fake” and may contain hallucinations. Then, an unsupervised contrastively learned encoder (e.g., Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, from which similar real documents are retrieved based on vector similarity. This second step grounds the generated document to the actual corpus, with the encoder’s dense bottleneck filtering out the hallucinations. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers across various tasks (e.g. web search, QA, fact verification) and in non-English languages (e.g., sw, ko, ja, bn).", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.18653/v1/2023.acl-long.99"}, "doi_lower": "10.18653/v1/2023.acl-long.99"}
{"paper_id": 279830355, "title": "Domain-Specific Graph RAG Pipelines", "author_names": ["Omkar Yadav"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {"DOI": "10.31979/etd.8yak-ujxc"}, "doi_lower": "10.31979/etd.8yak-ujxc"}
{"paper_id": 252408513, "title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators", "author_names": ["W. Yu", "Dan Iter", "Shuohang Wang", "Yichong Xu", "Mingxuan Ju", "Soumya Sanyal", "Chenguang Zhu", "Michael Zeng", "Meng Jiang"], "venue": "International Conference on Learning Representations", "abstract": "Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GenRead), which first prompts a large language model to generate contextutal documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, resulting in the generated documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GenRead achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https://github.com/wyu97/GenRead.", "year": 2022, "publicationdate": "2022-09-21", "externalids": {}, "doi_lower": null}
{"paper_id": 258866037, "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy", "author_names": ["Zhihong Shao", "Yeyun Gong", "Yelong Shen", "Minlie Huang", "Nan Duan", "Weizhu Chen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.15294"}, "doi_lower": "10.48550/arxiv.2305.15294"}
{"paper_id": 261076315, "title": "KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases", "author_names": ["Xintao Wang", "Qian Yang", "Yongting Qiu", "Jiaqing Liang", "Qi He", "Zhouhong Gu", "Yanghua Xiao", "W. Wang"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have demonstrated impressive impact in the field of natural language processing, but they still struggle with several issues regarding, such as completeness, timeliness, faithfulness and adaptability. While recent efforts have focuses on connecting LLMs with external knowledge sources, the integration of knowledge bases (KBs) remains understudied and faces several challenges. In this paper, we introduce KnowledGPT, a comprehensive framework to bridge LLMs with various knowledge bases, facilitating both the retrieval and storage of knowledge. The retrieval process employs the program of thought prompting, which generates search language for KBs in code format with pre-defined functions for KB operations. Besides retrieval, KnowledGPT offers the capability to store knowledge in a personalized KB, catering to individual user demands. With extensive experiments, we show that by integrating LLMs with KBs, KnowledGPT properly answers a broader range of questions requiring world knowledge compared with vanilla LLMs, utilizing both knowledge existing in widely-known KBs and extracted into personalized KBs.", "year": 2023, "publicationdate": "2023-08-17", "externalids": {"DOI": "10.48550/arXiv.2308.11761"}, "doi_lower": "10.48550/arxiv.2308.11761"}
{"paper_id": 281092240, "title": "HF-RAG: Hierarchical Fusion-based RAG with Multiple Sources and Rankers", "author_names": ["Payel Santra", "Madhusudan Ghosh", "Debasis Ganguly", "Partha Basuchowdhuri", "S. Naskar"], "venue": "Proceedings of the 34th ACM International Conference on Information and Knowledge Management", "abstract": "Leveraging both labeled (input-output associations) and unlabeled data (wider contextual grounding) may provide complementary benefits in retrieval augmented generation (RAG). However, effectively combining evidence from these heterogeneous sources is challenging as the respective similarity scores are not inter-comparable. Additionally, aggregating beliefs from the outputs of multiple rankers can improve the effectiveness of RAG. Our proposed method first aggregates the top-documents from a number of IR models using a standard rank fusion technique for each source (labeled and unlabeled). Next, we standardize the retrieval score distributions within each source by applying z-score transformation before merging the top-retrieved documents from the two sources. We evaluate our approach on the fact verification task, demonstrating that it consistently improves over the best-performing individual ranker or source and also shows better out-of-domain generalization.", "year": 2025, "publicationdate": "2025-09-02", "externalids": {"DOI": "10.1145/3746252.3760942"}, "doi_lower": "10.1145/3746252.3760942"}
{"paper_id": 258479968, "title": "Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory", "author_names": ["Xin Cheng", "Di Luo", "Xiuying Chen", "Lemao Liu", "Dongyan Zhao", "Rui Yan"], "venue": "Neural Information Processing Systems", "abstract": "With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot LLM. Our approach achieves state-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in enhancing retrieval-augmented generation models. Furthermore, we conduct thorough analyses of each component in the selfmem framework to identify bottlenecks and provide insights for future research.", "year": 2023, "publicationdate": "2023-05-03", "externalids": {"DOI": "10.48550/arXiv.2305.02437"}, "doi_lower": "10.48550/arxiv.2305.02437"}
{"paper_id": 247475919, "title": "Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data", "author_names": ["Shuo Wang", "Yichong Xu", "Yuwei Fang", "Yang Liu", "S. Sun", "Ruochen Xu", "Chenguang Zhu", "Michael Zeng"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Retrieval-based methods have been shown to be effective in NLP tasks via introducing external knowledge. However, the indexing and retrieving of large-scale corpora bring considerable computational cost. Surprisingly, we found that REtrieving from the traINing datA (REINA) only can lead to significant gains on multiple NLG and NLU tasks. We retrieve the labeled training instances most similar to the input text and then concatenate them with the input to feed into the model to generate the output. Experimental results show that this simple method can achieve significantly better performance on a variety of NLU and NLG tasks, including summarization, machine translation, language modeling, and question answering tasks. For instance, our proposed method achieved state-of-the-art results on XSum, BigPatent, and CommonsenseQA. Our code is released, https://github.com/microsoft/REINA .", "year": 2022, "publicationdate": "2022-03-16", "externalids": {"DOI": "10.48550/arXiv.2203.08773"}, "doi_lower": "10.48550/arxiv.2203.08773"}
{"paper_id": 265149581, "title": "From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL", "author_names": ["Xiaoqian Li", "Ercong Nie", "Sheng Liang"], "venue": "arXiv.org", "abstract": "The remarkable ability of Large Language Models (LLMs) to understand and follow instructions has sometimes been limited by their in-context learning (ICL) performance in low-resource languages. To address this, we introduce a novel approach that leverages cross-lingual retrieval-augmented in-context learning (CREA-ICL). By extracting semantically similar prompts from high-resource languages, we aim to improve the zero-shot performance of multilingual pre-trained language models (MPLMs) across diverse tasks. Though our approach yields steady improvements in classification tasks, it faces challenges in generation tasks. Our evaluation offers insights into the performance dynamics of retrieval-augmented in-context learning across both classification and generation domains.", "year": 2023, "publicationdate": "2023-11-11", "externalids": {"DOI": "10.48550/arXiv.2311.06595"}, "doi_lower": "10.48550/arxiv.2311.06595"}
{"paper_id": 257532394, "title": "UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation", "author_names": ["Daixuan Cheng", "Shaohan Huang", "Junyu Bi", "Yu-Wei Zhan", "Jianfeng Liu", "Yujing Wang", "Hao Sun", "Furu Wei", "Denvy Deng", "Qi Zhang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on a diverse set of tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs. Our model and code are available at https://github.com/microsoft/LMOps.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {"DOI": "10.48550/arXiv.2303.08518"}, "doi_lower": "10.48550/arxiv.2303.08518"}
{"paper_id": 252519173, "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples", "author_names": ["Zhuyun Dai", "Vincent Zhao", "Ji Ma", "Yi Luan", "Jianmo Ni", "Jing Lu", "A. Bakalov", "Kelvin Guu", "Keith B. Hall", "Ming-Wei Chang"], "venue": "International Conference on Learning Representations", "abstract": "Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by LLM's generalization ability, Promptagator makes it possible to create task-specific end-to-end retrievers solely based on a few examples {without} using Natural Questions or MS MARCO to train %question generators or dual encoders. Surprisingly, LLM prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on MS MARCO like ColBERT v2 by more than 1.2 nDCG on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point nDCG improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.", "year": 2022, "publicationdate": "2022-09-23", "externalids": {"DOI": "10.48550/arXiv.2209.11755"}, "doi_lower": "10.48550/arxiv.2209.11755"}
{"paper_id": 252692968, "title": "Recitation-Augmented Language Models", "author_names": ["Zhiqing Sun", "Xuezhi Wang", "Yi Tay", "Yiming Yang", "Denny Zhou"], "venue": "International Conference on Learning Representations", "abstract": "We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs' own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of \\method~on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at\"https://github.com/Edward-Sun/RECITE\".", "year": 2022, "publicationdate": "2022-10-04", "externalids": {"DOI": "10.48550/arXiv.2210.01296"}, "doi_lower": "10.48550/arxiv.2210.01296"}
{"paper_id": 255186555, "title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP", "author_names": ["O. Khattab", "Keshav Santhanam", "Xiang Lisa Li", "David Leo Wright Hall", "Percy Liang", "Christopher Potts", "M. Zaharia"], "venue": "arXiv.org", "abstract": "Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple\"retrieve-then-read\"pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp", "year": 2022, "publicationdate": "2022-12-28", "externalids": {"DOI": "10.48550/arXiv.2212.14024"}, "doi_lower": "10.48550/arxiv.2212.14024"}
{"paper_id": 258615731, "title": "Active Retrieval Augmented Generation", "author_names": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.", "year": 2023, "publicationdate": "2023-05-11", "externalids": {"DOI": "10.48550/arXiv.2305.06983"}, "doi_lower": "10.48550/arxiv.2305.06983"}
{"paper_id": 264288947, "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "author_names": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "venue": "International Conference on Learning Representations", "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.", "year": 2023, "publicationdate": "2023-10-17", "externalids": {}, "doi_lower": null}
{"paper_id": 266998785, "title": "Bridging the Preference Gap between Retrievers and LLMs", "author_names": ["Zixuan Ke", "Weize Kong", "Cheng Li", "Mingyang Zhang", "Qiaozhu Mei", "Michael Bendersky"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-\"friendly\"information and assembling a LLM-\"friendly\"context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks.", "year": 2024, "publicationdate": "2024-01-13", "externalids": {"DOI": "10.48550/arXiv.2401.06954"}, "doi_lower": "10.48550/arxiv.2401.06954"}
{"paper_id": 263605962, "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning", "author_names": ["Xi Victoria Lin", "Xilun Chen", "Mingda Chen", "Weijia Shi", "Maria Lomeli", "Rich James", "Pedro Rodriguez", "Jacob Kahn", "Gergely Szilvasy", "Mike Lewis", "Luke S. Zettlemoyer", "Scott Yih"], "venue": "International Conference on Learning Representations", "abstract": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.", "year": 2023, "publicationdate": "2023-10-02", "externalids": {"DOI": "10.48550/arXiv.2310.01352"}, "doi_lower": "10.48550/arxiv.2310.01352"}
{"paper_id": 266162497, "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "author_names": ["O. Ovadia", "Meni Brief", "Moshik Mishaeli", "Oren Elisha"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.", "year": 2023, "publicationdate": "2023-12-10", "externalids": {"DOI": "10.48550/arXiv.2312.05934"}, "doi_lower": "10.48550/arxiv.2312.05934"}
{"paper_id": 259298789, "title": "Copy is All You Need", "author_names": ["Tian Lan", "Deng Cai", "Yan Wang", "Heyan Huang", "Xian-Ling Mao"], "venue": "International Conference on Learning Representations", "abstract": "The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary. In this paper, we formulate text generation as progressively copying text segments (e.g., words or phrases) from an existing text collection. We compute the contextualized representations of meaningful text segments and index them using efficient vector search toolkits. The task of text generation is then decomposed into a series of copy-and-paste operations: at each time step, we seek suitable text spans from the text collection rather than selecting from a standalone vocabulary. Experiments on the standard language modeling benchmark (WikiText-103) show that our approach achieves better generation quality according to both automatic and human evaluations. Besides, its inference efficiency is comparable to token-level autoregressive models thanks to the reduction of decoding steps. We also show that our approach allows for effective domain adaptation by simply switching to domain-specific text collection without extra training. Finally, we observe that our approach attains additional performance gains by simply scaling up to larger text collections, again without further training.\\footnote{Our source codes are publicly available at \\url{https://github.com/gmftbyGMFTBY/Copyisallyouneed}.}", "year": 2023, "publicationdate": "2023-07-13", "externalids": {"DOI": "10.48550/arXiv.2307.06962"}, "doi_lower": "10.48550/arxiv.2307.06962"}
{"paper_id": 266163052, "title": "Dense X Retrieval: What Retrieval Granularity Should We Use?", "author_names": ["Tong Chen", "Hongwei Wang", "Sihao Chen", "Wenhao Yu", "Kaixin Ma", "Xinran Zhao", "Dong Yu", "Hongming Zhang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Distinct from the typical approach of using passages or sentences, we introduce a novel retrieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format. We conduct an empirical comparison of different retrieval granularity. Our experiments reveal that indexing a corpus by fine-grained units such as propositions significantly outperforms passage-level units in retrieval tasks. Moreover, constructing prompts with fine-grained retrieved units for retrieval-augmented language models improves the performance of downstream QA tasks given a specific computation budget.", "year": 2023, "publicationdate": "2023-12-11", "externalids": {"DOI": "10.48550/arXiv.2312.06648"}, "doi_lower": "10.48550/arxiv.2312.06648"}
{"paper_id": 265034153, "title": "Divide & Conquer for Entailment-aware Multi-hop Evidence Retrieval", "author_names": ["Fan Luo", "M. Surdeanu"], "venue": "arXiv.org", "abstract": "Lexical and semantic matches are commonly used as relevance measurements for information retrieval. Together they estimate the semantic equivalence between the query and the candidates. However, semantic equivalence is not the only relevance signal that needs to be considered when retrieving evidences for multi-hop questions. In this work, we demonstrate that textual entailment relation is another important relevance dimension that should be considered. To retrieve evidences that are either semantically equivalent to or entailed by the question simultaneously, we divide the task of evidence retrieval for multi-hop question answering (QA) into two sub-tasks, i.e., semantic textual similarity and inference similarity retrieval. We propose two ensemble models, EAR and EARnest, which tackle each of the sub-tasks separately and then jointly re-rank sentences with the consideration of the diverse relevance signals. Experimental results on HotpotQA verify that our models not only significantly outperform all the single retrieval models it is based on, but is also more effective than two intuitive ensemble baseline models.", "year": 2023, "publicationdate": "2023-11-05", "externalids": {"DOI": "10.48550/arXiv.2311.02616"}, "doi_lower": "10.48550/arxiv.2311.02616"}
{"paper_id": 264426178, "title": "Diversify Question Generation with Retrieval-Augmented Style Transfer", "author_names": ["Qi Gou", "Zehua Xia", "Bowen Yu", "Haiyang Yu", "Fei Huang", "Yongbin Li", "Nguyen Cam-Tu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Given a textual passage and an answer, humans are able to ask questions with various expressions, but this ability is still challenging for most question generation (QG) systems. Existing solutions mainly focus on the internal knowledge within the given passage or the semantic word space for diverse content planning. These methods, however, have not considered the potential of external knowledge for expression diversity. To bridge this gap, we propose RAST, a framework for Retrieval-Augmented Style Transfer, where the objective is to utilize the style of diverse templates for question generation. For training RAST, we develop a novel Reinforcement Learning (RL) based approach that maximizes a weighted combination of diversity reward and consistency reward. Here, the consistency reward is computed by a Question-Answering (QA) model, whereas the diversity reward measures how much the final output mimics the retrieved template. Experimental results show that our method outperforms previous diversity-driven baselines on diversity while being comparable in terms of consistency scores. Our code is available at https://github.com/gouqi666/RAST.", "year": 2023, "publicationdate": "2023-10-23", "externalids": {"DOI": "10.48550/arXiv.2310.14503"}, "doi_lower": "10.48550/arxiv.2310.14503"}
{"paper_id": 258960340, "title": "Prompt-Guided Retrieval Augmentation for Non-Knowledge-Intensive Tasks", "author_names": ["Zhicheng Guo", "Sijie Cheng", "Yile Wang", "Peng Li", "Yang Liu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Retrieval-augmented methods have received increasing attention to support downstream tasks by leveraging useful information from external resources. Recent studies mainly focus on exploring retrieval to solve knowledge-intensive (KI) tasks. However, the potential of retrieval for most non-knowledge-intensive (NKI) tasks remains under-explored. There are two main challenges to leveraging retrieval-augmented methods for NKI tasks: 1) the demand for diverse relevance score functions and 2) the dilemma between training cost and task performance. To address these challenges, we propose a two-stage framework for NKI tasks, named PGRA. In the first stage, we adopt a task-agnostic retriever to build a shared static index and select candidate evidence efficiently. In the second stage, we design a prompt-guided reranker to rerank the nearest evidence according to task-specific relevance for the reader. Experimental results show that PGRA outperforms other state-of-the-art retrieval-augmented methods. Our analyses further investigate the influence factors to model performance and demonstrate the generality of PGRA. Codes are available at https://github.com/THUNLP-MT/PGRA.", "year": 2023, "publicationdate": "2023-05-28", "externalids": {"DOI": "10.48550/arXiv.2305.17653"}, "doi_lower": "10.48550/arxiv.2305.17653"}
{"paper_id": 265157538, "title": "Learning to Filter Context for Retrieval-Augmented Generation", "author_names": ["Zhiruo Wang", "Jun Araki", "Zhengbao Jiang", "Md. Rizwan Parvez", "Graham Neubig"], "venue": "arXiv.org", "abstract": "On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.", "year": 2023, "publicationdate": "2023-11-14", "externalids": {"DOI": "10.48550/arXiv.2311.08377"}, "doi_lower": "10.48550/arxiv.2311.08377"}
{"paper_id": 267770281, "title": "Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks", "author_names": ["Minju Seo", "Jinheon Baek", "James Thorne", "Sung Ju Hwang"], "venue": "arXiv.org", "abstract": "Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available. Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs). However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data. Specifically, we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and then prompt LLMs to generate new samples with the contextual information within and across the original and retrieved samples. This approach can ensure that the generated data is not only relevant but also more diverse than what could be achieved using the limited seed data alone. We validate our proposed Retrieval-Augmented Data Augmentation (RADA) framework on multiple datasets under low-resource settings of training and test-time data augmentation scenarios, on which it outperforms existing LLM-powered data augmentation baselines.", "year": 2024, "publicationdate": "2024-02-21", "externalids": {"DOI": "10.48550/arXiv.2402.13482"}, "doi_lower": "10.48550/arxiv.2402.13482"}
{"paper_id": 257532405, "title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!", "author_names": ["Yubo Ma", "Yixin Cao", "YongChing Hong", "Aixin Sun"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for information extraction (IE) tasks, however, remains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings. Therefore, we conclude that LLMs are not effective few-shot information extractors in general. Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle challenging samples that SLMs struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters and LLMs serve as rerankers. By prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our preliminary system consistently achieves promising improvements (2.4% F1-gain on average) on various IE tasks, with an acceptable time and cost investment.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {"DOI": "10.18653/v1/2023.findings-emnlp.710"}, "doi_lower": "10.18653/v1/2023.findings-emnlp.710"}
{"paper_id": 253510351, "title": "Retrieval-Augmented Generative Question Answering for Event Argument Extraction", "author_names": ["Xinya Du", "Heng Ji"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Event argument extraction has long been studied as a sequential prediction problem with extractive-based methods, tackling each argument in isolation. Although recent work proposes generation-based methods to capture cross-argument dependency, they require generating and post-processing a complicated target sequence (template). Motivated by these observations and recent pretrained language models’ capabilities of learning from demonstrations. We propose a retrieval-augmented generative QA model (R-GQA) for event argument extraction. It retrieves the most similar QA pair and augments it as prompt to the current example’s context, then decodes the arguments as answers. Our approach outperforms substantially prior methods across various settings (i.e. fully supervised, domain transfer, and fewshot learning). Finally, we propose a clustering-based sampling strategy (JointEnc) and conduct a thorough analysis of how different strategies influence the few-shot learning performances.", "year": 2022, "publicationdate": "2022-11-14", "externalids": {"DOI": "10.48550/arXiv.2211.07067"}, "doi_lower": "10.48550/arxiv.2211.07067"}
{"paper_id": 259924840, "title": "Learning to Retrieve In-Context Examples for Large Language Models", "author_names": ["Liang Wang", "Nan Yang", "Furu Wei"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.", "year": 2023, "publicationdate": "2023-07-14", "externalids": {"DOI": "10.48550/arXiv.2307.07164"}, "doi_lower": "10.48550/arxiv.2307.07164"}
{"paper_id": 258564854, "title": "Recommender Systems with Generative Retrieval", "author_names": ["Shashank Rajput", "Nikhil Mehta", "Anima Singh", "Raghunandan H. Keshavan", "T. Vu", "L. Heldt", "Lichan Hong", "Yi Tay", "Vinh Q. Tran", "Jonah Samost", "Maciej Kula", "Ed H. Chi", "M. Sathiamoorthy"], "venue": "Neural Information Processing Systems", "abstract": "Modern recommender systems perform large-scale retrieval by first embedding queries and item candidates in the same unified space, followed by approximate nearest neighbor search to select top candidates given a query embedding. In this paper, we propose a novel generative retrieval approach, where the retrieval model autoregressively decodes the identifiers of the target candidates. To that end, we create semantically meaningful tuple of codewords to serve as a Semantic ID for each item. Given Semantic IDs for items in a user session, a Transformer-based sequence-to-sequence model is trained to predict the Semantic ID of the next item that the user will interact with. To the best of our knowledge, this is the first Semantic ID-based generative model for recommendation tasks. We show that recommender systems trained with the proposed paradigm significantly outperform the current SOTA models on various datasets. In addition, we show that incorporating Semantic IDs into the sequence-to-sequence model enhances its ability to generalize, as evidenced by the improved retrieval performance observed for items with no prior interaction history.", "year": 2023, "publicationdate": "2023-05-08", "externalids": {"DOI": "10.48550/arXiv.2305.05065"}, "doi_lower": "10.48550/arxiv.2305.05065"}
{"paper_id": 263909224, "title": "Language Models As Semantic Indexers", "author_names": ["Bowen Jin", "Hansi Zeng", "Guoyin Wang", "Xiusi Chen", "Tianxin Wei", "Ruirui Li", "Zhengyang Wang", "Zheng Li", "Yang Li", "Hanqing Lu", "Suhang Wang", "Jiawei Han", "Xianfeng Tang"], "venue": "International Conference on Machine Learning", "abstract": "Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss, and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. It is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMIndexer, a self-supervised framework to learn semantic IDs with a generative language model. We tackle the challenge of sequential discrete ID by introducing a semantic indexer capable of generating neural sequential discrete representations with progressive training and contrastive learning. In response to the semantic supervision deficiency, we propose to train the model with a self-supervised document reconstruction objective. We show the high quality of the learned IDs and demonstrate their effectiveness on three tasks including recommendation, product search, and document retrieval on five datasets from various domains. Code is available at https://github.com/PeterGriffinJin/LMIndexer.", "year": 2023, "publicationdate": "2023-10-11", "externalids": {"DOI": "10.48550/arXiv.2310.07815"}, "doi_lower": "10.48550/arxiv.2310.07815"}
{"paper_id": 266162438, "title": "Context Tuning for Retrieval Augmented Generation", "author_names": ["R. Anantha", "Tharun Bethi", "Danil Vodianik", "Srinivas Chappidi"], "venue": "UNCERTAINLP", "abstract": "Large language models (LLMs) have the remarkable ability to solve new tasks with just a few examples, but they need access to the right tools. Retrieval Augmented Generation (RAG) addresses this problem by retrieving a list of relevant tools for a given task. However, RAG’s tool retrieval step requires all the required information to be explicitly present in the query. This is a limitation, as semantic search, the widely adopted tool retrieval method, can fail when the query is incomplete or lacks context. To address this limitation, we propose Context Tuning for RAG, which employs a smart context retrieval system to fetch relevant information that improves both tool retrieval and plan generation. Our lightweight context retrieval model uses numerical, categorical, and habitual usage signals to retrieve and rank context items. Our empirical results demonstrate that context tuning significantly enhances semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for context retrieval and tool retrieval tasks respectively, and resulting in an 11.6% increase in LLM-based planner accuracy. Additionally, we show that our proposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART outperforms GPT-4 based retrieval. Moreover, we observe context augmentation at plan generation, even after tool retrieval, reduces hallucination.", "year": 2023, "publicationdate": "2023-12-09", "externalids": {"DOI": "10.48550/arXiv.2312.05708"}, "doi_lower": "10.48550/arxiv.2312.05708"}
{"paper_id": 251371732, "title": "Few-shot Learning with Retrieval Augmented Language Models", "author_names": ["Gautier Izacard", "Patrick Lewis", "M. Lomeli", "Lucas Hosseini", "F. Petroni", "Timo Schick", "Jane A. Yu", "Armand Joulin", "Sebastian Riedel", "Edouard Grave"], "venue": "Journal of machine learning research", "abstract": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.", "year": 2022, "publicationdate": "2022-08-05", "externalids": {}, "doi_lower": null}
{"paper_id": 260900354, "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models", "author_names": ["Jie Huang", "Wei Ping", "Peng Xu", "M. Shoeybi", "K. Chang", "Bryan Catanzaro"], "venue": "arXiv.org", "abstract": "In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of existing models and identify their limitations in in-context learning, primarily due to a mismatch between pretraining and inference, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training. Through extensive experiments, we demonstrate that our simple yet effective design significantly improves performance, achieving results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction.", "year": 2023, "publicationdate": "2023-08-15", "externalids": {"DOI": "10.48550/arXiv.2308.07922"}, "doi_lower": "10.48550/arxiv.2308.07922"}
{"paper_id": 258170263, "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study", "author_names": ["Boxin Wang", "Wei Ping", "P. Xu", "Lawrence C. McAfee", "Zihan Liu", "M. Shoeybi", "Yi Dong", "Oleksii Kuchaiev", "Bo Li", "Chaowei Xiao", "Anima Anandkumar", "Bryan Catanzaro"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pre-trained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B parameters while retrieving a text corpus with 330B tokens. Based on that, we have the following novel findings: i) RETRO outperforms GPT on text generation with much less degeneration (i.e., repetition), moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM Evaluation Harness benchmark, RETRO largely outperforms GPT on knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore, we introduce a simple variant of the model, RETRO++, which largely improves open-domain QA results of original RETRO (e.g., EM score +8.6 on Natural Question) and significantly outperforms retrieval-augmented GPT in both fine-tuning and zero-shot evaluation settings. Our findings highlight the promising direction of pretraining autoregressive LMs with retrieval as future foundation models. We release our code and model at: https://github.com/NVIDIA/Megatron-LM/blob/main/tools/retro/README.md", "year": 2023, "publicationdate": "2023-04-13", "externalids": {"DOI": "10.48550/arXiv.2304.06762"}, "doi_lower": "10.48550/arxiv.2304.06762"}
{"paper_id": 263835270, "title": "InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining", "author_names": ["Boxin Wang", "Wei Ping", "Lawrence C. McAfee", "Peng Xu", "Bo Li", "M. Shoeybi", "Bryan Catanzaro"], "venue": "International Conference on Machine Learning", "abstract": "Pretraining auto-regressive large language models~(LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Specifically, the average improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form QA and reading comprehension tasks, 10% over GPT across 4 challenging long-form QA tasks, and 16% over GPT across 3 summarization tasks. Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results. Our results highlight the promising direction to obtain a better GPT decoder through continued pretraining with retrieval before instruction tuning. Our code and checkpoints are publicly available at: https://huggingface.co/nvidia/retro-48b-instruct-4k.", "year": 2023, "publicationdate": "2023-10-11", "externalids": {"DOI": "10.48550/arXiv.2310.07713"}, "doi_lower": "10.48550/arxiv.2310.07713"}
{"paper_id": 252735056, "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering", "author_names": ["Shamane Siriwardhana", "Rivindu Weerasekera", "Elliott Wen", "Tharindu Kaluarachchi", "R. Rana", "Suranga Nanayakkara"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work’s credibility and technical consistency.", "year": 2022, "publicationdate": "2022-10-06", "externalids": {"DOI": "10.1162/tacl_a_00530"}, "doi_lower": "10.1162/tacl_a_00530"}
{"paper_id": 258960666, "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In", "author_names": ["Zichun Yu", "Chenyan Xiong", "S. Yu", "Zhiyuan Liu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM’s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.", "year": 2023, "publicationdate": "2023-05-27", "externalids": {"DOI": "10.48550/arXiv.2305.17331"}, "doi_lower": "10.48550/arxiv.2305.17331"}
{"paper_id": 263608822, "title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context", "author_names": ["Ori Yoran", "Tomer Wolfson", "Ori Ram", "Jonathan Berant"], "venue": "International Conference on Learning Representations", "abstract": "Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding relevant passages. Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time. We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.", "year": 2023, "publicationdate": "2023-10-02", "externalids": {"DOI": "10.48550/arXiv.2310.01558"}, "doi_lower": "10.48550/arxiv.2310.01558"}
{"paper_id": 264288955, "title": "Understanding Retrieval Augmentation for Long-Form Question Answering", "author_names": ["Hung-Ting Chen", "Fangyuan Xu", "Shane Arora", "Eunsol Choi"], "venue": "arXiv.org", "abstract": "We present a study of retrieval-augmented language models (LMs) on long-form question answering. We analyze how retrieval augmentation impacts different LMs, by comparing answers generated from models while using the same evidence documents, and how differing quality of retrieval document set impacts the answers generated from the same LM. We study various attributes of generated answers (e.g., fluency, length, variance) with an emphasis on the attribution of generated long-form answers to in-context evidence documents. We collect human annotations of answer attribution and evaluate methods for automatically judging attribution. Our study provides new insights on how retrieval augmentation impacts long, knowledge-rich text generation of LMs. We further identify attribution patterns for long text generation and analyze the main culprits of attribution errors. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for future work.", "year": 2023, "publicationdate": "2023-10-18", "externalids": {"DOI": "10.48550/arXiv.2310.12150"}, "doi_lower": "10.48550/arxiv.2310.12150"}
{"paper_id": 265212816, "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models", "author_names": ["W. Yu", "Hongming Zhang", "Xiaoman Pan", "Kaixin Ma", "Hongwei Wang", "Dong Yu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Retrieval-augmented language model (RALM) represents a significant advancement in mitigating factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed, and the retrieval of irrelevant data can mislead the response generation. Moreover, standard RALMs frequently neglect their intrinsic knowledge due to the interference from retrieved information. In instances where the retrieved information is irrelevant, RALMs should ideally utilize their intrinsic knowledge or, in the absence of both intrinsic and retrieved knowledge, opt to respond with “unknown” to avoid hallucination. In this paper, we introduces Chain-of-Note (CoN), a novel approach to improve robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for each retrieved document, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. Our experimental results show that GPT-4, when equipped with CoN, outperforms the Chain-of-Thought approach. Besides, we utilized GPT-4 to create 10K CoN data, subsequently trained on smaller models like OPT and LLaMa-2. Our experiments across four open-domain QA benchmarks show that fine-tuned RALMs equipped with CoN significantly outperform standard fine-tuned RALMs.", "year": 2023, "publicationdate": "2023-11-15", "externalids": {"DOI": "10.48550/arXiv.2311.09210"}, "doi_lower": "10.48550/arxiv.2311.09210"}
{"paper_id": 267938725, "title": "Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks", "author_names": ["Shicheng Xu"], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 264405922, "title": "Optimizing Retrieval-augmented Reader Models via Token Elimination", "author_names": ["Moshe Berchansky", "Peter Izsak", "Avi Caciularu", "Ido Dagan", "Moshe Wasserblat"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Fusion-in-Decoder (FiD) is an effective retrieval-augmented language model applied across a variety of open-domain tasks, such as question answering, fact checking, etc. In FiD, supporting passages are first retrieved and then processed using a generative model (Reader), which can cause a significant bottleneck in decoding time, particularly with long outputs. In this work, we analyze the contribution and necessity of all the retrieved passages to the performance of reader models, and propose eliminating some of the retrieved information, at the token level, that might not contribute essential information to the answer generation process. We demonstrate that our method can reduce run-time by up to 62.2%, with only a 2% reduction in performance, and in some cases, even improve the performance results.", "year": 2023, "publicationdate": "2023-10-20", "externalids": {"DOI": "10.48550/arXiv.2310.13682"}, "doi_lower": "10.48550/arxiv.2310.13682"}
{"paper_id": 266191420, "title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research", "author_names": ["Jakub L'ala", "Odhran O'Donoghue", "Aleksandar Shtedritski", "Sam Cox", "Samuel G. Rodriques", "Andrew D. White"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth. Retrieval-Augmented Generation (RAG) models have been proposed to reduce hallucinations and provide provenance for how an answer was generated. Applying such models to the scientific literature may enable large-scale, systematic processing of scientific knowledge. We present PaperQA, a RAG agent for answering questions over the scientific literature. PaperQA is an agent that performs information retrieval across full-text scientific articles, assesses the relevance of sources and passages, and uses RAG to provide answers. Viewing this agent as a question answering model, we find it exceeds performance of existing LLMs and LLM agents on current science QA benchmarks. To push the field closer to how humans perform research on scientific literature, we also introduce LitQA, a more complex benchmark that requires retrieval and synthesis of information from full-text scientific papers across the literature. Finally, we demonstrate PaperQA's matches expert human researchers on LitQA.", "year": 2023, "publicationdate": "2023-12-08", "externalids": {"DOI": "10.48550/arXiv.2312.07559"}, "doi_lower": "10.48550/arxiv.2312.07559"}
{"paper_id": 267301416, "title": "The Power of Noise: Redefining Retrieval for RAG Systems", "author_names": ["Florin Cuconasu", "Giovanni Trappolini", "F. Siciliano", "Simone Filice", "Cesare Campagnano", "Y. Maarek", "Nicola Tonellotto", "Fabrizio Silvestri"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Retrieval-Augmented Generation (RAG) has recently emerged as a method to extend beyond the pre-trained knowledge of Large Language Models by augmenting the original prompt with relevant passages or documents retrieved by an Information Retrieval (IR) system. RAG has become increasingly important for Generative AI solutions, especially in enterprise settings or in any domain in which knowledge is constantly refreshed and cannot be memorized in the LLM. We argue here that the retrieval component of RAG systems, be it dense or sparse, deserves increased attention from the research community, and accordingly, we conduct the first comprehensive and systematic examination of the retrieval strategy of RAG systems. We focus, in particular, on the type of passages IR systems within a RAG solution should retrieve. Our analysis considers multiple factors, such as the relevance of the passages included in the prompt context, their position, and their number. One counter-intuitive finding of this work is that the retriever's highest-scoring documents that are not directly relevant to the query (e.g., do not contain the answer) negatively impact the effectiveness of the LLM. Even more surprising, we discovered that adding random documents in the prompt improves the LLM accuracy by up to 35%. These results highlight the need to investigate the appropriate strategies when integrating retrieval with LLMs, thereby laying the groundwork for future research in this area.", "year": 2024, "publicationdate": "2024-01-26", "externalids": {"DOI": "10.1145/3626772.3657834"}, "doi_lower": "10.1145/3626772.3657834"}
{"paper_id": 265506019, "title": "IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions", "author_names": ["Zhebin Zhang", "Xinyu Zhang", "Yuanhang Ren", "Saijiang Shi", "Meng Han", "Yongkang Wu", "Ruofei Lai", "Zhao Cao"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023).", "year": 2023, "publicationdate": "2023-11-30", "externalids": {"DOI": "10.48550/arXiv.2311.18397"}, "doi_lower": "10.48550/arxiv.2311.18397"}
{"paper_id": 274060398, "title": "“Knowing When You Don’t Know”: A Multilingual Relevance Assessment Dataset for Robust Retrieval-Augmented Generation", "author_names": ["Nandan Thakur", "Luiz Bonifacio", "Crystina Zhang", "O. Ogundepo", "Ehsan Kamalloo", "David Alfonso-Hermelo", "Xiaoguang Li", "Qun Liu", "Boxing Chen", "Mehdi Rezagholizadeh", "Jimmy Lin"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2024.findings-emnlp.730"}, "doi_lower": "10.18653/v1/2024.findings-emnlp.730"}
{"paper_id": 264426402, "title": "Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models", "author_names": ["Gangwoo Kim", "Sungdong Kim", "Byeongguk Jeon", "Joonsuk Park", "Jaewoo Kang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al., (2022). While it provides a comprehensive response without bothering the user for clarification, considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge. To cope with the challenge, we propose a novel framework, Tree of Clarifications (ToC): It recursively constructs a tree of disambiguations for the AQ -- via few-shot prompting leveraging external knowledge -- and uses it to generate a long-form answer. ToC outperforms existing baselines on ASQA in a few-shot setup across the metrics, while surpassing fully-supervised baselines trained on the whole training set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at https://github.com/gankim/tree-of-clarifications.", "year": 2023, "publicationdate": "2023-10-23", "externalids": {"DOI": "10.48550/arXiv.2310.14696"}, "doi_lower": "10.48550/arxiv.2310.14696"}
{"paper_id": 263828724, "title": "Self-Knowledge Guided Retrieval Augmentation for Large Language Models", "author_names": ["Yile Wang", "Peng Li", "Maosong Sun", "Yang Liu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have shown superior performance without task-specific fine-tuning. Despite the success, the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs. As complementary, retrieval-based methods can offer non-parametric world knowledge and improve the performance on tasks such as question answering. However, we find that the retrieved knowledge does not always help and even has a negative impact on original responses occasionally. To better make use of both internal knowledge and external world knowledge, we investigate eliciting the model's ability to recognize what they know and do not know (which is also called self-knowledge) and propose Self-Knowledge guided Retrieval augmentation (SKR), a simple yet effective method which can let LLMs refer to the questions they have previously encountered and adaptively call for external resources when dealing with new questions. We evaluate SKR on multiple datasets and demonstrate that it outperforms chain-of-thought based and fully retrieval-based methods by using either InstructGPT or ChatGPT.", "year": 2023, "publicationdate": "2023-10-08", "externalids": {"DOI": "10.48550/arXiv.2310.05002"}, "doi_lower": "10.48550/arxiv.2310.05002"}
{"paper_id": 263830898, "title": "Retrieval-Generation Synergy Augmented Large Language Models", "author_names": ["Zhangyin Feng", "Xiaocheng Feng", "Dezhi Zhao", "Maojin Yang", "Bing Qin"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "abstract": "Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledge-intensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop QA and multi-hop QA tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines.", "year": 2023, "publicationdate": "2023-10-08", "externalids": {"DOI": "10.1109/ICASSP48485.2024.10448015"}, "doi_lower": "10.1109/icassp48485.2024.10448015"}
{"paper_id": 263620134, "title": "Retrieval meets Long Context Large Language Models", "author_names": ["Peng Xu", "Wei Ping", "Xianchao Wu", "Lawrence C. McAfee", "Chen Zhu", "Zihan Liu", "Sandeep Subramanian", "E. Bakhturina", "M. Shoeybi", "Bryan Catanzaro"], "venue": "International Conference on Learning Representations", "abstract": "Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.", "year": 2023, "publicationdate": "2023-10-04", "externalids": {"DOI": "10.48550/arXiv.2310.03025"}, "doi_lower": "10.48550/arxiv.2310.03025"}
{"paper_id": 254877499, "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "author_names": ["H. Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10509"}, "doi_lower": "10.48550/arxiv.2212.10509"}
{"paper_id": 259991467, "title": "Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation", "author_names": ["Ruiyang Ren", "Yuhao Wang", "Yingqi Qu", "Wayne Xin Zhao", "J. Liu", "Hao Tian", "Huaqin Wu", "Ji-rong Wen", "Haifeng Wang"], "venue": "International Conference on Computational Linguistics", "abstract": "Large language models (LLMs) have shown impressive prowess in solving a wide range of tasks with world knowledge. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly under retrieval augmentation settings. In this study, we present the first analysis on the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain question answering (QA), with a bunch of important findings. Specifically, we focus on three research questions and analyze them by examining QA, priori judgement and posteriori judgement capabilities of LLMs. We show evidence that LLMs possess unwavering confidence in their knowledge and cannot handle the conflict between internal and external knowledge well. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries. We further conduct thorough experiments to examine how different factors affect LLMs and propose a simple method to dynamically utilize supporting documents with our judgement strategy. Additionally, we find that the relevance between the supporting documents and the questions significantly impacts LLMs' QA and judgemental capabilities. The code to reproduce this work is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.", "year": 2023, "publicationdate": "2023-07-20", "externalids": {"DOI": "10.48550/arXiv.2307.11019"}, "doi_lower": "10.48550/arxiv.2307.11019"}
{"paper_id": 267334785, "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "author_names": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "venue": "International Conference on Learning Representations", "abstract": "Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy.", "year": 2024, "publicationdate": "2024-01-31", "externalids": {"DOI": "10.48550/arXiv.2401.18059"}, "doi_lower": "10.48550/arxiv.2401.18059"}
{"paper_id": 256459451, "title": "In-Context Retrieval-Augmented Language Models", "author_names": ["Ori Ram", "Yoav Levine", "Itay Dalmedigos", "Dor Muhlgay", "A. Shashua", "Kevin Leyton-Brown", "Y. Shoham"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1", "year": 2023, "publicationdate": "2023-01-31", "externalids": {"DOI": "10.1162/tacl_a_00605"}, "doi_lower": "10.1162/tacl_a_00605"}
{"paper_id": 259370571, "title": "Retrieve-and-Sample: Document-level Event Argument Extraction via Hybrid Retrieval Augmentation", "author_names": ["Yubing Ren", "Yanan Cao", "Ping Guo", "Fang Fang", "Wei Ma", "Zheng Lin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Recent studies have shown the effectiveness of retrieval augmentation in many generative NLP tasks. These retrieval-augmented methods allow models to explicitly acquire prior external knowledge in a non-parametric manner and regard the retrieved reference instances as cues to augment text generation. These methods use similarity-based retrieval, which is based on a simple hypothesis: the more the retrieved demonstration resembles the original input, the more likely the demonstration label resembles the input label. However, due to the complexity of event labels and sparsity of event arguments, this hypothesis does not always hold in document-level EAE. This raises an interesting question: How do we design the retrieval strategy for document-level EAE? We investigate various retrieval settings from the input and label distribution views in this paper. We further augment document-level EAE with pseudo demonstrations sampled from event semantic regions that can cover adequate alternatives in the same context and event schema. Through extensive experiments on RAMS and WikiEvents, we demonstrate the validity of our newly introduced retrieval-augmented methods and analyze why they work.", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.acl-long.17"}, "doi_lower": "10.18653/v1/2023.acl-long.17"}
{"paper_id": 252683285, "title": "Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks", "author_names": ["Zhenhailong Wang", "Xiaoman Pan", "Dian Yu", "Dong Yu", "Jianshu Chen", "Heng Ji"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Although large language models have achieved impressive zero-shot ability, the huge model size generally incurs high cost. Recently, semi-parametric language models, which augment a smaller language model with an external retriever, have demonstrated promising language modeling capabilities. However, it remains unclear whether such semi-parametric language models can perform competitively well as their fully-parametric counterparts on zero-shot generalization to downstream tasks. In this work, we introduce $\\text{Zemi}$, a zero-shot semi-parametric language model. To our best knowledge, this is the first semi-parametric language model that can demonstrate strong zero-shot performance on a wide range of held-out unseen tasks. We train $\\text{Zemi}$ with a novel semi-parametric multitask prompted training paradigm, which shows significant improvement compared with the parametric multitask training as proposed by T0. Specifically, we augment the multitask training and zero-shot evaluation with retrieval from a large-scale task-agnostic unlabeled corpus. In order to incorporate multiple potentially noisy retrieved augmentations, we further propose a novel $\\text{augmentation fusion}$ module leveraging perceiver resampler and gated cross-attention. Notably, our proposed $\\text{Zemi}_\\text{LARGE}$ outperforms T0-3B by 16% on all seven evaluation tasks while being 3.9x smaller in model size.", "year": 2022, "publicationdate": "2022-10-01", "externalids": {"DOI": "10.48550/arXiv.2210.00185"}, "doi_lower": "10.48550/arxiv.2210.00185"}
{"paper_id": 267312595, "title": "Corrective Retrieval Augmented Generation", "author_names": ["Shi-Qi Yan", "Jia-Chen Gu", "Yun Zhu", "Zhen-Hua Ling"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.", "year": 2024, "publicationdate": "2024-01-29", "externalids": {"DOI": "10.48550/arXiv.2401.15884"}, "doi_lower": "10.48550/arxiv.2401.15884"}
{"paper_id": 264451831, "title": "1-PAGER: One Pass Answer Generation and Evidence Retrieval", "author_names": ["Palak Jain", "Livio Baldini Soares", "T. Kwiatkowski"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "We present 1-Pager the first system that answers a question and retrieves evidence using a single Transformer-based model and decoding process. 1-Pager incrementally partitions the retrieval corpus using constrained decoding to select a document and answer string, and we show that this is competitive with comparable retrieve-and-read alternatives according to both retrieval and answer accuracy metrics. 1-Pager also outperforms the equivalent closed-book question answering model, by grounding predictions in an evidence corpus. While 1-Pager is not yet on-par with more expensive systems that read many more documents before generating an answer, we argue that it provides an important step toward attributed generation by folding retrieval into the sequence-to-sequence paradigm that is currently dominant in NLP. We also show that the search paths used to partition the corpus are easy to read and understand, paving a way forward for interpretable neural retrieval.", "year": 2023, "publicationdate": "2023-10-25", "externalids": {"DOI": "10.48550/arXiv.2310.16568"}, "doi_lower": "10.48550/arxiv.2310.16568"}
{"paper_id": 264590451, "title": "PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter", "author_names": ["Haoyan Yang", "Zhitao Li", "Yong Zhang", "Jianzong Wang", "Ning Cheng", "Ming Li", "Jing Xiao"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The Retrieval Question Answering (ReQA) task employs the retrieval-augmented framework, composed of a retriever and generator. The generator formulates the answer based on the documents retrieved by the retriever. Incorporating Large Language Models (LLMs) as generators is beneficial due to their advanced QA capabilities, but they are typically too large to be fine-tuned with budget constraints while some of them are only accessible via APIs. To tackle this issue and further improve ReQA performance, we propose a trainable Pluggable Reward-Driven Contextual Adapter (PRCA), keeping the generator as a black box. Positioned between the retriever and generator in a Pluggable manner, PRCA refines the retrieved information by operating in a token-autoregressive strategy via maximizing rewards of the reinforcement learning phase. Our experiments validate PRCA's effectiveness in enhancing ReQA performance on three datasets by up to 20% improvement to fit black-box LLMs into existing frameworks, demonstrating its considerable potential in the LLMs era.", "year": 2023, "publicationdate": "2023-10-23", "externalids": {"DOI": "10.48550/arXiv.2310.18347"}, "doi_lower": "10.48550/arxiv.2310.18347"}
{"paper_id": 264406035, "title": "Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking", "author_names": ["Shengyao Zhuang", "Bing Liu", "B. Koopman", "G. Zuccon"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are solely pre-trained on unstructured text data without supervised instruction fine-tuning. Our findings reveal the robust zero-shot ranking ability of such LLMs, highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking system that integrates LLM-based QLMs with a hybrid zero-shot retriever, demonstrating exceptional effectiveness in both zero-shot and few-shot scenarios. We make our codebase publicly available at https://github.com/ielab/llm-qlm.", "year": 2023, "publicationdate": "2023-10-20", "externalids": {"DOI": "10.48550/arXiv.2310.13243"}, "doi_lower": "10.48550/arxiv.2310.13243"}
{"paper_id": 263830734, "title": "RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation", "author_names": ["Fangyuan Xu", "Weijia Shi", "Eunsol Choi"], "venue": "arXiv.org", "abstract": "Retrieving documents and prepending them in-context at inference time improves performance of language model (LMs) on a wide range of tasks. However, these documents, often spanning hundreds of words, make inference substantially more expensive. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieves the burden of LMs to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs' performance on end tasks when the generated summaries are prepended to the LMs' input, while keeping the summary concise.If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, implementing selective augmentation.We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summarization models. We show that our compressors trained for one LM can transfer to other LMs on the language modeling task and provide summaries largely faithful to the retrieved documents.", "year": 2023, "publicationdate": "2023-10-06", "externalids": {"DOI": "10.48550/arXiv.2310.04408"}, "doi_lower": "10.48550/arxiv.2310.04408"}
{"paper_id": 256389797, "title": "REPLUG: Retrieval-Augmented Black-Box Language Models", "author_names": ["Weijia Shi", "Sewon Min", "Michihiro Yasunaga", "Minjoon Seo", "Rich James", "M. Lewis", "Luke Zettlemoyer", "Wen-tau Yih"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.", "year": 2023, "publicationdate": "2023-01-30", "externalids": {"DOI": "10.48550/arXiv.2301.12652"}, "doi_lower": "10.48550/arxiv.2301.12652"}
{"paper_id": 265043634, "title": "Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation", "author_names": ["Eric Melz"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g., (Bubeck et al., 2023)) on modern LLMs have shown that they are capable of performing amazing tasks typically necessitating human-level intelligence. However, unlike humans, frozen LLMs do not improve over time; they neither acquire new knowledge nor learn from their successes or failures. Some approaches to improving the intelligence of LLMs include fine-tuning models based on problem-solving performance (Zelikman et al., 2022), and building bigger and more sophisticated models (Bubeck et al., 2023). However, these methods have the drawback of requiring substantial data and computational resources to retrain existing models. In this paper, we explore the use of Retrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to improve problem-solving performance. We propose ARM-RAG (Auxiliary Rationale Memory for Retrieval Augmented Generation), a system that learns from its successes without incurring high training costs. We demonstrate that the storage and subsequent retrieval of reasoning chains have a positive influence on performance in grade-school math problems.", "year": 2023, "publicationdate": "2023-11-07", "externalids": {"DOI": "10.48550/arXiv.2311.04177"}, "doi_lower": "10.48550/arxiv.2311.04177"}
{"paper_id": 267200117, "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems", "author_names": ["Hongru Wang", "Wenyu Huang", "Yang Deng", "Rui Wang", "Zezhong Wang", "Yufei Wang", "Fei Mi", "Jeff Z. Pan", "Kam-Fai Wong"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems.", "year": 2024, "publicationdate": "2024-01-24", "externalids": {"DOI": "10.48550/arXiv.2401.13256"}, "doi_lower": "10.48550/arxiv.2401.13256"}
{"paper_id": 258556855, "title": "Augmented Large Language Models with Parametric Knowledge Guiding", "author_names": ["Ziyang Luo", "Can Xu", "Pu Zhao", "Xiubo Geng", "Chongyang Tao", "Jing Ma", "Qingwei Lin", "Daxin Jiang"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom data. Moreover, providing private data to the LLMs' owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters. Our PKG is based on open-source\"white-box\"language models, allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of\"black-box\"LLMs on a range of domain knowledge-intensive tasks that require factual (+7.9%), tabular (+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge.", "year": 2023, "publicationdate": "2023-05-08", "externalids": {"DOI": "10.48550/arXiv.2305.04757"}, "doi_lower": "10.48550/arxiv.2305.04757"}
{"paper_id": 258987900, "title": "Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data", "author_names": ["Xinze Li", "Zhenghao Liu", "Chenyan Xiong", "Shi Yu", "Yu Gu", "Zhiyuan Liu", "Ge Yu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch.", "year": 2023, "publicationdate": "2023-05-31", "externalids": {"DOI": "10.48550/arXiv.2305.19912"}, "doi_lower": "10.48550/arxiv.2305.19912"}
{"paper_id": 258967643, "title": "Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation", "author_names": ["Minki Kang", "Jin Myung Kwak", "Jinheon Baek", "Sung Ju Hwang"], "venue": "arXiv.org", "abstract": "Language models have achieved impressive performances on dialogue generation tasks. However, when generating responses for a conversation that requires factual knowledge, they are far from perfect, due to an absence of mechanisms to retrieve, encode, and reflect the knowledge in the generated responses. Some knowledge-grounded dialogue generation methods tackle this problem by leveraging facts from Knowledge Graphs (KGs); however, they do not guarantee that the model utilizes a relevant piece of knowledge from the KG. To overcome this limitation, we propose SUbgraph Retrieval-augmented GEneration (SURGE), a framework for generating context-relevant and knowledge-grounded dialogues with the KG. Specifically, our SURGE framework first retrieves the relevant subgraph from the KG, and then enforces consistency across facts by perturbing their word embeddings conditioned by the retrieved subgraph. Then, we utilize contrastive learning to ensure that the generated texts have high similarity to the retrieved subgraphs. We validate our SURGE framework on OpendialKG and KOMODIS datasets, showing that it generates high-quality dialogues that faithfully reflect the knowledge from KG.", "year": 2023, "publicationdate": "2023-05-30", "externalids": {"DOI": "10.48550/arXiv.2305.18846"}, "doi_lower": "10.48550/arxiv.2305.18846"}
{"paper_id": 264127831, "title": "Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System", "author_names": ["Weizhou Shen", "Yingqi Gao", "Canbin Huang", "Fanqi Wan", "Xiaojun Quan", "Wei Bi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Developing an efficient retriever to retrieve knowledge from a large-scale knowledge base (KB) is critical for task-oriented dialogue systems to effectively handle localized and specialized tasks. However, widely used generative models such as T5 and ChatGPT often struggle to differentiate subtle differences among the retrieved KB records when generating responses, resulting in suboptimal quality of generated responses. In this paper, we propose the application of maximal marginal likelihood to train a perceptive retriever by utilizing signals from response generation for supervision. In addition, our approach goes beyond considering solely retrieved entities and incorporates various meta knowledge to guide the generator, thus improving the utilization of knowledge. We evaluate our approach on three task-oriented dialogue datasets using T5 and ChatGPT as the backbone models. The results demonstrate that when combined with meta knowledge, the response generator can effectively leverage high-quality knowledge records from the retriever and enhance the quality of generated responses. The codes and models of this paper are available at https://github.com/shenwzh3/MK-TOD.", "year": 2023, "publicationdate": "2023-10-13", "externalids": {"DOI": "10.48550/arXiv.2310.08877"}, "doi_lower": "10.48550/arxiv.2310.08877"}
{"paper_id": 264426308, "title": "Dual-Feedback Knowledge Retrieval for Task-Oriented Dialogue Systems", "author_names": ["Tianyuan Shi", "Liangzhi Li", "Zijian Lin", "Tao Yang", "Xiaojun Quan", "Qifan Wang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Efficient knowledge retrieval plays a pivotal role in ensuring the success of end-to-end task-oriented dialogue systems by facilitating the selection of relevant information necessary to fulfill user requests. However, current approaches generally integrate knowledge retrieval and response generation, which poses scalability challenges when dealing with extensive knowledge bases. Taking inspiration from open-domain question answering, we propose a retriever-generator architecture that harnesses a retriever to retrieve pertinent knowledge and a generator to generate system responses.~Due to the lack of retriever training labels, we propose relying on feedback from the generator as pseudo-labels to train the retriever. To achieve this, we introduce a dual-feedback mechanism that generates both positive and negative feedback based on the output of the generator. Our method demonstrates superior performance in task-oriented dialogue tasks, as evidenced by experimental results on three benchmark datasets.", "year": 2023, "publicationdate": "2023-10-23", "externalids": {"DOI": "10.48550/arXiv.2310.14528"}, "doi_lower": "10.48550/arxiv.2310.14528"}
{"paper_id": 264425965, "title": "FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction", "author_names": ["P. Ranade", "Anupam Joshi"], "venue": "International Conference on Advances in Social Networks Analysis and Mining", "abstract": "Narrative construction is the process of representing disparate event information into a logical plot structure that models an end to end story. Intelligence analysis is an example of a domain that can benefit tremendously from narrative construction techniques, particularly in aiding analysts during the largely manual and costly process of synthesizing event information into comprehensive intelligence reports. Manual intelligence report generation is often prone to challenges such as integrating dynamic event information, writing fine-grained queries, and closing information gaps. This motivates the development of a system that retrieves and represents critical aspects of events in a form that aids in automatic generation of intelligence reports. We introduce a Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model. We apply our approach to the problem of neural intelligence report generation and introduce FABULA, framework to augment intelligence analysis workflows using RAG. An analyst can use FABULA to query an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be used to augment prompting of a Large Language Model (LLM) during intelligence report generation. Our evaluation studies show that the plot points included in the generated intelligence reports have high semantic relevance, high coherency, and low data redundancy.", "year": 2023, "publicationdate": "2023-10-20", "externalids": {"DOI": "10.1145/3625007.3627505"}, "doi_lower": "10.1145/3625007.3627505"}
{"paper_id": 281510752, "title": "Dynamic Parser on Graph: A Knowledge Graph Retrieval Approach with Dynamic Parsing Enhanced by Large Language Models", "author_names": ["Yuhang Wang", "Haopei Xu", "Lin Ni"], "venue": "2025 6th International Conference on Artificial Intelligence and Electromechanical Automation (AIEA)", "abstract": "In the context of Knowledge Graph (KG) based retrieval tasks, the inadequate capability for multi hop knowledge reasoning remains a critical challenge. This article proposes an architecture for recursively finding paths on graphs based on the Large Language Model (LLM), Dynamic Parser on Graph (DPoG). DPoG consists of multiple layers of DER (Dynamic Task Parser, Evaluator, Relationship Extractor). The Dynamic Task Parser in each layer uses LLM's semantic understanding ability to plan paths and locate nodes in the path. Evaluator is used to evaluate whether the path generated by Dynamic Task Parser meets the user's needs. If it is not satisfied, Relationship Extractor will extract the neighbor relationships of nodes from the knowledge graph as input to the next layer of Dynamic Task Parser. DER uses a dynamic recursive mechanism for path planning, evaluation, and relationship extension. The output of the upper level Relationship Extractor, Relationship Extraction, drives the next level DER to dynamically adjust the inference path. This mechanism can effectively utilize contextual information. The core innovation of DPoG is to decouple the path inference process through DER; By enhancing semantic understanding ability through LLM and combining the idea of dynamic programming, reliable path planning is provided for the next layer, suitable for multi hop inference scenarios. The experimental results indicate that DPoG has higher accuracy in knowledge graph question answering.", "year": 2025, "publicationdate": "2025-08-01", "externalids": {"DOI": "10.1109/AIEA66061.2025.11160556"}, "doi_lower": "10.1109/aiea66061.2025.11160556"}
{"paper_id": 264306280, "title": "Knowledge-Augmented Language Model Verification", "author_names": ["Jinheon Baek", "Soyeong Jeong", "Minki Kang", "Jong C. Park", "Sung Ju Hwang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons: 1) the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text. To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning. Then, when the verifier recognizes an error, we can rectify it by either retrieving new knowledge or generating new text. Further, we use an ensemble of the outputs from different instructions with a single verifier to enhance the reliability of the verification processes. We validate the effectiveness of the proposed verification steps on multiple question answering benchmarks, whose results show that the proposed verifier effectively identifies retrieval and generation errors, allowing LMs to provide more factually correct outputs. Our code is available at https://github.com/JinheonBaek/KALMV.", "year": 2023, "publicationdate": "2023-10-19", "externalids": {"DOI": "10.48550/arXiv.2310.12836"}, "doi_lower": "10.48550/arxiv.2310.12836"}
{"paper_id": 263605944, "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning", "author_names": ["Linhao Luo", "Yuan-Fang Li", "Gholamreza Haffari", "Shirui Pan"], "venue": "International Conference on Learning Representations", "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.", "year": 2023, "publicationdate": "2023-10-02", "externalids": {"DOI": "10.48550/arXiv.2310.01061"}, "doi_lower": "10.48550/arxiv.2310.01061"}
{"paper_id": 267626823, "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering", "author_names": ["Xiaoxin He", "Yijun Tian", "Yifei Sun", "N. Chawla", "T. Laurent", "Yann LeCun", "Xavier Bresson", "Bryan Hooi"], "venue": "Neural Information Processing Systems", "abstract": "Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/G-Retriever}}", "year": 2024, "publicationdate": "2024-02-12", "externalids": {"DOI": "10.48550/arXiv.2402.07630"}, "doi_lower": "10.48550/arxiv.2402.07630"}
{"paper_id": 259937503, "title": "TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT", "author_names": ["Liangyu Zha", "Junlin Zhou", "Liyao Li", "Rui Wang", "Qingyi Huang", "Saisai Yang", "Jing Yuan", "Changbao Su", "Xiang Li", "Aofeng Su", "Zhang Tao", "Chengcheng Zhou", "Kaizhe Shou", "Miao Wang", "Wufang Zhu", "Guoshan Lu", "Chaonan Ye", "Yali Ye", "Wen-song Ye", "Yiming Zhang", "Xing-yan Deng", "J. Xu", "Haobo Wang", "Gang Chen", "J. Zhao"], "venue": "arXiv.org", "abstract": "Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework's adaptability to specific use cases.", "year": 2023, "publicationdate": "2023-07-17", "externalids": {"DOI": "10.48550/arXiv.2307.08674"}, "doi_lower": "10.48550/arxiv.2307.08674"}
{"paper_id": 245131215, "title": "ISEEQ: Information Seeking Question Generation using Dynamic Meta-Information Retrieval and Knowledge Graphs", "author_names": ["Manas Gaur", "Kalpa Gunaratna", "Vijay Srinivasan", "Hongxia Jin"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Conversational Information Seeking (CIS) is a relatively new research area within conversational AI that attempts to seek information from end-users in order to understand and satisfy the users' needs. If realized, such a CIS system has far-reaching benefits in the real world; for example, CIS systems can assist clinicians in pre-screening or triaging patients in healthcare. A key open sub-problem in CIS that remains unaddressed in the literature is generating Information Seeking Questions (ISQs) based on a short initial query from the end-user. To address this open problem, we propose Information SEEking Question generator (ISEEQ), a novel approach for generating ISQs from just a short user query, given a large text corpus relevant to the user query. Firstly, ISEEQ uses a knowledge graph to enrich the user query. Secondly, ISEEQ uses the knowledge-enriched query to retrieve relevant context passages to ask coherent ISQs adhering to a conceptual flow. Thirdly, ISEEQ introduces a new deep generative-adversarial reinforcement learning-based approach for generating ISQs. We show that ISEEQ can generate high-quality ISQs to promote the development of CIS agents. ISEEQ significantly outperforms comparable baselines on five ISQ evaluation metrics across four datasets having user queries from diverse domains. Further, we argue that ISEEQ is transferable across domains for generating ISQs, as it shows the acceptable performance when trained and tested on different pairs of domains. A qualitative human evaluation confirms that ISEEQ generated ISQs are comparable in quality to human-generated questions, and it outperformed the best comparable baseline.", "year": 2021, "publicationdate": "2021-12-13", "externalids": {"DOI": "10.1609/aaai.v36i10.21312"}, "doi_lower": "10.1609/aaai.v36i10.21312"}
{"paper_id": 256459776, "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "author_names": ["Freda Shi", "Xinyun Chen", "Kanishka Misra", "Nathan Scales", "David Dohan", "Ed H. Chi", "Nathanael Scharli", "Denny Zhou"], "venue": "International Conference on Machine Learning", "abstract": "Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.", "year": 2023, "publicationdate": "2023-01-31", "externalids": {"DOI": "10.48550/arXiv.2302.00093"}, "doi_lower": "10.48550/arxiv.2302.00093"}
{"paper_id": 45159527, "title": "Age and ideal chunk size.", "author_names": ["Philip A. Allen", "L. C. Crozier"], "venue": "Journal of Gerontology", "abstract": null, "year": 1992, "publicationdate": null, "externalids": {"DOI": "10.1093/GERONJ/47.1.P47"}, "doi_lower": "10.1093/geronj/47.1.p47"}
{"paper_id": 121466870, "title": "A recursively enumerable degree which will not split over all lesser ones", "author_names": ["A. Lachlan"], "venue": "", "abstract": null, "year": 1976, "publicationdate": "1976-05-01", "externalids": {"DOI": "10.1016/0003-4843(76)90016-4"}, "doi_lower": "10.1016/0003-4843(76)90016-4"}
{"paper_id": 272271042, "title": "15 Advanced RAG Techniques from Pre-Retrieval to Generation", "author_names": ["Zakey Faieq", "Michelle Avery", "VP Group", "Boston Charlottesville", "Columbus Durham", "Lisbon Porto", "Alegre São", "Paulo Vancouver"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 261076072, "title": "Knowledge Graph Prompting for Multi-Document Question Answering", "author_names": ["Yu Wang", "Nedim Lipka", "Ryan A. Rossi", "Alexa F. Siu", "Ruiyi Zhang", "Tyler Derr"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "The `pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or document structural relations. For graph traversal, we design an LLM-based graph traversal agent that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the graph traversal agent acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design and retrieval augmented generation for LLMs. Our code: https://github.com/YuWVandy/KG-LLM-MDQA.", "year": 2023, "publicationdate": "2023-08-22", "externalids": {"DOI": "10.48550/arXiv.2308.11730"}, "doi_lower": "10.48550/arxiv.2308.11730"}
{"paper_id": 248986239, "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "author_names": ["Denny Zhou", "Nathanael Scharli", "Le Hou", "Jason Wei", "Nathan Scales", "Xuezhi Wang", "D. Schuurmans", "O. Bousquet", "Quoc Le", "Ed H. Chi"], "venue": "International Conference on Learning Representations", "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.", "year": 2022, "publicationdate": "2022-05-21", "externalids": {"DOI": "10.48550/arXiv.2205.10625"}, "doi_lower": "10.48550/arxiv.2205.10625"}
{"paper_id": 262062565, "title": "Chain-of-Verification Reduces Hallucination in Large Language Models", "author_names": ["S. Dhuliawala", "M. Komeili", "Jing Xu", "R. Raileanu", "Xian Li", "Asli Celikyilmaz", "Jason Weston"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.", "year": 2023, "publicationdate": "2023-09-20", "externalids": {"DOI": "10.48550/arXiv.2309.11495"}, "doi_lower": "10.48550/arxiv.2309.11495"}
{"paper_id": 262217025, "title": "AnglE-optimized Text Embeddings", "author_names": ["Xianming Li", "Jing Li"], "venue": "arXiv.org", "abstract": "High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works with LLM-annotated data. Extensive experiments were conducted on various tasks including short-text STS, long-text STS, and domain-specific STS tasks. The results show that AnglE outperforms the state-of-the-art (SOTA) STS models that ignore the cosine saturation zone. These findings demonstrate the ability of AnglE to generate high-quality text embeddings and the usefulness of angle optimization in STS.", "year": 2023, "publicationdate": "2023-09-22", "externalids": {"DOI": "10.48550/arXiv.2309.12871"}, "doi_lower": "10.48550/arxiv.2309.12871"}
{"paper_id": 213740369, "title": "Voyage’s Snap", "author_names": ["Sarut Ngamsarnugsak"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 263835099, "title": "Retrieve Anything To Augment Large Language Models", "author_names": ["Peitian Zhang", "Shitao Xiao", "Zheng Liu", "Zhicheng Dou", "Jian-Yun Nie"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) face significant challenges stemming from their inherent limitations in knowledge, memory, alignment, and action. These challenges cannot be addressed by LLMs alone, but should rely on assistance from the external world, such as knowledge base, memory store, demonstration examples, and tools. Retrieval augmentation stands as a vital mechanism for bridging the gap between LLMs and the external assistance. However, conventional methods encounter two pressing issues. On the one hand, the general-purpose retrievers are not properly optimized for the retrieval augmentation of LLMs. On the other hand, the task-specific retrievers lack the required versatility, hindering their performance across the diverse retrieval augmentation scenarios. In this work, we present a novel approach, the LLM-Embedder, which comprehensively supports the diverse retrieval augmentation needs of LLMs with one unified embedding model. Training such a unified model is non-trivial, as various retrieval tasks aim to capture distinct semantic relationships, often subject to mutual interference. To address this challenge, we systematically optimize our training methodology. This includes reward formulation based on LLMs' feedback, the stabilization of knowledge distillation, multi-task fine-tuning with explicit instructions, and homogeneous in-batch negative sampling. These optimization strategies contribute to the outstanding empirical performance of the LLM-Embedder. Notably, it yields remarkable enhancements in retrieval augmentation for LLMs, surpassing both general-purpose and task-specific retrievers in various evaluation scenarios. Our checkpoint and source code are publicly available at https://github.com/FlagOpen/FlagEmbedding.", "year": 2023, "publicationdate": "2023-10-11", "externalids": {"DOI": "10.48550/arXiv.2310.07554"}, "doi_lower": "10.48550/arxiv.2310.07554"}
{"paper_id": 259360665, "title": "Lost in the Middle: How Language Models Use Long Contexts", "author_names": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "F. Petroni", "Percy Liang"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.", "year": 2023, "publicationdate": "2023-07-06", "externalids": {"DOI": "10.1162/tacl_a_00638"}, "doi_lower": "10.1162/tacl_a_00638"}
{"paper_id": 257766541, "title": "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System", "author_names": ["Yunfan Gao", "Tao Sheng", "Youlin Xiang", "Yun Xiong", "Haofen Wang", "Jiawei Zhang"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of information into LLMs can also handle the cold-start scenarios with new items. In our experiments, Chat-Rec effectively improve the results of top-k recommendations and performs better in zero-shot rating prediction task. Chat-Rec offers a novel approach to improving recommender systems and presents new practical scenarios for the implementation of AIGC (AI generated content) in recommender system studies.", "year": 2023, "publicationdate": "2023-03-25", "externalids": {}, "doi_lower": null}
{"paper_id": 252186384, "title": "Lingua: Addressing Scenarios for Live Interpretation and Automatic Dubbing", "author_names": ["Nathan Anderson", "Caleb Wilson", "Stephen D. Richardson"], "venue": "Conference of the Association for Machine Translation in the Americas", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 263830692, "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression", "author_names": ["Huiqiang Jiang", "Qianhui Wu", "Xufang Luo", "Dongsheng Li", "Chin-Yew Lin", "Yuqing Yang", "Lili Qiu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In long context scenarios, large language models (LLMs) face three main challenges: higher computational cost, performance reduction, and position bias. Research indicates that LLM performance hinges on the density and position of key information in the input prompt. Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs' perception of the key information to simultaneously address the three challenges. Our extensive evaluation across various long context scenarios demonstrates that LongLLMLingua not only enhances performance but also significantly reduces costs and latency. For instance, in the NaturalQuestions benchmark, LongLLMLingua boosts performance by up to 21.4% with around 4x fewer tokens in GPT-3.5-Turbo, leading to substantial cost savings. It achieves a 94.0% cost reduction in the LooGLE benchmark. Moreover, when compressing prompts of about 10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency by 1.4x-2.6x. Our code is available at https://aka.ms/LongLLMLingua.", "year": 2023, "publicationdate": "2023-10-10", "externalids": {"DOI": "10.48550/arXiv.2310.06839"}, "doi_lower": "10.48550/arxiv.2310.06839"}
{"paper_id": 215737187, "title": "Dense Passage Retrieval for Open-Domain Question Answering", "author_names": ["Vladimir Karpukhin", "Barlas Oğuz", "Sewon Min", "Patrick Lewis", "Ledell Yu Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.", "year": 2020, "publicationdate": "2020-04-10", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.550"}, "doi_lower": "10.18653/v1/2020.emnlp-main.550"}
{"paper_id": 257532405, "title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!", "author_names": ["Yubo Ma", "Yixin Cao", "YongChing Hong", "Aixin Sun"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for information extraction (IE) tasks, however, remains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings. Therefore, we conclude that LLMs are not effective few-shot information extractors in general. Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle challenging samples that SLMs struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters and LLMs serve as rerankers. By prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our preliminary system consistently achieves promising improvements (2.4% F1-gain on average) on various IE tasks, with an acceptable time and cost investment.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {"DOI": "10.18653/v1/2023.findings-emnlp.710"}, "doi_lower": "10.18653/v1/2023.findings-emnlp.710"}
{"paper_id": 259274889, "title": "Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge Graph Enhanced Mixture-of-Experts Large Language Model", "author_names": ["Jiaxi Cui", "Zongjia Li", "Yang Yan", "Bohua Chen", "Li Yuan"], "venue": "", "abstract": "AI legal assistants based on Large Language Models (LLMs) can provide accessible legal consulting services, but the hallucination problem poses potential legal risks. This paper presents Chatlaw, an innovative legal assistant utilizing a Mixture-of-Experts (MoE) model and a multi-agent system to enhance the reliability and accuracy of AI-driven legal services. By integrating knowledge graphs with artificial screening, we construct a high-quality legal dataset to train the MoE model. This model utilizes different experts to address various legal issues, optimizing the accuracy of legal responses. Additionally, Standardized Operating Procedures (SOP), modeled after real law firm workflows, significantly reduce errors and hallucinations in legal services. Our MoE model outperforms GPT-4 in the Lawbench and Unified Qualification Exam for Legal Professionals by 7.73% in accuracy and 11 points, respectively, and also surpasses other models in multiple dimensions during real-case consultations, demonstrating our robust capability for legal consultation.", "year": 2023, "publicationdate": "2023-06-28", "externalids": {}, "doi_lower": null}
{"paper_id": 263608822, "title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context", "author_names": ["Ori Yoran", "Tomer Wolfson", "Ori Ram", "Jonathan Berant"], "venue": "International Conference on Learning Representations", "abstract": "Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding relevant passages. Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time. We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.", "year": 2023, "publicationdate": "2023-10-02", "externalids": {"DOI": "10.48550/arXiv.2310.01558"}, "doi_lower": "10.48550/arxiv.2310.01558"}
{"paper_id": 258833025, "title": "Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases", "author_names": ["Xingxuan Li", "Ruochen Zhao", "Yew Ken Chia", "Bosheng Ding", "Lidong Bing", "Shafiq R. Joty", "Soujanya Poria"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.13269"}, "doi_lower": "10.48550/arxiv.2305.13269"}
{"paper_id": 259075577, "title": "Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions", "author_names": ["Hui Yang", "Sifu Yue", "Yunzhong He"], "venue": "arXiv.org", "abstract": "Auto-GPT is an autonomous agent that leverages recent advancements in adapting Large Language Models (LLMs) for decision-making tasks. While there has been a growing interest in Auto-GPT stypled agents, questions remain regarding the effectiveness and flexibility of Auto-GPT in solving real-world decision-making tasks. Its limited capability for real-world engagement and the absence of benchmarks contribute to these uncertainties. In this paper, we present a comprehensive benchmark study of Auto-GPT styled agents in decision-making tasks that simulate real-world scenarios. Our aim is to gain deeper insights into this problem and understand the adaptability of GPT-based agents. We compare the performance of popular LLMs such as GPT-4, GPT-3.5, Claude, and Vicuna in Auto-GPT styled decision-making tasks. Furthermore, we introduce the Additional Opinions algorithm, an easy and effective method that incorporates supervised/imitation-based learners into the Auto-GPT scheme. This approach enables lightweight supervised learning without requiring fine-tuning of the foundational LLMs. We demonstrate through careful baseline comparisons and ablation studies that the Additional Opinions algorithm significantly enhances performance in online decision-making benchmarks, including WebShop and ALFWorld.", "year": 2023, "publicationdate": "2023-06-04", "externalids": {"DOI": "10.48550/arXiv.2306.02224"}, "doi_lower": "10.48550/arxiv.2306.02224"}
{"paper_id": 256697342, "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "author_names": ["Timo Schick", "Jane Dwivedi-Yu", "Roberto Dessì", "R. Raileanu", "M. Lomeli", "Luke Zettlemoyer", "Nicola Cancedda", "Thomas Scialom"], "venue": "Neural Information Processing Systems", "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.", "year": 2023, "publicationdate": "2023-02-09", "externalids": {"DOI": "10.48550/arXiv.2302.04761"}, "doi_lower": "10.48550/arxiv.2302.04761"}
{"paper_id": 258291494, "title": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT", "author_names": ["Jiawei Zhang"], "venue": "arXiv.org", "abstract": "In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}. To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools. Specifically, we will investigate to teach Graph-ToolFormer to handle various graph data reasoning tasks in this paper, including both (1) very basic graph data loading and graph property reasoning tasks, ranging from simple graph order and size to the graph diameter and periphery, and (2) more advanced reasoning tasks on real-world graph data, such as bibliographic networks, protein molecules, sequential recommender systems, social networks and knowledge graphs.", "year": 2023, "publicationdate": "2023-04-10", "externalids": {"DOI": "10.48550/arXiv.2304.11116"}, "doi_lower": "10.48550/arxiv.2304.11116"}
{"paper_id": 245329531, "title": "WebGPT: Browser-assisted question-answering with human feedback", "author_names": ["Reiichiro Nakano", "Jacob Hilton", "S. Balaji", "Jeff Wu", "Ouyang Long", "Christina Kim", "Christopher Hesse", "Shantanu Jain", "Vineet Kosaraju", "W. Saunders", "Xu Jiang", "K. Cobbe", "Tyna Eloundou", "Gretchen Krueger", "Kevin Button", "Matthew Knight", "Benjamin Chess", "John Schulman"], "venue": "arXiv.org", "abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.", "year": 2021, "publicationdate": "2021-12-17", "externalids": {}, "doi_lower": null}
{"paper_id": 86611921, "title": "Natural Questions: A Benchmark for Question Answering Research", "author_names": ["T. Kwiatkowski", "J. Palomaki", "Olivia Redfield", "Michael Collins", "Ankur P. Parikh", "Chris Alberti", "D. Epstein", "I. Polosukhin", "Jacob Devlin", "Kenton Lee", "Kristina Toutanova", "Llion Jones", "Matthew Kelcey", "Ming-Wei Chang", "Andrew M. Dai", "Jakob Uszkoreit", "Quoc V. Le", "Slav Petrov"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.", "year": 2019, "publicationdate": "2019-08-01", "externalids": {"DOI": "10.1162/tacl_a_00276"}, "doi_lower": "10.1162/tacl_a_00276"}
{"paper_id": 271601499, "title": "Exploring the Integration Strategies of Retriever and Large Language Models", "author_names": ["Ye Liu", "Semih Yavuz", "Rui Meng", "Meghana Moorthy", "Shafiq Joty", "Caiming Xiong", "Yingbo Zhou"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2308.12574"}, "doi_lower": "10.48550/arxiv.2308.12574"}
{"paper_id": 26501419, "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "author_names": ["Mandar Joshi", "Eunsol Choi", "Daniel S. Weld", "Luke Zettlemoyer"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study.", "year": 2017, "publicationdate": "2017-05-01", "externalids": {"DOI": "10.18653/v1/P17-1147"}, "doi_lower": "10.18653/v1/p17-1147"}
{"paper_id": 11816014, "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "author_names": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL", "year": 2016, "publicationdate": "2016-06-16", "externalids": {"DOI": "10.18653/v1/D16-1264"}, "doi_lower": "10.18653/v1/d16-1264"}
{"paper_id": 6401679, "title": "Semantic Parsing on Freebase from Question-Answer Pairs", "author_names": ["Jonathan Berant", "A. Chou", "Roy Frostig", "Percy Liang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.", "year": 2013, "publicationdate": "2013-10-01", "externalids": {"DOI": "10.18653/v1/d13-1160"}, "doi_lower": "10.18653/v1/d13-1160"}
{"paper_id": 260443047, "title": "When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories", "author_names": ["Alex Troy Mallen", "Akari Asai", "Victor Zhong", "Rajarshi Das", "Hannaneh Hajishirzi", "Daniel Khashabi"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2212.10511"}, "doi_lower": "10.48550/arxiv.2212.10511"}
{"paper_id": 260460088, "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "author_names": ["Payal Bajaj", "Daniel Fernando Campos", "Nick Craswell", "Li Deng", "Jianfeng Gao", "Xiaodong Liu", "Rangan Majumder", "Andrew McNamara", "Bhaskar Mitra", "Tri Minh Nguyen", "Mir Rosenberg", "Xia Song", "A. Stoica", "Saurabh Tiwary", "Tong Wang"], "venue": "", "abstract": null, "year": 2016, "publicationdate": "2016-11-28", "externalids": {}, "doi_lower": null}
{"paper_id": 52822214, "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering", "author_names": ["Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William W. Cohen", "R. Salakhutdinov", "Christopher D. Manning"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems’ ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.", "year": 2018, "publicationdate": "2018-09-25", "externalids": {"DOI": "10.18653/v1/D18-1259"}, "doi_lower": "10.18653/v1/d18-1259"}
{"paper_id": 226236740, "title": "Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps", "author_names": ["Xanh Ho", "A. Nguyen", "Saku Sugawara", "Akiko Aizawa"], "venue": "International Conference on Computational Linguistics", "abstract": "A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question. However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer. Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question. In this study, we present a new multi-hop QA dataset, called 2WikiMultiHopQA, which uses structured and unstructured data. In our dataset, we introduce the evidence information containing a reasoning path for multi-hop questions. The evidence information has two benefits: (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a model. We carefully design a pipeline and a set of templates when generating a question-answer pair that guarantees the multi-hop steps and the quality of the questions. We also exploit the structured format in Wikidata and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required.", "year": 2020, "publicationdate": "2020-11-02", "externalids": {"DOI": "10.18653/V1/2020.COLING-MAIN.580"}, "doi_lower": "10.18653/v1/2020.coling-main.580"}
{"paper_id": 236771976, "title": "♫ MuSiQue: Multihop Questions via Single-hop Question Composition", "author_names": ["H. Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Multihop reasoning remains an elusive goal as existing multihop benchmarks are known to be largely solvable via shortcuts. Can we create a question answering (QA) dataset that, by construction, requires proper multihop reasoning? To this end, we introduce a bottom–up approach that systematically selects composable pairs of single-hop questions that are connected, that is, where one reasoning step critically relies on information from another. This bottom–up methodology lets us explore a vast space of questions and add stringent filters as well as other mechanisms targeting connected reasoning. It provides fine-grained control over the construction process and the properties of the resulting k-hop questions. We use this methodology to create MuSiQue-Ans, a new multihop QA dataset with 25K 2–4 hop questions. Relative to existing datasets, MuSiQue-Ans is more difficult overall (3× increase in human–machine gap), and harder to cheat via disconnected reasoning (e.g., a single-hop model has a 30-point drop in F1). We further add unanswerable contrast questions to produce a more stringent dataset, MuSiQue-Full. We hope our datasets will help the NLP community develop models that perform genuine multihop reasoning.1", "year": 2021, "publicationdate": "2021-08-02", "externalids": {"DOI": "10.1162/tacl_a_00475"}, "doi_lower": "10.1162/tacl_a_00475"}
{"paper_id": 196170479, "title": "ELI5: Long Form Question Answering", "author_names": ["Angela Fan", "Yacine Jernite", "Ethan Perez", "David Grangier", "J. Weston", "Michael Auli"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We introduce the first large-scale corpus for long form question answering, a task requiring elaborate and in-depth answers to open-ended questions. The dataset comprises 270K threads from the Reddit forum “Explain Like I’m Five” (ELI5) where an online community provides answers to questions which are comprehensible by five year olds. Compared to existing datasets, ELI5 comprises diverse questions requiring multi-sentence answers. We provide a large set of web documents to help answer the question. Automatic and human evaluations show that an abstractive model trained with a multi-task objective outperforms conventional Seq2Seq, language modeling, as well as a strong extractive baseline.However, our best model is still far from human performance since raters prefer gold responses in over 86% of cases, leaving ample opportunity for future improvement.", "year": 2019, "publicationdate": "2019-07-01", "externalids": {"DOI": "10.18653/v1/P19-1346"}, "doi_lower": "10.18653/v1/p19-1346"}
{"paper_id": 2593903, "title": "The NarrativeQA Reading Comprehension Challenge", "author_names": ["Tomás Kociský", "Jonathan Schwarz", "Phil Blunsom", "Chris Dyer", "Karl Moritz Hermann", "Gábor Melis", "Edward Grefenstette"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Reading comprehension (RC)—in contrast to information retrieval—requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.", "year": 2017, "publicationdate": "2017-12-19", "externalids": {"DOI": "10.1162/tacl_a_00023"}, "doi_lower": "10.1162/tacl_a_00023"}
{"paper_id": 267682093, "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts", "author_names": ["Kuang-Huei Lee", "Xinyun Chen", "Hiroki Furuta", "John F. Canny", "Ian Fischer"], "venue": "International Conference on Machine Learning", "abstract": "Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3.5-20x.", "year": 2024, "publicationdate": "2024-02-15", "externalids": {"DOI": "10.48550/arXiv.2402.09727"}, "doi_lower": "10.48550/arxiv.2402.09727"}
{"paper_id": 248157463, "title": "ASQA: Factoid Questions Meet Long-Form Answers", "author_names": ["Ivan Stelmakh", "Yi Luan", "Bhuwan Dhingra", "Ming-Wei Chang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent progress on open domain factoid question answering (QA) does not easily transfer to the task of long-form QA, where the goal is to answer questions that require in-depth explanations. The hurdles include a lack of high-quality data and the absence of a well-defined notion of an answer’s quality. In this work, we address these problems by releasing a novel dataset and a task that we call ASQA (Answer Summaries for Questions which are Ambiguous); and proposing a reliable metric for measuring performance on ASQA. Our task focuses on ambiguous factoid questions which have different correct answers depending on the interpretation. Answers to ambiguous questions should combine factual information from multiple sources into a coherent long-form summary that resolves the ambiguity. In contrast to existing long-form QA tasks (such as ELI5), ASQA admits a clear notion of correctness: a user faced with a good summary should be able to answer different interpretations of the original ambiguous question. Our analysis demonstrates an agreement between this metric and human judgments, and reveals a considerable gap between human performance and strong baselines.", "year": 2022, "publicationdate": "2022-04-12", "externalids": {"DOI": "10.48550/arXiv.2204.06092"}, "doi_lower": "10.48550/arxiv.2204.06092"}
{"paper_id": 233219904, "title": "QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization", "author_names": ["Ming Zhong", "Da Yin", "Tao Yu", "A. Zaidi", "Mutethia Mutuma", "Rahul Jha", "A. Awadallah", "Asli Celikyilmaz", "Yang Liu", "Xipeng Qiu", "Dragomir R. Radev"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at https://github.com/Yale-LILY/QMSum.", "year": 2021, "publicationdate": "2021-03-23", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.472"}, "doi_lower": "10.18653/v1/2021.naacl-main.472"}
{"paper_id": 234093776, "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers", "author_names": ["Pradeep Dasigi", "Kyle Lo", "Iz Beltagy", "Arman Cohan", "Noah A. Smith", "Matt Gardner"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.", "year": 2021, "publicationdate": "2021-05-07", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.365"}, "doi_lower": "10.18653/v1/2021.naacl-main.365"}
{"paper_id": 229079829, "title": "COVID-QA: A Question Answering Dataset for COVID-19", "author_names": ["Timo Möller", "Anthony Reina", "Raghavan Jayakumar", "M. Pietsch"], "venue": "", "abstract": null, "year": 2020, "publicationdate": "2020-06-18", "externalids": {}, "doi_lower": null}
{"paper_id": 261030527, "title": "CMB: A Comprehensive Medical Benchmark in Chinese", "author_names": ["Xidong Wang", "Guiming Hardy Chen", "Dingjie Song", "Zhiyi Zhang", "Zhihong Chen", "Qingying Xiao", "Feng Jiang", "Jianquan Li", "Xiang Wan", "Benyou Wang", "Haizhou Li"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in contextual incongruities to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. We hope this benchmark provide first-hand experience in existing LLMs for medicine and also facilitate the widespread adoption and enhancement of medical LLMs within China. Our data and code are publicly available at https://github.com/FreedomIntelligence/CMB.", "year": 2023, "publicationdate": "2023-08-17", "externalids": {"DOI": "10.48550/arXiv.2308.08833"}, "doi_lower": "10.48550/arxiv.2308.08833"}
{"paper_id": 258309287, "title": "Measuring Massive Multitask Chinese Understanding", "author_names": ["Hui Zeng"], "venue": "arXiv.org", "abstract": "The development of large-scale Chinese language models is flourishing, yet there is a lack of corresponding capability assessments. Therefore, we propose a test to measure the multitask accuracy of large Chinese language models. This test encompasses four major domains, including medicine, law, psychology, and education, with 15 subtasks in medicine and 8 subtasks in education. We found that the best-performing models in the zero-shot setting outperformed the worst-performing models by nearly 18.6 percentage points on average. Across the four major domains, the highest average zero-shot accuracy of all models is 0.512. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot accuracy of 0.693 in clinical medicine, which was the highest accuracy among all models across all subtasks. All models performed poorly in the legal domain, with the highest zero-shot accuracy reaching only 0.239. By comprehensively evaluating the breadth and depth of knowledge across multiple disciplines, this test can more accurately identify the shortcomings of the models.", "year": 2023, "publicationdate": "2023-04-25", "externalids": {"DOI": "10.48550/arXiv.2304.12986"}, "doi_lower": "10.48550/arxiv.2304.12986"}
{"paper_id": 245218982, "title": "QuALITY: Question Answering with Long Input Texts, Yes!", "author_names": ["Richard Yuanzhe Pang", "Alicia Parrish", "Nitish Joshi", "Nikita Nangia", "Jason Phang", "Angelica Chen", "Vishakh Padmakumar", "Johnny Ma", "Jana Thompson", "He He", "Sam Bowman"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators working under tight time constraints, indicating that skimming and simple search are not enough to consistently perform well. Our baseline models perform poorly on this task (55.4%) and significantly lag behind human performance (93.5%).", "year": 2021, "publicationdate": "2021-12-16", "externalids": {"DOI": "10.18653/v1/2022.naacl-main.391"}, "doi_lower": "10.18653/v1/2022.naacl-main.391"}
{"paper_id": 3922816, "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge", "author_names": ["Peter Clark", "Isaac Cowhey", "Oren Etzioni", "Tushar Khot", "Ashish Sabharwal", "Carissa Schoenick", "Oyvind Tafjord"], "venue": "arXiv.org", "abstract": "We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering. Together, these constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the SQuAD and SNLI tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the ARC Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose ARC as a challenge to the community.", "year": 2018, "publicationdate": "2018-03-14", "externalids": {}, "doi_lower": null}
{"paper_id": 53296520, "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge", "author_names": ["Alon Talmor", "Jonathan Herzig", "Nicholas Lourie", "Jonathan Berant"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/N19-1421"}, "doi_lower": "10.18653/v1/n19-1421"}
{"paper_id": 53218829, "title": "Wizard of Wikipedia: Knowledge-Powered Conversational agents", "author_names": ["Emily Dinan", "Stephen Roller", "Kurt Shuster", "Angela Fan", "Michael Auli", "J. Weston"], "venue": "International Conference on Learning Representations", "abstract": "In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically \"generate and hope\" generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction.", "year": 2018, "publicationdate": "2018-09-27", "externalids": {}, "doi_lower": null}
{"paper_id": 264127797, "title": "Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue", "author_names": ["Hongru Wang", "Minda Hu", "Yang Deng", "Rui Wang", "Fei Mi", "Weichao Wang", "Yasheng Wang", "Wai-Chung Kwan", "Irwin King", "Kam-Fai Wong"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Open-domain dialogue system usually requires different sources of knowledge to generate more informative and evidential responses. However, existing knowledge-grounded dialogue systems either focus on a single knowledge source or overlook the dependency between multiple sources of knowledge, which may result in generating inconsistent or even paradoxical responses. To incorporate multiple knowledge sources and dependencies between them, we propose SAFARI, a novel framework that leverages the exceptional capabilities of large language models (LLMs) in planning, understanding, and incorporating under both supervised and unsupervised settings. Specifically, SAFARI decouples the knowledge grounding into multiple sources and response generation, which allows easy extension to various knowledge sources including the possibility of not using any sources. To study the problem, we construct a personalized knowledge-grounded dialogue dataset \\textit{\\textbf{K}nowledge \\textbf{B}ehind \\textbf{P}ersona}~(\\textbf{KBP}), which is the first to consider the dependency between persona and implicit knowledge. Experimental results on the KBP dataset demonstrate that the SAFARI framework can effectively produce persona-consistent and knowledge-enhanced responses.", "year": 2023, "publicationdate": "2023-10-13", "externalids": {"DOI": "10.48550/arXiv.2310.08840"}, "doi_lower": "10.48550/arxiv.2310.08840"}
{"paper_id": 264127797, "title": "Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue", "author_names": ["Hongru Wang", "Minda Hu", "Yang Deng", "Rui Wang", "Fei Mi", "Weichao Wang", "Yasheng Wang", "Wai-Chung Kwan", "Irwin King", "Kam-Fai Wong"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Open-domain dialogue system usually requires different sources of knowledge to generate more informative and evidential responses. However, existing knowledge-grounded dialogue systems either focus on a single knowledge source or overlook the dependency between multiple sources of knowledge, which may result in generating inconsistent or even paradoxical responses. To incorporate multiple knowledge sources and dependencies between them, we propose SAFARI, a novel framework that leverages the exceptional capabilities of large language models (LLMs) in planning, understanding, and incorporating under both supervised and unsupervised settings. Specifically, SAFARI decouples the knowledge grounding into multiple sources and response generation, which allows easy extension to various knowledge sources including the possibility of not using any sources. To study the problem, we construct a personalized knowledge-grounded dialogue dataset \\textit{\\textbf{K}nowledge \\textbf{B}ehind \\textbf{P}ersona}~(\\textbf{KBP}), which is the first to consider the dependency between persona and implicit knowledge. Experimental results on the KBP dataset demonstrate that the SAFARI framework can effectively produce persona-consistent and knowledge-enhanced responses.", "year": 2023, "publicationdate": "2023-10-13", "externalids": {"DOI": "10.48550/arXiv.2310.08840"}, "doi_lower": "10.48550/arxiv.2310.08840"}
{"paper_id": 247411350, "title": "Long Time No See! Open-Domain Conversation with Long-Term Persona Memory", "author_names": ["Xinchao Xu", "Zhibin Gou", "Wenquan Wu", "Zheng-Yu Niu", "Hua Wu", "Haifeng Wang", "Shihang Wang"], "venue": "Findings", "abstract": "Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations. The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information. To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a dialogue generation framework with Long-Term Memory (LTM) mechanism (called PLATO-LTM). This LTM mechanism enables our system to accurately extract and continuously update long-term persona memory without requiring multiple-session dialogue datasets for model training. To our knowledge, this is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot. Results on DuLeMon indicate that PLATO-LTM can significantly outperform baselines in terms of long-term dialogue consistency, leading to better dialogue engagingness.", "year": 2022, "publicationdate": "2022-03-11", "externalids": {"DOI": "10.48550/arXiv.2203.05797"}, "doi_lower": "10.48550/arxiv.2203.05797"}
{"paper_id": 1180118, "title": "Conditional Generation and Snapshot Learning in Neural Dialogue Systems", "author_names": ["Tsung-Hsien Wen", "Milica Gasic", "N. Mrksic", "L. Rojas-Barahona", "Pei-hao Su", "Stefan Ultes", "David Vandyke", "S. Young"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.", "year": 2016, "publicationdate": "2016-06-01", "externalids": {"DOI": "10.18653/v1/D16-1233"}, "doi_lower": "10.18653/v1/d16-1233"}
{"paper_id": 1964279, "title": "Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering", "author_names": ["Ruining He", "Julian McAuley"], "venue": "The Web Conference", "abstract": "Building a successful recommender system depends on understanding both the dimensions of people's preferences as well as their dynamics. In certain domains, such as fashion, modeling such preferences can be incredibly difficult, due to the need to simultaneously model the visual appearance of products as well as their evolution over time. The subtle semantics and non-linear dynamics of fashion evolution raise unique challenges especially considering the sparsity and large scale of the underlying datasets. In this paper we build novel models for the One-Class Collaborative Filtering setting, where our goal is to estimate users' fashion-aware personalized ranking functions based on their past feedback. To uncover the complex and evolving visual factors that people consider when evaluating products, our method combines high-level visual features extracted from a deep convolutional neural network, users' past feedback, as well as evolving trends within the community. Experimentally we evaluate our method on two large real-world datasets from Amazon.com, where we show it to outperform state-of-the-art personalized ranking measures, and also use it to visualize the high-level fashion trends across the 11-year span of our dataset.", "year": 2016, "publicationdate": "2016-02-04", "externalids": {"DOI": "10.1145/2872427.2883037"}, "doi_lower": "10.1145/2872427.2883037"}
{"paper_id": 233219850, "title": "Document-Level Event Argument Extraction by Conditional Generation", "author_names": ["Sha Li", "Heng Ji", "Jiawei Han"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.", "year": 2021, "publicationdate": "2021-04-13", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.69"}, "doi_lower": "10.18653/v1/2021.naacl-main.69"}
{"paper_id": 118783999, "title": "Argument structure and linking", "author_names": ["S. Wechsler"], "venue": "", "abstract": null, "year": 1991, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 4612975, "title": "T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples", "author_names": ["Hady ElSahar", "P. Vougiouklis", "Arslen Remaci", "C. Gravier", "Jonathon S. Hare", "F. Laforest", "E. Simperl"], "venue": "International Conference on Language Resources and Evaluation", "abstract": null, "year": 2018, "publicationdate": "2018-05-07", "externalids": {}, "doi_lower": null}
{"paper_id": 793385, "title": "Zero-Shot Relation Extraction via Reading Comprehension", "author_names": ["Omer Levy", "Minjoon Seo", "Eunsol Choi", "Luke Zettlemoyer"], "venue": "Conference on Computational Natural Language Learning", "abstract": "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.", "year": 2017, "publicationdate": "2017-06-13", "externalids": {"DOI": "10.18653/v1/K17-1034"}, "doi_lower": "10.18653/v1/k17-1034"}
{"paper_id": 159041722, "title": "HellaSwag: Can a Machine Really Finish Your Sentence?", "author_names": ["Rowan Zellers", "Ari Holtzman", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as “A woman sits at a piano,” a machine must select the most likely followup: “She sets her fingers on the keys.” With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical ‘Goldilocks’ zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.", "year": 2019, "publicationdate": "2019-05-01", "externalids": {"DOI": "10.18653/v1/P19-1472"}, "doi_lower": "10.18653/v1/p19-1472"}
{"paper_id": 258841149, "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning", "author_names": ["Seungone Kim", "Se June Joo", "Doyoung Kim", "Joel Jang", "Seonghyeon Ye", "Jamin Shin", "Minjoon Seo"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B&11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and +2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until the max length by a +13.98% margin. Our code, the CoT Collection data, and model checkpoints are publicly available.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14045"}, "doi_lower": "10.48550/arxiv.2305.14045"}
{"paper_id": 19240019, "title": "Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph", "author_names": ["Amrita Saha", "Vardaan Pahuja", "Mitesh M. Khapra", "Karthik Sankaranarayanan", "A. Chandar"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "While conversing with chatbots, humans typically tend to ask many questions, a significant portion of which can be answered by referring to large-scale knowledge graphs (KG). While Question Answering (QA) and dialog systems have been studied independently, there is a need to study them closely to evaluate such real-world scenarios faced by bots involving both these tasks. Towards this end, we introduce the task of Complex Sequential QA which combines the two tasks of (i) answering factual questions through complex inferencing over a realistic-sized KG of millions of entities, and (ii) learning to converse through a series of coherently linked QA pairs. Through a labor intensive semi-automatic process, involving in-house and crowdsourced workers, we created a dataset containing around 200K dialogs with a total of 1.6M turns. Further, unlike existing large scale QA datasets which contain simple questions that can be answered from a single tuple, the questions in our dialogs require a larger subgraph of the KG. Specifically, our dataset has questions which require logical, quantitative, and comparative reasoning as well as their combinations. This calls for models which can: (i) parse complex natural language questions, (ii) use conversation context to resolve coreferences and ellipsis in utterances, (iii) ask for clarifications for ambiguous queries, and finally (iv) retrieve relevant subgraphs of the KG to answer such questions. However, our experiments with a combination of state of the art dialog and QA models show that they clearly do not achieve the above objectives and are inadequate for dealing with such complex real world settings. We believe that this new dataset coupled with the limitations of existing models as reported in this paper should encourage further research in Complex Sequential QA.", "year": 2018, "publicationdate": "2018-01-31", "externalids": {"DOI": "10.1609/aaai.v32i1.11332"}, "doi_lower": "10.1609/aaai.v32i1.11332"}
{"paper_id": 221516475, "title": "Measuring Massive Multitask Language Understanding", "author_names": ["Dan Hendrycks", "Collin Burns", "Steven Basart", "Andy Zou", "Mantas Mazeika", "D. Song", "J. Steinhardt"], "venue": "International Conference on Learning Representations", "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.", "year": 2020, "publicationdate": "2020-09-07", "externalids": {}, "doi_lower": null}
{"paper_id": 16299141, "title": "Pointer Sentinel Mixture Models", "author_names": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "R. Socher"], "venue": "International Conference on Learning Representations", "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.", "year": 2016, "publicationdate": "2016-09-26", "externalids": {}, "doi_lower": null}
{"paper_id": 230799347, "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies", "author_names": ["Mor Geva", "Daniel Khashabi", "Elad Segal", "Tushar Khot", "D. Roth", "Jonathan Berant"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Abstract A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of ∼ 66%.", "year": 2021, "publicationdate": "2021-01-06", "externalids": {"DOI": "10.1162/tacl_a_00370"}, "doi_lower": "10.1162/tacl_a_00370"}
{"paper_id": 4711425, "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification", "author_names": ["James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.", "year": 2018, "publicationdate": "2018-03-14", "externalids": {"DOI": "10.18653/v1/N18-1074"}, "doi_lower": "10.18653/v1/n18-1074"}
{"paper_id": 224802782, "title": "Explainable Automated Fact-Checking for Public Health Claims", "author_names": ["Neema Kotonya", "Francesca Toni"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of fact-checking studies focus exclusively on political claims. Very little research explores fact-checking for other topics, specifically subject matters for which expertise is required. We present the first study of explainable fact-checking for claims which require specific expertise. For our case study we choose the setting of public health. To support this case study we construct a new dataset PUBHEALTH of 11.8K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check labels for claims. We explore two tasks: veracity prediction and explanation generation. We also define and evaluate, with humans and computationally, three coherence properties of explanation quality. Our results indicate that, by training on in-domain data, gains can be made in explainable, automated fact-checking for claims which require specific expertise.", "year": 2020, "publicationdate": "2020-10-19", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.623"}, "doi_lower": "10.18653/v1/2020.emnlp-main.623"}
{"paper_id": 1238927, "title": "Neural Text Generation from Structured Data with Application to the Biography Domain", "author_names": ["R. Lebret", "David Grangier", "Michael Auli"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.", "year": 2016, "publicationdate": "2016-03-24", "externalids": {"DOI": "10.18653/v1/D16-1128"}, "doi_lower": "10.18653/v1/d16-1128"}
{"paper_id": 226965071, "title": "WikiAsp: A Dataset for Multi-domain Aspect-based Summarization", "author_names": ["Hiroaki Hayashi", "Prashant Budania", "Peng Wang", "Chris Ackerson", "Raj Neervannan", "Graham Neubig"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Abstract Aspect-based summarization is the task of generating focused summaries based on specific points of interest. Such summaries aid efficient analysis of text, such as quickly understanding reviews or opinions from different angles. However, due to large differences in the type of aspects for different domains (e.g., sentiment, product features), the development of previous models has tended to be domain-specific. In this paper, we propose WikiAsp,1 a large-scale dataset for multi-domain aspect- based summarization that attempts to spur research in the direction of open-domain aspect-based summarization. Specifically, we build the dataset using Wikipedia articles from 20 different domains, using the section titles and boundaries of each article as a proxy for aspect annotation. We propose several straightforward baseline models for this task and conduct experiments on the dataset. Results highlight key challenges that existing summarization models face in this setting, such as proper pronoun handling of quoted sources and consistent explanation of time-sensitive events.", "year": 2020, "publicationdate": "2020-11-16", "externalids": {"DOI": "10.1162/tacl_a_00362"}, "doi_lower": "10.1162/tacl_a_00362"}
{"paper_id": 215768182, "title": "Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization", "author_names": ["Shashi Narayan", "Shay B. Cohen", "Mirella Lapata"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "We introduce “extreme summarization”, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question “What is the article about?”. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article’s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.", "year": 2018, "publicationdate": "2018-08-27", "externalids": {"DOI": "10.18653/v1/D18-1206"}, "doi_lower": "10.18653/v1/d18-1206"}
{"paper_id": 265258362, "title": "Vio-Lens: A Novel Dataset of Annotated Social Network Posts Leading to Different Forms of Communal Violence and its Evaluation", "author_names": ["Sourav Saha", "Jahedul Alam Junaed", "Maryam Saleki", "Arnab Sen Sharma", "Mohammad Rashidujjaman Rifat", "Mohamed Rahouti", "Syed Ishtiaque Ahmed", "Nabeel Mohammed", "Mohammad Ruhul Amin"], "venue": "BANGLALP", "abstract": "This paper presents a computational approach for creating a dataset on communal violence in the context of Bangladesh and West Bengal of India and benchmark evaluation. In recent years, social media has been used as a weapon by factions of different religions and backgrounds to incite hatred, resulting in physical communal violence and causing death and destruction. To prevent such abusive use of online platforms, we propose a framework for classifying online posts using an adaptive question-based approach. We collected more than 168,000 YouTube comments from a set of manually selected videos known for inciting violence in Bangladesh and West Bengal. Using both unsupervised and later semi-supervised topic modeling methods on those unstructured data, we discovered the major word clusters to interpret the related topics of peace and violence. Topic words were later used to select 20,142 posts related to peace and violence of which we annotated a total of 6,046 posts. Finally, we applied different modeling techniques based on linguistic features, and sentence transformers to benchmark the labeled dataset with the best-performing model reaching ~71% macro F1 score.", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.banglalp-1.9"}, "doi_lower": "10.18653/v1/2023.banglalp-1.9"}
{"paper_id": 11039301, "title": "Learning Question Classifiers", "author_names": ["Xin Li", "D. Roth"], "venue": "International Conference on Computational Linguistics", "abstract": "In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.This paper presents a machine learning approach to question classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into fine-grained classes. We show accurate results on a large collection of free-form questions used in TREC 10.", "year": 2002, "publicationdate": "2002-08-24", "externalids": {"DOI": "10.3115/1072228.1072378"}, "doi_lower": "10.3115/1072228.1072378"}
{"paper_id": 990233, "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author_names": ["R. Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "A. Ng", "Christopher Potts"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.", "year": 2013, "publicationdate": "2013-10-01", "externalids": {"DOI": "10.18653/v1/d13-1170"}, "doi_lower": "10.18653/v1/d13-1170"}
{"paper_id": 202712680, "title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search", "author_names": ["Hamel Husain", "Hongqiu Wu", "Tiferet Gazit", "Miltiadis Allamanis", "Marc Brockschmidt"], "venue": "arXiv.org", "abstract": "Semantic code search is the task of retrieving relevant code given a natural language query. While related to other information retrieval tasks, it requires bridging the gap between the language used in code (often abbreviated and highly technical) and natural language more suitable to describe vague concepts and ideas. \nTo enable evaluation of progress on code search, we are releasing the CodeSearchNet Corpus and are presenting the CodeSearchNet Challenge, which consists of 99 natural language queries with about 4k expert relevance annotations of likely results from CodeSearchNet Corpus. The corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation. In this article, we describe the methodology used to obtain the corpus and expert labels, as well as a number of simple baseline solutions for the task. \nWe hope that CodeSearchNet Challenge encourages researchers and practitioners to study this interesting task further and will host a competition and leaderboard to track the progress on the challenge. We are also keen on extending CodeSearchNet Challenge to more queries and programming languages in the future.", "year": 2019, "publicationdate": "2019-09-20", "externalids": {}, "doi_lower": null}
{"paper_id": 239998651, "title": "Training Verifiers to Solve Math Word Problems", "author_names": ["K. Cobbe", "Vineet Kosaraju", "Mo Bavarian", "Mark Chen", "Heewoo Jun", "Lukasz Kaiser", "Matthias Plappert", "Jerry Tworek", "Jacob Hilton", "Reiichiro Nakano", "Christopher Hesse", "John Schulman"], "venue": "arXiv.org", "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.", "year": 2021, "publicationdate": "2021-10-27", "externalids": {}, "doi_lower": null}
{"paper_id": 26124282, "title": "The JRC-Acquis: A Multilingual Aligned Parallel Corpus with 20+ Languages", "author_names": ["R. Steinberger", "B. Pouliquen", "Anna Widiger", "Camelia Ignat", "T. Erjavec", "D. Tufiș", "D. Varga"], "venue": "International Conference on Language Resources and Evaluation", "abstract": "We present a new, unique and freely available parallel corpus containing European Union (EU) documents of mostly legal nature. It is available in all 20 official EU languages, with additional documents being available in the languages of the EU candidate countries. The corpus consists of almost 8,000 documents per language, with an average size of nearly 9 million words per language. Pair-wise paragraph alignment information produced by two different aligners (Vanilla and HunAlign) is available for all 190+ language pair combinations. Most texts have been manually classified according to the EUROVOC subject domains so that the collection can also be used to train and test multi-label classification algorithms and keyword-assignment software. The corpus is encoded in XML, according to the Text Encoding Initiative Guidelines. Due to the large number of parallel texts in many languages, the JRC-Acquis is particularly suitable to carry out all types of cross-language research, as well as to test and benchmark text analysis software across different languages (for instance for alignment, sentence splitting and term extraction).", "year": 2006, "publicationdate": "2006-09-12", "externalids": {}, "doi_lower": null}
{"paper_id": 261049520, "title": "RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models", "author_names": ["Yasuto Hoshi", "D. Miyashita", "Youyang Ng", "Kento Tatsuno", "Yasuhiro Morioka", "Osamu Torii", "J. Deguchi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Retrieval-augmented large language models (R-LLMs) combine pre-trained large language models (LLMs) with information retrieval systems to improve the accuracy of factual question-answering. However, current libraries for building R-LLMs provide high-level abstractions without sufficient transparency for evaluating and optimizing prompts within specific inference processes such as retrieval and generation. To address this gap, we present RaLLe, an open-source framework designed to facilitate the development, evaluation, and optimization of R-LLMs for knowledge-intensive tasks. With RaLLe, developers can easily develop and evaluate R-LLMs, improving hand-crafted prompts, assessing individual inference processes, and objectively measuring overall system performance quantitatively. By leveraging these features, developers can enhance the performance and accuracy of their R-LLMs in knowledge-intensive generation tasks. We open-source our code at https://github.com/yhoshi3/RaLLe.", "year": 2023, "publicationdate": "2023-08-21", "externalids": {"DOI": "10.48550/arXiv.2308.10633"}, "doi_lower": "10.48550/arxiv.2308.10633"}
{"paper_id": 273041186, "title": "PandaChat-RAG: Towards the Benchmark for Slovenian RAG Applications", "author_names": [], "venue": "Proceedings of Slovenian Conference on Artificial Intelligence 2024", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.70314/is.2024.scai.538"}, "doi_lower": "10.70314/is.2024.scai.538"}
{"paper_id": 271244429, "title": "Optimizing Query Generation for Enhanced Document Retrieval in RAG", "author_names": ["Hamin Koo", "Minseon Kim", "Sung Ju Hwang"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) excel in various language tasks but they often generate incorrect information, a phenomenon known as\"hallucinations\". Retrieval-Augmented Generation (RAG) aims to mitigate this by using document retrieval for accurate responses. However, RAG still faces hallucinations due to vague queries. This study aims to improve RAG by optimizing query generation with a query-document alignment score, refining queries using LLMs for better precision and efficiency of document retrieval. Experiments have shown that our approach improves document retrieval, resulting in an average accuracy gain of 1.6%.", "year": 2024, "publicationdate": "2024-07-17", "externalids": {"DOI": "10.48550/arXiv.2407.12325"}, "doi_lower": "10.48550/arxiv.2407.12325"}
{"paper_id": 283038245, "title": "A Comparative Evaluation of RAG Architectures for Cross-Domain LLM Applications: Design, Implementation, and Assessment", "author_names": ["Pınar Ersoy", "Mustafa Erşahin"], "venue": "IEEE Access", "abstract": "Retrieval-Augmented Generation (RAG) pairs large language models with external search to constrain knowledge staleness and hallucination, a critical need in finance and e-commerce where numerical precision and regulatory auditability are non-negotiable. This study resembles four paradigms: sparse, dense, hybrid, and fusion retrieval, each realized as LangChain templates for cloud-hosted GPT-4o and on-premises LLaMA deployments. We embed a zero-shot baseline and assess every pipeline with the Retrieval-Augmented Generation Assessment (RAGAS) protocol, which scores faithfulness, relevancy, context quality, and latency without revealing proprietary gold data. Hybrid and fusion systems achieve the best combination of recall and faithfulness. Dense retrieval maximizes semantic coverage; sparse retrieval provides clearer, faster traceability. Each yields substantial gains over the baseline by anchoring profit metrics, compliance evidence, and pricing to audited documents. Although empirical evaluation primarily focuses on finance and e-commerce, the same blueprints and metrics can apply to healthcare, education, and legal technology. The domains in scope demand reasoning that is traceable, bias-aware, and up to date. This study, therefore, provides an auditable, production-ready framework for reliable RAG across knowledge-intensive settings.", "year": 2025, "publicationdate": null, "externalids": {"DOI": "10.1109/ACCESS.2025.3632404"}, "doi_lower": "10.1109/access.2025.3632404"}
{"paper_id": 263152733, "title": "RAGAs: Automated Evaluation of Retrieval Augmented Generation", "author_names": ["ES Shahul", "Jithin James", "Luis Espinosa Anke", "S. Schockaert"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAGAs is available at [https://github.com/explodinggradients/ragas]. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.", "year": 2023, "publicationdate": "2023-09-26", "externalids": {"DOI": "10.48550/arXiv.2309.15217"}, "doi_lower": "10.48550/arxiv.2309.15217"}
{"paper_id": 265221210, "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "author_names": ["Jon Saad-Falcon", "O. Khattab", "Christopher Potts", "Matei Zaharia"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.", "year": 2023, "publicationdate": "2023-11-16", "externalids": {"DOI": "10.48550/arXiv.2311.09476"}, "doi_lower": "10.48550/arxiv.2311.09476"}
{"paper_id": 280635516, "title": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter Serving", "author_names": ["Ferran Agulló", "Joan Oliveras", "Chen Wang", "Alberto Gutierrez-Torre", "Olivier Tardieu", "Alaa Youssef", "Jordi Torres", "Josep Ll. Berral"], "venue": "", "abstract": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause request starvation if GPU memory limits are exceeded. To address this issue, this study focuses on determining the joint configuration of concurrent and parallel adapters that maximizes GPU throughput without inducing starvation, given heterogeneous adapter and traffic properties. We propose a data-driven ML approach leveraging interpretable models to tackle this caching problem and introduce the first Digital Twin capable of reproducing an LLM-adapter serving system, enabling efficient training data generation. Experiments with the vLLM framework and LoRA adapters show that the Digital Twin reproduces throughput within 5.1% of real results, while the ML approach predicts optimal numbers of concurrent and parallel adapters with an error of at most 7.2% under heterogeneous, real-world workloads. The code is publicly available at https://github.com/FerranAgulloLopez/GPULLMAdapterOptimization.", "year": 2025, "publicationdate": "2025-08-11", "externalids": {}, "doi_lower": null}
{"paper_id": 261530434, "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "author_names": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.", "year": 2023, "publicationdate": "2023-09-04", "externalids": {"DOI": "10.48550/arXiv.2309.01431"}, "doi_lower": "10.48550/arxiv.2309.01431"}
{"paper_id": 265157546, "title": "RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge", "author_names": ["Yi Liu", "Lianzhe Huang", "Shicheng Li", "Sishuo Chen", "Hao Zhou", "Fandong Meng", "Jie Zhou", "Xu Sun"], "venue": "arXiv.org", "abstract": "LLMs and AI chatbots have improved people's efficiency in various fields. However, the necessary knowledge for answering the question may be beyond the models' knowledge boundaries. To mitigate this issue, many researchers try to introduce external knowledge, such as knowledge graphs and Internet contents, into LLMs for up-to-date information. However, the external information from the Internet may include counterfactual information that will confuse the model and lead to an incorrect response. Thus there is a pressing need for LLMs to possess the ability to distinguish reliable information from external knowledge. Therefore, to evaluate the ability of LLMs to discern the reliability of external knowledge, we create a benchmark from existing knowledge bases. Our benchmark consists of two tasks, Question Answering and Text Generation, and for each task, we provide models with a context containing counterfactual information. Evaluation results show that existing LLMs are susceptible to interference from unreliable external knowledge with counterfactual information, and simple intervention methods make limited contributions to the alleviation of this issue.", "year": 2023, "publicationdate": "2023-11-14", "externalids": {"DOI": "10.48550/arXiv.2311.08147"}, "doi_lower": "10.48550/arxiv.2311.08147"}
{"paper_id": 267320876, "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models", "author_names": ["Yuanjie Lyu", "Zhiyu Li", "Simin Niu", "Feiyu Xiong", "Bo Tang", "Wenjin Wang", "Hao Wu", "Huan Liu", "Tong Xu", "Enhong Chen"], "venue": "ACM Trans. Inf. Syst.", "abstract": "Retrieval-augmented generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate “hallucinated” content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question-answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this article constructs a large-scale and more comprehensive benchmark and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases and also categorize the range of RAG applications into four distinct types—create, read, update, and delete (CRUD). “Create” refers to scenarios requiring the generation of original, varied content. “Read” involves responding to intricate questions in knowledge-intensive situations. “Update” focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. “Delete” pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios. The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG.", "year": 2024, "publicationdate": "2024-01-30", "externalids": {"DOI": "10.1145/3701228"}, "doi_lower": "10.1145/3701228"}
{"paper_id": 263620134, "title": "Retrieval meets Long Context Large Language Models", "author_names": ["Peng Xu", "Wei Ping", "Xianchao Wu", "Lawrence C. McAfee", "Chen Zhu", "Zihan Liu", "Sandeep Subramanian", "E. Bakhturina", "M. Shoeybi", "Bryan Catanzaro"], "venue": "International Conference on Learning Representations", "abstract": "Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.", "year": 2023, "publicationdate": "2023-10-04", "externalids": {"DOI": "10.48550/arXiv.2310.03025"}, "doi_lower": "10.48550/arxiv.2310.03025"}
{"paper_id": 263909014, "title": "MemGPT: Towards LLMs as Operating Systems", "author_names": ["Charles Packer", "Vivian Fang", "Shishir G. Patil", "Kevin Lin", "Sarah Wooders", "Joseph Gonzalez"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.", "year": 2023, "publicationdate": "2023-10-12", "externalids": {"DOI": "10.48550/arXiv.2310.08560"}, "doi_lower": "10.48550/arxiv.2310.08560"}
{"paper_id": 263310483, "title": "Efficient Streaming Language Models with Attention Sinks", "author_names": ["Guangxuan Xiao", "Yuandong Tian", "Beidi Chen", "Song Han", "Mike Lewis"], "venue": "International Conference on Learning Representations", "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.", "year": 2023, "publicationdate": "2023-09-29", "externalids": {"DOI": "10.48550/arXiv.2309.17453"}, "doi_lower": "10.48550/arxiv.2309.17453"}
{"paper_id": 268510197, "title": "RAFT: Adapting Language Model to Domain Specific RAG", "author_names": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "M. Zaharia", "Ion Stoica", "Joseph Gonzalez"], "venue": "arXiv.org", "abstract": "Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a\"open-book\"in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT's code and demo are open-sourced at github.com/ShishirPatil/gorilla.", "year": 2024, "publicationdate": "2024-03-15", "externalids": {"DOI": "10.48550/arXiv.2403.10131"}, "doi_lower": "10.48550/arxiv.2403.10131"}
{"paper_id": 210861095, "title": "Scaling Laws for Neural Language Models", "author_names": ["J. Kaplan", "Sam McCandlish", "T. Henighan", "Tom B. Brown", "Benjamin Chess", "R. Child", "Scott Gray", "Alec Radford", "Jeff Wu", "Dario Amodei"], "venue": "arXiv.org", "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.", "year": 2020, "publicationdate": "2020-01-23", "externalids": {}, "doi_lower": null}
{"paper_id": 246431219, "title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval", "author_names": ["Uri Alon", "Frank F. Xu", "Junxian He", "Sudipta Sengupta", "D. Roth", "Graham Neubig"], "venue": "International Conference on Machine Learning", "abstract": "Retrieval-based language models (R-LM) model the probability of natural language text by combining a standard language model (LM) with examples retrieved from an external datastore at test time. While effective, a major bottleneck of using these models in practice is the computationally costly datastore search, which can be performed as frequently as every time step. In this paper, we present RetoMaton - retrieval automaton - which approximates the datastore search, based on (1) saving pointers between consecutive datastore entries, and (2) clustering of entries into\"states\". This effectively results in a weighted finite automaton built on top of the datastore, instead of representing the datastore as a flat list. The creation of the automaton is unsupervised, and a RetoMaton can be constructed from any text collection: either the original training corpus or from another domain. Traversing this automaton at inference time, in parallel to the LM inference, reduces its perplexity by up to 1.85, or alternatively saves up to 83% of the nearest neighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting perplexity. Our code and trained models are available at https://github.com/neulab/retomaton .", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 253802096, "title": "Retrieval-Augmented Multimodal Language Modeling", "author_names": ["Michihiro Yasunaga", "Armen Aghajanyan", "Weijia Shi", "Rich James", "J. Leskovec", "Percy Liang", "M. Lewis", "Luke Zettlemoyer", "Wen-tau Yih"], "venue": "International Conference on Machine Learning", "abstract": "Recent multimodal models such as DALL-E and\r\nCM3 have achieved remarkable progress in textto-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the\r\nmodel parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrievalaugmented multimodal model, which enables a base multimodal model (generator) to refer to relevant knowledge fetched by a retriever from external memory (e.g., multimodal documents on the web). Specifically, we implement a retriever using the pretrained CLIP model and a generator using the CM3 Transformer architecture, and\r\ntrain this model using the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate mixtures of text and images.\r\nWe show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks (12 FID and 17 CIDEr improvements on MSCOCO), while requiring much less compute for training (<30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel capabilities such as knowledge-intensive image generation and multimodal in-context learning", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2211.12561"}, "doi_lower": "10.48550/arxiv.2211.12561"}
{"paper_id": 256390509, "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "author_names": ["Junnan Li", "Dongxu Li", "S. Savarese", "Steven C. H. Hoi"], "venue": "International Conference on Machine Learning", "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.", "year": 2023, "publicationdate": "2023-01-30", "externalids": {"DOI": "10.48550/arXiv.2301.12597"}, "doi_lower": "10.48550/arxiv.2301.12597"}
{"paper_id": 252781022, "title": "Visualize Before You Write: Imagination-Guided Open-Ended Text Generation", "author_names": ["Wanrong Zhu", "An Yan", "Yujie Lu", "Wenda Xu", "X. Wang", "Miguel P. Eckstein", "William Yang Wang"], "venue": "Findings", "abstract": "Recent advances in text-to-image synthesis make it possible to visualize machine imaginations for a given context. On the other hand, when generating text, human writers are gifted at creative visualization, which enhances their writings by forming imaginations as blueprints before putting down the stories in words. Inspired by such a cognitive process, we ask the natural question of whether we can endow machines with the same ability to utilize visual information and construct a general picture of the context to guide text generation. In this work, we propose iNLG that uses machine-generated images to guide language models (LM) in open-ended text generation. The experiments and analyses demonstrate the effectiveness of iNLG on open-ended text generation tasks, including text completion, story generation, and concept-to-text generation in both few-shot and full-data scenarios. Both automatic metrics and human evaluations verify that the text snippets generated by our iNLG are coherent and informative while displaying minor degeneration.", "year": 2022, "publicationdate": "2022-10-07", "externalids": {"DOI": "10.48550/arXiv.2210.03765"}, "doi_lower": "10.48550/arxiv.2210.03765"}
{"paper_id": 252918531, "title": "Generating Synthetic Speech from SpokenVocab for Speech Translation", "author_names": ["Jinming Zhao", "Gholamreza Haffar", "Ehsan Shareghi"], "venue": "Findings", "abstract": "Training end-to-end speech translation (ST) systems requires sufficiently large-scale data, which is unavailable for most language pairs and domains. One practical solution to the data scarcity issue is to convert text-based machine translation (MT) data to ST data via text-to-speech (TTS) systems.Yet, using TTS systems can be tedious and slow. In this work, we propose SpokenVocab, a simple, scalable and effective data augmentation technique to convert MT data to ST data on-the-fly. The idea is to retrieve and stitch audio snippets, corresponding to words in an MT sentence, from a spoken vocabulary bank. Our experiments on multiple language pairs show that stitched speech helps to improve translation quality by an average of 1.83 BLEU score, while performing equally well as TTS-generated speech in improving translation quality. We also showcase how SpokenVocab can be applied in code-switching ST for which often no TTS systems exit.", "year": 2022, "publicationdate": "2022-10-15", "externalids": {"DOI": "10.48550/arXiv.2210.08174"}, "doi_lower": "10.48550/arxiv.2210.08174"}
{"paper_id": 255545899, "title": "Using External Off-Policy Speech-To-Text Mappings in Contextual End-To-End Automated Speech Recognition", "author_names": ["David Chan", "Shalini Ghosh", "A. Rastrow", "Björn Hoffmeister"], "venue": "arXiv.org", "abstract": "Despite improvements to the generalization performance of automated speech recognition (ASR) models, specializing ASR models for downstream tasks remains a challenging task, primarily due to reduced data availability (necessitating increased data collection), and rapidly shifting data distributions (requiring more frequent model fine-tuning). In this work, we investigate the potential of leveraging external knowledge, particularly through off-policy key-value stores generated with text-to-speech methods, to allow for flexible post-training adaptation to new data distributions. In our approach, audio embeddings captured from text-to-speech, along with semantic text embeddings, are used to bias ASR via an approximate k-nearest-neighbor (KNN) based attentive fusion step. Our experiments on LibiriSpeech and in-house voice assistant/search datasets show that the proposed approach can reduce domain adaptation time by up to 1K GPU-hours while providing up to 3% WER improvement compared to a fine-tuning baseline, suggesting a promising approach for adapting production ASR systems in challenging zero and few-shot scenarios.", "year": 2023, "publicationdate": "2023-01-06", "externalids": {"DOI": "10.48550/arXiv.2301.02736"}, "doi_lower": "10.48550/arxiv.2301.02736"}
{"paper_id": 257232853, "title": "Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning", "author_names": ["Antoine Yang", "Arsha Nagrani", "P. H. Seo", "Antoine Miech", "J. Pont-Tuset", "I. Laptev", "Josef Sivic", "C. Schmid"], "venue": "Computer Vision and Pattern Recognition", "abstract": "In this work, we introduce Vid2Seq, a multi-modal single-stage dense event captioning model pretrained on narrated videos which are readily-available at scale. The Vid2Seq architecture augments a language model with special time tokens, allowing it to seamlessly predict event boundaries and textual descriptions in the same output sequence. Such a unified model requires large-scale training data, which is not available in current annotated datasets. We show that it is possible to leverage unlabeled narrated videos for dense video captioning, by reformulating sentence boundaries of transcribed speech as pseudo event boundaries, and using the transcribed speech sentences as pseudo event captions. The resulting Vid2Seq model pretrained on the YT-Temporal-1B dataset improves the state of the art on a variety of dense video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions. Vid2Seq also generalizes well to the tasks of video paragraph captioning and video clip captioning, and to few-shot settings. Our code is publicly available at [1].", "year": 2023, "publicationdate": "2023-02-27", "externalids": {"DOI": "10.1109/CVPR52729.2023.01032"}, "doi_lower": "10.1109/cvpr52729.2023.01032"}
{"paper_id": 259860357, "title": "Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning", "author_names": ["Noor Nashid", "Mifta Sintaha", "A. Mesbah"], "venue": "International Conference on Software Engineering", "abstract": "Large language models trained on massive code corpora can generalize to new tasks without the need for task-specific fine-tuning. In few-shot learning, these models take as input a prompt, composed of natural language instructions, a few instances of task demonstration, and a query and generate an output. However, the creation of an effective prompt for code-related tasks in few-shot learning has received little attention. We present a technique for prompt creation that automatically retrieves code demonstrations similar to the developer task, based on embedding or frequency analysis. We apply our approach, Cedar, to two different programming languages, statically and dynamically typed, and two different tasks, namely, test assertion generation and program repair. For each task, we compare Cedar with state-of-the-art task-specific and fine-tuned models. The empirical results show that, with only a few relevant code demonstrations, our prompt creation technique is effective in both tasks with an accuracy of 76% and 52% for exact matches in test assertion generation and program repair tasks, respectively. For assertion generation, Cedar outperforms existing task-specific and fine-tuned models by 333% and 11%, respectively. For program repair, Cedar yields 189% better accuracy than task-specific models and is competitive with recent fine-tuned models. These findings have practical implications for practitioners, as Cedar could potentially be applied to multilingual and multitask settings without task or language-specific training with minimal examples and effort.", "year": 2023, "publicationdate": "2023-05-01", "externalids": {"DOI": "10.1109/ICSE48619.2023.00205"}, "doi_lower": "10.1109/icse48619.2023.00205"}
