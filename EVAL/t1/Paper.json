{
  "authors": [
    "Wayne Xin Zhao",
    "Kun Zhou",
    "Junyi Li",
    "Tianyi Tang",
    "Xiaolei Wang",
    "Yupeng Hou",
    "Yingqian Min",
    "Beichen Zhang",
    "Junjie Zhang",
    "Zican Dong",
    "Yifan Du",
    "Chen Yang",
    "Yushuo Chen",
    "Z. Chen",
    "Jinhao Jiang",
    "Ruiyang Ren",
    "Yifan Li",
    "Xinyu Tang",
    "Zikang Liu",
    "Peiyu Liu",
    "J. Nie",
    "Ji-rong Wen"
  ],
  "literature_review_title": "A Survey of Large Language Models",
  "year": "2023",
  "date": "2023-03-31",
  "category": "cs.CL",
  "abstract": "Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.",
  "structure": [
    {
      "section_title": "Introduction/Pre-section",
      "level": "1",
      "content": "\\documentclass[10pt,journal,compsoc,x11names]{IEEEtran} \\usepackage[switch]{lineno} amsfonts amsmath, bm xspace enumitem amssymb booktabs \\usepackage[hyphens]{url} ragged2e hyperref multirow graphicx xcolor color colortbl tablefootnote pifont makecell \\usepackage[most]{tcolorbox} framed mdframed subfigure caption longtable float booktabs \\paratitle[1]{1.5ex\\noindent#1} H{>{\\setbox0=\\hbox\\bgroup}c<{\\egroup}@{}} \\ie{i.e.,\\xspace} \\aka{a.k.a.,\\xspace} \\eg{e.g.,\\xspace} \\wrt{w.r.t.\\xspace} \\wo{w/o\\xspace} \\etc{etc} \\ignore[1]{} \\tabincell[2]{tabular{@{}#1@{}}#2tabular} gold{RGB}{205,133,63} fGreen{RGB}{34,139,34} tOrange{RGB}{255,215,0} tBlue{RGB}{135,206,250} tPink{RGB}{255,204,204} tGreen{RGB}{205,230,199} tGold{RGB}{255,215,0} cite \\usepackage[numbers,sort&compress]{natbib} \\ifCLASSINFOpdf \\else \\fi \\newcommandbookmarks=true,bookmarksnumbered=true, pdfpagemode={UseOutlines,plainpages=false,pdfpagelabels=true, colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black}, pdftitle={Bare Demo of IEEEtran.cls for Computer Society Journals},% pdfsubject={Typesetting},% pdfauthor={Michael D. Shell},% pdfkeywords={Computer Society, IEEEtran, journal, LaTeX, paper, template}}% op-tical net-works semi-conduc-tor document A Survey of Large Language Models Wayne Xin Zhao, Kun Zhou*, Junyi Li*, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Ji-Rong Wen \\IEEEcompsocitemizethanks{ \\IEEEcompsocthanksitem Version: v16 (major update on March 11, 2025). \\IEEEcompsocthanksitem GitHub link: \\url{https://github.com/RUCAIBox/LLMSurvey \\IEEEcompsocthanksitem Chinese book link: lmbook-zh.github.io \\IEEEcompsocthanksitem * K. Zhou and J. Li contribute equally to this work. \\IEEEcompsocthanksitem The authors are mainly with Gaoling School of Artificial Intelligence and School of Information, Renmin University of China, Beijing, China; Jian-Yun Nie is with DIRO, Universit\\'{e} de Montr\\'{e}al, Canada. \\\\ Contact e-mail: batmanfly@gmail.com \\IEEEcompsocthanksitem red{The authors of this survey paper reserve all the copyrights of the figures/tables, and any use of these materials for publication purpose must be officially granted by the survey authors.} }% } \\markboth{}% {Shell \\textit{et al.}: Bare Advanced Demo of IEEEtran.cls for IEEE Computer Society Journals} % \\begin{IEEEkeywords Large Language Models; Emergent Abilities; Adaptation Tuning; {Utilization; Alignment; Capacity Evaluation} IEEEkeywords} \\maketitle \\IEEEdisplaynontitleabstractindextext \\IEEEpeerreviewmaketitle \\IEEEraisesectionheading{",
      "origin_cites_number": 0
    },
    {
      "section_title": "Introduction",
      "level": "1",
      "content": "} flushright \\rightskip=0.8cm``The limits of my language mean the limits of my world.'' \\\\ .2em \\rightskip=.8cm---Ludwig Wittgenstein flushright 1em \\begin{figure*[h] \\centering minipage{0.45\\textwidth} \\includegraphics[width=\\textwidth]{images/paper_number1.pdf} \\captionof*{figure}{(a) Query=\"Language Model\"} minipage \\qquad minipage{0.45\\textwidth} \\includegraphics[width=\\textwidth]{images/paper_number2.pdf} \\captionof*{figure}{(b) Query=\"Large Language Model\"} minipage {The trends of the cumulative numbers of arXiv papers that contain the keyphrases ``\\emph{language model\" (since June 2018) and ``large language model'' (since October 2019), respectively. The statistics are calculated using exact match by querying the keyphrases in title or abstract by months. We set different x-axis ranges for the two keyphrases, because ``language models'' have been explored at an earlier time. We label the points corresponding to important landmarks in the research progress of LLMs. A sharp increase occurs after the release of ChatGPT: the average number of published arXiv papers that contain ``large language model'' in title or abstract goes from 0.40 per day to 8.58 per day (Figure~fig:paper_number(b)). }} figure*} figure*[h] \\centering minipage{0.45\\textwidth} \\includegraphics[width=\\textwidth]{images/paper_number1.pdf} \\captionof*{figure}{(a) Query=\"Language Model\"} minipage \\qquad minipage{0.45\\textwidth} \\includegraphics[width=\\textwidth]{images/paper_number2.pdf} \\captionof*{figure}{(b) Query=\"Large Language Model\"} minipage {The trends of the cumulative numbers of arXiv papers that contain the keyphrases ``\\emph{language model\" (since June 2018) and ``large language model'' (since October 2019), respectively. The statistics are calculated using exact match by querying the keyphrases in title or abstract by months. We set different x-axis ranges for the two keyphrases, because ``language models'' have been explored at an earlier time. We label the points corresponding to important landmarks in the research progress of LLMs. A sharp increase occurs after the release of ChatGPT: the average number of published arXiv papers that contain ``large language model'' in title or abstract goes from 0.40 per day to 8.58 per day (Figure~fig:paper_number(b)). }} figure* figure* \\centering \\includegraphics[width=.9\\textwidth]{images/task-solvers.pdf} An evolution process of the four generations of language models~(LM) from the perspective of task solving capacity. Note that the time period for each stage may not be very accurate, and we set the time mainly according to the publish date of the most representative studies at each stage. For neural language models, we abbreviate the paper titles of two representative studies to name the two approaches: NPLM~\\cite{Bengio-JMLR-2003-A (``A neural probabilistic language model'') and NLPS~Collobert-JMLR-2011 (``Natural language processing (almost) from scratch''). Due to the space limitation, we don't list all representative studies in this figure. } figure* L{anguage} is a prominent ability in human beings to express and communicate, which develops in early childhood and evolves over a lifetime~instinct-book,hauser-science-2002-faculty. Machines, however, cannot naturally grasp the abilities of understanding and communicating in the form of human language, unless equipped with powerful artificial intelligence~(AI) algorithms. It has been a longstanding research challenge to achieve this goal, to enable machines to read, write, and communicate like humans~turing-test. Technically, language modeling~(LM) is one of the major approaches to advancing language intelligence of machines. In general, LM aims to model the generative likelihood of word sequences, so as to predict the probabilities of future (or missing) tokens. The research of LM has received extensive attention in the literature, which can be divided into four major development stages: $\\bullet$ Statistical language models~(SLM). SLMs~NLP-speech-book,SLM-2004,rosenfeld2000two,stolcke2002srilm are developed based on statistical learning methods that rose in the 1990s. The basic idea is to build the word prediction model based on the Markov assumption, \\eg predicting the next word based on the most recent context. The SLMs with a fixed context length $n$ are also called $n$-gram language models, \\eg bigram and trigram language models. SLMs have been widely applied to enhance task performance in information retrieval~(IR)~SLM-IR1,SLM-IR2 and natural language processing~(NLP)~Thede-acl-1999-a,bahl1989tree,Brants-emnlp-2007-large. However, they often suffer from the curse of dimensionality: it is difficult to accurately estimate high-order language models since an exponential number of transition probabilities need to be estimated. {Thus}, specially designed smoothing strategies such as back-off estimation~Katz-IEEE-1987-estimation and Goodâ€“Turing estimation~Gale-JQL-1995-good have been introduced {to alleviate the data sparsity problem. } $\\bullet$ Neural language models~(NLM). NLMs~Bengio-JMLR-2003-A,Mikolov-INTERSPEECH-2010,Kombrink-INTERSPEECH-2011 characterize the probability of word sequences by neural networks, \\eg multi-layer perceptron~(MLP) and recurrent neural networks~(RNNs). As a remarkable contribution, the work in Bengio-JMLR-2003-A introduced the concept of distributed representation of words and built the word prediction function conditioned on the aggregated context features (\\ie the distributed word vectors). By extending the idea of learning effective features for text data, a general neural network approach was developed to build a unified, end-to-end solution for various NLP tasks~Collobert-JMLR-2011. Furthermore, word2vec~Mikolov-NIPS-2013,Mikolov-ICLR-2013 was proposed to build a simplified shallow neural network for learning distributed word representations, which were demonstrated to be very effective across a variety of NLP tasks. These studies have initiated the use of language models for representation learning (beyond word sequence modeling), having an important impact on the field of NLP. $\\bullet$ Pre-trained language models~(PLM). As an early attempt, ELMo~Peters-NAACL-2018 was proposed to capture context-aware word representations by first pre-training a bidirectional LSTM~(biLSTM) network ({instead of learning fixed word representations}) and then fine-tuning the biLSTM network according to specific downstream tasks. Furthermore, based on the highly parallelizable Transformer architecture~Vaswani-NIPS-2017-Attention with self-attention mechanisms, BERT~Devlin-NAACL-2019-BERT was proposed by pre-training bidirectional language models with specially designed pre-training tasks on large-scale unlabeled corpora. These pre-trained context-aware word representations are very effective as general-purpose semantic features, which have largely raised the performance bar of NLP tasks. This study has inspired a large number of follow-up work, which sets the ``pre-training and fine-tuning'' learning paradigm. Following this paradigm, a great number of studies on PLMs have been developed, introducing either different architectures~Lewis-ACL-2020-BART,Fedus-JMLR-2021-Switch (\\eg GPT-2~radford-blog-2019-language and BART~Lewis-ACL-2020-BART) or improved pre-training strategies~Liu-CoRR-2019-RoBERTa,Sanh-ICLR-2022-Multitask,Wang-ICML-2022-What. In this paradigm, it often requires fine-tuning the PLM for adapting to different downstream tasks. $\\bullet$ Large language models~(LLM). Researchers find that scaling PLM (\\eg scaling model size or data size) often leads to an improved model capacity on downstream tasks (\\ie following the scaling law~Kaplan-arxiv-2020-Scaling). A number of studies have explored the performance limit by training an ever larger PLM (\\eg the 175B-parameter GPT-3 and the 540B-parameter PaLM). Although scaling is mainly conducted in model size (with similar architectures and pre-training tasks), these large-sized PLMs display different behaviors from smaller PLMs (\\eg 330M-parameter BERT and 1.5B-parameter GPT-2) and show surprising abilities (called emergent abilities~Wei-arxiv-2022-Emergent) in solving a series of complex tasks. For example, GPT-3 can solve few-shot tasks through in-context learning, whereas GPT-2 cannot do well. Thus, the research community coins the term ``large language models~(LLM)''Note that a LLM is not necessarily more capable than a small PLM, and emergent abilities may not occur in some LLMs. for these large-sized PLMs~Shanahan-arxiv-2022-Talking,Wei-arxiv-2022-chain,Hoffmann-arxiv-2022-Training,Taylor-arxiv-2022-Galactica, which attract increasing research attention (See Figure~fig:paper_number). A remarkable application of LLMs is ChatGPThttps://openai.com/blog/chatgpt/ that adapts the LLMs from the GPT series for dialogue, which presents an amazing conversation ability with humans. We can observe a sharp increase of the arXiv papers that are related to LLMs after the release of ChatGPT in Figure~fig:paper_number. { As discussed before, language model is not a new technical concept specially for LLMs, but has evolved with the advance of artificial intelligence over the decades. Early language models mainly aim to model and generate text data, while latest language models (\\eg GPT-4) focus on {complex task solving}. From language modeling to task solving, it is an important leap in scientific thinking, which is the key to understand the development of language models in the research history. From the perspective of task solving, the four generations of language models have exhibited different levels of model capacities. In Figure~fig:task_solvers, we describe the evolution process of language models in terms of the task solving capacity. At first, statistical language models mainly assisted in some specific tasks (\\eg retrieval or speech tasks), in which the predicted or estimated probabilities can enhance the performance of task-specific approaches. Subsequently, neural language models focused on learning task-agnostic representations (\\eg features), aiming to reduce the efforts for human feature engineering. Furthermore, pre-trained language models learned context-aware representations that can be optimized according to downstream tasks. For the latest generation of language model, LLMs are enhanced by exploring the scaling effect on model capacity, which can be considered as general-purpose task solvers. To summarize, in the evolution process, the task scope that can be solved by language models have been greatly extended, and the task performance attained by language models have been significantly enhanced. } In the existing literature, PLMs have been widely discussed and surveyed~Liu-survey-2023-Pre-train,Zhou-arxiv-2023-A,Han-AIopen-2021-PTM,qiu-CoRR-2020-PTM, while LLMs are seldom reviewed in a systematic way. To motivate our survey, we first highlight three major differences between LLMs and PLMs. First, LLMs display some surprising emergent abilities that may not be observed in previous smaller PLMs. These abilities are key to the performance of language models on complex tasks, making AI algorithms unprecedently powerful and effective. Second, LLMs would revolutionize the way that humans develop and use AI algorithms. Unlike small PLMs, the major approach to accessing LLMs is through the prompting interface (\\eg GPT-4 API). Humans have to understand how LLMs work and format their tasks in a way that LLMs can follow. Third, the development of LLMs no longer draws a clear distinction between research and engineering. The training of LLMs requires extensive practical experiences in large-scale data processing and distributed parallel training. To develop capable LLMs, researchers have to solve complicated engineering issues, working with engineers or being engineers. Nowadays, LLMs are posing a significant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the rethinking of the possibilities of artificial general intelligence~(AGI). OpenAI has published a technical article entitled ``Planning for AGI and beyond'', which discusses the short-term and long-term plans to approach AGI~OpenAI-blog-2023-Planning, and a more recent paper has argued that GPT-4 might be considered as an early version of an AGI system~Bubeck-arxiv-2023-Sparks. The research areas of AI are being revolutionized by the rapid progress of LLMs. In the field of NLP, LLMs can serve as a general-purpose language task solver (to some extent), and the research paradigm has been shifting towards the use of LLMs. In the field of IR, traditional search engines are challenged by the new information seeking way through AI chatbots (\\ie ChatGPT), and New Binghttps://www.bing.com/new presents an initial attempt that enhances the search results based on LLMs. In the field of CV, the researchers try to develop ChatGPT-like vision-language models that can better serve multimodal dialogues~Huang-CoRR-2023,Cao-arxiv-2023-comprehensive, driess-arxiv-2023-palm,wu-arxiv-2023-visual, and GPT-4~OpenAI-OpenAI-2023-GPT-4 has supported multimodal input by integrating the visual information. This new wave of technology would potentially lead to a prosperous ecosystem of real-world applications based on LLMs. For instance, Microsoft 365 is being empowered by LLMs (\\ie Copilot) to automate the office work, and OpenAI supports the use of plugins in ChatGPT for implementing special functions. Despite the progress and impact, the underlying principles of LLMs are still not well explored. Firstly, it is mysterious why emergent abilities occur in LLMs, instead of smaller PLMs. As a more general issue, there lacks a deep, detailed investigation of the key factors that contribute to the superior abilities of LLMs. It is important to study when and how LLMs obtain such abilities~FU-blog-2022-how. Although there are some meaningful discussions about this problem~Wei-arxiv-2022-Emergent,FU-blog-2022-how, more principled investigations are needed to uncover the ``secrets`` of LLMs. Secondly, it is difficult for the research community to train capable LLMs. Due to the huge demand of computation resources, it is very costly to carry out repetitive, ablating studies for investigating the effect of various strategies for training LLMs. Indeed, LLMs are mainly trained by industry, where many important training details (\\eg data collection and cleaning) are not revealed to the public. Thirdly, it is challenging to align LLMs with human values or preferences. Despite the capacities, LLMs are also likely to produce toxic, fictitious, or harmful contents. It requires effective and efficient control approaches to eliminating the potential risk of the use of LLMs~OpenAI-OpenAI-2023-GPT-4. Faced with both opportunities and challenges, it needs more attention on the research and development of LLMs. In order to provide a basic understanding of LLMs, this survey conducts a literature review of the recent advances in LLMs from four major aspects, including pre-training (how to pre-train a capable LLM), adaptation (how to effectively adapt pre-trained LLMs for better use), utilization (how to use LLMs for solving various downstream tasks) and capability evaluation (how to evaluate the abilities of LLMs and existing empirical findings). We thoroughly comb the literature and summarize the key findings, techniques, and methods of LLMs. For this survey, we also create a GitHub project website by collecting the supporting resources for LLMs, at the link https://github.com/RUCAIBox/LLMSurvey. We are also aware of several related review articles on PLMs or LLMs~Han-AIopen-2021-PTM,qiu-CoRR-2020-PTM,Li-IJCAI-2021-Pretrained,Liu-survey-2023-Pre-train,Lu-arxiv-2022-Survey,Dong-arxiv-2023-A,Shanahan-arxiv-2022-Talking,Huang-arxiv-2022-Towards,Qiao-arxiv-2022-Reasoning,Cao-arxiv-2023-comprehensive,Zhou-FITEE-2023-ChatGPT,Zhao-arxiv-2022-Dense. These papers either discuss PLMs or some specific (or general) aspects of LLMs. Compared with them, we focus on the techniques and methods to develop and use LLMs and provide a relatively comprehensive reference to important aspects of LLMs. The remainder of this survey is organized as follows: Section 2 introduces the background for LLMs and the evolution of GPT-series models, followed by the summarization of available resources for developing LLMs in Section 3. Sections 4, 5, 6, and 7 review and summarize the recent progress from the four aspects of pre-training, adaptation, utilization, and capacity evaluation, respectively. Then, Section 8 discusses the practical guide for prompt design, and Section 9 reviews the applications of LLMs in several representative domains. Finally, we conclude the survey in Section 10 by summarizing the major findings and discuss the remaining issues for future work.",
      "origin_cites_number": 32
    },
    {
      "section_title": "Overview",
      "level": "1",
      "content": "In this section, we present an overview about the background of LLMs and then summarize the technical evolution of the GPT-series models.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Background for LLMs",
      "level": "2",
      "content": "Typically, large language models~(LLMs) refer to Transformer language models that contain hundreds of billions~(or more) of parametersIn existing literature, there is no formal consensus on the minimum parameter scale for LLMs, since the model capacity is also related to data size and total compute. In this survey, we take a slightly loose definition of LLMs, and mainly focus on discussing language models with a model size larger than 10B. , which are trained on massive text data~Shanahan-arxiv-2022-Talking, such as GPT-3~Brown-NeurIPS-2020-Language, PaLM~Chowdhery-arxiv-2022-PaLM, Galactica~Taylor-arxiv-2022-Galactica, and LLaMA~Touvron-arxiv-2023-LLaMA. LLMs exhibit strong capacities to understand natural language and solve complex tasks (via text generation). To have a quick understanding of how LLMs work, this part introduces the basic background for LLMs, including scaling laws, emergent abilities and key techniques. Formulation of Scaling Laws for LLMs. Currently, LLMs are mainly built upon the Transformer architecture~Vaswani-NIPS-2017-Attention, where multi-head attention layers are stacked in a very deep neural network. Existing LLMs adopt similar Transformer architectures and pre-training objectives (\\eg language modeling) as small language models. However, LLMs significantly extend the model size, data size, and total compute (orders of magnification). Extensive research has shown that scaling can largely improve the model capacity of LLMs~radford-blog-2019-language,Brown-NeurIPS-2020-Language,Chowdhery-arxiv-2022-PaLM. Thus, it is useful to establish a quantitative approach to characterizing the scaling effect. Next, we introduce two representative scaling laws for Transformer language models~Kaplan-arxiv-2020-Scaling,Hoffmann-arxiv-2022-Training. $\\bullet$ KM scaling lawSince there was not a model trained following this law in the original paper, we took the last names of the two co-first authors to name this scaling law. . In 2020, Kaplan et al.~Kaplan-arxiv-2020-Scaling (the OpenAI team) firstly proposed to model the power-law relationship of model performance with respective to three major factors, namely model size ($N$), dataset size ($D$), and the amount of training compute ($C$), for neural language models. Given a compute budget $c$, they empirically presented three basic formulas for the scaling lawHere, $N_c$, $D_c$ and $C_c$ are measured in the number of non-embedding parameters, the number of training tokens and the number of FP-days, respectively. According to the original paper~\\cite{Kaplan-arxiv-2020-Scaling, $C_c$ and $C$ should be denoted by $C_c^{min}$ and $C_{min}$, corresponding to the optimal use of compute. We use the simplified notations for ease of discussions. }: small eqnarray L(N) &=& \\bigg(N_c{N}\\bigg)^{\\alpha_N}, ~~~ \\alpha_N \\sim 0.076, N_c \\sim 8.8\\times 10^{13} \\\\\\nonumber L(D) &=& \\bigg(D_c {D}\\bigg)^{\\alpha_D}, ~~~ \\alpha_D \\sim 0.095, D_c \\sim 5.4\\times 10^{13} \\\\\\nonumber L(C) &=& \\bigg(C_c{C}\\bigg)^{\\alpha_C}, ~~~ \\alpha_C \\sim 0.050, C_c \\sim 3.1\\times 10^{8}\\nonumber eqnarray small \\noindent where $L(\\cdot)$ denotes the cross entropy loss in nats, and a follow-up study~Henighan-2020-scalinglaw from OpenAI has shown that the language modeling loss can be decomposed into two parts, namely irreducible loss (the entropy of the true data distribution) and reducible loss (an estimate of the KL divergence between the true and model distributions). The three laws were derived by fitting the model performance with varied data sizes (22M to 23B tokens), model sizes (768 to 1.5B non-embedding parameters) and training compute, under some assumptions (\\eg the analysis of one factor should be not bottlenecked by the other two factors). They showed that the model performance has a strong dependence relation on the three factors. $\\bullet$ Chinchilla scaling law. As another representative study, Hoffmann et al.~Hoffmann-arxiv-2022-Training (the Google DeepMind team) proposed an alternative form for scaling laws to instruct the compute-optimal training for LLMs. They conducted rigorous experiments by varying a larger range of model sizes (70M to 16B) and data sizes (5B to 500B tokens), and fitted a similar scaling law yet with different coefficients as below~Hoffmann-arxiv-2022-Training: equation L(N, D) = E + A{N^\\alpha} + B{D^{\\beta}}, equation where $E = 1.69, A = 406.4, B = 410.7$, $\\alpha=0.34$ and $\\beta=0.28$. By optimizing the loss $L(N, D)$ under the constraint $C\\approx 6ND$, they showed that the optimal allocation of compute budget to model size and data size can be derived as follows: small eqnarray N_{opt}(C)=G \\bigg(C{6}\\bigg)^a, ~~~ D_{opt}(C)=G^{-1} \\bigg(C{6}\\bigg)^b, eqnarray\\nonumber small \\noindent where $a=\\alpha{\\alpha+\\beta}$, $b=\\beta{\\alpha+\\beta}$ and $G$ is a scaling coefficient that can be computed by $A$, $B$, $\\alpha$ and $\\beta$. As analyzed in Hoffmann-arxiv-2022-Training, given an increase in compute budget, the KM scaling law favors a larger budget allocation in model size than the data size, while the Chinchilla scaling law argues that the two sizes should be increased in equal scales, \\ie having similar values for $a$ and $b$ in Equation~eq-CSL. Discussion on Scaling Laws. After introducing the formulations, we continue to discuss scaling law in the following two aspects, to enhance its understanding: { $\\bullet$ Predictable scaling. % In practice, scaling law can be used to instruct the training of LLMs, and it has been proven feasible to reliably estimate the performance of larger models based on that of smaller models, called predictable scaling~OpenAI-OpenAI-2023-GPT-4. The benefits of predictable scaling for training LLMs are mainly twofold. Firstly, for large models, it is infeasible to rigorously examine various training tricks or variants, and it would be very helpful if experiences gained from small models could also apply to large models. For instance, small proxy models can be trained to find the optimal schedule of the data mixture for large models~Xie-arxiv-2023-doremi. Secondly, the training of large-scale models takes a long time, often suffering from issues such as training loss spike, and scaling law can be employed to monitor the training status of LLMs, \\eg identifying abnormal performance at an early time. Despite that scaling law characterizes a smooth trend of performance increase (or loss decrease), it also indicates that diminishing returnshttps://en.wikipedia.org/wiki/Diminishing\\_returns might occur as model scaling. % An empirical study~Henighan-2020-scalinglaw from the OpenAI team has shown that representation quality or semantic content can still effectively improve even if approaching the point of diminishing returns (\\ie approaching the irreducible loss)~Henighan-2020-scalinglaw. This finding suggests that training large models are promising for improving the performance of downstream tasks. To further explore scaling effect, a potential issue is that the amount of available data for training LLMs is actually limited. With the ever-increasing model scale, the public text data would be soon ``exhausted'' for LLMs~Villalobos-arXiv-2023-runout. Thus, it will be meaningful to study how scaling laws apply to a data-constrained regime~Muennighoff-arXiv-2023-dataconstrained, where data repetition or augmentation might be useful to alleviate data scarcity. } { $\\bullet$ Task-level predictability. Existing research of scaling laws are mostly conducted in terms of language modeling loss (\\eg per-token cross-entropy loss in nats~Kaplan-arxiv-2020-Scaling), while in practice we are more concerned about the performance of LLMs on actual tasks. Thus, a basic problem is that how the decrease of language modeling loss translates into the improvement of task performance~Henighan-2020-scalinglaw. Intuitively, a model with a smaller language modeling loss tends to yield a better performance on downstream tasks, since language modeling loss can be considered as a general measure of the overall model capacity. GPT-4~OpenAI-OpenAI-2023-GPT-4 has reported that some capabilities (\\eg coding ability) can be accurately predicted via scaling law. Despite that, readers should be aware that a direct decrease in language modeling loss does not always indicate an improvement of model performance on downstream tasks. Specially, the phenomenon of inverse scaling would occur for some tasks, where task performance surprisingly becomes worse as the language modeling loss decreases~McKenzie-2022-inverse. Overall, it is more difficult to explore and characterize task-level scaling laws, since it might be also dependent on task-related information (task metric, task difficulty, etc.). Furthermore, some capacities (\\eg in-context learning~Brown-NeurIPS-2020-Language) are unpredictable according to the scaling law, which can be observed only when the model size exceeds a certain level (as discussed below). } $\\bullet$ \\emph{Diminishing returns and capacity improvement. Diminishing returnshttps://en.wikipedia.org/wiki/Diminishing\\_returns are commonly discussed in economics, which refers that the incremental output decreases (after some point) as the input for one factor incrementally increases (\\ie a standard unit), by holding all the other factors fixed. This issue is also key to consider when training LLMs. As the training cost significantly grows for increasingly large models, can we obtain sufficient benefits with such huge models, or more specifically, would the phenomenon of diminishing returns occur when scaling up language models? To investigate this problem, a scaling law study~Henighan-2020-scalinglaw from the OpenAI team decomposes the language model loss into two parts, namely reducible loss (the entropy of the true data distribution) and irreducible loss (an estimate of the KL divergence between the true and model distributions). They empirically find that representation quality or semantic content can still improve even if approaching the point of diminishing returns (\\ie approaching the irreducible loss)~Henighan-2020-scalinglaw. This finding suggests that training very large models can be very promising for improving task performance. However, more research work is still required to explore the scaling limits and the trade-off between training cost and model capacity. In addition, some abilities (\\eg in-context learning~Brown-NeurIPS-2020-Language) are unpredictable according to the scaling law, which can be observed only when the model size exceeds a certain level (as discussed below). } Emergent Abilities of LLMs. In the literature~Wei-arxiv-2022-Emergent, emergent abilities of LLMs are formally defined as ``the abilities that are not present in small models but arise in large models'', which is one of the most prominent features that distinguish LLMs from previous PLMs. It further introduces a notable characteristic when emergent abilities occur~Wei-arxiv-2022-Emergent: performance rises significantly above random when the scale reaches a certain level. By analogy, such an emergent pattern has close connections with the phenomenon of phase transition in physics~Wei-arxiv-2022-Emergent,Huberman-AI-1987-phase. In principle, emergent abilities can be defined in relation to some complex tasks~Rae-arxiv-2021-Scaling,Wei-arxiv-2022-Emergent, while we are more concerned with general abilities that can be applied to solve a variety of tasks. Here, we briefly introduce three typical emergent abilities for LLMs and representative models that possess such an abilityIt is difficult to accurately examine the critical size for emergent abilities of LLMs (\\ie the minimum size to possess an ability), since it might vary for different models or tasks. Also, existing studies often test emergent abilities on very limited model sizes for a specific LLM. For example, PaLM is often tested with three sizes of 8B, 62B and 540B. It is unclear about the model performance of the untested sizes.. $\\bullet$ In-context learning. {The in-context learning~(ICL) ability is formally introduced by GPT-3~Brown-NeurIPS-2020-Language: assuming that the language model has been provided with a natural language instruction and/or several task demonstrations, it can generate the expected output for the test instances by completing the word sequence of input text, without requiring additional training or gradient updateIn a recent study~\\cite{Dai-arxiv-2022-Why, it also shows that in-context learning implicitly performs meta-optimization through the attention mechanism.}.} Among the GPT-series models, the 175B GPT-3 model exhibited a strong ICL ability in general, but not the GPT-1 and GPT-2 models. Such an ability also depends on the specific downstream task. For example, the ICL ability can emerge on the arithmetic tasks (\\eg the 3-digit addition and subtraction) for the 13B GPT-3, but 175B GPT-3 even cannot work well on the Persian QA task~Wei-arxiv-2022-Emergent. $\\bullet$ Instruction following. By fine-tuning with a mixture of multi-task datasets formatted via natural language descriptions (called instruction tuning), LLMs are shown to perform well on unseen tasks that are also described in the form of instructions~Ouyang-arxiv-2022-Training,Wei-ICLR-2022-Finetuned,Sanh-ICLR-2022-Multitask. {With instruction tuning, LLMs are enabled to follow the task instructions for new tasks without using explicit examples, thus having an improved generalization ability.} According to the experiments in~Wei-ICLR-2022-Finetuned, instruction-tuned LaMDA-PT~Thoppilan-CoRR-2022-LaMDA started to significantly outperform the untuned one on unseen tasks when the model size reached 68B, but not for 8B or smaller model sizes. A recent study~Chung-arxiv-2022-Scaling found that a model size of 62B is at least required for PaLM to perform well on various tasks in four evaluation benchmarks (\\ie MMLU, BBH, TyDiQA and MGSM), though a much smaller size might suffice for some specific tasks (\\eg MMLU). $\\bullet$ Step-by-step reasoning. For small language models, it is usually difficult to solve complex tasks that involve multiple reasoning steps, \\eg mathematical word problems. {In contrast, with the chain-of-thought~(CoT) prompting strategy~Wei-arxiv-2022-chain, LLMs can solve such tasks by utilizing the prompting mechanism that involves intermediate reasoning steps for deriving the final answer.} This ability is speculated to be potentially obtained by training on code~FU-blog-2022-how,Wei-arxiv-2022-chain. An empirical study~Wei-arxiv-2022-chain has shown that CoT prompting can bring performance gains (on arithmetic reasoning benchmarks) when applied to PaLM and LaMDA variants with a model size larger than 60B, while its advantage over the standard prompting becomes more evident when the model size exceeds 100B. Furthermore, the performance improvement with CoT prompting seems to be also varied for different tasks, \\eg GSM8K $>$ MAWPS $>$ SWAMP for PaLM~Wei-arxiv-2022-chain. { How Emergent Abilities Relate to Scaling Laws. In existing literature~Kaplan-arxiv-2020-Scaling,Hoffmann-arxiv-2022-Training,Wei-arxiv-2022-Emergent, scaling laws and emergent abilities provide two perspectives to understand the advantage of large models over small models. In general, scaling law (often measured by language modeling loss) describes predictable performance relation with the potential effect of diminishing returns, while emergent abilities (often measured by task performance) are unpredictable but very profitable once such abilities actually emerge. Since the two perspectives reflect different performance trends (continuous improvement v.s. sharp performance leap), they might lead to misaligned findings or observations. There are also extensive debates on the rationality of emergent abilities. A popular speculation is that emergent abilities might be partially attributed to the evaluation setting for special tasks (\\eg the discontinuous evaluation metrics)~Srivastava-arxiv-2022-Beyond,Schaeffer-arXiv-2023-mirage: when evaluation metrics are altered accordingly, the sharpness of the emergent ability curve would disappear. However, the performance of LLMs on most tasks are perceived by users naturally in a discontinuous way. For instance, end users prefer a reliable code generated by LLMs that can successfully pass the test case, but are less interested in selecting a better code with fewer errors between two failed ones. More recently, a study~Hu-arXiv-2023-unlock proposes a new evaluation setting that can enlarge the resolution of task metrics, making task performance more predictable. Despite these efforts, more fundamental research (\\eg grokkingGrokking refers that ``a pattern in the data, improving generalization performance from random chance level to perfect generalization'', quoted from the original paper~\\cite{Power-arxiv-2022-grokking.}) about the working mechanism of LLMs is still in need to understand the emergence of certain abilities. The subtle relation between scaling law and emergent abilities can be explained by analogy with the ability acquisition of humanThis explanation is only for ease of understanding, and there is not direct evidence to connect the two points. . Take the speaking ability as an example. For children, language development (especially infants) can be also considered as a multi-level process where ``emergent abilities'' occur. Specially, the language ability would relatively stable within a time interval, but qualitative change only occurs when evolving into another ability level (\\eg from speaking simple words to speaking simple sentences). Such a learning process is essentially not smooth and stable (\\ie language ability does not develop at a constant rate over time), though a child actually grows every day. It is interesting that young parents would be often surprised by unexpected progress of the speaking ability exhibited by their babies. } Key Techniques for LLMs. It has been a long way that LLMs evolve into the current state: general and capable learners. In the development process, a number of important techniques are proposed, which largely improve the capacity of LLMs. Here, we briefly list several important techniques that (potentially) lead to the success of LLMs, as follows. $\\bullet$ Scaling. As discussed in previous parts, there exists an evident scaling effect in Transformer language models: larger model/data sizes and more training compute typically lead to an improved model capacity~Kaplan-arxiv-2020-Scaling,Hoffmann-arxiv-2022-Training. As two representative models, GPT-3 and PaLM explored the scaling limits by increasing the model size to 175B and 540B, respectively. Since compute budget is usually limited, scaling laws can be further employed to conduct a more compute-efficient allocation of the compute resources. For example, Chinchilla (with more training tokens) outperforms its counterpart model Gopher (with a larger model size) by increasing the data scale with the same compute budget~Hoffmann-arxiv-2022-Training. In addition, data scaling should be with careful cleaning process, since the quality of pre-training data plays a key role in the model capacity. $\\bullet$ Training. Due to the huge model size, it is very challenging to successfully train a capable LLM. Distributed training algorithms are needed to learn the network parameters of LLMs, in which various parallel strategies are often jointly utilized. To support distributed training, several optimization frameworks have been released to facilitate the implementation and deployment of parallel algorithms, such as DeepSpeed~Rasley-KDD-2020-DeepSpeed and Megatron-LM~Shoeybi-arXiv-2019-Megatron, Narayanan-ACM-2021-Efficient, Korthikanti-arxiv-2022-reducing. Also, optimization tricks are also important for training stability and model performance, \\eg restart to overcome training loss spike~Chowdhery-arxiv-2022-PaLM and mixed precision training~Scao-arxiv-2022-BLOOM. More recently, GPT-4~OpenAI-OpenAI-2023-GPT-4 proposes to develop special infrastructure and optimization methods that reliably predict the performance of large models with much smaller models. $\\bullet$ Ability eliciting. After being pre-trained on large-scale corpora, LLMs are endowed with potential abilities as general-purpose task solvers. These abilities might not be explicitly exhibited when LLMs perform some specific tasks. As the technical approach, it is useful to design suitable task instructions or specific in-context learning strategies to elicit such abilities. For instance, chain-of-thought prompting has been shown to be useful to solve complex reasoning tasks by including intermediate reasoning steps. Furthermore, we can perform instruction tuning on LLMs with task descriptions expressed in natural language, for improving the generalizability of LLMs on unseen tasks. These eliciting techniques mainly correspond to the emergent abilities of LLMs, which may not show the same effect on small language models. $\\bullet$ Alignment tuning. Since LLMs are trained to capture the data characteristics of pre-training corpora (including both high-quality and low-quality data), they are likely to generate toxic, biased, or even harmful content for humans. It is necessary to align LLMs with human values, \\eg helpful, honest, and harmless. For this purpose, InstructGPT~Ouyang-arxiv-2022-Training designs an effective tuning approach that enables LLMs to follow the expected instructions, which utilizes the technique of reinforcement learning with human feedback~Christiano-NeurIPS-2017-Deep,Ouyang-arxiv-2022-Training. It incorporates human in the training loop with elaborately designed labeling strategies. ChatGPT is indeed developed on a similar technique to InstructGPT, which shows a strong alignment capacity in producing high-quality, harmless responses, \\eg rejecting to answer insulting questions. $\\bullet$ Tools manipulation. In essence, LLMs are trained as text generators over massive plain text corpora, thus performing less well on the tasks that are not best expressed in the form of text (\\eg numerical computation). In addition, their capacities are also limited to the pre-training data, \\eg the inability to capture up-to-date information. To tackle these issues, a recently proposed technique is to employ external tools to compensate for the deficiencies of LLMs~Schick-arxiv-2023-Toolformer,Nakano-arxiv-2021-WebGPT. For example, LLMs can utilize the calculator for accurate computation~Schick-arxiv-2023-Toolformer and employ search engines to retrieve unknown information~Nakano-arxiv-2021-WebGPT. More recently, ChatGPT has enabled the mechanism of using external plugins (existing or newly created apps)https://openai.com/blog/chatgpt-plugins, which are by analogy with the ``eyes and ears'' of LLMs. Such a mechanism can broadly expand the scope of capacities for LLMs. In addition, many other factors (\\eg the upgrade of hardware) also contribute to the success of LLMs. Currently, we limit our discussion to the major technical approaches and key findings for developing LLMs. figure* \\centering \\includegraphics[width=\\textwidth]{images/Fig2-updated-2025-3-11.pdf} A timeline of existing large language models (having a size larger than 10B) in recent years. The timeline was established mainly according to the release date (\\eg the submission date to arXiv) of the technical paper for a model. If there was no corresponding paper, we set the date of a model as the earliest time of its public release or announcement. We mark the LLMs with publicly available model checkpoints in yellow color. Due to the space limit of the figure, we only include the LLMs with publicly reported evaluation results. figure* table*[htbp] \\centering Statistics of large language models (having a size larger than 10B in this survey) in recent years, including the capacity evaluation, pre-training data scale (either in the number of tokens or storage size) and hardware resource costs. In this table, we only include LLMs with a public paper about the technical details. Here, ``Release Time\" indicates the date when the corresponding paper was officially released. ``Publicly Available\" means that the model checkpoints can be publicly accessible while ``Closed Source\" means the opposite. ``Adaptation'' indicates whether the model has been with subsequent fine-tuning: IT denotes instruction tuning and RLHF denotes reinforcement learning with human feedback. ``Evaluation'' indicates whether the model has been evaluated with corresponding abilities in their original paper: ICL denotes in-context learning and CoT denotes chain-of-thought. ``*'' denotes the largest publicly available version. \\footnotesize \\renewcommand2.5pt tabular{llcrccccccccc} \\toprule & & & 1{c}{} & & 2{c}{Adaptation} & & & & & 2{c}{Evaluation} \\\\ -2{*}{} & -2{*}{Model} & -2{*}{\\begin{tabular[c]{@{}c@{}}Release\\\\ Timetabular}} & 1{c}{-2{*}{\\begin{tabular[c]{@{}c@{}}Size\\\\ (B)tabular}}} & -2{*}{\\begin{tabular[c]{@{}c@{}}Base\\\\ Modeltabular}} & IT & RLHF & -2{*}{\\begin{tabular[c]{@{}c@{}}Pre-train\\\\ Data Scaletabular}} & -2{*}{\\begin{tabular[c]{@{}c@{}}Latest Data\\\\ Timestamptabular}} & -2{*}{\\begin{tabular[c]{@{}c@{}}Hardware\\\\ (GPUs / TPUs)tabular}} & -2{*}{\\begin{tabular[c]{@{}c@{}}Training\\\\ Timetabular}} & ICL & CoT \\\\ \\midrule & T5~Raffel-JMLR-2020-Exploring & Oct-2019 & 11 & - & - & - & {1T tokens} & Apr-2019 & 1024 TPU v3 & - & $\\checkmark$ & - \\\\ & mT5~Xue-NAACL-2021-mT5 & Oct-2020 & 13 & - & - & - & 1T tokens & - & - & - & $\\checkmark$ & - \\\\ & {PanGu-$\\alpha$}~Zeng-arxiv-2021-PanGualpha & Apr-2021 & 13* & - & - & - & 1.1TB & - & 2048 Ascend 910 & - & $\\checkmark$ & - \\\\ & CPM-2~Zhang-arXiv-2021-CPM-2 & Jun-2021 & 198 & - & - & - & {2.6TB} & - & - & - & - & - \\\\ & T0~Sanh-ICLR-2022-Multitask & Oct-2021 & 11 & T5 & $\\checkmark$ & - & - & - & 512 TPU v3 & 27 h & $\\checkmark$ & - \\\\ & CodeGen~nijkamp-arxiv-2022-Codegen & Mar-2022 & 16 & - & - & - & 577B tokens & - & - & - & $\\checkmark$ & - \\\\ & GPT-NeoX-20B~Black-CoRR-2022-GPT & Apr-2022 & 20 & - & - & - & 825GB & - & 96 40G A100 & - & $\\checkmark$ & - \\\\ & Tk-Instruct~Wang-EMNLP-2022-Super & Apr-2022 & 11 & T5 & $\\checkmark$ & - & - & - & 256 TPU v3 & 4 h & $\\checkmark$ & - \\\\ & UL2~Tay-arxiv-2022-UL2 & May-2022 & 20 & - & - & - & 1T tokens & Apr-2019 & 512 TPU v4 & - & $\\checkmark$ & $\\checkmark$ \\\\ & OPT~Zhang-arxiv-2022-OPT & May-2022 & 175 & - & - & - & 180B tokens & - & 992 80G A100 & - & $\\checkmark$ & - \\\\ & NLLB~Marta-arxiv-2022-NLLB & Jul-2022 & 54.5 & - & - & - & - & - & - & - & $\\checkmark$ & - \\\\ & CodeGeeX~Zheng-arXiv-2023-CodeGeex & Sep-2022 & 13 & - & - & - & 850B tokens & - & 1536 Ascend 910 & 60 d & $\\checkmark$ & - \\\\ & GLM~Zeng-arxiv-2022-GLM & Oct-2022 & 130 & - & - & - & 400B tokens & - & 768 40G A100 & 60 d & $\\checkmark$ & - \\\\ & Flan-T5~Chung-arxiv-2022-Scaling & Oct-2022 & 11 & T5 & $\\checkmark$ & - & - & - & - & - & $\\checkmark$ & $\\checkmark$ \\\\ & BLOOM~Scao-arxiv-2022-BLOOM & Nov-2022 & 176 & - & - & - & 366B tokens & - & 384 80G A100 & 105 d & $\\checkmark$ & - \\\\ & mT0~Muennighoff-2022-arxiv-Crosslingual & Nov-2022 & 13 & mT5 & $\\checkmark$ & - & - & - & - & - & $\\checkmark$ & - \\\\ & Galactica~Taylor-arxiv-2022-Galactica & Nov-2022 & 120 & - & - & - & 106B tokens & - & - & - & $\\checkmark$ & $\\checkmark$ \\\\ & BLOOMZ~Muennighoff-2022-arxiv-Crosslingual & Nov-2022 & 176 & BLOOM & $\\checkmark$ & - & - & - & - & - & $\\checkmark$ & - \\\\ & OPT-IML~Iyer-arxiv-2022-OPT & Dec-2022 & 175 & OPT & $\\checkmark$ & - & - & - & 128 40G A100 & - & $\\checkmark$ & $\\checkmark$ \\\\ & LLaMA~Touvron-arxiv-2023-LLaMA & Feb-2023 & 65 & - & - & - & 1.4T tokens & - & 2048 80G A100 & 21 d & $\\checkmark$ & - \\\\ & Pythia~Biderman-arxiv-2023-Pythia & Apr-2023 & 12 & - & - & - & 300B tokens & - & 256 40G A100 & - & $\\checkmark$ & - \\\\ & CodeGen2~Nijkamp-2023-codegen2-arxiv & May-2023 & 16 & - & - & - & 400B tokens & - & - & - & $\\checkmark$ & - \\\\ & StarCoder~Li-2023-arxiv-Starcoder & May-2023 & 15.5 & - & - & - & 1T tokens & - & 512 40G A100 & - & $\\checkmark$ & $\\checkmark$ \\\\ & LLaMA2~Touvron-2023-llama2-arxiv & Jul-2023 & 70 & - & $\\checkmark$ & $\\checkmark$ & 2T tokens & - & 2000 80G A100 & - & $\\checkmark$ & - \\\\ & Baichuan2~yang-2023-baichuan2 & Sep-2023 & 13 & - & $\\checkmark$ & $\\checkmark$ & 2.6T tokens & - & 1024 A800 & - & $\\checkmark$ & - \\\\ & QWEN~bai-2023-qwen & Sep-2023 & 14 & - & $\\checkmark$ & $\\checkmark$ & 3T tokens & - & - & - & $\\checkmark$ & - \\\\ & FLM~Li-arxiv-2023-FLM & Sep-2023 & 101 & - & $\\checkmark$ & - & 311B tokens & - & 192 A800 & 22 d & $\\checkmark$ & - \\\\ -18{*}{tabular[c]{@{}c@{}}Publicly\\\\ Availabletabular} & Skywork~wei-2023-skywork & Oct-2023 & 13 & - & - & - & 3.2T tokens & - & 512 80G A800 & - & $\\checkmark$ & - \\\\ \\midrule \\midrule & GPT-3~Brown-NeurIPS-2020-Language & May-2020 & 175 & - & - & - & {300B tokens} & - & - & - & $\\checkmark$ & - \\\\ & GShard~Lepikhin-ILR-2021-GShard & Jun-2020 & 600 & - & - & - & 1T tokens & - & 2048 TPU v3 & 4 d & - & - \\\\ & Codex~Chen-arxiv-2021-evaluating & Jul-2021 & 12 & GPT-3 & - & - & 100B tokens & May-2020 & - & - & $\\checkmark$ & - \\\\ & ERNIE 3.0~Sun-arXiv-2021-ERNIE3.0 & Jul-2021 & 10 & - & - & - & 375B tokens & - & 384 V100 & - & $\\checkmark$ & - \\\\ & Jurassic-1~lieber-2021-jurassic & Aug-2021 & 178 & - & - & - & 300B tokens & - & 800 GPU & - & $\\checkmark$ & - \\\\ & HyperCLOVA~Kim-EMNLP-2021-HyperCLOVA & Sep-2021 & 82 & - & - & - & 300B tokens & - & 1024 A100 & 13.4 d & $\\checkmark$ & - \\\\ & FLAN~Wei-ICLR-2022-Finetuned & Sep-2021 & 137 & LaMDA-PT & $\\checkmark$ & - & - & - & 128 TPU v3 & 60 h & $\\checkmark$ & - \\\\ & Yuan 1.0~Wu-arxiv-2021-Yuan & Oct-2021 & 245 & - & - & - & 180B tokens & - & 2128 GPU & - & $\\checkmark$ & - \\\\ & Anthropic~Askell-arxiv-2021-Anthropic & Dec-2021 & 52 & - & - & - & {400B tokens} & - & - & - & $\\checkmark$ & - \\\\ & WebGPT~Nakano-arxiv-2021-WebGPT & Dec-2021 & 175 & GPT-3 & - & $\\checkmark$ & - & - & - & - & $\\checkmark$ & - \\\\ & Gopher~Rae-arxiv-2021-Scaling & Dec-2021 & 280 & - & - & - & 300B tokens & - & 4096 TPU v3 & 920 h & $\\checkmark$ & - \\\\ & ERNIE 3.0 Titan~Wang-arxiv-2021-ERNIE & Dec-2021 & 260 & - & - & - & - & - & - & - & $\\checkmark$ & - \\\\ & GLaM~Du-ICML-2022-GLaM & Dec-2021 & 1200 & - & - & - & 280B tokens & - & 1024 TPU v4 & 574 h & $\\checkmark$ & - \\\\ & LaMDA~Thoppilan-CoRR-2022-LaMDA & Jan-2022 & 137 & - & - & - & 768B tokens & - & 1024 TPU v3 & 57.7 d & - & - \\\\ & MT-NLG~Smith-CoRR-2022-Using & Jan-2022 & 530 & - & - & - & {270B tokens} & - & 4480 80G A100 & - & $\\checkmark$ & - \\\\ & AlphaCode~Li-Science-2022-AlphaCode & Feb-2022 & 41 & - & - & - & 967B tokens & Jul-2021 & - & - & - & - \\\\ & InstructGPT~Ouyang-arxiv-2022-Training & Mar-2022 & 175 & GPT-3 & $\\checkmark$ & $\\checkmark$ & - & - & - & - & $\\checkmark$ & - \\\\ & Chinchilla~Hoffmann-arxiv-2022-Training & Mar-2022 & 70 & - & - & - & 1.4T tokens & - & - & - & $\\checkmark$ & - \\\\ & PaLM~Chowdhery-arxiv-2022-PaLM & Apr-2022 & 540 & - & - & - & 780B tokens & - & 6144 TPU v4 & - & $\\checkmark$ & $\\checkmark$ \\\\ & AlexaTM~Soltan-arxiv-2022-AlexaTM20B & Aug-2022 & 20 & - & - & - & 1.3T tokens & - & 128 A100 & 120 d & $\\checkmark$ & $\\checkmark$ \\\\ & Sparrow~Glaese-arxiv-2022-Improving & Sep-2022 & 70 & - & - & $\\checkmark$ & - & - & 64 TPU v3 & - & $\\checkmark$ & - \\\\ & WeLM~Su-arxiv-2022-WeLM & Sep-2022 & 10 & - & - & - & 300B tokens & - & 128 A100 40G & 24 d & $\\checkmark$ & - \\\\ & U-PaLM~Tay-arxiv-2022-Transcending & Oct-2022 & 540 & PaLM & - & - & - & - & 512 TPU v4 & 5 d & $\\checkmark$ & $\\checkmark$ \\\\ & Flan-PaLM~Chung-arxiv-2022-Scaling & Oct-2022 & 540 & PaLM & $\\checkmark$ & - & - & - & 512 TPU v4 & 37 h & $\\checkmark$ & $\\checkmark$ \\\\ & Flan-U-PaLM~Chung-arxiv-2022-Scaling & Oct-2022 & 540 & U-PaLM & $\\checkmark$ & - & - & - & - & - & $\\checkmark$ & $\\checkmark$ \\\\ & GPT-4~OpenAI-OpenAI-2023-GPT-4 & Mar-2023 & - & - & $\\checkmark$ & $\\checkmark$ & - & - & - & - & $\\checkmark$ & $\\checkmark$ \\\\ & {PanGu-$\\Sigma$}~Ren-arXiv-2023-PanGusigma & Mar-2023 & 1085 & {PanGu-$\\alpha$} & - & - & 329B tokens & - & 512 Ascend 910 & 100 d & $\\checkmark$ & - \\\\ -25{*}{tabular[c]{@{}c@{}}Closed\\\\ Sourcetabular} & PaLM2~Anil-arxiv-2023-palm2 & May-2023 & 16 & - & $\\checkmark$ & - & 100B tokens & - & - & - & $\\checkmark$ & $\\checkmark$ \\\\ \\bottomrule tabular table* figure*[h] \\centering \\includegraphics[width=\\textwidth]{images/openai-v2.pdf} A brief illustration for the technical evolution of GPT-series models. We plot this figure mainly based on the papers, blog articles and official APIs from OpenAI. Here, \\emph{solid lines denote that there exists an explicit evidence (\\eg the official statement that a new model is developed based on a base model) on the evolution path between two models, while dashed lines denote a relatively weaker evolution relation. } figure*",
      "origin_cites_number": 115
    },
    {
      "section_title": "Technical Evolution  of GPT-series Models",
      "level": "2",
      "content": "Due to the excellent capacity in communicating with humans, ChatGPT has ignited the excitement of the AI community since its release. ChatGPT is developed based on the powerful GPT model with specially optimized conversation capacities. Considering the ever-growing interest in ChatGPT and GPT models, we add a special discussion about the technical evolution of the GPT-series models, to briefly summarize the progress how they have been developed in the past years. % {Meanwhile, we drew a schematic diagram depicting the technological evolution of the GPT-series models in Figure~fig:openai.} The basic principle underlying GPT models is to compress the world knowledge into the decoder-only Transformer model by language modeling, such that it can recover (or memorize) the semantics of world knowledge and serve as a general-purpose task solver. Two key points to the success are (I) training decoder-only Transformer language models that can accurately predict the next word and (II) scaling up the size of language models. Overall, the research of OpenAI on LLMs can be roughly divided into the following stagesNote that the discussion of this part can be somewhat subjective. The overall viewpoints and summaries are made based on the understanding of the survey authors by reading the papers, blog articles, interview reports and APIs released by OpenAI. . Early Explorations. According to one interview with Ilya Sutskever\\url{https://hackernoon.com/an-interview-with-ilya-sutskever-co-founder-of-openai} (a co-founder and chief scientist of OpenAI), the idea of approaching intelligent systems with language models was already explored in the early days of OpenAI, while it was attempted with recurrent neural networks~(RNN)~Radford-CoRR-2017-Learning. With the advent of Transformer, OpenAI developed two initial GPT models, namely GPT-1~radford-openai-2018-improving and GPT-2~radford-blog-2019-language, which can be considered as the foundation to more powerful models subsequently \\ie GPT-3 and GPT-4. $\\bullet$ GPT-1. In 2017, the Transformer model~Vaswani-NIPS-2017-Attention was introduced by Google, and the OpenAI team quickly adapted their language modeling work to this new neural network architecture. They released the first GPT model in 2018, \\ie GPT-1~radford-openai-2018-improving, and coined the abbreviation term GPT as the model name, standing for Generative Pre-Training. GPT-1 was developed based on a generative, decoder-only Transformer architecture, and adopted a hybrid approach of unsupervised pre-training and supervised fine-tuning. GPT-1 has set up the core architecture for the GPT-series models and established the underlying principle to model natural language text, \\ie predicting the next word. $\\bullet$ GPT-2. Following a similar architecture of GPT-1, GPT-2~radford-blog-2019-language increased the parameter scale to 1.5B, which was trained with a large webpage dataset WebText. As claimed in the paper of GPT-2, it sought to perform tasks via unsupervised language modeling, without explicit fine-tuning using labeled data. To motivate the approach, they introduced a probabilistic form for multi-task solving, \\ie $p(output|input, task)$ (similar approaches have been adopted in McCann-CoRR-2018-The), which predicts the output conditioned on the input and task information. To model this conditional probability, language text can be naturally employed as a unified way to format input, output and task information. In this way, the process of solving a task can be cast as a word prediction problem for generating the solution text. Further, they introduced a more formal claim for this idea: ``Since the (task-specific) supervised objective is the same as the unsupervised (language modeling) objective but only evaluated on a subset of the sequence, the global minimum of the unsupervised objective is also the global minimum of the supervised objective (for various tasks)''~radford-blog-2019-languageTo better understand this sentence, we put some explanation words in parentheses.. A basic understanding of this claim is that each (NLP) task can be considered as the word prediction problem based on a subset of the world text. Thus, unsupervised language modeling could be capable in solving various tasks, if it was trained to have sufficient capacity in recovering the world text. These early discussion in GPT-2's paper echoed in the interview of Ilya Sutskever by Jensen Huang: ``What the neural network learns is some representation of the process that produced the text. This text is actually a projection of the world...the more accurate you are in predicting the next word, the higher the fidelity, the more resolution you get in this process...''\\url{https://lifearchitect.ai/ilya/}. Capacity Leap. Although GPT-2 is intended to be an ``unsupervised multitask learner'', it overall has an inferior performance compared with supervised fine-tuning state-of-the-art methods. Because it has a relatively small model size, it has been widely fine-tuned in downstream tasks, especially the dialog tasks~Zhang-ACL-2020-DIALOGPT,Ham-ACL-2020-End. Based on GPT-2, GPT-3 demonstrates a key capacity leap by scaling of the (nearly same) generative pre-training architecture. $\\bullet$ GPT-3. GPT-3~Brown-NeurIPS-2020-Language was released in 2020, which scaled the model parameters to an ever larger size of 175B. In the GPT-3's paper, it formally introduced the concept of in-context learning~(ICL)GPT-2 essentially used ICL for unsupervised task learning, though it wasn't called ICL at that time. , which utilizes LLMs in a few-shot or zero-shot way. ICL can teach (or instruct) LLMs to understand the tasks in the form of natural language text. With ICL, the pre-training and utilization of LLMs converge to the same language modeling paradigm: pre-training predicts the following text sequence conditioned on the context, while ICL predicts the correct task solution, which can be also formatted as a text sequence, given the task description and demonstrations. GPT-3 not only demonstrates very excellent performance in a variety of NLP tasks, but also on a number of specially designed tasks that require the abilities of reasoning or domain adaptation. Although the GPT-3's paper does not explicitly discuss the emergent abilities of LLMs, we can observe large performance leap that might transcend the basic scaling law~Kaplan-arxiv-2020-Scaling, \\eg larger models have significantly stronger ICL ability (illustrated in the original Figure~1.2 of the GPT-3's paper~Brown-NeurIPS-2020-Language). Overall, GPT-3 can be viewed as a remarkable landmark in the journey evolving from PLMs to LLMs. It has empirically proved that scaling the neural networks to a significant size can lead to a huge increase in model capacity. Capacity Enhancement. Due to the strong capacities, GPT-3 has been the base model to develop even more capable LLMs for OpenAI. Overall, OpenAI has explored two major approaches to further improving the GPT-3 model, \\ie training on code data and alignment with human preference, which are detailed as follows. $\\bullet$ Training on code data. A major limitation of the original GPT-3 model (pre-trained on plain text) lies in the lack of the reasoning ability on complex tasks, \\eg completing the code and solving math problems. To enhance this ability, Codex~Chen-arxiv-2021-evaluating was introduced by OpenAI in July 2021, which was a GPT model fine-tuned on a large corpus of GitHub code. It demonstrated that Codex can solve very difficult programming problems, and also lead to a significant performance improvement in solving math problems~Drori-CoRR-2021-A. Further, a contrastive approach~Neelakantan-CoRR-2022-Text to training text and code embedding was reported in January 2022, which was shown to improve a series of related tasks (\\ie linear-probe classification, text search and code search). Actually, the GPT-3.5 models are developed based on a code-based GPT model (\\ie code-davinci-002), which indicates that training on code data is a very useful practice to improve the model capacity of GPT models, especially the reasoning ability. Furthermore, there is also a speculation that training on code data can greatly increase the chain-of-thought prompting abilities of LLMs~FU-blog-2022-how, while it is still worth further investigation with more thorough verification. $\\bullet$ Human alignment. The related research of human alignment can be dated back to the year 2017 (or earlier) for OpenAI: a blog article entitled ``learning from human preferences''\\url{https://openai.com/research/learning-from-human-preferences} was posted on the OpenAI blog describing a work that applied reinforcement learning~(RL) to learn from the preference comparisons annotated by humans~Christiano-NeurIPS-2017-Deep (similar to the reward training step in the aligning algorithm of InstructGPT in Figure~fig:RLHF). Shortly after the release of this RL paper~Christiano-NeurIPS-2017-Deep, the paper of the Proximal Policy Optimization~(PPO)~schulman-arxiv-2017-proximal was published in July 2017, which now has been the foundational RL algorithm for learning from human preferences~Ouyang-arxiv-2022-Training. Later in January 2020, GPT-2 was fine-tuned using the aforementioned RL algorithms~Christiano-NeurIPS-2017-Deep,schulman-arxiv-2017-proximal, which leveraged human preferences to improve the capacities of GPT-2 on NLP tasks. In the same year, another work~Stiennon-arxiv-2020-learning trained a summarization model for optimizing human preferences in a similar way. Based on these prior work, InstructGPT~Ouyang-arxiv-2022-Training was proposed in January 2022 to improve the GPT-3 model for human alignment, which formally established a three-stage reinforcement learning from human feedback~(RLHF) algorithm. Note that it seems that the wording of ``instruction tuning'' has seldom been used in OpenAI's paper and documentation, which is substituted by supervised fine-tuning on human demonstrations (\\ie the first step of the RLHF algorithm~Ouyang-arxiv-2022-Training). In addition to improving the instruction following capacity, the RLHF algorithm is particularly useful to mitigate the issues of generating harm or toxic content for LLMs, which is key to the safe deployment of LLMs in practice. OpenAI describes their approach to alignment research in a technical article~OpenAI-blog-2022-alignment, which has summarized three promising directions: ``training AI systems to use human feedback, to assist human evaluation and to do alignment research''. These enhancement techniques lead to the improved GPT-3 models with stronger capacities, which are called GPT-3.5 models by OpenAI (see the discussion about the OpenAI API in Section~subsec-models-apis). \\begin{figure*[h] \\centering \\includegraphics[width=1\\textwidth]{images/LLM pipeline.pdf} A brief summary of important aspects and techniques to develop and use large language models. figure* } The Milestones of Language Models. Based on all the exploration efforts, two major milestones have been achieved by OpenAI, namely ChatGPT~OpenAI-blog-2022-ChatGPT and GPT-4~OpenAI-OpenAI-2023-GPT-4, which have largely raised the capacity bar of existing AI systems. $\\bullet$ ChatGPT. In November 2022, OpenAI released the conversation model ChatGPT, based on the GPT models (GPT-3.5 and GPT-4). As the official blog article introduced~OpenAI-blog-2022-ChatGPT, ChatGPT was trained in a similar way as InstructGPT (called ``a sibling model to InstructGPT'' in the original post), while specially optimized for dialogue. They reported a difference between the training of ChatGPT and InstructGPT in the data collection setup: human-generated conversations (playing both the roles of user and AI) are combined with the InstructGPT dataset in a dialogue format for training ChatGPT. ChatGPT exhibited superior capacities in communicating with humans: possessing a vast store of knowledge, skill at reasoning on mathematical problems, tracing the context accurately in multi-turn dialogues, and aligning well with human values for safe use. Later on, the plugin mechanism has been supported in ChatGPT, which further extends the capacities of ChatGPT with existing tools or apps. So far, it seems to be the ever most powerful chatbot in the AI history. The launch of ChatGPT has a significant impact on the AI research in the future, which sheds light on the exploration of human-like AI systems. $\\bullet$ GPT-4. As another remarkable progress, GPT-4~OpenAI-OpenAI-2023-GPT-4 was released in March 2023, which extended the text input to multimodal signals. Overall, GPT-4 has stronger capacities in solving complex tasks than GPT-3.5, showing a large performance improvement on many evaluation tasks. A recent study~Bubeck-arxiv-2023-Sparks investigated the capacities of GPT-4 by conducting qualitative tests with human-generated problems, spanning a diverse range of difficult tasks, and showed that GPT-4 can achieve more superior performance than prior GPT models. Furthermore, GPT-4 responds more safely to malicious or provocative queries, due to a six-month iterative alignment (with an additional safety reward signal in the RLHF training). In the technical report, OpenAI has emphasized how to safely develop GPT-4 and applied a number of intervention strategies to mitigate the possible issues of LLMs, such as hallucinations, privacy and overreliance. For example, they introduced the mechanism called red teaming~Ganguli-arxiv-2022-Red to reduce the harm or toxic content generation. As another important aspect, GPT-4 has been developed on a well-established deep learning infrastructure with improved optimization methods. They introduced a new mechanism called predictable scaling that can accurately predict the final performance with a small proportion of compute during model training. { $\\bullet$ GPT-4V, GPT-4 turbo, and beyond. Based on the work done for GPT-4~OpenAI-OpenAI-2023-GPT-4, OpenAI further released GPT-4V in September 2023, which focused on the safe deployment of the vision capabilities of GPT-4. In the GPT-4V's system card~OpenAI-OpenAI-2023-GPT-4v, it has extensively discussed the assessment and mitigation of risks related to visually augmented inputs. Specially, GPT-4V exhibited strong vision capacities in various application scenarios, showing the great potential as a powerful multimodal learning system. More recently, in November 2023, OpenAI released an upgraded generation of GPT-4 model at DevDay, named GPT-4 Turbo, with a series of technical improvements. GPT-4 Turbo is featured by the improved model capacity (more capable than GPT-4), the extended knowledge source (up to April 2023), long context window (up to 128k tokens), optimized model performance (cheaper price), and other useful functionality updates (function call, reproducible outputs, etc.). At the same time, Assistants API was launched to ease the rapid development of agent-like assistants. With this API, developers can easily create goal-oriented assistants within their applications, by leveraging specific instruction, extra knowledge and tool use. Furthermore, multimodal capacities (see, hear, and speak) were also enhanced in this new release, supported by GPT-4 Turbo with vision, DALLÂ·E 3, Text-to-speech~(TTS), and Listen to voice samples. These improvements have greatly extended the capacity scope and enhanced the task performance of GPT models. % More importantly, the application ecosystem will be greatly strengthened with the technology upgrade in improved models, APIs, and functionalities. } Despite the huge progress, there are still limitations with these superior LLMs, \\eg generating hallucinations with factual errors or potentially risky response within some specific context~OpenAI-OpenAI-2023-GPT-4. More limitations or issues of LLMs will be discussed in Section~sec-evaluation. It poses long-standing research challenges to develop more capable, safer LLMs. From the perspective of engineering, OpenAI has adopted an iterative deployment strategy~OpenAI-blog-2022-lessons to develop the models and products by following a five-stage development and deployment life-cycle, which aims to effectively reduce the potential risks of using the models. In the following, we will dive into the technical details in order to have a specific understanding of how they have been developed. figure* \\centering \\includegraphics[width=\\textwidth]{images/llama-0628-final.pdf} An evolutionary graph of the research work conducted on LLaMA. Due to the huge number, we cannot include all the LLaMA variants in this figure, even much excellent work. To support incremental update, we share the source file of this figure, and welcome the readers to include the desired models by submitting the pull requests on our GitHub page. figure*",
      "origin_cites_number": 35
    },
    {
      "section_title": "Resources of LLMs",
      "level": "1",
      "content": "It is by no means an easy job to develop or reproduce LLMs, considering the challenging technical issues and huge demands of computation resources. A feasible way is to learn experiences from existing LLMs and reuse publicly available resources for incremental development or experimental study. In this section, we briefly summarize the publicly available resources for developing LLMs, including model checkpoints (or APIs), corpora and libraries.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Publicly Available Model Checkpoints or APIs",
      "level": "2",
      "content": "Given the huge cost of model pre-training, well-trained model checkpoints are critical to the study and development of LLMs for the research community. Due to space limitation, we can only selectively discuss several representative LLMs. In addition, for inference, we can directly employ public APIs to perform our tasks, without running the model {locally}. Next, we introduce the publicly available model checkpoints and APIs. Publicly Available Model Checkpoints. To assist researchers in selecting a suitable model based on the resource budget and usage needs, we focus on discussing the model's parameter size, data and computational resources required for training, the relevant technologies employed by the model, and its performance evaluation in downstream tasks. For more details of LLMs, see Table ~tab:resource_model. { $\\bullet$ LLaMA. The LLaMA series of models has gained immense popularity and widespread attention due to its openness and effectiveness. From LLaMA~Touvron-arxiv-2023-LLaMA, LLaMA-2~Touvron-2023-llama2-arxiv, LLaMA-3~LLaMa3 to LLaMA-3.1~LLaMA-3.1, continuous updates have been made and the development is still ongoing. With increased parameters (the largest version has 405B), more pre-training tokens (15T tokens), and an extended context window (128K), LLaMA-3.1 has significantly enhanced its capabilities, and it also integrates additional components that work in synergy with the model, including new security and safety tools. In evaluation, LLaMa-3.1~(405B version) achieves competitive performance against prominent closed-source LLMs, such as GPT-4, GPT-4o, and Claude 3.5 Sonnet in various benchmarks~(\\eg MMLU, GSM8k, and HumanEval). The pre-training of LLaMA~(65B version) involves 2,048 A100-80G GPUs, whereas LLaMA-3.1~(405B version) involves more than 16,000 H100 GPUs. } $\\bullet$ Mistral. The Mistral series~jiang-2023-arxiv-mistral, Jiang-2024-arxiv-Mixtral consist of Mistral~(7B), Mistral NeMo~(12B), Mistral Large 2~(123B), and Mixtral~(8$\\times$7B and 8$\\times$22B), which have been widely known for their strong performance on various mainstream benchmarks~(\\eg MMLU and GSM8k). Mistral NeMo is featured with a long context window of 128K at the parameter scale of 12B. Although Mistral NeMo is trained with quantization awareness, it enables FP8 inference without sacrificing performance. Mistral Large 2 is the largest and most powerful model of the Mistral series, which supports 11 natural languages and more than 80 programming languages. Mixtral is a kind of sparse Mixture-of-Experts (SMoE) model that activates only part of the parameters during inference, making it more efficient compared to dense models of the same size. { $\\bullet$ Gemma. Gemma~Mesnard-2024-arxiv-Gemma, Morgane-2024-arxiv-Gemma2 is a series of lightweight, strong, and open models, consisting of Gemma-1~(2B and 7B) and Gemma-2~(2B, 9B, and 27B). During the pre-training stage, Gemma-2 2B, 9B, and 27B versions are trained on 2T, 8T, and 13T primarily English tokens, respectively. The largest version of Gemma-2 is trained on 6144 TPUv5p chips. Gemma-2 has achieved excellent performance in multiple benchmarks~(\\eg ARC-c, MMLU, and GSM8k). } { $\\bullet$ Qwen. Qwen~qwen2, qwen2.5 is an open-source large model series consisting of Qwen~(raging from 7B to 72B), Qwen1.5~(raging from 0.5B to 110B), Qwen2~(ranging from 0.5B to 72B), and Qwen2.5~(ranging from 0.5B to 72B). Qwen2.5 is the newest LLM collection of Qwen, which is pre-trained on up to 18T tokens. Compared to Qwen2, Qwen2.5 demonstrates a significant increase in knowledge retention, as well as notable advancements in coding and mathematical abilities. Qwen2.5 has also shown large improvements in instruction following, long texts generation (over 8K tokens), structured data understanding and generation (\\eg JSON). } { $\\bullet$ GLM. GLM~glm-2024-arxiv-ChatGLM is a series of LLMs featuring comprehensive capabilities in both English and Chinese. GLM has been upgraded to its fourth-generation model, GLM-4, with a parameter scale of up to 9B, possesses excellent conversational abilities. It has achieved excellent performance in evaluations from multiple perspectives including semantics, mathematics, reasoning, code, and knowledge. In addition to the base model GLM-4-9B, it has open-sourced human preference-aligned model GLM-4-9B-Chat, and long context conversational model GLM-4-9B-Chat-1M. } {$\\bullet$ Baichuan.} Baichuan is a series of open-source bilingual LLMs and the latest version is Baichuan-2. Both Baichuan and Baichuan-2 have two available parameter sizes~(7B and 13B). Baichuan supports both Chinese and English, with pre-training data reaching 1.2 trillion tokens. Furthermore, Baichuan-2 expands its pre-training data to 2.6 trillion tokens. Baichuan-2 surpasses Baichuan in all evaluation benchmarks, demonstrating excellent multilingual capabilities and showing potential for vertical applications in the domains such as law and healthcare~(\\eg JEC-QA~Zhong-2024-AAAI-JECQA and MedQA~Jin-2021-MedQA). \\paratitle{Models with Hundreds of Billions of Parameters. For models in this category, only a handful of models have been publicly released. For example, OPT~Zhang-arxiv-2022-OPT, OPT-IML~Iyer-arxiv-2022-OPT, BLOOM~Scao-arxiv-2022-BLOOM, and BLOOMZ~Muennighoff-2022-arxiv-Crosslingual have nearly the same number of parameters as GPT-3~(175B version), while GLM~Zeng-arxiv-2022-GLM and Galactica~Taylor-arxiv-2022-Galactica have 130B and 120B parameters, respectively. blue{ Recently, Falcon and LLaMA-3.1 have publicly unveiled expanded versions, boasting 180 billion and 405 billion parameters respectively. (check the grammar, I don't think they should be mentioned together) } {Among them, OPT~(175B version), with the instruction-tuned version OPT-IML, has been specially motivated for open sharing, which aims to enable researchers to carry out reproducible research at scale. } For research in cross-lingual generalization, BLOOM~(176B version) and BLOOMZ~(176B version) can be used as base models, due to the competence in multilingual language modeling tasks. As a bilingual LLM, GLM has also provided a popular small-sized Chinese chat model ChatGLM2-6B (a updated version for ChatGLM-6B), which is featured with many improvements in efficiency and capacity (\\eg quantization, 32K-length context, fast inference rate). blue{Falcon~(180B parameters) is trained on 3.5 trillion tokens, achieving a stronger performance compared to many other publicly available LLMs. LLaMA-3.1 stands as the most extensive and powerful model within the LLaMA series, featuring a context window of 128K. Notably, LLaMA-3.1 aims to enhance its functionality by incorporating additional components that work synergistically with the model, including new security and safety tools. ä¸è¦ä¸ºåˆ«äººèƒŒä¹¦ã€‚ã€‚ã€‚Falcon-180Bä¹Ÿä¸æ˜¯å¾ˆå¼ºå•Šã€‚ã€‚ã€‚llama3.1çš„ç‰¹ç‚¹å†™å“ªäº›å®‰å…¨å·¥å…·å¹²å•¥ } Models of this scale typically require thousands of GPUs or TPUs to train. For instance, OPT~(175B version) used 992 A100-80GB GPUs, GLM~(130B version) used a cluster of 96 NVIDIA DGX-A100 (8x40G) GPU nodes, while LLaMA-3.1~(405B version) utilized over 16 thousand H100 GPUs. } LLaMA Model Family. The collection of LLaMA models~Touvron-arxiv-2023-LLaMA were introduced by Meta AI in February, 2023, consisting of four sizes (7B, 13B, 30B and 65B). Since released, LLaMA has attracted extensive attention from both research and industry communities. LLaMA models have achieved very excellent performance on various open benchmarks, which have become the most popular open language models thus far. A large number of researchers have extended LLaMA models by either instruction tuning or continual pre-training. In particular, instruction tuning LLaMA has become a major approach to developing customized or specialized models, due to the relatively low computational costs. To effectively adapt LLaMA models in non-English languages, it often needs to extend the original vocabulary (trained mainly on English corpus) or fine-tune it with instructions or data in the target language. Among these extended models, Stanford Alpaca~Taori-github-2023-Stanford is the first open instruct-following model fine-tuned based on LLaMA~(7B). It is trained by 52K instruction-following demonstrations generated via self-instruct~Wang-arXiv-2022-Self using text-davinci-003. The instruction data, named Alpaca-52K, and training code have been extensively adopted in subsequent work, such as Alpaca-LoRA~Alpaca-LoRA (a reproduction of Stanford Alpaca using LoRA~Hu-ICLR-2022-LoRA), Koala~koala_blogpost_2023, and BELLE~BELLE. In addition, Vicuna~vicuna2023 is another popular LLaMA variant, trained upon user-shared conversations collected from ShareGPT~ShareGPT. Due to the excellent performance and availability of the LLaMA model family, many multimodal models incorporate them as the base language models, to achieve strong language understanding and generation abilities. Compared with other variants, Vicuna is more preferred in multimodal language models, which have led to the emergence of a variety of popular models, including LLaVA~Liu-arxiv-2023-Visual, MiniGPT-4~Zhu-arxiv-2023-MiniGPT-4, InstructBLIP~Dai-2023-arxiv-InstructBLIP, and PandaGPT~su-2023-arxiv-pandagpt. % The release of LLaMA has greatly advanced the research progress of LLMs. % {To summarize the research work conducted on LLaMA, we present a brief evolutionary graph in Figure~fig:llama_family. } The openness and effectiveness of LLaMA~\\cite{Touvron-arxiv-2023-LLaMA have generated considerable interest within the research community. With the aim of implementing novel models or tools, multiple endeavors have been directed towards fine-tuning and continuously pre-training its diverse model versions (7B, 13B, 30B, and 65B). The most common training method is instructing tuning, which enables the model to better follow instructions. Stanford Alpaca~Taori-github-2023-Stanford is the first open instruct-following model fine-tuned from LLaMA~(7B). It is trained on 52K instruction-following demonstrations generated in the style of self-instruct~Wang-arXiv-2022-Self using text-davinci-003. The instruction data, named Alpaca-52k, and training code have been extensively applied in other subsequent models, such as Alpaca-LoRA~Alpaca-LoRA (a reproduction of Stanford Alpaca using LoRA~Hu-ICLR-2022-LoRA), Koala~koala_blogpost_2023, and BELLE~BELLE. In addition, Vicuna~vicuna2023 is trained upon user-shared conversations collected from ShareGPT~ShareGPT. Due to the outstanding performance of the LLaMA family models, many multimodal models also them as their base models for training. Actually, when it comes to selecting a base model for multimodal models, Vicuna is frequently preferred. LLaVA~Liu-arxiv-2023-Visual, MiniGPT-4~Zhu-arxiv-2023-MiniGPT-4, InstructBLIP~Dai-2023-arxiv-InstructBLIP, and PandaGPT~su-2023-arxiv-pandagpt) are all trained upon it. } Public API of LLMs. Instead of directly using the model copies, APIs provide a more convenient way for common users to use LLMs, without the need of running the model locally. As a representative interface for using LLMs, the APIs for the GPT-series models~Brown-NeurIPS-2020-Language, Chen-arxiv-2021-evaluating, Ouyang-arxiv-2022-Training, OpenAI-OpenAI-2023-GPT-4 have been widely used for both academia and industry{https://platform.openai.com/docs/api-reference/introduction}. OpenAI has provided {seven major interfaces to the models in GPT-3 series: ada, babbage, curie, davinci (the most powerful version in GPT-3 series), text-ada-001, text-babbage-001, and text-curie-001.} Among them, the first four interfaces can be further fine-tuned on the host server of OpenAI. In particular, babbage, curie, and davinci correspond to the GPT-3~(1B), GPT-3~(6.7B), and GPT-3~(175B) models, respectively~Brown-NeurIPS-2020-Language. In addition, there are also two APIs related to Codex~Chen-arxiv-2021-evaluating, called code-cushman-001 (a powerful and multilingual version of the Codex~(12B)~Chen-arxiv-2021-evaluating) and code-davinci-002. Further, GPT-3.5 series include one base model code-davinci-002 and three enhanced versions, namely text-davinci-002, text-davinci-003, and gpt-3.5-turbo. As more powerful alternatives, in this year, OpenAI has released the model interfaces for GPT-4 series, including gpt-4, gpt-4-32k, gpt-4-1106-preview~(\\ie GPT-4 Turbo) and gpt-4-vision-preview~(\\ie GPT-4 Turbo with vision, a multimodal model). It is worth noting that OpenAI has been maintaining and upgrading these model interfaces (gpt-3.5-turbo, gpt-4, gpt-4-32k), so the API name will actually point to the latest version. Currently, ChatGPT can be powered by either GPT-3.5 or GPT-4 models. Overall, one select the suitable model interface based on the specific application scenarios and response requirements. The detailed usage can be found on their project websiteshttps://platform.openai.com/docs/models/overview.",
      "origin_cites_number": 58
    },
    {
      "section_title": "Commonly Used Corpora for Pre-training",
      "level": "2",
      "content": "{In contrast to earlier PLMs, LLMs which consist of a significantly larger number of parameters require a higher volume of training data that covers a broad range of content. For this need, there are increasingly more accessible training datasets that have been released for research. In this section, we will briefly summarize several widely used corpora for training LLMs. } Based on their content types, we categorize these corpora into five groups: {web pages, books, Wikipedia, code, and others.} Web pages. Web pages are a primary data source for training language models. $\\bullet$ CommonCrawl. CommonCrawl~commoncrawl is one of the largest open-source web crawling databases, containing a petabyte-scale data volume, which has been widely used as training data for existing LLMs. As the whole dataset is very large, existing studies mainly extract subsets of web pages from it within a specific period or specific needs (\\eg extracting mathematical texts). However, due to the widespread existence of noisy and low-quality information in web page data, it is necessary to perform data preprocessing before usage. {One commonly used toolkit for cleaning CommonCrawl is CC-Net~Wenzek-2020-ccnet, which is developed by Facebook and has been used in processing datasets like RedPajama-Data~Together-2023-Redpajama.} $\\bullet$ C4. {The Colossal Clean Crawled Corpus (C4) includes five {variants}https://www.tensorflow.org/datasets/catalog/c4 , namely en~(806G), en.noclean~(6T), realnewslike~(36G), webtextlike~(17G), and multilingual~(38T). The en version has been utilized for pre-training T5~Raffel-JMLR-2020-Exploring, LaMDA~Thoppilan-CoRR-2022-LaMDA, Gopher~Rae-arxiv-2021-Scaling, and UL2~Tay-arxiv-2022-UL2. The multilingual C4, also called mC4, has been used in mT5~Xue-NAACL-2021-mT5.} $\\bullet$ RedPajama-Data. {RedPajama-Data~Together-2023-Redpajama} is a publicly available comprehensive web dataset, comprising 100 billion documents from Common Crawl. It has been cleaned, filtered, and deduplicated using the CCNet tool, resulting in approximately 30T tokens, which is available for download on Hugging Face. RedPajama-Data is a multilingual dataset that includes five languages: English, French, Spanish, German, and Italian. Additionally, it offers over 40 quality labels, making it feasible to filter or reweight the dataset according to specific criteria. The dataset is continuously updated and maintained, with all data processing scripts open-sourced on GitHub for convenient use. $\\bullet$ RefinedWeb. {RefinedWeb~Penedo-2023-arxiv-Refinedweb} is a web dataset obtained through rigorous selection and deduplication based on data from Common Crawl, encompassing all Common Crawl web records from 2008 to June 2023, totaling around 5T tokens. The open-source portion consists of 600B tokens, with a data size of approximately 500GB. After decompression, it requires 2.8TB of local storage space and is available for download on Hugging Face. This dataset serves as the primary training dataset for the open-source large language model Falcon. $\\bullet$ WebText. {WebText~radford-blog-2019-language is a well-known corpus composed of highly upvoted links from Reddit, a social media platform that enables users to submit links and text posts, but it is not publicly available. As a surrogate, there is a readily accessible open-source alternative called OpenWebText~Gokaslan2019OpenWeb.} {CC-Stories~(31G) is composed of a subset of CommonCrawl data, in which the contents are made in a story-like way. Because the original source of CC-Stories is not available now, blue{we(you ????)} include {a reproduction version, CC-Stories-R~CC-Stories-R,} in Table~tab:corpora. blue{CC-News~(76G) and REALNEWS~(120G), are two large news corpora extracted from Common Crawl. (so what???) RedPajama-Data is a multilingual corpora containing 100 billion documents in five languages from Common Crawl. RefinedWeb~(500G) is obtained through strict screening and deduplication based on Common Crawl data, and is the main training corpora for Falcon~Ebtesam-arxiv-2023-Falcon. WanJuan-CC~(400G) is a high-quality English(only English???) corpora extracted and cleaned from Common Crawl, and is the pre-training corpora for InternLM2~Cai-arxiv-2024-Internlm2. ChineseWebText(citations???, also too short) is a Chinese corpora carefully selected from Common Crawl. } } \\textcolor{blue{In addition to extracting data from Common Crawl, there are other web page corpora. WebText~radford-blog-2019-language is a well-known corpus composed of highly upvoted links from Reddit, a social media platform that enables users to submit links and text posts, but it is not publicly available. As a surrogate, there is a readily accessible open-source alternative called OpenWebText~Gokaslan2019OpenWeb. In addition, there are several Chinese corpora, WanJuan 1.0 Text~He-2023-arxiv-Wanjuan, WuDaoCorpora Text~Yuan-2021-aiopen-WuDaoCorpora, and SkyPile-150B~wei-2023-skywork. Among them, WanJuan 1.0 Text is used for training Intern Multimodal and Intern Puyu, and SkyPile-150B is used for training Skywork~wei-2023-skywork. (The logic of this part is very bad...) } } Books \\& Academic Data. Books and academic data contains a wealth of world knowledge and linguistic information, serving as a high-quality corpus for model learning. $\\bullet Book Data.$ BookCorpus~Zhu-ICCV-2015-Aligning is a commonly used dataset in previous small-scale models (\\eg GPT~radford-openai-2018-improving and GPT-2~radford-blog-2019-language), consisting of over 11,000 books covering a wide range of topics and genres (\\eg novels and biographies). Another large-scale book corpus is Project Gutenberg~Gutenberg, consisting of over 70,000 literary books including novels, essays, poetry, drama, history, science, philosophy, and other types of works in the public domain. It is currently one of the largest open-source book collections, which is used in training of MT-NLG~Smith-CoRR-2022-Using and LLaMA~Touvron-arxiv-2023-LLaMA. As for Books1~Brown-NeurIPS-2020-Language and Books2~Brown-NeurIPS-2020-Language used in GPT-3~Brown-NeurIPS-2020-Language, they are much larger than BookCorpus but have not been publicly released so far. $\\bullet$ Academic Data. In addition to book data, scientific publication data such as paper is also important for model pre-training. arXiv Dataset~clement-arxiv-2019-ontheuse is a corpus of 1.7 million academic papers, covering a wide range of papers in the fields of physics, mathematics, and computer science. S2ORC~lo-2020-ACL-s2orc is a corpora that consists of 136M academic papers {collected by Semantic Scholar. It also releases a derivative dataset peS2o~peS2o, which contains about 42B tokens. } \\paratitle{CommonCrawl. CommonCrawl~commoncrawl is one of the largest open-source web crawling databases, containing a petabyte-scale data volume, which has been widely used as training data for existing LLMs. As the whole dataset is very large, existing studies mainly extract subsets of web pages from it within a specific period. However, due to the widespread existence of noisy and low-quality information in web data, it is necessary to perform data preprocessing before usage. Based on CommonCrawl, there are four filtered datasets that are commonly used in existing work: C4~Raffel-JMLR-2020-Exploring, CC-Stories~Trinh-CoRR-2018-A, CC-News~Liu-CoRR-2019-RoBERTa, and RealNews~Zellers-NeurIPS-2019-Defending. The Colossal Clean Crawled Corpus (C4) includes five {variants}https://www.tensorflow.org/datasets/catalog/c4 , namely en~(806G), en.noclean~(6T), realnewslike~(36G), webtextlike~(17G), and multilingual~(38T). The en version has been utilized for pre-training T5~Raffel-JMLR-2020-Exploring, LaMDA~Thoppilan-CoRR-2022-LaMDA, Gopher~Rae-arxiv-2021-Scaling, and UL2~Tay-arxiv-2022-UL2. The multilingual C4, also called mC4, has been used in mT5~Xue-NAACL-2021-mT5. {CC-Stories~(31G) is composed of a subset of CommonCrawl data, in which the contents are made in a story-like way.} Because the original source of CC-Stories is not available now, we include {a reproduction version, CC-Stories-R~CC-Stories-R,} in Table~tab:corpora. {Moreover, two news corpora extracted from CommonCrawl, \\ie REALNEWS~(120G) and CC-News~(76G), are also commonly used as the pre-training data.} Reddit Links. Reddit is a social media platform that enables users to submit links and text posts, which can be voted on by others through ``upvotes'' or ``downvotes''. Highly upvoted posts are often considered useful, and can be utilized to create high-quality datasets. WebText~radford-blog-2019-language is a well-known corpus composed of highly upvoted links from Reddit, but it is not publicly available. As a surrogate, there is a readily accessible open-source alternative called OpenWebText~Gokaslan2019OpenWeb. Another corpus extracted from Reddit is PushShift.io~Baumgartner-AAAI-2020-The, a {real-time updated dataset that consists of historical data from Reddit since its creation day}. {Pushshift provides not only monthly data dumps but also useful utility tools to support users in searching, summarizing, and conducting preliminary investigations on the entire dataset. This makes it easy for users to collect and process Reddit data.} } Wikipedia. Wikipedia~Wikipedia is an online encyclopedia containing a large volume of high-quality articles on diverse topics. {Most of these articles are composed in an expository style of writing (with supporting references), covering a wide range of languages and fields. } Typically, the English-only filtered versions of Wikipedia are widely used in most LLMs (\\eg GPT-3~Brown-NeurIPS-2020-Language, LaMDA~Thoppilan-CoRR-2022-LaMDA, and LLaMA~Touvron-arxiv-2023-LLaMA). Wikipedia is available in multiple languages, so it can be used in multilingual settings. Code. To collect code data, existing work mainly crawls open-source licensed codes from the Internet. Two major sources are public code repositories under open-source licenses (\\eg GitHub) and code-related question-answering platforms (\\eg StackOverflow). {Google has publicly released the BigQuery dataset~bigquery-google, which includes a substantial number of open-source licensed code snippets in various programming languages, serving as a representative code dataset. CodeGen has utilized BIGQUERY~nijkamp-arxiv-2022-Codegen, a subset of the BigQuery dataset, for training the multilingual version of CodeGen (CodeGen-Multi).} {In addition, Hugging Face has collected and released a code dataset named The Stack~kocetkov-arxiv-2022-thestack, covering more than 30 programming languages. The Stack is continuously updated, and the v1.2 version has expanded to 358 programming languages. Based on this dataset, BigCode further { processed it} and released StarCoder~Li-2023-arxiv-Starcoder, which is also the pre-training data of the model StarCoder. } Mixed Data. In addition to the aforementioned specific types of datasets, different types of data have been combined to facilitate usage by researchers. The Pile~Gao-arxiv-2021-Pile is a large-scale, diverse, and open-source text dataset consisting of over 800GB of data from multiple sources, including books, websites, codes, scientific papers, and social media platforms. It is constructed from 22 diverse high-quality subsets. The Pile dataset is widely used in models with different parameter scales, such as GPT-J~(6B)~Wang-GitHub-2021-GPT-J, CodeGen~(16B)~nijkamp-arxiv-2022-Codegen, and Megatron-Turing NLG~(530B)~Smith-CoRR-2022-Using. ROOTS~Laurencon-NIPS-2022-The is composed of various smaller datasets (totally 1.61 TB of text) and covers 59 different languages (containing natural languages and programming languages), which have been used for training BLOOM~Scao-arxiv-2022-BLOOM. {Another mixture dataset is Dolma~Soldani-arxiv-2024-dolma, which includes web text from Common Crawl, academic papers from Semantic Scholar, GitHub code, books, social media from Reddit, and Wikipedia data. Dolma consisting of 3T tokens of approximately 200TB of raw text and has been used to train OLMo~groeneveld-2024-arxiv-olmo. } In practice, it commonly requires a mixture of different data sources for pre-training LLMs (see Figure~fig:source-ratio), instead of a single corpus. {Therefore, existing studies commonly mix several ready-made datasets (\\eg C4, OpenWebText, and the Pile), and then perform further processing to obtain the pre-training corpus. Furthermore, to train the LLMs that are adaptive to specific applications, it is also important to extract data from relevant sources (\\eg Wikipedia and BigQuery) for enriching the corresponding information in pre-training data.}",
      "origin_cites_number": 88
    },
    {
      "section_title": "Commonly Used Datasets for Fine-tuning",
      "level": "2",
      "content": "{After pre-training, it requires further fine-tuning LLMs to enhance the model capacity, which often involve two major steps, namely instruction tuning (supervised fine-tuning) and alignment tuning. In this section, we mainly focus on discussing the related available datasets for the two kinds of tuning approaches, and more algorithm details can be found in Section~sec-adaptation.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Instruction Tuning Datasets",
      "level": "3",
      "content": "{ After pre-training, instruction tuning (\\aka supervised fine-tuning) is an important method to enhance or unlock specific abilities of LLMs (\\eg instruction following). In this part, we introduce several widely used datasets for instruction tuning, and categorize them into three main types based on the construction method of formatted instruction instances, namely NLP task datasets, daily chat datasets and synthetic datasets. We show their details in Table~tab:instruction-collection.} NLP Task Datasets. % {This kind of datasets are formatted based on collected NLP task datasets~(\\eg text classification and summarization) with corresponding natural language task descriptions. In this category, P3~Sanh-2022-ICLR-P3 and FLAN~Wei-ICLR-2022-Finetuned, Longpre-2023-arxiv-Flan_v2 are two widely used datasets for instruction tuning. } $\\bullet$ P3~Sanh-2022-ICLR-P3 is composed of 170 English NLP datasets and 2,052 English prompt templates, where the input and output of each data example have been formatted with specific prompt templates for composing the training instance. $\\bullet$ FLAN{~Wei-ICLR-2022-Finetuned consists of 62 widely used NLP benchmarks in its original version. Recently, FLAN-v2~Longpre-2023-arxiv-Flan_v2 is also proposed, which expands FLAN by mixing additional instruction datasets, including Muffin~Wei-ICLR-2022-Finetuned, NIV2~Wang-EMNLP-2022-Super, T0-SF~Sanh-ICLR-2022-Multitask, and CoT~Cobbe-arxiv-2021-Training, Geva-tacl-2021-Did, Camburu-2020-ACL-Make. Muffin contains 62 tasks from the original FLAN and additional 26 tasks, including conversation and code synthesis tasks. T0-SF is extracted from T0~Sanh-ICLR-2022-Multitask while ensuring no overlap with Muffin. NIV2 refers to the Natural-Instructions v2 dataset~Wang-EMNLP-2022-Super, and CoT~Cobbe-arxiv-2021-Training, Geva-tacl-2021-Did, Camburu-2020-ACL-Make is a combination of nine reasoning tasks with corresponding chain-of-thought prompts and outputs. } Daily Chat Datasets. % {This kind of datasets are constructed based on real user conversations where queries are posed by humans and responses are mainly generated by human labelers or LLMs~(\\eg ChatGPT, GPT-4). The conversation types include open-ended generation, question answering, brainstorming, and chatting. In this category, ShareGPT~ShareGPT, OpenAssistant~kopf-arxiv-2023-openassistant and Dolly~Conover-2023-arxiv-Dolly are three commonly used datasets for LLM fine-tuning.} $\\bullet$ ShareGPT{~ShareGPT is collected from a data collection platform where users can upload their conversations with ChatGPT or GPT-4 through the ShareGPT API. Currently, this dataset consists of approximately 90,000 conversations, including real instructions or inquiries from human and responses from ChatGPT.} $\\bullet$ OpenAssistant{~kopf-arxiv-2023-openassistant is a multilingual corpus containing 66,497 real-world conversation trees between human and AI assistant. Each conversation tree consists of multiple nodes, and each node represents the information generated by a role in the dialogue. It spans 35 languages and includes 461,292 manually annotated quality ratings of responses. } $\\bullet$ Dolly{~Conover-2023-arxiv-Dolly is an English dataset comprising 15,000 human-generated data instances (prompt-response pairs) from Databricks. This dataset covers seven domains outlined in the InstructGPT~Ouyang-arxiv-2022-Training, including brainstorming, classification, closed-book quality assurance, generation, information extraction, open-book quality assurance, and summarization. } Synthetic Datasets. {This kind of datasets are typically constructed by instructing LLMs, based on pre-defined guidance rules or methods. In this category, Self-Instruct-52K~Wang-arXiv-2022-Self, Alpaca~Taori-github-2023-Stanford and Baize~xu-arxiv-2023-baize are three commonly used synthetic datasets for LLMs.} $\\bullet$ Self-Instruct-52K~Wang-arXiv-2022-Self is an instruction dataset generated through the self-instruct~Wang-arXiv-2022-Self method, consisting of 82,000 instances with 52,000 instructions. Concretely, the authors construct 175 seed instances, and then iteratively prompt the LLM~Brown-NeurIPS-2020-Language to synthesize additional instructions based on randomly selected 8 instructions as reference. Subsequently, the LLM is further instructed to generate instance inputs and their corresponding outputs based on the synthetic instructions, and finally obtain the Self-Instruct-52K dataset. $\\bullet$ Alpaca~Taori-github-2023-Stanford is also a synthetic dataset based on the self-instruct~Wang-arXiv-2022-Self method. It utilizes the text-davinci-003 model on the 175 seed datasets from Self-Instruct-52K to obtain 52,000 new instructions and corresponding inputs and outputs. Moreover, % 60\\% of the examples are pure instructions without the input part in the final dataset. $\\bullet$ Baize~xu-arxiv-2023-baize is an English multi-turn conversation corpus constructed using ChatGPT, comprising 111.5K instances. To create Baize, a method called ``self-chat\"~xu-arxiv-2023-baize is purposed, where ChatGPT takes on the roles of both the user and the AI assistant in turns, generating information in a conversational format.",
      "origin_cites_number": 29
    },
    {
      "section_title": "Alignment Datasets",
      "level": "3",
      "content": "Apart from instruction tuning, it is important to construct high-quality datasets for aligning LLMs with human values and preferences~(\\eg helpfulness, honesty, and harmlessness). In this section, we introduce several widely used datasets for alignment tuning, including HH-RLHF~Bai-arxiv-2022-Training, SHP~Ethayarajh-ICLM-2022-Understanding, PKU-SafeRLHF~Dai-arxiv-2023-SafeRLHF, Stack Exchange Preferences~Lambert-2023-StackH4 and Sandbox Alignment Data~Liu-arxiv-2023-training. We show their details in Table~tab:rlhf-datasets. $\\bullet$ HH-RLHF~Bai-arxiv-2022-Training consists of around 169K instances, and can be divided into two parts that focus on the helpfulness and harmlessness of LLMs, respectively. Each instance is an open-ended conversation between a crowdworker and a chat model, about seeking assistance, advice, or task completion. The chat model provides two responses to each user query, and the more helpful or harmful responses will be chosen as the annotations. $\\bullet$ {SHP}{~Ethayarajh-ICLM-2022-Understanding focuses on the helpfulness of responses. It comprises 385K collective human preferences over responses to questions/instructions across 18 diverse subject areas, spanning topics from cooking to legal advice. Each instance is a Reddit post containing a question or instruction and a pair of top-level comments, one of which is deemed as more preferable by Reddit users and the other one is deemed as less helpful. Different from HH-RLHF~Bai-arxiv-2022-Training, the data in SHP consists of naturally occurring and human-written responses. } $\\bullet$ {PKU-SafeRLHF}{~Dai-arxiv-2023-SafeRLHF encompasses more than 330K instances of expert comparison data, concentrating on the helpfulness and harmlessness. Each instance in the dataset includes a question and two responses, accompanied by safety labels for each response and two preference annotations between the two responses according to helpfulness and harmlessness. The harmlessness of a response indicates its classification as risk-neutral across all 14 harm categories, while the helpfulness of a response is evaluated based on its effectiveness in addressing the question. } $\\bullet$ {Stack Exchange Preferences}{~Lambert-2023-StackH4 focuses on the helpfulness of answers. It comprises about 10M questions and answers from Stack Overflow. Each instance consists of a question and more than two corresponding answers. Each answer is annotated with a score calculated based on its votes and a label denoting whether it is selected. } $\\bullet$ {Sandbox Alignment Data}{~Liu-arxiv-2023-training is an alignment dataset containing feedback from LLMs rather than human. It comes from a virtual interaction environment called SANDBOX, where the model simulates social interactions with other models and revise responses according to the feedback from other models. The dataset contains 169K instances, and each instance consists of a societal query, several responses, and corresponding ratings from other models. } }",
      "origin_cites_number": 11
    },
    {
      "section_title": "Library Resource",
      "level": "2",
      "content": "In this part, we briefly introduce a series of available libraries for developing LLMs. $\\bullet$ Transformers~Wolf-EMNLP-2020-Transformers is an open-source Python library for building models using the Transformer architecture, which is developed and maintained by Hugging Face. It has a simple and user-friendly API, {making it easy to use and customize various pre-trained models.} It is a powerful library with a large and active community of users and developers who regularly update and improve the models and algorithms. {$\\bullet$ DeepSpeed~Rasley-KDD-2020-DeepSpeed is a deep learning optimization library (compatible with PyTorch) developed by Microsoft, which has been used to train a number of LLMs, such as MT-NLG~Smith-CoRR-2022-Using and BLOOM~Scao-arxiv-2022-BLOOM.} It provides the support of various optimization techniques for distributed training, such as memory optimization (ZeRO technique, gradient checkpointing), and pipeline parallelism. $\\bullet$ Megatron-LM~Shoeybi-arXiv-2019-Megatron, Narayanan-ACM-2021-Efficient, Korthikanti-arxiv-2022-reducing is a deep learning library developed by NVIDIA for training large-scale language models. It also provides rich optimization techniques for distributed training, {including model and data parallelism, mixed-precision training, and FlashAttention.} These optimization techniques can largely improve the training efficiency and speed, enabling efficient distributed training across GPUs. $\\bullet$ JAX~Bradbury-github-2018-jax is a Python library for high-performance machine learning algorithms developed by Google, allowing users to easily perform computations on arrays with hardware acceleration (\\eg GPU or TPU). % {It enables efficient computation on various devices and also supports several featured functions, such as automatic differentiation and just-in-time compilation. } {$\\bullet$ Colossal-AI~Bian-CoRR-2021-Colossal-AI is a deep learning library developed by HPC-AI Tech for training large-scale AI models. It is implemented based on PyTorch and supports a rich collection of parallel training strategies. Furthermore, it can also optimize heterogeneous memory management with methods proposed by PatrickStar~Fang-arxiv-2021-PatrickStar. {Recently, a ChatGPT-like model called ColossalChat~ColossalChat has been publicly released with two versions (7B and 13B), which are developed using Colossal-AI based on LLaMA~Touvron-arxiv-2023-LLaMA.}} $\\bullet$ {BMTrain~BMTrain is an efficient library developed by OpenBMB for training models with large-scale parameters in a distributed manner, which emphasizes code simplicity, low resource, and high availability. BMTrain has already incorporated several common LLMs (\\eg Flan-T5~Chung-arxiv-2022-Scaling and GLM~Zeng-arxiv-2022-GLM) into its ModelCenter, where developers can use these models directly.} $\\bullet$ {FastMoE~He-arXiv-2021-FastMoE is a specialized training library for MoE (\\ie mixture-of-experts) models. It is developed {based on} PyTorch, prioritizing both efficiency and user-friendliness in its design. FastMoE simplifies the process of transferring Transformer models to MoE models and supports both data parallelism and model parallelism during training. } {$\\bullet$ {vLLM~kwon-2023-SIGOPS-efficient is a fast, memory efficient, and easy-to-use library for LLM inference and serving. To enable fast inference, it is specially optimized with high serving throughput, effective attention memory management using PagedAttention~kwon-2023-SIGOPS-efficient, continuous batching, and optimized CUDA kernels. Furthermore, vLLM also supports various decoding algorithms, tensor parallelism and streaming outputs. To ease the integration with other systems, vLLM is friendly to the use of HuggingFace models, and also provide OpenAI-compatible API servers. } } {$\\bullet$ {DeepSpeed-MII~DeepSpeed-MII is also a memory efficient Python library developed by DeepSpeed~Rasley-KDD-2020-DeepSpeed. It aims to democratize LLMs inference by prioritizing high throughput, low latency, and cost-effectiveness. DeepSpeed-MII achieves accelerated text generation inference by leveraging four essential technologies: blocked KV caching, continuous batching, dynamic SplitFuse, and high-performance CUDA Kernels. It currently supports over 13,000 models across three popular model architectures, such as LLaMA~Touvron-arxiv-2023-LLaMA, Mistral~jiang-2023-arxiv-mistral, and OPT~Zhang-arxiv-2022-OPT. } } {$\\bullet$ {DeepSpeed-Chat~yao-2023-arxiv-dschat} is a fast, cost-effective, and easy-to-use system framework that enables the integration of the complete RLHF process during model training. It is featured by three major functionalities: (1) it simplifies the training and inference process for ChatGPT-like models, enabling using a simple script to implement multiple training or inference steps; (2) it replicates the training mode of InstructGPT~Ouyang-arxiv-2022-Training and provides a complete pipeline for three training steps~(\\ie SFT, reward model fine-tuning, and RLHF); (3) it integrates the training engine and inference engine of Deepspeed into a unified hybrid engine~(Deepspeed HE) for RLHF training, which enables seamless switch between training and inference modes, and leveraging various optimizations from DeepSpeed Inference. } { In addition to the above library resources, existing deep learning frameworks {(\\eg PyTorch~Paszke-NeurIPS-2019-Pytorch, TensorFlow~Abadi-OSDI-2016-TensorFlow, MXNet~Chen-arxiv-2015-MXNet, PaddlePaddle~Ma-fodc-2019-PaddlePaddle, MindSpore~Huawei-Springer-2022-MindSpore and OneFlow~Yuan-arXiv-2021-OneFlow) have also provided the support for parallel algorithms, which are commonly used for training large-scale models. } } figure* \\centering \\includegraphics[width=\\textwidth]{images/source_ratio_20240923.pdf} Ratios of various data sources in the pre-training data for existing LLMs. figure*",
      "origin_cites_number": 29
    },
    {
      "section_title": "Pre-training",
      "level": "1",
      "content": "Pre-training establishes the basis of the abilities of LLMs. By pre-training on large-scale corpora, LLMs can acquire essential language understanding and % {generation} skills~Brown-NeurIPS-2020-Language,Chowdhery-arxiv-2022-PaLM. In this process, the scale and quality of the pre-training corpus are critical for LLMs to attain powerful capabilities. Furthermore, to effectively pre-train LLMs, model architectures, acceleration methods, and optimization techniques need to be well designed. In what follows, we first discuss the data collection and processing in Section~sec:data_collection, then introduce the commonly used model architectures in Section~sec:architecture, and finally present the training techniques to stably and efficiently optimize LLMs in Section~sec:training_settings.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Data Collection and Preparation",
      "level": "2",
      "content": "Compared with small-scale language models, LLMs have a stronger demand for high-quality data for model pre-training, and their model capacities largely rely on the pre-training corpus and how it has been preprocessed. In this part, we discuss the collection and processing of pre-training data, including data sources, preprocessing methods, and important analysis of how pre-training data affects the performance of LLMs.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Data Source",
      "level": "3",
      "content": "To develop a capable LLM, it is key to collect a large amount of natural language corpus from various data sources. Existing LLMs mainly leverage a mixture of diverse public textual datasets as the pre-training corpus. Figure~fig:source-ratio shows the distribution of the sources of pre-training data for a number of representative LLMs. The source of pre-training corpus can be broadly categorized into two types: general data and specialized data. General data, such as webpages, books, and conversational text, is utilized by most LLMs~Chowdhery-arxiv-2022-PaLM,Brown-NeurIPS-2020-Language,Zhang-arxiv-2022-OPT due to its large, diverse, and accessible nature, which can enhance the language modeling and generalization abilities of LLMs. In light of the impressive generalization capabilities exhibited by LLMs, there are also studies that extend their pre-training corpus to more specialized datasets, such as multilingual data, scientific data, and code, endowing LLMs with specific task-solving capabilities~Chowdhery-arxiv-2022-PaLM,Taylor-arxiv-2022-Galactica, nijkamp-arxiv-2022-Codegen. In what follows, we describe these two types of pre-training data sources and their effects on LLMs. {For a detailed introduction to the commonly used corpus, one can refer to Section~sec:commonly_used_corpora.} General Text Data. As we can see in Figure~fig:source-ratio, the vast majority of LLMs adopt general-purpose pre-training data, such as webpages, books, and conversational text, which provides rich text sources on a variety of topics. Next, we briefly summarize three important kinds of general data. $\\bullet$ Webpages. Owing to the proliferation of the Internet, various types of data have been created, which enables LLMs to gain diverse linguistic knowledge and enhance their generalization capabilities~radford-blog-2019-language,Raffel-JMLR-2020-Exploring. For convenient use of these data resources, a large amount of data is crawled from the web in previous work, such as CommonCrawl~commoncrawl. However, the crawled web data tends to contain both high-quality text, such as Wikipedia and low-quality text, like spam mail, thus it is important to filter and process webpages for improving the data quality. $\\bullet$ Conversation text. % Conversation data can enhance the conversational competence of LLMs~Zhang-arxiv-2022-OPT and potentially improve their performance on a range of question-answering tasks~Chowdhery-arxiv-2022-PaLM. Researchers can utilize subsets of public conversation corpus (\\eg PushShift.io Reddit corpus)~Roller-ACL-2021-Recipes,Baumgartner-AAAI-2020-The or collect conversation data from online social media. {Since online conversational data often involves discussions among multiple participants, an effective processing way is to transform a conversation into a tree structure, where the utterance is linked to the one it responds to. In this way, the multi-party conversation tree can be divided into multiple sub-conversations, which can be collected in the pre-training corpus.} Furthermore, a potential risk is that the excessive integration of dialogue data into LLMs may result in a side effect~Zhang-arxiv-2022-OPT: declarative instructions and direct interrogatives are erroneously perceived as the beginning of conversations, thus leading to a decline in the efficacy of the instructions. $\\bullet$ Books. Compared to other corpus, books provide an important source of formal long texts, which are potentially beneficial for % {LLMs to learn linguistic knowledge, model long-term dependency, and generate narrative and coherent texts.} To obtain open-source book data, existing studies usually adopt the Books3 and Bookcorpus2 datasets, which are available in the Pile dataset~Gao-arxiv-2021-Pile. Specialized Text Data. Specialized datasets are useful to improve the specific capabilities of LLMs on downstream tasks. Next, we introduce three kinds of specialized data. $\\bullet$ Multilingual text. In addition to the text in the target language, integrating a multilingual corpus can enhance the multilingual abilities of language understanding and generation. For example, BLOOM~Scao-arxiv-2022-BLOOM and PaLM~Chowdhery-arxiv-2022-PaLM have curated multilingual data covering 46 and 122 languages, respectively, within their pre-training corpora. FLM~Li-arxiv-2023-FLM mixes Chinese and English corpora in nearly equal proportions. These models demonstrate impressive performance in multilingual tasks, such as translation, multilingual summarization, and multilingual question answering, and achieve comparable or superior performance to the state-of-the-art models that are fine-tuned on the corpus in the target language(s). $\\bullet$ Scientific text. The exploration of science by humans has been witnessed by the increasing growth of scientific publications. In order to enhance the understanding of scientific knowledge for LLMs~Taylor-arxiv-2022-Galactica,Lewkowycz-arxiv-2022-Solving, it is useful to incorporate a scientific corpus for model pre-training~Taylor-arxiv-2022-Galactica,Lewkowycz-arxiv-2022-Solving. % By pre-training on a vast amount of scientific text, LLMs can achieve impressive performance in scientific and reasoning tasks~Saier-arxiv-2023-unarXive. To construct the scientific corpus, existing efforts mainly collect arXiv papers, {scientific textbooks}, math webpages, and other related scientific resources. Due to the complex nature of data in scientific fields, such as mathematical symbols and protein sequences, specific tokenization and preprocessing techniques are usually required to transform these different formats of data into a unified form that can be processed by language models. $\\bullet$ Code. Program synthesis has been widely studied in the research community~Simon-JACM-1963-Experiments,Manna-CommunACM-1971-Toward,Feng-EMNLPFindings-2020-CodeBERT,Chen-arxiv-2021-evaluating,Austin-arxiv-2021-Program, especially the use of PLMs trained on code~Black-GitHub-2021-GPT-Neo,Wang-GitHub-2021-GPT-J. However, it remains challenging for these PLMs (\\eg GPT-J~Wang-GitHub-2021-GPT-J) to generate high-quality and accurate programs. Recent studies~Chen-arxiv-2021-evaluating,Austin-arxiv-2021-Program have found that training LLMs on a vast code corpus can lead to a substantial improvement in the quality of the synthesized programs. The generated programs can successfully pass expert-designed unit-test cases~Chen-arxiv-2021-evaluating or solve competitive programming questions~Li-Science-2022-AlphaCode. % {In general, two types of code corpora are commonly used for pre-training LLMs. The first source is from programming question answering communities like Stack Exchange~Xu-SIGPLAN-2022-Systematic. The second source is from public software repositories such as GitHub~Chen-arxiv-2021-evaluating,Austin-arxiv-2021-Program,nijkamp-arxiv-2022-Codegen, where code data (including comments and docstrings) are collected for utilization.} Compared to natural language text, code is in the format of a programming language, corresponding to long-range dependencies and accurate execution logic~Madaan-emnlp-2022-Language. {A recent study~FU-blog-2022-how} also speculates that training on code might be a source of complex reasoning abilities (\\eg chain-of-thought ability~Wei-arxiv-2022-chain). {Furthermore, it has been shown that formatting reasoning tasks into code can help LLMs generate more accurate results~Madaan-emnlp-2022-Language.}",
      "origin_cites_number": 27
    },
    {
      "section_title": "Data Preprocessing",
      "level": "3",
      "content": "After collecting a large amount of text data, it is essential to preprocess the data for constructing the pre-training corpus, especially removing noisy, redundant, irrelevant, and potentially toxic data~Rae-arxiv-2021-Scaling, Chowdhery-arxiv-2022-PaLM, Longpre-arxiv-2023-pretrainer, which may largely affect the capacity and performance of LLMs. {To facilitate the data processing, a recent study~Chen-2023-arxiv-Data proposes a useful data processing system for LLMs, named Data-Juicer, which provides over 50 processing operators and tools.} In this part, we review the detailed data preprocessing strategies to improve the quality of the collected data~Rae-arxiv-2021-Scaling,Du-ICML-2022-GLaM,Scao-arxiv-2022-BLOOM. {A typical pipeline of preprocessing the pre-training data for LLMs has been illustrated in Figure~fig:processing-pipeline.} figure* \\centering \\includegraphics[width=1\\textwidth]{images/pipeline-20240908.pdf} An illustration of a typical data preprocessing pipeline for pre-training large language models. figure* Filtering and Selection. To remove low-quality data from the collected corpus, existing work generally adopts two approaches, namely classifier-based and heuristic-based. The former approach trains a selection classifier based on high-quality texts and leverages it to identify and filter out low-quality data. {Typically, these methods train a binary classifier using positive instances that are: well-curated data (\\eg Wikipedia pages)~Brown-NeurIPS-2020-Language,Du-ICML-2022-GLaM,Chowdhery-arxiv-2022-PaLM, high-quality synthesized data~LLaMa3,Abdin-arxiv-2024-phi3,Penedo-arxiv-2024-fineweb,Maini-ICLRworkshop-2024-rephrasing, or a combination of both.} They sample candidate data as negative instances and predict the score that measures the quality of each data example. However, several studies~Du-ICML-2022-GLaM, Rae-arxiv-2021-Scaling find that a classifier-based approach may result in the unintentional removal of high-quality texts in dialectal, colloquial, and sociolectal languages, which potentially leads to bias in the pre-training corpus and diminishes the corpus diversity. As the second approach, several studies, such as BLOOM~Scao-arxiv-2022-BLOOM and Gopher~Rae-arxiv-2021-Scaling, employ heuristic-based approaches to eliminate low-quality texts through a set of well-designed rules, which can be summarized as follows: itemize[leftmargin=0.5cm, itemsep=5pt] \\item Language based filtering. If a LLM would be mainly used in the tasks of certain languages, the text in other languages can be filtered. \\item Metric based filtering. Evaluation metrics about the generated texts, \\eg perplexity, can be employed to detect and remove unnatural sentences. \\item Statistic based filtering. Statistical features of a corpus, \\eg the punctuation distribution, symbol-to-word ratio, and sentence length, can be utilized to measure the text quality and filter the low-quality data. \\item Keyword based filtering. Based on specific keyword set, the noisy or unuseful elements in the text, such as HTML tags, hyperlinks, boilerplates, and offensive words, can be identified and removed. itemize In addition to the above methods, LLMs (especially relatively small models) can be also employed for data selection, either by computing perplexity~Marion-arxiv-2023-data_pruning or directly prompting LLMs~Sachdeva-arxiv-2024-askllm for measuring the sample importance. However, using LLMs is unavoidably computationally intensive for large-scale data selection. De-duplication. Existing work~Hernandez-arxiv-2022-Scaling has found that duplicate data in a corpus would reduce the diversity of language models, which may cause the training process to become unstable and thus affect the model performance. Therefore, it is necessary to de-duplicate the pre-training corpus. Specially, de-duplication can be performed at different granularities, including sentence-level, document-level, and dataset-level de-duplication. First, low-quality sentences that contain repeated words and phrases should be removed, as they may introduce repetitive patterns in language modeling~Holtzman-2019-ICLR-The. At the document level, existing studies mostly rely on the overlap ratio of surface features (\\eg words and $n$-grams overlap) between documents to detect and remove duplicate documents containing similar contents~Rae-arxiv-2021-Scaling,Touvron-arxiv-2023-LLaMA,Scao-arxiv-2022-BLOOM,Lee-ACL-2022-Deduplicating. Furthermore, to avoid the dataset contamination problem, it is also crucial to prevent the overlap between the training and evaluation sets~Chowdhery-arxiv-2022-PaLM, by removing the possible duplicate texts from the training set. It has been shown that the three levels of de-duplication are useful to improve the training of LLMs~Carlini-arxiv-2022-Quantifying,Chowdhery-arxiv-2022-PaLM, which should be jointly used in practice. Privacy Reduction. Thus, it is necessary to remove the personally identifiable information~(PII) from the pre-training corpus. One direct and effective approach is to employ rule-based methods, such as keyword spotting, to detect and remove PII such as names, addresses, and phone numbers~Laurencon-NIPS-2022-The. Furthermore, researchers also find that the vulnerability of LLMs under privacy attacks can be attributed to the presence of {duplicate PII data} in the pre-training corpus~Kandpal-ICML-2022-Deduplicating. Therefore, de-duplication can also reduce privacy risks to some extent. (this part is somehow difficult to follow) Instead of deciding which data to filter, some recent research focuses on actively selecting data samples for pre-training~\\cite{Sachdeva-arxiv-2024-askllm,Engstrom-ICML-2024-dsdm,Marion-arxiv-2023-data_pruning. Data selection is a process of selecting data samples and shaping the pre-training corpus so that it fits a certain distribution. By prioritizing important data samples, researchers aim to balance training cost and model performance (or even boost performance), achieving more data-efficient LLM pre-training~Sachdeva-arxiv-2024-askllm,Penedo-arxiv-2024-fineweb. Early methods follow the assumption of coverage sampling that a more diverse corpus leads to better performance~Lee-arxiv-2023-beyond_scale,Abbas-ICLRWorkshop-2023-SemDeDup. These methods enhance the semantic diversity of the corpus by encoding data samples into representations and measuring their semantic similarities. When certain tasks of interest are available, another practical idea is to select data samples that are more important for improving target downstream tasks~Xie-arxiv-2023-DSIR,Engstrom-ICML-2024-dsdm. In addition to these heuristic and target-aware methods, a recent trend is to select data samples using existing LLMs to pre-train new LLMs. To select data samples favored by the reference LLMs, the most straightforward method is to compute metrics such as perplexity using LLMs~Marion-arxiv-2023-data_pruning, or to prompt LLMs directly to decide whether a sample should be selected~Sachdeva-arxiv-2024-askllm. The main issue of the above methods is that they are unavoidably computation-intensive. Another line of work trains LLMs to fit the reference LLMs by leveraging synthetic data. LLaMA 3~LLaMa3, Phi-3~Abdin-arxiv-2024-phi3, and FineWeb-Edu~Penedo-arxiv-2024-fineweb all mention that they train classifiers using the LLM-generated data to further select pre-training data samples. Synthetic data can also be directly used as pre-training data samples, by rephrasing the original data samples to improve quality and diversity~Maini-ICLRworkshop-2024-rephrasing. } Tokenization. Tokenization is also a crucial step for data preprocessing. It aims to segment raw text into sequences of individual tokens, which are subsequently used as the inputs of LLMs. In traditional NLP research (\\eg sequence labeling with conditional random fields~Lafferty-ICML-2001), word-based tokenization is the predominant approach, which is more aligned with human's language cognition. However, word-based tokenization can yield different segmentation results for the same input in some languages (\\eg Chinese word segmentation), generate a huge word vocabulary containing many low-frequency words, and also suffer from the ``out-of-vocabulary'' issue. Thus, several neural network models employ character as the minimum unit to derive the word representation (\\eg a CNN word encoder in ELMo~Peters-NAACL-2018). Recently, subword tokenizers have been widely used in Transformer based language models, typically including Byte-Pair Encoding tokenization, WordPiece tokenization and Unigram tokenization. HuggingFace has maintained an excellent online NLP course on tokenizerhttps://huggingface.co/learn/nlp-course/chapter6 with running examples, and we refer to the beginners to this course. Next, we briefly describe the three representative tokenization methods. $\\bullet$ Byte-Pair Encoding~(BPE) tokenization. BPE was originally proposed as a general data compression algorithm in 1994~Philip-1994-BPE, and then adapted to NLP for tokenization~Sennrich-ACL-2016-nueral. It starts with a set of basic symbols (\\eg the alphabets and boundary characters), and iteratively combine frequent pairs of two consecutive tokens in the corpus as new tokens (called merge). For each merge, the selection criterion is based on the co-occurrence frequency of two contiguous tokens: the top frequent pair would be selected. The merge process continues until it reaches the predefined size. Further, Byte-level BPE has been used to improve the tokenization quality for multilingual corpus (\\eg the text containing non-ASCII characters) by considering bytes as the basic symbols for merge. % Representative language models with this tokenization approach include GPT-2, BART, and LLaMA. $\\bullet$ WordPiece tokenization. WordPiece was a Google internal subword tokenization algorithm. It was originally proposed by Google in developing voice search systems~Mike-ICASSP-2012-Japanese. Then, it was used in the neural machine translation system in 2016~Wu-CoRR-2016, and was adopted as the word tokenizer for BERT in 2018~Devlin-NAACL-2019-BERT. WordPiece has a very similar idea with BPE by iteratively merging consecutive tokens, whereas taking a slightly different selection criterion for the merge. To conduct the merge, it first trains a language model and employs it to score all possible pairs. Then, at each merge, it selects the pair that leads to the most increase in the likelihood of training data. Since Google has't released the official implementation of the WordPiece algorithm, HuggingFace gives a more intuitive selection measure in its online NLP course: a pair is scored by dividing the co-occurrence count by the product of the occurrence counts of two tokens in the pair based on training corpus. $\\bullet$ Unigram tokenization. Unlike BPE and WordPiece, Unigram tokenization~Kudo-ACL-2018-Subword starts with a sufficiently large set of possible substrings or subtokens for a corpus, and iteratively removes the tokens in the current vocabulary until the expected vocabulary size is reached. As the selection criterion, it calculates the yielded increase in the likelihood of training corpus by assuming that some token was removed from current vocabulary. This step is conducted based on a trained unigram language model. To estimate the unigram language model, it adopts an expectationâ€“maximization~(EM) algorithm: at each iteration, we first find the currently optimal tokenization of words based on the old language model, and then re-estimate the probabilities of unigrams to update the language model. During this procedure, dynamic programming algorithms (\\ie the Viterbi algorithm) are used to efficiently find the optimal decomposition way of a word given the language model. Representative models that adopt this tokenization approach include T5 and mBART. Although it is expedient to leverage an existing tokenizer (\\eg OPT~Zhang-arxiv-2022-OPT and GPT-3~Brown-NeurIPS-2020-Language utilize the tokenizer of GPT-2~radford-blog-2019-language), using a tokenizer specially designed for the pre-training corpus can be highly beneficial~Scao-arxiv-2022-BLOOM, especially for the corpus that consists of diverse domains, languages, and {formats}. Therefore, recent LLMs often train the customized tokenizers specially for the pre-training corpus with the SentencePiece library~Kudo-EMNLP-2018-SentencePiece, which includes Byte-level BPE and Unigram tokenization. A note is that normalization techniques in BPE, such as NFKC~Davis-arxiv-2001-Unicode, may degrade the tokenization performance~Hoffmann-arxiv-2022-Training,Rae-arxiv-2021-Scaling,Scao-arxiv-2022-BLOOM. When extending existing LLMs (\\ie continual pre-training or instruction tuning), we should be also aware of the potential side effect with customized tokenizers. For example, LLaMA trains the BPE tokenizer based on a pre-training corpus mainly consisting of English texts, and the derived vocabulary might be less capable in processing non-English data, \\eg taking longer inference latency to generate Chinese texts. Discussion on Effect of Data Quality. % For pre-training, the quality of pre-training data is vital to the model capacities of LLMs. Existing work has shown that pre-training on the low-quality corpus, such as noisy, toxic, and duplicate data, would largely hurt the performance of models~Rae-arxiv-2021-Scaling, Lee-ACL-2022-Deduplicating, Kandpal-ICML-2022-Deduplicating, Hernandez-arxiv-2022-Scaling. Recent studies, such as T5~Raffel-JMLR-2020-Exploring, GLaM~Du-ICML-2022-GLaM, and Gopher~Rae-arxiv-2021-Scaling, have investigated the influence of data quality on the LLMs' capacities. By comparing the performance of models trained on the filtered and unfiltered corpus, they have reached the similar conclusion that pre-training LLMs on cleaned data can improve the model performance. % More specifically, the duplication of data may result in ``double descent'' (referring to the phenomenon of performance initially deteriorating and subsequently improving)~Hernandez-arxiv-2022-Scaling,Nakkiran-ICLR-2020-Deep, or even overwhelm the training process~Hernandez-arxiv-2022-Scaling. In addition, it has been shown that duplicate data degrades the ability of LLMs to copy from the context, which might further affect the generalization capacity of LLMs using in-context learning~Hernandez-arxiv-2022-Scaling. Therefore, as suggested in~Rae-arxiv-2021-Scaling, Scao-arxiv-2022-BLOOM, Chowdhery-arxiv-2022-PaLM, Longpre-arxiv-2023-pretrainer, it is essential to utilize preprocessing methods like quality filtering, toxic filtering and deduplication to carefully clean the pre-training corpus (as illustrated in Section~sec:data_pre_processing), to improve stability of the training process and avoid affecting the model performance.",
      "origin_cites_number": 50
    },
    {
      "section_title": "Data Scheduling",
      "level": "3",
      "content": "After data preprocessing, it is essential to design suitable strategies to schedule these multi-source data for pre-training a capable LLM. Generally, two key aspects should be paid close attention for data scheduling: the proportion of each data source (data mixture), and the order in which each data source is scheduled for training (data curriculum). Next, we discuss the two aspects in detail. An illustration of data scheduling has been presented in Figure~fig:data_schedule. Data Mixture. Since each kind of data source is closely related to the development of certain capacities for LLMs (referring to the discussions in Section~sec:data_collection), it is important to set a suitable distribution to mix these data. The data mixture is generally set in a global level (\\ie the distribution of the entire pre-training data), and can be also locally set to varied proportions at different training stages. During pre-training, data samples from different sources would be selected according to the mixture proportions: more data will be sampled from a data source with a larger weight. Typically, existing LLMs such as LLaMA~Touvron-arxiv-2023-LLaMA may employ upsampling or downsampling on the full data of each source to create specific data mixtures as pre-training data. As Figure~fig:source-ratio illustrates, existing LLMs use different data mixtures to construct the pre-training data. As a representative model, the pre-training data of LLaMA~Touvron-arxiv-2023-LLaMA mainly consists of webpages (over 80\\%), alongside 6.5\\% of code-heavy data from GitHub and StackExchange, 4.5\\% from books, and 2.5\\% of scientific data sourced from arXiv, which has become an important reference for training general-purpose LLMs. Furthermore, special data mixtures can be used to facilitate different purposes. For example, Falcon~Penedo-2023-arxiv-Refinedweb is trained on pure webpages, and CodeGen~nijkamp-arxiv-2022-Codegen largely increases the amount of code data. In practice, data mixture is often determined empirically, and we summarize several common strategies for finding an effective data mixture as follows: $\\bullet$ Increasing the diversity of data sources. Recent studies have empirically shown that training on excessive data about a certain domain would degrade the generalization capability of LLMs on other domains~Taylor-arxiv-2022-Galactica,Rae-arxiv-2021-Scaling. In contrast, increasing the data source heterogeneity (\\eg including diverse data sources) is critical for improving the downstream performance of LLMs~Longpre-arxiv-2023-pretrainer,Tirumala-2023-arXiv-D4,Shen-2023-arXiv-SlimPajamaDC. To further examine the effect of different data sources, some studies have conducted ablation experiments by removing each data source one by one, and pre-train LLMs with specially curated datasets~Longpre-arxiv-2023-pretrainer. It has been shown that dropping data sources with high heterogeneity (\\eg webpages) impacts LLM's abilities more severely than dropping sources with low heterogeneity (\\eg academic corpus). $\\bullet$ Optimizing data mixtures. In addition to manually setting the data mixtures, several studies have proposed to optimize the data mixtures for improving the model pre-training~Xie-arxiv-2023-DSIR,Xie-arxiv-2023-doremi. Given the target downstream tasks, one can select pre-training data with either higher proximity in the feature space~Xie-arxiv-2023-DSIR or those that provide positive influences on downstream task performance~Wang-2023-arXiv-farewell. % Further, to reduce the reliance of target tasks, DoReMi~Xie-arxiv-2023-doremi first trains a small reference model using given initial domain weights, and then trains another small proxy model, upweighting the domains on which the greatest discrepancies in likelihood between the two models are observed. Finally, the learned domain weights of the proxy model are applied to train a much larger LLM. In a more simple way, one can train several small language models with different data mixtures, and select the data mixture that leads to the most desirable performance. However, an assumption made in this approach is, when trained in a similar way, small models would resemble with large models in model abilities or behaviors, which may not always hold in practice. $\\bullet$ Specializing the targeted abilities. The model capacities of LLMs heavily rely on data selection and mixture, and one can boost the proportions of specific data sources to enhance certain model abilities~Rae-arxiv-2021-Scaling,Longpre-arxiv-2023-pretrainer. For example, the mathematical reasoning and coding abilities can be specially enhanced by training with more mathematical texts and code data, respectively. Furthermore, experimental results on the LAMBADA dataset~Paperno-ACL-2016-LAMBADA show that increasing the proportion of books data can improve the model capacity in capturing long-term dependencies from text, and increasing the proportion of the C4 dataset~Raffel-JMLR-2020-Exploring leads to performance improvement on the C4 validation dataset~Rae-arxiv-2021-Scaling. Generally, it is important to identify more implicit relations between data sources and model abilities. { To enhance specific skills such as mathematics and coding in LLMs, or to develop specialized LLMs, a practical way is to employ a multi-stage training approach, \\eg general and skill-specific data can be scheduled at two consecutive stages. This approach of training LLMs on varying sources or proportions of data across multiple stages is also known as ``data curriculum'', which will be introduced below. Data Curriculum. After preparing the data mixture, it is important to schedule the order that specific data is presented to LLMs for pre-training. It has been shown that, in some cases, to learn a certain skill, learning in a skill-set sequence (\\eg basic skills $\\rightarrow$ target skill) outperforms direct learning from a corpus focused solely on the target skill~Chen-2023-arXiv-skill,Roziere-arxiv-2023-codellama. Following the idea of curriculum learning~Bengio-2009-arXiv-curriculum, data curriculum has been proposed and widely used in model pre-training~Chen-2023-arXiv-skill,Xu-2023-arXiv-contrastive,Roziere-arxiv-2023-codellama,Tworkowski-arxiv-2023-Focused. It aims to organize different parts of pre-training data for LLMs in a specific order, \\eg starting with easy/general examples and progressively introducing more challenging/specialized ones. More generally, it can broadly refer to the adaptive adjustment of data proportions for different sources during pre-training. Existing work about data curriculum mainly focuses on continual pre-training, such as specialized coding LLMs (\\eg CodeLLaMA~Roziere-arxiv-2023-codellama) or long context LLMs (\\eg LongLLaMA~Tworkowski-arxiv-2023-Focused). However, it still lacks of more detailed report about data curriculum for general-purpose LLMs (\\eg LLaMA) in the literature. To determine data curriculum, a practical approach is to monitor the development of key abilities of LLMs based on specially constructed evaluation benchmarks, and then adaptively adjust the data mixture during pre-training. Next, we take three common abilities as examples to introduce how the concept of data curriculumWe utilize the symbol ``$\\rightarrow$'' to represent the data order in data curriculum. For example, ``2T webpage tokens $\\rightarrow$ 500B code tokens'' means that the LLM is firstly trained with 2T webpage tokens and subsequently with 500B code data tokens. applies in continual pre-training. $\\bullet$ Coding. To improve the coding ability of LLMs, CodeLLaMA~Roziere-arxiv-2023-codellama is developed based on LLaMA 2~Touvron-2023-llama2-arxiv (2T general tokens $\\rightarrow$ 500B code-heavy tokens), aiming to improve the code generation ability and retain natural language understanding skills. CodeLLaMA also provides a version that is further specialized to a certain programming language, namely CodeLLaMA-Python (2T general tokens $\\rightarrow$ 500B code-heavy tokens $\\rightarrow$ 100B Python-heavy tokens). $\\bullet$ Mathematics. Llemma~Azerbayev-arxiv-2023-llemma is proposed to enhance the mathematical capacities of general-purpose LLMs. It is developed based on CodeLLaMA. Although CodeLLaMA~Roziere-arxiv-2023-codellama mainly focuses on the coding ability, experiments have shown that it performs better than its base model LLaMA 2 on mathematics benchmarks~Azerbayev-arxiv-2023-llemma. % Based on CodeLLaMA, Llemma is continually trained on mixtures of scientific papers, web data containing mathematical text and code (2T general tokens $\\rightarrow$ 500B code-heavy tokens $\\rightarrow$ 50$\\sim$200B math-heavy tokens). Note that the pre-training data of Llemma also contains 5\\% general domain data as a form of regularization. $\\bullet$ Long context. Long context modeling is an important ability for LLMs, and many studies have explored extending the context windows of LLMs via continually training~Roziere-arxiv-2023-codellama,Tworkowski-arxiv-2023-Focused. With modifications on position embeddings (\\ie position interpolation) of RoPE-based LLMs~Touvron-2023-llama2-arxiv,Touvron-arxiv-2023-LLaMA,Chen-arxiv-2023-Extending, CodeLLaMA further extends the context window of LLaMA 2 (2.5T tokens with 4K context window $\\rightarrow$ 20B tokens with 16K context window). LongLLaMA~Tworkowski-arxiv-2023-Focused also achieves longer context window with the help of external memory and a unique training objective (1T tokens with 2K context window $\\rightarrow$ 10B tokens with 8K context window). {",
      "origin_cites_number": 28
    },
    {
      "section_title": "Summary of Data Preparation",
      "level": "3",
      "content": "In this part, we summarize the general procedure and key points to prepare pre-training data for LLMs, which are detailed in the following three aspects. } { $\\bullet$ Data collection. It is suggested to include diverse data sources in the pre-training data. Although Falcon~Penedo-2023-arxiv-Refinedweb shows that webpages alone can be employed to train powerful LLMs, a more typical approach is to also incorporate diverse high-quality text like code, books, scientific papers, \\etc. If a LLM is specialized with a certain skill, the proportion of corresponding data source should be increased accordingly. % For example, Gopher~Rae-arxiv-2021-Scaling and Chinchilla~Hoffmann-arxiv-2022-Training are trained with approximately 40\\% of data from books. PaLM~driess-arxiv-2023-palm and LaMDA~Thoppilan-CoRR-2022-LaMDA use approximately 50\\% conversational data. } { $\\bullet$ Data cleaning. After data collection, it is crucial to clean the raw corpus to enhance its quality as possible. First, deduplication is commonly used in existing work~Touvron-2023-llama2-arxiv,Tirumala-2023-arXiv-D4,Penedo-2023-arxiv-Refinedweb. Second, low-quality text, toxic content, and data with privacy concerns should be removed at different granularities (\\eg document, passage or sentence). In practice, both heuristic and classifier-based methods can be employed for quality and toxicity filtering (\\eg CCNet~Wenzek-2020-LREC-CCNet, fastText~Joulin-2017-EACL-fasttext, and Data-Juicer~chen-2023-arXiv-DataJuicer). Third, with the cleaned data, one can further unify or specify the format for pre-training data, and perform the tokenization by training the tokenizer on the filtered and deduplicated corpus with libraries like SentencePiece~Kudo-EMNLP-2018-SentencePiece. } { $\\bullet$ Data scheduling. % With the preprocessed data, the next step is to determine the data mixture and the specific order of data for pre-training LLMs. To determine both settings, a practical way is to first train several small language models with multiple candidate plans and then select a good plan among them~Xie-arxiv-2023-doremi. Overall, it is more difficult to find a suitable data curriculum. In practice, one can monitor the performance of intermediate model checkpoints on specific evaluation benchmarks, and dynamically tune the data mixture and distribution during pre-training. In this process, it is also useful to explore the potential relations between data sources and model abilities to instruct the design of data curriculum. }",
      "origin_cites_number": 11
    },
    {
      "section_title": "Architecture",
      "level": "2",
      "content": "In this section, we review the architecture design of LLMs, \\ie mainstream architecture, pre-training objective, and detailed configuration. Table~model_card presents the model cards of several representative LLMs with public details. table*[htb] \\centering Model cards of several selected LLMs with public configuration details. Here, PE denotes position embedding, \\#L denotes the number of layers, \\#H denotes the number of attention heads, $d_{model$ denotes the size of hidden states, and MCL denotes the maximum context length during training.} tabular{lcrccccrrrr} \\toprule Model&Category&Size&Normalization&PE&Activation&Bias&\\#L&\\#H&$d_{model$}&MCL \\\\ \\midrule GPT3~Brown-NeurIPS-2020-Language&Causal decoder&175B&Pre LayerNorm&Learned&GeLU&\\checkmark&96&96&12288&2048\\\\ PanGU-~$\\alpha$~Zeng-arxiv-2021-PanGualpha&Causal decoder&207B&Pre LayerNorm&Learned&GeLU&\\checkmark&64&128&16384&1024\\\\ OPT~Zhang-arxiv-2022-OPT&Causal decoder&175B&Pre LayerNorm&Learned&ReLU&\\checkmark&96&96&12288&2048\\\\ PaLM~Chowdhery-arxiv-2022-PaLM&Causal decoder&540B&Pre LayerNorm&RoPE&SwiGLU&$\\times$&118&48&18432&2048\\\\ BLOOM~Scao-arxiv-2022-BLOOM&Causal decoder&176B&Pre LayerNorm&ALiBi&GeLU&\\checkmark&70&112&14336& 2048\\\\ MT-NLG~Smith-CoRR-2022-Using&Causal decoder&530B&-&-&-&-&105&128&20480& 2048\\\\ Gopher~Rae-arxiv-2021-Scaling&Causal decoder&280B&Pre RMSNorm&Relative&-&-& 80&128&16384&2048 \\\\ Chinchilla~Hoffmann-arxiv-2022-Training&Causal decoder&70B&Pre RMSNorm&Relative&-&-&80&64&8192&-\\\\ Galactica~Taylor-arxiv-2022-Galactica&Causal decoder&120B&Pre LayerNorm&Learned&GeLU&$\\times$&96&80&10240 &2048\\\\ LaMDA~Thoppilan-CoRR-2022-LaMDA&Causal decoder&137B&-&Relative&GeGLU&-&64&128&8192&-\\\\ Jurassic-1~lieber-2021-jurassic&Causal decoder&178B&Pre LayerNorm&Learned&GeLU&\\checkmark&76 &96&13824& 2048 \\\\ LLaMA~Touvron-arxiv-2023-LLaMA&Causal decoder&65B&Pre RMSNorm&RoPE&SwiGLU&$\\times$&80&64&8192&2048\\\\ LLaMA 2~Touvron-2023-llama2-arxiv &Causal decoder&70B&Pre RMSNorm&RoPE&SwiGLU&$\\times$&80&64&8192& 4096 \\\\ Falcon~Penedo-2023-arxiv-Refinedweb&Causal decoder&40B&Pre LayerNorm&RoPE&GeLU&$\\times$&60&64&8192&2048\\\\ GLM-130B~Zeng-arxiv-2022-GLM&Prefix decoder&130B&Post DeepNorm&RoPE&GeGLU&\\checkmark&70&96&12288&2048\\\\ T5~Raffel-JMLR-2020-Exploring&Encoder-decoder&11B&Pre RMSNorm&Relative&ReLU&$\\times$&24&128&1024&512\\\\ \\bottomrule tabular table*",
      "origin_cites_number": 16
    },
    {
      "section_title": "Typical Architectures",
      "level": "3",
      "content": "figure*[htb] \\includegraphics[width=\\textwidth]{images/architectures.pdf} A comparison of the attention patterns in three mainstream architectures. Here, the blue, green, yellow and grey rounded rectangles indicate the attention between prefix tokens, attention between prefix and target tokens, attention between target tokens, and masked attention respectively. figure* Due to the excellent parallelizability and capacity, the Transformer architecture~Vaswani-NIPS-2017-Attention has become the de facto backbone to develop various LLMs, making it possible to scale language models to hundreds or thousands of billions of parameters. In general, the mainstream architectures of existing LLMs can be roughly categorized into three major types, namely encoder-decoder, causal decoder, and prefix decoder, % as shown in Figure~fig:architectures. Encoder-decoder Architecture. The vanilla Transformer model is built on the encoder-decoder architecture~Vaswani-NIPS-2017-Attention, which consists of two stacks of Transformer blocks as the encoder and decoder, respectively. The encoder adopts stacked multi-head self-attention layers to encode the input sequence for generating its latent representations, while the decoder performs cross-attention on these representations and autoregressively generates the target sequence. % Encoder-decoder PLMs (\\eg T5~Raffel-JMLR-2020-Exploring and BART~Lewis-ACL-2020-BART) have shown effectiveness on a variety of NLP tasks. { So far, there are only a small number of LLMs that are built based on the encoder-decoder architecture, \\eg Flan-T5~Chung-arxiv-2022-Scaling. We leave a detailed discussion about the architecture selection in Section~sec-summary-arc. } Causal Decoder Architecture. The causal decoder architecture incorporates the unidirectional attention mask, to guarantee that each input token can only attend to the past tokens and itself. The input and output tokens are processed in the same fashion through the decoder. As representative language models of this architecture, the GPT-series models~radford-openai-2018-improving,radford-blog-2019-language,Brown-NeurIPS-2020-Language are developed based on the causal-decoder architecture. In particular, GPT-3~Brown-NeurIPS-2020-Language has successfully demonstrated the effectiveness of this architecture, also showing an amazing in-context learning capability of LLMs. Interestingly, GPT-1~radford-openai-2018-improving and GPT-2~radford-blog-2019-language do not exhibit such superior abilities as those in GPT-3, and it seems that scaling plays an important role in increasing the model capacity of this model architecture. So far, the causal decoders have been widely adopted as the architecture of LLMs by various existing LLMs, such as OPT~Zhang-arxiv-2022-OPT, BLOOM~Scao-arxiv-2022-BLOOM, and Gopher~Rae-arxiv-2021-Scaling. { Note that both the causal decoder and prefix decoder discussed next belong to decoder-only architectures. When mentioning ``{decoder-only architecture}'', it mainly refers to the causal decoder architecture in existing literature, unless specified. } Prefix Decoder Architecture. The prefix decoder architecture {(\\aka non-causal decoder~Zhang-ICML-2022-Examining)} revises the masking mechanism of causal decoders, to enable performing bidirectional attention over the prefix tokens~Dong-NIPS-2019-Unified and unidirectional attention only on generated tokens. In this way, like the encoder-decoder architecture, the prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens one by one, where the same parameters are shared during encoding and decoding. Instead of pre-training from scratch, a practical suggestion is to continually train causal decoders and then convert them into prefix decoders for accelerating convergence~Wang-ICML-2022-What, \\eg U-PaLM~Tay-arxiv-2022-Transcending is derived from PaLM~Chowdhery-arxiv-2022-PaLM. Existing representative LLMs based on prefix decoders include GLM-130B~Zeng-arxiv-2022-GLM and U-PaLM~Tay-arxiv-2022-Transcending. Mixture-of-Experts. For the above three types of architectures, we can further extend them via the mixture-of-experts (MoE) scaling, in which a subset of neural network weights for each input are sparsely activated, \\eg Switch Transformer~Fedus-JMLR-2021-Switch and GLaM~Du-ICML-2022-GLaM. % {The major merit is that MoE is a flexible way to scale up the model parameter while maintaining a constant computational cost~Fedus-JMLR-2021-Switch.} It has been shown that substantial performance improvement can be observed by increasing either the number of experts or the total parameter size~Clark-ICML-2022-Unified. {Despite the merits, training large MoE models may suffer from instability issues due to the complex, hard-switching nature of the routing operation. To enhance the training stability of MoE-based language models, techniques such as selectively using high-precision tensors in the routing module or initializing the model with a smaller range have been introduced~Fedus-JMLR-2021-Switch. More recently, there is widespread speculation that GPT-4 has been developed based on the MoE architecture, but without official verification. } blue{ } Emergent Architectures. The conventional Transformer architecture typically suffers from quadratic computational complexity with respect to sequence length, resulting in a high processing cost for dealing with long inputs. % To improve efficiency, recent studies aim to devise new architectures for language modeling, most based on parameterized state space models {(SSM)~gu-2022-iclr-efficiently}, which can be viewed as a combination of RNN and CNN. On the one hand, SSM can generate outputs recursively like RNN, meaning that they only need to refer to the single previous state during decoding. It makes the decoding process more efficient as it eliminates the need to revisit all previous states as in conventional Transformers. On the other hand, these models have the capability to encode an entire sequence in parallel like Transformers via convolution computation. Thus, they can benefit from the parallelism of GPUs with techniques such as Parallel Scan~smith-2023-iclr-s5,orvieto-2023-icml-lru, FFT~poli-2023-icml-hyena,peng-2023-arxiv-rwkv, and Chunkwise Recurrent~sun-2023-arxiv-retnet. Despite the high computation efficiency of SSMs, their performance still lags behind Transformer. Thus, several variants of SSM have been proposed, including Mamba~gu-arxiv-2023-mamba, RetNet~sun-2023-arxiv-retnet, RWKV~peng-23-arxiv-rwkv, and Hyena~poli-2023-icml-hyena. $\\bullet$ Mamba. Mamba~gu-arxiv-2023-mamba aims to selectively {filter out or remember} information during state update. {It replaces the original fixed parameters of SSM layers with functions of the input}, selectively {filtering out} information of the previous state and {the current input depending on the current input}. Compared with traditional SSMs, Mamba has demonstrated improved text modeling capacities. $\\bullet$ RWKV. RWKV~peng-23-arxiv-rwkv combines the advantages of Transformer and RNN. It employs time-mixing modules, \\ie RNN with gating, and channel-mixing modules that are special {feedforward neural networks}~peng-23-arxiv-rwkv. Within these modules, token shift, a linear combination of the current and previous token, is used instead of the token representation as the input}. {$\\bullet$ RetNet. RetNet~sun-2023-arxiv-retnet proposes multi-scale retention (MSR) to replace the attention module in Transformer. Similar to linear attention, in the MSR module, the input is first mapped into query, key, and value, and the product of key and value is employed to update the state. Then, the query is used to project the state into the output. Similar to traditional SSMs, RetNet keeps the parallel and recurrent computation capacity at the same time.} {$\\bullet$ Hyena. Hyena employs long convolution to replace the attention module. In the long convolution module, the filters based on relative positions are used to aggregate information at different positions into the middle representations, and gating functions are employed to further project intermediate representations into the final output. However, due to the long convolution, Hyena can not infer like RNN and must explicitly access all previous states.}",
      "origin_cites_number": 36
    },
    {
      "section_title": "Detailed Configuration",
      "level": "3",
      "content": "table*[htb] \\centering Detailed formulations for the network configurations. Here, Sublayer denotes a FFN or a self-attention module in a Transformer layer, $d$ denotes the size of hidden states, $\\mathbf{p_i$ denotes position embedding at position $i$, $A_{ij}$ denotes the attention score between a query and a key, $r_{i-j}$ denotes a learnable scalar based on the offset between the query and the key, and $R_{\\Theta, t}$ denotes a rotary matrix with rotation degree $t\\cdot\\Theta$.} tabular{c|c|l} \\toprule Configuration& Method & Equation\\\\\\midrule 3{*}{Normalization position} &Post Norm~Vaswani-NIPS-2017-Attention & $Norm(x + Sublayer(x))$ \\\\ &Pre Norm~radford-blog-2019-language &$x+Sublayer(Norm(x))$ \\\\ &Sandwich Norm~Ding-NIPS-2021-CogView & $x+Norm(Sublayer(Norm(x)))$\\\\ \\midrule 3{*}{Normalization method}& LayerNorm~Jimmy-arxiv-2016-Layer&$ \\mathbf {x -\\mathbf \\mu}{\\mathbf \\sigma}\\cdot \\gamma +\\beta, ~~~ \\mathbf \\mu=\\frac 1 d \\sum_{i=1}^d x_i, ~~~ \\mathbf \\sigma=\\frac 1 d \\sum_{i=1^d ( x_i-\\mathbf \\mu))^2}$ \\\\ & RMSNorm~Zhang-NIPS-2019-Root& $ \\frac {\\mathbf x}{RMS(\\mathbf x)} \\cdot \\gamma, ~~~ RMS(\\mathbf x)=\\sqrt {\\frac 1 d \\sum_{i=1}^d x_i^2}$\\\\ & DeepNorm~Wang-arxiv-2022-DeepNet& $\\mathrm {LayerNorm}(\\alpha \\cdot \\mathbf x + Sublayer(\\mathbf x))$\\\\ \\midrule 5{*}{Activation function}& ReLU~Vinod-ICML-2010-Rectified& $\\mathrm {ReLU}(x) = \\max(x,0)$\\\\ & GeLU~Wang-EMNLP-2018-GLUE&$\\mathrm {GeLU}(x) = 0.5x \\otimes [1+erf(x/2)], ~~~ erf(x)=\\frac 2 {\\sqrt \\pi}\\int_0^x e^{-t^2} dt $\\\\ &Swish~Ramachandran-arXiv-2017-searching & $Swish(x) = \\mathbf x \\otimes sigmoid(x) $\\\\ & SwiGLU~Shazeer-arxiv-2020-GLU&$SwiGLU(x_1,x_2) = Swish(x_1)\\otimes x_2$\\\\ & GeGLU~Shazeer-arxiv-2020-GLU& $GeGLU(x_1,x_2) = GeLU(x_1)\\otimes x_2$ \\\\ \\midrule 4{*}{Position embedding}& Absolute~Vaswani-NIPS-2017-Attention&$\\mathbf x_i = \\mathbf x_i + \\mathbf p_i$ \\\\ &Relative~Raffel-JMLR-2020-Exploring& $A_{ij} = \\mathbf W_q\\mathbf x_i \\mathbf x_j^T \\mathbf W_k^T + r_{i-j}$\\\\ &RoPE~Su-arxiv-2021-Roformer& $A_{ij} = \\mathbf W_q\\mathbf x_i \\mathbf R_{\\Theta, i-j}\\mathbf x_j^T\\mathbf W_k^T = (\\mathbf W_q\\mathbf x_i \\mathbf R_{\\Theta, i})(\\mathbf W_k \\mathbf x_j R_{\\Theta, j })^T$\\\\ &ALiBi~Press-ICLR-2022-Train& $A_{ij} = \\mathbf W_q \\mathbf x_i \\mathbf x_j^T\\mathbf W_k^T - m (i-j)$\\\\ \\bottomrule tabular table* Since the launch of Transformer~Vaswani-NIPS-2017-Attention, various improvements have been proposed to enhance its training stability, performance, and computational efficiency. In this part, we will discuss the corresponding configurations for four major parts of the Transformer, including normalization, position embeddings, activation functions, and attention and bias. To make this survey more self-contained, we present the detailed formulations for these configurations in Table~tab:detailed_configuration. Normalization Methods. {Training instability is a challenging issue for pre-training LLMs. To alleviate this issue, normalization is a widely adopted strategy to stabilize the training of neural networks. In the vanilla Transformer~Vaswani-NIPS-2017-Attention, LayerNorm~Jimmy-arxiv-2016-Layer is employed. Recently, several advanced normalization techniques have been proposed as alternatives to LayerNorm, \\eg RMSNorm, and DeepNorm. } { $\\bullet$ LayerNorm. In the early research, BatchNorm~Ioffe-2015-ICML-Batch is a commonly used normalization method. However, it is difficult to deal with sequence data of variable lengths and small-batch data. Thus, LayerNorm~Jimmy-arxiv-2016-Layer is introduced to conduct layerwise normalization. Specifically, the mean and variance over all activations per layer are calculated to re-center and re-scale the activations. } { $\\bullet$ RMSNorm. To improve the training speed of LayerNorm (LN), RMSNorm~Zhang-NIPS-2019-Root is proposed by re-scaling the activations with only the root mean square (RMS) of the summed activations, instead of the mean and variance. Related research has demonstrated its superiority in training speed and performance on Transformer~Narang-EMNLP-2021-Do. Representative models that adopt RMSNorm include Gopher~Rae-arxiv-2021-Scaling and Chinchilla~Hoffmann-arxiv-2022-Training.} { $\\bullet$ DeepNorm. DeepNorm is proposed by Microsoft~Wang-arxiv-2022-DeepNet to stabilize the training of deep Transformers. With DeepNorm as residual connections, Transformers can be scaled up to 1,000 layers~Wang-arxiv-2022-DeepNet, which has shown the advantages of stability and good performance. It has been adopted by GLM-130B~Zeng-arxiv-2022-GLM. } Normalization Position. {In addition to the normalization method, normalization position also plays a crucial role in the LLMs. There are generally three choices for the normalization position, \\ie post-LN, pre-LN, and sandwich-LN. } { $\\bullet$ Post-LN. Post-LN is used in the vanilla Transformer~Vaswani-NIPS-2017-Attention, which is placed between residual blocks. However, existing work has found that the training of Transformers with post-LN tends to be instable due to the large gradients near the output layer~Xiong-ICML-2020-On. Thus, post-LN is rarely employed in existing LLMs except combined with other strategies (\\eg combining post-LN with pre-LN in GLM-130B~Zeng-arxiv-2022-GLM). } { $\\bullet$ Pre-LN. Different from post-LN, pre-LN~Baevski-2019-ICLR-Adaptive is applied before each sub-layer, and an additional LN is placed before the final prediction. Compared with post-LN, the Transformers with pre-LN are more stable in training. However, it performs worse than the variants with post-LN~liu-2020-EMNLP-Understanding. Despite the decreasing performance, most LLMs still adopt pre-LN due to the training stability. } { However, one exception is that pre-LN has been found unstable in GLM when training models more than 100B parameters~Zeng-arxiv-2022-GLM. } $\\bullet$ Sandwich-LN. Based on pre-LN, Sandwich-LN~Ding-NIPS-2021-CogView adds extra LN before the residual connections to avoid the {value explosion issues in Transformer layer outputs.} However, it has been found that Sandwich-LN sometimes fails to stabilize the training of LLMs and may lead to the collapse of training~Zeng-arxiv-2022-GLM. Activation Functions. To obtain good performance, activation functions also need to be properly set in feed-forward networks. In existing LLMs, GeLU activations~Dan-arxiv-2016-Gaussian are widely used. % Specially, in the latest LLMs (\\eg PaLM and LaMDA), variants of GLU activation~Dauphin-ICML-2017-Language,Shazeer-arxiv-2020-GLU have also been utilized, especially the SwiGLU and GeGLU variants, which often achieve better performance in practice~Narang-EMNLP-2021-Do. However, compared with GeLU, they require extra parameters (about 50\\%) in the feed-forward networks~Le-EMNLP-2022-What. Position Embeddings. Since the self-attention modules in Transformer are permutation equivariant, position embeddings~(PE) are employed to inject absolute or relative position information for modeling sequences. { $\\bullet$ Absolute position embedding. In the vanilla Transformer~Vaswani-NIPS-2017-Attention, absolute position embeddings are employed. At the bottoms of the encoder and the decoder, the absolute positional embeddings are added to the input embeddings. There are two variants of absolute position embeddings proposed in the vanilla Transformer~Vaswani-NIPS-2017-Attention, \\ie sinusoidal and learned position embeddings, where the latter is commonly used in existing pre-trained language models. } % $\\bullet$ Relative position embedding. Unlike absolute position embeddings, relative positional embeddings are generated according to the offsets between keys and queries~shaw-2018-acl-self. % {A popular variant of relative PE was introduced in Transformer-XL~dai-2019-acl-transformer,Yang-NeurIPS-2019-xlnet. The calculation of attention scores between keys and queries has been modified to introduce learnable embeddings corresponding to relative positions.} {T5~Raffel-JMLR-2020-Exploring further simplified relative positional embeddings, which was subsequently adopted by Gopher~Rae-arxiv-2021-Scaling.} Specifically, it adds learnable scalars to the attention scores, where the scalars are calculated based on the distances between the positions of the query and the key. Compared with the absolute PE, Transformers with relative position embedding can generalize to sequences longer than those sequences for training, \\ie extrapolation~Press-ICLR-2022-Train. $\\bullet$ Rotary position embedding. {Rotary position embedding (RoPE)~Su-arxiv-2021-Roformer sets specific rotatory matrices based on the absolute position of each key or query. The scores between keys and queries can be computed with relative position information (Table~tab:detailed_configuration).} {RoPE combines each consecutive pair of elements in query and key vectors as a dimension, so there are $d/2$ dimensions for an original $d$-length embedding. For each dimension $i \\in \\{1, \\dots, d/2\\}$, the pair of involved elements will rotate based on the rotation angle $t\\cdot \\theta_i$, where $t$ denotes the position index and $\\theta_i$ is the basis in the dimension. Following sinusoidal position embeddings~Vaswani-NIPS-2017-Attention, RoPE defines the basis $\\theta_i$ as an exponentiation of the base $b$ (set to $10000$ by default):} equation \\Theta = \\{\\theta_i = b^{-2(i-1)/d} | i \\in \\{1, 2, \\dots , d/2 \\}\\}. equation {Furthermore, a recent study~Peng-arxiv-2023-Yarn defines the distance required to rotate one cycle ($2\\pi$) for each dimension as wavelength:} equation \\lambda_i = 2\\pi b^{2(i-1)/d}=2 \\pi / \\theta_i. equation Due to the excellent performance and the long-term decay property, RoPE is widely adopted in the latest LLMs, \\eg PaLM~Chowdhery-arxiv-2022-PaLM and LLaMA~Touvron-arxiv-2023-LLaMA. Based on RoPE, xPos~Sun-2022-arxiv-Length further improves the translation invariance and length extrapolation of Transformer. At each dimension of the rotation angle vector, xPos adds a special exponential decay that is smaller when the basis is larger. It can alleviate the unstable phenomenon {during training} as the distance increases. $\\bullet$ ALiBi. ALiBi~Press-ICLR-2022-Train is proposed to improve the extrapolation of Transformer. Similar to relative position embedding, it biases attention scores with a penalty based on the distances between keys and queries. {Different from the relative positional embedding methods like T5~Raffel-JMLR-2020-Exploring, the penalty scores in ALiBi are pre-defined without any trainable parameters.} Empirical results in~Press-ICLR-2022-Train have shown that ALiBi has {a better extrapolation performance on sequences that are longer than those for training than several popular position embedding methods such as sinusoidal PE~Vaswani-NIPS-2017-Attention, RoPE~Su-arxiv-2021-Roformer, and T5 bias~Raffel-JMLR-2020-Exploring. } In addition, it has been shown that ALiBi can also improve training stability in BLOOM~Scao-arxiv-2022-BLOOM. Attention. Attention mechanism is a critical component of Transformer. It allows the tokens across the sequence to interact with each other and compute the representations of the input and output sequence. { $\\bullet$ Full attention. In the vanilla Transformer~Vaswani-NIPS-2017-Attention, the attention mechanism is conducted in a pairwise way, considering the relations between all token pairs in a sequence. It adopts scaled dot-product attention, in which the hidden states are mapped into queries, keys, and values. Additionally, Transformer uses multi-head attention instead of single attention, projecting the queries, keys, and values with different projections in different heads. The concatenation of the output of each head is taken as the final output.} { $\\bullet$ Sparse attention. A crucial challenge of full attention is the quadratic computational complexity, which becomes a burden when dealing with long sequences. Therefore, various efficient Transformer variants are proposed to reduce the computational complexity of the attention mechanism~Peng-ICLR-2021-Random, Zaheer-NIPS-2020-Big. For instance, locally banded sparse attention (\\ie Factorized Attention~Child-arxiv-2019-Generating has been adopted in GPT-3~Brown-NeurIPS-2020-Language. Instead of the whole sequence, each query can only attend to a subset of tokens based on the positions. } $\\bullet$ Multi-query/grouped-query attention. Multi-query attention refers to the attention variant where different heads share {the same linear transformation matrices on the keys and values~Shazeer-2019-arxiv-Fast.} It achieves higher inference speed {with only a minor sacrifice in model quality.} Representative models with multi-query attention include PaLM~Chowdhery-arxiv-2022-PaLM and StarCoder~Li-2023-arxiv-Starcoder. % {To make a trade-off between multi-query attention and multi-head attention, grouped-query attention (GQA)~Ainslie-2023-arxiv-gqa has been explored. In GQA, heads are assigned into different groups, and those heads that belong to the same group will share the same transformation matrices. Specially, GQA has been adopted and empirically tested in the recently released LLaMA 2 model~Touvron-2023-llama2-arxiv.} $\\bullet$ FlashAttention. Different from most existing approximate attention methods that trade-off model quality to improve the computing efficiency, FlashAttention~Dao-NeurIPS-2020-FLASH proposes to optimize the speed and memory consumption of attention modules on GPUs from an IO-aware perspective. There exist different levels of memory on modern GPUs, \\eg SRAM with a fast IO and HBM with a relatively slow IO. FlashAttention organizes the input into blocks and introduces necessary recomputation, both to make better use of the fast memory SRAM. Implemented as a fused kernel in CUDA, FlashAttention has been integrated into PyTorch~Paszke-NeurIPS-2019-Pytorch, DeepSpeed~Rasley-KDD-2020-DeepSpeed, and Megatron-LM~Shoeybi-arXiv-2019-Megatron. {The updated version FlashAttention-2~Dao-2023-arxiv-flashattention2 further optimizes the work partitioning of GPU thread blocks and warps, leading to around 2$\\times$ speedup when compared to the original FlashAttention.} $\\bullet$ PagedAttention. % {It has been observed when LLM are deployed on servers, GPU memory is largely occupied by cached attention key and value tensors (called KV cache). The major reason is that the input lengths are often varied, leading to fragmentation and over-reservation issues. Inspired by the classic paging technique in operating systems, PagedAttention has been proposed to improve the memory efficiency and throughput of deployed LLMs~vllm-pagedattention. In detail, PagedAttention partitions each sequence into subsequences, and the corresponding KV caches of these subsequences are allocated into non-contiguous physical blocks. The paging technique increases the GPU utilization and enables efficient memory sharing in parallel sampling.} To put all these discussions together, we summarize the suggestions from existing literature for detailed configuration. For stronger generalization and training stability, it is suggested to choose the pre RMSNorm for layer normalization, and SwiGLU or GeGLU as the activation function. % {In addition, LN may not be used immediately after embedding layers, which is likely to incur performance degradation.} As for position embeddings, {RoPE} or ALiBi is a better choice since it performs better on long sequences. %",
      "origin_cites_number": 74
    },
    {
      "section_title": "Pre-training Tasks",
      "level": "3",
      "content": "Pre-training plays a key role that encodes general knowledge from large-scale corpus into the massive model parameters. For training LLMs, there are two commonly used pre-training tasks, namely language modeling and denoising autoencoding. % Language Modeling. The language modeling task (LM) is the most commonly used objective to pre-train decoder-only LLMs, \\eg GPT3~Brown-NeurIPS-2020-Language and PaLM~Chowdhery-arxiv-2022-PaLM. Given a sequence of tokens $x=\\{x_1,\\dots,x_n\\}$, the LM task aims to autoregressively predict the target tokens % {$x_i$ based on the preceding tokens $x_{<i}$ in a sequence}. A general training objective is to maximize the following likelihood: % equation L_{LM}(x)=\\sum_{i=1}^n \\log P(x_i|x_{<i}). equation Since most language tasks can be cast as the prediction problem based on the input, % {these decoder-only} LLMs might be potentially advantageous to implicitly learn how to accomplish these tasks in a unified LM way. Some studies have also revealed that % {decoder-only} LLMs can be naturally transferred to certain tasks by autoregressively predicting the next tokens~radford-blog-2019-language,Brown-NeurIPS-2020-Language, without fine-tuning. An important variant of LM is the prefix language modeling task, % {which is designed for pre-training models with the prefix decoder architecture. The tokens within a randomly selected prefix would not be used in computing the loss of prefix language modeling.} { With the same amount of tokens seen during pre-training, prefix language modeling performs slightly worse than language modeling, since fewer tokens in the sequence are involved for model pre-training~Wang-ICML-2022-What.} Denoising Autoencoding. { In addition to conventional LM, the denoising autoencoding task (DAE) has also been widely used to pre-train language models~Lewis-ACL-2020-BART,Raffel-JMLR-2020-Exploring. The inputs $x_{\\backslash \\mathbf{x}}$ for DAE task are corrupted text with randomly replaced spans. Then, the language models are trained to recover the replaced tokens $\\mathbf{x}$. Formally, the training objective of DAE is denoted as follows:} equation L_{DAE}(x)= \\log P(\\mathbf{x}|x_{\\backslash \\mathbf{x}}). equation However, the DAE task seems to be more complicated in implementation than LM task. As a result, it has not been widely used to pre-train large language models. Existing LLMs that take DAE as pre-training objectives include T5~Raffel-JMLR-2020-Exploring and GLM-130B~Zeng-arxiv-2022-GLM. These models are mainly trained to recover the replaced spans in an autoregressive way. Mixture-of-Denoisers. Mixture-of-Denoisers (MoD)~Tay-arxiv-2022-UL2, also known as UL2 loss, was introduced as a unified objective for pre-training language models. MoD regards both LM and DAE objectives as different types of denoising tasks, namely % {S-denoiser (LM), R-denoiser (DAE, short span and low corruption), and X-denoiser (DAE, long span or high corruption).} Among the three denoising tasks, S-denoiser is similar to the conventional LM objective (Equation~eq:lm), while R-denoiser and X-denoiser are similar to DAE objectives (Equation~eq:dae) but differ from each other in the lengths of spans and ratio of corrupted text. {For input sentences started with different special tokens (\\ie \\{[R], [S], [X]\\}), the model will be optimized using the corresponding denoisers.} MoD has been applied in the latest PaLM 2 model~Anil-arxiv-2023-palm2.",
      "origin_cites_number": 9
    },
    {
      "section_title": "Decoding Strategy",
      "level": "3",
      "content": "After the LLMs have been pre-trained, it is essential to employ a specific decoding strategy to generate the appropriate output from the LLMs. Background. We start the discussion with the prevalent decoder-only architecture, and introduce the auto-regressive decoding mechanism. Since such LLMs are pre-trained based on the language modeling task (Equation~eq:lm), a basic decoding method is greedy search that predicts the most likely token at each step based on the previously generated tokens, formally modeled as: equation { x_i = x{\\arg\\max} P(x |x_{<i}),} equation where $x_i$ is the token with the highest probability at $i$-th step of generation conditioned on the context $x_{<i}$. For instance in Figure~fig:decoding-example, when predicting the next token of the sentence ``I am sleepy. I start a pot of'', greedy search selects the token ``coffee'' which has the highest probability at the current step. Greedy search can achieve satisfactory results in text generation tasks (\\eg machine translation and text summarization), in which the output is highly dependent on the input~Murray-WMT-2018-Correcting. However, in terms of open-ended generation tasks (\\eg story generation and dialog), greedy search sometimes tends to generate awkward and repetitive sentences~Holtzman-2020-ICLR-The. As another alternative decoding strategy, sampling-based methods are proposed to randomly select the next token based on the probability distribution to enhance the randomness and diversity during generation: equation x_i \\sim P(x|x_{<i}). equation For the example in Figure~fig:decoding-example, sampling-based methods will sample the word ``coffee'' with higher probability while also retaining the possibilities of selecting the rest words, ``water'', ``tea'', ``rice'', \\etc. Not limited to the decoder-only architecture, these two decoding methods can be generally applied to encoder-decoder models and prefix decoder models in a similar way. Improvement for Greedy Search. Selecting the token with the highest probability at each step may result in overlooking a sentence with a higher overall probability but a lower local estimation. Next, we introduce several improvement strategies to alleviate this issue. $\\bullet$ Beam search. Beam search~CMU-book-1977-speech retains the sentences with the $n$ (beam size) highest probabilities at each step during the decoding process, and finally selects the generated response with the top probability. Typically, the beam size is configured within the range of 3 to 6. However, opting for a larger beam size might result in a decline in performance~Koehn-ACL-2017-Six. $\\bullet$ Length penalty. Since beam search favours shorter sentences, imposing length penalty (\\aka length normalization) is a commonly used technique~Wu-arxiv-2016-Google to overcome this issue, which normalizes the sentence probability according to the sentence length (divided by an exponential power $\\alpha$ of the length). Besides, some researchers~Paulus-iclr-2018-A propose to penalize the generation of previously generated tokens or $n$-grams to alleviate the issue of repetitive generation. In addition, diverse beam search~Vijayakumar-arxiv-2016-Diverse can be leveraged to produce a set of diverse outputs based on the same input. Improvement for Random Sampling. Sampling-based methods sample the token over the whole vocabulary, which may select wrong or irrelevant tokens (\\eg ``happy'' and ``Boh'' in Figure~fig:decoding-example) based on the context. To improve the generation quality, several strategies have been proposed for mitigating or preventing the selection of words with exceedingly low probabilities. $\\bullet$ Temperature sampling. To modulate the randomness of sampling, a practical method is to adjust the temperature coefficient of the softmax function for computing the probability of the $j$-th token over the vocabulary: equation P(x_j|x_{<i}) = \\exp{(l_j/t)}{\\sum_{j'} (l_{j'/t)}}, equation where $l_{j'}$ is the logits of each word and $t$ is the temperature coefficient. Reducing the temperature $t$ increases the chance of selecting words with high probabilities while decreases the chances of selecting words with low probabilities. When $t$ is set to 1, it becomes the default random sampling; when $t$ is approaching 0, it is equivalent to greedy search. In addition, when $t$ goes to infinity, it degenerates to uniform sampling. $\\bullet$ Top-$k$ sampling. Different from temperature sampling, top-$k$ sampling directly truncates the tokens with lower probability and only samples from the tokens with the top $k$ highest probabilities~Fan-2018-ACL-Hierarchical. For example in Figure~fig:decoding-example, top-$5$ sampling will sample from the words ``coffee'', ``water'', ``tea'', ``rice'', and ``chai'' from their re-scaled probabilities. $\\bullet$ Top-$p$ sampling. Since top-$k$ sampling does not consider the overall possibility distribution, a constant value of $k$ may be not be suitable for different contexts. Therefore, top-$p$ sampling (\\aka nucleus sampling) is proposed by sampling from the smallest set having a cumulative probability above (or equal to) $p$~Holtzman-2020-ICLR-The. In practice, the smallest set can be constructed by gradually adding tokens from the vocabulary sorted in descending order of generative probability, until their cumulative value exceeds $p$. Recently, researchers have also explored other sampling strategies for LLMs. For instance, $\\eta$-sampling~Hewitt-emnlp-2022-Truncation further improves top-$p$ sampling by introducing a dynamic threshold based on the probability distribution. Furthermore, contrastive search~Su-2022-NIPS-A and typical sampling~Meister-TACL-2023-Locally can be utilized to improve the generation coherence during decoding. {Since it has been found that large models tend to assign higher probability to important tokens compared to small models, contrastive decoding~Li-ACL-2023-Contrastive utilizes a larger LM (\\eg OPT-13B) and a smaller LM (\\eg OPT-125M) to measure their log-likelihood differences. Subsequently, tokens are sampled based on the delta value of the probability distribution, thereby amplifying the impact of important tokens.} {Based on this contrastive idea, DoLa~Chuang-arxiv-2023-DoLa further extends this approach to contrasting the logits across different layers of a single LLM, as higher layers tend to assign more weight to important tokens.} Practical Settings. In practice, existing libraries (\\eg Transformers~Wolf-EMNLP-2020-Transformers) and public APIs of LLMs (\\eg OpenAI) have supported various decoding strategies to serve different scenarios of text generation. Next, we present the decoding settings of several representative LLMs: $\\bullet$ T5~Raffel-JMLR-2020-Exploring utilizes greedy search as the default setting and applies beam search (beam size of 4) with a length penalty {of 0.6} for translation and summarization tasks. $\\bullet$ GPT-3~Brown-NeurIPS-2020-Language employs beam search with a beam size of 4 and a length penalty of 0.6 for all generation tasks. $\\bullet$ Alpaca~Taori-github-2023-Stanford utilizes sampling-based strategies % {with top-$k$ ($k=50$), top-$p$ ($p=0.9$)}, and temperature of 0.7 for open-ended generation. $\\bullet$ LLaMA~Touvron-arxiv-2023-LLaMA applies diverse decoding strategies tailored to specific tasks. For instance, it employs the greedy search for question answering tasks while utilizes a sampling strategy with the temperature settings of 0.1 (pass@1) and 0.8 (pass@100) for code generation. $\\bullet$ OpenAI API % {supports several basic decoding strategies, including greedy search (by setting temperature to 0), beam search (with the setting best\\_of), temperature sampling (with the setting temperature), nucleus sampling (with the setting top\\_p). It also introduce parameters presence\\_penalty and frequency\\_penalty to control the repetition degree of generation.} {According to the OpenAI's document, their APIs would produce different outputs even if the input and the hyper-parameters are the same. Setting temperature to 0 can yield more deterministic outputs, albeit with a slight chance of variability.}",
      "origin_cites_number": 19
    },
    {
      "section_title": "Summary and Discussion",
      "level": "3",
      "content": "The choice of architecture and pre-training tasks may incur different inductive biases for LLMs, which would lead to different model capacities. % In this part, we discuss one open issue about the architecture choice for LLMs. center tcolorbox[colback=blue!5!white,colframe=blue!55!black,width=0.48\\textwidth,title={Why does Predicting the Next Word Works?}] { The essence of decoder-only architecture is to accurately predict the next word for reconstructing the pre-training data. Till now, there has been no formal study that theoretically demonstrates its advantage over other architectures. An interesting explanation was from Ilya Sutskever during the interview held by Jensen Huanghttps://www.nvidia.com/en-us/on-demand/session/gtcspring23-S52092/. The original transcript from the interview was copied belowhttps://lifearchitect.ai/ilya/: \\\\ Say you read a detective novel. Itâ€™s like complicated plot, a storyline, different characters, lots of events, mysteries like clues, itâ€™s unclear. Then, letâ€™s say that at the last page of the book, the detective has gathered all the clues, gathered all the people and saying, \"okay, Iâ€™m going to reveal the identity of whoever committed the crime and that personâ€™s name is\". Predict that word. ...\\\\ Now, there are many different words. But predicting those words better and better, the understanding of the text keeps on increasing. GPT-4 predicts the next word better. } tcolorbox center Architecture Choice. {In earlier literature of pre-trained language models, there are lots of discussions on the effects of different architectures~Wang-ICML-2022-What,Tay-arxiv-2022-UL2}. However, most LLMs are developed based on the causal decoder architecture, and there still lacks a theoretical analysis on its advantage over the other alternatives. Next, we briefly summarize existing discussions on this issue. $\\bullet$ By pre-training with the LM objective, it seems that causal decoder architecture can achieve a superior zero-shot and few-shot generalization capacity. Existing research has shown that without multi-task fine-tuning, the causal decoder has better zero-shot performance than other architectures~Wang-ICML-2022-What. The success of GPT-3~Brown-NeurIPS-2020-Language has demonstrates that the large causal decoder model can be a good few-shot learner. In addition, instruction tuning and alignment tuning discussed in Section~sec-adaptation have been proven to further enhance the capability of large causal decoder models~Wei-ICLR-2022-Finetuned,Ouyang-arxiv-2022-Training,Chung-arxiv-2022-Scaling. $\\bullet$ Scaling law has been widely observed in causal decoders. By scaling the model size, the dataset size, and the total computation, the performance of causal decoders can be substantially improved~Kaplan-arxiv-2020-Scaling,Brown-NeurIPS-2020-Language. Thus, it has become an important strategy to increase the model capacity of the causal decoder via scaling. % However, more detailed investigation on encoder-decoder models is still lacking, and more efforts are needed to investigate the performance of encoder-decoder models at a large scale. More research efforts about the discussions on architectures and pre-training objectives are in need to analyze how the choices of the architecture and pre-training tasks affect the capacity of LLMs, especially for encoder-decoder architectures. Despite the effectiveness of decoder-only architecture, it is also suggested to make more diverse exploration on architecture design. Besides the major architecture, the detailed configuration of LLM is also worth attention, which has been discussed in Section~sec:configuration. table*[htb] \\centering Detailed optimization settings of several existing LLMs. 2.05\\columnwidth{!}{ tabular{lrrccccccc} \\toprule Model & tabular[r]{@{}r@{}}Batch Size\\\\ (\\#tokens)tabular & tabular[r]{@{}r@{}}Learning\\\\ Ratetabular & Warmup & Decay Method & Optimizer & tabular[l]{@{}l@{}}Precision\\\\ Typetabular & tabular[l]{@{}l@{}}Weight\\\\ Decaytabular & tabular[l]{@{}l@{}}Grad\\\\ Cliptabular & Dropout \\\\ \\midrule GPT3~(175B) & 32Kâ†’3.2M & $6 \\times 10^{-5}$ & yes & cosine decay to 10\\% & Adam & FP16 & 0.1 & 1.0 & - \\\\ PanGu-$\\alpha$~(200B) & - & $2 \\times 10^{-5}$ & - & - & Adam & - & 0.1 & - & - \\\\ OPT~(175B) & 2M & $1.2 \\times 10^{-4}$ & yes & manual decay & AdamW & FP16 & 0.1 & - & 0.1 \\\\ PaLM~(540B) & 1Mâ†’4M & $1 \\times 10^{-2}$ & no & inverse square root & Adafactor & BF16 & $lr^2$ & 1.0 & 0.1 \\\\ BLOOM~(176B) & 4M & $6 \\times 10^{-5}$ & yes & cosine decay to 10\\% & Adam & BF16 & 0.1 & 1.0 & 0.0 \\\\ MT-NLG~(530B) & 64 Kâ†’3.75M & $5 \\times 10^{-5}$ & yes & cosine decay to 10\\% & Adam & BF16 & 0.1 & 1.0 & - \\\\ Gopher~(280B) & 3Mâ†’6M & $4 \\times 10^{-5}$ & yes & cosine decay to 10\\% & Adam & BF16 & - & 1.0 & - \\\\ Chinchilla~(70B) & 1.5Mâ†’3M & $1 \\times 10^{-4}$ & yes & cosine decay to 10\\% & AdamW & BF16 & - & - & - \\\\ Galactica~(120B) & 2M & $7 \\times 10^{-6}$ & yes & linear decay to 10\\% & AdamW & - & 0.1 & 1.0 & 0.1 \\\\ LaMDA~(137B) & 256K & - & - & - & - & BF16 & - & - & - \\\\ Jurassic-1~(178B) & 32 Kâ†’3.2M & $6 \\times 10^{-5}$ & yes & - & - & - & - & - & - \\\\ LLaMA~(65B) & 4M & $1.5 \\times 10^{-4}$ & yes & cosine decay to 10\\% & AdamW & - & 0.1 & 1.0 & - \\\\ LLaMA 2~(70B) & 4M & $1.5 \\times 10^{-4}$ & yes & cosine decay to 10\\% & AdamW & - & 0.1 & 1.0 & - \\\\ Falcon~(40B) & 2M & $1.85 \\times 10^{-4}$ & yes & cosine decay to 10\\% & AdamW & BF16 & 0.1 & - & - \\\\ GLM~(130B) & 0.4Mâ†’8.25M & $8 \\times 10^{-5}$ & yes & cosine decay to 10\\% & AdamW & FP16 & 0.1 & 1.0 & 0.1 \\\\ T5~(11B) & 64K & $1 \\times 10^{-2}$ & no & inverse square root & AdaFactor & - & - & - & 0.1 \\\\ ERNIE 3.0 Titan~(260B) & - & $1 \\times 10^{-4}$ & - & - & Adam & FP16 & 0.1 & 1.0 & - \\\\ PanGu-$\\Sigma$~(1.085T) & 0.5M & $2 \\times 10^{-5}$ & yes & - & Adam & FP16 & - & - & - \\\\ \\bottomruletabular } table*",
      "origin_cites_number": 5
    },
    {
      "section_title": "Model Training",
      "level": "2",
      "content": "In this part, we review the important settings, techniques, or tricks for training LLMs.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Optimization Setting",
      "level": "3",
      "content": "For parameter optimization of LLMs, we present the commonly used settings for batch training, learning rate, optimizer, and training stability. Batch Training. For language model pre-training, existing work generally sets the batch size to a large number (\\eg 2,048 examples or 4M tokens) to improve the training stability and {throughput}. For LLMs such as GPT-3 and PaLM, they have introduced a new strategy that dynamically increases the batch size during training, ultimately reaching a million scale. Specifically, the batch size of GPT-3 is gradually increasing from 32K to 3.2M tokens. Empirical results have demonstrated that the dynamic schedule of batch size can effectively stabilize the training process of LLMs~Chowdhery-arxiv-2022-PaLM. Learning Rate. Existing LLMs usually adopt a similar learning rate schedule with the warm-up and decay strategies during pre-training. Specifically, in the initial 0.1\\% to 0.5\\% of the training steps, a linear warm-up schedule is employed for gradually increasing the learning rate to the maximum value that ranges from approximately $5 \\times 10^{-5}$ to $1 \\times 10^{-4}$ (\\eg $6 \\times 10^{-5}$ for GPT-3). Then, a cosine decay strategy is adopted in the subsequent steps, gradually reducing the learning rate to approximately 10\\% of its maximum value, until the convergence of the training loss. Optimizer. The Adam optimizer~Kingma-arXiv-2015-Adam and {AdamW optimizer~Loshchilov-arxiv-2017-Fixing} are widely utilized for training LLMs (\\eg GPT-3), which are based on adaptive estimates of lower-order moments for first-order gradient-based optimization. Commonly, its hyper-parameters are set as follows: $\\beta_1 = 0.9$, $\\beta_2 = 0.95$ and $\\epsilon = 10^{-8}$. Meanwhile, the Adafactor optimizer~Shazeer-ICML-2018-Adafactor has also been utilized in training LLMs (\\eg PaLM and T5), which is a variant of the Adam optimizer specially designed for conserving GPU memory during training. The hyper-parameters of the Adafactor optimizer are set as: $\\beta_1 = 0.9$ and $\\beta_2 = 1.0 - k^{-0.8}$, where $k$ denotes the number of training steps. Stabilizing the Training. During the pre-training of LLMs, it often suffers from the training instability issue, which may cause the model collapse. To address this issue, weight decay and gradient clipping have been widely utilized, where existing studies~Brown-NeurIPS-2020-Language,Zhang-arxiv-2022-OPT,Scao-arxiv-2022-BLOOM,Smith-CoRR-2022-Using,Zeng-arxiv-2022-GLM commonly set the threshold of gradient clipping to 1.0 and weight decay rate to 0.1. However, with the scaling of LLMs, the training loss spike is also more likely to occur, leading to unstable training. To mitigate this problem, PaLM~Chowdhery-arxiv-2022-PaLM and OPT~Zhang-arxiv-2022-OPT use a simple strategy that restarts the training process from an earlier checkpoint before the occurrence of the spike and skips over the data that may have caused the problem. Further, GLM~Zeng-arxiv-2022-GLM finds that the abnormal gradients of the embedding layer usually lead to spikes, and proposes to shrink the embedding layer gradients to alleviate it.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Scalable Training Techniques",
      "level": "3",
      "content": "As the model and data sizes increase, it has become challenging to efficiently train LLMs under a limited computational resource. Especially, two primary technical issues are required to be resolved, \\ie increasing training throughput and loading larger models into GPU memory. In this part, we review several widely used approaches in existing work to address the above two challenges, namely 3D parallelism~Huang-NeurIPS-2019-GPipe,Harlap-arXiv-2018-PipeDream,Shoeybi-arXiv-2019-Megatron and mixed precision training~Micikevicius-arXiv-2017-Mixed, and also give general suggestions about how to utilize them for training. 3D Parallelism. % {3D parallelism is actually a combination of three commonly used parallel training techniques, namely data parallelism, pipeline parallelism~Huang-NeurIPS-2019-GPipe,Harlap-arXiv-2018-PipeDream, and tensor parallelism~Shoeybi-arXiv-2019-MegatronModel parallelism is a more broader term that includes tensor parallelism and pipeline parallelism in some work~\\cite{Shoeybi-arXiv-2019-Megatron.}.} We next introduce the three parallel training techniques. $\\bullet$ Data parallelism. Data parallelism is one of the most fundamental approaches to improving the training throughput. It replicates the model parameters and optimizer states across multiple GPUs and then distributes the whole training corpus into these GPUs. In this way, each GPU only needs to process the assigned data for it, and performs the forward and backward propagation to obtain the gradients. The computed gradients on different GPUs will be further aggregated to obtain the gradients of the entire batch for updating the models in all GPUs. In this way, as the calculations of gradients are independently performed on different GPUs, the data parallelism mechanism is highly scalable, % {enabling the way that increases the number of GPUs to improve training throughput.} Furthermore, this technique is simple in implementation, and most of existing popular deep learning libraries have already implemented data parallelism, such as TensorFlow and PyTorch. $\\bullet$ Pipeline parallelism. Pipeline parallelism aims to distribute the different layers of a LLM into multiple GPUs. Especially, in the case of a Transformer model, pipeline parallelism loads consecutive layers onto the same GPU, to reduce the cost of transmitting the computed hidden states or gradients between GPUs. However, a naive implementation of pipeline parallelism may result in a lower GPU utilization rate as each GPU has to wait for the previous one to complete the computation, leading to the unnecessary cost of bubbles overhead{~Huang-NeurIPS-2019-GPipe}. To reduce these bubbles in pipeline parallelism, GPipe~Huang-NeurIPS-2019-GPipe and PipeDream~Harlap-arXiv-2018-PipeDream propose the techniques of padding multiple batches of data and asynchronous gradient update to improve the pipeline efficiency. $\\bullet$ Tensor parallelism. Tensor parallelism is also a commonly used technique that aims to decompose the LLM for multi-GPU loading. Unlike pipeline parallelism, tensor parallelism focuses on decomposing the tensors (the parameter matrices) of LLMs. For a matrix multiplication operation $Y = XA$ in the LLM, the parameter matrix $A$ can be split into two submatrices, $A_1$ and $A_2$, by column, which can be expressed as $Y = [X A_1, X A_2]$. By placing matrices $A_1$ and $A_2$ on different GPUs, the matrix multiplication operation would be invoked at two GPUs in parallel, and the final result can be obtained by combining the outputs from the two GPUs through across-GPU communication. Currently, tensor parallelism has been supported in several open-source libraries, \\eg Megatron-LM~Shoeybi-arXiv-2019-Megatron, and can be extended to higher-dimensional tensors. Also, Colossal-AI has implemented tensor parallelism for higher-dimensional tensors~Xu-arXiv-2021-An,Wang-ICPP-2022-Tesseract,Bian-arXiv-2021-Maximizing and proposed sequence parallelism~Li-arXiv-2021-Sequence especially for sequence data, which can further decompose the attention operation of the Transformer model. Mixed Precision Training. In previous PLMs (\\eg BERT~Devlin-NAACL-2019-BERT), 32-bit floating-point numbers, also known as FP32, have been predominantly used for pre-training. In recent years, to pre-train extremely large language models, some studies~Micikevicius-arXiv-2017-Mixed have started to utilize 16-bit floating-point numbers (FP16), which reduces memory usage and communication overhead. Additionally, as popular NVIDIA GPUs (\\eg A100) have twice the amount of FP16 computation units as FP32, the computational efficiency of FP16 can be further improved. However, existing work has found that FP16 may lead to the loss of {computational accuracy~Scao-arxiv-2022-BLOOM,Rae-arxiv-2021-Scaling}, which affects the final model performance. To alleviate it, an alternative called Brain Floating Point (BF16) has been used for training, which {allocates more exponent bits and fewer significant bits than FP16.} For pre-training, BF16 generally performs better than FP16 on representation accuracy~Scao-arxiv-2022-BLOOM. Overall Training Suggestion. In practice, the above training techniques, especially 3D parallelism, are often jointly used to improve the training throughput and large model loading. For instance, researchers have incorporated 8{-way} data parallelism, 4{-way} tensor parallelism, and 12{-way} pipeline parallelism, enabling the training of BLOOM~Scao-arxiv-2022-BLOOM on 384 A100 GPUs. % Currently, open-source libraries like DeepSpeed~Rasley-KDD-2020-DeepSpeed, Colossal-AI~Bian-CoRR-2021-Colossal-AI, and Alpa~Zheng-OSDI-2022-Alpa can well support the three parallel training methods. {To reduce the memory redundancy, ZeRO, FSDP, and activation recomputation techniques~Chen-arxiv-2016-training,Korthikanti-arxiv-2022-reducing can be also employed for training LLMs, which have already been integrated into DeepSpeed, PyTorch, and Megatron-LM. } In addition, the mixed precision training technique such as BF16 can be also leveraged to improve the training efficiency and reduce GPU memory usage, while it requires necessary support on hardware (\\eg A100 GPU). Because training large models is a time-intensive process, it would be useful to forecast the model performance and detect abnormal issues at an early stage. For this purpose, GPT-4~OpenAI-OpenAI-2023-GPT-4 has recently introduced a new mechanism called predictable scaling built on a deep learning stack, enabling the performance prediction of large models with a much smaller model, which might be quite useful for developing LLMs. In practice, one can further leverage the supporting training techniques of mainstream deep learning frameworks. For instance, PyTorch supports the data parallel training algorithm FSDP~FairScale2021 (\\ie fully sharded data parallel), which allows for partial offloading of training computations to CPUs if desired.",
      "origin_cites_number": 22
    },
    {
      "section_title": "Post-training of LLMs",
      "level": "1",
      "content": "After pre-training, LLMs can acquire the general abilities for solving various tasks. However, an increasing number of studies have shown that LLM's abilities can be further adapted according to specific goals. In this section, we introduce two major approaches to adapting pre-trained LLMs, namely instruction tuning and alignment tuning. The former approach mainly aims to enhance (or unlock) the abilities of LLMs, while the latter approach aims to align the behaviors of LLMs with human values or preferences. Further, we will also discuss efficient tuning and quantization for model adaptation in resource-limited settings. In what follows, we will introduce the four parts in detail. figure*[h] \\centering \\includegraphics[width=1\\textwidth]{images/Instruction-Tuning-new.pdf} An illustration of instance formatting and three different methods for constructing the instruction-formatted instances. figure*",
      "origin_cites_number": 0
    },
    {
      "section_title": "Instruction Tuning",
      "level": "2",
      "content": "In essence, instruction tuning is the approach to fine-tuning pre-trained LLMs on a collection of formatted instances in the form of natural language~Wei-ICLR-2022-Finetuned, which is highly related to supervised fine-tuning~Ouyang-arxiv-2022-Training and multi-task prompted training~Sanh-ICLR-2022-Multitask. % In order to perform instruction tuning, we first need to collect or construct instruction-formatted instances. Then, we employ these formatted instances to fine-tune LLMs in a supervised learning way (\\eg training with the sequence-to-sequence loss). After instruction tuning, LLMs can demonstrate superior abilities to generalize to unseen tasks~Wei-ICLR-2022-Finetuned,Sanh-ICLR-2022-Multitask,Chung-arxiv-2022-Scaling, even in a multilingual setting~Muennighoff-2022-arxiv-Crosslingual. A recent survey~Lou-arXiv-2023-Is presents a systematic overview of the research on instruction tuning. In comparison to that, we mainly focus on the effect of instruction tuning on LLMs and provide detailed guidelines or strategies for instance collection and tuning. In addition, we also discuss the use of instruction tuning for satisfying the real needs of users, which has been widely applied in existing LLMs, \\eg InstructGPT~Ouyang-arxiv-2022-Training and GPT-4~OpenAI-OpenAI-2023-GPT-4.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Formatted Instance Construction",
      "level": "3",
      "content": "Generally, an instruction-formatted instance consists of a task description (called an instruction), an optional input, the corresponding output, and a small number of demonstrations (optional). As important public resources, existing studies have released a large number of labeled data formatted in natural language (see the list of available resources in Table~tab:instruction-collection) as introduced in Section~sec:it-dataset. Next, we introduce four major methods for constructing formatted instances (see an illustration in Figure~fig:instruction-tuning) and then discuss several key factors for instance construction. Formatting NLP Task Datasets. Before instruction tuning was proposed, several early studies~Liu-ACL-2019-Multi,Aghajanyan-EMNLP-2021-Muppet,Tang-arxiv-2022-MVP collected the instances from a diverse range of traditional NLP tasks (\\eg text summarization, text classification, and translation) to create supervised multi-task training datasets. As a major source of instruction tuning instances, it is convenient to format these multi-task training datasets with natural language task descriptions. Specifically, recent work~Wei-ICLR-2022-Finetuned,Sanh-ICLR-2022-Multitask,Ouyang-arxiv-2022-Training,Wang-EMNLP-2022-Super augments the labeled datasets with human-written task descriptions, which instructs LLMs to understand the tasks by explaining the task goal. % For example, in Figure~fig:instruction-tuning(a), a task description ``Please answer this question'' is added for each example in the question-answering task. After instruction tuning, LLMs can generalize well to other unseen tasks by following their task descriptions~Wei-ICLR-2022-Finetuned,Sanh-ICLR-2022-Multitask,Chung-arxiv-2022-Scaling. In particular, it has been shown that instructions are the crucial factor in task generalization ability for LLMs~Wei-ICLR-2022-Finetuned: by fine-tuning the model on labeled datasets with the task descriptions removed, it results in a dramatic drop in model performance. To better generate labeled instances for instruction tuning, a crowd-sourcing platform, PromptSource~Bach-ACL-2022-PromptSource has been proposed to effectively create, share, and verify the task descriptions for different datasets. To enrich the training instances, several studies~Sanh-ICLR-2022-Multitask,Tang-arxiv-2022-MVP,Longpre-arxiv-2023-The also try to invert the input-output pairs of existing instances with specially designed task descriptions for instruction tuning. For instance, given a question-answer pair, we can create a new instance by predicting the answer-conditioned question (\\eg ``Please generate a question based on the answer:''). % Formatting Daily Chat Data. Despite that a large number of training instances have been formatted with instructions, they mainly come from public NLP datasets, either lacking instruction diversity or mismatching with real human needs~Ouyang-arxiv-2022-Training. To overcome this issue, InstructGPT~Ouyang-arxiv-2022-Training proposes to take the queries that real users have submitted to the OpenAI API as the task descriptions. % Additionally, to enrich the task diversity, human labelers are also asked to compose the instructions for real-life tasks, including open-ended generation, open question answering, brainstorming, and chatting. Then, they let another group of labelers directly answer these instructions as the output. Finally, they pair one instruction (\\ie the collected user query) and the expected output (\\ie the human-written answer) as a training instance. Note that InstructGPT also employs these real-world tasks formatted in natural language for alignment tuning (discussed in Section~sec-alignment). Further, GPT-4~OpenAI-OpenAI-2023-GPT-4 has designed potentially high-risk instructions and guided the model to reject these instructions through supervised fine-tuning for safety concerns. {Considering the absence of high-quality public chat data, several studies have also collected users' chat requests as input data, and then utilized ChatGPT or GPT-4 to generate responses as output data. A notable example of such a dataset is the conversational data from ShareGPT~ShareGPT. Additionally, Dolly~Conover-2023-arxiv-Dolly and OpenAssistant~kopf-arxiv-2023-openassistant have further released their conversation data, which has been carefully labeled by human annotators to attain a high level of quality.} Formatting Synthetic Data. To reduce the burden of human annotation or manual collection, several semi-automated approaches~Wang-arXiv-2022-Self have been proposed for constructing instances by feeding existing instances into LLMs to synthesize diverse task descriptions and {instances}. As illustrated in Figure~fig:instruction-tuning(c), the Self-Instruct method only needs 175 instances as the initial task pool. Then, they randomly select a few instances from the pool as demonstrations and prompt a LLM to generate new instructions and corresponding input-output pairs. After the quality and diversity filtering, newly generated instances would be added into the task pool. Hence, the synthetic method is an effective and economical way to generate large-scale instruction data for LLMs. However, the instances generated by the Self-Instruct method might be simplistic or lack the diversity. To improve the quality of synthetic instructions, WizardLM~Xu-arxiv-2023-WizardLM introduces Evol-Instruct by proposing in-depth and in-breadth evolving to enrich the complexity and diversity of the instances. Furthermore, Self-Align~Sun-arxiv-2023-Principle establishes multiple human-aligned principles to filter the synthesized instances. It then employs these instances to train a LLM in order to yield more aligned instances. To enhance the quality of the instance output, researchers directly adopt human-written texts as the output and synthesize corresponding instructions using ICL examples~Li-arxiv-2023-Self. Key Factors for Instruction Dataset Construction. The quality of instruction instances has an important impact on the performance of the model. Here, we discuss some essential factors for instance construction. $\\bullet$ Scaling the instructions. It has been widely shown that scaling the number of tasks can largely enhance the generalization ability of LLMs~Wei-ICLR-2022-Finetuned,Sanh-ICLR-2022-Multitask,Wang-EMNLP-2022-Super. With the increasing of the task number, the model performance initially shows a continuous growth pattern, while the gain becomes negligible when it reaches a certain level~Wang-EMNLP-2022-Super,Chung-arxiv-2022-Scaling. A plausible speculation is that a certain number of representative tasks can provide relatively sufficient knowledge and adding more tasks may not bring additional gains~Chung-arxiv-2022-Scaling. Also, it is beneficial to enhance the diversity of the task descriptions in several aspects, such as length, structure, and creativity~Sanh-ICLR-2022-Multitask. As for the number of instances per task, it has been found that a small number of instances can usually saturate the generalization performance of the model to perform a specific task~Wei-ICLR-2022-Finetuned,Chung-arxiv-2022-Scaling. {Specially, several recent work~zhou-arxiv-2023-lima,Chen-arxiv-2023-AlpaGasus has explored the effect of fine-tuning with a small amount of high-quality instruction data (\\eg one or a few thousand instances), showing very promising results on the evaluation tasks. In contrast, another line of studies continue to explore the scaling effect of instruction data~Mukherjee-arxiv-2023-Orca,YuLan-Chat. For example, Orca~Mukherjee-arxiv-2023-Orca scales up the synthesized instances to 5 million with step-by-step explanations, and it achieves superior performance across a wide range of tasks.} $\\bullet$ Formatting design. As an important factor, the design of natural language format also highly impacts the generalization performance of LLMs~Wang-EMNLP-2022-Super. Typically, we can add task descriptions and optional demonstrations to the input-output pairs of existing datasets, where the task description is the most key part for LLMs to understand the task~Wang-EMNLP-2022-Super. Further, it can lead to substantial improvements by using an appropriate number of exemplars as demonstrations~Chung-arxiv-2022-Scaling, which also alleviates the model sensitivity to instruction engineering~Wei-ICLR-2022-Finetuned,Chung-arxiv-2022-Scaling. However, incorporating other components (\\eg things to avoid, reasons, and suggestions) into instructions may have a negligible or even adverse effect on the performance of LLMs~Mishra-ACL-2022-Cross, Wang-EMNLP-2022-Super. Recently, to elicit the step-by-step reasoning ability of LLMs, some work~Chung-arxiv-2022-Scaling proposes to include chain-of-thought (CoT) examples for some reasoning datasets, such as arithmetic reasoning. It has been shown that fine-tuning LLMs with both CoT and non-CoT examples can lead to a good performance across various reasoning tasks, including those that require multi-hop reasoning ability (\\eg commonsense question answering and arithmetic reasoning) as well as those without the need for such a reasoning way (\\eg sentiment analysis and extractive question answering)~Chung-arxiv-2022-Scaling,Iyer-arxiv-2022-OPT. $\\bullet$ {Instruction quality improvement.} Data quality is very important for the performance of instruction tuning, and a surge of work has been proposed to further improve the quality of existing instruction datasets. Typically, these methods mostly rely on carefully designed prompts, to guide LLMs to refine or rewrite the given instruction. WizardLM~Xu-arxiv-2023-WizardLM aims to complexify and diversify the Alpaca dataset~alpaca by devising prompts to widen and deepen the required knowledge of given instructions. It also crafts the filter strategy to remove the low-quality instructions. To further provide fine-grained knowledge guidance, recent work also involves the knowledge taxonomy into the input prompt, \\eg knowledge key points~Huang-arxiv-2024-Key and the human-AI conversation topic taxonomy~Ding-2023-EMNLP-Enhancing. To guarantee the instruction quality, early methods mainly employ close-source API or powerful open-source LLMs, which would take a huge cost for large-scale instructions synthesis. Considering this issue, recent studies widely explore the potential of relatively small models for data synthesis. For instance, JiuZhang3.0~Zhou-arxiv-2024-Jiuzhang3.0 fine-tunes a 7B language model to synthesize questions by distilling the knowledge from GPT-4, and then utilizes it to synthesize massive high-quality instructions based on pre-training corpus. Such a way can achieve better performance on mathematical reasoning tasks than baseline methods, with only 20\\% data synthesis cost. $\\bullet$ {Instruction selection.} {As a surge of instruction datasets are proposed, it is non-trivial to select the high-quality ones from them to construct the training dataset. Generally, existing work either leverages quality estimation metrics or employs LLMs as the judge model to rank all the instruction instances, and then selects those with relatively higher scores. Concretely, for metrics, perplexity and other heuristic measurements (\\eg length)~gao-arxiv-2023-instructionmining have been widely used in practice, \\eg we can consider removing high-perplexity or very short instructions, which might correspond to low-quality ones. To better estimate the effect of an instruction for the LLM capability, more complex metrics (\\eg IFD~li-arxiv-2023-boosting) have also been proposed, which are computed by combining multiple simple metrics. { Additionally, diversity-aware sampling methods have been introduced to ensure the overall coverage of representative instruction data~iclr-2018-sener-active. Besides, when downstream task data is available, cross-instance gradient similarity can be employed to measure the value of training instances for the target task. LESS~arxiv-2024-xia-less computes gradients for both downstream validation and training instruction data, to evaluate the contribution of instruction data based on extensions of influence function~koh-2017-understanding. } } To summarize, diversity and quality of instructions are important factors to consider when scaling the number of instances~zhou-arxiv-2023-lima. % As the capacities of LLMs improve, data synthesis methods have become the mainstream approach for generating large amount of instruction data. Following this trend, there are increasingly more automatically generated instruction datasets available, and selection and refining methods are key to effectively use these datasets. To help readers understand how different factors affect instruction tuning, we conduct an empirical study by experimenting with multiple specially constructed instruction datasets in Section~instruction-results. table*[t] \\centering Basic statistics of the required number of GPUs, tuning time, batch size (denoted as BS) per device (full tuning and LoRA tuning), and inference rate (the number of generated tokes per second). Our experiments are conducted based on two Linux servers having 8 A800-80G SXM4 GPUs with 6 NVSwitch and 8 3090-24G GPUs, respectively. {The major difference between A800 and A100 lies in the NVLink interconnect speed. Thus, our estimations about training and inference efficiency would be slightly improved for A100, while the rest memory consumption would remain the same. { For full tuning experiments, we use data parallel training, ZeRO Stage 3, BF16, and gradient checkpointing. Additionally, the LoRA tuning can be executed on one 80G GPU utilizing INT8 quantization with the rank setting set to 16. {All the experiments are conducted with Alpaca-52K dataset by training LLaMA models three epochs.} The max sequence length for both training settings is set to 512. The inference experiments are performed with the batch size set to 1.} } \\renewcommand2.5pt tabular{c|ccc|ccc|cc|cc|cc} \\toprule 2{*}{Models} & 3{c|}{A800 Full Tuning} & 3{c|}{A800 LoRA Tuning} & 2{c|}{A800 Inference (16-bit)} & 2{c|}{3090 Inference (16-bit)} & 2{c}{3090 Inference (8-bit)} \\\\ & \\#GPU & BS & Time & \\#GPU & BS & Time & \\#GPU & \\#Token/s & \\#GPU & \\#Token/s & \\#GPU & \\#Token/s \\\\ \\midrule LLaMA (7B) & 2 & 8 & 3.0h & 1 & 80 & 3.5h & 1 & 36.6 & 1 & 24.3 & 1 & 7.5 \\\\ LLaMA (13B) & 4 & 8 & 3.1h & 1 & 48 & 5.1h & 1 & 26.8 & 2 & 9.9 & 1 & 4.5 \\\\ LLaMA (30B) & 8 & 4 & 6.1h & 1 & 24 & 14.3h & 1 & 17.7 & 4 & 3.8 & 2 & 2.6 \\\\ LLaMA (65B) & 16 & 2 & 11.2h & 1 & 4 & 60.6h & 2 & 8.8 & 8 & 2.0 & 4 & 1.5 \\\\ \\bottomrule tabular table*",
      "origin_cites_number": 42
    },
    {
      "section_title": "Instruction Tuning Strategies",
      "level": "3",
      "content": "Unlike pre-training, instruction tuning is often more efficient since only a moderate number of instances are used for training. Since instruction tuning can be considered as a supervised training process, its optimization is different from pre-training in several aspects~Chung-arxiv-2022-Scaling, {such as the training objective (\\ie sequence-to-sequence loss) and optimization configuration (\\eg smaller batch size and learning rate)}, which require special attention in practice. In addition to these optimization configurations, there are also four important aspects to consider for instruction tuning: Balancing the Data Distribution. Since instruction tuning involves a mixture of different tasks, it is important to balance the proportion of different tasks during fine-tuning. A widely used method is the examples-proportional mixing strategy~Raffel-JMLR-2020-Exploring, \\ie combining all the datasets and sampling each instance equally from the mixed datasets. Furthermore, increasing the sampling ratio of high-quality collections (\\eg FLAN~Wei-ICLR-2022-Finetuned and P3~Bach-ACL-2022-PromptSource) can generally lead to performance improvement according to recent findings~Chung-arxiv-2022-Scaling,Iyer-arxiv-2022-OPT. Further, it is common to set a maximum cap to control the maximum number of examples that a dataset can contain during instruction tuning~Raffel-JMLR-2020-Exploring, which is set to prevent larger datasets from overwhelming the entire distribution~Raffel-JMLR-2020-Exploring,Iyer-arxiv-2022-OPT. In practice, the maximum cap is typically set to several thousands or tens of thousands according to different datasets~Wei-ICLR-2022-Finetuned,Chung-arxiv-2022-Scaling. {Recently, it has been empirically found that existing instruction datasets (Table~tab:instruction-collection) mainly focus on enhancing LLMs' capabilities in certain aspects, and a single dataset alone cannot lead to a comprehensive enhancement in model capacity~Wang-arxiv-2023-How. Therefore, it is often suggested to use a mixture of existing instruction datasets to achieve a balanced improvement in different capacities, including NLP task data (\\eg FLAN v2~Liu-arxiv-2023_scaling), chat data (\\eg ShareGPT~ShareGPT), and synthetic data (\\eg GPT4-Alpaca~Peng-23-arxiv-Instruction).} Combining Instruction Tuning and Pre-Training. To make the tuning process more effective and stable, OPT-IML~Iyer-arxiv-2022-OPT incorporates pre-training data during instruction tuning, which can be regarded as regularization for model tuning. {Further, instead of using a separate two-stage process (pre-training then instruction tuning), some studies attempt to train a model from scratch with a mixture of pre-training data (\\ie plain texts) and instruction tuning data (\\ie formatted datasets)} using multi-task learning~Raffel-JMLR-2020-Exploring. Specifically, GLM-130B~Zeng-arxiv-2022-GLM and Galactica~Taylor-arxiv-2022-Galactica integrate instruction-formatted datasets as a small proportion of the pre-training corpora to pre-train LLMs, which potentially achieves the advantages of pre-training and instruction tuning at the same time. Multi-stage Instruction Tuning. For instruction tuning, there are two kinds of important instruction data, namely task-formatted instructions and daily chat instructions. Generally, the former has a significantly larger volume than the latter. It is important to balance the training with the two kinds of instruction data. In addition to carefully mixing different instruction data, we can also adopt a multi-stage instruction tuning strategy~YuLan-Chat, where LLMs are first fine-tuned with large-scale task-formatted instructions and subsequently fine-tuned on daily chat ones. To avoid the capacity forgetting issue, it is also useful to add an amount of task-formatted instructions at the second stage. Actually, such a multi-stage tuning strategy can be also applied to other settings for instruction tuning. For example, we can schedule different fine-tuning stages with progressively increased levels on difficulty and complexity, and gradually improve the capacities of LLMs to follow complex instructions. To improve the capacities of LLMs for generalizing to unseen tasks and handling open-ended generation, task-formatted and daily chat instructions are commonly used for tuning. However, high-quality daily chat instructions are much fewer than task-formatted ones (shown in Table~\\ref{tab-instructions), hence a simple mixture of these instructions would drown the daily chat ones, affecting the chatting performance. Thus, it is more reasonable to perform the multi-stage instruction tuning strategy~YuLan-Chat, where LLMs are first tuned by large-scale task-formatted instructions, and then tuned on daily chat ones. To avoid forgetting the task generalization capacity, it is also feasible to add a small subset of the task-formatted instructions in the second stage. Further, it is also applicable to progressively improve the difficulty and complexity of the instructions in the multi-stage tuning process, which can gradually improve the capacities of LLMs to follow complex instructions.} Other Practical Tricks. {In practice, there are also several useful strategies and tricks that are helpful to improve the fine-tuning performance of LLMs. We list several representative ones as follows:} $\\bullet$ Efficient training for multi-turn chat data. Given a multi-turn chat example (the conversation between a user and chatbot), a straightforward fine-tuning way is to split it into multiple context-response pairs for training: a LLM is fine-tuned to generate the response based on the corresponding context for all splits (\\ie at each utterance from the user). In such a fine-tuning way, it is apparent that there exist overlapping utterances in the split examples from a conversation. To save the training cost, Vicuna~vicuna2023 has adopted an efficient way that feeds the whole conversation into the LLM, but relies on a loss mask that only computes the loss on the responses of the chatbot for training. It can significantly reduce the compute costs derived from the overlapped utterances. $\\bullet$ Establishing self-identification for LLM. To deploy LLMs for real-world applications, it is necessary to establish its identity and make LLMs aware of these identity information, such as name, developer and affiliation. A practical way is to create identity-related instructions for fine-tuning the LLM. It is also feasible to prefix the input with the self-identification prompt, \\eg ``The following is a conversation between a human and an AI assistant called \\textsc{ChatbotName, developed by Developer.}'', where ChatbotName and Developer refer to the name and developer of the chatbot, respectively. In addition to the above practical strategies and tricks, existing work has also used other tricks, \\eg concatenating multiple examples into a single sequence to approach the max length~Krell-2021-arxiv-efficient.",
      "origin_cites_number": 20
    },
    {
      "section_title": "The Effect of Instruction Tuning",
      "level": "3",
      "content": "In this part, we discuss the effect of instruction tuning on LLMs in three major aspects. Performance Improvement. Despite being tuned on a moderate number of instances, instruction tuning has become an important way to improve or unlock the abilities of LLMs~Chung-arxiv-2022-Scaling. % Recent studies have experimented with language models in multiple scales (ranging from 77M to 540B), showing that the models of different scales can all benefit from instruction tuning~Chung-arxiv-2022-Scaling,Longpre-arxiv-2023-The, yielding improved performance as the parameter scale increases~Muennighoff-2022-arxiv-Crosslingual. Further, smaller models with instruction tuning can even perform better than larger models without fine-tuning~Sanh-ICLR-2022-Multitask,Chung-arxiv-2022-Scaling. Besides the model scale, instruction tuning demonstrates consistent improvements in various model architectures, pre-training objectives, and model adaptation methods~Chung-arxiv-2022-Scaling. In practice, instruction tuning offers % {a general approach to enhancing the abilities of existing language models~Chung-arxiv-2022-Scaling (including small-sized PLMs). Also, it is much less costly than pre-training, since the amount of instruction data required by LLMs is significantly smaller than pre-training data.} Task Generalization. Instruction tuning encourages the model to understand natural language instructions for task completion. It endows LLMs with the ability (often considered as an emergent ability) to follow human instructions~Wei-arxiv-2022-Emergent to perform specific tasks without demonstrations, even on unseen tasks~Chung-arxiv-2022-Scaling. A large number of studies have confirmed the effectiveness of instruction tuning to achieve superior performance on both seen and unseen tasks~Iyer-arxiv-2022-OPT,Longpre-arxiv-2023-The. Also, instruction tuning has been shown to be useful in alleviating several weaknesses of LLMs (\\eg repetitive generation or complementing the input without accomplishing a certain task)~Ouyang-arxiv-2022-Training,Chung-arxiv-2022-Scaling, leading to a superior capacity to solve real-world tasks for LLMs. Furthermore, LLMs trained with instruction tuning can generalize to related tasks across languages. For example, BLOOMZ-P3~Muennighoff-2022-arxiv-Crosslingual is fine-tuned based on BLOOM~Scao-arxiv-2022-BLOOM using English-only task collection P3~Bach-ACL-2022-PromptSource. Interestingly, BLOOMZ-P3 can achieve a more than 50\\% improvement in multilingual sentence completion tasks compared to BLOOM, which shows that instruction tuning can help LLMs acquire general task skills from English-only datasets and transfer such skills into other languages~Muennighoff-2022-arxiv-Crosslingual. In addition, it has been found that using English-only instructions can produce satisfactory results on multilingual tasks~Muennighoff-2022-arxiv-Crosslingual, which helps reduce the effort of instruction engineering for a specific language. Domain Specialization. Existing LLMs have showcased superior capabilities in traditional NLP tasks (\\eg generation and reasoning) and daily questions. However, they may still lack domain knowledge to accomplish specific tasks, such as medicine, law, and finance (See Section~sec-application for a detailed discussion of LLMs in different applications). Instruction tuning is an effective approach to adapting existing general LLMs to be domain-specific experts. For instance, researchers propose to fine-tune Flan-PaLM~Chung-arxiv-2022-Scaling using medical datasets to create Med-PaLM~singhal-arxiv-2022-large, a medical knowledge assistant that achieves performance levels comparable to those of expert clinicians. Furthermore, a recent study~Zhang-2023-arxiv-recommendation fine-tunes FLAN-T5 to support e-commerce recommender systems with natural language instructions, showing strong performance in a variety of recommendation tasks. There are also several open-sourced medical models instruction-tuned based on LLaMA~Touvron-arxiv-2023-LLaMA, such as BenTsao~wang-arxiv-2023-huatuo. Also, researchers explore instruction tuning on law~huang-arxiv-2023-lawyer, finance~wu-arxiv-2023-bloomberggpt, and arithmetic computation~liu-arxiv-2023-goat. % \\begin{table*[htb] \\centering Results of instruction-tuning experiments (all in a single-turn conversation) based on the LLaMA (7B) and LLaMA (13B) model under the chat and QA setting. We employ four instruction improvement strategies on the Self-Instruct-52K dataset, \\ie enhancing the complexity (\\emph{w/ complexity), increasing the diversity (w/ diversity), balancing the difficulty (w/ difficulty), and scaling the instruction number (w/ scaling). $^*$Since we select the LLaMA (7B)/(13B) model fine-tuned on Self-Instruct-52K as the baseline, we omit the win rate of the fine-tuned model with Self-Instruct-52K against itself.} 1.6\\columnwidth{!}{ tabular{llrcH|Hccc} \\toprule 2.5{*}{Models} & 2.5{*}{tabular[c]{@{}c@{}}Dataset\\\\ Mixturestabular} & 2.5{*}{tabular[c]{@{}c@{}}Instruction\\\\ Numberstabular}& 2.5{*}{tabular[c]{@{}c@{}}Lexical\\\\ Diversitytabular} & 2.5{*}{tabular[c]{@{}c@{}}Topic~($\\uparrow$)\\\\ Diversitytabular} & 2{c}{Chat} & 2{c}{QA} \\\\ \\cmidrule(r){6-7}\\cmidrule(r){8-9} & & & & & Human$^*$ & AlpacaFarm & MMLU & BBH \\\\ \\midrule LLaMA~(7B) & 172~FLAN-T5 & 80,000 & 48.48 & 26.79 & - & 27.70 & \\cellcolor[HTML]{C4DDEC}{35.04} & \\cellcolor[HTML]{A7CBE2}{27.57}\\\\ & 173~ShareGPT & 63,184 & 77.31 & 28.86 & - & \\cellcolor[HTML]{A7CBE2}{91.57} & \\cellcolor[HTML]{E5F0F7}{42.34} & 32.83\\\\ & 174~Self-Instruct-52K & 82,439 & 25.92 & 23.41 & / & /$^*$ & 34.15 & \\cellcolor[HTML]{E5F0F7}{31.29} \\\\ & 173 + 174 & 145,623 & 48.22 & 26.89 & - & \\cellcolor[HTML]{E5F0F7}{86.49} & \\cellcolor[HTML]{A7CBE2}{43.28} & 33.46\\\\ & 172 + 173 + 174 & 225,623 & 48.28 & 27.32 & - & 87.05 & \\cellcolor[HTML]{92BFDB}{42.64} & 34.13\\\\ 2-9 & 174~Self-Instruct-52K & 82,439 & 25.92 & 23.41 & / & /$^*$ & 34.15 & 31.29\\\\ & \\makecell[l]{w/ complexity} & 70,000 & 70.43 & 27.97 & - & \\cellcolor[HTML]{C4DDEC}{88.54} & \\cellcolor[HTML]{C6DEED}{36.55} & \\cellcolor[HTML]{92BFDB}{33.55}\\\\ & \\makecell[l]{w/ diversity} & 70,000 & 75.59 & 26.10 & - & \\cellcolor[HTML]{92BFDB}{88.85} & 35.88 & \\cellcolor[HTML]{C4DDEC}{32.85}\\\\ & \\makecell[l]{w/ difficulty} & 70,000 & 73.48 & 20.77 & - & \\cellcolor[HTML]{C6DEED}{80.24} & 32.62 & \\cellcolor[HTML]{C6DEED}{32.85}\\\\ & \\makecell[l]{w/ scaling} & 220,000 & 57.78 & 23.78 & - & 64.62 & 33.04 & 32.51\\\\ \\midrule LLaMA~(13B) & 172~FLAN-T5 & 80,000 & 48.48 & 26.79 & - & 31.47 & 38.67 & \\cellcolor[HTML]{C4DDEC}{28.88}\\\\ & 173~ShareGPT & 63,184 & 77.31 & 28.86 & - & \\cellcolor[HTML]{C4DDEC}{88.35} & \\cellcolor[HTML]{92BFDB}{50.42} & \\cellcolor[HTML]{E5F0F7}{35.28}\\\\ & 174~Self-Instruct-52K & 82,439 & 25.92 & 23.41 & / & /$^*$ & 37.40 & 32.44 \\\\ & 173 + 174 & 145,623 & 48.22 & 26.89 & - & \\cellcolor[HTML]{E5F0F7}{81.02} & 49.07 & 33.08\\\\ & 172 + 173 + 174 & 225,623 & 48.28 & 27.32 & - & 79.93 & \\cellcolor[HTML]{C4DDEC}{48.06} & 29.07\\\\ 2-9 & 174~Self-Instruct-52K & 82,439 & 25.92 & 23.41 & / & /$^*$ & 37.40 & 32.44\\\\ & \\makecell[l]{w/ complexity} & 70,000 & 70.43 & 27.97 & - & \\cellcolor[HTML]{C6DEED}{83.02} & \\cellcolor[HTML]{A7CBE2}{47.36} & \\cellcolor[HTML]{A7CBE2}{37.18}\\\\ & \\makecell[l]{w/ diversity} & 70,000 & 75.59 & 26.10 & - & \\cellcolor[HTML]{A7CBE2}{86.62} & \\cellcolor[HTML]{C6DEED}{47.74} & \\cellcolor[HTML]{92BFDB}{38.20}\\\\ & \\makecell[l]{w/ difficulty} & 70,000 & 73.48 & 20.77 & - & \\cellcolor[HTML]{92BFDB}{78.86} & \\cellcolor[HTML]{E5F0F7}{46.80} & \\cellcolor[HTML]{C6DEED}{35.60}\\\\ & \\makecell[l]{w/ scaling} & 220,000 & 57.78 & 23.78 & - & 58.87 & 42.64 & 33.15\\\\ \\midrule LLaMA~(30B) & 172~FLAN-T5 & 80,000 & 48.48 & 26.79 & - & 31.04 & 36.62 & \\cellcolor[HTML]{C4DDEC}{-}\\\\ & 173~ShareGPT & 63,184 & 77.31 & 28.86 & - & \\cellcolor[HTML]{C4DDEC}{90.33} & \\cellcolor[HTML]{92BFDB}{58.35} & \\cellcolor[HTML]{E5F0F7}{39.18}\\\\ & 174~Self-Instruct-52K & 82,439 & 25.92 & 23.41 & / & /$^*$ & 36.56 & 33.20 \\\\ & 173 + 174 & 145,623 & 48.22 & 26.89 & - & \\cellcolor[HTML]{E5F0F7}{80.92} & 52.13 & 33.75\\\\ & 172 + 173 + 174 & 225,623 & 48.28 & 27.32 & - & 75.96 & \\cellcolor[HTML]{C4DDEC}{51.08} & 35.99\\\\ 2-9 & 174~Self-Instruct-52K & 82,439 & 25.92 & 23.41 & / & /$^*$ & 36.56 & 33.20\\\\ & \\makecell[l]{w/ complexity} & 70,000 & 70.43 & 27.97 & - & \\cellcolor[HTML]{C6DEED}{85.13} & \\cellcolor[HTML]{A7CBE2}{48.86} & \\cellcolor[HTML]{A7CBE2}{35.77}\\\\ & \\makecell[l]{w/ diversity} & 70,000 & 75.59 & 26.10 & - & \\cellcolor[HTML]{A7CBE2}{88.34} & \\cellcolor[HTML]{C6DEED}{55.75} & \\cellcolor[HTML]{92BFDB}{40.56}\\\\ & \\makecell[l]{w/ difficulty} & 70,000 & 73.48 & 20.77 & - & \\cellcolor[HTML]{92BFDB}{81.91} & \\cellcolor[HTML]{E5F0F7}{52.56} & \\cellcolor[HTML]{C6DEED}{38.63}\\\\ & \\makecell[l]{w/ scaling} & 220,000 & 57.78 & 23.78 & - & 60.66 & 32.05 & 31.45\\\\ \\bottomrule tabular } table*} table*[htb] \\centering Results of instruction-tuning experiments (all in a single-turn conversation) based on the LLaMA (7B) and LLaMA (13B) model under the chat and QA setting. We employ four instruction improvement strategies on the Self-Instruct-52K dataset, \\ie enhancing the complexity (\\emph{w/ complexity), increasing the diversity (w/ diversity), balancing the difficulty (w/ difficulty), and scaling the instruction number (w/ scaling). $^*$Since we select the LLaMA (7B)/(13B) model fine-tuned on Self-Instruct-52K as the baseline, we omit the win rate of the fine-tuned model with Self-Instruct-52K against itself.} 1.6\\columnwidth{!}{ tabular{llrcH|Hccc} \\toprule 2.5{*}{Models} & 2.5{*}{tabular[c]{@{}c@{}}Dataset\\\\ Mixturestabular} & 2.5{*}{tabular[c]{@{}c@{}}Instruction\\\\ Numberstabular}& 2.5{*}{tabular[c]{@{}c@{}}Lexical\\\\ Diversitytabular} & 2.5{*}{tabular[c]{@{}c@{}}Topic~($\\uparrow$)\\\\ Diversitytabular} & 2{c}{Chat} & 2{c}{QA} \\\\ \\cmidrule(r){6-7}\\cmidrule(r){8-9} & & & & & Human$^*$ & AlpacaFarm & MMLU & BBH3k \\\\ \\midrule LLaMA~(7B) & 172~FLAN-T5 & 80,000 & 48.48 & 26.79 & - & 23.77 & \\cellcolor[HTML]{C4DDEC}{38.58} & \\cellcolor[HTML]{A7CBE2}{32.79}\\\\ & 173~ShareGPT & 63,184 & 77.31 & 28.86 & - & \\cellcolor[HTML]{A7CBE2}{81.30} & \\cellcolor[HTML]{E5F0F7}{38.11} & 27.71\\\\ & 174~Self-Instruct-52K & 82,439 & 25.92 & 23.41 & / & /$^*$ & 37.52 & \\cellcolor[HTML]{E5F0F7}{29.81} \\\\ & 173 + 174 & 145,623 & 48.22 & 26.89 & - & \\cellcolor[HTML]{E5F0F7}{71.36} & \\cellcolor[HTML]{A7CBE2}{41.26} & 28.36\\\\ & 172 + 173 + 174 & 225,623 & 48.28 & 27.32 & - & 70.00 & \\cellcolor[HTML]{92BFDB}{43.69} & 29.69\\\\ 2-9 & 174~Self-Instruct-52K & 82,439 & 25.92 & 23.41 & / & /$^*$ & 37.52 & 29.81\\\\ & \\makecell[l]{w/ complexity} & 70,000 & 70.43 & 27.97 & - & \\cellcolor[HTML]{C4DDEC}{76.96} & \\cellcolor[HTML]{C6DEED}{39.73} & \\cellcolor[HTML]{92BFDB}{33.25}\\\\ & \\makecell[l]{w/ diversity} & 70,000 & 75.59 & 26.10 & - & \\cellcolor[HTML]{92BFDB}{81.55} & 38.01 & \\cellcolor[HTML]{C4DDEC}{30.03}\\\\ & \\makecell[l]{w/ difficulty} & 70,000 & 73.48 & 20.77 & - & \\cellcolor[HTML]{C6DEED}{79.15} & 32.55 & \\cellcolor[HTML]{C6DEED}{31.25}\\\\ & \\makecell[l]{w/ scaling} & 220,000 & 57.78 & 23.78 & - & 51.13 & 33.81 & 26.63\\\\ \\midrule LLaMA~(13B) & 172~FLAN-T5 & 80,000 & 48.48 & 26.79 & - & 22.12 & 34.12 & \\cellcolor[HTML]{C4DDEC}{34.05}\\\\ & 173~ShareGPT & 63,184 & 77.31 & 28.86 & - & \\cellcolor[HTML]{C4DDEC}{77.13} & \\cellcolor[HTML]{92BFDB}{47.49} & \\cellcolor[HTML]{E5F0F7}{33.82}\\\\ & 174~Self-Instruct-52K & 82,439 & 25.92 & 23.41 & / & /$^*$ & 36.73 & 25.43 \\\\ & 173 + 174 & 145,623 & 48.22 & 26.89 & - & \\cellcolor[HTML]{E5F0F7}{72.85} & 41.16 & 29.49\\\\ & 172 + 173 + 174 & 225,623 & 48.28 & 27.32 & - & 69.49 & \\cellcolor[HTML]{C4DDEC}{43.50} & 31.16\\\\ 2-9 & 174~Self-Instruct-52K & 82,439 & 25.92 & 23.41 & / & /$^*$ & 36.73 & 25.43\\\\ & \\makecell[l]{w/ complexity} & 70,000 & 70.43 & 27.97 & - & \\cellcolor[HTML]{C6DEED}{77.94} & \\cellcolor[HTML]{A7CBE2}{46.89} & \\cellcolor[HTML]{A7CBE2}{35.75}\\\\ & \\makecell[l]{w/ diversity} & 70,000 & 75.59 & 26.10 & - & \\cellcolor[HTML]{A7CBE2}{78.92} & \\cellcolor[HTML]{C6DEED}{44.97} & \\cellcolor[HTML]{92BFDB}{36.40}\\\\ & \\makecell[l]{w/ difficulty} & 70,000 & 73.48 & 20.77 & - & \\cellcolor[HTML]{92BFDB}{80.45} & \\cellcolor[HTML]{E5F0F7}{43.15} & \\cellcolor[HTML]{C6DEED}{34.59}\\\\ & \\makecell[l]{w/ scaling} & 220,000 & 57.78 & 23.78 & - & 58.12 & 38.07 & 27.28\\\\ \\bottomrule tabular } table*",
      "origin_cites_number": 23
    },
    {
      "section_title": "Empirical Analysis for Instruction Tuning",
      "level": "3",
      "content": "Fine-tuning LLMs with different instruction sets tend to lead to model variants with varied performance on downstream tasks. In this section, we will explore the effect of different types of instructions in fine-tuning LLMs (\\ie LLaMA (7B) and LLaMA (13B)Due to the limit of computational resources, we cannot conduct large-scale experiments on larger LLaMA variants right now, which would be scheduled in a future version.), as well as examine the usefulness of several instruction improvement strategies. Instruction Datasets. According to the discussion in Section~sec-instruction-formatted, we mainly consider three common kinds of instructions as follows: \\textbullet~Task-specific instructions. For the first type of instructions, we adopt the most commonly-used multi-task instruction dataset, FLAN-T5~Chung-arxiv-2022-Scaling, which contains 1,836 tasks and over 15M instructions by combining four data mixtures from prior work. \\textbullet~Daily chat instructions. This type of instructions are conversations posed by users about daily life, which are more closely related to real-life scenarios. We adopt the ShareGPT instruciton set, consisting of 63K real-user instructions. It has been used as the core instructions for Vicuna. \\textbullet~Synthetic instructions. In addition to reusing existing instructions, we can also automatically synthesize massive instructions using LLMs. We adopt the popular synthetic instruction dataset Self-Instruct-52K~Wang-arXiv-2022-Self, consisting of 52K instructions paired with about 82K instance inputs and outputs. These generated instructions have a similar data distribution as the human-written seed tasks (\\eg grammar checking, brainstorming). {As the original FLAN-T5 dataset is very large (\\ie over 15M), we randomly sample 80,000 instructions from it for conducting a fair comparison with other instruction datasets (\\ie ShareGPT and Self-Instruct-52K) at a similar scale. In our experiments, we test on each individual instruction set to explore their own effects and also examine their combinatorial effects on model performance. } Improvement Strategies. Although real-world instructions from human users are more suitable for fine-tuning LLMs, it is difficult to collect them at a large scale. As alternatives to human-generated instructions, most existing research mainly adopts synthetic instructions generated by LLMs. However, there are some potential problems with synthetic instructions, such as poor topic diversity and uneven instruction difficulty (either too simple or too difficult). Thus, it is necessary to improve the quality of the synthetic instructions. Next, we summarize four major improvement strategies widely used in existing work as follows: \\textbullet~Enhancing the instruction complexity. As discussed in existing work~Xu-arxiv-2023-WizardLM, enhancing the complexity of instructions can improve the model capacity of LLMs in following complex instructions, % {\\eg including more task demands or requiring more reasoning steps.} To validate this strategy, we follow WizardLM~Xu-arxiv-2023-WizardLM by gradually increasing the complexity levels, \\eg adding constraints, increasing reasoning steps, and complicating the input. {We leverage the publicly released WizardLM-70K instructions~Xu-arxiv-2023-WizardLM as the complexity-enhanced instruction dataset, which has been generated via the above enhancement approach based on the Self-Instruct-52K dataset~Xu-arxiv-2023-WizardLM.} \\textbullet~Increasing the topic diversity. In addition to the complexity, improving the topic diversity of the instruction dataset % {can help elicit different abilities of LLMs on diverse tasks in real world~Sun-arxiv-2023-Principle.} However, it is difficult to directly control the self-instruct process for generating diverse instructions. Following YuLan-Chat~YuLan-Chat, we employ ChatGPT to rewrite the instructions from Self-Instruct-52K dataset for adapting them into 293 topics via specific prompts. Finally, we obtain 70K instructions as the diversity-increased dataset. \\textbullet~Scaling the instruction number. In addition to the above aspects, the number of instructions is also an important factor that may affect the model performance. Specially, using more instructions can extend the task knowledge and improve the ability of instruction following for LLMs~Chung-arxiv-2022-Scaling. To examine this strategy, we sample new instructions from the synthesized instruction set released from the MOSS project~sun2023moss, {as they are also synthesized using the same self-instruct method~Wang-arXiv-2022-Self.} We mix them with the Self-Instruct-52K dataset to compose a larger one containing 220K instructions. \\textbullet~Balancing the instruction difficulty. As the synthetic instructions tend to contain too easy or too hard ones, it is likely to result in training instability or even overfitting for LLMs. To explore the potential effects, we leverage the perplexity score of LLMs to estimate the difficulty of instructions and remove too easy or too hard instructions. To generate the same scale of instructions for fair comparison, we adopt a LLaMA (7B) model to compute the perplexity for the 220K instructions from the large instruction dataset, and then keep 70K instructions of moderate perplexity scores as the difficulty-balanced dataset. Experimental Setup. To conduct the experiments on the effect of instruction data, we leverage these new instruction datasets for tuning LLaMA, a popular LLM backbone that has been widely used for instruction-tuning. We use the code from YuLan-Chat~YuLan-Chat for our experiments, and train LLaMA 7B and 13B on a server of 8 A800-80G GPUs. All the hyper-parameters settings remain the same as Stanford Alpaca. To better evaluate the {instruction following ability of fine-tuned models, we consider two settings, namely Chat setting and QA setting. The chat setting mainly utilizes user instructions and queries from daily chat, whereas the QA setting mainly employs question answering examples from existing NLP datasets. } The evaluation on the chat setting is conducted based on the AlpacaFarm evaluation set~Dubois-arxiv-2023-AlpacaFarm. Instead of using a full pairwise comparison, % {we select the LLaMA 7B and 13B models fine-tuned on Self-Instruct-52K as the reference baselines, and then compare them with other fine-tuned LLaMA 7B and 13B models using different instructions, respectively.} Since our focus is to examine the usefulness of different strategies to generate the instructions, the model fine-tuned on Self-Instruct-52K can serve as a good reference. Following AlpacaFarm~Dubois-arxiv-2023-AlpacaFarm, for each comparison, we employ ChatGPT to automatically annotate which response from two compared models each time is the best for the user query, and report the win rate (\\%) as the evaluation metric. For the QA setting, we select two benchmarks, MMLU~Hendrycks-ICLR-2021-Measuring and BBH~Suzgun-arxiv-2022-Challenging, and evaluate the accuracy based on their default settings by using heuristic rules to parse the answers from these LLMs. For both instruction tuning and evaluation, we adopt the following prompt: ``The following is a conversation between a human and an AI assistant. The AI assistant gives helpful, detailed, and polite answers to the user's questions.$\\backslash$n [\\textbar Human\\textbar]:\\{input\\$\\backslash$n[\\textbar AI\\textbar]:}''. To reproduce our results, we release the code and data at the link: https://github.com/RUCAIBox/LLMSurvey/tree/main/Experiments. Results and Analysis. The results using different instruction datasets based on 7B and 13B LLaMA are in Table~tab-instruction-tuning-res. Next, we summarize and analyze our findings in detail. \\textbullet~{Task-formatted instructions are more proper for the QA setting, but may not be useful for the chat setting.} By comparing the performance of instruction tuning using FLAN-T5 with that of ShareGPT and Self-Instruct-52K, we can observe that FLAN-T5 mostly achieves a better performance on QA benchmarks while underperforms ShareGPT on the chat setting. % The reason is that FLAN-T5 is composed of a mixture of instructions and examples from existing NLP tasks, \\eg translation and reading comprehension. As a result, LLaMA fine-tuned with FLAN-T5 performs better on QA tasks, but poorly on user queries. In contrast, ShareGPT consists of real-world human-ChatGPT conversations, which is able to better elicit LLaMA to {follow user instructions} in daily life, while may not be suitable for accomplishing the QA tasks. \\textbullet~A mixture of different kinds of instructions are helpful to improve the comprehensive abilities of LLMs. After mixing the three kinds of instructions for fine-tuning, we can see that the derived LLaMA variant (with FLAN-T5, ShareGPT and Self-Instruct-52K) performs well in both task settings. In MMLU, the performance of LLaMA (7B) can surpass the ones using individual instruction set by a large margin, \\ie 43.69 vs. 38.58 (FLAN-T5). It shows that mixing multiple sources of instruction datasets is helpful to improve the performance of instruction-tuned LLMs, which scales the instruction number as well as increases the diversity. \\textbullet~Enhancing the complexity and diversity of instructions leads to an improved model performance. By increasing the complexity and diversity of the Self-Instruct-52K dataset respectively, the chat and QA performance of LLaMA can be consistently improved, {\\eg from 37.52 to 39.73 in MMLU for LLaMA (7B)}. It demonstrates that both strategies are useful to improve the instruction following ability of LLMs. Further, we can see that improving the complexity yields a larger performance improvement on QA tasks. The reason is that the QA tasks mostly consist of difficult questions for evaluating LLMs, which can be better solved by LLMs that have learned complex instructions at the fine-tuning stage. \\textbullet~{Simply increasing the number of instructions may not be that useful, and balancing the difficulty is not always helpful.} As the results shown in Table~tab-instruction-tuning-res, balancing the difficulty and increasing the number of fine-tuning instructions are not very helpful in our experiments. {Especially for scaling the instruction number, it even hurts the performance, \\eg a decrease from 29.81 to 26.63 in BBH for LLaMA (7B). It shows that simply scaling the number of synthesized instructions without quality control may not be effective to improve the performance.} Furthermore, fine-tuning with the instructions of moderate difficulty also performs well in the chat setting, while slightly decreasing the performance in the QA setting. A possible reason is that we filter complex and hard instructions with large perplexity scores, hurting the model performance in answering complex questions. \\textbullet~{A larger model scale leads to a better instruction following performance.} {By comparing the performance of LLaMA (7B) and LLaMA (13B) models fine-tuned with the same set of instruction data, we can see that LLaMA (13B) mostly achieves a better performance. It indicates that scaling the model size is helpful for improving the instruction following capability. Besides, we can see that the QA performance has been improved a lot, \\eg from 38.11 to 47.49 in MMLU. It is likely because that the larger models generally have better knowledge utilization and reasoning capability~Wei-arxiv-2022-chain,Brown-NeurIPS-2020-Language, which can accurately answer more complex questions. } center tcolorbox[colback=blue!5!white,colframe=blue!55!black,width=0.48\\textwidth,title={Instruction Tuning Suggestions}] { To conduct instruction tuning on LLMs, one can prepare the computational resources according to the basic statistics about the required number of GPUs and tuning time in Table~tab:instruction-time. After setting up the development environment, we recommend beginners to follow the code of Alpaca repository~alpaca for instruction tuning. % Subsequently, one should select the base model and construct the instruction datasets as we discuss in this section. When computational resources for training are constrained, users can utilize LoRA for parameter-efficient tuning (see Section~sec-PEFT). As for inference, users can further use quantization methods to deploy LLMs on fewer or smaller GPUs (see Section~sec-PEFT).} tcolorbox center",
      "origin_cites_number": 18
    },
    {
      "section_title": "Alignment Tuning",
      "level": "2",
      "content": "This part first presents the background of alignment with its definition and criteria, then focuses on the collection of human feedback data for aligning LLMs, and finally discusses the key technique of reinforcement learning from human feedback (RLHF) for alignment tuning.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Background and Criteria for Alignment",
      "level": "3",
      "content": "Background. LLMs have shown remarkable capabilities in a wide range of NLP tasks~Brown-NeurIPS-2020-Language,Chowdhery-arxiv-2022-PaLM,Wei-ICLR-2022-Finetuned,Zhang-arxiv-2022-OPT. However, these models may sometimes exhibit unintended behaviors, \\eg fabricating false information, pursuing inaccurate objectives, and producing harmful, misleading, and biased expressions~Ouyang-arxiv-2022-Training,Kenton-arxiv-2021-Alignment. For LLMs, the language modeling objective pre-trains the model parameters by word prediction while lacking the consideration of human values or preferences. {To avert these unexpected behaviors, human alignment has been proposed to make LLMs act in line with human expectations}~Ouyang-arxiv-2022-Training,Ziegler-arxiv-2019-Fine-Tuning. However, unlike the original pre-training and adaptation tuning (\\eg instruction tuning), such an alignment requires considering very different criteria (\\eg helpfulness, honesty, and harmlessness). It has been shown that alignment might harm the general abilities of LLMs to some extent, which is called alignment tax in related literature~Askell-arxiv-2021-A. Alignment Criteria. {Recently, there is increasing attention on developing multifarious criteria to regulate the behaviors of LLMs. Here, we take three representative alignment criteria (\\ie helpful, honest, and harmless) as examples for discussion, which have been widely adopted in existing literature~Askell-arxiv-2021-A,Ouyang-arxiv-2022-Training. } In addition, there are other alignment criteria for LLMs from different perspectives including behavior, intent, incentive, and inner aspects~Kenton-arxiv-2021-Alignment, which are essentially similar (or at least with similar alignment techniques) to the above three criteria. It is also feasible to modify the three criteria according to specific needs, \\eg substituting honesty with correctness~Glaese-arxiv-2022-Improving. Next, we give brief explanations about the three representative alignment criteria: $\\bullet$ Helpfulness. To be helpful, the LLM should demonstrate a clear attempt to assist users in solving their tasks or answering questions in a concise and efficient manner as possible. At a higher level, when further clarification is needed, the LLM should demonstrate the capability of eliciting additional relevant information through pertinent inquiries and exhibit suitable levels of sensitivity, perceptiveness, and prudence~Askell-arxiv-2021-A. Realizing the alignment of helpful behavior is challenging for LLMs since it is difficult to precisely define and measure the intention of users~Kenton-arxiv-2021-Alignment. $\\bullet$ Honesty. At a basic level, a LLM aligned to be honest should present accurate content to users instead of fabricating information. Additionally, it is crucial for the LLM to convey appropriate degrees of uncertainty in its output, in order to avoid any form of deception or misrepresentation of information. This requires the model to know about its capabilities and levels of knowledge (\\eg ``know unknowns''). According to the discussion in Askell-arxiv-2021-A, honesty is a more objective criterion compared to helpfulness and harmlessness, hence honesty alignment could potentially be developed with less reliance on human efforts. $\\bullet$ Harmlessness. To be harmless, it requires that the language produced by the model should not be offensive or discriminatory. To the best of its abilities, the model should be capable of detecting covert endeavors aimed at soliciting requests for malicious purposes. Ideally, when the model was induced to conduct a dangerous action (\\eg committing a crime), the LLM should politely refuse. Nonetheless, what behaviors are deemed harmful and to what extent vary amongst individuals or societies~Askell-arxiv-2021-A highly depend on who is using the LLM, the type of the posed question, and the context (\\eg time) at which the LLM is being used. As we can see, these criteria are quite subjective, and are developed based on human cognition. Thus, it is difficult to directly formulate them as optimization objectives for LLMs. {In existing work, there are many ways to fulfill these criteria when aligning LLMs. A promising technique is red teaming~Perez-EMNLP-2022-Red, which involves using manual or automated means to probe LLMs in an adversarial way to generate harmful outputs and then updates LLMs to prevent such outputs.",
      "origin_cites_number": 12
    },
    {
      "section_title": "Collecting Human Feedback",
      "level": "3",
      "content": "During the pre-training stage, LLMs are trained using the language modeling objective on a large-scale corpus. However, it cannot take into account the subjective and qualitative evaluations of LLM outputs by humans (called human feedback in this survey). High-quality human feedback is extremely important for aligning LLMs with human preferences and values. In this part, we discuss how to select a team of human labelers for feedback data collection. Human Labeler Selection. In existing work, the dominant method for generating human feedback data is human annotation~Ziegler-arxiv-2019-Fine-Tuning,Ouyang-arxiv-2022-Training,Glaese-arxiv-2022-Improving. This highlights the critical role of selecting appropriate human labelers. To provide high-quality feedback, human labelers are supposed to have a qualified level of education and excellent proficiency in English. For example, Sparrow~Glaese-arxiv-2022-Improving requires human labelers to be UK-based native English speakers who have obtained at least an undergraduate-level educational qualification. Even then, several studies~Ziegler-arxiv-2019-Fine-Tuning have found that there still exists a mismatch between the intentions of researchers and human labelers, which may lead to low-quality human feedback and cause LLMs to produce unexpected output. To address this issue, % InstructGPT~Ouyang-arxiv-2022-Training further conducts a screening process to filter labelers by assessing the agreement between human labelers and researchers. Specifically, researchers first label a small amount of data and then measure the agreement between themselves and human labelers. The labelers with the highest agreement will be selected to proceed with the subsequent annotation work. In some other work~Menick-arxiv-2022-teaching, ``super raters'' are used to ensure the high quality of human feedback. Researchers evaluate the performance of human labelers and select a group of well-performing human labelers (\\eg high agreement) as super raters. The super raters will be given priority to collaborate with the researchers in the subsequent study. When human labelers annotate the output of LLMs, it is helpful to specify detailed instructions and provide instant guidance for human labelers, which can further regulate the annotation of labelers. Human Feedback Collection. In existing work, there are mainly three kinds of approaches to collecting feedback and preference data from human labelers. $\\bullet$ Ranking-based approach. In early work~Ziegler-arxiv-2019-Fine-Tuning, human labelers often evaluate model-generated outputs in a coarse-grained manner (\\ie only selecting the best) without taking into account more fine-grained alignment criteria. Nonetheless, different labelers may hold diverse opinions on the selection of the best candidate output, and this method disregards the unselected samples, which may lead to inaccurate or incomplete human feedback. To address this issue, subsequent studies~Glaese-arxiv-2022-Improving introduce {the Elo rating system} to derive the preference ranking by comparing candidate outputs. {The ranking of outputs serves as the training signal that guides the model to prefer certain outputs over others, thus inducing outputs that are more reliable and safer.} $\\bullet$ Question-based approach. Further, human labelers can provide more detailed feedback by answering certain questions designed by researchers~Nakano-arxiv-2021-WebGPT, covering the alignment criteria as well as additional constraints for LLMs. Specially, in WebGPT~Nakano-arxiv-2021-WebGPT, to assist the model in filtering and utilizing relevant information from retrieved documents, human labelers are required to answer questions with multiple options about whether the retrieved documents are useful for answering the given input. $\\bullet$ Rule-based approach. Many studies also develop rule-based methods to provide more detailed human feedback. As a typical case, Sparrow~Glaese-arxiv-2022-Improving not only selects the response that labelers consider the best but also uses a series of rules to test whether model-generated responses meet the alignment criteria of being helpful, correct, and harmless. In this way, two kinds of human feedback data can be obtained: (1) the response preference feedback is obtained by comparing the quality of model-generated output in pairs, and (2) the rule violation feedback is obtained by collecting the assessment from human labelers (\\ie a score indicating to what extent the generated output has violated the rules). {Furthermore, GPT-4~OpenAI-OpenAI-2023-GPT-4 utilizes a set of zero-shot classifiers (based on GPT-4 itself) as rule-based reward models, which can automatically determine whether the model-generated outputs violate a set of human-written rules. } In the following, we focus on a well-known technique, reinforcement learning from human feedback (RLHF), which has been widely used in the recent powerful LLMs such as ChatGPT.} As discussed below, the alignment criteria introduced in % {Section~sec-alignment-background} can be fulfilled by learning from human feedback on the responses of LLMs to users' queries.",
      "origin_cites_number": 11
    },
    {
      "section_title": "Reinforcement Learning from Human Feedback",
      "level": "3",
      "content": "To align LLMs with human values, reinforcement learning from human feedback (RLHF)~Christiano-NeurIPS-2017-Deep,Ziegler-arxiv-2019-Fine-Tuning has been proposed to fine-tune LLMs with the collected human feedback data, which is useful to improve the alignment criteria (\\eg helpfulness, honesty, and harmlessness). RLHF employs reinforcement learning~(RL) algorithms~(\\eg Proximal Policy Optimization~(PPO)~schulman-arxiv-2017-proximal) to adapt LLMs to human feedback by learning a reward model. Such an approach incorporates humans in the training loop for developing well-aligned LLMs, as exemplified by InstructGPT~Ouyang-arxiv-2022-Training. figure*[!t] \\centering \\includegraphics[width=1\\textwidth]{images/Efficient-Tuning.pdf} An illustration of four different parameter-efficient fine-tuning methods. MHA and FFN denote the multi-head attention and feed-forward networks in the Transformer layer, respectively. figure* RLHF System. The RLHF system mainly comprises three key components: a pre-trained LM to be aligned, a reward model learning from human feedback, and a RL algorithm training the LM. Specifically, the pre-trained LM is typically a generative model that is initialized with existing pre-trained LM parameters. For example, OpenAI uses 175B GPT-3 for its first popular RLHF model, InstructGPT~Ouyang-arxiv-2022-Training, and DeepMind uses the 280 billion parameter model Gopher~Rae-arxiv-2021-Scaling for {its GopherCite model~Menick-arxiv-2022-teaching. } % Further, the reward model~(RM) provides (learned) guidance signals that reflect human preferences for the text generated by the LM, usually in the form of a scalar value. The reward model can take on two forms: a fine-tuned LM or a LM trained de novo using human preference data. {Existing work typically employs reward models having a parameter scale different from that of the aligned LM~Menick-arxiv-2022-teaching,Ouyang-arxiv-2022-Training.} For example, OpenAI uses 6B GPT-3 and DeepMind uses 7B Gopher as the reward model, respectively. Finally, to optimize the pre-trained LM using the signal from the reward model, a specific RL algorithm is designed for large-scale model tuning. Specifically, Proximal Policy Optimization (PPO)~schulman-arxiv-2017-proximal is a widely used RL algorithm for alignment in existing work~Ouyang-arxiv-2022-Training,Menick-arxiv-2022-teaching,Glaese-arxiv-2022-Improving. Key Steps for RLHF. Figure~fig:RLHF illustrates the overall three-step process of RLHF~Ouyang-arxiv-2022-Training as introduced below. $\\bullet$ Supervised fine-tuning. To make the LM initially perform desired behaviors, it usually needs to collect a supervised dataset containing input prompts (instruction) and desired outputs for fine-tuning the LM. These prompts and outputs can be written by human labelers for some specific tasks while ensuring the diversity of tasks. {For example, InstructGPT~Ouyang-arxiv-2022-Training asks human labelers to compose prompts (\\eg ``List five ideas for how to regain enthusiasm for my career'') and desired outputs for several generative tasks such as open QA, brainstorming, chatting, and rewriting. } Note that the first step is optional in specific settings or scenarios. $\\bullet$ Reward model training. The second step is to train the RM using human feedback data. Specifically, we employ the LM to generate a certain number of output texts using sampled prompts (from either the supervised dataset or the human-generated prompt) as input. We then invite human labelers to annotate the preference for these pairs. The annotation process can be conducted in multiple forms, and a common approach is to annotate by ranking the generated candidate texts, which can reduce the inconsistency among annotators. Then, the RM is trained to predict the human-preferred output. In InstructGPT, labelers rank model-generated outputs from best to worst, and the RM (\\ie 6B GPT-3) is trained to predict the ranking. Note that, in recent work~Bai-arXiv-2022-Constitutional, the annotation of preference on response pairs has been conducted by an AI agent (usually an aligned LLM) instead of humans, which is called ``reinforcement learning from AI feedback (RLAIF)''. {LLMs trained with typical RLHF algorithms tend to generate harmless responses with less helpfulness, which is called evasion problem~Bai-arXiv-2022-Constitutional. To guarantee both the harmlessness and helpfulness, RLAIF generates the AI feedback based on pre-set alignment principles in instructions~Bai-arXiv-2022-Constitutional,Lee-CoRR-2023-RLAIF, which can also reduce the efforts of human annotation.} $\\bullet$ RL fine-tuning. At this step, aligning (\\ie fine-tuning) the LM is formalized as an RL problem. In this setting, the pre-trained LM acts as the policy that takes as input a prompt and returns an output text, the action space of it is the vocabulary, the state is the currently generated token sequence, and the reward is provided by the RM. To avoid eviating significantly from the initial (before tuning) LM, a penalty term is commonly incorporated into the reward function. For example, InstructGPT optimizes the LM against the RM using the PPO algorithm. For each input prompt, InstructGPT calculates the KL divergence between the generated results from the current LM and the initial LM as the penalty. It is noted that the second and final steps can be iterated in multiple turns for better aligning LLMs. Due to the instability of the RL algorithm, recent work~Dong-RAFT-2023-arxiv replaces the RL tuning with another supervised fine-tuning by reusing the best ranked samples with higher rewards. Practical Strategies for RLHF. {Although RLHF is promising to effectively improve the alignment of LLMs with humans, it is practically challenging for researchers to successfully implement it. In this part, we focus on discussing several useful strategies and tricks for improving the effectiveness and efficiency of RLHF. Concretely, we focus on the effective training of reward models, efficient and effective RL training, respectively.} {$\\bullet$ Effective reward model training. Despite that InstructGPT used a small reward model (6B GPT model), increasing work~Touvron-2023-llama2-arxiv has shown it is often more effective to use a large reward model (\\eg equal or greater than the original model size), since large reward models generally perform better in judging the quality of the LLM generated outputs. In LLaMa 2~Touvron-2023-llama2-arxiv, pretrained chat model checkpoints are used to initialize the reward model, they argue that such an approach can effectively reduce the information mismatch between the model to be aligned and the reward model by sharing the same pre-training knowledge. Whereas, it is common to encounter the overfitting problem when training large-scale reward models. As a simple yet effective solution, existing work~askell2021general,zheng2023secrets has introduced the LM loss on the preferred response of the input prompt from the human-annotated alignment dataset as a regularizer, which alleviates the overfitting of the reward model on the binary classification task. In addition, as there are multiple criteria for alignment (\\eg helpfulness and honesty), it is often difficult to train a single reward model that can satisfy all the alignment criteria. Therefore, it is useful to train multiple reward models that focus on different alignment criteria~Touvron-2023-llama2-arxiv, and compute the final reward based on the produced ones from them via special combination strategies (\\eg mean pooling and weighted sum). Such a way enables more flexible rules or standards on multiple criteria, \\eg relaxing the requirement on helpfulness while posing more strict limits on harmfulness.} {$\\bullet$ Effective RL training. As the RL training process tends to be unstable and hyper-parameter sensitive, it is suggested that the language model should be well supervised fine-tuned before RL training, so as to reaching a good model capacity. A commonly-used way is to fine-tune the LLM on its best outputs of the prompts (referred to as rejection sampling or best-of-$N$) from the alignment dataset until convergence before RL. Given a prompt, the LLM would first produce $N$ outputs via the sampling algorithm, and then the best candidate from the model will be selected by the reward model for learning. After fine-tuning the LLM on the best samples until convergence, the RL process will be performed to further improve the performance. LLaMA 2~Touvron-2023-llama2-arxiv has successively trained five versions of RLHF models, where the LLM has been progressively improved with the improvement of the reward models. In this way, the collected prompts and annotations of human preference data can better reflect the issues of the current model checkpoint, thus making special tuning to address these issues. In addition, LLaMA 2 also adds samples from prior iterations into the subsequent ones, to alleviate the possible capacity regression issue during iterative optimization.} {$\\bullet$ Efficient RL training. As the RL training requires to iterate the inference process of both the LLM and reward models, it would greatly increase the total memory and computation cost, especially for larger reward models and LLMs. As a practical trick, we can deploy the reward model on a separate server, and invoke the corresponding API to work with the LLM on its own server. In addition, as RLHF requires the LLM to generate multiple candidate outputs, instead of calling the sample decoding procedure for multiple times, it is more efficient to utilize the beam search decoding algorithm\\url{https://huggingface.co/docs/transformers/v4.31.0/en/main\\_classes/text\\_generation\\#transformers.GenerationMixin.group\\_beam\\_search}. It only needs to perform one-pass decoding for response generation, meanwhile such a strategy can also enhance the diversity of the generated candidate responses. } Process-Supervised RLHF. In existing literature of RLHF~Uesato-arxiv-2022-Solving, the supervision approach for RL training generally takes two major forms, either using outcome-supervision signals or process-supervision signals. The outcome-supervised RLHF employs a quantitative score to assess the quality of the whole text generated by LLMs. In contrast, process-supervised RLHF offers an evaluation of each individual component (\\eg sentence, word, or reasoning step) within the generated content, which leverage fine-grained supervision signals to guide the training, helping LLMs refine the undesired generation contents~Uesato-arxiv-2022-Solving, Lightman-arxiv-2023-let. In what follows, we discuss two key aspects of process-supervised RLHF. $\\bullet$ Obtaining Fine-grained Supervision Signals. Compared with outcome rewards, it is more difficult to obtain fine-grained supervision signals. OpenAI has released a fine-grained annotation dataset named PRM800k~Lightman-arxiv-2023-let consisting of 12K process-annotated mathematical problems~(\\ie MATH dataset~Hendrycks-nips-2021-Measuring) and 75K solutions generated by LLMs of these problems, where each reasoning step of mathematical problems is labeled as positive, negative or neutral in PRM800k. Considering the cost and efficiency of the human annotation process, several methods aim to automatically annotate the correctness of intermediate reasoning steps, {\\eg using powerful LLMs to directly replace human annotators~Wang-23-arxiv-Shepherd or Monte Carlo tree search~Chen-arxiv-2024-AlphaMath}. After obtaining fine-grained supervision signals, existing work typically leverages them to train process-supervised reward models~(PRM)~Ma-arxiv-2023-Let, Lightman-arxiv-2023-let, which can produce step-level rewards (\\eg sentence based or token based rewards) during the RLHF procedure. {Furthermore, rather than leveraging the discriminative model to produce the rewards, RLMEC~Chen-arxiv-2024-Improving utilizes a generative reward model trained on rewriting tasks with the minimum editing constraint, to provide token-level rewards.% } {In addition, for the downstream tasks where fine-grained supervision signals are difficult to collected, outcome-supervision signals can also be utilized to perform process-supervised RLHF~Xi-arxiv-2024-Training.çœ‹ä¸æ‡‚ï¼Œè¿˜æ˜¯ä¸æ¸…æ¥šã€‚ã€‚ã€‚æ˜¯è¯´ç¡¬è¦ç”¨å—ï¼Ÿ} $\\bullet$ Utilizing the PRMs. { To effectively leverage process-supervision signals from PRMs, existing work mainly utilizes these fine-grained signals to evaluate individual parts within the LLM responses and then guides LLMs to adjust their generation behaviors to maximize the received reward of the response. Concretely, expert iteration~Silver-nat-2017-Mastering,Anthony-nips-2017-Thinking, an effective RL algorithm, has been utilized to improve the base policy via learning from expert policy~Uesato-arxiv-2022-Solving. Typically, expert iteration contains two main stages: policy improvement and distillation~Uesato-arxiv-2022-Solving. In the policy improvement stage, expert policy processes the systematic search procedure to produce the samples under the guidance of PRMs. Subsequently, during the distillation stage, the samples generated by expert policy in the first stage are utilized to improve the base policy through supervised fine-tuning. In addition to expert iteration, PRMs can also be utilized to re-rank the candidates of the final answers generated by LLMs~Lightman-arxiv-2023-let or to select better intermediate reasoning steps during step by step reasoning~Ma-arxiv-2023-Let, Luo-arxiv-2023-WizardMath. }",
      "origin_cites_number": 34
    },
    {
      "section_title": "Alignment without RLHF",
      "level": "3",
      "content": "Although RLHF has achieved great success in aligning the behaviors of LLMs with human values and preferences, it also suffers from notable limitations. First, RLHF needs to train multiple LMs including the model being aligned, the reward model, and the reference model at the same time, which is tedious in algorithmic procedure and memory-consuming in practice. Besides, the commonly-used PPO algorithm in RLHF is rather complex and often sensitive to hyper-parameters. As an alternative, increasing studies explore to directly optimize LLMs to adhere to human preferences, using supervised fine-tuning without reinforcement learning~zhou-arxiv-2023-lima. Overview. The basic idea of non-RL alignment approaches is to directly fine-tune LLMs with supervised learning on high-quality alignment dataset. It basically assumes that response feedback or golden rules to avert unsafe behaviors have been injected or included in the specially curated alignment dataset, so that LLMs can directly learn aligned behaviors from these demonstration data via suitable fine-tuning strategies. Thus, to implement this approach, two key issues are the construction of alignment dataset and the design of fine-tuning loss. % {For the first issue, the alignment dataset can be automatically constructed by an aligned LLMs according to human-written safety principles~Sun-arxiv-2023-Principle or refining existing examples using edits operations~Liu-NeurIPS-2022-Second. In addition, we can also reuse existing reward models to select high-rated responses from existing human feedback data~Dong-RAFT-2023-arxiv. For the second issue, non-RL alignment approaches mainly fine-tune LLMs in a supervised learning way (the same as the original instruction tuning loss) on a high-quality alignment dataset, meanwhile auxiliary learning objectives can be used to enhance the alignment performance, \\eg ranking responses or contrasting instruction-response pairs. } Alignment Data Collection. {The construction of alignment data is important to effectively align the behaviors of LLMs with human preferences. To collect high-quality alignment data, some work tries to reuse existing reward models to {select} high-rated responses, and others explore to leverage powerful LLMs (\\eg ChatGPT) or build a simulated environment to generate synthetic alignment examples. Next, we will discuss these three lines of research. } $\\bullet$ {Reward model based approaches. The reward model in RLHF has been trained to measure the alignment degree on {the responses of LLMs}. It is straightforward to leverage existing reward models to select high-quality responses as alignment data for subsequent fine-tuning. Based on this idea, RAFT~Dong-RAFT-2023-arxiv adopts reward models trained on human preference data to rank the responses of LLMs and collect those with higher rewards for supervised fine-tuning. In addition, the reward model can be also used to score model responses and assign them to different quality groups. Quark~Lu-nips-2022-quark sorts the responses of LLMs into different quantiles based on the reward scores. Each quantile is attached with a special reward token to represent the reward level of the quantile. Conditioned on the highest-reward tokens, LLMs are subsequently prompted to generate high-quality responses. {Given an initial answer and the corresponding human feedback, ILF~Scheurer-arxiv-2023-ILF first adopts LLMs to generate refined answers, then utilizes the reward model to select the answer that best matches the feedback for further training.} As valuable resources for aligning LLMs, several reward models have been released, including DeBERTa-base/large/xxlarge from OpenAssistanthttps://huggingface.co/OpenAssistant, Moss-7B from Fudanhttps://github.com/OpenLMLab/MOSS-RLHF, and Flan-T5-xl from Stanfordhttps://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xl. $\\bullet$ {LLM based generative approaches. Reward models help to select aligned data from model responses. However, training reward models itself necessitates substantial high-quality human-labeled data, which is typically expensive and in short supply. In addition, although existing reward models can be reused, they might not be able to accurately capture the nonalignment behaviors in another separately trained LLM. Therefore, some work explores leveraging powerful LLMs to automatically generate human-aligned data. As a representative work, constitutional AI~Bai-arXiv-2022-Constitutional proposes that human supervision comes from a set of principles (\\ie natural language instructions) governing AI behaviors. Based on these principles, LLMs will critique their own harmful responses and revise them repeatedly into finally aligned responses. Similarly, Self-Align~Sun-arxiv-2023-Principle first adopts self-instruct~Wang-arXiv-2022-Self to generate instructions focusing on covering diverse topics. Then, the model is also prompted with multiple human-written principles that describe the rules of expected model behaviors (also with several in-context exemplars), to generate helpful, ethical, and reliable responses as alignment data. {To mitigate the limit that the original SFT method can only learn from positive responses, FIGA~Guo-arxiv-2023-Beyond develops an improved supervised alignment approach, where both negative (the original output of low quality) and positive (the refined output by LLMs) responses are leveraged in a contrastive way, to enable LLMs to deeply understand what fine-grained revisions actually lead to good response. } $\\bullet$ {LLM based interactive approaches. Most existing approaches train LLMs in isolation, where LLMs are not present in actual environments to improve themselves through external feedback signals. As a comparison, humans learn social norms and values from interactions with others in social environments~Krishna-PNAS-2022-Socially. To mimic such a learning approach, Stable Alignment~Liu-arxiv-2023-training builds a simulated interaction environment consisting of a number of LLM agents, where AI agents keep interacting with and each other, receiving feedback on improvement. Once a central agent receives an instruction, it produces a response and shares it with nearby agents. These critic agents generate feedback comprising ratings about the response and revision suggestions. Then the central agent would revise the original response following these suggestions. % } Such an alignment approach can be also extended to real-world environment with humans. Supervised Alignment Tuning. {After obtaining alignment data,} it is also key to design suitable fine-tuning strategies for direct alignment. A straightforward approach is to optimize LLMs using the conventional sequence-to-sequence objective based on the alignment data. In addition to the conventional optimization objective, several studies further explore auxiliary losses that enhance the learning from the alignment data.} $\\bullet$ Primary training objective. Since the alignment data typically consists of an input instruction and an output response, the primary training loss is still the traditional cross-entropy loss for sequence-to-sequence learning. Based on this loss, many studies propose a number of improvement variants for enhancing the supervised alignment tuning. For example, CoH~Liu-arxiv-2023-Chain constructs the training data by prepending ``A helpful answer:'' and ``An unhelpful answer:'' to the annotated good and bad responses, respectively, and only compute losses for those response tokens with special masking. Quark~Lu-nips-2022-quark sorts model responses into different quantiles with varying alignment quality, it prepends a special reward token to each model response to represent the reward level of the response. These studies basically adopt the maximum likelihood objective, and employ instruction prefixes to guide the learning of human preference. $\\bullet$ Direct preference optimization. To better mimic the learning approach of RLHF in a supervised learning way, DPO~Rafailov-arxiv-2023-Direct proposes to reparameterize the response rewards using the policy model (\\ie the language model being optimized), and then the original reward modeling objective can be reformulated only based on the policy model. In this way, DPO removes the explicit reward modeling step, and optimizing the new learning objective that only involves the policy model is equivalent to optimizing the rewards. { Based on DPO, existing work has proposed several improvement strategies for enhancing the effectiveness or efficiency, \\eg decomposing the optimization of positive responses and negative responses into two independent components~Ethayarajh-arxiv-2024-KTO or removing the probability of the reference model in the objective function~Meng-arxiv-2024-SimPO. Furthermore, FIGA~Guo-arxiv-2023-Beyond designs a token-level contrastive loss that aims to encourage desirable tokens, penalize undesirable ones, and disregard trivial tokens. } Despite the effectiveness, recent work has also revealed that DPO may have inherent limitations in several aspects. First, based on the analysis about the magnitude and gradient directions, recent work reveals that DPO might have difficulty in well balancing the learning of positive instances and negative instances~Feng-arxiv-2024-Towards. In addition, as the reference model provides the reward scores for itself in DPO algorithm, a weak reference model would also influence the alignment performance~Gorbatovski-arxiv-2024-Learn, which can be enhanced by improved learning strategies~Kim-arxiv-2024-sDPO or well-trained policy model~Gorbatovski-arxiv-2024-Learn. $\\bullet$ Auxiliary optimization objectives. Besides the primary cross-entropy loss, several studies propose auxiliary training loss to enhance the learning from the alignment data. First, since the responses of each instruction can be scored by the reward model, the ranking loss can be used to train the model to preserve the ranking order of these responses. For example, RRHF~Yuan-RRHF-2023-arxiv samples responses from multiple sources, including model-generated responses, such as those derived from the model itself, ChatGPT, and GPT-4, as well as human-written responses, spanning both high-quality and low-quality instances. To align with the scores from reward models, it further optimizes the ranking loss by encouraging the model to have a higher conditional log probability for the response with a higher ranking. Moreover, SLiC-HF~Zhao-arxiv-2023-slichf proposes to assess the similarity between model outputs and human preference via the distance in the latent space, and introduces specific calibration and regularization loss to calibrate the candidate sequences based on human-preference data. Similarly, the difference between positive and negative responses from the reward model can be employed to construct the regularization loss~Fisch-arxiv-2024-Robust, to enhance the discrimination between positive and negative responses by LLMs. % Second, to enhance the relatedness between the response and the instruction, some work adopts contrastive learning to push up the probability of correct instruction-response pairs while pushing down incorrect instruction-response pairs. Specifically, for an output response, the proposed approach in Zhang-arxiv-2023-The contrasts the target instruction to the other irrelevant instructions. By doing so, it can enable the model to learn the right correlation between instructions and responses. \\textcolor{blue{ è¿™æ®µåˆ°åº•è¦è¯´å•¥ï¼Œlimitationè¿˜æ˜¯æ”¹è¿›æ–¹å‘ï¼Ÿï¼Ÿæœ€åŽçš„ä¸€ç‚¹åˆä¸æ˜¯limitationã€‚ã€‚ã€‚ $\\bullet$ Limitation of DPO. Despite the success of DPO, existing studies have shown their remaining challenges and characteristics for human alignment. First, based on the analysis about the magnitude and gradient directions, recent work finds that DPO can not well balance the learning of positive instances and negative instances~Feng-arxiv-2024-Towards. In addition, as the reference model provides the reward scores for itself in DPO algorithm, a weak reference model would also influence the alignment performance~Gorbatovski-arxiv-2024-Learn. Therefore, previous work utilizes curriculum learning to iteratively optimize the reference model~Kim-arxiv-2024-sDPO, or replace the it by a well-trained policy model~Gorbatovski-arxiv-2024-Learn. } } \\paratitle{Alignment with External Modules. blue{ä¸çŸ¥é“åœ¨è¯´å•¥ã€‚ã€‚ã€‚ä¸ºå•¥è¯´ï¼Œé¢„å¤„ç†ã€åŽå¤„ç†ã€‚ã€‚ã€‚ To well align LLMs with human preferences, it is promising to leverage the learnable external modules to enhance the abilities of original LLMs. Previous work either pre-processes the human requests, post-processes LLM responses, or optimizes the parameters of LLMs, based on external modules. Concretely, to pre-process the prompt from users, existing study~Kong-arxiv-2024-Aligning adopts a value model to evaluate and modify the human requests at the representation level, and feed the modified safe representations into LLMs to generate the human-aligned responses. For post-processing, an obvious approach is utilizing the responses from LLMs and human labelers to train an external module, which possesses the ability to rewrite the generated responses to the human-preferred responses~Ji-arxiv-2024-Aligner. Besides, post-processing can also be employed at the decoding stage. For details, DeRa~Liu-arxiv-2024-Decoding utilizes the weighted average of the hidden stage of the SFT model and the aligned model, to realign the LLMs and achieve a controllable alignment degree of LLMs. Finally, for LLM parameter optimization, ExPO~Zheng-arxiv-2024-Weak % utilizes the difference of the parameters between RLHF model and SFT model for extrapolating to obtain the strong model, instead of further training. Moreover, recent work~Chen-arxiv-2024-Low points out the redundancy in the alignment process,% and utilizes neuron masks and token-level rewards to enable LLMs to focus on the important information in the training instances. }} \\begin{table*[htb] \\centering Results of RLHF experiments based on the YuLan-Chat-2-13B model. 1.6\\columnwidth{!}{ tabular{llr|cccc} \\toprule 2.5{*}{Models} & 2.5{*}{tabular[c]{@{}c@{}}Dataset\\\\ Nametabular} & 2.5{*}{tabular[c]{@{}c@{}}Reward\\\\ Modeltabular} & 2{c}{Chat} & 2{c}{QA} \\\\ \\cmidrule(r){4-5}\\cmidrule(r){6-7} & & & Alignment & AlpacaFarm & MMLU & BBH3k \\\\ \\midrule YuLan-Chat-2-13B & 172~HH-RLHF & LLaMA-7B & \\\\ & & LLaMA-13B & \\\\ & & T5-large & \\\\ 2-7 & 173~OpenAI Summarize & LLaMA-13B & \\\\ & & LLaMA-7B & \\\\ & & T5-large & \\\\ \\bottomrule tabular } table*}",
      "origin_cites_number": 36
    },
    {
      "section_title": "Remarks on SFT and RLHF",
      "level": "3",
      "content": "As discussed in Section~sec-instruction, instruction tuning is the process of training pre-trained language models with formatted demonstration data (instructions paired with desired outputs). At early exploration, instruction data was mainly collected from NLP tasks~Wei-ICLR-2022-Finetuned, while it has been now extended to more diverse supervision data that pairs input and output texts (\\eg the utterances of open-ended dialogues). Training with such paired texts is also called supervised fine-tuning~(SFT) in the context of LLMs~Ouyang-arxiv-2022-Training. In this part, we mainly use the abbreviation SFT for discussion but not instruction tuning, due to the simplicity and popularity. Since SFT and RLHF are two major adaptation tuning methods for LLMs, it is important to understand the connections and difference between them. Next, we make some discussions on this issueThis part would be somehow subjective, mainly based on the authors' opinions and experiences. Comments or corrections are welcome to enhance this part. . Overall Comparison with RL Formulation. Following the discussion in Section~sub:RLHF (the part related to RL training), the text generation problem can be formulated as a decision-making process based on RL. Taking a prompt as input, the task of a LLM is to generate a text completion that appropriately responds to the prompt. This task would be completed step by step. At each step, an agent (\\ie LLM) will perform an action (\\ie generating a token) according to the policy (\\ie the generative probability distribution of LLM) conditioned on the current state (currently generated token sequence and other available context information). It is expected that a high-quality output text would be produced by the LLM, which can earn a large reward score based on the entire response. Overall, RLHF and SFT can be considered as two different training approaches to optimizing the above decision making process for LLMs. Specially, RLHF firstly learns the reward model, and then employs it to improve the LLM with RL training (\\eg PPO). As a comparison, SFT adopts a teacher-forcing approach, which directly optimizes the likelihood of a demonstration output. Such a token-level training way essentially does behavior cloning (a special algorithm of imitation learning~Ahmed-ACM-2017-Imitation): it utilizes the expert's action (\\ie the target token at each step) as the supervision label and directly learns to imitate the demonstrations from experts without specifying a reward model as in typical RL algorithms. To learn the desired policies, SFT adopts a ``local'' optimization way (\\ie token-level loss) based on demonstration data, while RLHF takes a ``global'' optimization way (\\ie text-level loss) by involving human preference. More theoretical analysis about imitation learning and reinforcement learning can be referred to the related RL literature Ahmed-ACM-2017-Imitation,Levine-youtube-2022-Imitate. $\\bullet$ \\emph{General comparison. Actually, the connections between imitation learning and reinforcement learning can be discussed in a more general way~\\cite{}, not limited to the text generation. Firstly, since SFT actually does behavior cloning, a direct problem would be whether we imitate or reinforce for text completion tasks? % In essence, RL needs to consider two major goals, namely fitting the available data and maximizing the reward; while imitation learning mainly focuses on the first goal. In the context of LLMs, SFT somehow advocates that the LLMs ``memorize'' the desired output but not necessarily learning the underlying ground-truth policy, which does not satisfy our expectation. } Pros and Cons of SFT. SFT has been shown to be an effective approach to boosting the performance of LLMs on various benchmarks~Wei-ICLR-2022-Finetuned,Chung-arxiv-2022-Scaling,alpaca,vicuna2023, which can largely enhance the task generalization ability and flexibly endow specific functions (\\eg establishing the chatbot's identity). More discussions about the usefulness of SFT can be found in Section~subsec:effectIT. It has been widely recognized that SFT mainly unlocks the abilities but not inject new abilities into LLMs. Thus, it might become problematic when one tries to stimulate the non-endogenous abilities of LLMs via SFT. % As a concrete scenario, it would potentially advocate the hallucination behaviors when demonstration data is beyond the knowledge or ability scope of LLMs, \\eg training a LLM to answer questions about its unknown facts. An interesting viewpoint from John Schulman's talk on RLHF~John-youtube-2023-RLHF is that distilling superior models to train less capable models (\\eg prompting GPT-4 to generate the response as fine-tuning data) might increase the possibilities of generating the hallucinated texts, thus likely affecting the factual accuracy of LLMs. Furthermore, as a behavior cloning method, SFT aims to imitate the behaviors (without explorations) of the experts who construct the demonstration data. However, there often exist variations among different annotators on the writing styles, quality, and preferences of demonstration data, which tends to affect the learning performance of SFT. Thus, high-quality instruction data (but not the quantity) is the primary factor for effective training of LLMs during the SFT stage~Touvron-2023-llama2-arxiv. % Pros and Cons of RLHF. % RLHF was early explored in the literature of deep RL~Christiano-NeurIPS-2017-Deep, then borrowed to improve the capacity of language models (\\eg summarization~Stiennon-arxiv-2020-learning), and subsequently adopted as the fundamental technique to develop InstructGPT~Ouyang-arxiv-2022-Training. Recently, increasing evidence~Touvron-2023-llama2-arxiv,Bai-arXiv-2022-Constitutional has demonstrated the effectiveness of RLHF in mitigating the harmful responses and enhancing the model capacity. Specially, LLaMA 2 has demonstrated that RLHF can improve both the helpfulness and harmlessness scores~Touvron-2023-llama2-arxiv, and attributed this to a better human-LLM synergy for data annotation. They explain this reason in two major aspects as follows. First, since human annotators mainly provide preference annotations for RLHF, it can largely alleviate the discrepancies of annotators as that in SFT. Secondly, preference annotation is much easier than writing the demonstration data, and annotators can even judge the quality of more superior generations than those they create, making it possible to explore a broader state space beyond what can be demonstrated by human annotators. Another key point is that RLHF essentially encourages LLMs to learn correct policies by contrasting the self-generated responses (discriminating between good and bad responses). It no longer forces the model to imitate external demonstration data, and thus can mitigate the hallucination issues with SFT as discussed aboveIn RLHF, it seems to be also important that reward models should be aware of the knowledge or ability of a LLM to be aligned. For example, LLaMA 2 adopts pre-trained chat model checkpoints to initialize reward models~\\cite{Touvron-2023-llama2-arxiv. }. Actually, RLHF has been demonstrated to be an important approach to reduce the hallucination behaviors in GPT-4~OpenAI-OpenAI-2023-GPT-4. However, RLHF inherits the drawbacks of classic RL algorithms, \\eg sample inefficiency and training instability. When adapted to LLMs, RLHF further relies on a strong SFT model as initial model checkpoint for efficiently achieving good performance. In addition, human annotators are involved in a complex iterative optimization process, in which a number of important details (\\eg the prompt selection, the schedule of reward model training and PPO training, and the settings of hyper-parameters) have important impact on the whole model performance. Overall, SFT is particularly useful to increase the model capacity of pre-trained model checkpoints right after pre-training, while RLHF is promising to further improve the model capacity of SFT models. However, RLHF has been difficult to implement, and far from well explored (according to public literature), and more improvements (\\eg efficient and reliable annotation~Bai-arXiv-2022-Constitutional and simplified optimization~Rafailov-arxiv-2023-Direct) are still needed for further research. \\ignore{",
      "origin_cites_number": 17
    },
    {
      "section_title": "Empirical Analysis for RLHF",
      "level": "3",
      "content": "Dataset $\\bullet$ HH-RLHF. $\\bullet$ OpenAI Summarize. $\\bullet$ GSM8k. Experiment Settings Results and Analysis $\\bullet$ Does RLHF improve the human alignment of the LLM? $\\bullet$ Will RLHF hurt the performance of the LLM? $\\bullet$ How the scale of Reward Model affect RLHF? $\\bullet$ How the scale of LLM affect RLHF?}",
      "origin_cites_number": 0
    },
    {
      "section_title": "Parameter-Efficient Model Adaptation",
      "level": "2",
      "content": "In the above, we have discussed the approaches of instruction tuning and alignment tuning to adapt LLMs according to specific goals. Since LLMs consist of a huge amount of model parameters, it would be costly to perform the full-parameter tuning. In this section, we will discuss how to conduct efficient tuning on LLMs. We first review several representative parameter-efficient fine-tuning methods for Transformer language models, and then summarize existing work on parameter-efficient fine-tuned LLMs.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Parameter-Efficient Fine-Tuning Methods",
      "level": "3",
      "content": "In existing literature, parameter-efficient fine-tuning~Li-ACL-2021-prefix,Lester-ACL-2021-The,Hu-ICLR-2022-LoRA has been an important topic that aims to reduce the number of trainable parameters while retaining a good performance as possible. In what follows, we briefly review four parameter-efficient fine-tuning methods for Transformer language models, including adapter tuning, prefix tuning, prompt tuning and LoRA. The illustration of these four methods are shown in Figure~fig:efficient-tuning. Adapter Tuning. Adapter tuning incorporates small neural network modules (called adapter) into the Transformer models~Houlsby-ICML-2019-Parameter. To implement the adapter module, a bottleneck architecture has been proposed in Houlsby-ICML-2019-Parameter,Hu-arXiv-2023, which first compresses the original feature vector into a smaller dimension (followed by a nonlinear transformation) and then recovers it to the original dimension. The adapter modules would be integrated into each Transformer layer, typically using a serial insertion after each of the two core parts (\\ie attention layer and feed-forward layer) of a Transformer layer. Alternatively, parallel adapters~He-ICLR-2022-towards can be also used in Transformer layers, where it places two adapter modules in parallel with the attention layer and feed-forward layer accordingly. During fine-tuning, the adapter modules would be optimized according to the specific task goals, while the parameters of the original language model are frozen in this process. In this way, we can effectively reduce the number of trainable parameters during fine-tuning. Prefix Tuning. Prefix tuning~Li-ACL-2021-prefix prepends a sequence of prefixes, which are a set of trainable continuous vectors, to each Transformer layer in language models. These prefix vectors are task-specific, which can be considered as virtual token embeddings. To optimize the prefix vectors, a reparameterization trick~Li-ACL-2021-prefix has been proposed by learning a MLP function that maps a smaller matrix to the parameter matrix of prefixes, instead of directly optimizing the prefixes. It has been shown that this trick is useful for stable training. After optimization, the mapping function would be discarded, and only the derived prefix vectors are kept to enhance task-specific performance. Since only the prefix parameters would be trained, it can lead to a parameter-efficient model optimization. Similar to prefix tuning, p-tuning v2~Liu-arXiv-2021-P-tuning incorporates layer-wise prompt vectors into the Transformer architecture specially for natural language understanding, which also utilizes multi-task learning for jointly optimizing shared prompts. It has been shown to be useful in improving the model performance of different parameter scales on natural language understanding tasks. Prompt Tuning. Different from prefix tuning, prompt tuning~Liu-arXiv-2021-GPT,Lester-ACL-2021-The mainly focuses on incorporating trainable prompt vectors at the input layerHere, prompt tuning denotes a category of related efficient tuning methods exemplified by the work~\\cite{Liu-arXiv-2021-GPT,Lester-ACL-2021-The,gu-ACL-2022-ppt, instead of a specific method as used in Lester-ACL-2021-The. Indeed, the prefix based tuning methods~Li-ACL-2021-prefix,Liu-arXiv-2021-P-tuning can be also considered as prompting methods, which are called deep prompting tuning in Liu-arXiv-2021-P-tuning. In this survey, prompt tuning specially refer to the methods that only include the prompt tokens at the input layer, in the context of LLMs. We assign p-tuning v2~Liu-arXiv-2021-P-tuning to the category of prefix tuning, because it incorporates layerwise prompts in langauge models. }. Based on the discrete prompting methods~jiang-TACL-2020-how,shin-EMNLP-2020-autoprompt, it augments the input text by including a group of soft prompt tokens (either in a free form~Liu-arXiv-2021-GPT or a prefix form~Lester-ACL-2021-The), and then takes the prompt-augmented input to solve specific downstream tasks. In implementation, task-specific prompt embeddings are combined with the input text embeddings, which are subsequently fed into language models. P-tuning~Liu-arXiv-2021-GPT has proposed a free form to combine the context, prompt and target tokens, which can be applied to the architectures for both natural language understanding and generation. They further learn the representations of soft prompt tokens by a bidirectional LSTM. Another representative approach~Lester-ACL-2021-The named prompt tuning directly prepends prefix prompts to the input. During training, only the prompt embeddings would be learned according to task-specific supervisions. Since this method only includes a small number of trainable parameters at the input layer, it has been found that the performance highly relies on the model capacity of the underlying language models~Lester-ACL-2021-The. Low-Rank Adaptation~(LoRA). LoRA~Hu-ICLR-2022-LoRA imposes the low-rank constraint for approximating the update matrix at each dense layer, so as to reduce the trainable parameters for adapting to downstream tasks. Consider the case of optimizing a parameter matrix $W$. The update process can be written in a general form as: $W \\leftarrow W + \\Delta W$. The basic idea of LoRA is to freeze the original matrix $W \\in R^{m \\times n}$ while approximating the parameter update $\\Delta W$ by low-rank decomposition matrices, \\ie $\\Delta W=A\\cdot B^\\top$, where $A\\in R^{m \\times k}$ and $B\\in R^{n \\times k}$ are the trainable parameters for task adaptation and $k \\ll \\min(m,n)$ is the reduced rank. The major merit of LoRA is that it can largely save the memory and storage usage (\\eg VRAM). Further, one can only keep a single large model copy, while maintaining a number of task-specific low-rank decomposition matrices for adapting to different downstream tasks. Further, several studies have also discussed how to set the rank in a more principled approach, \\eg importance score based allocation~Zhang-arXiv-2023-Adaptive and search-free optimal rank selection~Valipour-arXiv-2022-DyLoRA. Besides the above methods, there is extensive research on efficient tuning of Transformer language models. However, a more comprehensive discussion of efficient tuning is beyond the scope of this article, which can be found in the related papers on this topic~He-ICLR-2022-towards,Ding-NMI-2023-Parameter.",
      "origin_cites_number": 23
    },
    {
      "section_title": "Parameter-Efficient Fine-Tuning on LLMs",
      "level": "3",
      "content": "With the rising of LLMs, efficient tuning has attracted increasing research attention for developing a more lightweight adaptation approach in downstream tasks. In particular, LoRA~Hu-ICLR-2022-LoRA has been widely applied to open-source LLMs (\\eg LLaMA and BLOOM) for parameter-efficient fine-tuning. Among these research attempts, LLaMA and its variants have gained much attention for parameter-efficient tuning. For example, Alpaca-LoRA~Alpaca-LoRA has been trained using LoRA as a lightweight tuned version of Alpaca~Taori-github-2023-Stanford (a fine-tuned 7B LLaMA model with 52K human demonstrations of instruction following). There are extensive explorations of Alpaca-LoRA ranging in different languages or model sizes, which can be found in the collection pagehttps://github.com/tloen/alpaca-lora. A recent study LLaMA-Adapter~Zhang-arXiv-2023-LLaMA-Adapter inserts learnable prompt vectors into each Transformer layer, in which zero-initialized attention has been proposed to improve the training by mitigating the influence of under-fitted prompt vectors. They also extend this approach to a multi-modal setting, \\eg visual question answering. Further, an empirical study~Hu-arXiv-2023 has been conducted to examine the effect of different tuning methods on language models. They compare four efficient tuning methods including serial adapter tuning~Houlsby-ICML-2019-Parameter, parallel adapter tuning~He-ICLR-2022-towards,Pfeiffer-EMNLP-2022-MAD-X, and LoRA~Hu-ICLR-2022-LoRA, on three open-source LLMs, namely GPT-J (6B), BLOOM (7.1B) and LLaMA (7B), for evaluation. Based on the experimental results on six math reasoning datasets, they show that these efficient-tuning methods under-perform the reference baseline GPT-3.5 on difficult tasks, while achieving a comparable performance on simple tasks. Overall, LoRA performs relatively well among these comparison methods, using significantly fewer trainable parameters. As an important resource, the library PEFT~peft-github-2022 (standing for parameter-efficient fine-tuning) has been released on GitHubhttps://github.com/huggingface/peft. It has included several widely used efficient tuning methods, including LoRA~Hu-ICLR-2022-LoRA/AdaLoRA~Zhang-arXiv-2023-Adaptive, prefix-tuning~Li-ACL-2021-prefix,Liu-arXiv-2021-P-tuning, P-Tuning~Liu-arXiv-2021-GPT, and prompt-tuning~Lester-ACL-2021-The. Further, it supports a number of language models such as GPT-2 and LLaMA, and also covers several representative vision Transformer models (\\eg ViT and Swin Transformer). As discussed in Section~sec-PEFT-methods, there have been a large number of efficient tuning methods proposed in the existing literature. However, most of these approaches are tested on small-sized pre-trained language models, instead of the LLMs. So far, there still lacks a thorough investigation on the effect of different efficient tuning methods on large-sized language models at different settings or tasks.",
      "origin_cites_number": 14
    },
    {
      "section_title": "Utilization",
      "level": "1",
      "content": "table*[t] \\centering Typical LLM utilization methods and their key points for ICL, CoT, and planning. Note that the key points only highlight the most important technical contribution. \\textwidth{!}{% tabular{l|l|l} \\toprule Approach & Representative Work & Key Point \\\\ \\midrule 6{*}{tabular[c]{@{}l@{}}In-context\\\\Learning~(ICL)tabular} & KATE~Liu-ACL-2022-What & Demonstration selection (similar; k-NN) \\\\ & EPR~Rubin-NAACL-2022-Learning & Demonstration selection (dense retrieval; constrative learning) \\\\ & SG-ICL~Kim-2022-arxiv-Self & Demonstration selection (LLM as the demonstration generator) \\\\ & APE~Zhou-2023-ICLR-Large & Demonstration format (automatic generation \\& selection) \\\\ & Structured Prompting~Hao-2022-arxiv-Structured & Demonstration format (grouped context encoding; rescaled attention) \\\\ & GlobalE \\& LocalE~Lu-ACL-2022-Fantasically & Demonstration order (entropy-based metric; probing set generation with LLM) \\\\ \\midrule 6{*}{tabular[c]{@{}l@{}}Chain-of-thought\\\\Prompting~(CoT)tabular} & Complex CoT~Fu-arxiv-2022-Complexity & Demonstration (complexity-based selection) \\\\ & Auto-CoT~Zhang-arxiv-2022-Automatic & Demonstration (automatic generation) \\\\ & Selection-Inference~Creswell-2022-arXiv-selection & Generation (alternate between selection and inference) \\\\ & Self-consistency~Wang-arxiv-2022-Self-Consistency & Generation (diverse paths; self-ensemble) \\\\ & DIVERSE~Li-arxiv-2022-On & Generation (diverse paths); Verification (step-wise voting) \\\\ & Rationale-augmented ensembles~Wang-arxiv-2022-Rationale & Generation (rationale sampling) \\\\ \\midrule 13{*}{Planning} & Least-to-most prompting~Zhou-arxiv-2022-Least & Plan generation (text-based; problem decomposition) \\\\ & DECOMP~Khot-2022-arXiv-Decomposed & Plan generation (text-based; problem decomposition) \\\\ & PS~Wang-arXiv-2023-Plan & Plan generation (text-based) \\\\ & Faithful CoT~Lyu-arxiv-2023-Faithful & Plan generation (code-based) \\\\ & PAL~Gao-arxiv-2022-PAL & Plan generation (code-based; Python) \\\\ & HuggingGPT~Shen-2023-arXiv-Hugginggpt & Plan generation (code-based; models from HuggingFace) \\\\ & AdaPlanner~Sun-2023-arXiv-adaplanner & Plan refinement (skill memory) \\\\ & TIP~Lu-2023-arXiv-multimodal & Feedback acquisition (visual perception) \\\\ & RAP~Hao-2023-arXiv-reasoning & Feedback acquisition (LLM as the world model); Plan refinement (Monte Carlo Tree Search) \\\\ & ChatCoT~Chen-2023-arXiv-chatcot & Feedback acquisition (tool); Plan refinement (conversation between LLM and tools) \\\\ & ReAct~Yao-2022-arXiv-react & Feedback acquisition (tool); Plan refinement (synergizing reasoning and acting) \\\\ & Reflexion~Shinn-2023-arXiv-Reflexion & Feedback acquisition (text-based self-reflection); Plan refinement (dynamic memory) \\\\ & Tree of Thoughts~Yao-arxiv-2023-Tree & Feedback acquisition (vote comparison); Plan refinement (tree-based search) \\\\ \\bottomrule tabular% } table* After pre-training or adaptation tuning, a major approach to using LLMs is to design suitable prompting strategies for solving various tasks. {In existing literature, task-specific prompts can be effectively learned through manual creation and automatic optimization.} {A representative prompting method is in-context learning~Brown-NeurIPS-2020-Language, Dong-arxiv-2023-A, which formulates the task description and/or demonstrations in the form of natural language text.} In addition, chain-of-thought prompting~Wei-arxiv-2022-chain can be employed to enhance in-context learning by involving a series of intermediate reasoning steps in prompts. Furthermore, planning~Zhou-arxiv-2022-Least is proposed for solving complex tasks, which first breaks them down into smaller sub-tasks and then generates a plan of action to solve these sub-tasks one by one. We summarize representative work for these prompting approaches in Table~tab:utilization. Next, we will elaborate on the details of the four techniques.",
      "origin_cites_number": 28
    },
    {
      "section_title": "Prompting",
      "level": "2",
      "content": "As discussed in previous work~Liu-survey-2023-Pre-train, prompting is the major approach to utilizing LLMs for solving various tasks. Since the quality of prompts will largely influence the performance of LLMs in specific tasks, there have been a series of studies proposed to generate suitable task prompts through manual creation or automatic optimization, which will be introduced in this section.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Prompt Creation",
      "level": "3",
      "content": "The process of manually creating a suitable prompt is also called prompt engineering~Liu-arxiv-2022-Design,White-arxiv-2023-Prompt. A well-designed prompt is very helpful to elicit the abilities of LLMs for accomplishing specific tasks. In this part, we will first introduce the key components of prompts and discuss several principles for prompt design. Then, we evaluate ChatGPT with different prompts to show the results on several representative tasks. We are aware that there have been several existing papers~Santu-arxiv-2023-TELeR,White-arxiv-2023-Prompt and websites~OpenAI-OpenAI-2023-PromptGuide,Contributors-AIShort-2023-AIShort,Contributors-Github-2023-Awesome that present the suggestions and guidelines to design good prompts. As a comparison, we mainly aim to discuss the key factors (ingredients and principles) that are useful for prompt creation, and provide experimental results and analysis on popular tasks as the reference to the beginners. Key Ingredients. Typically, there are four key ingredients that {depict the functionality of a prompt for eliciting the abilities of LLMs to complete the tasks}, including task description, input data, contextual information, and prompt style. To have an intuitive understanding of our discussion, we also present three prompt examples for question answering, meta-review generation, and text-to-SQL in Table~tab:prompt-examples. \\textbullet~Task description. A task description is typically a specific instruction that LLMs are expected to follow. In general, one should clearly describe the task goal in natural language. For the tasks with special input or output format, detailed clarifications are often needed, and one can further utilize keywords to highlight the special settings for better guiding LLMs in task completion. \\textbullet~Input data. In common cases, it is straightforward to describe input data (\\eg an instance to be responded by LLMs) in natural language. For special input data, such as knowledge graph and table, it is necessary to {apply an appropriate and convenient way} % to make them readable for LLMs. % For structured data, linearization is commonly used to transform the original records (\\eg knowledge triples) into sequences~Jiang-2023-arxiv-StructGPT due to the simplicity. Further, the programming language (\\eg executable code) has also been utilized to formulate the structured data, % {which can also support using external tools (\\eg program executor) to produce the precise results~Beurer-arxiv-2023-Prompting,Lu-arxiv-2023-Chameleon. } \\textbullet~Contextual information. In addition to the task description and input data, contextual or background information is also essential for specific tasks. For example, retrieved documents are highly useful for open-domain question answering as supporting evidence. Both the quality of the retrieved documents and their relevance to the question have an impact on the generated answers~Ren-arxiv-2023-Investigating. Thus, it needs to include such information in a proper prompt pattern or expression format. Furthermore, in-context task exemplars are also helpful for eliciting LLMs to accomplish a complex task, which can better depict the task goal, {the special output formats, and the mapping relation between input and output.} \\textbullet~{Prompt style.} For different LLMs, it is important to design a suitable prompt style for eliciting their abilities to solve specific tasks. Overall, one should express the prompt as a clear question or detailed instruction that can be well understood and answered. In some cases, it is also useful to add the {prefix or suffix} to better guide LLMs. For example, using the prefix ``Let us think step by step'' can help elicit LLMs perform step-by-step reasoning, and using the prefix ``You are an expert on this task (or in this domain)'' can boost the performance of LLMs in some specific tasks. Further, for chat-based LLMs (\\eg ChatGPT), instead of directly feeding a long or complex task prompt, it is suggested to decompose it into multiple prompts for the sub-tasks and then feed them into LLMs via a multi-turn conversation~Chen-2023-arXiv-chatcot. Design Principles. Based on the key ingredients of prompts, we summarize several critical design principles that can help create more effective prompts for solving various tasks. \\textbullet~Expressing the task goal clearly. {Task descriptions should not be ambiguous or unclear, which likely lead to inaccurate or inappropriate responses.} This highlights the need for clear and unambiguous directives when utilizing these models~Ouyang-arxiv-2022-Training. A clear and detailed description should contain various elements to explain a task, including task objective, input/output data (\\eg ``Given a long document, I want you to generate a concise summary.''), and the response constraints (\\eg ``the length of the summary cannot exceed 50.''). By providing a well-clarified task description, LLMs can more effectively understand the target task and generate the desired output. { \\textbullet~Decomposing into easy, detailed sub-tasks. To solve complex tasks, it is important to decompose the difficult task into several more easier, detailed sub-tasks for helping LLMs accomplish the goal step by step, which is closely related to the planning technique in Section~subsec-planning. } For example, following the suggestion~Santu-arxiv-2023-TELeR, we can explicitly list the {sub-tasks in the form of multiple numbered items} (\\eg ``Braid a coherent narrative by performing the following tasks: 1. ...; 2. ...; 3. ...''). By decomposing a target task into sub-tasks, LLMs can focus on solving easier sub-tasks and finally achieve more accurate results for complex tasks. \\textbullet~Providing few-shot demonstrations. As discussed in Section~subsec-icl, LLMs can benefit from in-context learning for solving complex tasks, where the prompts contain a small number of task examples of the desired input-output pairs, \\ie few-shot demonstrations. Few-shot demonstrations can help LLMs learn the semantic mapping between input and output without parameter tuning. In practice, it is suggested that one should generate a few high-quality demonstrations for the target task, which would highly benefit the final task performance. \\textbullet~Utilizing model-friendly format. Since LLMs are pretrained on specially constructed datasets, there are some prompt formats that can make LLMs better understand the instruction. For example, as the OpenAI documentation suggests, we can use \\#\\#\\# or \"\"\" as a stop symbol to separate the instruction and context, which can be better understood by LLMs. As a general guideline, most existing LLMs perform a task better in English, thus it is useful to employ English instructions to solve difficult tasks based on machine translation. { \\textbullet~Adopting role-playing strategies. Since LLMs are pretrained on extensive corpora containing diverse characters and dialogues, they possess an inherent ability for role-playing. This feature can be harnessed through specific prompts to enhance the corresponding capacity for some specific domains~Amatriain-CoRR-2024-Prompt. For instance, when solving a math problem, we can use a prompt prefix like ``You are an expert in mathematics''. This enables LLMs to solve the problem from an expert's perspective, thereby leveraging their pretrained knowledge more effectively. By guiding LLMs with role-playing prompts, they can often generate more reasonable and accurate solutions. } Useful Tips. In addition to the design principles, we also present a collection of useful prompt tips based on existing work or our empirical experiences in Table~tab-tips. Note that these tips are suggested in a general manner, it does not indicate that they are the best prompts for the corresponding tasks. This part will be continuously updated with more guidelines or tips. We welcome readers to contribute to this collection of prompt tips. We present the detailed procedure to contribute to the prompt tips, at the link: https://github.com/RUCAIBox/LLMSurvey/tree/main/Prompts. table*[htb] \\centering A collection of useful tips for designing prompts {that are collected from online notes~\\cite{Santu-arxiv-2023-TELeR,White-arxiv-2023-Prompt,OpenAI-OpenAI-2023-PromptGuide,Contributors-AIShort-2023-AIShort and experiences from our authors}, where we also show the related ingredients and principles (introduced in Section~subsec:promptdesign). We abbreviate principles as Prin. and list the IDs of the related principles for each prompt. {1: expressing the task goal clearly; 2: decomposing into easy, detailed sub-tasks; 3: providing few-shot demonstrations; 4: utilizing model-friendly format.}} \\scriptsize % tabular{cp{0.75\\textwidth}c} \\toprule Ingredient & Collected Prompts & Prin.\\\\ \\midrule 4{*}{Task Description} & T1. Make your prompt \\textbf{as detailed as possible}, \\eg ``Summarize the article into a short paragraph within 50 words. The major storyline and conclusion should be included, and the unimportant details can be omitted.'' & 1 \\\\ & T2. It is helpful to let the LLM know that it is \\underline{an expert with a prefixed prompt}, \\eg ``You are a sophisticated expert in the domain of compute science.'' & 1 \\\\ & T3. Tell the model \\underline{more what it should do}, but not what it should not do. & 1 \\\\ & T4. To avoid the LLM to generate too long output, you can just use the prompt: ``Question: { Short Answer: {}}''. Besides, you can also use the following suffixes, ``in a or a few words'', ``in one of two sentences''. & 1 \\\\ \\midrule 2{*}{Input Data} & I1. For the question required factual knowledge, it is useful to first \\textbf{retrieve relevant documents} via the search engine, and then \\textbf{concatenate them into the prompt} as reference. & 4\\\\ & I2. To highlight some important parts in your prompt, please \\textbf{use special marks}, \\eg quotation ($\"\"$) and line break ($\\backslash$n). You can also use both of them for emphasizing. & 4 \\\\ \\midrule 4{*}{Contextual Information} & C1. For complex tasks, you can \\underline{clearly describe the required intermediate steps} to accomplish it, \\eg ``Please answer the question step by step as: Step 1 - Decompose the question into several sub-questions, $\\cdots$'' & 2 \\\\ & C2. If you want LLMs to provide the score for a text, it is necessary to provide a \\underline{detailed description about the} \\underline{scoring standard} with examples as reference. & 1 \\\\ & C3. When LLMs generate text according to some context (\\eg making recommendations according to purchase history), instructing them with \\underline{the explanation about the generated result} conditioned on context is helpful to improve the quality of the generated text. & 2 \\\\ & C4. An approach similar to \\underline{tree-of-thoughts} but can be \\underline{done in one prompt}: \\eg Imagine three different experts are answering this question. All experts will write down one step of their thinking, then share it with the group of experts. Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave. The question is & 2 \\\\ \\midrule 9{*}{Demonstration} & D1. \\textbf{Well-formatted in-context exemplars} are very useful, especially for producing the outputs with complex formats. & 3 \\\\ & D2. For few-shot chain-of-thought prompting, you can also use the prompt ``Let's think step-by-step'', and the few-shot examples should be \\underline{separated by ``$\\backslash$n''} instead of full stop. & 13 \\\\ & D3. You can also \\underline{retrieve similar examples} in context to supply the useful task-specific knowledge for LLMs. To retrieve more relevant examples, it is useful to \\underline{first obtain the answer} of the question, and then concatenate it with the question for retrieval. & 34 \\\\ & D4. The \\underline{diversity of the in-context exemplars} within the prompt is also useful. If it is not easy to obtain diverse questions, you can also seek to keep the \\underline{diversity of the solutions} for the questions. & 3 \\\\ & D5. When using chat-based LLMs, you can \\underline{decompose in-context exemplars into multi-turn messages}, to better match the human-chatbot conversation format. Similarly, you can also decompose the reasoning process of an exemplars into multi-turn conversation. & 3 \\\\ & D6. \\underline{Complex and informative} in-context exemplars can help LLMs answer complex questions. & 3 \\\\ & D7. As a symbol sequence can typically be divided into multiple segments (\\eg $i_1, i_2, i_3$ $\\longrightarrow$ $i_1, i_2$ and $i_2, i_3$), the preceding ones can be used \\underline{as in-context exemplars} to guide LLMs to predict the subsequent ones, meanwhile providing historical information. & 23 \\\\ % & D8. \\underline{Order matters} for in-context exemplars and prompts components. For very long input data, the position of the question (first or last) may also affect the performance. & 3 \\\\ & D9. If you can not obtain the in-context exemplars from existing datasets, an alternative way is to use the \\underline{zero-shot} \\underline{generated ones} from the LLM itself. & 3 \\\\ \\midrule 8{*}{Other Designs} & O1. Let the \\textbf{LLM check its outputs} before draw the conclusion, \\eg ``Check whether the above solution is correct or not.'' & 2 \\\\ & O2. If the LLM can not well solve the task, you can \\underline{seek help from external tools} by prompting the LLM to manipulate them. In this way, the tools should be encapsulated into callable APIs with detailed description about their functions, to better guide the LLM to utilize the tools. & 4 \\\\ & O3. The prompt should be \\underline{self-contained}, and better not include pronouns (\\eg it and they) in the context. & 1 \\\\ & O4. When using LLMs for \\underline{comparing} two or more examples, the order affects the performance a lot. & 1 \\\\ & O5. Before the prompt, \\underline{assigning a role for the LLM} is useful to help it better fulfill the following task instruction, \\eg ``I want you to act as a lawyer''. & 1\\\\ & O6. OpenAI models can perform a task better in English than other languages. Thus, it is useful to first \\underline{translate the input into English} and then feed it to LLMs. & 4 \\\\ & O7. For multi-choice questions, it is useful to \\underline{constrain the output space} of the LLM. You can use a more detailed explanation or just imposing constraints on the logits. & 1 \\\\ & O8. For sorting based tasks (\\eg recommendation), instead of directly outputting the complete text of each item after sorting, one can \\underline{assign indicators} (\\eg ABCD) to the unsorted items and instruct the LLMs to directly output the sorted indicators. & 1 \\\\ \\bottomrule tabular table* \\begin{enumerate \\item Make your prompt \\textbf{as detailed as possible}, \\eg ``Summarize the given article into a short paragraph with less than 50 words. The major storyline and conclusion should be included, and the unimportant details can be omitted.'' \\item It is helpful to let the LLM know that it is \\underline{an expert with a prefixed prompt}, \\eg ``You are a sophisticated expert in the domain of compute science.'' \\item To highlight some important parts in your prompt, please \\textbf{use special marks}, \\eg quotation ($\"\"$) and line break ($\\backslash$n). You can also use both of them for emphasizing. \\item For complex tasks, you can \\underline{clearly describe the required intermediate steps} to accomplish it, \\eg ``Please answer the question step by step as: Step 1 - Decompose the question into several sub-questions, $\\cdots$'' \\item Few \\textbf{in-context well-formulated exemplars} are very useful to guide LLMs, especially for producing the outputs with complex formats. \\item To avoid the LLM to generate too long output, you can \\textbf{set up a target length} in the prompt, \\eg ``within 50 words''. \\item For the question required factual knowledge, it is useful to first \\textbf{retrieve relevant documents} via the search engine, and then \\textbf{concatenate them into the prompt} as reference. \\item Let the \\textbf{LLM check its generated results} before draw the conclusion, \\eg ``Judge whether the above solution is correct or not.'' \\item If the LLM can not well solve the task, you can \\underline{seek help from external tools} by prompting the LLM to manipulate them. In this way, the tools should be encapsulated into several APIs with detailed description about their functionalities, to better guide the LLM to utilize the tools. \\item The prompt should be \\underline{self-contained}, and better not include the information in the context with pronouns (\\eg it and they). \\item Tell the model \\underline{more what it should do}, but not what it should not do. \\item For few-shot chain-of-thought prompting, you can also use the prompt ``Let's think step-by-step'', and the few-shot examples should be \\underline{separated by ``$\\backslash$n''} instead of full stop. \\item You can also \\underline{retrieve similar examples} in context to supply the useful task-specific knowledge for LLMs. To retrieve more relevant examples, it is useful to \\underline{first obtain the answer} of the question, and then concatenate it with the question for retrieval. \\item The \\underline{diversity of the in-context exemplars} within the prompt is also useful. If it is not easy to obtain diverse questions, you can also seek to keep the \\underline{diversity} \\underline{of the solutions} for the questions. \\item When using chat-based LLMs, you can \\underline{decompose the} \\underline{in-context exemplars into multi-turn messages}, to better match the human-chatbot conversation format. Similarly, you can also decompose the reasoning process of an exemplars into multi-turn conversation. \\item Using \\underline{more complex and informative} in-context exemplars are helpful for LLMs to answer more complex questions. \\item As a sequential data can typically be divided into multiple spans, the front ones can be used as in-context exemplars to guide LLMs and meanwhile provide historical information. Similarly, such in-context exemplars can also be \\underline{formulated as a multi-turn conversation} according to the sequential order, where the conversation naturally flows with the sequential data going. \\item \\underline{Order matters} for in-context exemplars and prompts components. For very long input data, the location of the question (First or Last) may also affect the performance. \\item If you want LLMs to provide the score for the text, it is necessary to provide a \\underline{detailed description} \\underline{about the scoring standard} with examples as reference. \\item If you want LLMs to generate a short answer that can be easily used for parsing, a simple way is just using the prompt: ``Question: { Short Answer: {}}''. Besides, you can also use the following suffixes, ``in a or a few words'', ``in one of two sentences''. \\item When using LLMs for \\underline{comparing} two or more examples, the order would affect the performance a lot. \\item Before the prompt, \\underline{assigning a role for the LLM} is useful to help it better fulfill the following task instruction, \\eg I want you to act as a lawyer. \\item OpenAI models can perform a task better in English than other languages. Thus, it is useful to first \\underline{translate the input into English} and then feed it to LLMs. \\item For multi-choice questions, it is useful to \\underline{constrain} \\underline{the output space} of the LLM. You can use a more detailed explanation or just imposing constrain on the logits. \\item If you can not obtain the in-context exemplars from existing datasets, an alternative way is to use the \\underline{zero-shot generated ones} from the LLM itself. \\item For sorting tasks (e.g. recommendation), instead of directly outputting the complete text of each item after sorting, one can choose to \\underline{assign indicators} (\\eg ABCD) to the unsorted items and instruct the LLMs to directly output the sorted indicators. \\item When LLMs generate text according to some context (\\eg recommend according to purchase history), instructing them to \\underline{explain the connections} between the generated text and the context will contribute to higher quality of the generated text. enumerate } Empirical Analysis. We further conduct empirical studies to present the impact of prompts on task performance. To conduct the experiments, we select a variety of tasks that span language generation, knowledge utilization, complex reasoning, structure data generation, and information retrieval. For each task, we manually write a prompt that follows general guidelines introduced above. Note that the tested prompts may not be the optimal for these tasks, since they mainly aim to help readers understand how to write an effective prompt for solving different tasks. Also, we add a simplified prompt as the comparison for most tasks. Following the experimental settings in Section~sec-empirical, we examine the 3-shot performance of ChatGPT on complex reasoning tasks (Colored Objects and GSM8k), and zero-shot performance on other tasks. We report the experimental results in Table~tab-instructions, where we also include the supervised performance in existing papers as reference. $\\bullet$ Carefully designed prompts can boost the zero-shot or few-shot performance of ChatGPT. By comparing the results of using different prompts on the same task, we can see that using the carefully designed prompts can achieve better performance than the simpler ones. {In the carefully designed prompts, we provide a more clearly expressed task description (\\eg WMT and WikiFact), or use a model-friendly format (\\eg GSM8k and OBQA). For example, for WikiFact task, the prompt with a more detailed task description leads to a performance increase from 29.25 to 31.21.} $\\bullet$ {More complex tasks can benefit more from careful prompt engineering on ChatGPT. In the WikiFact and Colored Objects tasks, the designed prompts have greatly improved the performance of ChatGPT, \\ie from 23.61 to 28.47 on WikiFact and from 53.20 to 66.75 on Colored Objects. It indicates the necessity of prompt engineering for LLMs to perform well on complex tasks, since these tasks typically have specific output formats or require background knowledge. Our example prompts provide more detailed task description (\\eg output format and task goal), which can help ChatGPT better understand the complex task requirement for fulfilling it.} $\\bullet$ {For mathematical reasoning tasks, it is more effective to design specific prompts based on the format of programming language. For GSM8k, the designed prompt employs code-formatted few-shot demonstrations to convert this mathematical reasoning task into code generation task, which can leverage the strong code synthesis ability of ChatGPT for solving mathematical problems. Further, with the help of an external program executor, we are able to obtain more precise results instead of using LLMs for arithmetic operation. As we can see, the performance is boosted from 78.47 to 79.30 on GSM8k, indicating the usefulness of programming language in mathematical reasoning tasks.} % $\\bullet$ {In knowledge utilization and complex reasoning tasks, ChatGPT with proper prompts achieves comparable performance or even outperforms the supervised baselines methods. In knowledge utilization and complex reasoning tasks, ChatGPT with proper zero-shot or few-shot prompts can achieve comparable performance or even outperform the supervised methods, {\\eg 31.21 (ChatGPT) v.s. 34.20 (supervised baseline) on WikiFact.} Despite that, ChatGPT still performs worse than supervised baseline models on some specific tasks (\\eg ARC and WikiFact), since these supervised models have been specially optimized with task-specific data. } $\\bullet$ Through suitable prompt engineering, LLMs can handle some non-traditional NLP tasks. {With the help of specific prompts, ChatGPT can also accomplish non-traditional NLP tasks, \\ie the general recommendation and conversational recommendation. A key point is that these tasks can be well expressed or described in natural language. } However, the performance of ChatGPT is still far from the referenced performance in these tasks, as LLMs cannot directly fit these tasks, which require specific domain knowledge and task adaptation~Zhang-2023-arxiv-recommendation,Hou-2023-arxiv-large. table*[ht] \\footnotesize \\centering Example instructions collected from \\cite{Santu-arxiv-2023-TELeR,Chang-arxiv-2023-How. The LightSkyBlue1{blue} text denotes the task description, the tPink{red} text denotes the contextual information, the tGreen{green} text denotes the demonstrations, and the Khaki1{gold} text denotes the prompt style.} tabular{p\\textwidth} \\toprule LightSkyBlue1{ppl\\selectfont Use the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write ``I could not find an answer.''} \\\\ 0em{1pt}{1pt} tGreen tPink{ppl\\selectfont Articles: ``````Joao Moutinho is a Portuguese footballer who last played as a central midfielder for Premier League club Wolverhampton Wanderers and the Portugal national team.'''''' } \\\\ 0em{1pt}{1pt} tGreen{ppl\\selectfont Question: Is the following sentence plausible? 'Joao Moutinho was out at third.' } \\\\ tGreen{ppl\\selectfont Answer: Khaki2{Let's think step by step. Joao Moutinho is a soccer player. Being out at third is part of baseball, not soccer.} So the answer is No.} \\\\ tGreen{ppl\\selectfont ...} \\\\ tGreen{ppl\\selectfont $<$Demonstrations$>$} \\\\ \\\\ {ppl\\selectfont Articles: $<$insert articles, each delimited by triple quotes$>$} \\\\ {ppl\\selectfont Question: $<$insert question$>$} \\\\ {ppl\\selectfont Answer:} \\\\ \\midrule[0.9pt] \\midrule[0.9pt] LightSkyBlue1{ppl\\selectfont Prepare a meta-review by answering the following questions from the reviewer comments (provided after the questions).} \\\\ 0em{1pt}{1pt} Khaki1{ppl\\selectfont 1. Based on the reviewerâ€™s comments, what are the core contributions made by this manuscript?} \\\\ Khaki1{ppl\\selectfont 2. What are the common strengths of this work, as mentioned by multiple reviewers?} \\\\ Khaki1{ppl\\selectfont 3. What are the common weaknesses of this work, as highlighted by multiple reviewers?} \\\\ Khaki1{ppl\\selectfont 4. What suggestions would you provide for improving this paper?} \\\\ Khaki1{ppl\\selectfont 5. What are the missing references mentioned by the individual reviews?} \\\\ 0em{1pt}{1pt} tGreen{ppl\\selectfont The review texts are below: $<$insert three comments $R_1$, $R_2$, $R_3$ from the reviewers$>$} \\\\ tGreen{ppl\\selectfont Meta-review: $<$insert meta-review$>$} \\\\ tGreen{ppl\\selectfont ...} \\\\ tGreen{ppl\\selectfont $<$Demonstrations$>$} \\\\ \\\\ {ppl\\selectfont Provide justification for your response in detail by explaining why you made the choices you actually made. A good output should be coherent, highlight major strengths/issues mentioned by multiple reviewers, be less than 400 words in length, and finally, the response should be in English only.} \\\\ \\\\ {ppl\\selectfont The review texts are below: $<$insert three comments $R_1$, $R_2$, $R_3$ from the reviewers$>$} \\\\ {ppl\\selectfont Meta-review: } \\\\ \\midrule[0.9pt] \\midrule[0.9pt] tPink{ppl\\selectfont CREATE TABLE Highschooler (} \\\\ tPink{ppl\\selectfont ID int primary key,} \\\\ tPink{ppl\\selectfont name text,} \\\\ tPink{ppl\\selectfont grade int} \\\\ tPink{ppl\\selectfont );} \\\\ tPink{ppl\\selectfont /*} \\\\ tPink{ppl\\selectfont 3 example rows:} \\\\ tPink{ppl\\selectfont SELECT * FROM Highschooler LIMIT 3;} \\\\ tPink{ppl\\selectfont ID \\quad name \\quad grade} \\\\ tPink{ppl\\selectfont 1234 \\quad Janie \\quad 8} \\\\ tPink{ppl\\selectfont 5678 \\quad Mary \\quad 8} \\\\ tPink{ppl\\selectfont 9012 \\quad Mike \\quad 9} \\\\ tPink{ppl\\selectfont */} \\\\ 0em{1pt}{1pt} LightSkyBlue1{ppl\\selectfont Using valid SQLite, answer the following questions for the tables provided above.} \\\\ 0em{1pt}{1pt} tGreen{ppl\\selectfont Question: What is Kyle's id?} \\\\ tGreen{ppl\\selectfont SQL: SELECT ID FROM Highschooler WHERE name=``Kyle'';} \\\\ tGreen{ppl\\selectfont ...} \\\\ tGreen{ppl\\selectfont $<$Demonstrations$>$} \\\\ \\\\ {ppl\\selectfont Question: $<$insert question$>$} \\\\ {ppl\\selectfont SQL: } \\\\ \\bottomrule tabular table*",
      "origin_cites_number": 13
    },
    {
      "section_title": "Prompt Optimization",
      "level": "3",
      "content": "{Although manually creating task prompts is more intuitive, it is time consuming and, more importantly, models are highly sensitive to the crafted prompts---improper prompts will lead to low task performance (as shown in Table~tab-instructions). Therefore, a large body of studies propose automatic optimization approaches for discrete prompts and continuous prompts to achieve the optimal performance~shin-EMNLP-2020-autoprompt,Li-ACL-2021-prefix. In this part, we will detail these studies from two perspectives, \\ie discrete prompts and continuous prompts.} Discrete Prompt Optimization. {Discrete prompt is typically composed of a sequence of natural language tokens. Despite that the form is simple and flexible, optimizing prompts in discrete space is a challenging problem due to the combinatorial huge search space. To automatically search effective prompts for downstream tasks, existing studies propose a wide spectrum of discrete prompt optimization approaches, which are detailed as follows.} $\\bullet$ {Gradient-based approaches. This kind of approaches aims to optimize the prompt search process by maximizing the output likelihood via gradient update~shin-EMNLP-2020-autoprompt,Wen-CoRR-2023-Hard,Gao-ACL-2021-Making,Zhou-CoRR-2023-InstructZero, Lin-arxiv-2023-Use. As a representative work, Auto-Prompt~shin-EMNLP-2020-autoprompt proposes a gradient-guided method to greedily search the optimal token for each position of the prompt, leveraging the gradient approximated by the change in the log-likelihood when replacing a prompt token with another candidate token from vocabulary. However, such a search process can be extremely expensive since it needs to evaluate each candidate token for each position of the prompt, leading to a number of additional forward passes. Therefore, an improved gradient method~Wen-CoRR-2023-Hard has been proposed by transforming discrete tokens into continuous embeddings and computing the gradient on continuous space during optimization. $\\bullet$ {RL-based approaches. Since discrete prompts are difficult to be learned through gradient back-propagation, a number of studies propose to formulate the discrete prompt optimization as a reinforcement learning (RL) problem and leverage RL algorithms for optimization~Deng-EMNLP-2022-RLPrompt, Zhang-ICLR-2023-TEMPERA,Jafari-arxiv-2024-MORL,Kong-arxiv-2024-PRewrite. For example, RLPrompt~Deng-EMNLP-2022-RLPrompt {trains a policy network to generate desired prompts with multiple reward functions}. In this approach, several effective reward stabilization strategies are also proposed to enhance the RL training efficiency. Compared to previous work that requires sufficient data for training, TEMPERA~Zhang-ICLR-2023-TEMPERA proposes to edit prompts at test time {by utilizing a pre-trained RL agent to sequentially edit different parts of a manually-written initial prompt. { Although these methods are simple and effective, they explore a manually defined edit space (\\eg add, swap and delete) and focus on modifying the original prompt, which limits the flexibility of prompt search. In contrast, PRewrite~Kong-arxiv-2024-PRewrite employs RL to train a prompt rewriter for generating new prompts instead of modification, which does not impose any restrictions in the prompt rewriting and offers improved flexibility in the action space. }} $\\bullet$ {Edit-based approaches. For the above methods, gradient-based and RL-based tuning can be extremely computationally demanding for ever larger models, and may not be feasible for API-based model calls (\\eg ChatGPT). Therefore, another line of work aims to directly edit existing prompts based on the task performance. Specifically, GPS~Xu-EMNLP-2022-GPS borrows an idea from the genetic algorithm and proposes a genetic prompt search method that utilizes a language model (\\ie T5) to edit prompts by taking the cloze task form. In addition to model based edit methods, human-defined operations can be also employed for prompt editing~Prasad-EACL-2023-GrIPS, including delete, swap, paraphrase, and addition. Based on these operations, they iteratively edit the prompts and greedily search for the best prompt guided by the model performance on a small pool of examples. $\\bullet$ {LLM-based approaches. Due to the exceptional capacities of LLMs, an increasing number of studies directly leverage LLMs as prompt generator~Zhou-ICLR-2023-Large,Pryzant-CoRR-2023-Automatic,Yang-CoRR-2023-Large,Ye-arxiv-2023-prompt,Tang-arxiv-2024-unleashing,Yang-EMNLP-2023-InstOptima,Guo-arxiv-2023-connecting,Do-arxiv-2023-prompt. Specifically, APE~Zhou-ICLR-2023-Large utilizes an LLM to generate initial prompts, then selects the best prompt with the highest accuracy, and finally improves the best candidate through an iterative Monte Carlo search method. { èŽ«åå…¶å¦™ï¼Œæ²¡æœ‰ä»»ä½•è½¬æŠ˜å’Œè¿‡æ¸¡ However, this method does not effectively constrain the prompt search space, which might likely lead to unstable results. To achieve good performance and fast convergence, one line of work utilizes heuristic methods (\\eg evolutionary algorithms~Yang-EMNLP-2023-InstOptima,Guo-arxiv-2023-connecting and adversarial learning~Do-arxiv-2023-prompt) for prompt optimization. Another line of work draws an analogy to gradient-based model optimizers for LLM-based prompt optimization. } { For example, APO~Pryzant-CoRR-2023-Automatic instructs the LLM to generate text feedback on how to refine an old prompt into new improved prompts and then execute textual gradient descent. } However, their search in the prompt space might be inefficient {without fully considering the whole refinement trace of previous prompts}, thus potentially leading to sub-optimal results. { Therefore, some recent studies~Yang-CoRR-2023-Large,Ye-arxiv-2023-prompt incorporate the previous prompts with their scores to instruct LLMs for progressively generating better new prompts. } To further design formalized guidelines about the design of prompt optimizers, GPO~Tang-arxiv-2024-unleashing conducts a systematic analogy for LLM-based prompt optimizers with gradient-based model optimizers. It further develops a {more formal} LLM-based prompt optimization framework, which extensively borrows the idea of machine learning optimization. Specifally, it retrieves relevant prompts from the previous prompts and utilizes the generation-based refinement strategy to perform the update. In order to avoid large variation at each iteration, GPO further adopts a cosine-based decay strategy to control the edit distance. However, these approaches still struggle in exploring the vast space of effective prompts. Inspired by human-like trial-and-error, prompt optimization is further formulated as a strategic planning problem~Wang-CoRR-2023-PromptAgent and uses Monte Carlo tree search to navigate the vast prompt space.} Continuous Prompt Optimization. % {Different from discrete prompts, continuous prompts consist of a set of continuous embeddings, which can be directly optimized through the gradient update based on the loss of downstream tasks. Note that continuous prompt optimization has been mainly studied in PLMs, but draws limited attention in era of LLMs due to their massive magnitudes of parameters. We include the discussion of this part for content completeness. In prior work, most studies typically rely on supervised learning to train continuous prompts based on task data. Furthermore, in data-scarce scenarios, transfer learning methods can be employed to alleviate the lack of labeled data on target tasks. These two approaches are detailed below. } $\\bullet$ {Prompt learning with sufficient data. In this approach, most existing methods regard continuous prompts as trainable model parameters and then leverage supervised learning to optimize the continuous prompts by minimizing the cross-entropy loss based on sufficient downstream task data~Li-ACL-2021-prefix,Lester-ACL-2021-The,Tang-COLING-2022-Context,Liu-arXiv-2021-P-tuning. As discussed in Section~sec-PEFT-methods, prefix tuning~Li-ACL-2021-prefix prepends a sequence of prefixes (\\ie a set of trainable continuous vectors) to each Transformer layer in language models, while prompt tuning~Lester-ACL-2021-The only incorporates trainable prompt vectors at the input layer. By fixing the large-scale parameters of LLMs and only tuning continuous prompt vector, this kind of approaches can be extremely parameter-efficient (Section~sec-PEFT). However, these approaches are typically independent of the inputs, lacking sufficient consideration of input semantics. Therefore, the authors in Tang-COLING-2022-Context propose context tuning, where the continuous prompts are derived based on the input text and learned through the downstream task losses.} $\\bullet$ {Prompt transferring with scarce data. Supervised learning approaches demand in sufficient training data to learn optimal continuous prompts, which may not work well in data-scarce domains and tasks. To address this problem, SPoT~Vu-ACL-2022-SPoT proposes a prompt-based transfer learning approach, which first learns % {a single continuous prompt} for several representative source tasks and then uses this prompt to initialize the prompt for a target task. {However, this approach leverages the same prompt for solving all instances of the target task. For a single task, even a well-learned prompt may not be suitable for all the data instances from a large population.} To address this issue, an improved method~Li-NAACL-2022-Learning designs an adaptive attention mechanism during the prompt transfer process to derive the target prompts, considering both task- and instance-level information. % {The prompt transfer paradigm can leverage the knowledge of data-sufficient source tasks encoded in source prompts for solving data-scarce target tasks.} }",
      "origin_cites_number": 24
    },
    {
      "section_title": "In-Context Learning",
      "level": "2",
      "content": "As a special prompting form, in-context learning~(ICL) is first proposed along with GPT-3~Brown-NeurIPS-2020-Language, which has become a typical approach to utilizing LLMs.",
      "origin_cites_number": 1
    },
    {
      "section_title": "ICL Formulation",
      "level": "3",
      "content": "As stated in~Brown-NeurIPS-2020-Language, ICL uses a formatted natural language prompt, consisting of the task description and/or a few task examples as demonstrations. Figure~fig:utilization presents an illustration of ICL. First, starting with a task description, a few examples are selected from the task dataset as demonstrations. Then, they are combined in a specific order to form natural language prompts with specially designed templates. Finally, the test instance is appended to the demonstration as the input for LLMs to generate the output. Based on task demonstrations, LLMs can recognize and perform a new task without explicit gradient update. Formally, let $D_k = \\{ f(x_1, y_1), \\dots, f(x_k, y_k) \\}$ represent a set of demonstrations with $k$ examples, where $f(x_k, y_k)$ is the prompt function that transforms the $k$-th task example into natural language prompts. Given the task description $I$, demonstration $D_k$, and a new input query $x_{k+1}$, the prediction of the output $y_{k+1}$ generated from LLMs can be formulated as follows When ICL was introduced in the GPT-3's paper~\\cite{Brown-NeurIPS-2020-Language, it was originally defined to be a combination of the task description and demonstration examples, wherein either component is dispensable. Following this definition, when a LLM is required to solve an unseen task by using only task descriptions, it can be also considered to perform ICL for task solving, whereas the ICL ability can be enhanced by instruction tuning. }: equation LLM \\big(I, f(x_1, y_1), \\dots, f(x_k, y_k)_{demonstrations}, f(x_{k+1}_{input}, \\vphantom{\\hat{y_{k+1}} \\_\\_\\_}_{answer}) \\big) \\rightarrow y_{k+1}. equation where the actual answer $y_{k+1}$ is left as a blank to be predicted by the LLM. % Since the performance of ICL heavily relies on demonstrations, it is important to properly design them in the prompts. According to the construction process in Equation~eq-ICL-prompting, we focus on three major aspects of formatting demonstrations in the prompts, including how to select examples that make up demonstrations, format each example into the prompt with the function $f(\\cdot)$, and arrange demonstrations in a reasonable order. { A comprehensive review of ICL has been presented in the survey paper~Dong-arxiv-2023-A, and we suggest the readers referring to it for a more general, detailed discussion on this topic. Compared with this survey, we specially focus on the discussion of applying ICL to LLMs in two major aspects, \\ie demonstration design and the underlying mechanism of ICL. } Also, ICL has a close connection with instruction tuning (discussed in Section~sec-instruction) in that {both utilize natural language to format the task or instances}. However, instruction tuning needs to fine-tune LLMs for adaptation, while ICL only prompts LLMs for utilization. {Furthermore, instruction tuning can enhance the ICL ability of LLMs to perform target tasks, especially in the zero-shot setting (only using task descriptions)~Chung-arxiv-2022-Scaling. } figure*[t] \\centering \\includegraphics[width=\\textwidth]{images/utilization.pdf} A comparative illustration of in-context learning~(ICL) and chain-of-thought~(CoT) prompting. ICL prompts LLMs with a natural language description, several demonstrations, and a test query, while CoT prompting involves a series of intermediate reasoning steps in prompts. figure*",
      "origin_cites_number": 4
    },
    {
      "section_title": "Demonstration Design",
      "level": "3",
      "content": "{Several studies have shown that the effectiveness of ICL is highly affected by the design of demonstrations~Min-EMNLP-2022-Rethinking, Lu-ACL-2022-Fantasically,Zhao-ICML-2021-Calibrate} Following the discussion in {Section~subsubsec-icl-formulation}, we will introduce the demonstration design of ICL from three major aspects, \\ie demonstration selection, format, and order. Demonstration Selection. { The performance of ICL tends to have a large variance with different demonstration examples~Liu-ACL-2022-What, so it is important to select a subset of examples that can effectively leverage the ICL capability of LLMs.} There are two main demonstration selection approaches, namely heuristic and LLM-based approaches: $\\bullet$~Heuristic approaches. {Due to their simplicity and low costs,} existing work widely adopts heuristic methods to select demonstrations. Several studies employ a $k$-NN based retriever to select examples that are semantically relevant to the query~Liu-ACL-2022-What, Lee-COLING-2022-Does. {However, they perform the selection individually for each example, rather than evaluating the example set as a whole.} To resolve this issue, diversity-based selection strategies are proposed to choose the most representative set of examples for specific tasks~Levy-arxiv-2022-Diverse, Su-arxiv-2022-selective. Furthermore, in~Ye-arxiv-2022-Complementary, both relevance and diversity are taken into consideration when selecting demonstrations. $\\bullet$~LLM-based approaches. Another line of work selects demonstrations by making use of LLMs. For example, LLMs can be utilized to directly measure the informativeness of each example according to the performance gain after adding the example~Li-arxiv-2023-Finding. In addition, EPR~Rubin-NAACL-2022-Learning proposes a two-stage retrieval approach that first recalls similar examples with an unsupervised method (\\eg BM25) and then ranks them using a dense retriever (trained with positive and negative examples labeled by LLMs). As an alternative approach, the task of demonstration selection can be formulated into a RL problem, where LLMs serve as the reward function to provide feedback for training the policy model~Zhang-EMNLP-2022-Active. Since LLMs perform well for text annotation~Gilardi-arXiv-2023-Crowd, some recent studies employ LLM itself as the demonstration generator without human intervention~Kim-arxiv-2022-Self-Generated. {To summarize, as discussed in~Michael-ICLR-2022-An, the selected demonstration examples in ICL should contain sufficient information about the task to solve as well as be relevant to the test query, for the above two selection approaches.} Demonstration Format. After selecting task examples, the next step is to integrate and format them into a natural language prompt for LLMs. A straightforward method is to instantiate a pre-defined template with the corresponding input-output pairs~Liu-survey-2023-Pre-train. To construct more informative templates, recent studies consider adding task descriptions~Chung-arxiv-2022-Scaling or enhancing the reasoning capability of LLMs with chain-of-thought prompts~Wei-arxiv-2022-chain. For instance, in~Mishra-ACL-2022-Cross, the authors collect a large-scale dataset with task descriptions written by humans. After tuning with this dataset, the performance on seen tasks can be boosted, and LLMs can also generalize to unseen tasks to some extent. To reduce the annotation costs, a semi-automated approach has been proposed in~Wang-arXiv-2022-Self by employing a seed set consisting of human-written task descriptions to guide LLMs to generate task descriptions for new tasks. Since it is costly to manually annotate demonstration formats for different tasks, some work also studies how to automatically generate high-quality ones. As two representative methods, Auto-CoT~Zhang-arxiv-2022-Automatic leverages LLMs with the zero-shot prompt ``Letâ€™s think step by step'' for generating intermediate reasoning steps, while least-to-most prompting~Zhou-arxiv-2022-Least first queries LLMs to perform problem decomposition and then utilizes LLMs to sequentially solve sub-problems based on the intermediate answers to previously solved ones. Demonstration Order. LLMs are shown to sometimes suffer from the {recency} bias, \\ie they are prone to repeat answers that are near the end of demonstrations~Zhao-ICML-2021-Calibrate. Thus, it is important to arrange demonstrations (\\ie task examples) in a reasonable order. Early work proposes several heuristic methods to quickly find a good order. For example, demonstrations can be directly organized according to their similarity to the query in the embedding space~Liu-ACL-2022-What: the more similar, the closer to the end. In addition, global and local entropy metrics can be used to score different demonstration orders~Lu-ACL-2022-Fantasically. To integrate more task information, some recent studies propose to minimize the {code length} required to compress and transmit task labels, which is inspired by information theory~Wu-arxiv-2022-Self. However, these methods need additional labeled data as the {validation set to evaluate the performance of specific demonstration orders}. To eliminate this need, the authors in~Lu-ACL-2022-Fantasically propose to sample the validation data from the LLM itself.",
      "origin_cites_number": 23
    },
    {
      "section_title": "Underlying Mechanism",
      "level": "3",
      "content": "After pre-training, LLMs can exhibit intriguing ICL capability without being updated. In what follows, we discuss two key questions about the ICL ability of LLMs, \\ie ``how does pre-training affect the ICL ability'' and ``how do LLMs perform ICL during inference''. How Pre-Training Affects ICL? ICL is first proposed in GPT-3~Brown-NeurIPS-2020-Language, and it has been shown that the ICL ability becomes more significant with a larger model size. Further, some studies reveal that small-scale PLMs can also demonstrate a strong ICL ability by continual pre-training~Gu-arXiv-2023-Pre or fine-tuning~Min-NAACL-2022-MetaICL on specially designed training tasks, which typically involve additional task examples in the input during the training process. It suggests that the design of training tasks is an important influence factor on the ICL capability of LLMs. Besides training tasks, recent studies have also investigated the relationship between ICL and pre-training corpora~Michael-ICLR-2022-An, Hahn-2023-arXiv-a. For example, ICL can be theoretically explained as the product of pre-training on documents that exhibit long-range coherence~Michael-ICLR-2022-An. { Further, another study~Hahn-2023-arXiv-a theoretically analyzes that when scaling parameters and data, LLMs based on next-word prediction can emerge the ability of ICL by learning from the compositional structure (\\eg how words and phrases are combined to form larger linguistic units like sentences) present in language data. % } How LLMs Perform ICL? At the inference stage, researchers focus on analyzing how the ICL capability operates based on given demonstrations since no explicit learning or updating is involved. According to the discussion in~Pan-2023-arXiv-what, there are two main ways for LLMs to utilize demonstrations: task recognition and task learning. $\\bullet$~Task recognition. {In the first way, LLMs recognize the task from demonstrations and utilize the prior knowledge obtained from pre-training to solve new test tasks. A Probably Approximately Correct~(PAC) framework~Wies-2023-arXiv-the has been proposed to assess the learnability of ICL. It assumes that there exists a latent variable representing the task in the pre-training data, and LLMs have been shown to be capable of capturing this variable from demonstrations, enabling them to recognize the task in ICL. Also, the interpretation of ICL as task recognition is supported by several empirical studies~Min-EMNLP-2022-Rethinking, Webson-2022-NAACL-do. For example, it has been observed that replacing the inputs or labels of demonstrations with random ones sampled from the input or label space does not seriously hurt the performance of LLMs, indicating that LLMs mainly recognize the target task from demonstrations instead of learning from them~Min-EMNLP-2022-Rethinking, Pan-2023-arXiv-what. Similarly, LLMs can exhibit decent performance even if the prompt template is irrelevant or misleading~Webson-2022-NAACL-do.} $\\bullet$~Task learning. {In the second way, LLMs learn new tasks unseen in the pre-training stage only through demonstrations. Specially, task learning is analyzed mainly from the perspective of gradient descent and considered as implicit fine-tuning~Oswald-arxiv-2022-Transformers, Dai-arxiv-2022-Why.} Then, ICL can be explained as follows: by means of forward computation, LLMs generate meta-gradients with respect to demonstrations and implicitly perform gradient descent via the attention mechanism. Experiments also show that certain attention heads in LLMs are capable of performing task-agnostic atomic operations~(\\eg copying and prefix matching), which are closely related to the ICL ability~Olsson-arxiv-2022-In. Furthermore, some studies abstract ICL as an algorithm learning process~rek-arxiv-2022-what. For example, the authors in~rek-arxiv-2022-what find that LLMs essentially encode implicit models through their parameters during pre-training. With the examples provided in ICL, LLMs can implement learning algorithms such as gradient descent or directly compute the closed-form solution to update these models during forward computation. Under this explanation framework, it has been shown that LLMs can effectively learn simple linear functions and even some complex functions like decision trees with ICL~rek-arxiv-2022-what. As discussed in a recent study~Pan-2023-arXiv-what, LLMs exhibit the abilities of both task recognition and task learning in ICL, but the two abilities seem to be possessed with different model scales. As shown in the experiments~Pan-2023-arXiv-what, the ability of task recognition is easier to obtain, and even a small LM with only 350M parameters can exhibit this ability, while task learning can only emerge for LLMs with at least 66B parameters. Another study~Wei-arxiv-2023-Larger also supports this finding with specially designed experiments. They set up the tasks with flipped and semantically unrelated labels in the experiment, which require task learning when performing ICL. The results suggest that small LMs tend to disregard the labels and mainly depend on their prior knowledge to accomplish the task, while LLMs have the ability to surpass their prior knowledge and acquire new knowledge from demonstrations, resulting in better outcomes. Furthermore, to improve the task learning ability, Meta-In-Context Learning~Forno-2023-arXiv-meta proposes to include multiple related tasks instead of just a single one in the prompt. In addition, Symbol Tuning~Wei-2023-arXiv-symbol fine-tunes LLMs on demonstrations with semantically unrelated labels (\\eg foo/bar instead of positive/negative for sentiment analysis), forcing LLMs to learn the task from demonstrations instead of relying on prior knowledge. figure*[t] \\centering \\includegraphics[width=\\textwidth]{images/XoT.pdf} An illustration of the evolution of CoT prompting strategies. It begins with the basic CoT approach and progresses to enhanced CoT generation techniques, including sampling-based and verification-based methods. Finally, it extends to variations of the chain structure, such as trees and graphs. Here, ``thought'' refers to an intermediate reasoning step as stated in~\\cite{Wei-arxiv-2022-chain, Yao-arxiv-2023-Tree. } figure*",
      "origin_cites_number": 22
    },
    {
      "section_title": "Chain-of-Thought Prompting",
      "level": "2",
      "content": "Chain-of-Thought~(CoT) prompting~Wei-arxiv-2022-chain, Chu-arxiv-2023-A is an improved prompting strategy to boost the performance of LLMs on complex reasoning tasks, such as arithmetic reasoning~Miao-ACL-2020-A, commonsense reasoning~Talmor-naacl-2019-CommonsenseQA, and symbolic reasoning~Wei-arxiv-2022-chain. Instead of simply constructing the prompts with input-output pairs like ICL, CoT prompting further incorporates intermediate reasoning steps, which serve as the bridge between inputs and outputs. { Figure~fig:utilization presents an illustration of CoT. In the following part, we will first elaborate on the basic CoT prompting approach and its improved strategies, then discuss when and why CoT prompting works. }",
      "origin_cites_number": 4
    },
    {
      "section_title": "Basic CoT Prompting Approach",
      "level": "3",
      "content": "CoT prompting is first proposed as an extension of ICL~Wei-arxiv-2022-chain, which augments each demonstration $\\langle$input, output$\\rangle$ as $\\langle$input, CoT, output$\\rangle$. A CoT is a series of intermediate reasoning steps for connecting the input and output. With these augmented demonstrations, LLMs can follow them to % {generate CoTs and the answer for a new input.} However, unlike $\\langle$input, output$\\rangle$ pairs in ICL, CoTs are difficult to obtain and usually require human annotation. Fortunately, it has been found that LLMs can be triggered to generate CoTs through simple instructions like ``Let's think step by step.''~Kojima-arxiv-2022-Large, making CoT prompting easy to use. There are also alternative magic prompts that {can elicit the ability of CoT reasoning and further improve the performance of LLMs}, such as ``Take a deep breath and work on this problem step-by-step.''~Yang-CoRR-2023-Large. { As illustrated in Figure~fig:extension_of_CoT, the generation process of CoT follows a chain structure in the basic CoT prompting approach, where LLMs generate CoTs step by step. Typically, CoT takes the format of natural language text. However, textual CoTs may not work well on complex tasks that require rigorous logic for reasoning. Considering this, some work uses code~Chen-arxiv-2022-Program, Gao-ICML-2023-PAL due to its structured and precise nature. Furthermore, the authors in~Zhao-arxiv-2023-Automatic propose to dynamically select text or code as the format of CoTs to combine their advantages. }",
      "origin_cites_number": 5
    },
    {
      "section_title": "Improved CoT Prompting Strategies",
      "level": "3",
      "content": "{ Despite the performance improvement in complex reasoning tasks, CoT prompting still suffers from problems like incorrect reasoning and instability. In this part, we first introduce how to design better CoT prompts and enhanced CoT generation strategies, and then introduce the extension of the basic chain structure of CoT. Figure~fig:extension_of_CoT illustrates the evolution of representative CoT prompting strategies. } Better Prompt Design. Since CoT prompting relies on prompts to elicit the reasoning capabilities of LLMs, the design of prompts is critical to its performance. As a direct approach, it is shown that using diverse CoTs (\\ie multiple reasoning paths for each problem) can effectively enhance the performance~Li-arxiv-2022-On. Another intuitive idea is that prompts with more complex reasoning paths are more likely to elicit the reasoning ability of LLMs~Fu-arxiv-2022-Complexity, which can result in higher accuracy in generating correct answers. However, all these approaches rely on annotated CoT datasets, which limits their use in practice. To overcome this limitation, magic instructions such as ``Let's think step by step'' can be used to automatically construct CoTs by prompting LLMs~Zhang-arxiv-2022-Automatic. Enhanced CoT Generation. { Since LLMs are prone to producing incorrect reasoning steps and exhibiting instability in the generation process, there are a number of studies~Li-arxiv-2023-Making, Wang-arxiv-2022-Self-Consistency to improve the generation of CoT. In this part, we will introduce two typical approaches to enhancing the generation of CoT: sampling- and verification-based methods. } $\\bullet$ Sampling-based methods. { LLMs are known to suffer from instability during inference, which can lead to unfaithfulness in the generated reasoning steps. To address this issue, some work proposes to sample multiple reasoning paths instead of using greedy decoding. As a representative solution, self-consistency~Wang-arxiv-2022-Self-Consistency first generates several reasoning paths and then takes an ensemble over the corresponding answers, selecting the most consistent one through majority voting. However, such a method can still lead to wrong answers when most of the reasoning paths are misled. Considering this, the authors in~Fu-arxiv-2022-Complexity only vote on the $k$ most complex reasoning paths based on their observation that reasoning paths with higher complexity (\\eg more reasoning steps) usually have better performance. {Furthermore, MCR~Yoran-arxiv-2023-Answering proposes referring to the steps from other reasoning paths when generating the next step, and performs reasoning across multiple reasoning paths to generate the final answer.} } $\\bullet$ Verification-based methods. { The sequential nature of reasoning steps in CoTs can lead to the accumulation of errors in the generated CoTs when certain steps are incorrect. To mitigate this problem, recent studies propose to verify the correctness of generated reasoning steps with either trained verifiers or LLMs themselves. For example, DIVERSE~Li-arxiv-2023-Making trains solution-level and step-level verifiers respectively to examine the reasoning steps at different granularities. Another approach~Ling-arxiv-2023-Deductive utilizes LLMs to verify the correctness of reasoning steps through step-by-step self-verification with a specially designed reasoning format. In addition, several studies propose backward reasoning for verification: it first deduces the necessary question conditions~Xue-arxiv-2023-RCOT, Weng-arxiv-2023-Large or variables~Jiang-arxiv-2023-Forward {from the model's predictions}, and then compares them with the original ones. } Reasoning Structure Extension. { Despite the generality, the chain reasoning structure of basic CoT prompting limits its effectiveness in solving complex tasks, which require exploration like foresight and backtracking during inference. Therefore, many studies have been devoted to extending the reasoning structure by designing more intricate thought processes, \\eg tree- and graph-structured reasoning. % } $\\bullet$ Tree-structured reasoning. This approach (exemplified by Tree of Thoughts~(ToT)~Yao-arxiv-2023-Tree, Long-arxiv-2023-Large) formulates the reasoning process in a hierarchical tree structure, where intermediate thoughts are nodes. {In this way, it enables LLMs to explore multiple reasoning paths in parallel and further supports the operation of lookahead and backtracking to facilitate more comprehensive decisions.} In addition, TouT~Mo-arxiv-2023-Tree takes the uncertainty of intermediate thoughts into account for thought evaluation based on Monte Carlo Dropout. $\\bullet$ Graph-structured reasoning. { Although the tree structure facilitates parallel reasoning, it also imposes restrictions on the reasoning process. With more complex topological structures, graphs offer greater flexibility in reasoning, enabling the characterization of more intricate relationships and interactions. For instance, Graph of Thoughts~(GoT)~Besta-arxiv-2023-Graph, Lei-arxiv-2023-Boosting conceptualizes the reasoning process as an arbitrary graph, where vertices denote intermediate thoughts and edges denote the interdependence between these thoughts. {Compared with ToT, it can further utilize thoughts from other reasoning paths when generating new thoughts.} However, such an approach requires a large number of interactions with LLMs, making the thought exploration process highly inefficient. } { To reduce potentially meaningless thought exploration, XoT~ding-arxiv-2023-everything further proposes to guide the search of thoughts with pre-trained policy and value networks. }",
      "origin_cites_number": 15
    },
    {
      "section_title": "Further Discussion on CoT Prompting",
      "level": "3",
      "content": "In this part, we present discussions regarding two fundamental questions related to CoT prompting, \\ie ``when does CoT prompting work for LLMs'' and ``why can LLMs perform CoT reasoning''. {When CoT Prompting Works For LLMs?} Since CoT reasoning is an emergent ability~Wei-arxiv-2022-Emergent, it only has a positive effect on sufficiently large models (typically containing 10B or more parameters~Wei-arxiv-2022-chain) but not on small models. Moreover, since CoT prompting augments the standard prompting with intermediate reasoning steps, it is mainly effective for the tasks that require step-by-step reasoning~Wei-arxiv-2022-chain, \\eg arithmetic reasoning, commonsense reasoning, and symbolic reasoning. Whereas, for other tasks that do not rely on complex reasoning, CoT prompting might lead to worse performance than standard prompting~Wang-arxiv-2022-Rationale, \\eg MNLI-m/mm, SST-2, and QQP from GLUE~Wang-EMNLP-2018-GLUE. Interestingly, it seems that the performance gain brought by CoT prompting could be significant only when standard prompting yields poor results~Wei-arxiv-2022-chain. {Why LLMs Can Perform CoT Reasoning?} As the second question, we discuss the underlying mechanism of CoT prompting in the following two aspects. $\\bullet$ The source of CoT reasoning ability. Regarding the source of CoT reasoning capability, it is widely hypothesized that it can be attributed to training on code since models trained on it show a strong reasoning ability~Liang-arxiv-2022-Holistic, FU-blog-2022-how, Bi-arxiv-2023-When. Intuitively, code data is well organized with algorithmic logic and programming flow, which may be useful to improve the reasoning performance of LLMs. However, this hypothesis still lacks publicly reported evidence of ablation experiments (with and without training on code). In addition, instruction tuning seems not to be the key reason for obtaining the CoT reasoning ability, since it has been empirically shown that instruction tuning on non-CoT data does not improve the performance on held-out CoT reasoning benchmarks~Chung-arxiv-2022-Scaling. $\\bullet$ The effect of CoT prompting components. The major distinction between CoT prompting and standard prompting is the incorporation of reasoning paths prior to the final answer. Thus, some researchers investigate the effects of different components in the reasoning paths. Specifically, a recent study identifies three key components in CoT prompting, namely symbols~(\\eg numerical quantities in arithmetic reasoning), patterns~(\\eg equations in arithmetic reasoning), and text~(\\ie the rest of tokens that are not symbols or patterns)~Madaan-arxiv-2022-Text. It is shown that the latter two parts (\\ie patterns and text) are essential to the model performance, and removing either one would lead to a significant performance drop. However, the correctness of symbols and patterns does not seem critical. Further, there exists a symbiotic relationship between text and patterns: the text helps LLMs to generate useful patterns, and patterns aid LLMs to understand tasks and generate texts that help solve them~Madaan-arxiv-2022-Text. In summary, CoT prompting provides a general and flexible approach to eliciting the reasoning ability of LLMs. There are also some preliminary attempts to extend this technique to solve multimodal~Zhang-arxiv-2022-Multimodal and multilingual tasks~Shi-arxiv-2022-Language.",
      "origin_cites_number": 12
    },
    {
      "section_title": "Planning",
      "level": "2",
      "content": "Prompting with ICL and CoT is a conceptually simple yet general approach to solving various tasks. However, this approach struggles with complex tasks like mathematical reasoning~Qian-2022-arXiv-limitations and multi-hop question answering~Ning-arxiv-2023-ChatGPT. As an enhanced approach, prompt-based planning has been proposed to break down complex tasks into smaller sub-tasks and generate a plan of actions to accomplish the task.",
      "origin_cites_number": 2
    },
    {
      "section_title": "The Overall Framework",
      "level": "3",
      "content": "In this part, we first formulate the general planning paradigm of LLMs for solving complex tasks, which is illustrated in Figure~fig:planning. In this paradigm, there are typically three components: task planner, plan executor, and environment Despite the similarity with RL, our formulation decouples the planning and execution phases, whereas in RL, they are typically interleaved in the agent. This paradigm is defined in a general yet slightly loose way, and it mainly aims to help readers understand the key idea underlying the planning approaches of LLMs. . Specifically, task planner, which is played by LLMs, aims to generate the whole plan to solve a target task. The plan can be presented in various forms, \\eg an action sequence in the form of natural language~Zhou-arxiv-2022-Least or an executable program written in programming language~Gao-arxiv-2022-PAL. The LLM-based task planner can be enhanced with the memory mechanism for plan storage and retrieval, which is helpful for long-horizon tasks. Then, plan executor is responsible for executing the actions in the plan. It can be implemented by models like LLMs for textual tasks~Wang-arXiv-2023-Plan or by tools like code interpreters for coding tasks~Shinn-2023-arXiv-Reflexion. Furthermore, environment refers to where the plan executor carries out the actions, which can be set differently according to specific tasks, \\eg the LLM itself~Yao-2023-arXiv-tree or an external virtual world like Minecraft~Wang-2023-arXiv-voyager. It provides feedback about the execution result of the action to the task planner, either in the form of natural language~Shinn-2023-arXiv-Reflexion or from other multimodal signals~Lu-2023-arXiv-multimodal. For solving a complex task, the task planner first needs to clearly understand the task goal and generate a reasonable plan based on the reasoning of LLMs (See Section~sec:plan-gen). Then, the plan executor acts according to the plan in the environment, and the environment will produce feedback for the task planner (See Section~sec:feedback). The task planner can further incorporate the feedback obtained from the environment to refine its initial plan and iteratively perform the above process to get better results as the task solution (See Section~sec:plan-refine).",
      "origin_cites_number": 8
    },
    {
      "section_title": "Plan Generation",
      "level": "3",
      "content": "Plan generation focuses on directly generating action sequences by prompting LLMs. Based on the format of the generated plans, existing work can be divided into two groups: text-based and code-based approaches. Text-based Approaches. It is straightforward for LLMs to generate plans in the form of natural language. In this approach, LLMs are prompted to generate a sequence of actions for the plan executor to perform and solve the complex task. For example, Plan-and-Solve~Wang-arXiv-2023-Plan adds explicit instructions like ``devise a plan'' to directly prompt the LLM for planning in a zero-shot manner, while Self-planning~Jiang-arXiv-2023-Self and DECOMP~Khot-2022-arXiv-Decomposed add demonstrations in the prompt to guide the LLM to devise a plan through ICL. Following this way, some work further considers incorporating extra tools or models when planning. For example, ToolFormer~Schick-arxiv-2023-Toolformer first annotates a pre-training corpus with potential API calls using LLMs, and then fine-tunes LLMs on it, so that LLMs can learn when and how to call APIs and incorporate the results returned by APIs during generation. HuggingGPT~Shen-2023-arXiv-Hugginggpt introduces the models available in HuggingFace and regards LLMs as the controller to select suitable models based on their descriptions and aggregate their results as the final solution. Code-based Approaches. Although text-based approaches sound intuitive, they cannot guarantee faithful execution of the plan, which may lead to failure even when the plan is sound. To address this issue, code-based approaches have been proposed to generate more verifiable plans in the form of executable code in programming languages, \\eg Python or PDDL. In this way, LLMs are first prompted to generate the program and then utilize a deterministic solver to execute it. For example, Faithful CoT~Lyu-arxiv-2023-Faithful and PAL~Gao-arxiv-2022-PAL decompose a reasoning task into two stages: at the first stage, the LLM generates a plan conditioned on the query; at the second stage, a deterministic solver executes the plan to derive the final answer. Furthermore, code-based approaches can be applied to embodied agents in a similar way. For example, PROGPROMPT~Singh-arxiv-2022-ProgPrompt and LLM+P~Liu-2023-arXiv-LLM+P first utilize LLMs to generate plans in the form of python functions or PDDL files, and then leverage a virtual agent or classical planner to solve the problem according to the code-based plans.",
      "origin_cites_number": 9
    },
    {
      "section_title": "Feedback Acquisition",
      "level": "3",
      "content": "After executing the generated plan, the environment would produce the feedback signal to the LLM-based task planner, which can be used to refine its initial plan for better results. In existing work, there are typically two sources of feedback from the environment, depending on their relationship with the LLM-based task planner: internal (\\ie the LLM itself) and external (\\eg tools or virtual worlds) feedback. Internal Feedback. The LLM itself can be utilized as a feedback provider. One straightforward way is to directly evaluate the quality of the generated plans through prompting. For example, RAP~Hao-2023-arXiv-reasoning evaluate the likelihood that each candidate plan can lead to task success, while Tree of Thoughts~Yao-2023-arXiv-tree proposes to vote across plans by making comparisons between them. Further, LLMs can provide feedback based on the intermediate results from the plan executor. For example, Reflexion~Shinn-2023-arXiv-Reflexion utilizes LLMs to transform sparse result signals (\\eg success or failure) into concrete {text-based feedback (\\eg ``You should recommend comedies that the user mentions in the query instead of horror movies'') and stores this feedback in long-term memory for future planning.} External Feedback. In addition to LLMs, external objects can also provide feedback signals. For example, tools like code interpreters are widely used in programming tasks to provide real-time error messages~Shinn-2023-arXiv-Reflexion, models like stable diffusion~Rombach-2022-CVPR-high can be used in multimodal tasks to provide visual perception~Lu-2023-arXiv-multimodal, and {virtual worlds} like Minecraft can provide immersive experiences~Wang-2023-arXiv-voyager. Besides, some work (\\eg Generative Agents~Park-arxiv-2023-Generative) explores multi-agent collaboration in simulated environments, where each agent receives feedback not only from interaction with the environment but also from communication with other agents.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Plan Refinement",
      "level": "3",
      "content": "With access to feedback from the environment, the task planner can accordingly refine its current plan and iteratively go through the ``planning -- execution -- refinement'' loop for better results. In this part, we summarizes three major refinement approaches in existing work. Reasoning. The feedback data from the environment may not be directly suitable to be utilized by LLMs for plan refinement, \\eg containing irrelevant information or taking a non-language form. To solve this, some work adds the explicit reasoning process to extract critical information from feedback~Yao-2022-arXiv-react, Chen-2023-arXiv-chatcot. For example, React~Yao-2022-arXiv-react prompts LLMs with demonstrations to generate reasoning traces over feedback. It has been widely used in autonomous agent projects, such as AutoGPT~AutoGPT, which can automatically reason over the observed feedback to revise the initial plan for solving various user requests. However, these approaches typically fix the order of reasoning and planning. To support flexible switching between the two processes for better performance, ChatCoT~Chen-2023-arXiv-chatcot further unifies the tool-augmented reasoning process into a multi-turn conversation between the LLM-based task planner and the tool-based environment. Backtracking. Early methods mainly consider planning forward actions while maintaining the existing plan, thus likely leading to local optimal plans based on a short-term evaluation. To solve this, Tree of Thoughts~Yao-2023-arXiv-tree allows backtracking with search algorithms like breadth-first and depth-first search to make global planning. It refines the plan step by step by backtracking to the last state in the initial plan and choosing the next unexplored action. Furthermore, some studies~Wang-2023-arXiv-describe, Lu-2023-arXiv-multimodal utilize feedback signals to revise the entire plan. For example, DEPS~Wang-2023-arXiv-describe selects a better plan according to feedback signals, while TIP~Lu-2023-arXiv-multimodal adds feedback signals to prompts for the LLM-based planner to revise each step in the initial plan. Memorization. In order to handle long-horizon tasks, it has become a key approach to aid plan refinement with long-term memory in addition to utilizing the short-term memory of LLMs through ICL. For example, Reflexion~Shinn-2023-arXiv-Reflexion stores the feedback from self-reflection into the memory, so previous feedback can be retrieved for plan refinement. Generative Agents~Park-arxiv-2023-Generative designs the memory stream mechanism as the core component of agents for action planning and reflection. Further, the skill library mechanism~Wang-2023-arXiv-voyager, Sun-2023-arXiv-adaplanner is proposed to store successful plans in the library, which can be reused and synthesized as complex plans for novel tasks. To implement the long-term memory mechanism, tools like vector databases (\\eg milvus~Wang-2021-ICDM-Milvus) can be used to encode plans or feedbacks into high-dimensional vectors for efficient storage and retrieval at a large scale. MemoryBank~Zhong-2023-arxiv-MemoryBank further proposes the memory updating mechanism to allow memory forgetting and strengthening following the Ebbinghaus Forgetting Curve theory. In addition to utilizing LLMs with ICL, CoT, and task planning, some recent studies explore how to specialize the ability of LLMs towards specific tasks~\\cite{Shridhar-arxiv-2022-Distilling, Ho-2022-arxiv-Large, Magister-arxiv-2022-Teaching, which is called model specialization~Fu-arxiv-2023-Specializing. For example, the researchers in~Fu-arxiv-2023-Specializing specialize the ability of mathematical reasoning from LLMs through fine-tuning the small-scale Flan-T5~Chung-arxiv-2022-Scaling on CoT reasoning paths generated by LLMs. Model specialization can also be applied to solve a variety of tasks like question answering~chan-ICLR-2023-knife, code synthesis~Li-arxiv-2023-on, and information retrieval~Dai-ARXIV-2022-Promptagator.}",
      "origin_cites_number": 20
    },
    {
      "section_title": "Capacity and Evaluation",
      "level": "1",
      "content": "To examine the effectiveness and superiority of LLMs, a surge of tasks and benchmarks have been proposed for conducting empirical ability evaluation and analysis. In this section, we first introduce three types of basic ability evaluation of LLMs for language generation and understanding, then present several advanced ability evaluations with more complicated settings or goals, and finally discuss existing benchmarks, evaluation approaches, and empirical analysis. table*[htbp] \\centering Representative basic and advanced abilities and corresponding representative datasets for evaluating. \\footnotesize \\renewcommand2.5pt tabular{cccc} \\toprule Level & Ability & Task & Dataset \\\\ \\midrule 27{*}{Basic} & 6{*}{Language Generation} &Language Modeling &Penn Treebank~Marcus-CL-1993-Building, WikiText-103~Merity-ICLR-2017-Pointer, the Pile~Gao-arxiv-2021-Pile, LAMBADA~Paperno-ACL-2016-LAMBADA \\\\ \\addlinespace & &3{*}{Conditional Text Generation} & WMT'14,16,19,20,21,22~Bojar-WMT-2014-Findings,Bojar-WMT-2016-Findings,Barrault-WMT-2019-Findings,Barrault-WMT-2020-Findings,Akhbardeh-WMT-2021-Findings,Kocmi-WMT-2022-Findings, Flores-101~Goyal-TACL-2022-The, DiaBLa~Bawden-journal-2021-DiaBLa, \\\\ & & & CNN/DailyMail~Nallapati-acl-2016-Abstractive, XSum~Naryan-EMNLP-2018-XSUM, WikiLingua~Ladhak-EMNLP-2020-WikiLingua\\\\ & & & OpenDialKG~Moon-ACL-2019-OpenDialKG \\\\ \\addlinespace & &2{*}{Code Synthesis} & APPS~Hendrycks-nips-2021-Measuring, HumanEval~Chen-arxiv-2021-evaluating, MBPP~Austin-arxiv-2021-Program, CodeContest~Li-Science-2022-AlphaCode, MTPB~nijkamp-arxiv-2022-Codegen, \\\\ & & & DS-1000~Lai-arxiv-2022-DS, ODEX~Wang-arxiv-2022-Execution \\\\ \\cmidrule(r){2-4} &9{*}{Knowledge Utilization} &3{*}{Closed-Book QA} & Natural Questions~Kwiatkowski-ACL-2019-Natural, ARC~Clark-arxiv-2018-Think, TruthfulQA~Lin-ACL-2022-TruthfulQA, Web Questions~Berant-EMNLP-2013-Semantic,\\\\ & & & TriviaQA~Joshi-ACL-2017-TriviaQA, PIQA~Bisk-AAAI-2020-PIQA, LC-quad2.0~Dubey-ISWC-2019-LC, GrailQA~Gu-WWW-2021-Beyond, KQApro~Cao-ACL-2022-KQA,\\\\ & & & CWQ~Hu-COLING-2022-Logical, MKQA~Longpre-TACL-2021-MKQA, ScienceQA~Saikh-IJDL-2022-ScienceQA \\\\ \\addlinespace & &3{*}{Open-Book QA} & Natural Questions~Kwiatkowski-ACL-2019-Natural, OpenBookQA~Mihaylov-EMNLP-2018-Can, ARC~Clark-arxiv-2018-Think, TriviaQA~Joshi-ACL-2017-TriviaQA, \\\\ & & & Web Questions~Berant-EMNLP-2013-Semantic, MS MARCO~Nguyen-NIPS-2016-MS, QASC~Khot-AAAI-2020-QASC, SQuAD~Rajpurkar-EMNLP-2016-SQuAD, \\\\ & & & WikiMovies~Miller-EMNLP-2016-Key \\\\ \\addlinespace & &2{*}{Knowledge Completion} & WikiFact~Goodrich-KDD-2019-Assessing, FB15k-237~Toutanova-CVSC-2015-Observed, Freebase~Bollacker-SIGMOD-2008-Freebase, WN18RR~Dettmers-AAAI-2018-Convolutional, \\\\ & & & WordNet~Miller-Commun-1995-WordNet, LAMA~Petroni-EMNLP-2019-Language, YAGO3-10~Mahdisoltani-CIDR-2015-YAGO3, YAGO~Suchanek-WWW-2007-Yago\\\\ \\cmidrule(r){2-4} & 12{*}{Complex Reasoning} &4{*}{Knowledge Reasoning} & CSQA~Talmor-naacl-2019-CommonsenseQA, StrategyQA~Geva-tacl-2021-Did, HotpotQA~yang-2018-acl-HotpotQA, ARC~Clark-arxiv-2018-Think, BoolQ~Clark-naacl-2019-BoolQ, \\\\ & & & PIQA~Bisk-AAAI-2020-PIQA, SIQA~Sap-arxiv-2019-SocialIQA, HellaSwag~Zellers-acl-2019-HellaSwag, WinoGrande~Sakaguchi-aaai-2020-WinoGrande, COPA~Roemmele-aaai-2011-Choice, \\\\ & & & OpenBookQA~Mihaylov-EMNLP-2018-Can, ScienceQA~Saikh-IJDL-2022-ScienceQA, proScript~Sakaguchi-acl-2021-proScript, ProPara~Dalvi-acl-2018-Tracking, \\\\ & & & ExplaGraphs~Saha-acl-2021-ExplaGraphs, ProofWriter~Tafjord-acl-2021-ProofWriter, EntailmentBank~Dalvi-acl-2021-Explaining, \\\\ & & & ProOntoQA~Saparov-arxiv-2022-Language \\\\ \\addlinespace & &3{*}{Symbolic Reasoning} & CoinFlip~Wei-arxiv-2022-chain, ReverseList~Wei-arxiv-2022-chain, LastLetter~Wei-arxiv-2022-chain, Boolean Assignment~Anil-arxiv-2022-Exploring, \\\\ & & & Parity~Anil-arxiv-2022-Exploring, Colored Object~Srivastava-arxiv-2022-Beyond, Penguins in a Table~Srivastava-arxiv-2022-Beyond, \\\\ & & & Repeat Copy~Gao-arxiv-2022-PAL, Object Counting~Gao-arxiv-2022-PAL \\\\ \\addlinespace & &3{*}{Mathematical Reasoning} & MATH~Hendrycks-ICLR-2021-Measuring, GSM8k~Cobbe-arxiv-2021-Training, SVAMP~Patel-NAACL-2021-Are, MultiArith~Roy-acl-2015-Solving, ASDiv~Miao-ACL-2020-A, \\\\ & & & MathQA~Amini-acl-2019-MathQA, AQUA-RAT~Ling-acl-2017-Program, MAWPS~Koncel-NAACL-2016-MAWPS, DROP~Dua-NAACL-2019-DROP, \\\\ & & & NaturalProofs~Welleck-NIPS-2021-NaturalProofs, PISA~Jiang-AITP-2021-LISA, miniF2F~Zheng-ICLR-2022-miniF2F, ProofNet~Azerbayev-arxiv-2023-ProofNet \\\\ \\midrule 18{*}{Advanced} & 5{*}{Human Alignment} & Honestness &TruthfulQA~Lin-ACL-2022-TruthfulQA, HaluEval~Li-arxiv-2023-HaluEval \\\\ \\addlinespace & & Helpfulness & HH-RLHF~Bai-arxiv-2022-Training \\\\ \\addlinespace & & 2{*}{Harmlessness} & HH-RLHF~Bai-arxiv-2022-Training, Crows-Pairs~Nangia-EMNLP-2020-CrowS \\\\ & & & WinoGender~Rudinger-NAACL-2018-Gender, RealToxicityPrompts~Gehman-2023-arxiv-RealToxicityPrompts \\\\ \\cmidrule(r){2-4} & 4{*}{\\makecell[c]{Interaction with \\\\ External Environment}} & 1{*} Household & VirtualHome~Puig-CVPR-2018-VirtualHome, BEHAVIOR~Srivastava-CoRL-2021-BEHAVIOR, ALFRED~Shridhar-CVPR-2020-ALFRED,ALFWorld~Shridhar-2021-iclr-ALFWorld \\\\ \\addlinespace & & 1{*} Website Environment & WebShop~Yao-2022-nips-WebShop, Mind2Web~Deng-2023-arxiv-Mind2Web \\\\ \\addlinespace & & 1{*} Open World &MineRL~Guss-2019-ijcai-MineRL, MineDojo~Fan-2022-nips-minedojo \\\\ \\cmidrule(r){2-4} & 8{*}{Tool Manipulation} & 1{*} Search Engine & HotpotQA~yang-2018-acl-HotpotQA, TriviaQA~Joshi-ACL-2017-TriviaQA, Natural Questions~Kwiatkowski-ACL-2019-Natural \\\\ \\addlinespace & & 1{*} Code Executor & GSM8k~Cobbe-arxiv-2021-Training, TabMWP~Lu-2022-arxiv-Dynamic, Date Understanding~Srivastava-arxiv-2022-Beyond \\\\ \\addlinespace & & 1{*} Calculator & GSM8k~Cobbe-arxiv-2021-Training, MATH~Hendrycks-ICLR-2021-Measuring, CARP~Zhang-2023-arxiv-Evaluating \\\\ \\addlinespace & & 1{*} Model Interface & GPT4Tools~yang-2023-arxiv-GPT4Tools, Gorilla~Patil-2023-arxiv-Gorilla \\\\ \\addlinespace & & 2{*}{Data Interface} & WebQSP~Yih-2016-acl-The, MetaQA~Puerto-2023-eacl-MetaQA, WTQ~Pasupat-2015-acl-Compositional \\\\ & & & WikiSQL~Zhang-2017-arxiv-Seq2SQL, TabFact~Chen-2020-iclr-TabFact, Spider~Yu-2018-emnlp-Spider \\\\ \\addlinespace \\bottomrule tabular table*",
      "origin_cites_number": 119
    },
    {
      "section_title": "Basic Ability",
      "level": "2",
      "content": "In this part, we mainly focus on three basic types of ability evaluation for LLMs, \\ie language generation, knowledge utilization, and complex reasoning. It is noted that we do not intend to have complete coverage of all the related tasks, but instead only focus on the most widely discussed or studied tasks for LLMs. Next, we introduce these tasks in detail.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Language Generation",
      "level": "3",
      "content": "According to the task definition, existing tasks about language generation can be roughly categorized into language modeling, conditional text generation, and code synthesis tasks. Note that code synthesis is not a typical NLP task, we include it for discussion because it can be directly solved by a number of LLMs (trained on code data) in a similar generation approach as natural language text. Language Modeling. As the most fundamental ability of LLMs, language modeling aims to predict the next token based on the previous tokens~Bengio-JMLR-2003-A, which mainly focuses on the capacity of basic language understanding and generation. For evaluating such an ability, typical language modeling datasets that existing work uses include Penn Treebank~Marcus-CL-1993-Building, WikiText-103~Merity-ICLR-2017-Pointer, and the Pile~Gao-arxiv-2021-Pile, where the metric of perplexity is commonly used for evaluating the model performance under the zero-shot setting. Empirical studies~Brown-NeurIPS-2020-Language,Zeng-arxiv-2022-GLM show that LLMs bring substantial performance gains over the previous state-of-the-art methods on these evaluation datasets. To better test the modeling capacity of long-range dependencies in text, the LAMBADA dataset~Paperno-ACL-2016-LAMBADA has been introduced, where LLMs are required to predict the last word of sentences based on a paragraph of context. Then, the accuracy and perplexity of the predicted last words are employed to evaluate LLMs. As shown in existing work, the performance on the language modeling tasks typically follows the scaling law~Kaplan-arxiv-2020-Scaling, which means that scaling language models would improve the accuracy and reduce the perplexity. Conditional Text Generation. As an important topic in language generation, conditional text generation~Li-IJCAI-2021-Pretrained focuses on generating texts satisfying specific task demands based on the given conditions, typically including machine translation~Bahdanau-ICLR-2015-Neural, text summarization~Nallapati-acl-2016-Abstractive, and question answering~Berant-EMNLP-2013-Semantic. {To measure the quality of the generated text, automatic metrics (\\eg Accuracy, BLEU~Papineni-acl-2002-bleu and ROUGE~lin-acl-2004-rouge) and human ratings have been typically used for evaluating the performance. } Due to the powerful language generation capabilities, LLMs have achieved remarkable performance on existing datasets and benchmarks. {For instance, GPT-4 exhibits comparable performance as commercial translation products, {even for the translation task of languages that are with significant linguistic distance~Jiao-arxiv-2023-mt.} On news summarization tasks~(\\ie CNN/DM and XSUM), LLMs also demonstrate comparable performance with human freelance writers~Zhang-2023-arxiv-Benchmarking. Despite the rapid progress on model capacity, there are increasing concerns on the feasibility of existing automatic metrics to faithfully assess the performance of LLMs in conditional text generation tasks~Zhang-2023-arxiv-Benchmarking,Goyal-2023-arxiv-News,Gehrmann-2022-arxiv-Repairing. As the alternatives to automatic metrics, recent studies also propose to incorporate LLMs as generation evaluators to examine the quality of the generated content~Wang-2023-arxiv-Is,Liu-2023-arxiv-G-Eval,vicuna2023. Moreover, researchers also explore more challenging language generation tasks for LLMs, such as structured data generation~Jiang-2023-arxiv-StructGPT and long text generation~Yang-EMNLP-2022-Re3,OpenAI-OpenAI-2023-GPT-4,Zhou-2023-arxiv-RecurrentGPT.} Code Synthesis. In addition to generating high-quality natural language text, existing LLMs also show strong abilities to generate {formal language, especially computer programs (\\ie code)} that satisfy specific conditions, called code synthesis~Gulwani-Found-2017-Program. Unlike natural language generation, as the generated code can be directly checked by execution with corresponding compilers or interpreters, existing work mostly evaluates the quality of the generated code from LLMs by calculating the pass rate against the test cases, \\ie pass@$k$Given $k$ programs generated by the LLM, pass@$k$ is computed as 1 when at least one program passes all test cases, or else 0. % Recently, several code benchmarks focusing on functional correctness are proposed to assess the code synthesis abilities of LLMs, such as APPS~Hendrycks-nips-2021-Measuring, HumanEval~Chen-arxiv-2021-evaluating, and MBPP~Austin-arxiv-2021-Program. Typically, they consist of diverse programming problems, with text specification and test cases for correctness checking. To improve such an ability, it is key to fine-tuning (or pre-training) LLMs on code data, which can effectively adapt LLMs to code synthesis tasks~nijkamp-arxiv-2022-Codegen. {In addition, existing work has proposed new strategies to generate code, \\eg sampling multiple candidate solutions~Austin-arxiv-2021-Program and planning-guided decoding~Zhang-ICLR-2023-Planning, which can be considered as the imitation of bug-fixing and code-planning processes by programmers.} Impressively, LLMs have recently shown competitive performance with humans by achieving a ranking of the top 28\\% among users on the programming contest platform Codeforces~Li-Science-2022-AlphaCode. Further, GitHub Copilot has been released to assist programming in coding IDEs (\\eg Visual Studio and JetBrains IDEs), which can support a variety of languages including Python, JavaScript, and Java. A viewpoint article entitled ``The End of Programming''~Welsh-ACM-2023-The in Communications of the ACM has discussed the impact of AI programming in the field of computer science, emphasizing an important shift towards the highly adaptive LLM as a new atomic unit of computation. Major Issues. Although LLMs have achieved splendid performance in generating human-like text, they are susceptible to suffering from two major issues in language generation as discussed below. $\\bullet$ Unreliable generation evaluation. With the advancement of language generation ability of LLMs, existing studies find that the generated texts from LLMs have reached a comparable quality to the reference texts on a variety of text generation tasks. However, due to the intrinsic weakness of existing evaluation benchmarks, there exists pronounced inconsistency between human evaluation and automatic reference-based metrics~Zhang-2023-arxiv-Benchmarking,Goyal-2023-arxiv-News,Gehrmann-2022-arxiv-Repairing,Bang-arxiv-2023-A. For example, in OpenDialKG~Moon-ACL-2019-OpenDialKG, ChatGPT underperforms a fine-tuned GPT-2 on BLEU and ROUGE-L metrics, while earning more favor from human judgment~Bang-arxiv-2023-A. Furthermore, existing work argues that even human evaluation may not be robust enough~Goyal-2023-arxiv-News,Zhang-2023-arxiv-Benchmarking,Liu-arxiv-2022-Revisiting,Fabri-2021-tacl-SummEval. In some cases, it is difficult to achieve a high level of consensus among human annotators~Goyal-2023-arxiv-News, and there is also a large gap between the annotation quality of crowdworkers and experts~Fabri-2021-tacl-SummEval,Liu-arxiv-2022-Revisiting. Thus, how to conduct reliable evaluation for language generation tasks in the era of LLMs has become a fundamental yet challenging research topic. Recently, increasing research work proposes to leverage LLMs to improve the evaluation quality of the generated texts. Specially, LLMs can be used to improve the evaluation quality of existing metrics. For example, Para-Ref~Tang-2023-arxiv-Not augments various automatic metrics by leveraging LLMs to paraphrase existing references into semantically equivalent references with diverse expressions. Further, LLMs are widely employed as the evaluators of text generation in a reference-free manner, including evaluating a single prediction~Wang-2023-arxiv-Is,Liu-2023-arxiv-G-Eval,Wang-2023-arxiv-rethinking or comparing several candidates~Gao-arxiv-2023-Human,Ji-2023-arxiv-Exploring,vicuna2023,Bai-2023-arxiv-Benchmarking. Nevertheless, LLMs may expose bias (\\eg order bias or preference for LLM-generated texts over human-written texts) as language generation evaluators, demonstrating disparities when compared to human evaluation~Liu-2023-arxiv-Evaluate,Liu-2023-arxiv-G-Eval,Wang-2023-arxiv-Large. center tcolorbox[colback=blue!5!white,colframe=blue!55!black,width=0.46\\textwidth,title={Unreliable Generation Evaluation}] LLMs have been capable of generating texts with a comparable quality to human-written texts, which however might be underestimated by automatic reference-based metrics. % As an alternative evaluation approach, LLMs can serve as language generation evaluators to evaluate a single text, compare multiple candidates, and improve existing metrics. However, this evaluation approach still needs more inspections and examinations in real-world tasks. tcolorbox center $\\bullet$ {Underperforming specialized generation}. Although LLMs have learned general language patterns to generate coherent text, their proficiency in generation might be constrained when dealing with a specialized domain or task. % {For instance, a language model that has been trained on general web articles may face challenges when generating a medical report which involves many medical jargon and methods.} Intuitively, domain knowledge should be critical for model specialization. However, it is not easy to inject such specialized knowledge into LLMs. As discussed in recent analyses~FU-blog-2022-how,Ye-arxiv-2023-A, when LLMs are trained to exhibit some specific ability that allows them to excel in some areas, they might struggle in others. Such an issue is related to catastrophic forgetting~Michael-Psychology-1989-Catastrophic, Kemker-AAAI-2018-Measuring in training neural networks, which refers to the conflict phenomenon of integrating new and old knowledge. Similar cases also occur in human alignment of LLMs, where ``alignment tax''~Ouyang-arxiv-2022-Training (\\eg a potential loss in the in-context learning ability) has to be paid for aligning to human values and needs. { Moreover, due to the limitations of sequence modeling architecture, LLMs still face challenges in the understanding and generation of structured data. Consequently, they often fall behind task-specific models on complex structured data tasks, such as knowledge-base question answering and semantic parsing~Xie-EMNLP-2022-UnifiedSKG,Jiang-2023-arxiv-StructGPT. } Therefore, it is important to develop effective model specialization methods that can flexibly adapt LLMs to various task scenarios, meanwhile retaining the original abilities as possible. center tcolorbox[colback=blue!5!white,colframe=blue!55!black,width=0.46\\textwidth,title={Underperforming Specialized Generation}] LLMs may fall short in mastering generation tasks that require domain-specific knowledge or generating structured data. It is non-trivial to inject specialized knowledge into LLMs, meanwhile maintaining the original abilities of LLMs. tcolorbox center figure*[h] \\centering \\includegraphics[width=1\\textwidth]{images/hallucination.pdf} Examples of intrinsic and extrinsic hallucination for a public LLM (access date: March 19, 2023). As an example of intrinsic hallucination, the LLM gives a conflicting judgment about the relationship between Cindy and Amy, which contradicts the input. {For extrinsic hallucination, in this example, the LLM seems to have an incorrect understanding of the meaning of RLHF (reinforcement learning from human feedback), though it can correctly understand the meaning of LLMs (in this context). } figure*",
      "origin_cites_number": 42
    },
    {
      "section_title": "Knowledge Utilization",
      "level": "3",
      "content": "Knowledge utilization is an important ability of intelligent systems to accomplish knowledge-intensive tasks (\\eg commonsense question answering and fact completion) based on supporting factual evidence. Concretely, it requires LLMs to {properly utilize the rich factual knowledge from the pre-training corpus or retrieve external data when necessary.} In particular, question answering~(QA) and knowledge completion have been two commonly used tasks for evaluating this ability. According to the test tasks (question answering or knowledge completion) and evaluation settings (with or without external resources), we categorize existing knowledge utilization tasks into three types, namely closed-book QA, open-book QAIn this part, open-book QA refers to the QA tasks that require to extract and utilize useful information from external knowledge resources, as the antithesis of closed-book QA (only using the encoded information from pre-training corpus). Note that there is a dataset also named OpenBookQA~\\cite{Mihaylov-EMNLP-2018-Can, which follows the settings of open-book QA tasks by extracting and utilizing external science facts.}, and knowledge completion. Closed-Book QA. Closed-book QA tasks~Roberts-EMNLP-2020-How test the acquired factual knowledge of LLMs from the pre-training corpus, where LLMs should answer the question only based on the given context without using external resources. For evaluating this ability, there are several datasets that can be leveraged, including Natural Questions~Kwiatkowski-ACL-2019-Natural, Web Questions~Berant-EMNLP-2013-Semantic, and TriviaQA~Joshi-ACL-2017-TriviaQA, % {where the accuracy metric is widely adopted.} Empirical results have revealed that % LLMs can perform well in this setting and even match the performance of state-of-the-art {open-domain} QA systems~Chowdhery-arxiv-2022-PaLM. Also, the performance of LLMs on closed-book QA tasks shows a scaling law pattern in terms of both model size and data size: scaling the parameters and training tokens can increase the capacity of LLMs and help them learn (or memorize) more knowledge from the pre-training data~Chowdhery-arxiv-2022-PaLM. Further, under a similar parameter scale, LLMs with more pre-training data relevant to the evaluated tasks would achieve better performance~Nakano-arxiv-2021-WebGPT. Also, the closed-book QA setting provides a testbed for probing the accuracy of the factual knowledge encoded by LLMs. {However, as shown in existing work~Brown-NeurIPS-2020-Language, LLMs might perform less well on QA tasks relying on fine-grained knowledge, even when it exists in the pre-training data.} Open-Book QA. % Unlike closed-book QA, in open-book QA tasks, LLMs can extract useful evidence from the external knowledge base or document collections, and then answer the question based on the extracted evidence~Izacard-arxiv-2022-Few, Guu-ICML-2020-Retrieval, Lewis-NeurIPS-2020-Retrieval,Lan-2021-arxiv-Complex. {Typical open-book QA datasets (\\eg Natural Questions~Kwiatkowski-ACL-2019-Natural, OpenBookQA~Mihaylov-EMNLP-2018-Can, and SQuAD~Rajpurkar-EMNLP-2016-SQuAD) have overlap with closed-book QA datasets, but they incorporate external data sources, \\eg Wikipedia. The metrics of accuracy and F1 score are widely used in open-book QA tasks for evaluation.} To select relevant knowledge from external resources, LLMs are often paired with a text retriever (or even a search engine), which is trained independently or jointly with LLMs~Izacard-arxiv-2022-Few,Borgeaud-icml-2022-Improving,Nakano-arxiv-2021-WebGPT. { Also, previous work~Xu-arxiv-2023-Search,Peng-arxiv-2023-Check,Jiang-2023-arxiv-Active has indicated that retrievers can assist LLMs in verifying and rectifying the reasoning path. } In evaluation, existing studies mainly focus on testing how LLMs utilize the extracted knowledge to answer the question and show that the retrieved evidence can largely improve the accuracy of the generated answers, even enabling a smaller LLM to outperform $10\\times$ larger ones~Izacard-arxiv-2022-Few,Borgeaud-icml-2022-Improving. Further, open-book QA tasks can be also employed to evaluate the recency of knowledge information. Pre-training or retrieving from outdated knowledge resources may cause LLMs to generate incorrect answers for time-sensitive questions~Izacard-arxiv-2022-Few. Knowledge Completion. In knowledge completion tasks, LLMs might be (to some extent) considered as a knowledge base~Petroni-EMNLP-2019-Language, which can be leveraged to complete or predict the missing parts of knowledge units (\\eg knowledge triples). Such tasks can probe and evaluate how much and what kind of knowledge LLMs have learned from the pre-training data. Existing knowledge completion tasks can be roughly divided into knowledge graph completion tasks (\\eg FB15k-237~Toutanova-CVSC-2015-Observed and WN18RR~Dettmers-AAAI-2018-Convolutional) and fact completion tasks (\\eg WikiFact~Goodrich-KDD-2019-Assessing), which aim to complete the triples from a knowledge graph and incomplete sentences about specific facts, respectively. Empirical studies have revealed that it is difficult for existing LLMs to accomplish knowledge completion tasks {related to specific relation types~Liang-arxiv-2022-Holistic.} As shown in the evaluation results on WikiFact, LLMs perform well on several frequent relations that occur in the pre-training data (\\eg currency and author), while not well on rare ones (\\eg discoverer\\_or\\_inventor and place\\_of\\_birth). Interestingly, % {under the same evaluation settings (\\eg in-context learning), InstructGPT (\\ie text-davinci-002)} outperforms GPT-3 in all subsets of WikiFact. Major Issues. Although LLMs have achieved key progress in capturing and utilizing knowledge information, they suffer from two major issues as discussed below. $\\bullet$ Hallucination. In generating factual texts, a challenging issue is hallucination generations~Bang-arxiv-2023-A, Huang-arxiv-2023-A, where the generated information is either in conflict with the existing source (intrinsic hallucination) or cannot be verified by the available source (extrinsic hallucination), which are illustrated by two examples in Figure~fig:hallucination. Hallucination widely occurs in existing LLMs, even the most superior LLMs such as GPT-4~OpenAI-OpenAI-2023-GPT-4. { Furthermore, existing work shows that LLMs encounter difficulties in recognizing the hallucinated content in text~Li-arxiv-2023-HaluEval, even the powerful ChatGPT. Additionally, beyond language tasks, a recent study has shown that large vision-language models (LVLM) also face challenges with hallucination, \\ie generating objects that are not present in the accompanying images~Li-arxiv-2023-Evaluating. } In essence, LLMs seem to ``unconsciously'' utilize the knowledge in task solving, which still lack an ability to accurately control the use of internal or external knowledge. Hallucinations would mislead LLMs to generate undesired outputs and mostly degrade the performance, leading to potential risks when deploying LLMs in real-world applications. To alleviate this problem, alignment tuning strategies (as discussed in Section~sec-alignment) have been widely utilized in existing work~Ouyang-arxiv-2022-Training, which rely on tuning LLMs on high-quality data or using human feedback. { Moreover, the integration of external tools for the provision of credible information sources can help alleviate the hallucination issue~Li-arxiv-2023-HaluEval,Peng-arxiv-2023-Check,Nakano-arxiv-2021-WebGPT. Another line of research work leverages uncertainty estimation of LLMs to identify hallucinations~Kadavath-arxiv-2023-Language,Manakul-arxiv-2023-SelfCheckGPT. For instance, considering that hallucinated facts are prone to exhibit inconsistency across different sampled outputs, SelfCheckGPT~Manakul-arxiv-2023-SelfCheckGPT detects hallucination by measuring information inconsistency within sampled outputs. } For the evaluation of the hallucination problem, a set of hallucination detection tasks have been proposed, \\eg TruthfulQA~Lin-ACL-2022-TruthfulQA for detecting human falsehood mimicked by models. More recently, {HaluEval~Li-arxiv-2023-HaluEval creates a large-scale LLM-generated and human-annotated hallucinated samples to evaluate the ability of language models to recognize hallucination in both task-specific and general scenarios.} center tcolorbox[colback=blue!5!white,colframe=blue!55!black,width=0.46\\textwidth,title={Hallucination}] LLMs are prone to generate untruthful information that either conflicts with the existing source or cannot be verified by the available source. Even the most powerful LLMs such as ChatGPT face great challenges in migrating the hallucinations of the generated texts. This issue can be partially alleviated by special approaches such as alignment tuning and tool utilization. tcolorbox center $\\bullet$ Knowledge recency. % {As another major challenge, LLMs would encounter difficulties when solving tasks that require the latest knowledge beyond the training data. % To tackle this issue, a straightforward approach is to regularly update LLMs with new data. However, it is very costly to fine-tune LLMs, and also likely to cause the catastrophic forgetting issue when incrementally training LLMs. Therefore, it is necessary to develop efficient and effective approaches that can integrate new knowledge into existing LLMs, making them up-to-date. Existing studies have explored how to utilize the external knowledge source (\\eg search engine) to complement LLMs, which can be either jointly optimized with LLMs~Izacard-arxiv-2022-Few or used as a plug-and-play module~Peng-arxiv-2023-Check. For instance, ChatGPT utilizes a retrieval plugin to access up-to-date information sources~OpenAI-blog-2023-plugins. By incorporating the extracted relevant information into the context~Lazaridou-arxiv-2022-Internet,Qian-2023-arxiv-WebBrain,Liu-2023-arxiv-RETA-LLM, LLMs can acquire new factual knowledge and perform better on relevant tasks. However, such an approach seems to be still at a superficial level. In addition, {existing studies also explore editing parameters of language models to update intrinsic knowledge~Dai-ACL-2022-Knowledge,Meng-NIPS-2022-Locating,Geva-2021-emnlp-Transformer. Nevertheless, previous work~Yao-arxiv-2023-Editing has shown that several parameter editing methods perform not well on LLMs, though they can improve the performance of small language models. Therefore, it is still difficult to directly amend intrinsic knowledge or inject specific knowledge into LLMs, which remains {an open research problem~Yao-arxiv-2023-Editing}. Recently, a useful framework EasyEdit~wang-CoRR-2023-EasyEdit has been released to facilitate the research of knowledge editing for LLMs. }% } center tcolorbox[colback=blue!5!white,colframe=blue!55!black,width=0.46\\textwidth,title={Knowledge Recency}] The parametric knowledge of LLMs is hard to be updated in a timely manner. Augmenting LLMs with external knowledge sources is a practical approach to tackling the issue. However, how to effectively update knowledge within LLMs remains an open research problem. tcolorbox center",
      "origin_cites_number": 40
    },
    {
      "section_title": "Complex Reasoning",
      "level": "3",
      "content": "Complex reasoning refers to the ability of understanding and utilizing supporting evidence or logic to derive conclusions or make decisions~Huang-arxiv-2022-Towards,Qiao-arxiv-2022-Reasoning. According to the type of involved logic and evidence in the reasoning process, we consider dividing existing evaluation tasks into three major categories, namely knowledge reasoning, symbolic reasoning, and mathematical reasoning. Knowledge Reasoning. The knowledge reasoning tasks rely on logical relations and evidence about factual knowledge to answer the given question. Existing work mainly uses specific datasets to evaluate the reasoning capacity of the corresponding type of knowledge, \\eg CSQA~Talmor-naacl-2019-CommonsenseQA/StrategyQA~Geva-tacl-2021-Did for commonsense knowledge reasoning and ScienceQA~Saikh-IJDL-2022-ScienceQA for science knowledge reasoning. {In addition to the accuracy of the predicted results, existing work~Saikh-IJDL-2022-ScienceQA has also evaluated the quality of the generated reasoning process, via automatic metrics (\\eg BLEU) or human evaluation.} Typically, these tasks require LLMs to perform step-by-step reasoning based on factual knowledge, until reaching the answer to the given question. To elicit the % {step-by-step reasoning} ability, chain-of-thought~(CoT) prompting strategy~Wei-arxiv-2022-chain has been proposed for enhancing the complex reasoning capacity of LLMs. As discussed in Section~subsec-cot, CoT involves the intermediate reasoning steps, which can be manually created~Wei-arxiv-2022-chain or automatically generated~Shao-arxiv-2023-Synthetic, into the prompts to guide LLMs to perform multi-step reasoning. Such a way largely improves the reasoning performance of LLMs, leading to new state-of-the-art results on several complex knowledge reasoning tasks~Wei-arxiv-2022-chain,Chowdhery-arxiv-2022-PaLM,Ning-arxiv-2023-ChatGPT. Further, after reformulating knowledge reasoning tasks into code generation tasks, researchers have found that the performance of LLMs can be further improved~Madaan-emnlp-2022-Language, especially with the LLMs pre-trained on code. {However, due to the complexity of knowledge reasoning tasks, the performance of current LLMs still lags behind human results on tasks such as commonsense reasoning~Wei-arxiv-2022-chain,Chowdhery-arxiv-2022-PaLM,Sifatkaur-arxiv-2023-Mind.} As a common type of mistakes, LLMs might generate inaccurate {intermediate steps}, leading to a wrong final result. To address this issue, existing work has proposed special decoding or ensemble strategies to improve the accuracy of the whole reasoning chain~Wang-arxiv-2022-Self-Consistency,Li-arxiv-2022-On. More recently, an empirical study~\\cite{Ning-arxiv-2023-ChatGPT reveals that LLMs may have difficulty in explicitly inferring the commonsense knowledge required by a specific task, though they can successfully solve it. Further, it shows that leveraging self-generated knowledge may not be beneficial for improving the reasoning performance. } Symbolic Reasoning\\footnote{{Following~\\cite{Wei-arxiv-2022-chain, we mainly discuss symbolic reasoning tasks specially designed for evaluating LLMs. We do not consider symbolic reasoning methods in traditional NLP tasks, such as deducing logical rules from the knowledge graphs in KBQA.}}.} { The symbolic reasoning tasks mainly focus on manipulating the symbols in a formal rule setting to fulfill some specific goal~Huang-arxiv-2022-Towards, where the operations and rules may have never been seen by LLMs during pre-training. } { Existing work~Wei-arxiv-2022-chain,Kojima-arxiv-2022-Large,Zhou-arxiv-2022-Least commonly evaluates LLMs on the task of last letter concatenation and coin flip, where the evaluation examples require the same reasoning steps as the in-context examples (called in-domain test) or more steps (called out-of-domain test). For an example of the out-of-domain test, LLMs could only see the examples with two words in context, but it requires LLMs to concatenate the last letters of three or more words. Typically, the accuracy of the generated symbols is adopted to evaluate the performance of LLMs on these tasks.} Thus, LLMs need to understand the semantic relations among the symbolic operations and % {their composition in complex scenarios. However, under the out-of-domain setting, as LLMs have not seen the complex compositions of symbolic operations and rules (\\eg twice the number of operations in context examples), it is hard for LLMs to capture their accurate meanings.} To solve this issue, existing studies incorporate scratchpad~Anil-arxiv-2022-Exploring,Nye-arxiv-2021-Show and tutor~Qian-arxiv-2022-Limitations strategies to help LLMs better manipulate symbolic operations, for generating longer and more complex reasoning processes. Another line of research work utilizes the formal programming language to represent the symbolic operations and rules, which requires LLMs to generate code and perform the reasoning process by executing it with external interpreters. Such a way can decompose the complex reasoning process into code synthesis and program execution for LLMs and interpreters, respectively, leading to a simplified reasoning process with yet more accurate results~Gao-arxiv-2022-PAL. Mathematical Reasoning. The mathematical reasoning tasks need to comprehensively utilize mathematical knowledge, logic, and computation for solving problems or generating proof statements. Existing mathematical reasoning tasks can be mainly categorized into math problem solving and automated theorem proving. { For math problem solving tasks, SVAMP~Patel-NAACL-2021-Are, GSM8k~Cobbe-arxiv-2021-Training and MATH~Hendrycks-ICLR-2021-Measuring datasets are commonly used for evaluation, where LLMs need to generate accurate concrete numbers or equations to answer the mathematical problem. As these tasks also require multi-step reasoning, the CoT prompting strategy has been widely adopted for LLMs to improve the reasoning performance~Wei-arxiv-2022-chain. } As another practical strategy, continually pre-training LLMs on large-scale mathematical corpora can largely boost their performance on {mathematical reasoning tasks~Zhao-KDD-2022-JiuZhang,Taylor-arxiv-2022-Galactica,Lewkowycz-arxiv-2022-Solving.} Further, since math problems in different languages share the same mathematical logic, researchers also propose a multilingual math word problem benchmark~Shi-arxiv-2022-Language to evaluate the multilingual mathematical reasoning capacity of LLMs. As another challenging task, automated theorem proving (ATP)~Zheng-ICLR-2022-miniF2F,Welleck-NIPS-2021-NaturalProofs,Wang-CICM-2018-First requires the reasoning model to strictly follow the reasoning logic and mathematical skills. % {To evaluate the performance on this task, PISA~Jiang-AITP-2021-LISA and miniF2F~Zheng-ICLR-2022-miniF2F are two typical ATP datasets with the proof success rate as the evaluation metric.} As a typical approach, existing work on ATP utilizes LLMs to aid the search for proofs using an interactive theorem prover (ITP), such as Lean, Metamath, and Isabelle~Polu-arxiv-2020-Generative,Jiang-arxiv-2022-Thor,Polu-arxiv-2022-Formal. {A major limitation of ATP research is the lack of related corpora in formal language. To tackle it, several studies utilize LLMs to convert informal statements into formal proofs for augmenting new data~Wu-arxiv-2022-Autoformalization or generate drafts and proof sketches to} % {reduce the search space of the proofs}~Jiang-arxiv-2022-Draft. Major Issues. In spite of the advancements, LLMs still have several limitations in solving complex reasoning tasks. $\\bullet$ Reasoning inconsistency. With improved reasoning strategies (\\eg CoT prompting), LLMs can solve some complex reasoning tasks, by performing step-by-step reasoning based on the supporting logic and evidence. Despite the effectiveness, the reasoning inconsistency issue often occurs in the decomposed reasoning process. Concretely, LLMs may generate the correct answer following an invalid reasoning path, or produce a wrong answer after a correct reasoning process~Wei-arxiv-2022-chain,Lyu-arxiv-2023-Faithful, leading to inconsistency between the derived answer and the reasoning process. { To alleviate this problem, existing work has proposed to guide the whole generation process of LLMs via external tools or models~Zhang-ICLR-2023-Planning,Li-arxiv-2022-On,Yao-arxiv-2023-Tree, to re-check the reasoning process and final answer for correcting the potential errors~Madaan-arxiv-2023-Refine,Shinn-arxiv-2023-Reflexion,Gou-arxiv-2023-Critic or fine-tune LLMs with process-based feedback~Uesate-2023-arxiv-Solving,Lightman-2023-arxiv-Let. For instance, Tree of Thoughts~(ToT)~Yao-arxiv-2023-Tree empowers LLMs to engage in the decision-making process by concurrently {exploring and self-evaluating various reasoning paths}. To refine the {reasoning processes}, Self-Refine~Madaan-arxiv-2023-Refine elicits feedback from LLMs on self-generated solutions, {enabling} the iterative refinement of solutions based on the feedback. Moreover, several studies improve the consistency in the reasoning chain of LLMs through the integration of process-based supervision during training~Uesate-2023-arxiv-Solving,Lightman-2023-arxiv-Let. } { As a promising solution, recent approaches reformulate the complex reasoning tasks into code generation tasks, where the strict execution of the generated code ensures the consistency between the reasoning process and the outcome. } Also, it has been revealed that there might exist inconsistency between tasks with similar inputs, where small changes in the task description may cause the model to produce different results~Patel-NAACL-2021-Are,Lu-arxiv-2022-Survey. { To mitigate this problem, self-consistency~Wang-arxiv-2022-Self-Consistency adopts the ensemble of multiple reasoning paths to enhance the decoding process of LLMs. } center tcolorbox[colback=blue!5!white,colframe=blue!55!black,width=0.46\\textwidth,title={Reasoning Inconsistency}] LLMs may generate the correct answer following an invalid reasoning path, or produce a wrong answer after a correct reasoning process, leading to inconsistency between the derived answer and the reasoning process. {The issue can be alleviated by fine-tuning LLMs with process-level feedback, using an ensemble of diverse reasoning paths, and refining the reasoning process with self-reflection or external feedback.} tcolorbox center $\\bullet$ Numerical computation. {For complex reasoning tasks, LLMs still face difficulties in the involved numerical computation, especially for the symbols that are seldom encountered during pre-training, such as arithmetic with large numbers~Qian-arxiv-2022-Limitations,Lu-arxiv-2022-Survey,Yuan-arxiv-2023-Arithmetic. To tackle this issue, a direct way is to tune LLMs on synthesized arithmetic problems~Pi-EMNLP-2022-Reasoning,liu-arxiv-2023-goat. {Also, a surge of studies improve the numerical computation performance by tracing intermediate calculation steps in training and inference stages~Nye-arxiv-2021-Show,Zhou-2023-arxiv-Teaching,liu-arxiv-2023-goat, \\eg scratchpad tracing.} In addition, existing work~Schick-arxiv-2023-Toolformer has also incorporated external tools (\\eg calculator), especially for handling arithmetic operations. More recently, ChatGPT has provided a plugin mechanism to use external tools~OpenAI-blog-2023-plugins. In this way, LLMs need to learn how to properly manipulate the tools. For this purpose, researchers have augmented the examples using tools (even the LLM itself) for tuning the LLM~Parisi-arxiv-2022-TALM,Schick-arxiv-2023-Toolformer, or devised instructions and exemplars for in-context learning~Gao-arxiv-2022-PAL.} { In addition to the aid of external tools, recent studies find that tokenizing digits into individual tokens (\\eg LLaMA and Galactica tokenizers) is a useful approach to enhancing the inherent arithmetic ability of LLMs~Yuan-arxiv-2023-Arithmetic,liu-arxiv-2023-goat. % {One possible explanation is that subword tokenization techniques can result in inconsistent sequences when tokenizing numbers. For instance, with a subword tokenizer the integer 7481 may be tokenized as $7\\_481$, while 74815 may be tokenized as $748\\_15$ (the same numerical substrings with different splits)~liu-arxiv-2023-goat.} As a comparison, digit-based tokenization for numbers can avoid such an inconsistency, thus likely improving the numerical computation ability of LLMs. } center tcolorbox[colback=blue!5!white,colframe=blue!55!black,width=0.46\\textwidth,title={Numerical Computation}] LLMs face difficulties in numerical computation, especially for the symbols that are seldom encountered during pre-training. In addition to using mathematical tools, tokenizing digits into individual tokens is also an effective design choice for improving the arithmetic ability of LLMs. tcolorbox center",
      "origin_cites_number": 49
    },
    {
      "section_title": "Advanced Ability",
      "level": "2",
      "content": "{In addition to the above basic evaluation tasks, LLMs also exhibit some superior abilities that require special considerations for evaluation. In this part, we discuss several representative advanced abilities and the corresponding evaluation approaches, including human alignment, interaction with the external environment, and tool manipulation. % Next, we discuss these advanced abilities in detail. }%",
      "origin_cites_number": 0
    },
    {
      "section_title": "Human Alignment",
      "level": "3",
      "content": "It is desired that LLMs could well conform to human values and needs, \\ie human alignment, which is a key ability for the broad use of LLMs in real-world applications. % To evaluate this ability, existing studies consider multiple criteria for human alignment, such as helpfulness, honesty, and safety~Askell-arxiv-2021-A,OpenAI-OpenAI-2023-GPT-4,Bai-arxiv-2022-Training. For helpfulness and honesty, adversarial question answering tasks (\\eg TruthfulQA~Lin-ACL-2022-TruthfulQA) can be utilized to examine LLM's ability in detecting possible falsehood in the text~Nakano-arxiv-2021-WebGPT,OpenAI-OpenAI-2023-GPT-4. Furthermore, harmlessness can be also evaluated by several existing benchmarks, \\eg CrowS-Pairs~Nangia-EMNLP-2020-CrowS and Winogender~Rudinger-NAACL-2018-Gender. Despite the automatic evaluation with the above datasets, human evaluation is still a more direct way to effectively test the human alignment ability of LLMs. {OpenAI invites many experts in domains related to AI risks to evaluate and improve the behaviors of GPT-4 when encountering risky contents~OpenAI-OpenAI-2023-GPT-4. } In addition, for other aspects of human alignment (\\eg truthfulness), several studies propose to use specific instructions and devise annotation rules to guide the annotation process~Nakano-arxiv-2021-WebGPT. Empirical studies have revealed that these strategies can greatly improve the human alignment ability of LLMs~Bai-arxiv-2022-Training. % {For instance, after alignment tuning on data collected through interactions with experts, the incorrect behavior rate of GPT-4 can be largely reduced when it deals with sensitive or disallowed prompts. } {In addition, high-quality pre-training data can reduce the effort required for alignment~OpenAI-OpenAI-2023-GPT-4.} For instance, Galactica is potentially more harmless due to the less biased contents in the scientific corpus~Taylor-arxiv-2022-Galactica.",
      "origin_cites_number": 10
    },
    {
      "section_title": "Interaction with External Environment",
      "level": "3",
      "content": "In addition to standard evaluation tasks, LLMs have the ability to receive feedback from the external environment and perform actions according to the behavior instruction, \\eg generating action plans in natural language to manipulate agents~Huang-ICML-2022-Language,Carta-arxiv-2023-Grounding. Such an ability is also emergent in LLMs that can generate detailed and highly realistic action plans, while smaller models (\\eg GPT-2) tend to generate shorter or meaningless plans~Huang-ICML-2022-Language. To test this ability, several embodied AI environments and benchmarks can be used for evaluation, described as follows. VirtualHome~Puig-CVPR-2018-VirtualHome builds a 3D simulator for household tasks such as cleaning and cooking, in which the agent can execute natural language actions generated by LLMs. ALFRED~Shridhar-CVPR-2020-ALFRED includes more challenging tasks that require LLMs to accomplish compositional targets. BEHAVIOR~Srivastava-CoRL-2021-BEHAVIOR focuses on {everyday chores} in simulation environments and requires LLMs to generate complex solutions, \\eg changing the internal status of objects. { Apart from restricted environments such as household tasks, a line of research work investigates the proficiency of LLM-based agents to explore open-world environments, such as Minecraft and the Internet~Zhu-arxiv-2023-Ghost, Wang-arxiv-2023-Voyager. Voyager~Wang-arxiv-2023-Voyager introduces an automatic curriculum module that enables LLMs to continuously acquire new skills based on feedback from the environment. GITM~Zhu-arxiv-2023-Ghost focuses on solving various challenges in Minecraft based on LLM, through task decomposition, planning, and invocation of interfaces. } Based on the generated action plans or task completions, existing work either adopts the regular metrics (\\eg executability and correctness of the generated action plans)~Huang-ICML-2022-Language in the benchmark or directly conducts real-world experiments and measures the success rate~Ahn-arxiv-2022-Do, to evaluate such ability. It has been shown that LLMs are capable in interacting with the external environment and generating accurate action plans~Liang-arxiv-2022-Code. Recently, several improvement methods have been proposed to enhance the interaction ability of LLMs, \\eg designing code-like prompts~Singh-arxiv-2022-ProgPrompt and providing real-world grounding~Ahn-arxiv-2022-Do. { In addition, recent work also explores multi-agent collaboration based on LLMs in simulated environments~Park-arxiv-2023-Generative,Fu-arxiv-2023-Improving,Metha-arxiv-2023-Improving. These studies simulate human social behaviors by instantiating multiple LLM-based agents with observations, planning, and memories in a sandbox environment. In controlled evaluation, the abilities of generative agents to search, plan, and think are evaluated by humans in an interview-like manner. Further, they also conduct descriptive measurements on multiple agents within a simulated environment to examine emergent social behaviors. }",
      "origin_cites_number": 14
    },
    {
      "section_title": "Tool Manipulation",
      "level": "3",
      "content": "When solving complex problems, LLMs can turn to external tools if they determine it is necessary. % By encapsulating available tools with API calls, existing work has involved a variety of external tools, \\eg search engine~Nakano-arxiv-2021-WebGPT, calculator~Schick-arxiv-2023-Toolformer, and compiler~Gao-arxiv-2022-PAL, to enhance the performance of LLMs on several specific tasks. Recently, OpenAI has supported the use of plugins in ChatGPT~OpenAI-blog-2023-plugins, which can equip LLMs with broader capacities beyond language modeling. For example, the web browser plugin enables ChatGPT to access fresh information. Further, incorporating third-party plugins is particularly key for creating a prosperous ecosystem of applications based on LLMs. {To examine the ability of tool manipulation, existing work mostly adopts complex reasoning tasks for evaluation, such as mathematical problem solving (\\eg GSM8k~Cobbe-arxiv-2021-Training and SVAMP~Patel-NAACL-2021-Are) or knowledge question answering (\\eg TruthfulQA~Lin-ACL-2022-TruthfulQA), where the successful utilization of tools is very important for enhancing the required skills that LLMs are incapable in (\\eg numerical calculation). In this way, the evaluated performance on these tasks can reflect the ability of LLMs in tool manipulation.} To teach LLMs to utilize tools, existing studies add exemplars using tools in context to elicit LLMs~Gao-arxiv-2022-PAL, or fine-tune LLMs on simulated data about tool utilization~Parisi-arxiv-2022-TALM, Schick-arxiv-2023-Toolformer. It has been found that with the help of tools, LLMs become more capable of handling the issues that they are not good at, \\eg equation calculation and answering timely questions~Schick-arxiv-2023-Toolformer,Chen-2023-arXiv-chatcot. { However, as the number of available tools increases, the limited context length of LLMs may pose challenges in describing and demonstrating extensive tool APIs. To address this issue, existing work retrieves the usage of relevant tools, or encoding tool information as tokens within the embedding space~Shishir-2023-arxiv-Gorilla,Hao-2023-arxiv-ToolkenGPT,Liang-2023-arxiv-TaskMatrix.} { In addition to existing tools developed by humans, LLMs possess the capability to make their own tools for specific tasks autonomously~Cai-arxiv-2023-Tool. This enables the models to independently explore and manipulate these self-created tools, thereby expanding their potential for autonomous exploration in solving a wide range of real-world tasks.} {Summary. The above three abilities are of great value to the practical performance of LLMs: conforming to human values and preferences (human alignment), acting properly in real-world scenarios (interaction with the external environment), and expanding the ability scope (tool manipulation). In addition to the above three advanced abilities, LLMs might also show other abilities that are specially related to some tasks (\\eg data annotation~Gilardi-arXiv-2023-Crowd) or learning mechanisms (\\eg self-improvement~Huang-arxiv-2022-Large). It will be an open direction to discover, measure and evaluate these newly emerging abilities, so as to better utilize and improve LLMs. } table*[htbp] \\centering {A category of existing evaluation work. ``General'' denotes that the evaluation focuses on an overall performance of multiple abilities. The evaluated abilities are not limited to the representative basic and advanced abilities mentioned in Section~\\ref{sec:basicability and sec:superior.}} \\footnotesize tabular{ccccc} \\toprule Method & Evaluation & Model Types & Abilities/Domain & Data Source\\\\ \\midrule 28{*}{Benchmark} & MMLU~Hendrycks-ICLR-2021-Measuring & Base/Fine-tuned/Specialized & General & Human exam/practice \\\\ & BIG-bench~Srivastava-arxiv-2022-Beyond & Base/Fine-tuned/Specialized & General & Human annotation \\\\ & HELM~Liang-arxiv-2022-Holistic & Base/Fine-tuned/Specialized & General & Benchmark collection \\\\ & Open LLM Leaderboard~Edward-2023-hf-open & Base/Fine-tuned/Specialized & General & Benchmark collection \\\\ & AGIEval~Zhong-2023-arxiv-AGIEval & Base/Fine-tuned/Specialized & General & Human exam/practice \\\\ & MMCU~Zeng-arxiv-2023-MMCU & Base/Fine-tuned/Specialized & General & Human exam/practice \\\\ & M3KE~Liu-2023-arxiv-M3KE & Base/Fine-tuned/Specialized & General & Human exam/practice \\\\ & C-Eval~Huang-arxiv-2023-CEval & Base/Fine-tuned/Specialized & General & Human exam/practice \\\\ & Xiezhi~Gu-2023-arxiv-Xiezhi & Base/Fine-tuned/Specialized & General & Human exam/practice \\\\ & OpenCompass~2023opencompass & Base/Fine-tuned/Specialized & General & Benchmark collection \\\\ & Chain-of-Thought Hub~Fu-arxiv-2023-Chain & Base/Fine-tuned & General & Benchmark collection \\\\ & KoLA~Yu-arxiv-2023-KoLA & Base/Fine-tuned & Knowledge utilization & Web \\\\ & ARB~Sawada-arxiv-2023-ARB & Fine-tuned & Complex reasoning & Human exam/practice \\\\ & APIBench~Peng-arxiv-2023-Revisiting & Base/Fine-tuned & Tool manipulation & Web \\\\ & APIBank~Li-arxiv-2023-API-Bank & Fine-tuned & Tool manipulation & Synthesis \\\\ & ToolAlpaca~Tang-arxiv-2023-ToolAlpaca & Base/Fine-tuned & Tool manipulation & Synthesis \\\\ & T-Bench~Xu-arxiv-2023-On & Fine-tuned & Tool manipulation & Synthesis \\\\ & ToolBench~Qin-arxiv-2023-ToolLLM & Fine-tuned & Tool manipulation & Synthesis \\\\ & BOLAA~Liu-arxiv-2023-BOLAA & Base/Fine-tuned & Environment interaction & Benchmark collection \\\\ & AgentBench~Liu-arxiv-2023-AgentBench & Base/Fine-tuned & Environment interaction & Human annotation/Synthesis \\\\ & HaluEval~Li-arxiv-2023-HaluEval & Base/Fine-tuned & Human alignment & Human annotation/Synthesis \\\\ & PromptBench~Zhu-arxiv-2023-PromptBench & Base/Fine-tuned & Robustness & Benchmark collection \\\\ & HumanEval~Chen-arxiv-2021-evaluating & Base/Fine-tuned/Specialized & Code synthesis & Human annotation \\\\ & MultiMedQA~singhal-arxiv-2022-large & Specialized & Healthcare & Benchmark collection \\\\ & FLUE~Shah-arxiv-2023-FLUE & Specialized & Finance & Benchmark collection \\\\ & LegalBench~Guha-arxiv-2022-LegalBench & Specialized & Legal & Human annotation \\\\ \\midrule 2{*}{Human} & Chatbot Arena~Zheng-2023-arxiv-Judging & Base/Fine-tuned/Specialized & Human Alignment & Human annotation \\\\ & SciBench~Wang-arxiv-2023-SciBench & Fine-tuned & Complex reasoning & Human exam/practice \\\\ \\midrule 5{*}{Model} & AlpacaEval~Li-2023-github-alpaca_eval & Fine-tuned & Instruction following & Synthesis \\\\ & MT-bench~Zheng-2023-arxiv-Judging & Fine-tuned & Human alignment & Human annotation \\\\ & TrustGPT~Huang-arxiv-2023-TrustGPT & Base/Fine-tuned & Human alignment & Benchmark collection \\\\ & LMExamQA~Bai-arxiv-2023-Benchmarking & Base/Fine-tuned & Knowledge utilization & Synthesis \\\\ & ChatEval~Chan-arixiv-2023-ChatEval & Base/Fine-tuned & Knowledge utilization & Benchmark collection \\\\ \\bottomrule tabular table*",
      "origin_cites_number": 47
    },
    {
      "section_title": "Benchmarks and Evaluation Approaches",
      "level": "2",
      "content": "In the above, we have discussed the basic and advanced abilities of LLMs. Next, we will introduce existing evaluation benchmarks and approaches~Chang-2023-arxiv-A,Zhuang-2023-arxiv-Through.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Comprehensive Evaluation Benchmarks",
      "level": "3",
      "content": "Recently, several comprehensive benchmarks~Hendrycks-ICLR-2021-Measuring,Srivastava-arxiv-2022-Beyond,Liang-arxiv-2022-Holistic have been released for the evaluation of LLMs. In this part, we introduce several widely used benchmarks, \\ie MMLU, BIG-bench, HELM, and a series of human exam benchmarks. $\\bullet$ MMLU~Hendrycks-ICLR-2021-Measuring is a versatile benchmark for large-scale evaluation of multi-task knowledge understanding, covering a wide range of knowledge domains {from mathematics and computer science to humanities and social sciences. The difficulties of these tasks vary from basic to advanced.} {As shown in existing work, LLMs mostly outperform small models by a substantial margin on this benchmark~Chowdhery-arxiv-2022-PaLM,Chung-arxiv-2022-Scaling,Taylor-arxiv-2022-Galactica,Touvron-arxiv-2023-LLaMA, which shows the scaling law in model size. % {More recently, GPT-4 achieves a remarkable record (86.4\\% in 5-shot setting) in MMLU, which is significantly better than the previous state-of-the-art models~OpenAI-OpenAI-2023-GPT-4.}} $\\bullet$ BIG-bench~Srivastava-arxiv-2022-Beyond is a collaborative benchmark intended to probe existing LLMs from various aspects. {It comprises 204 tasks that encompass a broad range of topics, including linguistics, childhood development, mathematics, commonsense reasoning, biology, physics, social bias, software development, and so on.} {By scaling the model size, LLMs can even outperform the average human performance under the few-shot setting on 65\\% of tasks in BIG-bench~Chowdhery-arxiv-2022-PaLM.} {Considering the high evaluation cost of the entire benchmark, a lightweight benchmark BIG-bench-Lite has been proposed, which contains 24 small yet diverse and challenging tasks from BIG-bench.} Additionally, the BIG-bench hard (BBH) benchmark~Suzgun-arxiv-2022-Challenging has been proposed to concentrate on investigating the currently unsolvable tasks of LLMs by selecting the challenging tasks in which LLMs exhibit inferior performance compared to humans. Since BBH becomes more difficult, small models mostly achieve performance close to random. {As a comparison, CoT prompting can elicit the abilities of LLMs to perform step-by-step reasoning for enhancing the performance, even exceeding the average human performance in BBH.} $\\bullet$ HELM~Liang-arxiv-2022-Holistic is a comprehensive benchmark that currently implements a core set of 16 scenarios and 7 categories of metrics. It is built on top of many prior studies, conducting a holistic evaluation of language models. As shown in the experimental results of HELM, instruction tuning can consistently boost the performance of LLMs in terms of accuracy, robustness, and fairness. Further, for reasoning tasks, the LLMs that have been pre-trained on the code corpus show superior performance. $\\bullet$ \\emph{Xiezhi~Gu-2023-arxiv-Xiezhi is an extensive benchmark for domain knowledge evaluation, consisting of a vast collection of multiple-choice questions across 516 diverse disciplines spanning 13 different subjects. Considering different domain knowledge utilization scenarios, Xiezhi also provides two specific benchmarks, \\ie Xiezhi-Specialty and Xiezhi-Interdiscipline. Experimental results reveal that the top-performing LLMs outperform average human performance in the fields of science, engineering, agronomy, and medicine, while humans still exhibit significantly higher performance than all LLMs in the domains such as economics, jurisprudence, and literature. } $\\bullet$ Human-level test benchmarks aim to evaluate the comprehensive ability of LLMs with questions designed for testing humans, such as AGIEval~Zhong-2023-arxiv-AGIEval, MMCU~Zeng-arxiv-2023-MMCU, M3KE~Liu-2023-arxiv-M3KE, C-Eval~Huang-arxiv-2023-CEval and {Xiezhi}~Gu-2023-arxiv-Xiezhi. These benchmarks encompass a wide range of domains, difficulty levels, and languages to provide a comprehensive evaluation of LLMs' general capabilities. Compared to publicly available models, models offering API services (\\eg GPT-4, ChatGPT, Claude) demonstrate superior performance compared to publicly available models on these evaluation benchmarks. As the best-performing model in evaluations, GPT-4 surpasses average human performance in AGIEval~Zhong-2023-arxiv-AGIEval. {However, it still lags behind the top human performance on these challenging benchmarks.} Hence, there remains ample room for further enhancements in the overall abilities of LLMs, particularly for publicly accessible models. The above benchmarks cover a variety of mainstream evaluation tasks and real-world human exam questions for the evaluation of LLMs. Also, there are several benchmarks that focus on evaluating specific abilities of LLMs, such as TyDiQA~Clark-trans-2020-TyDi for multilingual knowledge utilization and MGSM~Shi-arxiv-2022-Language for multilingual mathematical reasoning. To conduct the evaluation, one can select suitable benchmarks according to specific goals. {In addition, there are also several open-source evaluation frameworks for researchers to evaluate LLMs on existing benchmarks or extend new tasks for customized evaluations, such as Language Model Evaluation Harness~Leo-zenodo-2021-A and OpenAI Evals~OpenAI-OpenAI-2023-GPT-4.} {Further, some researchers also construct continuously updated leaderboards by aggregating representative benchmarks, to compare the performance of existing LLMs, such as Open LLM Leaderboard~Edward-2023-hf-open. The above benchmarks and leaderboards provide important references to demonstrate the basic and advanced abilities of LLMs. We will give more deep discussions on pros and cons on evaluation approaches in Section~subsec-evaapp. }",
      "origin_cites_number": 20
    },
    {
      "section_title": "Evaluation Approaches",
      "level": "3",
      "content": "After introducing existing benchmarks, in this part, we will review existing evaluation approaches for assessing the performance of LLMs. To organize our discussion, we categorize LLMs into three different types: base LLMs (pre-trained model checkpoints), fine-tuned LLMs (instruction or alignment fine-tuned model checkpoints), and specialized LLMs (adapted model checkpoints for some specific task or domain). Here, we keep both fine-tuned LLMs and specialized LLMs, to distinguish the different purposes of LLMs: general or specific task solvers. To evaluate the three types of LLMs, we can test the LLM's performance related to different abilities (\\eg basic or advanced abilities as discussed in Section~sec:basicability and sec:superior). In general, there are three main approaches to evaluating LLMs, namely benchmark-based approach~Hendrycks-ICLR-2021-Measuring, human-based approach~Zheng-2023-arxiv-Judging, and model-based approach~Li-2023-github-alpaca_eval. Table~tab:benchmark shows an illustration of the relationship among LLM type, evaluation approach, and tested abilities. Next, we will discuss the evaluation approaches for different types of LLMs. Evaluation of Base LLMs. Base LLMs refer to the model checkpoints obtained right after pre-training. For base LLMs, we mainly focus on examining the basic abilities (Section~sec:basicability), such as complex reasoning and knowledge utilization. Since most of these basic abilities can be assessed with well-defined tasks, benchmark-based approaches have been widely used to evaluate base LLMs. Next, we will introduce common evaluation benchmarks and evaluation procedures for base LLMs. $\\bullet$~Common benchmarks. To evaluate base LLMs, typical benchmarks are designed in the form of close-ended problems like multiple-choice questions. These commonly used benchmarks can be mainly divided into two categories: knowledge-oriented and reasoning-oriented benchmarks. Knowledge-oriented benchmarks (\\eg MMLU~Hendrycks-ICLR-2021-Measuring and C-Eval~Huang-arxiv-2023-CEval) aim to evaluate the capacity of world knowledge, while reasoning-oriented benchmarks (\\eg GSM8K~Gao-arxiv-2023-Human, BBH~Suzgun-arxiv-2022-Challenging, and MATH~Hendrycks-ICLR-2021-Measuring) focus on evaluating the capability of solving complex reasoning tasks. Further, some recently proposed benchmarks (\\eg OpenCompass~2023opencompass) combine these two types for a comprehensive comparison. $\\bullet$~Benchmark based evaluation procedure. To perform the benchmark evaluation, each problem will first be formatted into a prompt for LLMs to generate the result text. Then, the generated result text will be parsed with human-written rules to get the predicted answer. Finally, the performance of LLMs can be automatically calculated using standard metrics like accuracy by comparing the predicted answer with the ground-truth one. The evaluation approach can be conducted in either the few-shot or zero-shot setting, which might lead to different evaluation results or rankings. Since base LLMs have not been instruction fine-tuned (with relatively weak task generalization ability), the few-shot setting is often more suitable for evaluation. For some complex reasoning tasks, CoT prompts also need to be used to fully exhibit the capacity during evaluation. Another note is that this evaluation approach can also be applied to assess the abilities of fine-tuned LLMs. Actually, several leaderboards (\\eg Open LLM Leaderboard~Edward-2023-hf-open) are built upon this approach, evaluating both base and fine-tuned LLMs. Evaluation of Fine-tuned LLMs. Fine-tuned LLMs in this part refer to the model checkpoints obtained after instruction tuning or alignment tuning based on pre-trained model weightsIn some cases, it is also called \\emph{chat models.}. Typically, fine-tuned LLMs will be tested on various abilities (\\eg knowledge utilization and human alignment), and thus it is common that they are assessed with multiple evaluation approaches. In addition to benchmark-based evaluation, human-based and model-based approaches have also been widely used to evaluate the advanced abilities of fine-tuned LLMs. Next, we will introduce the two evaluation methods. $\\bullet$~Human-based evaluation. Unlike automatic evaluation for basic abilities, human evaluation typically considers more factors or abilities in real-world use, such as human alignment and tool manipulation. In this evaluation approach, test tasks are usually in the form of open-ended questions, and human evaluators are invited to make judgments on the quality of answers generated by LLMs. Typically, there are two main types of scoring methods for human evaluators: pairwise comparison and single-answer grading. In pairwise comparison, given the same question, humans are assigned two answers from different models to determine which one is better, while in single-answer grading, they only need to score a single answer at a time. For example, HELM~Liang-arxiv-2022-Holistic employs humans to perform single-answer grading on summarization and disinformation tasks, while Chatbot Arena~Zheng-2023-arxiv-Judging constructs a crowdsourcing platform that allows users to engage in conversations with two anonymous chat LLMs and report pairwise comparison results. $\\bullet$~Model-based evaluation. Since human-based evaluation is both expensive and time-consuming, some work has proposed leveraging powerful closed-source LLMs such as ChatGPT and GPT-4 as a surrogate for human evaluators~Zheng-2023-arxiv-Judging, Li-2023-github-alpaca_eval. For example, AlpacaEval~Li-2023-github-alpaca_eval collects a set of instructions and utilizes a capable LLM (\\eg GPT-4) as the judge to perform pair-wise comparisons against the reference outputs. Furthermore, MT-bench~Zheng-2023-arxiv-Judging collects a set of multi-turn questions for evaluation and improves the reliability of LLM-based evaluators through methods like ICL and CoT. Compared with human evaluators, LLMs such as ChatGPT and GPT-4 can achieve high agreement with humans, in both small-scale handcrafted and large-scale crowdsourced evaluation tasks. Despite this, these closed-source LLMs are limited in access and have the potential risk of data leakage. To address this, recent work~Zheng-2023-arxiv-Judging has explored fine-tuning open-source LLMs (\\eg Vicuna~vicuna2023) as model evaluators using scoring data from human evaluators, which has narrowed the gap with powerful closed-source LLMs (\\eg GPT-4). In addition to these two main evaluation approaches, the authors in~\\cite{Jain-2023-arxiv-Bring further propose self-supervised evaluation to get rid of the involvement of humans or models, which performs interventions on the input text and takes the sensitivity of LLMs as the evaluation metric. } Evaluation of Specialized LLMs. Specialized LLMs refer to the model checkpoints specially adapted to some domains or applications like healthcare~singhal-arxiv-2022-large and finance~Shah-2022-EMNLP-When. As special task solvers, specialized LLMs will be tested not only on general abilities (\\eg basic ability like complex reasoning and advanced ability like human alignment), but also on specific abilities related to their designated domains or applications. For this purpose, one often needs to construct specific benchmarks tailored for the target domains or applications. Then, these domain-specific benchmarks can be combined with general benchmarks to conduct both comprehensive and targeted evaluation for specialized LLMs. For example, MultiMedQA~singhal-arxiv-2022-large is a specific benchmark in healthcare, which includes medical examinations and healthcare questions. In this work~singhal-arxiv-2022-large, MultiMedQA has been combined with MMLU~Hendrycks-ICLR-2021-Measuring to assess the performance of specialized LLMs for healthcare, such as Med-PaLM~singhal-arxiv-2022-large. Similarly, FLUE~Shah-2022-EMNLP-When constructs a benchmark for finance, spanning from financial sentiment analysis to question answering. It has been used collaboratively with BBH~Suzgun-arxiv-2022-Challenging to evaluate finical LLMs like BloombergGPT~wu-arxiv-2023-bloomberggpt. Pros and Cons of Different Evaluation Approaches. In the above, we have discussed different evaluation approaches to assess the abilities of LLMs. Next, we simply analyze the pros and cons of each evaluation approach. % $\\bullet$~Benchmark-based approach. This evaluation approach can leverage existing benchmarks for assessing the performance of LLMs. The tasks involved in these benchmarks often contain sufficient test samples to measure the core abilities (\\eg reasoning). The whole evaluation procedure can be (almost) automatic, and it is convenient to carry out test experiments for various base LLMs, especially useful for monitoring the performance of model checkpoints during pre-training. However, LLMs are often sensitive to the evaluation settings, including the question prompts, zero-shot or few-shot tests, and the answer parsing methods. Thus, one should take possible influencing factors into consideration when conducting the evaluation experiments. The evaluation results should be noted with the adopted evaluation settings. Another issue is the data contamination~Chowdhery-arxiv-2022-PaLM,zhou-arxiv-2023-dont, \\ie the test data itself or relevant content has been contained in the pre-training corpora. This phenomenon has become increasingly severe since more and more open data has been collected for developing LLMs. $\\bullet$~Human-based approach. Human evaluation offers several advantages when assessing the capabilities of LLMs to solve real-world tasks. One of the key benefits is its ability to directly reflect the actual abilities of LLMs. Based on feedback and experiences from real users, human evaluation provides a more direct measure of LLMs' performance in real-world scenarios. Further, it can conduct more flexible and diverse evaluation tasks based on human evaluators. For instance, users can submit various queries and test the abilities of LLMs according to their own task cognition. It allows for a deep understanding of the strengths and weaknesses of LLMs across different types of tasks and contexts. However, human evaluation also has inherent limitations that could potentially affect its accuracy and consistency. Factors such as personalized tastes and varying education levels among evaluators can introduce biases or even inconsistencies in the evaluation process. In some cases, users' judgments are likely to be subjective, which may not reflect the true capabilities of the LLMs. Moreover, conducting robust and reliable human evaluations often requires a large number of evaluators, which can be very expensive and time-consuming. In addition, human evaluation is often not reproducible, making it infeasible to extend existing evaluation results or track the progress of LLMs. $\\bullet$~Model-based approach. As a surrogate for human-based approaches, model-based approaches serve to diminish the reliance on human involvement, and enable more efficient and scalable evaluation. In addition, LLMs can provide meaningful explanations for the assigned rating scores, thereby enhancing the interpretability of evaluations. Despite their scalability and explanability, model-based approaches have been found to suffer from several issues, including position, verbosity, and self-enhancement bias~Zheng-2023-arxiv-Judging. Specially, position bias (\\ie the order to present the responses) refers to the fact that LLMs tend to assign high scores for the answers at specific positions over others, verbosity bias means that LLMs favor verbose answers even if they are short in quality compared with shorter answers, and self-enhancement bias indicates that LLMs often overrate in their own generations. In addition, since LLMs have limited capacities in solving complex reasoning problems, they cannot serve as qualified evaluators for some difficult tasks (\\eg mathematical reasoning). These limitations can be mitigated to some extent by specific prompt engineering and fine-tuning strategies~Zheng-2023-arxiv-Judging. To summarize, our categorization (Table~tab:benchmark) of existing work on LLM evaluation is mainly based on two major dimensions, namely evaluation methodology and model type, which are further extended with the test abilities. There are some recent work~Chang-2023-arxiv-A,Zhuang-2023-arxiv-Through that also has discussed the categorization or taxonomies of existing work for LLM evaluation. table*[htb] \\renewcommand1.1 \\setlength2.5pt \\centering Evaluation on the eight abilities of LLMs with specially selected tasks. The shade of the \\colorbox[HTML]{FC8D59{Orange} and \\colorbox[HTML]{92BFDB}{Blue} fonts denote the performance orders of the results in closed-source and open-source models, respectively. This table will be continuously updated by incorporating the results of more models. } 1.85\\columnwidth{!}{ tabular{lccccccccc} \\toprule 2.5{*}{Models} & 4{c}{Language Generation} & 5{c}{Knowledge Utilization} \\\\ \\cmidrule(r){2-5}\\cmidrule(r){6-10} & LBD$\\uparrow$ & WMT$\\uparrow$ & XSum$\\uparrow$ & HumanEval$\\uparrow$ & TriviaQA$\\uparrow$ & NaturalQ$\\uparrow$ & WebQ$\\uparrow$ & ARC$\\uparrow$ & WikiFact$\\uparrow$ \\\\ \\midrule ChatGPT & \\cellcolor[HTML]{FEE8DD}55.81 & \\cellcolor[HTML]{FCA77F}{36.44} & \\cellcolor[HTML]{FC8D59}{21.71} & \\cellcolor[HTML]{FC8D59}{79.88} & \\cellcolor[HTML]{FC8D59}{54.54} & \\cellcolor[HTML]{FC8D59}{21.52} & \\cellcolor[HTML]{FEDCCC}17.77 & \\cellcolor[HTML]{FC8D59}{93.69} & \\cellcolor[HTML]{FEDCCC}{29.25} \\\\ Claude & \\cellcolor[HTML]{FCA77F}64.47 & \\cellcolor[HTML]{FEE8DD}{31.23} & \\cellcolor[HTML]{FEE8DD}{18.63} & \\cellcolor[HTML]{FEF7F3}{51.22} & \\cellcolor[HTML]{FEF7F3}40.92 & \\cellcolor[HTML]{FEF7F3}13.77 & \\cellcolor[HTML]{FEF7F3}14.57 & \\cellcolor[HTML]{FEF7F3}66.62 & \\cellcolor[HTML]{FCA77F}{34.34} \\\\ Claude 2 & \\cellcolor[HTML]{FEF7F3}45.20 & \\cellcolor[HTML]{FEF7F3}12.93 & \\cellcolor[HTML]{FEDCCC}19.13 & \\cellcolor[HTML]{FCA77F}78.04 & \\cellcolor[HTML]{FCA77F}54.30 & \\cellcolor[HTML]{FCA77F}21.30 & \\cellcolor[HTML]{FC8D59}21.06 & \\cellcolor[HTML]{FEE8DD}79.97 & \\cellcolor[HTML]{FC8D59}35.83\\\\ Davinci003 & \\cellcolor[HTML]{FC8D59}69.98 & \\cellcolor[HTML]{FC8D59}{37.46} & \\cellcolor[HTML]{FEF7F3}{18.19} & \\cellcolor[HTML]{FEDCCC}{67.07} & \\cellcolor[HTML]{FEE8DD}51.51 & \\cellcolor[HTML]{FEE8DD}17.76 & \\cellcolor[HTML]{FEE8DD}16.68 & \\cellcolor[HTML]{FEDCCC}88.47 & \\cellcolor[HTML]{FEF7F3}{28.29} \\\\ Davinci002 & \\cellcolor[HTML]{FEDCCC}58.85 & \\cellcolor[HTML]{FEDCCC}{35.11} & \\cellcolor[HTML]{FCA77F}{19.15} & \\cellcolor[HTML]{FEE8DD}{56.70} & \\cellcolor[HTML]{FEDCCC}52.11 & \\cellcolor[HTML]{FEDCCC}20.47 & \\cellcolor[HTML]{FCA77F}{18.45} & \\cellcolor[HTML]{FCA77F}89.23 & \\cellcolor[HTML]{FEE8DD}{29.15} \\\\ \\midrule LLaMA 2-Chat~(7B) & 56.12 & 12.62 & \\cellcolor[HTML]{A7CBE2}16.00 & 11.59 & \\cellcolor[HTML]{92BFDB}38.93 & \\cellcolor[HTML]{92BFDB}12.96 & \\cellcolor[HTML]{A7CBE2}11.32 & \\cellcolor[HTML]{92BFDB}72.35 & 23.37 \\\\ Vicuna~(13B) & 62.45 & \\cellcolor[HTML]{A7CBE2}20.49 & \\cellcolor[HTML]{92BFDB}17.87 & \\cellcolor[HTML]{92BFDB}20.73 & \\cellcolor[HTML]{C4DDEC}29.04 & \\cellcolor[HTML]{C6DEED}10.75 & \\cellcolor[HTML]{92BFDB}11.52 & \\cellcolor[HTML]{E5F0F7} 20.69 & \\cellcolor[HTML]{92BFDB}28.76 \\\\ Vicuna~(7B) & \\cellcolor[HTML]{C4DDEC}63.90 & \\cellcolor[HTML]{C6DEED}{19.95} & \\cellcolor[HTML]{C6DEED}{13.59} & \\cellcolor[HTML]{A7CBE2}{17.07} & 28.58 & \\cellcolor[HTML]{C4DDEC}9.17 & 6.64 & 16.96 & \\cellcolor[HTML]{C6DEED}{26.95} \\\\ Alpaca~(7B) & \\cellcolor[HTML]{E5F0F7}63.35 & \\cellcolor[HTML]{92BFDB}{21.52} & {8.74} & {13.41} & 17.14 & 3.24 & 3.00 & \\cellcolor[HTML]{C6DEED}49.75 & \\cellcolor[HTML]{C4DDEC}{26.05}\\\\ ChatGLM~(6B) & 33.34 & \\cellcolor[HTML]{C4DDEC}{16.58} & \\cellcolor[HTML]{C4DDEC}{13.48} & {13.42} & 13.42 & 4.40 & \\cellcolor[HTML]{C4DDEC}9.20 & \\cellcolor[HTML]{A7CBE2}{55.39} & {16.01} \\\\ \\midrule LLaMA 2~(7B) & \\cellcolor[HTML]{C6DEED}66.39 & 11.57 & \\cellcolor[HTML]{E5F0F7}11.57 & \\cellcolor[HTML]{A7CBE2}17.07 & \\cellcolor[HTML]{C6DEED}30.92 & 5.15 & 2.51 & \\cellcolor[HTML]{C4DDEC}24.16 & \\cellcolor[HTML]{A7CBE2}28.06\\\\ LLaMA~(7B) & \\cellcolor[HTML]{92BFDB}67.68 & \\cellcolor[HTML]{E5F0F7}{13.84} & {8.77} & \\cellcolor[HTML]{C4DDEC}{15.24} & \\cellcolor[HTML]{A7CBE2}{34.62} & \\cellcolor[HTML]{E5F0F7}7.92 & \\cellcolor[HTML]{C6DEED}{11.12} & 4.88 & {19.78} \\\\ Falcon~(7B) & \\cellcolor[HTML]{A7CBE2}66.89 & {4.05} & {10.00} & {10.37} & \\cellcolor[HTML]{E5F0F7}28.74 & \\cellcolor[HTML]{A7CBE2}{10.78} & \\cellcolor[HTML]{E5F0F7}8.46 & 4.08 & \\cellcolor[HTML]{E5F0F7}{23.91} \\\\ Pythia~(12B) & 61.19 & {5.43} & {8.87} & \\cellcolor[HTML]{E5F0F7}{14.63} & 15.73 & 1.99 & 4.72 & 11.66 & {20.57} \\\\ Pythia~(7B) & 56.96 & {3.68} & {8.23} & {9.15} & 10.16 & 1.77 & 3.74 & 11.03 & {15.75}\\\\ \\midrule[0.8pt] 2.5{*}{Models} & 3{c}{Knowledge Reasoning} & 2{c}{Symbolic Reasoning} & 2{c}{Mathematical Reasoning} & 2{c}{Interaction with Environment}\\\\ \\cmidrule(r){2-4}\\cmidrule(r){5-6}\\cmidrule(r){7-8}\\cmidrule(r){9-10} & OBQA$\\uparrow$ & HellaSwag$\\uparrow$ & SocialIQA$\\uparrow$ & C-Objects$\\uparrow$ & Penguins$\\uparrow$ & GSM8k$\\uparrow$ & MATH$\\uparrow$ & ALFW$\\uparrow$ & WebShop$\\uparrow$ \\\\ \\midrule ChatGPT & \\cellcolor[HTML]{FCA77F}81.20 & \\cellcolor[HTML]{FCA77F}61.43 & \\cellcolor[HTML]{FC8D59}73.23 & \\cellcolor[HTML]{FEF7F3}53.20 & \\cellcolor[HTML]{FEF7F3}40.27 & \\cellcolor[HTML]{FCA77F}{78.47} & \\cellcolor[HTML]{FC8D59}{33.78} & \\cellcolor[HTML]{FEF7F3}58.96 & \\cellcolor[HTML]{FEDCCC}45.12/15.60 \\\\ Claude & \\cellcolor[HTML]{FC8D59}81.80 & \\cellcolor[HTML]{FEDCCC}54.95 & \\cellcolor[HTML]{FC8D59}73.23 & \\cellcolor[HTML]{FEE8DD}59.95 & \\cellcolor[HTML]{FEE8DD}47.65 & \\cellcolor[HTML]{FEDCCC}70.81 & \\cellcolor[HTML]{FEDCCC}20.18 & \\cellcolor[HTML]{FCA77F}76.87 & \\cellcolor[HTML]{FCA77F}47.72/23.00 \\\\ Claude 2 & \\cellcolor[HTML]{FEE8DD}71.60 & \\cellcolor[HTML]{FEE8DD}50.75 & \\cellcolor[HTML]{FEE8DD}58.34 & \\cellcolor[HTML]{FC8D59}{66.76} & \\cellcolor[HTML]{FC8D59}{74.50} & \\cellcolor[HTML]{FC8D59}{82.87} & \\cellcolor[HTML]{FCA77F}{32.24} & \\cellcolor[HTML]{FC8D59}77.61 & \\cellcolor[HTML]{FEE8DD}34.96/19.20 \\\\ Davinci003 & \\cellcolor[HTML]{FEDCCC}74.40 & \\cellcolor[HTML]{FC8D59}62.65 & \\cellcolor[HTML]{FEDCCC}69.70 & \\cellcolor[HTML]{FCA77F}{64.60} & \\cellcolor[HTML]{FEDCCC}61.07 & \\cellcolor[HTML]{FEE8DD}57.16 & \\cellcolor[HTML]{FEE8DD}17.66 & \\cellcolor[HTML]{FEDCCC}65.67 & \\cellcolor[HTML]{FC8D59}{64.08/32.40} \\\\ Davinci002 & \\cellcolor[HTML]{FEF7F3}69.80 & \\cellcolor[HTML]{FEF7F3}47.81 & \\cellcolor[HTML]{FEF7F3}57.01 & \\cellcolor[HTML]{FEDCCC}62.55 & \\cellcolor[HTML]{FCA77F}{67.11} & \\cellcolor[HTML]{FEF7F3}49.96 & \\cellcolor[HTML]{FEF7F3}14.28 & \\cellcolor[HTML]{FCA77F}{76.87} & \\cellcolor[HTML]{FEF7F3}29.66/15.20 \\\\ \\midrule LLaMA 2-Chat~(7B) & \\cellcolor[HTML]{A7CBE2}45.62 & \\cellcolor[HTML]{C6DEED} 74.01 & \\cellcolor[HTML]{C4DDEC}43.84 & \\cellcolor[HTML]{C4DDEC}43.40 & \\cellcolor[HTML]{A7CBE2}38.93 & 9.63 & 2.22 & \\cellcolor[HTML]{92BFDB}11.19 & \\cellcolor[HTML]{92BFDB}{24.51/5.60} \\\\ Vicuna~(13B) & \\cellcolor[HTML]{E5F0F7}43.65 & \\cellcolor[HTML]{E5F0F7}70.51 & \\cellcolor[HTML]{C6DEED}45.97 & \\cellcolor[HTML]{92BFDB}53.55 & \\cellcolor[HTML]{C6DEED}36.91 & \\cellcolor[HTML]{92BFDB}18.50 & \\cellcolor[HTML]{A7CBE2}3.72 & \\cellcolor[HTML]{A7CBE2}8.96 & \\cellcolor[HTML]{A7CBE2}{22.74/5.00} \\\\ Vicuna~(7B) & \\cellcolor[HTML]{C4DDEC}43.84 & 69.25 & \\cellcolor[HTML]{A7CBE2} 46.27 & \\cellcolor[HTML]{A7CBE2}44.25 & \\cellcolor[HTML]{C4DDEC}36.24 & \\cellcolor[HTML]{A7CBE2}14.03 & \\cellcolor[HTML]{C6DEED}3.54 & 1.49 & \\cellcolor[HTML]{C4DDEC} 6.90/1.40\\\\ Alpaca~(7B) & \\cellcolor[HTML]{92BFDB}47.82 & 69.81 & \\cellcolor[HTML]{92BFDB}47.55 & 39.35 & \\cellcolor[HTML]{92BFDB}40.27 & \\cellcolor[HTML]{E5F0F7}4.93 & \\cellcolor[HTML]{92BFDB}4.16 & 4.48 & 0.00/0.00\\\\ ChatGLM~(6B) & 30.42 & 29.27 & 33.18 & 14.05 & 14.09 & 3.41 & 1.10 & 0.00 & 0.00/0.00 \\\\ \\midrule LLaMA 2~(7B) & \\cellcolor[HTML]{C6DEED}44.81 & \\cellcolor[HTML]{A7CBE2}74.25 &41.72 & \\cellcolor[HTML]{C6DEED}43.95 & \\cellcolor[HTML]{E5F0F7}35.75 & \\cellcolor[HTML]{C6DEED}10.99 & \\cellcolor[HTML]{E5F0F7}2.64 & \\cellcolor[HTML]{A7CBE2}8.96 & 0.00/0.00 \\\\ LLaMA~(7B) & 42.42 & \\cellcolor[HTML]{C4DDEC}73.91 & 41.46 & \\cellcolor[HTML]{E5F0F7}39.95 & 34.90 & \\cellcolor[HTML]{C6DEED}10.99 & \\cellcolor[HTML]{C4DDEC}3.12 & 2.24 & 0.00/0.00 \\\\ Falcon~(7B) & 39.46 & \\cellcolor[HTML]{92BFDB}74.58 & \\cellcolor[HTML]{E5F0F7} 42.53 & 29.80 & 24.16 & 1.67 & 0.94 & \\cellcolor[HTML]{C4DDEC} {7.46} & 0.00/0.00 \\\\ Pythia~(12B) & 37.02 & 65.45 & 41.53 & 32.40 & 26.17 & 2.88 & 1.96 & 5.22 & \\cellcolor[HTML]{E5F0F7}3.68/0.60 \\\\ Pythia~(7B) & 34.88 & 61.82 & 41.01 & 29.05 & 27.52 & 1.82 & 1.46 & \\cellcolor[HTML]{C4DDEC}{7.46} & \\cellcolor[HTML]{C6DEED}{10.75/1.80}\\\\ \\midrule[0.8pt] 2.5{*}{Models} & 5{c}{Human Alignment} & 4{c}{Tool Manipulation} \\\\ \\cmidrule(r){2-6}\\cmidrule(r){7-10} & TfQA$\\uparrow$ & C-Pairs$\\downarrow$ & WinoGender$\\uparrow$ & RTP$\\downarrow$ & HaluEval$\\uparrow$ & HotpotQA$\\uparrow$ & Gorilla-TH$\\uparrow$ & Gorilla-TF$\\uparrow$ & Gorilla-HF$\\uparrow$ \\\\ \\midrule ChatGPT & \\cellcolor[HTML]{FCA77F}{69.16} & \\cellcolor[HTML]{FEE8DD}{18.60} & \\cellcolor[HTML]{FCA77F}{62.50}/\\cellcolor[HTML]{FCA77F}{72.50}/\\cellcolor[HTML]{FCA77F}{79.17} & \\cellcolor[HTML]{FC8D59}{3.07} & \\cellcolor[HTML]{FC8D59}{66.64} & \\cellcolor[HTML]{FEF7F3}{23.80} & \\cellcolor[HTML]{FCA77F}{67.20} & \\cellcolor[HTML]{FC8D59} {44.53} & \\cellcolor[HTML]{FCA77F}{19.36}\\\\ Claude & \\cellcolor[HTML]{FEDCCC}{67.93} & \\cellcolor[HTML]{FEF7F3}{32.73} & \\cellcolor[HTML]{FEE8DD}{71.67}/\\cellcolor[HTML]{FEE8DD}{55.00}/\\cellcolor[HTML]{FEE8DD}{52.50} & \\cellcolor[HTML]{FEDCCC}{3.75} & \\cellcolor[HTML]{FCA77F}{63.75} & \\cellcolor[HTML]{FEDCCC}{33.80} & \\cellcolor[HTML]{FEE8DD}{22.04} & \\cellcolor[HTML]{FEDCCC}{7.74} & \\cellcolor[HTML]{FEDCCC}{7.08}\\\\ Claude 2 & \\cellcolor[HTML]{FC8D59}{71.11} & \\cellcolor[HTML]{FEDCCC}{10.67} & \\cellcolor[HTML]{FEF7F3}{60.00}/60.00/55.83 & \\cellcolor[HTML]{FCA77F}{3.20} & \\cellcolor[HTML]{FEF7F3}50.63 & \\cellcolor[HTML]{FC8D59}{36.4} & \\cellcolor[HTML]{FEDCCC}{61.29} & \\cellcolor[HTML]{FCA77F}{22.19} & \\cellcolor[HTML]{FC8D59}{23.67} \\\\ Davinci003 & \\cellcolor[HTML]{FEE8DD}{60.83} & \\cellcolor[HTML]{FC8D59}{0.99} & \\cellcolor[HTML]{FC8D59}67.50/68.33/{79.17} & \\cellcolor[HTML]{FEE8DD}{8.81} & \\cellcolor[HTML]{FEE8DD}{58.94} & \\cellcolor[HTML]{FCA77F} {34.40} & \\cellcolor[HTML]{FC8D59} {72.58} & \\cellcolor[HTML]{FEE8DD}{3.80} & \\cellcolor[HTML]{FEE8DD}{6.42}\\\\ Davinci002 & \\cellcolor[HTML]{FEF7F3}{53.73} & \\cellcolor[HTML]{FCA77F}{7.56} & \\cellcolor[HTML]{FEDCCC}{72.50}/70.00/64.17 & \\cellcolor[HTML]{FEF7F3}{10.65} & \\cellcolor[HTML]{FEDCCC}{59.67} & \\cellcolor[HTML]{FEE8DD}{26.00} & \\cellcolor[HTML]{FEF7F3}{2.69} & \\cellcolor[HTML]{FEF7F3}{1.02} & \\cellcolor[HTML]{FEF7F3}{1.00}\\\\ \\midrule LLaMA 2-Chat~(7B) & \\cellcolor[HTML]{92BFDB}{69.77} & \\cellcolor[HTML]{A7CBE2}{48.54} & 47.50/46.67/46.67 & \\cellcolor[HTML]{A7CBE2}{4.61} & \\cellcolor[HTML]{C6DEED}43.82 & \\cellcolor[HTML]{C4DDEC}{4.40} & 0.00 & 0.00 & \\cellcolor[HTML]{C6DEED}{0.22} \\\\ Vicuna~(13B) & \\cellcolor[HTML]{C6DEED}{62.30} & \\cellcolor[HTML]{92BFDB}{45.95} & \\cellcolor[HTML]{C6DEED}{50.83}/50.83/52.50 & \\cellcolor[HTML]{E5F0F7}{5.00} & \\cellcolor[HTML]{92BFDB}49.01 & \\cellcolor[HTML]{A7CBE2}{11.20} & 0.00 & \\cellcolor[HTML]{92BFDB}{0.44} & \\cellcolor[HTML]{92BFDB}{0.89} \\\\ Vicuna~(7B) & \\cellcolor[HTML]{C4DDEC}{57.77} & {67.44} & 49.17/49.17/49.17 & \\cellcolor[HTML]{C6DEED}{4.70} & \\cellcolor[HTML]{C4DDEC}{43.44} & \\cellcolor[HTML]{C6DEED}{6.20} & {0.00} & {0.00} & \\cellcolor[HTML]{A7CBE2}{0.33}\\\\ Alpaca~(7B) & {46.14} & {65.45} & \\cellcolor[HTML]{A7CBE2}{53.33}/51.67/{53.33} & \\cellcolor[HTML]{C4DDEC}{4.78} & \\cellcolor[HTML]{A7CBE2}{44.16} & \\cellcolor[HTML]{92BFDB}{11.60} & {0.00} & {0.00} & \\cellcolor[HTML]{C4DDEC}{0.11}\\\\ ChatGLM~(6B) & \\cellcolor[HTML]{A7CBE2}{63.53} & \\cellcolor[HTML]{C6DEED}{50.53} & 47.50/47.50/46.67 & \\cellcolor[HTML]{92BFDB}{2.89} & {41.82} & \\cellcolor[HTML]{E5F0F7}{4.00} & {0.00} & {0.00} & {0.00}\\\\ \\midrule LLaMA 2~(7B) & 50.06 & \\cellcolor[HTML]{C4DDEC}{51.39} & 48.83/48.83/50.83 & 6.17 & \\cellcolor[HTML]{E5F0F7}42.23 & 3.80 & 0.00 & 0.00 & \\cellcolor[HTML]{C4DDEC}{0.11} \\\\ LLaMA~(7B) & {47.86} & {67.84} & \\cellcolor[HTML]{92BFDB}54.17/{52.50}/51.67 & {5.94} & {14.18} & {1.60} & {0.00} & {0.00} & \\cellcolor[HTML]{C4DDEC}{0.11}\\\\ Falcon~(7B) & {53.24} & {68.04} & \\cellcolor[HTML]{E5F0F7}50.00/50.83/50.00 & {6.71} & {37.41} & {1.00} & {0.00} & {0.00} & {0.00}\\\\ Pythia~(12B) & \\cellcolor[HTML]{E5F0F7}{54.47} & {65.78} & 49.17/48.33/49.17 & {6.59} & {27.09} & {0.40} & {0.00} & {0.00} & {0.00}\\\\ Pythia~(7B) & {50.92} & \\cellcolor[HTML]{E5F0F7}{64.79} & \\cellcolor[HTML]{C4DDEC}51.67/49.17/50.00 & {13.02} & {25.84} & {0.20} & {0.00} & {0.00} & {0.00}\\\\ \\bottomruletabular } table* table*[!h] \\centering Prompt examples and their performance of ChatGPT on representative tasks. For most tasks, we compare the performance for \\emph{simple and complex prompts. We also present the reported performance of supervised methods. ``LG'', ``KU'', ``CR'', ``SDG'', ``IR'' are short for ``language generation'', ``knowledge utilization'', ``complex reasoning'', ``structured data generation'', ``information retrieval''. ``-'' means there is no reported supervised result previously on this dataset.} \\scriptsize % tabular{cp{0.10\\textwidth}c p{0.55\\textwidth} rr} \\toprule 2{c}{Tasks} & Datasets & \\makecell[c]{Instructions} & ChatGPT & Supervised \\\\ \\midrule 13.5{*}{LG} & 4.5{*}{Translation} & 4.5{*}{WMT} & I want you to act as a translator. Please translate the English sentence into Czech. & 20.66 & 4.5{*}{41.40~Zan-WMT-2022-Vega-MT}\\\\ 4-5 & & & I want you to act as a translator. Translate the given English sentence into Czech, and ensure that the translated sentence is semantically consistent with the given sentence. $\\backslash$n Sentence: \\{source sentence\\ $\\backslash$n Translation:} & 21.12 \\\\ 2-6 & 4.5{*}{Summarization} & 4.5{*}{XSum} & Please generate a one-sentence summary for the given document. & 21.71 & 4.5{*}{42.08~Zhao-arxiv-2022-Calibrating} \\\\ 4-5 & & & \\{document\\ Try your best to summarize the main content of the given document. And generate a short summary in 1 sentence for it.$\\backslash$n Summary: } & 23.01 &\\\\ \\midrule 13{*}{KU} & 2.5{*}{Closed-Book QA} & 2.5{*}{ARC} & Choose your answer to the question. \\{query\\ \\{options\\}} & 85.19 & 2.5{*}{92.00~Khashabi-EMNLP-2020-UnifiedQA} \\\\ 4-5 & & & Choose a correct answer according to the given question, and output the corresponding id, do not answer other content except the answer id. & 85.86 \\\\ 2-6 & 6.5{*}{Open-Book QA} & 6.5{*}{OBQA} & Choose your answer to the question: \\{question\\ \\{choices\\}. You must only output A, B, C, or D without any extra explanation. The answer is} & 81.20 & 6.5{*}{87.20~Khashabi-EMNLP-2020-UnifiedQA} \\\\ 4-5 & & & Following is a question that requires multi-step reasoning, use of additional common and commonsense knowledge, and rich text comprehension. Choose your answer to the question: $\\backslash$n Question: Frilled sharks and angler fish live far beneath the surface of the ocean, which is why they are known as $\\backslash$n Choices: $\\backslash$n A. Deep sea animals $\\backslash$n B. fish $\\backslash$n C. Long Sea Fish $\\backslash$n D. Far Sea Animals $\\backslash$n You must only output A, B, C, or D without any extra explanation. The answer is & 82.20 \\\\ 2-6 & 2.5{*}{Fact Extraction} & 2.5{*}{WikiF} & Complete the sentence with one or a few words. & 29.25 & 2.5{*}{34.20~Liang-arxiv-2022-Holistic} \\\\ 4-5 & & & Complete the given sentence with one entity name in Wikipedia (MUST be a noun) as short as possible, and ensure that the completed sentence conforms to the facts. & 31.21\\\\ \\midrule 13{*}{CR} & 2.5{*}{Symbolic Reasoning} & 2.5{*}{C-Objects} & Problem: \\{problem\\$\\backslash$n Answer:} & 53.20 & 2.5{*}{---} \\\\ 4-5 & & & You are an expert in reasoning problem. Here are some examples about symbolic reasoning. You can use the knowledge in examples and solve the last problem. You should follow the examples and generate the final answer without external solution or words. & 66.75\\\\ 2-6 2-6 & 4.5{*}{Math Word Problems} & 4.5{*}{GSM8k} & Problem: \\{problem\\$\\backslash$n Solution: Let's think step by step. } & 78.47 & 4.5{*}{63.20~Zhu-arxiv-2022-Solving} \\\\ 4-5 & & & Let's use python to solve math problems. Here are three examples how to do it,$\\backslash$n Q: Olivia has \\$23. She bought five bagels for \\$3 each. How much money does she have left?$\\backslash$n```def solution():$\\backslash$n ~~~~\"\"\"Olivia has \\$23. She bought five bagels for \\$3 each. How much money does she have left?\"\"\"$\\backslash$n ~~~~money\\_initial = 23$\\backslash$n ~~~~bagels = 5$\\backslash$n ~~~~bagel\\_cost = 3$\\backslash$n ~~~~money\\_spent = bagels * bagel\\_cost$\\backslash$n ~~~~money\\_left = money\\_initial - money\\_spent$\\backslash$n ~~~~result = money\\_left$\\backslash$n ~~~~return result```$\\backslash$n ...... $\\backslash$n How about this question?$\\backslash$n Q: & 79.30\\\\ \\midrule 7{*}{SDG} & Code Synthesis & HumanEval & I want you act as a code completer. Given a code snippet, your objective is to complete the code and ensure that it can achieve the described functionality. & 79.88 & 48.20~Nguyen-arxiv-2023-Meet \\\\ 2-6 & \\makecell[l]{Text-to-SQL} & Spider & \\#\\#\\# Complete sqlite SQL query only and with no explanation.$\\backslash$n \\#$\\backslash$n\\#\\#\\# Sqlite SQL tables, with their properties: $\\backslash$n\\#$\\backslash$n\\{table\\$\\backslash$n\\# \\{foreign\\_key\\}$\\backslash$n\\#$\\backslash$n\\#\\#\\# \\{question\\}$\\backslash$n SELECT} & 70.10 & 84.10~Li-arxiv-2023-RESDSQL \\\\ \\midrule 15{*}{IR} & Recommendation & MovieLens & I've watched the following movies in the past in order: $\\backslash$n \\{user\\_his\\_text\\ $\\backslash$n$\\backslash$n Now there are \\{recall\\_budget\\} candidate movies that I can watch next: $\\backslash$n \\{candidate\\_text\\_order\\} $\\backslash$n Please rank these \\{recall\\_budget\\} movies by measuring the possibilities that I would like to watch next most, according to my watching history. Please think step by step. $\\backslash$n Note that my most recently watched movie is \\{recent\\_item\\}. Please show me your ranking results with order numbers. Split your output with line break. You MUST rank the given candidate movies. You can not generate movies that are not in the given candidate list.} & 48.80 & 76.25~Kang-ICDM-2018-Self \\\\ 2-6 & Conversational \\quad Recommendation & ReDial & Recommend 10 items that are consistent with user preference. The recommendation list can contain items that the dialog mentioned before. The format of the recommendation list is: no. title (year). Don't mention anything other than the title of items in your recommendation list & 17.20 & 25.60~Yang-NAACL-2022-Improving \\\\ \\bottomruletabular table*",
      "origin_cites_number": 41
    },
    {
      "section_title": "Empirical Evaluation",
      "level": "2",
      "content": "The above evaluation benchmarks and approaches are mainly employed to evaluate the overall abilities of LLMs. In this part, we conduct a fine-grained evaluation of the abilities discussed in Section~sec:basicability and Section~sec:superior. For each kind of ability, we select representative tasks and datasets for conducting evaluation experiments to examine the corresponding performance of LLMs. %",
      "origin_cites_number": 0
    },
    {
      "section_title": "Experimental Settings",
      "level": "3",
      "content": "In this part, we introduce the experimental settings for our evaluation. Evaluation Models. To conduct the evaluation, we consider representative LLMs from open-source models to closed-source API-accessing models as follows: \\textbullet~Open-source models. Existing open-source models can be categorized into base models and instruction-tuned models. Base models are only pre-trained on a large general-purpose corpus with the language modeling objective, but without further supervised fine-tuning. In our evaluation, we select four representative base models including LLaMA (7B)~Touvron-arxiv-2023-LLaMA, LLaMA 2 (7B)~Touvron-2023-llama2-arxiv, Pythia (7B and 12B)~Biderman-arxiv-2023-Pythia, and Falcon (7B)~Ebtesam-arxiv-2023-FalconExperiments with larger models are still in schedule due to the limit of computational resources. . Instruction-tuned models are those fine-tuned using instructions (\\ie task datasets, daily chat, or synthetic instructions). In our experiments, we select four representative instruction-tuned models including Vicuna (7B and 13B)~vicuna2023, Alpaca (7B)~alpaca, and ChatGLM (6B)~Zeng-arxiv-2022-GLM. {In addition, we also include LLaMA 2-Chat (7B)~Touvron-2023-llama2-arxiv for comparison, and it is a representative model that has been aligned with human via instruction tuning and RLHF, based on LLaMA 2 (7B).} \\textbullet~Closed-source models. In addition to the open-source models, there are also closed-source models that can only be accessed via APIs, which have gained much attention from both developers and researchers. Here, we select four representative closed-source models including text-davinci-002/003 (short as Davinci002/003), ChatGPT, Claude, % {and Claude 2, where the first three models are developed by OpenAI and the other two are developed by Anthropic.} Tasks and Datasets. Next, we set up the evaluation tasks and datasets for the abilities discussed in Section~sec:basicability and Section~sec:superior. % {We mainly evaluate the zero-shot performance of LLMs on these datasets. For more complex tasks that are hard to be solved in the zero-shot manner (\\eg mathematical reasoning and tool manipulation), we mainly report the 3-shot performance, considering the context length limit of open-source models. } \\textbullet~Language generation. As discussed before, for language generation, we consider evaluating three kinds of tasks, \\ie language modeling, conditional text generation, and code synthesis. Specially, we select four commonly-used datasets, namely LAMBADA~Paperno-ACL-2016-LAMBADA (language modeling), WMT'22~Kocmi-WMT-2022-Findings (machine translation), XSum~Naryan-EMNLP-2018-XSUM (text summarization), and HumanEval~Chen-arxiv-2021-evaluating (code synthesis) for evaluation. { In WMT'22, we construct a new evaluation set by selecting 1000 examples for each language pair from the original large-scale test set to examine the average performance of LLMs in machine translation.} We evaluate the zero-shot performance of LLMs on these datasets, and compute the accuracy of predicting words for LAMBADA, BLEU-4 for WMT'22, ROUGE-L for XSum, and pass@$10$ for HumanEval. \\textbullet~Knowledge utilization. To evaluate the ability of knowledge utilization, % we select four question answering datasets (\\ie TriviaQA~Joshi-ACL-2017-TriviaQA, Natural Questions~Kwiatkowski-ACL-2019-Natural, Web Questions~Berant-EMNLP-2013-Semantic, and ARC~Clark-arxiv-2018-Think), and a fact extraction dataset, WikiFact~Goodrich-KDD-2019-Assessing. We also report the zero-shot performance of LLMs on these datasets, {and compute accuracy for ARC and exact match for other datasets.} \\textbullet~Complex reasoning. % For complex reasoning, we evaluate the comparison models on OpenbookQA~Mihaylov-EMNLP-2018-Can, HellaSwag~Zellers-acl-2019-HellaSwag, and SocialIQA~Sap-arxiv-2019-SocialIQA for knowledge reasoning; Colored Objects~Srivastava-arxiv-2022-Beyond and Penguins in the Table~Srivastava-arxiv-2022-Beyond for symbolic reasoning; GSM8k~Cobbe-arxiv-2021-Training and MATH~Hendrycks-ICLR-2021-Measuring for mathematical reasoning. We compute the accuracy for OpenbookQA, HellaSwag, and SocialIQA; solve rate for Colored Objects and Penguins in the Table; and accuracy for GSM8k and MATH. {For knowledge reasoning tasks, we evaluate the zero-shot performance, since they are all QA tasks that can be solved in a zero-shot setting. For complex symbolic reasoning and mathematical reasoning tasks, we leverage 3-shot in-context exemplars to better elicit LLMs to accomplish them. Following existing work~Gao-arxiv-2022-PAL,Wei-arxiv-2022-chain, we also utilize the chain-of-thought prompting strategy for better solving the mathematical reasoning tasks.} \\textbullet~Human alignment. For human alignment, % we select TruthfulQA~Lin-ACL-2022-TruthfulQA to measure whether a LLM is truthful in generating answers to questions, CrowS-Pairs~Nangia-EMNLP-2020-CrowS and WinoGender~Rudinger-NAACL-2018-Gender to assess the stereotypes in LLMs, RealToxityPrompts~Gehman-2023-arxiv-RealToxicityPrompts to evaluate the extent to which LLMs generate toxic language, and HaluEval~Li-arxiv-2023-HaluEval to test the ability of LLMs to recognize hallucination. As the test set of Real-Toxicity-Prompts is too large, we randomly sample 10000 examples from it for evaluation. We follow LLaMA~Touvron-arxiv-2023-LLaMA to report the zero-shot performance, and compute the accuracy of identifying a claim as true for TruthfulQA, accuracy of recognizing biased sentences (high perplexity) for CrowS-Pairs, coreference resolution accuracy (he/she/they) for WinoGender, toxicity score for RealToxityPrompts, and average accuracy of recognizing hallucinations for HaluEval. {For TruthfulQA, we follow existing work~Touvron-arxiv-2023-LLaMA that utilizes text-davinci-003 to replace humans for scoring.} For Crows-Pairs and WinoGender, we follow the experimental settings of LLaMA~Touvron-arxiv-2023-LLaMA to compute the perplexity and coreference resolution score. For RealToxityPrompts, we utilize the {Perspective-API\\url{https://perspectiveapi.com/}} for toxicity evaluation. \\textbullet~Interaction with environment. % To test this ability, we select ALFWorld~Shridhar-2021-iclr-ALFWorld and WebShop~Yao-2022-nips-WebShop for evaluation, which simulate real-world scenarios such as household and e-commerce environments. We follow the setting of ReAct~Yao-2022-arXiv-react that evaluate the 1-shot and 2-shot performance of LLMs on WebShop and ALFWorld respectively, and compute success rate for ALFWorld and average score/success rate for WebShop. Further, we also follow ReAct~Yao-2022-arXiv-react to reduce the length of the input prompt and utilize line break as the EOS token. \\textbullet~Tool manipulation. % For tool manipulation, we consider two kinds of tools including search engine and model interfaces. Therefore, we adopt two tool manipulation benchmarks, \\ie HotpotQA~yang-2018-acl-HotpotQA and Gorilla~Patil-2023-arxiv-Gorilla. HotpotQA requires LLMs to use search engine to retrieve documents from the web, and Gorilla to invoke model APIs from three hubs of TorchHub, TensorHub and HuggingFace. We compute exact match for HotpotQA and accuracy for Gorilla. For HotpotQA, we follow ReAct~Yao-2022-arXiv-react to report the 3-shot performance. For Gorilla, we follow the code released by its paper~Patil-2023-arxiv-Gorilla, and evaluate the zero-shot performance. Implementation Details. For each task and dataset, we evaluate the compared LLMs using the same % prompts and results parsing method provided by existing work {(\\ie TruthfulQA, HotPotQA, Gorilla, HaluEval)} or designed according to our empirical experience {(\\ie TriviaQA, Natural Questions, Web Questions, ARC, WikiFact, GSM8k, MATH, C-Objects, Penguins, LAMBADA, WMT'22, XSum, HumanEval, CrowS-Pairs, WinoGender, RealToxityPrompt).} Specifically, all the experiments about closed-source models are based on invoking their official APIs, while for open-source models, we utilize their publicly available code and model parameters, and perform the inference on 8 A800-80G GPUs. For TriviaQA, OpenbookQA, HellaSwag, and SocialIQA, we experiment on the development set since the test set is not publicly released. While for other datasets, we experiment on the test set. % To reproduce our experiments, we also publicly release our experimental code and data in https://github.com/RUCAIBox/LLMSurvey/tree/main/Experiments.",
      "origin_cites_number": 41
    },
    {
      "section_title": "Results Analysis and Findings",
      "level": "3",
      "content": "We report the experimental results in Table~tab-experimental-res, and analyze the results in the following. Analysis of Closed-Source Models. We summarize our analysis and findings of {the four} closed-source models (\\ie ChatGPT, Claude, Davinci003 and Davinci002) as follows: $\\bullet$ % {These five closed-source models achieve promising results as general-purpose task solvers, in which ChatGPT mostly performs the best. ChatGPT, Claude, Claude 2, Davinci003 and Davinci002 perform well in most of tasks, including complex tasks (\\eg GSM8k), which have shown great potential to be general-purpose task solvers. % Among them, ChatGPT exhibits a more superior model capacity on the evaluation tasks, winning the most across all tasks. In some evaluation tasks, the performance gap between ChatGPT and other closed-source models is very large, especially for complex tasks \\eg 78.47 (ChatGPT) v.s. 49.96 (Davinci002) on GSM8k, and 79.88 (ChatGPT) v.s. 51.22 (Claude) on HumanEval. } $\\bullet$ {Claude 2, ChatGPT and Davinci003 perform better on interaction with environment and tool manipulation tasks.} On the two evaluation tasks, Claude 2, ChatGPT and Davinci003, perform better than other models by a large margin, \\eg 36.40 (Claude 2) v.s. 26.00 (Davinci002) on HotpotQA, 44.53 (ChatGPT) v.s. 7.74 (Claude) on Gorilla-TF, and 72.58 (Davinci003) v.s. 22.04 (Claude) on Gorilla-TH. {A possible reason is that these three models have been specially optimized towards these advanced abilities, \\eg supporting the use of external plugins. } \\textcolor{blue{ $\\bullet$ Davinci002 performs better on symbolic reasoning tasks. {Davinci002 performs well on symbolic reasoning tasks, \\eg outperforming ChatGPT on C-Objects and Penguins. Compared with Davinci002, the performance degradation in ChatGPT might be caused by the fact that RLHF (without that in Davinci002) may affect the general abilities (\\eg symbolic reasoning) of LLMs to some extent, (\\eg alignment tax~Ouyang-arxiv-2022-Training), as it mainly focuses on aligning with human expectations (\\eg helpfulness, honesty and harmfulness).} }} $\\bullet$ All the comparison models perform not well on very difficult reasoning tasks. On MATH and HotpotQA, all models (including ChatGPT) perform not well. The two tasks are very difficult to solve, requiring accurate understanding of complex mathematical knowledge and performing multi-hop reasoning across documents, respectively. {Further, these models also have a relatively weak performance on machine translation task (WMT). A possible reason is that WMT also contains many evaluation examples in minor languages, which might not be well covered in the pre-training data of these LLMs. } Analysis of Open-Source Models. Next, we continue to show our analysis and findings about eight open-source models (\\ie LLaMA 2-Chat, Vicuna, Alpaca, ChatGLM, LLaMA 2, LLaMA, Pythia and Falcon) as follows: $\\bullet$ Instruction-tuned models mostly perform better than the base models. {Among all the compared open-source methods, the instruction-tuned models (\\ie LLaMA 2-Chat, Vicuna, Alpaca and ChatGLM) mostly perform better than non-instruction-tuned models (\\ie LLaMA 2, LLaMA, Pythia and Falcon).} It indicates that instruction tuning is generally capable of improving the few-shot or zero-shot ability of LLMs in solving various tasks. However, after instruction tuning, Vicuna (7B) and Alpaca (7B) suffer from performance degradations on LAMBADA, a language modeling task. {The reason may be that the instruction data mainly focuses on enabling LLMs to follow human instructions, which is not always useful for the general language generation task. } $\\bullet$ These small-sized open-source models perform not well on {mathematical reasoning, interaction with environment, and tool manipulation tasks.} On the tasks of mathematical reasoning, interaction with environment and tool manipulation, all these evaluated open-source models perform not well, including instruction-tuned ones. % A possible reason is that the instruction data for fine-tuning these models is not specifically designed for these tasks. In addition, these closed-source models may have limited model capacities due to small model sizes. $\\bullet$ The top-performing model varies on different human alignment tasks. For different human alignment tasks, we can see that these models achieve inconsistent performance rankings. For example, LLaMA 2-Chat (7B) performs the best among the compared open-source models on TruthfulQA, while Vicuna (13B) performs the best on CrowS-Pairs. A possible reason is that these tasks are designed with specific purposes for evaluating different aspects of human alignment, and these models exhibit varied performance on different tasks, even for the variants of the same model (\\eg Pythia (7B) and Pythia (12B)). More experiments and analysis on human alignment evaluation are needed to reveal more detailed findings. $\\bullet$ \\emph{As a more recently release model, Falcon (7B) achieves a decent performance, especially on language generation tasks. For language generation tasks, Falcon (7B) mostly performs better than other base models, \\eg 10.00 (Falcon (7B)) v.s. 8.77 (LLaMA (7B)) in LAMABDA. For other tasks (\\eg knowledge utilization and complex reasoning), Falcon (7B) can also achieve comparable performance as LLaMA (7B). It has adopted a careful data pre-processing pipeline to filter low-quality and duplicate content from the web data, which mainly contributes to the excellent performance. } $\\bullet$ As a more recently released model, LLaMA 2 (7B) overall achieves a good performance, especially on complex reasoning tasks. {For complex reasoning tasks, LLaMA 2 (7B) mostly performs better than other base models, \\eg 43.95 (LLaMA 2 (7B)) v.s. 29.80 (Falcon (7B)) in C-Objects. For other tasks (\\eg language generation and knowledge utilization), LLaMA 2 (7B) can also achieve comparable performance as the best-performing base models. It has used more data for pre-training (\\ie about 2 trillion tokens), which mainly contributes to the excellent performance. Furthermore, it also conducts a more robust data cleaning process.} $\\bullet$ Scaling the open-source modes can improve the performance consistently. {By comparing the performance of Vicuna (7B) and Vicuna (13B), Pythia (7B) and Pythia (13B), we can see that the models with larger scales mostly perform better than smaller ones on these evaluation tasks, indicating the effectiveness of scaling up the model size. Across different tasks, scaling model is more beneficial for more complex tasks (\\eg symbolic and mathematical reasoning), where the larger models mostly outperform smaller ones in a large margin. } The readers should be note that these findings about open-source language models are limited to the model sizes. We will continually update this part by including the results of larger versions of these models, and also call for the support of computational resources for more experiments.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Applications",
      "level": "1",
      "content": "figure*[h] \\centering \\includegraphics[width=1\\textwidth]{images/application.pdf} The applications of LLMs in representative research directions and downstream domains. figure* {In this section, we briefly review the recent progress on the applications of LLMs in two aspects, namely the impact to research community and representative domains.} Figure~fig:application shows a content organization of this sectionNote that we don't aim to cover all the related research directions or domains, but instead demonstrating the use or impact of LLMs via these selected examples. .",
      "origin_cites_number": 0
    },
    {
      "section_title": "LLM for Research Community",
      "level": "2",
      "content": "As LLMs have revolutionized the way how we develop AI algorithms, it poses significant impact on the research community. In this part, we briefly review the advances that led by LLMs for several representative research directions.",
      "origin_cites_number": 0
    },
    {
      "section_title": "LLM for Classic NLP Tasks",
      "level": "3",
      "content": "As pre-trained language models (\\eg BERT) have originated in the field of NLP, the technical advances of language models has an important impact on the research of NLP. In this part, we discuss the application of LLMs on five kinds of classic NLP tasks, including word-level, sentence-level, sequence tagging, relation extraction, and text generation tasks, which had been the foundation of many existing NLP systems and applications. Note that we do not intend to comprehensively cover all NLP tasks, but instead try to analyze the impact of LLMs for fundamental NLP research through the basic tasks. We also omit the discussion of several tasks (\\eg language modeling) that have been discussed early in this survey. Word/Sentence-level Tasks. As long-standing NLP tasks, word-level (\\eg word clustering~Martin-Speech-1998-clustering and sense disambiguation~Navigli-ACM-2009-disambiguation) and sentence-level tasks (sentence matching~Gomaa-International-2013-similarity and sentiment classification~Minaee-ACM-2021-classification) have been widely studied in the literature and applied in real-world platforms. To solve these tasks, the key is to accurately understand the semantic information about the words or sentences. As rich high-quality labeled data about these tasks has been accumulated so far, existing work~qiu-CoRR-2020-PTM,Devlin-NAACL-2019-BERT finds that small language models can achieve very good performance by fine-tuning on it. Recent studies~Brown-NeurIPS-2020-Language,Alex-NIPS-2021-RAFT have also tested the performance of LLMs on these tasks, showing that LLMs can also perform well via in-context learning (with very few examples). Whereas, {as small models can be specially optimized on these tasks to learn the specific task requirement and domain knowledge}, full-data fine-tuned small models can mostly outperform LLMs using in-context learning on several classic tasks~Qin-arxiv-2023-Is,Chen-arxiv-2023-Robust, \\eg semantic matching and sentiment analysis. Sequence Tagging. The sequence tagging tasks, \\eg named entity recognition~(NER)~Nadeau-Lingvisticae-2007-NER and part-of-speech~(POS) tagging~Ratnaparkhi-EMNLP-1996-maximum, are also fundamental tasks. Typically, such tasks require assigning each token in the input sequence a proper semantic category label, \\eg the classic B-I-O (Beginning, Inside and Outside) tagging scheme for NER tasks. In the era of deep learning, early efforts~Yadav-COLING-2018-survey,Souza-arxiv-2019-portuguese mainly integrate the learned sequence representations (\\eg using CNN, LSTM, and BERT) into the classic conditional random field model~(CRF), which performs the tagging task based on structural prediction. Recently, researchers have tested the performance of LLMs in sequence tagging tasks, but observed that LLMs still face challenges in solving them using in-context learning~Qin-arxiv-2023-Is, especially for special categories with ambiguous or rare names, \\eg the ``MISC'' (miscellaneous entity) and ``ORG'' (organization) classes. A possible reason is that LLMs may misunderstand the meanings of these classes in the human-annotated dataset, making it difficult to accurately understand their semantics according to the instruction and limited examples in the context. Information Extraction. The information extraction task focuses on automatically extracting useful structured information from unstructured text data, such as relation extraction~Pawar-arxiv-2017-relation and event extraction~walker-ldc-2006-ace, which is also a crucial task relating to many NLP applications. Typically, previous studies formulate this task as a text classification task or a sequential labeling task. {As information extraction often needs to accurately understand and process complex semantic relations (multiple relations within one sentence), in-context learning with LLMs typically underperform state-of-the-art full-data fine-tuning methods~gao-arxiv-2023-exploring,ma-arxiv-2023-Large.} Whereas, it is shown that enabling collaboration between LLMs and small models can further boost the performance of specific tasks~tang-arxiv-2023-does,ma-arxiv-2023-Large. In addition, a recent study~wei-arxiv-2023-zero also reveals that LLMs can achieve competitive zero-shot performance for information extraction with a two-stage workflow, making this approach attractive in future applications. Text Generation. {Text generation tasks, \\eg machine translation~Bahdanau-ICLR-2015-Neural and automatic summarization~Nallapati-acl-2016-Abstractive, are long-standing NLP tasks that have been widely studied, and there have been a number of deployed products and systems based on fine-tuned small models~Wu-arxiv-2016-Google,vaswani-CAMT-2018-tensor2tensor. Since the pre-training of LLMs is established on text prediction, they exhibit strong language generation abilities as commercial products~Jiao-arxiv-2023-mt and humans~Zhang-2023-arxiv-Benchmarking, with the help of proper prompts~zhang-arxiv-2023-prompting,ghazvininejad-arxiv-2023-dictionary. Additionally, LLMs are flexible to effectively handle special requirement in real-world application scenarios, \\eg document-level translation~wang-arxiv-2023-document, and also enable natural language interaction with users to further improve the generation quality~jiao-arxiv-2023-parrot. Despite the above success, recent work also reveals that LLMs are hard to well address the generation tasks about low-resource languages and domains, \\eg Marathi-to-English translation~yang-arxiv-2023-bigtrans, due to their unbalanced training data across different languages. } Summary. {Based on the above discussion, we summarize the suggestions, and future direction about the use of LLMs in classic NLP tasks as follows:} $\\bullet$ Suggestions: LLMs and small models have their own merits in different aspects: LLMs are can provide unified solutions to various NLP tasks and achieve competitive performance (especially in the zero/few-shot setting), while small models are economical to develop and can be specially tuned according to target tasks, which can achieve good performance with sufficient high-quality labeled data~Qin-arxiv-2023-Is,Chen-arxiv-2023-Robust,Kocon-arxiv-2023-ChatGPT,Zhong-arxiv-2023-Can. In applications, one can make suitable choices based on the actual needs, comprehensively considering flexibility, data availability, training compute, and efficiency. $\\bullet$ Future direction: {% {Despite the excellent general capacities, LLMs still cannot effectively process the NLP tasks in low-resource domains, \\eg minor language translation. To tackle such tasks, it needs to develop effective approaches to injecting necessary task information or domain-specific knowledge into LLMs, either through fine-tuning or prompting. % In addition, it is still challenging for LLMs to handle complex semantic relations in classic NLP tasks (\\eg nested entity extraction), which is worth more exploration from the underlying working mechanism of LLMs. } It is also promising to combine LLMs and fine-tuned small language models for complementing with each other in solving complex cases of classic NLP tasks~Cheng-arxiv-2023-UPRISE. Another promising direction is to conduct human-machine collaborative research (\\eg conversational translation~jiao-arxiv-2023-parrot) on NLP tasks, since LLMs can effectively understand human instructions and make meaningful responses. }",
      "origin_cites_number": 28
    },
    {
      "section_title": "LLM for Information Retrieval",
      "level": "3",
      "content": "{ The goal of information retrieval~(IR) systems is to assist users in discovering ideal information resources~(typically documents) and mitigating the information overload issue. Typically, contemporary IR systems adopt a retrieve-then-rerank pipeline framework~Zhao-arxiv-2022-Dense. Within this framework, the retriever initially retrieves relevant information from a large-scale corpus, and the reranker subsequently performs multi-stage ranking procedure to acquire the most relevant information~Ren-EMNLP-2021-rocketqav2. Since the advent of LLMs has significant impact on the way of information access, we discuss how it advances the development of IR from two main aspects, namely LLMs as IR models and LLM-enhanced IR models. } LLMs as IR Models. {Existing IR models can be overall categorized into sparse models (relying on term-based lexical similarity) and dense models (relying on embedding based semantic similarity)~Zhao-arxiv-2022-Dense. Specially, dense models are mainly implemented by fine-tuned PLMs (\\eg BERT). Compared to PLMs, LLMs have more strong model capacities in capturing text semantics, thus having the potential to improve existing dense IR models. However, due to the high overhead of LLMs, % the majority of studies concentrate on employing LLMs as rerankers, aiming to refine the ranking of retrieved candidates. % To achieve this, recent efforts often formulate special instructions that enable LLMs to perform reranking on a small set of provided candidate documents. Typically, such an approach does not necessitate model training, and achieve promising results compared with well-trained reranking methods~sun-arxiv-2023-chatgpt, qin-arxiv-2023-large. Specially, the LLM-based reranking approach can be implemented in different ways by zero-shot or few-shot instruction, including pointwise (estimating the relevance scores for query-document pairs)~Cho-ACL-2023-Discrete, pairwise (determining the relevance order of two documents)~qin-arxiv-2023-large, or listwise ranking (sorting a subset of candidate documents)~tang-arxiv-2023-found. {The essence of these methods lies in the special design of instructions for text reranking, such as sliding window strategy for document lists~sun-arxiv-2023-chatgpt, ma-arxiv-2023-zero, setwise selection prompting~zhuang-arxiv-2023-setwise, fine-grained relevance labels incorporation~zhuang-arxiv-2023-beyond, and pairwise comparison prompting~qin-arxiv-2023-large.} In addition, recent efforts employ LLMs to generate intermediate texts (\\eg URLs) as retrieval results using few-shot demonstrations~ziems-arxiv-2023-large. To further enhance the model performance, LLMs can be specially fine-tuned as backbones for reranking~Ma-arxiv-2023-fine, pradeep-arxiv-2023-rankvicuna or retrieval~(including dense retrieval~Zhao-arxiv-2022-Dense and model-based retrieval~Tay-NIPS-2022-transformer, Ren-ACL-2023-tome), similar to the fine-tuning process for traditional PLM-based IR models~Ma-arxiv-2023-fine. However, fine-tuning LLMs as IR models entails considerable expenses given the huge parameter scale of LLMs. } LLM-Enhanced IR Models. {As another major research direction, LLMs can be employed to improve existing IR models (\\eg small models). A common challenge faced by existing IR models is the lack of relevant judgment annotation~Qu-NAACL-2021-rocketqa, Ren-ACL-2021-PAIR. To tackle this problem, LLMs can be instructed to annotate positive or negative documents for a given query~peng-arxiv-2023-soft, or to generate corresponding queries based on a set of documents in the corpus by referring to a few demonstrations~Dai-ICLR-2023-promptagator, askari-arxiv-2023-generating. In addition to training data augmentation, LLM has the potential to improve existing IR models by refining the search-oriented informativeness of both queries and documents. In IR systems, the input queries may be constrained by a user's cognitive and cultural competency, making it challenging to accurately express the real intent, and irrelevant content present in documents can also impact the relevance evaluation with the query. As a solution, LLM can be utilized to rewrite the query for enhancing the understanding of the query intent and incorporating additional knowledge into the query through well-designed instructions. The rewritten query can take the form of an improved version of the original query~mao-arxiv-2023-large, {a document in the corpus that related to the query~Gao-ACL-2023-precise, or an expansion of the query that concatenated with a pseudo generated document~Wang-arxiv-2023-query2doc.} In addition, documents can also be expanded with queries that are generated based on the original documents using LLMs for context extension~ma-arxiv-2023-pre.} Remaining Issues. { In this part, we further discuss several important issues to apply LLMs to improve IR systems. First, though LLMs are capable of being as general-purpose task solvers, they are not directly well suited for existing IR systems: they require high overhead for inference~sun-arxiv-2023-chatgpt, Ma-arxiv-2023-fine, have limitations in modeling long texts or document lists~ma-arxiv-2023-zero, and need special adaptation (\\eg instruction tuning) to perform the text ranking task~sun-arxiv-2023-instruction. Therefore, more systematic approaches to adapt LLMs for modern IR systems should be investigated, to leverage their benefits and meanwhile overcome these limitations. Secondly, the advent of LLMs sheds lights on the development of new information seeking ways (\\eg New Bing). It is meaningful to explore how to reshape the architecture and paradigm of IR by integrating the LLMs' capacities and the merits of existing IR systems~wang-arxiv-2023-largesearchmodel. Thirdly, existing work mainly focuses on text retrieval tasks, lacking a comprehensive consideration of multimodal information sources. As will be discussed in Section~sec-MLLM, multimodal large language models~Li-arXiv-2023-Multimodal are also widely studied, making it feasible to develop more powerful multimedia retrieval systems. }",
      "origin_cites_number": 28
    },
    {
      "section_title": "LLM for Recommender Systems",
      "level": "3",
      "content": "{ Unlike IR systems that analyze user search queries to retrieve relevant documents, recommender systems (RS) aim to capture the underlying user preference and provide appropriate information resources to users~zhao-cikm-2021-recbole, zhou-cikm-2021-s3rec, Zhao-cikm-2022-recbole-2, Xu-sigir-2023-towards. Typically, existing studies train a recommendation model (either classic or deep learning model) by fitting it over the user's logged data (\\eg click data)~Rendle-arxiv-2012-bpr, Kang-ICDM-2018-Self. However, these models often suffer from a series of technical issues, \\eg cold-start recommendation, domain transfer, and poor explainability. Recently, LLMs have demonstrated the potential to alleviate these issues of recommendation models~Zhang-2023-arxiv-recommendation, fan-arxiv-2023-recommender, wu-arixv-2023-a, due to the strong capacities of domain generalization and language generation. In this part, we briefly review the recent progress of LLMs in recommender systems, from the following three aspects, namely LLMs as recommendation models, LLM-enhanced recommendation models, and LLMs as recommendation simulators. } LLMs as Recommendation Models. { With specific methods or mechanisms, LLMs can be adapted to serve as recommendation models. Existing work along this line can be generally divided into two main categories. First, some methods prompt LLMs for completing the recommendation task in a zero-shot paradigm (\\ie without parameter tuning)~Gao-arxiv-2023-chat-rec, dai-recsys-2023-uncovering. A series of prompt engineering methods like recency-focused and in-context learning are introduced to improve recommendation performance as well as alleviate the potential model biases~hou-arxiv-2023-large, Liu-arxiv-2023-is. Second, another category of studies aim to specialize LLMs for personalized recommendation through instruction tuning~Zhang-2023-arxiv-recommendation,bao-recsys-2023-tallrec. Specially, high-quality instruction data is key to adapt LLMs to the recommendation tasks, which can be constructed based on user-item interactions with heuristic templates. To further improve the instruction diversity, InstructRec~Zhang-2023-arxiv-recommendation employs self-instruct technique to simulate large amounts of potential user instructions in various scenarios like product search and personalized recommendations. In addition to representing each item by its text description, there is also growing attention on extending LLM's vocabulary with semantic identifiers in recommender systems~Zhu-arxiv-2023-Collaborative,Zheng-2023-arxiv-Adapting, to incorporate collaborative semantics into LLMs.} LLM-enhanced Recommendation Models. { In addition to instructing LLMs to directly provide recommendations, researchers also propose leveraging the universal knowledge encoded in LLMs to improve traditional recommender systems. Existing approaches in this line can be divided into three main categories. The first category employs LLMs to infer users' potential intention from their historical interaction data. Furthermore, traditional recommendation/search models employ the inferred intentions to improve the retrieval of relevant items~xi-arxiv-2023-towards, liu-arxiv-2023-a. Additionally, several studies explore the use of LLMs as feature encoders. % They employ LLMs to encode the side information of items and users (\\eg item's descriptions and user's reviews), % thus deriving more informative representations of users and items. These representations are then fed into traditional recommender systems as augmented input~li-arxiv-2023-exploring, Wei-arixiv-2023-llmrec. % { As another alternative approach, several studies~Li-arxiv-2023-ctrl, Muhamed-nips-2021-ctr-bert adopt a distillation-like way to transfer LLM's capacities (\\eg semantic encoding) to improve traditional recommenders (\\ie small models). Specially, they align the hidden states of LLMs and traditional recommendation models via joint training. After training, since only the enhanced small model will be deployed online, it can avoid the huge overhead of LLMs in online service. } LLM as Recommendation Simulator. {Inspired by the recent success of autonomous AI agents~wang-arxiv-2023-a, LLMs have been also utilized to develop recommendation simulators~Wang-arxiv-2023-RecAgent, Ie-arxiv-2019-recsim (exemplified by RecAgent~Wang-arxiv-2023-RecAgent), showing great potential to simulate user real behaviors in recommender systems~Wang-arxiv-2023-RecAgent, Zhang-arxiv-2023-AgentCF, zhang-arxiv-2023-on. Specifically, to make personalized simulation, an agent will be equipped with a profiling module that encompasses relevant identity information. Then, a memory module is introduced to store agents' past interaction experiences. During the process of simulation, agents are further prompted to conduct self-reflection based on their past experiences, to capture their underlying user preference. Most of existing recommendation simulators are conducted in a user-oriented way, without explicitly modeling the items in the interaction process. To address this, AgentCF~Zhang-arxiv-2023-AgentCF models both users and items as agents, and further facilitates collaborative reflections to simulate user-item interactions, so as to capturing the two-sided relations between users and items.} Remaining Issues. { Despite these efforts, there are still several challenges to address when applying LLMs in recommender systems. First, existing studies have shown that LLM-based recommendation models in zero/few-shot settings tend to perform worse than traditional ID-based recommenders~hou-arxiv-2023-large, dai-recsys-2023-uncovering. This indicates that LLMs might lack an understanding of personalized user behaviors and domain-specific collaborative semantics. Although instruction tuning alleviates this issue to some extent~bao-recsys-2023-tallrec, Zhang-2023-arxiv-recommendation, it can't fully reduce the semantic gap between LLMs and recommender systems, and also suffers from high tuning costs. Furthermore, recommender systems prioritize minimizing inference latency to enhance users' experience in low-resourced environments (\\eg phones), which poses a challenge to LLMs' inference speed as well as memory overhead. Therefore, it is important to explore improvement techniques, such as efficient tuning and quantization methods, to deploy LLMs efficiently and effectively in real-world recommender systems. In addition, existing LLMs have limited capacities in long context modeling, make it difficult to process the huge amount of user-item interaction data. Improved context length extension and context information utilization approaches should be developed to improve the modeling capacities of LLMs in long interaction sequences. }",
      "origin_cites_number": 18
    },
    {
      "section_title": "Multimodal Large Language Model",
      "level": "3",
      "content": "{In existing literature~Du-arxiv-2023-survey,Gan-Foundation-2022-vision, multimodal models mainly refer to the models that can process and integrate information of various modalities (\\eg text, image, and audio) from input, and further produce corresponding output in certain modalities. In this part, we mainly focus on the multimodal extension of LLMs by enabling the information modeling of non-textual modalities, especially the vision modality, called multimodal large language models~(MLLMs)~Li-arXiv-2023-MultimodalIn existing work, large vision language models~(LVLMs)~\\cite{Li-arxiv-2023-Evaluating are also used to term such bimodal models that are developed based on LLMs. We use the naming of MLLMs in this part due to its wide use in existing literature. }. To start our discussion, we specify the input to be text-image pairs and the output to be text responses. Similar discussions can be made for other modalities, \\eg language-audio models~Rubenstein-2023-arxiv-audiopalm, which is beyond our scope here. In essence, MLLMs are developed by adapting the information from other modalities to the text modality, so as to leverage the excellent model capacities of LLMs that are learned based on world text. Typically, a MLLM comprises an image encoder for image encoding and a LLM for text generation, associated by a connection module that aligns vision and language representations. During generation, the image is first split into patches, and then transformed into patch embeddings by the image encoder and the connection module, to derive a visual representation that can be understood by the LLM. Subsequently, the patch embeddings and text embeddings are concatenated, and fed into the MLLM, allowing the language model to generate the response autoregressively. In the following, we will discuss the training, evaluation, and key points to develop capable MLLMs.} Training Process. {The training process of the MLLM includes two major stages: vision-language alignment pre-training and visual instruction tuning.} \\textbullet~Vision-language alignment pre-training. {To develop MLLMs, existing work mostly initializes the vision encoder and the LLM with pre-trained models~Alayrac-nips-2022-flamingo, Liu-arxiv-2023-Visual, Zhu-arxiv-2023-MiniGPT-4. These models retain excellent vision and language capacities, but span different semantic spaces. Thus, the goal of vision-language alignment pre-training (\\ie the first-stage training) is to align the vision encoder and the LLM through end-to-end training on large-scale image-text pairs~Schuhmann-nips-2022-laion5b, Changpinyo-cvpr-2023-conceptual. However, directly tuning these two models on image-text pairs may cause the degradation of the original representation capacities. % To improve the alignment performance, it is crucial to design effective training strategies and select appropriate pre-training data~Ye-arxiv-2023-mplug, Bai-arxiv-2023-qwen. Existing work mainly employs the following strategies for cross-modality alignment: (1) if the number of image-text pairs is not sufficiently large~(\\eg less than 1M), it is often suggested to only update the connection module~Liu-arxiv-2023-improved; (2) if the training data includes high-quality text corpora~Zhang-arxiv-2023-internlm or image-text pairs with fine-grained annotations~Chen-arxiv-2023-shikra, fine-tuning the LLM can be conducted to boost the performance; (3) if the number of image-text pairs is very large~(\\eg about 1B), fine-tuning the vision encoder is also plausible~Ye-arxiv-2023-mplug,Bai-arxiv-2023-qwen, but the benefit remains further verification.} \\textbullet~Visual instruction tuning. % {After vision-language pre-training, the second-stage training, \\ie visual instruction tuning, aims to improve the instruction-following and task-solving abilities of MLLMs. Generally, the input of visual instruction tuning consists of an image and a task description, and the task is to generate a corresponding text output. To boost the performance, high-quality visual instruction data is key to eliciting and enhancing the abilities of MLLMs. Therefore, most studies are dedicated to constructing various visual instruction datasets. As the basic approaches, early studies construct visual instructions by distilling from GPT-4~Liu-arxiv-2023-Visual or reformulating vision-language task datasets~Dai-2023-arxiv-InstructBLIP. To enhance the quality of instruction data, recent work further proposes improved strategies by increasing the instruction diversity~liu-arxiv-2023-aligning, incorporating fine-grained information~(\\eg coordinate of objects) into the instruction~Chen-arxiv-2023-shikra, or synthesizing complex visual reasoning instructions~Du-arxiv-2023-What. Regarding trainable parameters, the authors in~\\cite{Wang-arxiv-2023-what find that updating the parameters of vision encoder on small-scale instruction tuning dataset results in the performance degradation on semantic understanding tasks. Freezing the vision encoder and fine-tuning the connection module and the LLM~(either full parameter or use LoRA) gives promising results~Liu-arxiv-2023-improved, Zhang-arxiv-2023-internlm on the public benchmarks~(\\eg SEED-Bench~Li-arxiv-2023-seed, MME~Fu-arxiv-2023-mme, and MMBench~Liu-arxiv-2023-mmbench).}} Evaluation of MLLM. {After introducing the approaches to developing MLLMs, we further discuss how to effectively assess the multimodal capabilities of MLLMs from the following three aspects. } \\textbullet~Evaluation perspectives. {The evaluation tasks for MLLMs can be categorized into two main types: perception and cognition tasks. Specifically, perception tasks aim to assess the model's abilities in understanding the basic semantics of the image content, while cognition tasks evaluate models with more complex tasks that require reasoning based on perception results. The perception ability is typically evaluated through classification tasks about attributes of image (\\eg topic and style) and object (\\eg existence and color) or OCR-related tasks, based on existing datasets or new datasets derived from existing images with annotations by humans or LLMs~Gurari-cvpr-2018-vizwiz, Mishra-cvpr-2012-top, Liu-arxiv-2023-mmbench, Fu-arxiv-2023-mme. A notable perception issue is hallucination~Zhang-arxiv-2023-siren, where the model's responses contain inconsistent content with the image. Among existing studies about hallucination in MLLMs~liu-arxiv-2023-aligning, Gunjal-arxiv-2023-detecting, Lu-arxiv-2023-evaluation, object hallucination~Rohrbach-emnlp-2018-object has received much research attention. To conduct a stable, robust evaluation of object hallucination, POPE~Li-emnlp-2023-evaluating proposes a polling-based object probing approach for converting object recognition into a series of binary questions, and the results indicate that current MLLMs often struggle with object hallucination. Cognition tasks, on the other hand, require MLLMs to perform reasoning based on image perception. A common reasoning task is visual question answering (VQA), where models answer questions about images that demand reasoning about spatial relationships~Hudson-cvpr-2019-gqa, general knowledge~Lu-nips-2022-learn, or scene text~Amanpreet-cvpr-2019-textvqa. To fully explore the capabilities of MLLMs, HallusionBench~Liu-arxiv-2023-hallusionbench collects 200 sophisticated visual dependent or supplement questions, on which even the most advanced MLLMs like LLaVA-1.5~Liu-arxiv-2023-improved and GPT-4V~OpenAI-OpenAI-2023-GPT-4v fail to achieve good performance.} \\textbullet~Evaluation paradigms.{ The responses of MLLMs can be evaluated either in a closed-ended or an open-ended manner. Traditional multimodal tasks often rely on a closed-ended evaluation framework, where the assessment is based on the exact match between the model's response and the ground-truth answer. Examples include the VQA score~Antol-iccv-2015-vqa for visual question answering tasks and the CIDEr~Vedantam-cvpr-2015-cider score for captioning tasks. However, MLLMs generate responses in an open-ended way, which may contain the correct answer but not exactly match the ground-truth perfectly. This discrepancy can lead to the underestimation of the model's performance in previous evaluation paradigms. To address this issue, recent approaches have incorporated humans or LLMs as evaluators~\\cite {Ye-arxiv-2023-mplug}. For instance, MMBench~Liu-arxiv-2023-mmbench employs ChatGPT to align the model responses with the most relevant option in a set of multiple-choice questions. Similarly, LLaVA~Liu-2023-arxiv-Visual utilizes GPT-4 for evaluating MLLMs' output, where GPT-4 takes the generated image captions and object bounding boxes as visual inputs for assessment. Such open-ended evaluation methods can improve assessment accuracy while incurring higher costs due to the involvement of humans or LLMs.} \\textbullet~Evaluation benchmarks. {To facilitate a more thorough evaluation of MLLMs, various benchmarks have been developed. Part of them collect existing vision-language tasks for comprehensive evaluation. For instance, LVLM-eHub~Xu-arxiv-2023-lvlm aggregates 47 existing text-related visual tasks to assess six distinct capabilities of MLLMs, and Reform-Eval~Li-arxiv-2023-reform takes this a step further by standardizing questions from existing benchmarks into a uniform format and discusses how the backbone models influence MLLMs' performance. In addition to incorporating existing tasks, several work also derives new questions annotated by humans or with the help of LLMs. MME~Fu-arxiv-2023-mme creates a dataset by pairing images from public sources with manually-collected text instructions for perception and cognition evaluations. MMBench~Liu-arxiv-2023-mmbench transforms these instructions into multiple-choice questions and introduces CircularEval to ensure evaluation consistency. SEED-Bench~Li-arxiv-2023-seed further considers temporal understanding tasks and enlarges the evaluation scale to 19K multiple-choice questions with the assistance of LLMs. MM-Vet~Yu-arxiv-2023-mmvet presents more complex tasks to assess the integrated multimodal capabilities of MLLMs. It starts by defining six essential multimodal abilities and then creates intricate questions by combining multiple abilities. In summary, the above benchmarks collectively contribute to the comprehensive evaluation and improved development of MLLMs.} Key Points for Improving MLLMs. To develop capable MLLMs, we continue to discuss three key points to improve the model capacities, from the perspectives of instruction data, training strategy, and safety and alignment. \\textbullet~Visual instruction data. {% Extensive work~Liu-arxiv-2023-improved,Wang-arxiv-2023-To has empirically found that both quantity and quality of visual instructions have an important impact on model performance of MLLMs. One basic way to construct visual instructions is to leverage the exceptional capability of LLMs to synthesize instructions based on text descriptions of images~Liu-2023-arxiv-Visual. % To further enhance the quality of instructions, one can construct fine-grained visual instructions with the help of human annotation~Chen-arxiv-2023-shikra,Zhang-arxiv-2023-LLaVAR or synthesize more complex data through carefully-designed prompts~Du-arxiv-2023-What. Despite the effectiveness of the above LLM-based approaches, one primary question emerges as to whether a LLM (\\ie text generation model without training on any images) possesses the ability to generate sufficiently good visual instructions solely based on verbalized visual information~(\\eg captions and coordinates). Specially, existing work has also revealed that visual instructions generated by LLMs sometimes contain misinterpretations about the visual information, \\eg object hallucination~Li-emnlp-2023-evaluating. Therefore, it is crucial to design effective verification methods to control the quality of instruction data generated by LLMs~Du-arxiv-2023-What. Furthermore, it still needs more investigation about what makes good visual instructions and how visual instructions elicit specific multimodal abilities in MLLMs. } \\textbullet~Model training. Different from LLMs, MLLMs are not trained from scratch, but instead developed based on pre-trained language and vision models. Existing work employs a typical two-stage approach for training MLLMs, \\ie vision-language alignment pre-training and visual instruction tuning. In essence, existing MLLMs aim to (1) preserve the inherent capabilities and parametric knowledge of LLMs as possible, and meanwhile (2) effectively adapt to multimodal tasks by leveraging the pre-trained LLMs and visual encoders. To achieve the above two goals, two typical training strategies are often employed for visual instruction tuning, either only optimizing the connection module~Dai-2023-arxiv-InstructBLIP or fine-tuning both the connector module and LLM component~Liu-2023-arxiv-Visual. As we can see, the former can reserve the original capacities of LLMs but likely have a weak an adaptation performance, while the latter can fully adapt to multimodal tasks but suffer from the loss of original capacities of LLMs. More efforts should be made to investigate how to effectively balance the two aspects, so as to achieving improved multimodal capacities. In addition, existing MLLMs are still overly dependent on the capacities of LLMs, which pose the limits on many multimodal tasks (\\eg space positioning). It will be meaningful to explore improved training approaches of language models, so that multimodal information can be also utilized in this process. \\textcolor{blue {As discussed before, most existing work employs a typical two-stage training approach for developing MLLMs. An optimal training strategy for MLLMs should enable the LLM component to adapt to visual input while preserving its inherent reasoning capabilities and parametric knowledge. Such strategy empowers MLLMs with the capability to tackle complex multimodal tasks~(\\eg knowledge-based visual reasoning). To achieve the aforementioned goals, various approaches employ different training strategies. For instance, InstructBLIP~Dai-2023-arxiv-InstructBLIP only optimizes the parameters of the Q-former module during visual instruction tuning, while LLaVA~Liu-2023-arxiv-Visual fine-tunes both the connector module and LLM component. Consequently, the LLM component may not be sufficiently adapted to the visual representations without fine-tuning its parameters, but fine-tuning the extensive number of parameters from LLM could cause the issue of catastrophic forgetting~Zhai-2023-arxiv-Investigating, even leading to the loss of LLM's original ability~(\\eg human alignment)~Qi-2023-NAML-Visual. Thus, a key question is how to effectively balance the two aspects of considerations, so as to maintain LLM's original ability while simultaneously enhancing its higher-level multimodal abilities during training. } } \\textbullet~Safety and alignment. { Safety and alignment has been widely discussed in LLMs, which aim to regulate the behaviors of models by technical approaches~Ouyang-arxiv-2022-Training. This topic is also important to MLLMs. Even a highly advanced MLLM~(\\eg GPT-4V~OpenAI-OpenAI-2023-GPT-4v) can be susceptible to safety issues. For example, GPT-4V might occasionally exhibit factual inaccuracies and baseless inferences about images. In some cases, it may even generate harmful content targeting specific individuals or groups~OpenAI-OpenAI-2023-GPT-4v. Furthermore, open-sourced MLLMs are also prone to generate hallucinated response~Li-emnlp-2023-evaluating and can be easily manipulated to produce harmful content~Qi-2023-NAML-Visual. To address the aforementioned issues, some studies collect specialized visual instructions to mitigate the problem of hallucination~liu-arxiv-2023-aligning. Another alternative approach is to train a revision model to rectify hallucinated response generated by MLLMs in a post-hoc way~Zhou-arxiv-2023-analyzing. Additionally, aligning MLLMs with RLHF can also assist MLLMs in generating responses with improved factuality~Sun-arxiv-2023-Aligning. Despite these efforts, existing alignment techniques for MLLMs mainly concentrate on several specific aspects~(\\eg hallucination), lacking a comprehensive consideration of alignment criteria. More efforts should be made to promote the research of safety and alignment for MLLMs. } As a promising solution, knowledge graphs (KGs), which store enormous knowledge in the triple format, \\ie $\\langle$ head\\_entity, relation, tail\\_entity $\\rangle$, can be utilized to enhance the task performance of LLMs by providing precise and necessary knowledge. Generally, knowledge enhanced approaches can be expanded into other forms of structured data (\\eg tables and databases)~Ruiz-arxiv-2023-SemTab, while we limit our discussion to the integration of KG for improving LLMs, which are detailed in two aspects, namely retrieval-augmented LLM and synergy-augmented LLM. }",
      "origin_cites_number": 64
    },
    {
      "section_title": "KG-Enhanced LLM",
      "level": "3",
      "content": "Despite the excellent capacities, LLMs often suffer from challenges on knowledge-intensive tasks, such as the potential to generate hallucinated content~Li-arxiv-2023-HaluEval and the lack of domain-specific knowledge~Pan-arxiv-2023-Unifying. As a promising solution, knowledge graphs (KGs), which store enormous knowledge in the triple format, \\ie $\\langle$ head\\_entity, relation, tail\\_entity $\\rangle$, can be utilized to enhance the task performance of LLMs by providing precise and necessary knowledge. Generally, knowledge enhanced approaches can be expanded into other forms of structured data (\\eg tables and databases)~Ruiz-arxiv-2023-SemTab, while we limit our discussion to the integration of KG for improving LLMs, which are detailed in two aspects, namely retrieval-augmented LLM and synergy-augmented LLM. Retrieval-Augmented LLM. { Due to the huge amount of fact records in a KG, existing work typically adopts a retrieval model to first obtain a relatively small subgraph from KG, and then leverages it to enhance LLMs by enriching the relevant knowledge. Before the advent of LLMs, the retrieved subgraphs are often supplemented into training data, injecting knowledge information into PLMs via parameter learning~ERNIE3, Zhang-ACL-19-ERNIE, Wang-TACL-21-KEPLER. In contrast, to leverage the retrieved knowledge, LLMs mainly incorporate it as part of the prompt, without parameter update. To implement this approach, there are two main technical problems, \\ie how to retrieve relevant knowledge from KGs and how to make better use of the structured data by LLMs. For the first issue (\\ie retrieving relevant knowledge), a typical approach is to train a small language model (\\eg RoBERTa) to identify question-related fact triples~Zhang-ACL-2022-Subgraph. To further improve the retrieval performance, several studies also propose an iterative reading-then-reasoning framework, enabling the LLM to interact with the KG multiple times and acquire the required knowledge in a more accurate way~Jiang-2023-arxiv-StructGPT. For the second issue (\\ie utilizing retrieved knowledge), a straightforward approach is to serialize the retrieved subgraph and craft specific prompts to include it as the input of LLMs~Xie-EMNLP-2022-UnifiedSKG, Zhou-ICLR-2023-Large. However, due to the loss of structured information in knowledge serialization, LLMs cannot fully capture the structural semantics conveyed by original KGs. To address this issue, several model-based approaches train a specialized language model (\\eg T5) to transform the subgraph into the natural language text~Ke-ACL-21-JointGT. To guarantee the transformation accuracy, it relies on sufficient training pairs (often unsupervised constructed)~Agarwal-arxiv-2020-Large and excellent model capability~Chen-EMNLP-2020-KGPT. } Synergy-Augmented LLM. { To solve complex tasks (\\eg multi-hop question answering~Lan-2021-arxiv-Complex), it often requires LLMs to query a KG multiple times, following a systematic solution plan. We call such a multi-turn interaction approach to enhancing LLM synergy-augmented LLM. To better synergize the LLM and KG in a complementary manner, recent studies propose to decompose the complex task into multiple sub-goals and iteratively solve each one by leveraging the necessary knowledge from KG~Jiang-2023-arxiv-StructGPT, Gu-ACL-23-Pangu, Luo-arxiv-23-Reasoning. In this process, the LLM can be regarded as an autonomous agent~(detailed in Section~sec:llm_based_agent), which automatically generates the plan and executes it through interaction with the KG environment~Gu-ACL-23-Pangu. {Specially, the mainstream approaches typically start by enumerating the candidates using the available knowledge information at the current step, and then retrieve the most appropriate candidates for the next step according to the question~Gu-ACL-23-Pangu, Luo-arxiv-23-Reasoning.} By iterating the above two steps, LLMs can gradually collect relevant evidence~Gu-ACL-23-Pangu,Luo-arxiv-23-Reasoning, and finally approach the correct solution. % { Despite the effectiveness, enumeration of the candidates over the KG would lead to a vast search space~Lan-2020-ACL-Query.} To address it, StructGPT~Jiang-2023-arxiv-StructGPT proposes a more efficient way to access knowledge information using the specialized interfaces for KGs. Specifically, it carefully designs the specialized interfaces according to the common data operations on KG (\\eg relation extraction and triple extraction), to ensure efficient and accurate data extraction. In this way, LLMs can be instructed to better manipulate and process the structural information of KGs, thus achieving improved task performance. } Future Directions. { Besides the above approaches, there are several promising directions for KG-enhanced LLM remaining underexplored. { First, due to the variety of structured data, it is still difficult for LLMs to directly leverage various kinds of knowledge sources, \\eg domain-specific KGs. Therefore, it is essential to explore the unified way to manipulate and utilize different knowledge sources by LLMs. As a potential solution, it is promising to develop effective approaches to help LLMs comprehend and make use of the access interfaces provided by specific knowledge sources to acquire precise knowledge~Jiang-2023-arxiv-StructGPT, while more efforts should be made to investigate how to adapt to the data variety in a cost-effective way. } Second, with the evolution of real-world information, the knowledge stored in LLMs may become outdated or incorrect. It is necessary to explore how to synchronize the updated knowledge into LLMs through a cost-effective manner~Wang-arxiv-23-easyedit, Yao-arxiv-23-editing. Third, it is promising to investigate the use of factual information from KG to align LLMs in generating more faithful content~Choi-arxiv-23-KCTs, Zhang-arxiv-23-Mitigating, which can help reduce the hallucination of LLMs. } In addition to exploring KG-enhanced LLMs, it is also meaningful to leverage LLMs to improve the tasks on the KG side (\\ie LLM4KG)~Pan-arxiv-2023-Unifying,Zhu-arxiv-23-LLMs. A typical example is that LLMs can help supplement or construct the KG. We omit the discussion of this part, since it is beyond our scope. \\ignore{",
      "origin_cites_number": 21
    },
    {
      "section_title": "LLM-based Agent",
      "level": "3",
      "content": "{The research on agents in AI aims to develop entities that can perceive the environment, make decisions, and take actions to achieve specific goals~Russell-Pearson-2020-Artificial. However, traditional agents are often limited to heuristic rules or specific environments, which constrain their generalization to open-domain scenarios~Lake-arxiv-2016-Building. Given that LLMs possess excellent capacities in solving complex tasks, they have rapidly emerged as promising solutions for serving as the core computation unit of agents~wang-arxiv-2023-a. In this part, we will first introduce the framework for LLM-based agents and then discuss their applications.} Overall Framework. {Next, we first detail the key components of an LLM-based agent and then present the typical workflow.} \\textbullet~Components. { Typically, there are three main components in an LLM-based agent: memory, planning Section~\\ref{subsec-planning introduces planning as a utilization approach for LLMs, while in this section, we describe its utilization as a functional component in LLM-based agents. }, and execution. Specifically, the memory component aims to store the information perceived from the environment and can be utilized to support decision-making. In particular, LLM-based agents usually maintain information in both short-term memory and long-term memory with the operations of reading and writing. Short-term memory usually refers to the internal context window of LLMs (\\ie input), where LLMs can read and write through actions like reasoning~Yao-arxiv-2022-ReAct. While long-term memory can be mapped to the external storage like vector databases~Zhong-2023-arxiv-MemoryBank, where LLMs can read through retrieval and write with reflection~Shinn-arxiv-2023-Reflexion. Specially, profiles are usually implemented with long-term memory, which is an important feature for an agent that specifies its role and function~wang-arxiv-2023-a. The planning component % {is responsible for generating the action plan} based on the information from the memory component. In data format, the plan usually takes the form of text-based instructions~Wang-arXiv-2023-Plan or code-based programs~Gao-arxiv-2022-PAL. To generate it, LLM-based agents will first propose several candidates and then select a more suitable one among them~Wang-arxiv-2022-Self-Consistency. The initial plan can be further refined with execution feedback from the environment~Wang-2023-arXiv-voyager. The execution component is in charge of carrying out the plan from the planning component, {which can be fulfilled by the internal LLM~Wang-arXiv-2023-Plan or external tools~Yao-arxiv-2022-ReAct.} } \\textbullet~Workflow. { With the three components mentioned above, a typical workflow of an LLM-based agent is as follows. First, it receives information from the environment and writes it into short-term memory. {Then, the agent processes the newly received information in the short-term memory. Such a process can be enhanced with information retrieved from long-term memory. Subsequently, the planning component utilizes the processed information from short-term memory to generate the next plan.} Finally, the execution component carries out the plan generated from the planning component, which can be further assisted with external tools. By repeating the aforementioned process, the LLM-based agent can autonomously adjust its behavior in response to feedback from the environment and ultimately achieve its goal. Once LLM-based agents receive user requests or are assigned goals, they follow the above workflow to accomplish tasks through multi-turn interactions with the environment. } { To summarize, in an LLM-based agent, the LLM serves as the core computation unit and is equipped with components including memory, planning, and execution. These components are integrated in a systematic way under the control of the LLM during interactions with the environment. For more details, the readers might refer to the comprehensive survey for LLM-based AI agents~wang-arxiv-2023-a. } Applications. Recently, LLM-based agents have shown great potential in autonomously solving complex tasks, making it feasible to rapidly develop capable applications for specific domains or tasks. In this section, we will discuss the applications in single-agent and multi-agent scenarios. With the amazing capabilities of LLM-based agents, they have revolutionized numerous tasks and domains. In this section, we will discuss the applications in single-agent and multi-agent scenarios. \\textbullet~Single-agent based applications. { Applications based on a single-agent mode mainly aim to develop capable task solvers that can autonomously complete user requests. A large number of single-agent projects have been developed, which focus on general-purpose task solving. % As a representative project, AutoGPT~AutoGPT empowers LLMs with long/short-term memory management and external tools like search engines. In order to autonomously address a user request, AutoGPT understands the request with knowledge from its memory and actions like reasoning, decomposes it into a detailed plan, executes the plan step-by-step with the assistance of tools, and refines the rest plan based on feedback from the environment. Such an iterative process continues until the user request is successfully resolved. Other similar projects include GPT-Engineer~GPT-Engineer and XAgent~xagent2023. In addition, there is also some work that aims to develop autonomous agents for specific domains, such as WebGPT~Nakano-arxiv-2021-WebGPT for the web-browsing environment, ProgPrompt~Singh-arxiv-2022-ProgPrompt for the real-life environment, and Voyager~Wang-arxiv-2023-Voyager for the Minecraft environment. } \\textbullet~Multi-agent based applications. { Different from single-agent systems where agents work independently, multi-agent systems work in collaboration to unleash collective intelligence. Typically, multiple agents can be instantiated from the same or different LLMs, each with their respective roles and functions. According to the coordinating strategies among these agents, multi-agent systems can be divided into two categories: cooperation-based and competition-based. In the cooperation-based mode, to share information and seek collaborative actions among agents, various communication protocols have been proposed, including {free-form dialogue~Li-arxiv-2023-CAMEL, structured document~Hone-arxiv-2023-MetaGPT, and data embedding~Pham-arxiv-2023-Let.} Based on the communication protocol, agents can be effectively organized for downstream applications, such as software engineering~Hone-arxiv-2023-MetaGPT, user behavior analysis~Wang-arxiv-2023-RecAgent, Zhang-arxiv-2023-AgentCF, and society simulation~Park-arxiv-2023-Generative. In the competition-based mode, debate serves as one of the popular communication protocols to foster divergent thinking and elicit valuable external feedback among agents. Such a way is beneficial for domains that demand precise decision-making and accurate responses, such as mathematical reasoning~Du-arxiv-2023-Improving and evaluation~Chan-arixiv-2023-ChatEval. } Remaining Issues. { Despite the huge success, there are still several issues that limit the development and applications of LLM-based agents. First, with the explosive growth of the model scale, the efficiency of LLM-based agents, including both the time and memory overhead, becomes an important issue for large-scale deployment, especially for multi-agent systems with numerous instances of LLMs. Second, with the scaling of the number of LLM-based agents, more effective and efficient communication protocols and architectures are required to support the increased complexity of coordination among agents. Furthermore, building capable agents poses technical challenges for the capacities of LLMs like instruction following and long text modeling. Since existing LLMs are not specially optimized for instantiating agents, most public-sourced LLMs like LLaMA cannot effectively facilitate the development of agents. Therefore, it is crucial to develop capable, specialized models to serve as the core computation unit of agents. } }",
      "origin_cites_number": 28
    },
    {
      "section_title": "LLM for Evaluation",
      "level": "3",
      "content": "{ While human evaluation can generally offer reliable quality assessment, it is also often hindered by high annotation costs, significant time requirements, and annotation inconsistencies~Karpinska-arxiv-23-The. In contrast, automatic evaluation can be employed as a scalable alternative to human evaluation. Traditional automatic evaluations have relied on reference-based metrics (\\eg BLEU and ROUGE). Recently, with the emergence of LLMs as general task solvers highlights their potential as automatic evaluators~Zheng-2023-arxiv-Judging,Wang-2023-arxiv-Large, making it promising to conduct LLM based evaluation. In the following part, we will introduce the recent progress on LLM for evaluation, including evaluation formats, methods, meta-evaluation, and the remaining issues. } Evaluation Formats. { Depending on the type of evaluation outcome, the evaluation format can be categorized into score-based evaluation and language-based evaluation. Score-based evaluation employs measurable metrics to assign quality scores (\\eg ratings or rankings) for evaluated texts. A prevalent way is to conduct pairwise comparison, where LLMs are used to determine the partial order relation of candidate texts following specific guidelines~Peng-23-arxiv-Instruction,Zheng-2023-arxiv-Judging,Wang-2023-arxiv-Large, which greatly simplifies the evaluation task. However, it may face the inefficiency issue when scaling up the number of candidates~Zheng-2023-arxiv-Judging. {When high-quality reference texts are available during evaluation, LLMs can be instructed to score texts under the guidance provided by references~Zheng-2023-arxiv-Judging,Wang-arxiv-2023-SciBench,Sawada-arxiv-2023-ARB.} On the other hand, language-based evaluation focuses on generating critiques and suggestions, offering qualitative explanation beyond simple quantitative scoring~Bai-arXiv-2022-Constitutional,Lee-23-arxiv-RLAIF,Wang-23-arxiv-Shepherd,Cui-23-arxiv-UltraFeedback. It is particularly useful for gathering language feedback signals for human alignment tuning~Bai-arXiv-2022-Constitutional,Lee-23-arxiv-RLAIF. {Furthermore, it can evolve into a multi-turn interaction framework, where LLM-based evaluators provide natural language feedback to existing solutions from task solvers~Wang-23-arxiv-MINT. This framework evaluates the ability of LLMs to leverage language feedback for refining self-generated solutions. } Evaluation Methods. { {A common method for LLM-based evaluation involves prompting LLMs with specific instructions. To further improve the quality of LLM-based evaluation, recent work proposes to prompt LLMs with varied contexts to generate diverse evaluation feedback. These contexts vary in aspects such as the candidate order~Zheng-2023-arxiv-Judging,Wang-2023-arxiv-Large, evaluation perspectives~Saha-23-arxiv-Branch,Zhang-23-arxiv-Wider (\\eg relevance, clarity, originality), and evaluation explanation~Wang-2023-arxiv-Large. The generated multiple evaluation feedbacks are then aggregated to produce a final evaluation result, which makes the evaluation process less prone to biases from individual feedback and allows for a more thorough evaluation by covering a wider range of evaluation aspects.} To further improve the quality of the single-model evaluation, recent studies also develop multi-agent collaboration frameworks~Chan-23-arxiv-ChatEval,Li-23-arxiv-PRD,Zhang-23-arxiv-Wider or fine-tune LLMs as specified evaluators~Bai-arXiv-2022-Constitutional,Lee-23-arxiv-RLAIF,Wang-23-arxiv-Shepherd,Cui-23-arxiv-UltraFeedback,Zhu-23-arxiv-Judge. In a multi-model collaboration mode, different LLMs evaluate the candidates by engaging in discussions to align preferences and reach a consensus~Chan-23-arxiv-ChatEval,Li-23-arxiv-PRD. This method helps reduce the potential biases in individual models through the consensus reached by multiple agents. Another approach to improving single-model evaluation is to specialize LLMs as scores or critics through fine-tuning~Bai-arXiv-2022-Constitutional,Lee-23-arxiv-RLAIF,Wang-23-arxiv-Shepherd,Cui-23-arxiv-UltraFeedback,Zhu-23-arxiv-Judge. This process involves creating datasets annotated with preferences and feedback from humans or proficient LLMs. These datasets are then used to train evaluation-oriented models, enabling them to generate pairwise preference or language feedback. The specialized LLM evaluators demonstrate competitive performance with fewer parameters~Wang-23-arxiv-Shepherd,Cui-23-arxiv-UltraFeedback,Zhu-23-arxiv-Judge. } Meta-Evaluation. { To effectively assess the quality of LLM-based evaluators, meta-evaluation benchmarks have been introduced, for gauging the agreement with human preferences and the fairness of the evaluations made by LLMs~Zheng-2023-arxiv-Judging,Wang-2023-arxiv-Large,Zeng-23-arxiv-Evaluating,Zhang-23-arxiv-Wider,Koo-23-arxiv-Benchmarking. As a representative benchmark, MT-Bench~Zheng-2023-arxiv-Judging evaluates the agreement between LLMs and human judgments, demonstrating that GPT-4 aligns closely with human preferences in no-tie comparisons on 80 multi-turn questions. In addition, to address potential biases arising from subjective human evaluations, LLMBar~Zeng-23-arxiv-Evaluating manually designs outputs that are objectively worse but superficially appealing, which could mislead evaluators. The evaluation results reveal that even the most advanced LLMs still fall short of human-level evaluation in the challenging setting. } Remaining Issues. { As discussed in Section~sec-langauge-generation, recent studies demonstrate that LLM-based evaluators expose multiple types of bias, such as order bias, self-preference bias, and length bias~Zheng-2023-arxiv-Judging,Wang-2023-arxiv-Large. Although some biases can be mitigated through methods like multi-path ensemble or multi-agent collaboration, they remain inherent to LLM-based evaluators. Consequently, addressing these biases intrinsically within the models continues to be an a challenging issue. {In addition, recent work has revealed that LLMs may be incapable of understanding the self-generated content, exhibiting a weaker understanding capacity compared to their generation capabilities~West-23-arxiv-The. Even the most advanced LLMs still struggle identifying their reasoning or factual errors without external feedback~Huang-23-arixv-Large,Stechly-23-arxiv-GPT4.} Consequently, current LLM-based evaluators might not be adequate for evaluating top-tier LLMs or complex tasks. This underscores the importance of improvement approaches for LLM-based evaluators, especially for evaluating capable LLMs and complex tasks demanding sophisticated reasoning, planning, and domain-specific knowledge. }",
      "origin_cites_number": 22
    },
    {
      "section_title": "LLM for Specific Domains",
      "level": "2",
      "content": "{In this part, we discuss the applications of LLMs on several representative domains, including healthcare, education, law, finance, and scientific research assistance.} Healthcare is a vital application field closely related to human life. Ever since the advent of ChatGPT, a number of studies have applied ChatGPT or other LLMs to the medical domain. It has been shown that LLMs are capable of handling % {a variety of healthcare tasks, \\eg biology information extraction~tang-arxiv-2023-does, medical advice consultation~Nov-arxiv-2023-Medical, mental health analysis~Yang-arxiv-2023-mental, and report simplification~Jeblick-arxiv-2023-Medicine}. {As the major technical approach, researchers typically design specific prompts or instructions to guide LLMs to perform a wide range of medical tasks. } To further harness the power of LLMs in the healthcare domain, researchers propose to develop healthcare-related LLMs~singhal-arxiv-2022-large,Singhal-2023-arxiv-Towards,Yang-arxiv-2023-zhongjing. Specifically, the Med-PaLM models~singhal-arxiv-2022-large,Singhal-2023-arxiv-Towards achieves expert-level performance on the {United States Medical Licensing Examination (USMLE)}, and earns greater approval from physicians in answering consumer's medical questions. However, LLMs may fabricate medical misinformation~Jeblick-arxiv-2023-Medicine,Chen-medrxiv-2023-cancer, \\eg misinterpreting medical terms and suggesting advice inconsistent with medical guidelines. In addition, it would also raise privacy concerns to upload the health information of patients~tang-arxiv-2023-does into a commercial server that support the LLM. { Education is also an important application domain where LLMs potentially exert significant influence. % Existing work has found that LLMs can achieve student-level performance on standardized tests~OpenAI-OpenAI-2023-GPT-4 in a variety of subjects of mathematics (\\eg physics, computer science) on both multiple-choice and free-response problems. In addition, empirical studies have shown that LLMs can serve as writing or reading assistant for education~Malinka-arxiv-2023-Education,Susnjak-arxiv-2022-Education. A recent study~Susnjak-arxiv-2022-Education reveals that ChatGPT is capable of generating logically consistent answers across disciplines, balancing both depth and breadth. Another quantitative analysis~Malinka-arxiv-2023-Education shows that {students utilizing ChatGPT (either keeping or refining the results from LLMs as their own answers) perform better than average students in some courses from the computer security field. {Recently, several perspective papers~Tan-arxiv-2023-towards,Kamalov-2023-arxiv-A also explore various application scenarios of LLMs in classroom teaching, such as teacher-student collaboration, personalized learning, and assessment automation.} {However, the application of LLMs in education may lead to a series of practical issues, \\eg plagiarism, potential bias in AI-generated content, overreliance on LLMs, and inequitable access for non-English speaking individuals~Kasneci-learning-2023-chatgpt.} } Law is a specialized domain that is built on professional domain knowledge. {Recently, a number of studies have applied LLMs} to solve various legal tasks, \\eg legal document analysis~Stanek-arxiv-2023-Can, legal judgment prediction~Trautmann-arxiv-2022-Legal, and legal document writing~Choi-SSRN-2023-Chatgpt. A recent study~Nay-arxiv-2022-Law has found that {LLMs exhibit powerful abilities of legal interpretation and reasoning.} Moreover, the latest GPT-4 model achieves a top 10\\% score in a simulated bar exam compared with human test-takers~OpenAI-OpenAI-2023-GPT-4. { To further improve the performance of LLMs in the law domain, specially designed legal prompt engineering are employed to yield advanced performance in long legal document comprehension and complex legal reasoning~Yu-2022-arxiv-Legal,Trautmann-2022-arxiv-Legal. To summarize the progress, LLMs can act as helpful assistants to legal profession. Despite the progress, the use of LLMs in law raises concerns about legal challenges, including copyright issues~Tamkin-arxiv-2021-Understanding, personal information leakage~Sun-arxiv-2023-A, or bias and discrimination~Abid-AIES-2021-Persistent. } Finance is an important field where LLMs have promising application prospects. LLMs have been employed on various finance related tasks, such as numerical claim detection~Shah-arxiv-2023-Zero, financial sentiment analysis~Araci-arxiv-2023-FinBERT, financial named entity recognition~Alvarado-ALTA-2015-Domain, and financial reasoning~Son-arxiv-2023-Beyond. Despite the competitive zero-shot performance exhibited by general-purpose LLMs in the finance tasks, they still underperform domain-specific PLMs containing million-scale parameters~Shah-arxiv-2023-Zero. To leverage the scaling effect of LLMs, researchers collect large-scale finance corpora for continually pre-training LLMs (\\eg BloombergGPT~wu-arxiv-2023-bloomberggpt, XuanYuan 2.0~zhang-arxiv-2023-xuanyuan, and FinGPT~Yang-2023-arxiv-FinGPT). BloombergGPT has demonstrated remarkable performance across a diverse range of financial tasks while maintaining competitive performance in general-purpose tasks~wu-arxiv-2023-bloomberggpt. Nevertheless, it is imperative to consider the potential risks in the application of LLMs in finance, as the generation of inaccurate or harmful content by LLMs could have significant adverse implications for financial markets~wu-arxiv-2023-bloomberggpt. Therefore, it needs more strict reviewing and monitoring on the use of LLMs in the financial field. Scientific research is another promising field that LLMs can empower the development progress. % Prior research demonstrates the effectiveness of LLMs in handling knowledge-intensive scientific tasks (\\eg PubMedQA~Jin-emnlp-2019-PubMedQA, BioASQ~Anastasia-blog-2022-BioASQ), especially for LLMs that are trained on scientific-related corpora~Taylor-arxiv-2022-Galactica,Lewkowycz-arxiv-2022-Solving,Bi-arxiv-2023-OceanGPT. Given the excellent general abilities and broad scientific knowledge, LLMs hold significant potential as helpful assistants across various stages of the scientific research pipeline~Zhang-arxiv-2023-One. First, during the literature survey stage, LLMs can help conduct a comprehensive overview of the progress in a specific research field~Haman-2023-air-Using,Aydn-2022-ssrn-OpenAI. Second, during the research idea generation stage, LLMs demonstrate the ability to generate intriguing scientific hypotheses~Part-2023-arxiv-Can. Third, during the data analysis stage, LLMs can be employed to conduct automatic approaches to analyzing the data characteristics, including data exploration, visualization, and deriving analytical conclusions~Hasaan-2023-arxiv-ChatGPT,Cheng-2023-arxiv-Is. Fourth, during the paper writing stage, researchers can also benefit from the assistance of LLMs in scientific writing~Alkaissi-pubmed-2023-Artificial,Azaria-2023-arxiv-ChatGPT, in which {LLMs can offer valuable support for scientific writing through diverse means, such as summarizing the existing content and polishing the writing~Buruk-2023-arxiv-Academic. } In addition, LLMs can aid in the automated paper review process, encompassing tasks such as error detection, checklist verification, and candidate ranking~Liu-2023-arxiv-ReviewerGPT. Despite these advances, there is much room for improving the capacities of LLMs to serve as helpful, trustworthy scientific assistants, to both increase the quality of the generated scientific content and reduce the harmful hallucinations. \\textcolor{blue{However, when it comes to the depth and overall quality of scientific content, scientific content generated by LLMs still lag behind those produced by human researchers~Ma-arxiv-2023-AI. (seem to be strange to mention this point, since it just follows paper review) } Furthermore, since LLMs suffer from the issue of hallucinations, they may generate mixed or even completely fabricated information, which significantly reduces the trustworthiness of LLMs as research assistants~Alkaissi-pubmed-2023-Artificial. } Summary. In addition to the aforementioned work, the applications of LLMs have been also discussed in several other domains. For instance, in the psychologic domain, some recent work has studied the human-like characteristics of LLMs, such as self-awareness, theory of mind~(ToM), and affective computing~Kosinski-arxiv-2023-tom,Amin-arxiv-2023-affective. In particular, % {an empirical evaluation of ToM conducted on two classic false-belief tasks} speculates that LLMs may have ToM-like abilities since the model in the GPT-3.5 series achieves comparable performance with nine-year-old children in ToM task~Kosinski-arxiv-2023-tom. In addition, another line of work has investigated applying LLMs into the software development domain, \\eg code suggestion~Sridhara-2023-arxiv-ChatGPT, code summarization~Sun-2023-arxiv-Automatic, and automated program repair~Xia-2023-arxiv-Conversational. To summarize, to assist humans by LLMs in real-world tasks has become a significant area of research. However, it also presents challenges. Ensuring the accuracy of LLM-generated content, addressing biases, and maintaining user privacy and data security are crucial considerations when applying LLMs to real-world scenarios.",
      "origin_cites_number": 50
    },
    {
      "section_title": "Advanced Topics",
      "level": "1",
      "content": "In this section, we focus on discussing several advanced topics that have attracted extensive attention in the research community, and these topics are related to challenging technical issues that largely limit LLM's capacity. Next, we will introduce these issues and discuss how to address them with feasible approaches.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Long Context Modeling",
      "level": "2",
      "content": "In real-world application scenarios, there are increasing demands for long context modeling capacities of LLMs, especially for text file processing (\\eg information parsing, extraction, and summarization). Many mainstream LLMs have provided support for long context window. To enhance the long context modeling abilities, there are generally two widely used approaches, namely {scaling position embeddings and adapting context window}. Next, we introduce the two approaches in detail.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Scaling Position Embeddings",
      "level": "3",
      "content": "{ Transformer-based LLMs can learn effective position embeddings within the maximum training length. When adapting LLMs to language tasks beyond the maximum training length, it is necessary to scale to larger position indices. Specially, some position embedding methods have been shown to possess a certain degree of ability to generalize to text beyond the training length, which is termed as extrapolation capability, including T5 bias~Raffel-JMLR-2020-Exploring, ALiBi~Press-ICLR-2022-Train, xPos~Sun-2022-arxiv-Length and even NoPE~kazemnejad-arxiv-2023-impact. {However, as one of the mainstream position embedding methods, RoPE exhibits limited extrapolation ability in empirical studies~Chen-arxiv-2023-Extending.} In the following, we discuss several methods that adapt RoPE to longer texts. } { $\\bullet$~Direct model fine-tuning. To adapt LLMs to a long context window, a straightforward approach is to directly fine-tune the models on long texts with the target length. The context extension can be scheduled with gradually increased lengths in a multi-stage manner (\\eg 2K $\\rightarrow$ 8K $\\rightarrow$ 32K). To conduct effective extension, it often requires specially prepared long text data for training (Section~subsec-longtextdata), and data quality plays a critical role in improving LLM's long context capacities~xiong-arxiv-2023-effective. However, such a direct fine-tuning approach tends to be inherently slow when adapting LLMs for long texts~Chen-arxiv-2023-Extending. } $\\bullet$~Position interpolation. This method downscales the position indices within the original context window, to avoid out-of-distribution rotation angles during pre-training~Chen-arxiv-2023-Extending, kaiokendev-github-2023-Things. Specifically, this approach multiplies all position indices by a scaling coefficient $L/L'$ ($L < L'$), where $L$ and $L'$ denote the original and target context window length, respectively. Experimental results~Chen-arxiv-2023-Extending have shown that this method can extend the context window effectively and efficiently, compared to the above approach of direct model fine-tuning. However, it is worth noting that this technique may have an adverse impact on the model's performance when handling normal texts within the original context window~Chen-arxiv-2023-Extending,Dong-arxiv-2023-BAMBOO. $\\bullet$~Position truncation. To mitigate the challenges posed by out-of-distribution rotation angles, another practical approach is to truncate longer relative positions to satisfy the requirement of the maximum training length. ReRoPE and LeakyReRoPE~su-online-2023-Rerope introduce a pre-defined window length for truncation, which is smaller than the maximum training length. Specifically, position indices within this pre-defined window would be retained, while those indices beyond the window are either truncated to the pre-defined window length or interpolated to align with the maximum training length. {This strategy can preserve the attention mechanism with the neighbor tokens (within the window length), } and further enhance the extrapolation capacity. % {However, this approach needs to compute the attention matrices twice, accommodating additional computational costs.} { $\\bullet$ Base modification. Since LLMs are usually trained with a pre-set maximum training length, wavelengths in certain dimensions of RoPE may exceed the training length for longer text~Peng-arxiv-2023-Yarn, on which language models may not be sufficiently trained, \\ie training data can't cover a complete rotation cycle. Thus, when processing long text, some rotation angles for certain dimensions would never be seen in the training phase~Liu-arxiv-2023_scaling. Formally, given a fixed rotation angle $t\\cdot \\theta_i$, a smaller basis $\\theta_i$ allows for a greater distance $t$, \\ie enabling the modeling of longer texts~Peng-arxiv-2023-Yarn, Roziere-arxiv-2023-codellama,xiong-arxiv-2023-effective. According to the formula $\\theta_i=b^{-2(i-1)/d}$ in Equation~eq:basis, decreasing the basis can be achieved by increasing the value of the base. In addition, decreasing the base can also help re-scale the wavelengths of all dimensions below the training length, while it often needs continual pre-training to adapt the LLMs to long context windows~Liu-arxiv-2023_scaling. A recent study~Liu-arxiv-2023_scaling has empirically compared these two base modification methods, and shown that decreasing the base demonstrates better extrapolation performance, while increasing the base performs better within the training length. } $\\bullet$ Basis truncation. Similar to the base modification, the truncation of the basis also concentrates on dealing with the singular dimensions with wavelengths exceeding the training length~Pal-arxiv-2023-giraffe. According to the definition $\\lambda_i=2\\pi/\\theta_i$ in Equation~eq:wavelength, the dimension with a large wavelength $\\lambda_i$ has a small basis $\\theta_i$ accordingly. Based on this observation, this approach first defines a basis range $[a, c]$. { Given the basis range, the value of basis is modified according to the following ways: (1) when $\\theta_i\\geq c$, the value is retained, (2) when $\\theta_i \\leq a$, the value is set to zero, and (3) when $a < \\theta_i < c$, the value is truncated to a fixed small value.} {Via basis truncation, the out-of-distribution rotation angles can be avoided at larger position indices. However, this approach does not perform very well at long context tasks~Pal-arxiv-2023-giraffe.}",
      "origin_cites_number": 18
    },
    {
      "section_title": "Adapting Context Window",
      "level": "3",
      "content": "{Since Transformer-based LLMs have limited context windows, they can not directly integrate or utilize the entire information of the long sequences exceeding the context window. To alleviate the limitation, several methods have been proposed to adapt LLMs to long context, as discussed below. } { $\\bullet$ Parallel context window. Inspired by fusion-in-decoder~Izacard-EACL-2021-Leveraging, parallel context window methods~Partner-ACL-2023-Parallel,Hao-2022-arxiv-Structured adopt a divide-and-conquer strategy to process input text. Specially, it divides the input text into multiple segments, each independently encoded with shared position embeddings. At the generation stage, the attention masks are modified to make that subsequent tokens can access to previous tokens in each segment. Nevertheless, this method cannot distinguish the order of different segments, resulting in a limited model capacity on certain tasks.} { $\\bullet$ $\\Lambda$-shaped context window. Some prior work has revealed that LLMs tend to allocate greater attention weights to the starting and nearest tokens among all previous tokens~Beltagy-arxiv-2020-longformer,Xiao-arxiv-2023-Efficient, and it potentially results in the ``lost in the middle'' phenomenon~Liu-arxiv-2023-Lost.} Based on this observation, LM-Infinite~Han-arxiv-2023-LMinfinite and StreamingLLM~Xiao-arxiv-2023-Efficient propose to employ a ``$\\Lambda$-shaped'' attention mask, which selectively preserves the initial tokens and the nearest tokens that each query can attend to and then discards any tokens beyond this scope. % {Experiments demonstrate that this method can facilitate extra-long text generation with a fixed memory~Xiao-arxiv-2023-Efficient.} However, it may struggle to model the long-range dependency in the context window, {since it cannot effectively utilize the information from the discarded tokens~Xiao-arxiv-2023-Efficient.} $\\bullet$ Token selection. It has been shown that a relatively small subset of tokens can effectively capture the majority of attention patterns in a Transformer~Bertsch-arxiv-2023-Unlimiformer, \\eg {the top-$k$ attention scores} can well approximate the original full attention. Therefore, a number of studies propose different methods to select the most relevant tokens from token-level or block-level memory units for generation. Token-level selection methods store the past keys in external memory and utilize a $k$-NN search method to retrieve the $k$ most relevant tokens for generation~Wu-ICLR-2022-Memorizing, Bertsch-arxiv-2023-Unlimiformer, Tworkowski-arxiv-2023-Focused. For a decoder model, it typically employs one certain layer to access these top-$k$ external tokens, while still adopting the normal context window in the rest layers~Wu-ICLR-2022-Memorizing,Tworkowski-arxiv-2023-Focused. Block-level selection methods~Lu-arxiv-2024-LongHeads,Xiao-arxiv-2024-InfLLM first segment the long sequence into blocks with the same length and {represent each block into several key vectors for retrieval}. Then, the most relevant blocks to the query as well as the neighbor and initial blocks will be selected for attention computations. Unlike token-level selection methods, {block-level selection methods typically retrieve different tokens with specific heads.}",
      "origin_cites_number": 12
    },
    {
      "section_title": "Long Text Data",
      "level": "3",
      "content": "To further enhance the long context modeling capacity, it typically requires continual pre-training with specially curated long text data. Next, we discuss how to prepare the long text data from the two aspects of quantity and quality. { $\\bullet$ Quantity effect. Different from the pre-training phase that requires vast amounts of data, a small amount of long-text data for continual pre-training is sufficient for context window extension~Chen-arxiv-2023-Extending. Several studies show that LLMs have obtained the capability of utilizing {distant information via large-scale pre-training data, and thus it only needs to adapt for extended context windows during continual pre-training}~fu-icml-2024-data. Typically, it has shown that LLaMA-2-7B or LLaMA-2-13B can achieve a context window length of over 100K tokens and effective context utilization~fu-icml-2024-data with the training on several billion tokens. % However, the ability to handle short text of LLMs may be affected to some extent~Chen-arxiv-2023-Extending. $\\bullet$ Quality effect. In addition to the quantity, the quality of long text data is essential to long context modeling for LLMs. For instance, LongWanjuan~Lv-arxiv-2024-Longwanjuan categorize long texts into holistic, aggregated, and chaotic long texts based on three metrics, \\ie coherence, cohesion, and complexity, and they show that removing chaotic data and keeping coherent and cohesive data are useful to enhance the long text modeling capacities of LLMs. Further, up-sampling cohesive data can lead to further improvement. In addition, when preparing long text data, data mixture should be carefully adjusted for avoiding large distribution drift with the original pre-training data. {In addition to the studies based on vanilla Transformer, there are a surge of Transformer variants with efficient attentions and other efficient architectures, aiming to alleviate the high computational costs for modeling long texts. These studies are discussed in Section~sec:archs and Section~sec:configuration. {Furthermore, context compression and prompting techniques (\\eg iterative reasoning~Chen-arxiv-2023-Walking) have also been proven to be a viable strategy for handling long text tasks}~zhou-arxiv-2023-recurrentgpt,Packer-arxiv-2023-MemGPT,Chen-arxiv-2023-Walking,Xu-arxiv-2023-retrieval, without the need of model adaption.}",
      "origin_cites_number": 7
    },
    {
      "section_title": "LLM-empowered Agent",
      "level": "2",
      "content": "{The research on agents in AI aims to develop entities that can perceive the environment, make decisions, and take actions to achieve specific goals~Russell-Pearson-2020-Artificial. However, traditional agents are often limited to heuristic rules or specific environments, which constrain their generalization to open-domain scenarios~Lake-arxiv-2016-Building. Given that LLMs possess excellent capacities in solving complex tasks, they have rapidly emerged as promising solutions for serving as the core computation unit of agents~wang-arxiv-2023-a. In this part, we will first introduce the framework for LLM-based agents, then explore their applications, and finally discuss the future directions.}",
      "origin_cites_number": 3
    },
    {
      "section_title": "Overall Framework.",
      "level": "3",
      "content": "Next, we first detail the key components of an LLM-based agent and then present the typical workflow. Components. { Typically, there are three main components in an LLM-based agent: memory, planning Section~\\ref{subsec-planning introduces planning as a utilization approach for LLMs, while in this section, we describe its utilization as a functional component in LLM-based agents. }, and execution. Specifically, the memory component aims to store the information perceived from the environment and can be utilized to support decision-making. In particular, LLM-based agents usually maintain information in both short-term memory and long-term memory with the operations of reading and writing. Short-term memory usually refers to the internal context window of LLMs (\\ie input), where LLMs can read and write through actions like reasoning~Yao-arxiv-2022-ReAct. While long-term memory can be mapped to the external storage like vector databases~Zhong-2023-arxiv-MemoryBank, where LLMs can read through retrieval and write with reflection~Shinn-arxiv-2023-Reflexion. Specially, profiles are usually implemented with long-term memory, which is an important feature for an agent that specifies its role and function~wang-arxiv-2023-a. The planning component % {is responsible for generating the action plan} based on the information from the memory component. In data format, the plan usually takes the form of text-based instructions~Wang-arXiv-2023-Plan or code-based programs~Gao-arxiv-2022-PAL. To generate it, LLM-based agents will first propose several candidates and then select a more suitable one among them~Wang-arxiv-2022-Self-Consistency. The initial plan can be further refined with execution feedback from the environment~Wang-2023-arXiv-voyager. The execution component is in charge of carrying out the plan from the planning component, {which can be fulfilled by the internal LLM~Wang-arXiv-2023-Plan or external tools~Yao-arxiv-2022-ReAct.} } Workflow. { With the three components mentioned above, a typical workflow of an LLM-based agent is as follows. First, it receives information from the environment and writes it into short-term memory. {Then, the agent processes the newly received information in the short-term memory. Such a process can be enhanced with information retrieved from long-term memory. Subsequently, the planning component utilizes the processed information from short-term memory to generate the next plan.} Finally, the execution component carries out the plan generated from the planning component, which can be further assisted with external tools. By repeating the aforementioned process, the LLM-based agent can autonomously adjust its behavior in response to feedback from the environment and ultimately achieve its goal. Once LLM-based agents receive user requests or are assigned goals, they follow the above workflow to accomplish tasks through multi-turn interactions with the environment. } { To summarize, in an LLM-based agent, the LLM serves as the core computation unit and is equipped with components including memory, planning, and execution. These components are integrated in a systematic way under the control of the LLM during interactions with the environment. For more details, the readers might refer to the comprehensive survey for LLM-based AI agents~wang-arxiv-2023-a. }",
      "origin_cites_number": 11
    },
    {
      "section_title": "Applications",
      "level": "3",
      "content": "Recently, LLM-based agents have shown great potential in autonomously solving complex tasks, making it feasible to rapidly develop capable applications for specific domains or tasks. In this section, we will discuss the applications in single-agent and multi-agent scenarios. With the amazing capabilities of LLM-based agents, they have revolutionized numerous tasks and domains. In this section, we will discuss the applications in single-agent and multi-agent scenarios. Single-agent based Applications. { Applications based on a single-agent mode mainly aim to develop capable task solvers that can autonomously complete user requests. A large number of single-agent projects have been developed, which focus on general-purpose task solving. % As a representative project, AutoGPT~AutoGPT empowers LLMs with long/short-term memory management and external tools like search engines. In order to autonomously address a user request, AutoGPT understands the request with knowledge from its memory and actions like reasoning, decomposes it into a detailed plan, executes the plan step-by-step with the assistance of tools, and refines the rest plan based on feedback from the environment. Such an iterative process continues until the user request is successfully resolved. Other similar projects include GPT-Engineer~GPT-Engineer and XAgent~xagent2023. In addition, there is also some work that aims to develop autonomous agents for specific domains, such as WebGPT~Nakano-arxiv-2021-WebGPT for the web-browsing environment, ProgPrompt~Singh-arxiv-2022-ProgPrompt for the real-life environment, and Voyager~Wang-arxiv-2023-Voyager for the Minecraft environment. } Multi-agent based Applications. { Different from single-agent systems where agents work independently, multi-agent systems work in collaboration to unleash collective intelligence. Typically, multiple agents can be instantiated from the same or different LLMs, each with their respective roles and functions. According to the coordinating strategies among these agents, multi-agent systems can be divided into two categories: cooperation-based and competition-based. In the cooperation-based mode, to share information and seek collaborative actions among agents, various communication protocols have been proposed, including {free-form dialogue~Li-arxiv-2023-CAMEL, structured document~Hone-arxiv-2023-MetaGPT, and data embedding~Pham-arxiv-2023-Let.} Based on the communication protocol, agents can be effectively organized for downstream applications, such as software engineering~Hone-arxiv-2023-MetaGPT, user behavior analysis~Wang-arxiv-2023-RecAgent, Zhang-arxiv-2023-AgentCF, and society simulation~Park-arxiv-2023-Generative. As a representative project, LangChainhttps://www.langchain.com/ is a framework for developing multi-agent based applications powered by LLMs. It enables users to deploy different roles of LLM-based agents and utilize them to solve tasks via working in collaboration. In addition, other similar frameworks, such as AgentVerse~Chen-arxiv-2023-agentverse and AutoGen~Wu-arxiv-2023-autogen, can also be utilized for developing multi-agent collaborative systems. In the competition-based mode, debate serves as one of the popular communication protocols to foster divergent thinking and elicit valuable external feedback among agents. Such a way is beneficial for domains that demand precise decision-making and accurate responses, such as mathematical reasoning~Du-arxiv-2023-Improving and evaluation~Chan-arixiv-2023-ChatEval. }",
      "origin_cites_number": 16
    },
    {
      "section_title": "Discussion",
      "level": "3",
      "content": "Despite the huge success, there still remain several technical challenges that limit the development and application of LLM-based agents. In this part, we discuss the remaining challenges from the perspective of computational burden, human alignment, complex capability extension, and robustness.} Computational Costs. With the ever-increasing capabilities of LLMs~wang-arxiv-2023-a, their performance on agent applications demonstrate promising performance. However, it also introduces significant issues in terms of efficiency due to the high computational demands and intricate interaction mechanisms involved. Furthermore, in multi-agent systems with numerous LLM instances, as the number of agents increases, this issue would be more severe, since the communication network within multi-agent systems also becomes increasingly complex. Therefore, more effective and efficient communication protocols and architectures are essential to support the heightened coordination demands among agents. {Alignment with Human Sociality.} LLM-based agents can be conceptualized as individual entities, with the emergence of sociability resulting from the interaction among these agents. Autonomous agents often assume specific roles such as coders or researchers, making role-playing a vital capability for agents to solve downstream tasks~Shao-Character-LLM-2023-arxiv. However, LLMs, typically trained on web corpora, face difficulties in accurately mimicking roles that are infrequently discussed online or are emergent. They also lack self-awareness in conversational scenarios due to inadequate modeling of human cognitive psychology. Thus, it is imperative to develop improved agent technique, including both training methods and architectures, to better align LLMs with human preferences and enhance their role-playing abilities. Capability Extension. LLM-based agents, similar to humans, require advanced capabilities (\\eg tool learning) to fulfill complex functions or tasks, which might be beyond their capacity scope. To address this issue, tool use has become a widely-used approach to enhancing LLMs' capacities in various complex tasks. For example, when answering informative user questions, they use search engines to retrieve information from the internet. {However, the quality and quantity of existing available tools impose limitations on their accessibility and comprehensiveness. And it would become more difficult for LLM-based agents to use such limited tools when interacting with dynamic and changing environments}. In addition, as the scale of tools expands, the compatibility and extensibility between the agents and tools must be further improved to facilitate complex task resolution. Robustness and Trustworthiness. The deployment of LLM-based agent systems necessitates robustness and trustworthiness~Hua-2024-TrustAgent-arxiv. The system should be resilient against adversarial inputs from various modalities such as text, image, or audio. Incorporating existing techniques like adversarial training, data augmentation, and sample detection to increase sensitivity to aggressive information in the input can fortify the system's security. Concurrently, it is challenging to ensure the credibility of LLM-based agents given the severe hallucination issues inherently rooted in LLMs. While existing methods such as constrained decoding during inference and external knowledge integration can mitigate these issues to some extent~Huang-2023-A-arxiv, further exploration of efficient and effective alignment methods is necessary to develop reliable agent systems.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Analysis and Optimization for Model Training",
      "level": "2",
      "content": "{In Section~sec:training_settings, we have introduced basic techniques for training LLMs. As the scale of model parameters and data continues to expand, efficiently training larger models with limited computational resources has become a critical technical challenge in the development of LLMs. This challenge primarily encompasses two technical issues: firstly, how to optimize memory usage when loading and processing models across GPU clusters, and secondly, how to maintain or improve training efficiency as models scale. Next, we will conduct quantitative analyses and introduce advanced training techniques addressing the two aforementioned issues.}",
      "origin_cites_number": 0
    },
    {
      "section_title": "Estimation of Training Memory Consumption",
      "level": "3",
      "content": "In this part, we will first estimate the GPU memory consumption for training LLMs. Model States Cost. Model states often occupy the majority of memory during training, typically consisting of model parameters, gradients, and optimizer states. As introduced in Section~subsub:scalable, mixed precision training has been widely utilized in LLM training. {For a model containing $P$ parameters, both the model parameters and their gradients are typically stored as 16-bit floating-point numbers, requiring a total storage of $4P$ bytes ($2P$ for the parameters and $2P$ for the gradients). When using optimizers such as Adam~Kingma-arXiv-2015-Adam or AdamW~Loshchilov-ICLR-2019-Decoupled, an additional set of 32-bit floating-point numbers are needed to store the optimizer states, including the copy of model parameters, gradient momenta, and gradient variances, which leads to a total storage of $12P$ bytes ($4P$ each for each of these components). Consequently, the total memory required for storing the model states during training is $16P$ bytes. For instance, training LLaMA-7B ($P \\approx 6.7 \\times 10^9$) requires around 100GB memory to store the model states alone.} Activations Cost. Activations are the intermediate states that require to be stored in the forward pass for gradient computation during backpropagation. For example, for a binary operation $Y=WX$, calculating the gradient $\\partial Y{\\partial W}$ necessitates the input $X$, which should be preserved during the forward pass. In Table~tab:activation, we list the estimation of the activation memory consumption for different components within the Transformer model. % Take LLaMA-7B ($V=32,000,L=32,H=4,096,H'=11,008,N=32$) as an example, it would take 16GB memory to store activations per device under the setting $B=1,T=2,048$. } Other Memory Cost. In addition to the main factors affecting GPU memory consumption discussed above, the memory usage also includes the following aspects: $\\bullet$ Deep learning frameworks. The PyTorch framework requires approximately 1GB of GPU memory when loading its core functions. This is the essential overhead for the framework to operate. $\\bullet$ Distributed frameworks. When utilizing distributed training frameworks (\\eg DeepSpeed), its GPU memory usage can fluctuate between 1GB and 4GB. The exact amount depends on the level of optimization and the hyper-parameter settings. This portion of the memory is primarily used to optimize memory management and communication efficiency during the training process. $\\bullet$ Intermediate results and memory fragmentation. Besides the activations, there also exist intermediate results that will affect the peak memory consumption. Take the computation of the softmax function in Equation~201 as an example, the implementation of the Transformers library requires an additional $8BTV$ bytes of memory, as it needs to store two additional copies of the 32-bit input ($4BTV$ bytes each). Moreover, during the training process, memory fragmentation occurs due to the non-contiguous allocation and release of memory, typically leading to an additional 0.5GB to 1GB of memory consumption.",
      "origin_cites_number": 3
    },
    {
      "section_title": "{Memory Optimization Methods",
      "level": "3",
      "content": "} Based on the aforementioned analysis, we will next introduce several typical methods for optimizing the memory usage for training LLMs. Gradient Checkpointing. Gradient checkpointing~Chen-arxiv-2016-training, also known as activation recomputation, is a technique used to optimize memory usage during backpropagation. Specifically, the activations need to be retained during the forward pass. However, storing all activation values for each layer requires a significant amount of memory resources (detailed in Table~tab:activation). To reduce the memory cost, gradient checkpointing retains only a subset of the activations during the forward pass and recomputes these values during the backward pass to save memory, albeit with additional computational overhead. In implementation, gradient checkpointing typically involves storing the input of each Transformer layer and recomputing the corresponding activation values during backpropagation. ZeRO. Zero redundancy optimizer (ZeRO)~Rajbhandari-IEEE-2020-ZeRO technique, proposed by the DeepSpeed library, focuses on alleviating the issue of memory redundancy in data parallelism. As mentioned in Section~subsub:scalable, data parallelism requires each GPU to store the same copy of the model states, resulting in a memory consumption of $16P$ bytes per GPU. A direct side effect of data parallelism is that it memory redundancy issues, since not all of the above data is necessary to be retained on each GPU. To resolve it, the ZeRO technique aims to retain only a fraction of data on each GPU, while the rest data can be obtained from other GPUs when required. Specifically, ZeRO provides three strategies, depending on how the three parts of the data are stored, namely optimizer state partitioning (ZeRO-1), gradient partitioning (ZeRO-2), and parameter partitioning (ZeRO-3). Empirical results indicate that the first two strategies do not increase the communication overhead, and the third solution increases about 50\\% communication overhead but saves memory proportional to the number of GPUs. PyTorch has implemented a similar technique as ZeRO, called fully sharded data parallel (FSDP)~FairScale2021. Offload. In GPU-limited environments, DeepSpeed has proposed the offload technique~Ren-USENIX-2021-ZeRO, which can significantly reduce the GPU memory required for training by offloading part of the model states and computational overhead to CPU memory. {Specifically, gradients and optimizer states would be offloaded to CPU memory, with only the model parameters kept on GPU.} The computationally intensive forward and backward propagation still need to be performed on GPU to ensure efficiency, while {parameter update, which requires relatively fewer computations,} are executed on CPU to reduce GPU memory overhead. Furthermore, Infinity~Rajbhandari-sc-2021-zero allows training models that exceed the GPU memory limits by utilizing high-speed disk storage (\\eg NVMe).",
      "origin_cites_number": 5
    },
    {
      "section_title": "{Efficiency Optimization Methods",
      "level": "3",
      "content": "} {In addition to memory-saving techniques, it is also crucial to maintain computational throughput as the model scales. % } In what follows, we will describe two representative efficiency optimization methods. FlashAttention. FlashAttention~Dao-nips-2022-flashattention,Dao-2023-arxiv-flashattention2 is an optimization method for the attention mechanism that significantly reduces the memory transfer during attention computation. The core idea is to minimize the storage of intermediate results and directly obtain the final result. According to the attention computation equation $softmax(\\bm{QK^{\\intercal}}{D})V$, multiple intermediate results, such as $QK^{\\intercal}$ and the attention score matrix, need to be explicitly retained, leading to numerous memory read-write operations. FlashAttention uses specially designed methods, such as matrix partition and operator fusion, to keep intermediate results in the cache until the final result is obtained, thus reducing the amount of memory read and write operations. Additionally, FlashAttention can effectively reduce the peak memory usage and activation memory consumption (Section~sec:training-effi) during the LLM training and inference. By using FlashAttention, LLaMA-2~(7B) with a sequence length of 2,048 and a batch size of 8 requires only one-tenth of the computation time compared to the standard method. Sequence Parallelism. Compared with the 3D parallelism introduced in Section~sec:training_settings, sequence parallelism can be considered a fourth parallelism dimension in pre-training, particularly when handling long data sequences. The core idea is to partition the sequence across multiple devices for parallel computation. The primary challenge lies in minimizing communication across the devices during attention computation. DeepSpeed-Ulysses~Jacobs-arxiv-2023-deepspeed partitions the sequence along the hidden dimension, allowing each device to receive a subset of the attention heads and compute attention for different heads in parallel. In comparison, Ring Attention~Liu-arxiv-2023-ring partitions the sequence along the {length dimension}, where the query matrices on each device are in turn computed with the key and value matrices on other devices. Furthermore, Ring Attention is also compatible with FlashAttention and can be considered as its distributed extension. table*[!t] \\centering \\small The computation, data transfer, and arithmetic intensity during the prefill stage. We use the asymptotic notation $O$ to denote the complexity of data transfer amount, where the constant factor of the complexity is related to the specific implementation method. Table source:~\\cite{Chen-arxiv-2024-towards.} tabular{llll} \\toprule Equations & Computation & Data transfer & Arithmetic intensity \\\\ \\midrule 192 $Q, K, V = X W^{Q,K,V}$ & $6 B T H^2$ & $O(B T H + H^2)$ & $O \\left( 1{1{H} + 1{B T}} \\right)$ \\\\ 193 $Q, K = RoPE(Q, K)$ & $6 B T H$ & $O(B T H)$ & $O(1)$ \\\\ 194 $O = Attn(Q, K, V)$ & $4 B T^2 N D + 4 B T^2 N$ & $O(B T^2 N + B T N D)$ & $O \\left( 1 + \\frac{1{D}}{1{D} + 1{T}} \\right)$ \\\\ 195 $X = O W^O$ & $2 B T H^2$ & $O(B T H + H^2)$ & $O \\left( 1{1{H} + 1{B T}} \\right)$ \\\\ 196 $X = Add\\&Norm(X)$ & $5 B T H$ & $O(B T H + H)$ & $O \\left( 1{1 + 1{B T}} \\right)$ \\\\ 197 $G, U = \\bm X [W^G, W^U]$ & $4 B T H H'$ & $O(B T H + B T H' + H H')$ & $O \\left( 1{1{H} + 1{H'} + 1{B T}} \\right)$ \\\\ 198 $D = Swish(G) \\cdot U$ & $2 B T H'$ & $O(B T H')$ & $O(1)$ \\\\ 199 $X = D W^D$ & $2 B T H H'$ & $O(B T H + B T H' + H H')$ & $O \\left( 1{1{H} + 1{H'} + 1{B T}} \\right)$ \\\\ 200 $X = Add\\&Norm(X)$ & $5 B T H$ & $O(B T H + H)$ & $O \\left( 1{1 + 1{B T}} \\right)$ \\\\ \\bottomrule tabular table* table*[!t] \\centering \\small The computation, data transfer, and arithmetic intensity during the decoding stage. Table source:~\\cite{Chen-arxiv-2024-towards.} tabular{llll} \\toprule Equations & Computation & Data transfer & Arithmetic intensity \\\\ \\midrule 192 $q, k, v = X W^{QKV}$ & $6 B H^2$ & $O(B H + H^2)$ & $O \\left( 1{1{H} + 1{B}} \\right)$ \\\\ 193 $q, k = RoPE(q, k)$ & $6 B H$ & $O(B H)$ & $O(1)$ \\\\ 194 $K, V = Cache(k, v)$ & - & $O(B T N D)$ or $O(B N D)$ & - \\\\ 195 $o = Attn(q, K, V)$ & $4 B T N D + 4 B T N$ & $O(B T N + B T N D + B N D)$ & $O \\left( 1 + \\frac{1{D}}{1 + 1{D} + 1{T}} \\right)$ \\\\ 196 $X = o W^O$ & $2 B H^2$ & $O(B H + H^2)$ & $O \\left( 1{1{H} + 1{B}} \\right)$ \\\\ 197 $X = Add\\&Norm(X)$ & $5 B H$ & $O(B H + H)$ & $O \\left( 1{1 + 1{B}} \\right)$ \\\\ 198 $g, u = X [W^G, W^U]$ & $4 B H H'$ & $O(B H + B H' + H H')$ & $O \\left( 1{1{H} + 1{H'} + 1{B}} \\right)$ \\\\ 199 $d = Swish(g) \\cdot u$ & $2 B H'$ & $O(B H')$ & $O(1)$ \\\\ 200 $X = d W^D$ & $2 B H H'$ & $O(B H + B H' + H H')$ & $O \\left( 1{1{H} + 1{H'} + 1{B}} \\right)$ \\\\ 201 $X = Add\\&Norm(X)$ & $5 B H$ & $O(B H + H)$ & $O \\left( 1{1 + 1{B}} \\right)$ \\\\ \\bottomrule tabular table*",
      "origin_cites_number": 5
    },
    {
      "section_title": "Analysis and Optimization for {Model Inference",
      "level": "2",
      "content": "} {In Section~sec-decoding, we have introduced the basic decoding strategies for using LLMs. As inference efficiency is critically important for the application of LLMs, we next will quantitatively analyze the efficiency of the inference process and also present corresponding optimization methods. }",
      "origin_cites_number": 0
    },
    {
      "section_title": "Analysis of Inference Efficiency",
      "level": "3",
      "content": "Overall, the inference process of LLMs can be divided into two stages for overhead analysis: (1) the prefill stage, which computes the states and caches the key-value tensors for the input sequence; and (2) the decoding stage, which computes the states of the newly generated tokens, updates the key-value cache (KV cache, and continuously generate tokens in an auto-regressive way until the generation process is complete~Sheng-ICML-2023-FlexGen. Inference Efficiency Measurement. To quantitatively analyze the inference efficiency, we next will introduce two widely-used metrics for measuring inference efficiency. $\\bullet$ GPU performance metrics. First, we introduce the compute capability and memory bandwidth to evaluate the efficiency of a certain GPU. The compute capability of a GPU refers to the number of floating-point operations (FLOP) that it can perform per second, measured in FLOP/s. The bandwidth of a GPU refers to the amount of memory read and write operations it can perform per second, measured in byte/s. The ratio of compute to bandwidth is known as the maximum arithmetic intensity of the GPU, denoted as $I_{max}$, which is measured in FLOP/byte. For example, the half-precision compute and bandwidth of the A100 GPU are 312 TFLOP/s and 2039GB/s, respectively. Correspondingly, its maximum arithmetic intensity is 142.51 FLOP/byte\\url{https://www.nvidia.com/en-us/data-center/a100/}. $\\bullet$ Model efficiency metrics. Similarly, each operation (\\eg matrix multiplication) of the model can be measured by two corresponding metrics: the computation amount and the data transfer amount. The former refers to the total number of floating-point operations, measured in FLOPs. The latter refers to the total amount of GPU memory read and write operations, measured in bytes. Analogous to the arithmetic intensity of a GPU, the arithmetic intensity $I$ of a model operation (\\eg matrix multiplication) can be defined as the ratio of computation to data transfer, with units of FLOP/byte. % When the model's arithmetic intensity $I$ is less than the GPU's maximum arithmetic intensity $I_{max}$, it indicates that the maximum memory bandwidth of the GPU is lower than the speed required. Consequently, the model's efficiency will primarily be limited by memory bandwidth, and the operation is called memory-bound. Conversely, when $I$ exceeds $I_{max}$, it suggests that the GPU's maximum floating-point operation speed is lower than the speed required. In this case, the model's efficiency will mainly be constrained by the GPU's compute capability, and the operation is called compute-bound. Bottleneck Analysis. Based on the above analysis, we can obtain the arithmetic intensity for each operation during both the prefill and decoding stages, as shown in Tables~tab:prefill-flops and~tab:decoding-flops, thereby better identifying the bottleneck operations in the inference process. $\\bullet$ Prefill stage. In the following analysis, we will still take the LLaMA (7B) model in Table~tab:activation as an example ($N=32, D=128, H=4096$) and assume a batch size of 8 and a sequence length of 1024 (\\ie $B=8, T=1024$). Substituting these values into Table~tab:prefill-flops, we can find that the arithmetic intensity for linear transformations (Equations~192195197199) is approximately 2730.67, for multi-head attention (Equation~194) it is approximately 114.67, while the intensity for other operations (Equations~193196198201) is around 1. When using an A100 (80G) GPU with $I_{max}=142.51$, the arithmetic intensities of the linear transformations and multi-head attention operations are all above or close to the maximum value. Given that these operations occupy the majority of the computations during the prefill stage, we can conclude that prefill stage is actually compute-bound. $\\bullet$ Decoding stage. Similarly, substituting these values into the arithmetic intensity formulas in Table~tab:decoding-flops for the decoding stage reveals that the arithmetic intensities of the linear transformations and multi-head attention are all below 8, which is much lower than the A100 GPU's maximum intensity 142.51. This indicates that the decoding stage is constrained by the GPU's data transfer speed (\\ie memory-bound), a problem commonly referred to as the memory wall. The analysis indicates that inefficiencies in LLM inference primarily occur during the decoding stage.",
      "origin_cites_number": 1
    },
    {
      "section_title": "System-level Optimization",
      "level": "3",
      "content": "To mitigate the memory wall issue, an intuitive idea is to reduce the data transfer operations as possible, thereby enhancing the arithmetic intensity. In this part, we will introduce several system-level optimization methods to achieve the reduction in data transfer. FlashAttention and Flash-Decoding. The FlashAttention method discussed in Section~subsub:training-effi can also be applied at the prefill stage, as it reduces data transfer operations and effectively increases arithmetic intensity. However, this optimization technique is not directly applicable during the decoding stage, where only the {current query vector needs to be computed with the KV cache matrices}. To further optimize the decoding process, Flash-Decoding~dao-web-2023-flash has been proposed based on FlashAttention, particularly for long sequences, which shares a similar idea with sequence parallelism. Specifically, Flash-Decoding splits the KV cache into smaller chunks, {allowing the computation of the query vector with these chunks in parallel}, thereby improving the decoding efficiency. PagedAttention. PagedAttention~vllm-pagedattention focuses on optimizing {KV cache} and attention computation, significantly reducing data transfer operations in these two aspects. In KV cache concatenation, traditional methods often need to allocate new GPU memory for each concatenation, copying the original KV cache and the new hidden states into the newly allocated memory. This process leads to repeated memory read-write operations and substantial memory fragmentation. PagedAttention addresses this issue by introducing a memory paging management method, preallocating several blocks of memory for future KV caches, which can largely reduce the memory allocation operations during concatenation. Additionally, PagedAttention optimizes the attention computation by increasing the parallelism. It uses operator fusion to parallelize the {computation of the query vector with multiple KV cache chunk}, thereby enhancing the computational efficiency. Batch Management Optimization. Batch management optimization aims to increase the batch size during the decoding stage to enhance arithmetic intensity. A representative method is continuous batching, proposed by vLLM~vllm-pagedattention. Unlike traditional fixed-length batch processing, this technique breaks down each request into a prefill iteration and several single-step {decoding iterations}, and continuous batching further employ heuristic algorithms to select requests for prefill or single-step decoding iteration. This fine-grained batching mechanism allows for handling more requests simultaneously, which is has the same effect as increasing the batch size. Furthermore, DeepSpeed-MII~Holmes-arxiv-2024-deepspeed introduces Dynamic SplitFuse, which splits the prefill stage into multiple iterations and allows simultaneous prefill and decoding in one computation, resulting in larger batches and higher inference throughput.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Algorithm-level Optimization",
      "level": "3",
      "content": "In addition to system-level optimization methods, existing research work has proposed a series of improvements for autoregressive inference algorithms aimed at enhancing inference efficiency. This part introduces four typical inference optimization algorithms. Speculative Decoding. Intuitively, the generation steps in language modeling have varied difficulty levels. For example, predicting the next word of ``The founder of Microsoft is\" may be more challenging than predicting the next word of ``The founder of Microsoft is Bill\". Even a small model may successfully predict the answer in this case. Based on this idea, speculative decoding~Yaniv-ICML-2023-Fast,Chen-arxiv-2023-Accelerating has been proposed to accelerate the inference speed. Specifically, it employs a relatively smaller yet more efficient model (such as an $n$-gram statistical model or a small pre-trained model) to autoregressively generate several tokens. Then, a larger model then verifies these tokens, determining {whether each token is the top-ranked prediction at the each generation step}. The small and large models iteratively repeat this process until decoding is complete. Speculative decoding can lead to a notable 2$\\times$ to 3$\\times$ speedup without compromising the generation quality. Researchers further suggest several variants to improve the efficiency of this approach, such as a learning-based method to combine several small models~Miao-arxiv-2023-SpecInfer and a stage-wise acceleration which employs a more smaller model to accelerate the small model first~Spector-2023-arxiv-Accelerating. Cascade Inference. {Cascade inference optimizes the inference efficiency by addressing requests of varying difficulty with models of different scales.} FrugalGPT~Chen-arxiv-2023-frugalgpt introduces a series of models arranged by efficiency from high to low, sequentially processing a request through these models. A specially trained binary classification model then evaluates whether the generated result meets the task requirements. If the result is deemed reliable, subsequent models would be bypassed, thus improving the inference speed. This strategy can be applied to various open-source models and commercial APIs, allowing for the flexible adjustment the classification threshold to balance inference efficiency and generation quality according to specific needs. For reasoning tasks, researchers~Yue-arxiv-2023-large further propose to utilize the self-consistency~Wang-arxiv-2022-Self-Consistency of generated answers to evaluate the quality of the small model: the large model is employed for generation only when the small model's answers exhibit a low consistency. Non-autoregressive Decoding. Existing decoding methods predominantly adopt the autoregressive mechanism, generating tokens one by one, which is a primary reason for lower inference efficiency. Therefore, non-autoregressive decoding~Gu-iclr-2018-Non has been proposed by generating {all tokens} based on the input {at once}. However, the generation quality of this method still largely lags behind autoregressive methods. To improve the quality of the generated text, several studies attempt to combine both decoding methods, proposing semi-autoregressive decoding methods~Wang-emnlp-2018-Semi that generate a group of tokens (\\eg 3 to 10 tokens) at each step and use these tokens as input to generate the next group. However, existing mainstream LLMs are pre-trained to predict the next token, making direct non- or semi-autoregressive generation infeasible. To address this, Medusa~Cai-arxiv-2024-Medusa trains two additional prediction heads on the Vicuna model to predict the second and third tokens respectively, thereby achieving the generation of three tokens simultaneously. However, due to the decreased generation quality, these methods have been rarely used directly in practice, but are more often combined with other methods (\\eg speculative decoding) to accelerate the inference process of LLMs. For instance, after Medusa generates three tokens in parallel, the original Vicuna model would still be employed to verify the generation quality. Early Exit. It has been found that in multi-layer Transformer models, it may not be necessary to perform the computation through all layers to reliably predict the next token~Teerapittayanon-icpr-2016-branchynet. Based on this idea, {several studies~Huang-iclr-2018-multi,Teerapittayanon-icpr-2016-branchynet} have proposed improved generation methods based on early exit. During model decoding, when the conditions for early exit are satisfied, the model can directly use intermediate computation results from certain layers to generate tokens, thereby improving the inference efficiency. To determine the exit condition, prediction confidence~Huang-iclr-2018-multi or the entropy~Teerapittayanon-icpr-2016-branchynet of the next token's generation probability distribution can be used as reference measures. More recently, mixture-of-depths~raposo-arxiv-2024-mixture has proposed to dynamically adjust the computation load of each layer. Similar to MoE networks, the mixture-of-depths method calculates a score for each layer's input via a routing network. If the score exceeds a preset threshold, the layer would be computed; otherwise, the layer would be skipped. Unlike traditional early exit mechanisms that skip all subsequent layers, the mixture-of-depths method selectively skips certain layers, which can adaptively utilize the characteristics of different layers during generation.",
      "origin_cites_number": 14
    },
    {
      "section_title": "Model Compression",
      "level": "2",
      "content": "Due to the huge number of model parameters, LLMs take a significant memory footprint for inference, making it very costly to be deployed in real-world applications~Wan-2024-Efficient-arXiv. In this section, we focus on how to reduce the memory footprint of LLMs via technical approaches. In particular, we will primarily introduce the model quantization approach, and also briefly discuss other model compression methods, \\eg model pruning and distillation. \\ignore{",
      "origin_cites_number": 1
    },
    {
      "section_title": "Background for Quantization",
      "level": "3",
      "content": "In this part, we present a general introduction of quantization techniques for neural networks. In neural network compression, quantization often refers to the mapping process from floating-point numbers to integers~Gholami-CoRR-2022-A, especially the 8-bit integer quantization (\\ie INT8 quantization). For neural network models, there are typically two kinds of data to be quantized, namely weights (model parameters) and activations (hidden activations), which are originally represented in floating-point numbers. To illustrate the essential idea of model quantization, we introduce a simple yet popular quantization function: $x_q = R(x/S) - Z$, which transforms a floating number $x$ into a quantized value $x_q$. In this function, $S$ and $Z$ denote the scaling factor (involving two parameters $\\alpha$ and $\\beta$ that determine the clipping range) and zero-point factor (determining symmetric or asymmetric quantization), respectively, and $R(\\cdot)$ denotes the rounding operation that maps a scaled floating value to an approximate integer. As the reverse process, dequantization recovers the original value from the quantized value accordingly: $x = S\\cdot (x_q + Z)$. The quantization error is calculated as the numerical difference between the original value $x$ and the recovered value $x$. {The range parameters $\\alpha$ and $\\beta$ have a large impact on the quantization performance}, which often need to be calibrated according to real data distributions, in either a static (offline) or dynamic way (runtime). For more details, we refer to the readers to the excellent survey~Gholami-CoRR-2022-A about quantization methods on neural networks. }",
      "origin_cites_number": 2
    },
    {
      "section_title": "Quantization Methods",
      "level": "3",
      "content": "There are generally two major model quantization approaches, namely quantization-aware training~(QAT)~(requiring additional full model retraining) and post-training quantization~(PTQ) (requires no model retraining). Compared with small-sized language models, two major differences need to be considered when designing or selecting quantization methods for LLMs. Firstly, LLMs consist of a huge number of parameters, and thus PTQ methods are more preferred due to a much lower computational cost than QAT methods. Secondly, LLMs exhibit very different activation patterns (\\ie large outlier features), and it becomes more difficult to quantize LLMs, especially hidden activations. Next, we will briefly review several representative PTQ methodsSince we mainly focus on discussing quantization methods in the context of LLMs, the line of quantization work on small-sized language models (\\eg BERT) has not been included in this survey. for LLMs. Background for Quantization. In this part, we present a general introduction of quantization techniques for neural networks. In neural network compression, quantization often refers to the mapping process from floating-point numbers to integers~Gholami-CoRR-2022-A, especially the 8-bit integer quantization (\\ie INT8 quantization). For neural network models, there are typically two kinds of data to be quantized, namely weights (model parameters) and activations (hidden activations), which are originally represented in floating-point numbers. To illustrate the essential idea of model quantization, we introduce a simple yet popular quantization function: $x_q = R(x/S) - Z$, which transforms a floating number $x$ into a quantized value $x_q$. In this function, $S$ and $Z$ denote the scaling factor (involving two parameters $\\alpha$ and $\\beta$ that determine the clipping range) and zero-point factor (determining symmetric or asymmetric quantization), respectively, and $R(\\cdot)$ denotes the rounding operation that maps a scaled floating value to an approximate integer. As the reverse process, dequantization recovers the original value from the quantized value accordingly: $x = S\\cdot (x_q + Z)$. The quantization error is calculated as the numerical difference between the original value $x$ and the recovered value $x$. {The range parameters $\\alpha$ and $\\beta$ have a large impact on the quantization performance}, which often need to be calibrated according to real data distributions, in either a static (offline) or dynamic way (runtime). For more details, we refer to the readers to the excellent survey~Gholami-CoRR-2022-A about quantization methods on neural networks. Post-Training Quantization~(PTQ). We first introduce the PTQ methods for LLMs. $\\bullet$ Mixed-precision decomposition. As found in Dettmers-arxiv-2022-LLM, {extremely} large values would occur in hidden activations (called the emergence of outliers) when the model size reaches 6.7B parameters or above. These outliers significantly influence the data distribution ranges of the hidden activations, making it challenging to conduct effective model quantization. Interestingly, these outliers are mainly distributed in some specific feature dimensions at Transformer layers. Based on this finding, a vector-wise quantization approach, called \\emph{LLM.int8(), has been proposed in Dettmers-arxiv-2022-LLM, which separates the feature dimensions with outliers and the rest dimensions in matrix multiplication. Then, the calculations for the two parts are performed with 16-bit floating numbers and 8-bit integers, respectively, so as to recover these outliers in a high precision. } {To reduce the quantization error, a straightforward method is to separately process the outliers and the rest weight values. Specifically, LLM.int8()~Dettmers-arxiv-2022-LLM has observed that these outliers are mainly distributed in certain feature dimensions at Transformer layers. Based on this finding, a vector-wise quantization approach is proposed to separate the outliers and the rest in matrix multiplication. $\\bullet$ Salient weights protection. For Transformer based language models, there often exists a subset of weight values that are more sensitive to quantization, {which are also referred to as~salient weights~Lin-arXiv-2023-AWQ. } {Unlike activation outliers, which occur dynamically during inference and may require complex runtime handling, weight outliers are static and can be pre-processed before model deployment.} By identifying and preserving these salient weights, the error associated with model quantization can be effectively reduced. In existing literature, various methods have been proposed to detect these salient weights. For instance, PB-LLM~Shang-CORR-2023-PBLLM utilizes the magnitude of weights for finding critical weights, {SpQR~Tim-arxiv-2023-SpQR categorizes the outliers in weights into small groups by investigating the structural patterns,} APTQ~Guan-CORR-2024-APTQ employs the Hessian trace as a sensitivity metric, and OWQ~Lee-AAAI-2024-OWQ selects the top sensitive columns based on both the Hessian matrix and weight perturbations. $\\bullet$ Fine-grained quantization. For Transformer models, weights and activations are usually represented in the form of tensors. A straightforward approach is to use coarse-grained quantization parameters for the whole tensor (\\ie per-tensor quantization)~Xiao-CoRR-2022-SmoothQuant. However, it usually leads to inaccurate reconstruction results. Thus, fine-grained methods are proposed to reduce the quantization error. % ZeroQuant~Yao-NeurlPS-2022-ZeroQuant adopts a token-wise quantization approach with dynamic calibration for compressing activations. Whereas for weights (easier to be quantized), it uses a group-wise quantization. In practice, a group size of 128~Yao-NeurlPS-2022-ZeroQuant,Lin-arXiv-2023-AWQ is commonly used for model quantization. % $\\bullet$ Balancing the quantization difficulty. Considering that weights are easier to be quantized than activations, SmoothQuant~Xiao-CoRR-2022-SmoothQuant proposes to migrate the difficulty from activations to weights. Specially, they incorporate a scaling transformation to balance the difficulty between weights and activations in a linear layer: $Y = (Xdiag(s)^{-1}) \\cdot (diag(s)W)$. % By introducing an mathematically equivalent transformation, this formula controls the quantization difficulty through the scaling factor $s$. To set $s$, it incorporates a migration strength parameter $\\alpha$ to balance the difficulties, where each entry $s_j=\\max(x_j)^\\alpha / \\max(w_j)^{(1-\\alpha)}$ is determined by the migration strength. % { $\\bullet$ Layerwise quantization. This approach finds optimal quantized weights that minimize a layerwise reconstruction loss: {$\\arg\\min_{\\mathbf{W}}\\parallel WX - \\mathbf{W} X\\parallel_2^2$}. % To efficiently optimize this objective, GPTQ~frantar-arxiv-2022-gptq improves the original optimal brain quantization~(OBQ)~Frantar-NeurIPS-2022-Optimal method by fixing the quantization order of weights for all rows. Further, with specially designed methods (\\ie lazy batch-updates and Cholesky reformulation), GPTQ is feasible to quantize very large models (\\eg 175B OPT) in 3 or 4 bit precision. More recently, AWQ~Lin-arXiv-2023-AWQ further simplifies the optimization form by incorporating activation-aware scaling for weights, which resembles the idea of SmoothQuant~Xiao-CoRR-2022-SmoothQuant: weights corresponding to outlier activations are more important to be precisely quantized. It does not directly optimize the reconstruction loss, but instead performs simple hyper-parameter search to achieve the minimal loss on calibration data. } $\\bullet$ \\emph{INT8 weight quantization can often yield satisfying results, while the performance of low-bit weight quantization depends on specific models or quantization methods~Yao-CoRR-2023-ZeroQuant-V2,Xiao-CoRR-2022-SmoothQuant,frantar-arxiv-2022-gptq,Lin-arXiv-2023-AWQ. In most cases, INT8 weight quantization can be effectively applied to reduce the memory footprint without performance degradation. While for INT4 weight quantization, existing methods usually adopt specific strategies to reduce the performance degradation, \\eg layerwise method~frantar-arxiv-2022-gptq,Yao-NeurlPS-2022-ZeroQuant and activation-aware scaling~Lin-arXiv-2023-AWQ. Interestingly, LLMs seem to be less sensitive to low-bit quantization than small-sized language models~Yao-CoRR-2023-ZeroQuant-V2. } $\\bullet$ \\emph{Low-rank compensation. For post-training quantization, low-bit quantization (\\eg INT4 quantization) often results in large performance degradation. To overcome this challenge, QLoRA~Dettmers-CoRR-2023-QLoRA incorporates additional small tunable adapters (16-bit precision) into quantized models, to achieve an efficient, high-performance model fine-tuning. It combines the merits of LoRA~(See Section~sec-PEFT-methods) and quantization methods. Their experiments show that 4-bit quantized models can reach the full 16-bit fine-tuning performance by QLoRA. Besides, ZeroQuant-v2~Yao-CoRR-2023-ZeroQuant-V2 employs low-rank matrix factorization (16-bit precision) to compensate the quantization error for weight matrix, which can reduce performance degradation of low-bit quantization. } These strategies in the above methods can be jointly used to improve the quantization performance. {In order to achieve high-efficiency implementation, quantization methods also rely on hardware- or system-level support (\\eg efficient GPU kernels or hardware-friendly group partition). % } Other Quantization Methods. In the above, we mainly focus on PTQ methods, and next introduce two recent studies that explore efficient fine-tuning methods or QAT methods for quanitizing LLMs. $\\bullet$ Efficient fine-tuning enhanced quantization. For post-training quantization, direct low-bit quantization (\\eg INT4 quantization) often results in large performance degradation. To overcome this challenge, QLoRA~Dettmers-CoRR-2023-QLoRA incorporates additional small tunable adapters (16-bit precision) into the quantized models, to achieve an efficient, high-precision model fine-tuning. It combines the merits of LoRA~(See Section~sec-PEFT-methods) and quantization methods. The experiment results show that 4-bit quantized models can achieve the full 16-bit fine-tuning performance by QLoRA. $\\bullet$ Quantization-aware training~(QAT) for LLMs. A recent study~liu-2023-arxiv-LLM-QAT explores the effect of QAT methods by applying a data-free distillation method to compress the weights, activations as well as key-value cache. By conducting extensive experiments based on LLaMA, they show promising results with 4-bit quantization on both weights and key-value cache, but not on 4-bit activation quantization, which still needs more exploration. {Empirical Analysis and Findings.} Quantization has currently become a common technique to reduce the memory footprint and latency of LLMs in deployment. In particular, it is important to understand what level of precision (\\eg INT8 or INT4) can be applied to quantize different parts of LLMs (\\eg weights or activations), while retaining a high accuracy. In this part, we first summarize the major findings about the quantization of LLMs in existing literature, and then present some empirical analysis with quantization experiments. \\paratitle{Important Findings from Existing Work. Recently, a very comprehensive evaluation~Yao-CoRR-2023-ZeroQuant-V2 has been conducted about the impact of multiple factors (\\eg model size and sensitivity) on the post-training quantization methods. Another study~Dettmers-2022-arxiv-case examines the scaling law of $k$-bit quantization in inference performance. {In addition to the overall performance, the study~Liu-2023-arxiv-Do_emergent specifically focuses on the potential impact of quantification on emergent capabilities, as well as the levels of performance that can be achieved across various levels of bit precision.} Also, prior work (\\eg LLM.int8()~Dettmers-CoRR-2022-LLM.int8, GPTQ~frantar-arxiv-2022-gptq, QLoRA~Dettmers-CoRR-2023-QLoRA, and GLM~Zeng-arxiv-2022-GLM) has also extensively examined the performance of quantization methods in various settings. Next, we summarize several important findings from these studies, which will be useful for those who may not want to delve into the technical details of quantization methods. } $\\bullet$ INT8 weight quantization can often yield very good results on LLMs, while the performance of lower precision weight quantization depends on specific methods~Yao-CoRR-2023-ZeroQuant-V2,Xiao-CoRR-2022-SmoothQuant,frantar-arxiv-2022-gptq,Lin-arXiv-2023-AWQ. In most cases, INT8 weight quantization can be effectively applied to reduce the memory footprint without performance degradation. While for INT4 (or INT3) weight quantization, existing methods rely on specific strategies to reduce the performance degradation, \\eg layerwise method~frantar-arxiv-2022-gptq,Yao-NeurlPS-2022-ZeroQuant, activation-aware scaling~Lin-arXiv-2023-AWQ and low-rank adapter tuning~Dettmers-CoRR-2023-QLoRA. Interestingly, LLMs seem to be less sensitive to low-bit weight quantization than small-sized language models~Yao-CoRR-2023-ZeroQuant-V2. In practice, with the same memory cost, it is suggested to use a larger language model with a lower quantization precision rather than a smaller language model with a higher quantization precision. For example, a 4-bit 60B LLM is demonstrated to have better performance than an 8-bit 30B LLM~Dettmers-2022-arxiv-case. {Moreover, focusing on emergent capabilities, the study~Liu-2023-arxiv-Do_emergent finds that in-context learning, step-by-step reasoning, and instruction following all seem to be seldom affected with 4-bit weight quantization. This result suggests that INT4 quantization exhibits a favorable trade-off in terms of both total bits and performance of emergent abilities.} $\\bullet$ Activations are more difficult to be quantized than weights~Yao-CoRR-2023-ZeroQuant-V2,Dettmers-arxiv-2022-LLM,Xiao-CoRR-2022-SmoothQuant. It has been found that large outliers would occur for Transformer language models having a size of 6.7B or above~Dettmers-arxiv-2022-LLM. This issue has been one of the most fundamental difficulties to quantize LLMs. To overcome this issue, various methods, \\eg mixed-precision decomposition~Dettmers-arxiv-2022-LLM, fine-grained quantization~wei-arxiv-2023-zero,Dettmers-arxiv-2022-LLM and difficulty migration~Xiao-CoRR-2022-SmoothQuant, can be applied to alleviate the influence of outlier values. Since large outliers mainly exist in the activations of LLMs, small language models are more resistant to activation quantization~Yao-CoRR-2023-ZeroQuant-V2,Liu-2023-arxiv-Do_emergent. % {In practice, high-quality INT8 activation quantization is still a difficult task, though several methods can attain satisfying results. } Further, lower precision activation quantization has still not been successfully explored, even for QAT methods~liu-2023-arxiv-LLM-QAT. { $\\bullet$ Efficient fine-tuning enhanced quantization is a good option to enhance the performance of quantized LLMs~Dettmers-CoRR-2023-QLoRA,Hu-ICLR-2022-LoRA. The benefits of efficient fine-tuning methods in quantization can be twofold. {Firstly, it can directly compensate for the performance degradation suffered from low-bit quantization. This can be achieved either by increasing the fitting capacity via updating high precision adapters~Yao-CoRR-2023-ZeroQuant-V2,Liu-2023-arxiv-Do_emergent,Xu-CORR-2023-qalora, or {by finding a proper low-rank initizalization for LoRA fine-tuning~Li-ICLR-2024-loftq.} Secondly, it is flexible to support task-specific or goal-specific fine-tuning of LLMs in a lightweight way~Dettmers-CoRR-2023-QLoRA, \\eg instruction tuning or chat-oriented tuning, by only tuning the small adapters. Overall, it makes a good trade-off between the effectiveness and training cost, which provides a promising approach to enhancing the performance of quantized LLMs. } Empirical Analysis on Quantization Experiments. To further help readers understand the impact of quantization on LLMs, we also conduct a group of experiments to investigate the inference performance of quantized models here. Specifically, we focus on the fine-tuned LLaMA models~(\\ie 7B and 13B) using popular SFT datasets, including FLAN-v2~Chung-arxiv-2022-Scaling, Alpaca-52K~alpaca and ShareGPT~ShareGPT. For evaluation, we utilize the same tasks in Table~tab-instruction-tuning-res, and follow the quantization settings in the study~Liu-2023-arxiv-Do_emergent examining the performance of quantized language models at three precision levels: 4-bit, 8-bit and 16-bit. The results are summarized in Table~tab-quantization-tuning-res. As can be observed from Table~tab-quantization-tuning-res, the results obtained with 8-bit and 4-bit weight quantization are close to the performance of 16-bit models while significantly reducing memory consumption. In practice, it is recommended to first examine the performance of 4-bit weight quantization for LLMs if reducing memory usage is a critical consideration for deployment. table*[htb] \\centering Evaluation results for quantized LLaMA models~(7B and 13B). {We employ existing model checkpoints provided by~\\cite{Wang-arxiv-2023-How for quantization experiments, which have been fine-tuned on FLAN-v2, Alpaca-52K, and ShareGPT, respectively.} Specifically, we report the performance with AlpacaFarm, MMLU, and BBH, as well as the memory usage of the loaded model~(Mem.). For quantization, we employ bitsandbytes to quantize the 16-bit models to 8/4 bits by specifying the commands load\\_in\\_8bit and load\\_in\\_4bit when loading the weights. It is worth noting that we select~text-davinci-003 as the baseline model for the AlpacaFarm dataset.} 2.0\\columnwidth{!}{ tabular{llcccccccccccc} \\toprule 2.5{*}{Models} & 2.5{*}{tabular[c]{@{}c@{}}SFT Datasettabular} & 4{c}{16-bit}& 4{c}{8-bit} & 4{c}{4-bit} \\\\ \\cmidrule(r){3-6}\\cmidrule(r){7-10}\\cmidrule(r){11-14} & & AlpacaFarm & MMLU & BBH & Mem.$_{(GiB)}$ & AlpacaFarm & MMLU & BBH & Mem.$_{(GiB)}$& AlpacaFarm & MMLU & BBH & Mem.$_{(GiB)}$\\\\ \\midrule LLaMA~(7B) & FLAN-v2 & 6.65 & 47.34 & 35.05 & 12.58 & 6.15 & 47.02 & 35.17 & 6.65 & 7.83 & 46.23 & 34.77 & 3.94 \\\\ & Alpaca-52K & 32.55 & 40.87 & 33.66 & 12.58 & 33.60 & 39.98 & 34.38 & 6.65 & 29.57 & 39.24 & 32.80 & 3.94 \\\\ & ShareGPT & 72.05 & 41.30 & 32.90 & 12.58 & 72.86 & 39.34 & 32.71 & 6.65 & 70.31 & 40.08 & 32.11 & 3.94 \\\\ \\midrule LLaMA~(13B) & FLAN-v2 & 8.14 & 51.67 & 41.46 & 24.40 & 7.64 & 51.02 & 41.25 & 12.53 & 7.52 & 50.48 & 40.68 & 7.34 \\\\ & Alpaca-52K & 33.60 & 47.63 & 36.10 & 24.40 & 31.43 & 47.04 & 35.98 & 12.53 & 30.87 & 46.20 & 36.16 & 7.34 \\\\ & ShareGPT & 75.59 & 47.58 & 38.00 & 24.40 & 73.79 & 47.71 & 38.31 & 12.53 & 71.99 & 45.77 & 36.97 & 7.34 \\\\ \\bottomrule tabular } table* {",
      "origin_cites_number": 56
    },
    {
      "section_title": "Other Model Compression Methods",
      "level": "3",
      "content": "In addition to model quantization, we next introduce two other model compression methods for LLMs, namely model distillation and model pruning. Unlike model quantization, model distillation and pruning aim to simplify the model architecture, thereby reducing the total number of parameters.} Distillation for LLMs. {In general, model distillation aims to transfer the capabilities from a capable model~(referred to as the~teacher model) to a less capable model (referred to as the~student model), thereby achieving the compression of the capable model. Based on whether the weights of {teacher models} are accessible, one can employ either the white-box approach or the black-box approach for LLM distillation. The white-box approach often employs the traditional knowledge distillation technique, which incorporates additional loss functions~(\\ie distillation loss) for aligning the outputs or intermediate states of the student model to those of the teacher model. Based on this approach, MINILLM~Gu-CORR-2023-knowledge effectively distills the 13B LLaMA model down to a 7B model. The black-box approach~Hsieh-ACL-2023-distilling, on the other hand, can only make use of the textual response of the teacher model for training the student model. These studies mainly focus on utilizing the generated responses for enhancing the key capabilities from the teacher {model~Taori-github-2023-Stanford, Luo-arxiv-2023-WizardMath}, such as in-context learning and chain-of-thought prompting.} Pruning for LLMs. {The goal of model pruning is to minimize the number of parameters in a model while preserving its performance as much as possible. In general, model pruning methods can be categorized into two lines: structured pruning and unstructured pruning. Structured pruning aims to remove certain less important model components~(\\eg neurons, channels, layers) that have minimal impact on performance. On the other hand, unstructured pruning mainly focuses on {removing individual weights or connections within a neural network model} without changing the model's main structure. As for LLMs, unstructured pruning can generally lead to higher compression rates. For instance, SparseGPT~Frantar-ICLR-2024-Sparsegpt achieves {60\\% unstructured sparsity % for OPT-175B using unstructured pruning~(\\ie 60\\% of the elements in the weights are masked)}, and the pruned LLM still retains a relatively low perplexity. {With suitable strategies, structured pruning for LLMs can also achieve promising model compression rate.} For instance, LLM-pruner~Ma-NIPS-2023-Llm-pruner {selectively removes 20\\% of the non-essential parameters from LLaMA~(7B) based on gradient information}, while maintaining 93.6\\% performance of the original model. Furthermore, Sheared LLaMA~Xia-CORR-2023-Sheared {introduces two techniques: targeted structured pruning and dynamic batch loading, which effectively prunes LLaMA-2~(7B) to a parameter size of 2.7B, while preserving 87.8\\% of the original model's performance.} The core idea of model distillation involves introducing an additional loss function (referred to as distillation loss), which guides the student model to align its outputs to those of the teacher model.",
      "origin_cites_number": 6
    },
    {
      "section_title": "Open-source Libraries",
      "level": "3",
      "content": "In this part, we briefly introduce the available open-source libraries for memory-efficient deployment. Quantization Libraries. Next, we introduce three popular quantization libraries for LLMs, including: $\\bullet$ Bitsandbyteshttps://github.com/TimDettmers/bitsandbytes is developed based on the methods introduced in the papers of LLM.int8()~Dettmers-arxiv-2022-LLM and 8-bit optimizers~Dettmers-ICLR-2022-8bit. {It focuses on the quantization of both activations and weights for LLMs, including the support on 8-bit and 4-bit~(NF4,FP4) matrix multiplication for efficient inference, as well as an 8-bit optimizer for efficient training.} $\\bullet$ GPTQ-for-LLaMAhttps://github.com/qwopqwop200/GPTQ-for-LLaMa is developed specially for quantizing LLaMA models. It enables 4-bit quantization of LLaMA models of varied sizes based on the GPTQ algorithm~frantar-arxiv-2022-gptq. Also, it provides a comparison with bitsandbytes in both memory and performance (PPL) on the project website. $\\bullet$ AutoGPTQhttps://github.com/PanQiWei/AutoGPTQ is a quantization package developed based on the GPTQ algorithm~frantar-arxiv-2022-gptq, which supports INT4 quantization for LLMs. It includes a number of quantized models in the library, and supports LoRA by integrating with HuggingFace PEFT library. $\\bullet$ llama.cpphttps://github.com/ggerganov/llama.cpp makes it feasible to run quantized LLaMA models on a MacBook device. It supports INT4, INT5 and INT8 quantization, which is developed in efficient C/C++ implementation. It also supports a number of LLaMA based models, such as Alpaca and Vicuna. Other Libraries. In addition, there are also libraries for supporting other model compression methods. $\\bullet$ Torch-Pruning https://github.com/VainF/Torch-Pruning is a toolkit developed for general-purpose structural pruning, including the pruning for vision models, diffusion models and large language models. {It employs dependency graph for automatic structural pruning and supports several high-level pruners~(\\eg MetaPruner and BNScalePruner). } $\\bullet$ LLM-Prunerhttps://github.com/horseee/LLM-Pruner is designed specifically for the pruning of LLMs. {It enables efficient gradient-based structral pruning for LLMs with minimal training samples and training time. Currently, it supports a number of LLMs, such as Baichuan, BLOOM, and LLaMA3.}",
      "origin_cites_number": 4
    },
    {
      "section_title": "Retrieval-Augmented Generation",
      "level": "2",
      "content": "When dealing with real-time information or specialized domain knowledge, LLMs may struggle to generate accurate outputs solely based on their internal knowledge. To address this issue, retrieval-augmented generation~(RAG) technique~ding2024survey, gao2023retrieval has been proposed by incorporating external knowledge source for improving the model response. This technique aims to retrieve relevant information from external sources (\\eg the internet or domain-specific knowledge bases) using an information retrieval system, thereby providing LLMs with timely or domain-relevant context to reduce the factual errors in generated content. In the format, RAG can also be considered as a specific prompting strategy that integrates auxiliary information from external sources into the original prompt. Next, we will introduce the basic workflow of the retrieval-augmented generation technique and related optimization strategies. Basic Workflow. Typically, the standard RAG procedure consists of three steps, including context retrieval, prompt construction, and response generation. \\textbullet~Context Retrieval. The retrieval step primarily focuses on finding relevant context information from existing information sources that are helpful for addressing the current information need. To achieve efficient retrieval, it is often necessary to build a search index over the collection of candidate documents and then use appropriate methodologies for text retrieval. There are two commonly used retrieval approaches: lexical-based retrieval~robertson2009probabilistic using sparse vector representations and semantic retrieval methods using dense vector representations~Zhao-arxiv-2022-Dense. The former tokenizes the documents and building an inverted index based on a vocabulary, followed by retrieving relevant documents using lexical matching. The latter maps documents to low-dimensional dense vectors and then constructs an efficient index of document vectors using approximate nearest neighbor search algorithms, ranking candidate documents based on the similarity of embeddings. Both methods can often perform well for large-scale document collection, which are widely used in existing RAG systems. \\textbullet~Prompt Construction. After the retrieval stage returns the relevant documents, these documents need to be incorporated into the input prompt of the LLM along with the task description. The prompt should guide the model to utilize the retrieved information to complete the corresponding task. For example, a prompt could be, ``Please refer to the information contained in the following documents to complete the task''. Since the retrieved documents are typically lengthy, simply concatenating them into the prompt might lead to a poor utilization of the provided context due to the biased attention (\\eg lost in the middle~Liu-arxiv-2023-Lost). To address this issue, existing approaches often introduce reranking models to select the most relevant documents from the retrieval results~wang2024rear. Alternatively, information extraction or text compression techniques can be used to retain only the highly relevant information from the documents, thereby reducing the input context length~rau2024context, xu2024recomp. \\textbullet~Response Generation. In this step, the constructed prompt is input into the LLM, enabling it to utilize the retrieved content to better accomplish the corresponding task. However, the retrieved documents may contain irrelevant information or even contradictory information to the true answer, which might affect the output generated by the LLM. To address this, the LLM can be further prompted to self-check the quality of the generated output and decide {whether to re-perform the retrieval based on the new outputs~shao2023enhancingå•¥æ„æ€ï¼Ÿï¼Ÿå¦‚æžœè´¨é‡ä¸å¥½ï¼Œå†æ¬¡æ£€ç´¢å°±èƒ½æ£€ç´¢åˆ°å¥½çš„å—ï¼ŸKun: æ˜¯çš„ï¼ŒåŸºäºŽæ–°çš„ç»“æžœåŽ»æ£€ç´¢}, or it can perform a confidence assessment to determine whether the current task requires retrieval or the use of retrieved content~Jiang-2023-arxiv-Active. Improvement Strategies. In practice, factors such as the quality of retrieved documents, prompt design, and the generation method of LLMs might impact the final performance of RAG. Next, we discuss how to enhance the RAG performance by summarizing existing improvement strategies. \\textbullet~Retrieval method improvement. The incorporation of retrieval supplements the LLM with relevant contextual information, and the retrieval performance directly affects the quality of the final generated response~Ren-arxiv-2023-Investigating. To design effective retrieval strategy, an important factor to consider is the text granularity. Intuitively, a coarser granularity (\\eg document-level) may result in efficient retrieval but tend to incorporate substantial irrelevant information, while a finer granularity (\\eg sentence-level) increases the proportion of relevant content in the retrieval results but can lead to higher retrieval latency. To balance relevance and latency, existing research work proposes using ``propositions'' as the retrieval unit~chen2023dense, corresponding to semantically complete and relatively independent text fragments, which can effectively reduce the recall of irrelevant information. In particular, they mainly use GPT-4 to synthesize instruction data for the extraction of proposition text, training a smaller model specifically to construct proposition text data~chen2023dense. Furthermore, to improve retrieval performance, methods such as query expansion and query rewriting can be utilized to optimize query formulation. Query expansion focuses on adding supplementary information to the original query, such as incorporating related entity information or providing detailed explanations of key information in the query~Wang-arxiv-2023-query2doc, which helps strengthen relevance matching. However, traditional query expansion methods may disrupt the original semantics for complex queries. To address this issue, we can employ LLMs to decompose complex queries into several sub-queries, which are subsequently expanded individually, allowing for multi-path recall of related information~huang2023question. As another query enhancment technique, query rewriting focuses on modifying the query content to highlight key information and eliminate potential ambiguities, facilitating the retrieval of related documents~he2016learning. LLMs can be applied directly to query rewriting, transforming the original query into a more suitable form through well-designed prompts~liu2024query. To reduce inference overhead, the query optimization capabilities of LLMs can also be transferred to smaller models through knowledge distillation~ye2023enhancing. {\\textbullet~Retrieval results refinement.} (è¿™ä¸€éƒ¨åˆ†æˆ‘å»ºè®®é‡å†™ï¼Œè²Œä¼¼å’Œæç¤ºæ²¡å•¥å…³ç³»ï¼Œä¸»è¦æ˜¯å¯¹æ£€ç´¢ç»“æžœçš„refineï¼Œå†çœ‹çœ‹å¦‚ä½•å†™ä¸€ä¸‹ï¼Œå¦å¤–ï¼Œé¿å…å’Œretrieval optimizationå†…å®¹å’Œé‡ç‚¹é‡å ) {In addition to the initial retrieval methods, the refinement of retrieval results also plays an important role in RAG systems, since the retrieved documents may be not best suited for RAG systems, \\eg LLMs might have difficulty in utilizing long contexts or be affected by irrelevant information in the retrieved documents. } As a solution, the documents returned during the retrieval stage can be reranked according to their relevance to the input~jeong2024adaptive, filtering out low-quality or irrelevant documents or placing less relevant documents in non-optimal positions within the prompt. Furthermore, both generation and reranking tasks~wang2024rear can be jointly optimized to faciliate better utilize of context documents. Additionally, LLMs can be directly used for document re-ranking by designing specific prompts or using context examples to accomplish this task~sun-arxiv-2023-chatgpt. In addition to document filtering or reranking, % information extraction or automatic summarization techniques can be employed to refine the retrieved content by extracting more concise and query-relevant content from the retrieved documents. Furthermore, existing research has proposed token-level compression strategies~jiang2023llmlingua, which select important tokens and remove unimportant parts from the candidate documents. \\textbullet~Iterative retrieval enhancement. In some complex application scenarios, a single retrieval procedure may not suffice for RAG systems. To address this issue, we can further use iterative retrieval augmentation and adaptive retrieval augmentation. Iterative retrieval augmentation aims to iteratively refine the initial query based on the model's generated results to achieve a comprehensive coverage of the required information. As it involves accumulating multiple rounds of retrieval information, the performance of RAG systems may be affected by redundant or conflicting information. To address this issue, stop mechanism has been introduced for retrieval iteration, using the LLM to evaluate the confidence of the current generation results to determine whether to continue the iteration process~Jiang-2023-arxiv-Active. Additionally, for more complex scenarios, iterative retrieval can be combined with the LLM's own CoT reasoning capability. For example, intermediate results from the chain of thought can be used as the query input for the next round of retrieval, and after completing the retrieval process, the returned results can be integrated into the chain of thought. Building on the iterative retrieval augmentation method, adaptive retrieval augmentation further enhances the LLM's autonomous use of the retrieval mechanism~xu2024sayself, thereby improving the overall framework's efficacy in using the retrieval systems. {In practical implementation, for the above two types of augmentation methods, LLM first need to determine when to use the retriever and then utilize pre-set prompts to initiate query generation and retrieval result processing~asai2023self.(è¿™å¥è¯çš„ç›®çš„æ˜¯ä»€ä¹ˆï¼Ÿå’Œå‰é¢çš„å…³ç³»æ˜¯ï¼Ÿéš¾é“iterativeçš„å°±ä¸ç”¨è¿™ä¸ªæ–¹æ³•äº†å—ï¼ŸKun: éƒ½ç”¨ï¼Œå·²æ”¹)} \\textbullet~RAG-enhanced training. In addition to the improvement strategies mentioned above, specialized training tasks can be designed to further enhance the LLM's ability to utilize the retrieved content, including both instruction tuning and pre-training tasks. By constructing instruction data focused on retrieval context utilization~luo-arxiv-2023-sail, instruction tuning can improve the LLM's ability to utilize relevant retrieval information. When curating the instruction data, {it is essential to consider two important issues: positional bias and irrelevant information within the input contextéœ€è¦é‡å†™ï¼Œè¿™ä¹Ÿä¸æ˜¯ä¸¤ä¸ªissueå•Šï¼Œç‰¹åˆ«æ˜¯ç¬¬äºŒç‚¹; Kun: å·²æ”¹}. Specifically, relevant documents can be placed at different positions within the prompt, which can enhance the model's attention to relevant content in various positions and prevent the model from neglecting certain positions~Liu-arxiv-2023-Lost. Additionally, irrelevant information can be added to the instructions data, so as to improving the model's ability to resist interference from such information~lin-arxiv-2023-ra. In addition, special training tasks can be introduced during the pre-training stage to further enhance the LLM's retrieval and generation capabilities~guu-PMLR-2020-retrieval, Lewis-NeurIPS-2020-Retrieval. Existing work mainly constructs unsupervised pre-training data aimed at retrieval augmentation. A common data construction method uses portions of the original document as queries and then trains the model to reconstruct the remaining content of the original document based on the retrieval results~lee-ACL-2019-latent.",
      "origin_cites_number": 28
    },
    {
      "section_title": "Hallucination",
      "level": "2",
      "content": "Hallucination, which refers to the phenomenon that LLMs generate content inconsistent with factual information, has become a significant issue that greatly affects the task performance of LLMs~Li-arxiv-2024-dawn. In this section, we focus on discussing the topic of LLM hallucination, first introducing the definition and source of hallucination and then summarizing the detection and mitigation methods.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Definition of Hallucination",
      "level": "3",
      "content": "Early research typically defines hallucinations based on the relationship between a model's output and the given input~Ji-ACM-2023-survey. In this manner, hallucinations are categorized into intrinsic hallucinations where the model's output does not match the input text and extrinsic hallucinations where the model's output cannot be verified against the input. However, in real-world scenarios, user inputs often do not contain reference documents, and thus existing work mainly focuses on open-domain factual hallucinations, where the model-generated content does not align with or cannot be verified by existing world knowledge~Zhang-CoRR-2023-Siren,Li-arxiv-2024-dawn. According to a recent study~Li-arxiv-2024-dawn, factual hallucinations can be further categorized into the following types: \\textbullet~Entity-error hallucination. This type of hallucination refers to LLMs generating text containing incorrect entities, such as names of people, dates, locations, or objects that contradict world knowledge. \\textbullet~Relation-error hallucination. This type of hallucination involves LLMs generating incorrect relationships between entities, such as inaccurate quantitative or chronological connections. \\textbullet~Incompleteness hallucination. LLMs may produce incomplete outputs, especially when generating lengthy or list-based responses. This hallucination arises when LLMs are asked about aggregated facts and they fail to reserve the factual completeness. \\textbullet~Outdatedness hallucination. This type of hallucination occurs when LLMs generate information that was accurate at a past time but is no longer correct at present. This issue typically arises due to that most LLMs were trained on time-limited corpora. \\textbullet~Overclaim hallucination. This type of hallucination refers to cases where the statement expressed in the generated text of LLMs is beyond the scale of factual knowledge. \\textbullet~Unverifiability hallucination. This hallucination refers to cases where the information produced by LLMs cannot be verified against existing information sources, making it difficult to assess its accuracy.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Source of Hallucination",
      "level": "3",
      "content": "In this part, we will discuss the potential factors that might lead to hallucination for LLMs. Training Data. The quality of training data significantly impacts the model's output and is a primary source of hallucinations. Further, the distribution of training data also plays a key role in shaping the behaviors of LLMs. We next introduce the effect of training data on hallucinations from these two aspects. \\textbullet~Data quality. In practice, the pre-training dataset is typically constructed by collecting diverse data from various sources. While increasing pre-training data can lead to improved model performance, low-quality data can severely damage the generation performance of large models. On the one hand, pre-training data may contain erroneous information, and the goal of training large models is to imitate and memorize the training data as possible. If inaccurate information frequently appears in the training data, the model may memorize and directly copy this content during generation, leading to the phenomenon known as ``imitative falsehoods''~Lin-ACL-2022-TruthfulQA. On the other hand, pre-training data may contain biased content and the subjective views of its creators. Such biased content can severely affect the model's learning of world knowledge, possibly leading to inappropriate representations. \\textbullet~Data distribution. The distribution of pre-training data also significantly affects the model's behavior. Firstly, regarding the recency factor, LLMs are typically trained on data from a limited period. As world knowledge continuously evolves, the model's stored knowledge can become outdated, thereby likely leading to fabrications or outdated information when addressing questions beyond its knowledge scope. In terms of data composition, pre-training data may lack domain-specific knowledge, which would affect the model performance on tasks requiring specialized knowledge, such as medical or legal issues, and it will also result in significant hallucinations. Additionally, recent studies show that when addressing questions involving long-tail knowledge that appears infrequently in the training corpus, models are more likely to generate inaccurate content~Li-arxiv-2024-dawn. Training Methods. The training process of LLMs typically includes two major stages: pre-training and post-training. Inappropriate training methods across the two stages are also likely to result in the hallucination behaviors of LLMs. % \\textbullet~Pre-training. Currently, the pre-training stage primarily employs the next token prediction method for model training. {Recent studies~Liu-arxiv-2023-Lost} indicate that under the autoregressive training method, the model's attention distribution tends to decay as the sequence length increases. This would prevent LLMs from effectively modeling long-range dependencies, potentially resulting in inference errors or hallucinations. Additionally, the teacher-forcing strategy is commonly used during the training of large models. In this approach, the correct tokens from the previous steps are used to predict the next token instead of the model output. However, during model inference, the model can only use its own generated content for subsequent predictions. This discrepancy between the training and generation phases leads to ``exposure bias''~Bengio-nips-2015-scheduled, which may in turn cause hallucination issues. \\textbullet~Post-training. During the instruction-tuning process, existing works typically employ knowledge distillation to improve the model's instruction-following ability. This involves using high-performance models (such as GPT-4) to generate large-scale instruction data and then fine-tuning weaker models with this data. However, these synthesized data may contain hallucinated content, which might lead to more hallucinations for the trained model. Additionally, during the human alignment process, existing training methods may also cause hallucination issues. Some research work has revealed that LLMs may cater to human responses for earning higher rewards, likely resulting in answers that do not align with factual knowledge~Sharma-arxiv-2023-towards. Response Generation. Given the input prompt, LLMs employ decoding strategies (\\eg top-$k$ sampling in {Section~sec-decoding}) for generating the response. In this process, the prompt formulation and the decoding strategies potentially affect the generation behaviors of LLMs. % \\textbullet~Prompt design. Prompting has become the primary way for using LLMs to solve downstream tasks. However, inappropriate prompt design can cause the model to overlook or misunderstand important information, leading to incorrect or irrelevant content~Li-arxiv-2024-dawn. Recent studies have shown that the readability, format, and concreteness of user instructions would impact the model's output~Rawte-arxiv-2023-exploring. For instance, the use of complex words or long phrases in the prompt reduces the readability, which makes LLMs more difficult to understand the real intentions of user instruction, thereby increasing the chance of hallucination. Additionally, non-standard expressions or abstract concepts can also exacerbate hallucinations. \\textbullet~Decoding strategy. To improve the diversity of the generated content, multiple random sampling strategies are introduced (\\eg beam search, top-$p$ sampling). However, increasing diversity also brings a higher likelihood of generating hallucinated content. For example, increasing the temperature $t$ (Equation~eqn:temperature) will result in a more uniform token probability distribution, which potentially leads to more hallucinations, since low-frequency yet irrelevant words would be assigned a higher probability for generation in this setting.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Hallucination Detection",
      "level": "3",
      "content": "To effectively detect the hallucinated content, existing work mainly adopts three approaches, namely model-based, uncertainty-based and tool-based methods. Model-based Methods. Due to the powerful language capabilities and rich world knowledge, existing work extensively adopts powerful LLMs to detect hallucinations from the model-generated text. In this approach, hallucination detection can be considered as a normal text task that requires prompt formulation. To facilitate the research in this line, HaluEval~Li-arxiv-2023-HaluEval introduces a comprehensive dataset of model-generated and human-annotated hallucinated samples to evaluate how well LLMs can identify such instances, and they empirically show specific prompting strategies such as CoT can effectively improve the model's accuracy in detecting hallucinations. Furthermore, research work proposes to decompose the hallucination detection into two subtasks: first, extract factual statements, and then assess whether each statement is hallucinated or not~Li-arxiv-2024-dawn, Dhuliawala-arxiv-2023-chain. Uncertainty-based Methods. Recent studies suggest that the occurrence of hallucinations in LLMs may be related to the uncertainty of their outputs~Manakul-emnlp-2023-selfcheckgpt. Based on such assumptions, a series of works propose detecting hallucinations by assessing the uncertainty of model-generated content. Some research work focuses on the internal features of LLMs, such as token probability and logits. For key concepts in the generated text, a lower token probability indicates a higher uncertainty, which represents an increased likelihood of hallucination~Varshney-arxiv-2023-stitch. Other research efforts evaluate the uncertainty by examining the consistency of the models' responses. For instance, SelfCheckGPT~Manakul-emnlp-2023-selfcheckgpt lets LLMs answer the same questions multiple times to judge whether the generated answers are consistent or not. Another alternative way requires LLMs to reconstruct the input questions based on the responses and then check the consistency between the generated and original questions~Yehuda-arxiv-2023-search. Tool-based Methods. LLMs can detect hallucinations by calling external tools to verify the model-generated content. Typically, the model's output contains various segments of factual knowledge, which can be broken down into fine-grained factual statements. FActScore~Min-arxiv-2023-FActScore refers to knowledge sources like search engines to verify these statements. FacTool~Chern-arxiv-2023-FacTool further proposes to use a series of external verification tools such as calculators and code interpreters to check different types of text. In addition, HaluAgent~Chen-arxiv-2024-haluagent proposes an agent framework to employ smaller open-source models for hallucination detection. With the assistance of tools like search engines and calculators, HaluAgent enables 7B-size models to achieve comparable performance as GPT-4 in hallucination detection. %",
      "origin_cites_number": 9
    },
    {
      "section_title": "{Hallucination Mitigation",
      "level": "3",
      "content": "} In practice, it is essential to effectively mitigate the hallucination behaviors of LLMs, to provide accurate and helpful responses. In this part, we will introduce several widely-used approaches for alleviating the hallucination, including human alignment, retrieval-augmented generation and improved decoding strategy. Human Alignment. Hallucination mitigation is closely related to the honest criterion in ``3H'' standards for human alignment, and various alignment methods like RLHF can be adopted to mitigate the model hallucination. HaluEval 2.0~Li-arxiv-2024-dawn proposes to first collect hallucinated and non-hallucinated responses to train a reward model, and then fine-tune the LLM with the reward modelâ€™s feedback using RL algorithms. However, recent research shows that human preference data may lead LLMs to exhibit sycophantic behavior~Sharma-iclr-2024-Sycophancy, where models prioritize catering to human demands over maintaining truthfulness. Some work proposes to refine the annotation process of preference data, such as by aggregating multiple human preferences to improve feedback quality~Sharma-iclr-2024-Sycophancy or fine-tuning LLMs on prompts where the truthfulness of a claim is independent of the userâ€™s opinion~Wei-arxiv-2023-sycophancy. Retrieval-Augmented Generation. Providing LLMs with highly reliable external knowledge as context can help reduce hallucinations. RARR~Gao-2023-ACL-RARR first generates multiple questions about the generated text, then retrieves web pages from Google Search as evidence, and finally, an editing model is employed if any disagreement is detected between the evidence and the generated text. LLM-Augmenter~Peng-arxiv-2023-Check further expands the knowledge source to local databases, devising an agent framework to retrieve, consolidate, and generate feedback to the LLM for the final answer. Other research explores placing the retrieval process at different positions relative to the generation process. Verify-and-Edit~Zhao-acl-2023-VE proposes to perform the retrieval procedure after the generation process, allowing the original answer to be edited based on the retrieved documents. Furthermore, to help LLMs better handle complex tasks, IRCoT~Trivedi-arxiv-2022-Interleaving interleaves the knowledge retrieval process with CoT generation, where the retrieved documents guide the LLM in generating additional reasoning steps and CoT sentences assist in retrieving more relevant and diverse documents. Improved Decoding Strategy. In addition to the above methods, hallucinations can also be mitigated by using improved decoding strategies. Typically, the internal states or knowledge of LLMs themselves can be exploited to reduce the hallucinations. DoLa~Chuang-arxiv-2023-DoLa proposes that the lower layers of LLMs tend to assign higher probabilities to syntactically plausible words, while higher layers encode more factual knowledge. Therefore, DoLa devises a contrastive decoding strategy by subtracting the lower logits from the last layer's logits and using the results for next-token prediction. ITI~Li-2023-nips-ITI finds that specific attention heads show high linear probing accuracy and regards their activation as truth-correlated directions. During inference, certain heads' activations would be shifted along these pivot directions. Some other work introduces external knowledge sources to aid the decoding process. CAD~Shi-arxiv-2023-Trusting provides LLMs with extra context about the query, and then contrasts the output probabilities by those without using context, {thereby adjusting the influence of the model's prior knowledge. æœ‰ç‚¹å¤¸å¼ äº†ï¼Œä¸å¯èƒ½è¦†å†™å§ï¼Ÿ} KCTS~Choi-arxiv-23-KCTs applies an auxiliary knowledge classifier on top of the LLM to detect hallucinations, and uses its knowledge faithfulness score to reweight the token distribution.",
      "origin_cites_number": 12
    },
    {
      "section_title": "Complex Reasoning",
      "level": "2",
      "content": "In this section, we introduce a new reasoning paradigm for LLMs aimed at solving complex tasks by allocating more time to thinking before responding to a problem, \\ie conducting complex reasoning. Specially, we focus on long chain-of-thought~(CoT) reasoningThe phrase ``\\emph{long CoT'' may not be conceptually precise since the model's thought process could be tree- or graph-structured rather than strictly linear. We use this terminology in line with OpenAI's introduction of the o1 model, which generally refers to extended thought processes for complex reasoning.}, which is the mainstream approach taken by recent large reaonsing models, such as OpenAI's o-series models. We will begin by providing an overview of long CoT reasoning, then introduce the construction of long CoT data and the corresponding training methods, and finally discuss more general test-time scaling methods.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Overview and Analysis",
      "level": "3",
      "content": "Generally, long CoT reasoning is a method to search for solutions within the natural language space, as reflected in the output responses of LLMs. This approach is akin to the slow thinking mode of the human brain~kahneman-2011-thinking-book, which takes significantly more time to think through difficult problems compared to the fast thinking mode used for simpler ones. This subsection will first qualitatively analyze the reasoning patterns and then briefly discuss the main advantages of this reasoning mode. Reasoning Patterns Analysis. As demonstrated in Example~example-long-cot, existing long CoT reasoning models typically generate an extended thought process (in grey) before arriving at the final answer (in italic). It is crucial to understand how this thought process is conducted and the types of reasoning patterns generated by LLMs during problem-solving. To provide an intuitive understanding of this reasoning process, we present two examples from the DeepSeek-R1 model. In the first example, we present a mathematical problem to the model, and the corresponding long CoT can be observed in the reasoning portion of the response. The thought process here is informal and flexible, while showcasing a systematic exploration of the solution within the natural language space. Concretely, the model follows a complete reasoning process consisting of action steps like ``factorize 196'' and``take the exponents''. Notably, the thought process naturally includes trigger keywords like ``double check'' and ``wait'', which invoke the corresponding verification or reflection actions. In the second example, we ask the model which Chinese city has the largest population. Interestingly, it exhibits similar thought patterns, even though the question could be addressed in a more compact and straightforward manner. The model generates a comprehensive reasoning process with actions such as ``confirm the latest data'' and ``clarify'', with trigger keywords like ``make sure'' and ``avoid''. To gain a more comprehensive understanding of this complex reasoning mode, some research has further analyzed the reasoning patterns exhibited in the o1 model~wu-2024-comparative-arxiv. These studies, based on empirical investigation, have identified several key reasoning patterns, including systematic analysis, method reuse, divide-and-conquer, self-refinement, context identification, and constraint emphasis. Additionally, the use of these reasoning patterns varies across different tasks, significantly enhancing cognitive processes compared to standard CoT reasoning. Reasoning Advantages. Unlike standard CoT reasoning, this approach does not enforce a linear reasoning chain. Instead, it integrates various reasoning actions and strategies, such as reflection and backtracking, into a single response. Overall, it has two major advantages compared to the standard CoT method or direct prompting methods. Firstly, due to the autoregressive nature, the standard generation paradigm of LLMs is a ``one-time'' reasoning process. This means that if the generated solution contains obvious mistakes, or even if LLMs are aware of other promising solutions, there are no opportunities for refinement or verification. This issue becomes more pronounced in complex reasoning tasks, where the search space is much larger, preventing LLMs from fully exploring it zhong-2024-evaluation-arxiv. In contrast, long CoT reasoning mitigates this problem by allowing the model to autonomously check and revise its attempts, thus enabling more effective reasoning. Secondly, this text-based reasoning process can, in principle, emulate various search algorithms that rely on more complex search structures. For example, to represent a tree-structured search space, one might employ a textual process that combines forward exploration with backward revisits, incorporating necessary reflection and verification steps along the way. Consequently, long CoT reasoning can replicate the effects of previously introduced methods like tree-of-thought (ToT) and graph-of-thought (GoT). However, this capability is not inherently present in the LLM; it emerges in a manner similar to the standard CoT ability, developing through appropriate training (see Section~subsec-long-cot-training). Overall, long CoT represents a significant different reasoning mode compared to the standard CoT method, facilitating search algorithms within the natural language space of LLMs. It emphasizes how to navigate correct paths through a trial-and-error approach, typically incorporating critical reasoning actions such as planning, evaluation, reflection, and exploration. In contrast, short CoT data typically presents a direct solution process in which all reasoning steps are expected to be correct.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Construction of Long CoT Data",
      "level": "3",
      "content": "To guide LLMs in producing long-form reasoning followed by solutions, it is crucial to curate high-quality long CoT data for warming up or training the models. While human annotators can construct extended CoT data, this process is costly and requires professional expertise for challenging problems, making it difficult to scale. Consequently, existing studies often develop various methods for automatically constructing long CoT data, such as distillation from more advanced models, search based data synthesis, and multi-agent collaboration, which are detailed below. Long CoT Data Distillation. Benefiting from the openness of o1-like LLMs endowed with powerful reasoning capabilities, the leading approach to curating high-quality long CoT data involves using open models or APIs for data synthesis. The basic idea is to first construct a set of prompts (\\ie problems) and then feed them into the teacher model to collect the corresponding long CoT response data. Specifically, STILL-2~Min-arxiv-2024-Imitate utilizes two slow-thinking systems, \\ie DeepSeek-R1-Lite-Preview~DeepSeek-arxiv-2024-Deepseek and QwQ-32B-preview~Qwen-arxiv-2024-Qwq for distillation to construct a dataset of long-form thought processes. A key finding is that length distribution is a critical factor in determining the quality of long CoT data. They suggest that length directly reflects the difficulty of prompt problems, with mathematical problems being particularly important to collect, as they often involve extensive thought processes in their solutions. The research shows that even a small amount of carefully curated long CoT data can effectively activate the slow-thinking mode in LLMs. Furthermore, this effect is corroborated by the work on DeepSeek-R1~Deepseek-arxiv-2025-DeepSeek, which demonstrates that training with distilled data from DeepSeek-R1 consistently enhances the performance of multiple Qwen and Llama models. Search based Data Synthesis. Search algorithms like Monte Carlo Tree Search (MCTS) Silver-nat-2017-Mastering have been widely applied to synthesizing long-form reasoning data. As a representative technique, MCTS integrates the principles of tree exploration and random simulation to estimate potential outcomes of actions, making it particularly effective for decision-making tasks with large action spaces. In complex problem-solving, MCTS decomposes the process into multi-step generation, with each node at a specific tree layer representing a step in the solution~jiang-2024-arxiv-still1. At each step, a LLM, serving as the policy model, samples several candidate nodes, each generating a one-step CoT. MCTS extensively uses rollouts to automatically assign a Q-value to each intermediate step based on its contribution: steps potentially leading to more trajectories that correctly solve the problem receive higher Q-values. After iterating through multiple steps to successfully address the problem, the complete reasoning trajectories from the root node to the terminal node can be viewed as long-form CoT data, where intermediate nodes represent either correct reasoning steps or trial-and-error attempts. Multi-Agent Collaboration. Beyond relying on a single model, an alternative approach to generating long CoT data is to construct a multi-agent framework~Liang-arxiv-2023-Encouraging in which several models collaborate or debate to produce long-form reasoning data. The multi-agent framework for synthesizing long-form CoT data typically involves the coordination of multiple autonomous agents, each specializing in distinct roles or functions. These agents work together using iterative reflection and strategic debate to enhance the reasoning process. Within this framework, one agent might initiate a chain of thought by presenting an initial hypothesis or argument, while others critique and challenge these ideas through logical examination and counter-arguments. This process encourages deep reflection by prompting agents to reconsider assumptions, address potential biases, and refine conclusions through continuous discourse. In this context, reflection involves not only reconsidering past decisions but also assessing whether each agent's contribution is grounded in logical consistency. Additionally, the debate mechanism incorporates alternative perspectives and counterarguments into the reasoning process, resulting in more robust and nuanced outcomes for complex decision-making tasks. By combining these cognitive processes, the framework fosters an environment where complex problems can be tackled collaboratively, with diverse viewpoints contributing to more comprehensive solutions.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Training Methods",
      "level": "3",
      "content": "To elicit and enhance long CoT reasoning capabilities, the existing literature extensively explore two methods: long CoT instruction tuning and scaling reinforcement learning (RL) training. We will describe each approach in detail below. Long CoT Instruction Tuning. As discussed in Section~subsec-long-cot-intro, long-form thought processes require models to engage in extended reasoning before responding. To develop this reasoning capability, we can instruction-tune LLMs using carefully curated long CoT data. The core concept is to train LLMs to ``imitate'' the demonstrated behaviors presented in the long CoT data. In general, this fine-tuning method aims to achieve two key objectives: format adherence (\\ie following a long CoT format) and ability elicitation (\\ie activating the complex reasoning mode). Specifically, format adherence requires the model to produce outputs consisting of two sequential partsâ€”thought and solutionâ€”while ability elicitation activates the model's inherent capacities for executing appropriate long-form thought processes. It has been shown that both objectives can be effectively achieved through supervised fine-tuning: a small amount of high-quality long CoT data can suffice to elicit the long CoT reasoning capabilities of LLMs. For instance, by fine-tuning Qwen2.5 (32B) on just 3.9K distilled long CoT data, STILL-2 Min-arxiv-2024-Imitate achieved performance comparable to industry counterparts such as o1-preview and QwQ in mathematical problem-solving. This effectiveness is largely because strong LLMs inherently possess various specific reasoning abilities (\\eg reflection and backtracking). Instruction tuning with long CoT data further enhances these innate abilities, comprehensively integrating and extending their utilization, which enables the model to manage more complex reasoning processes. An interesting finding is that this reasoning capability appears to generalize well across different domains. For example, when trained exclusively on mathematical data, it can lead to significant improvements in other disciplines, such as physics and chemistry~Min-arxiv-2024-Imitate. This is primarily because long CoT reasoning is inherently a reasoning mode rather than a specific ability tied to any particular domain. This can be seen in the example shown in Example~example-long-cot, where the query, ``Which city in China has the largest population?'', is answered through a complex thought process, despite being solvable in a more straightforward manner. Moreover, this capability can be naturally extended to multimodal LLMs, as these models are typically built on the backbone of language models~Du-arxiv-2025-Virgo. Furthermore, this training approach can be naturally enhanced by other supervised training strategies~Min-arxiv-2024-Imitate, such as rejection sampling and directional preference optimization. In general, one can begin by warming up a LLM through instruction tuning with long CoT instruction data and then use the model itself to generate rollout samples as training data. These enhancements can have a certain effect, particularly when the amount of warmup instruction data is limited. However, their impact tends to diminish when sufficient long CoT instruction data is available, especially if the quality of self-generated samples is not superior to that of the demonstration data~Min-arxiv-2024-Imitate. These findings suggest that this advanced capability of a model may quickly reach a performance ceiling when trained through supervised fine-tuning, due to the inherent limitations typical of imitation learning (for further discussion, see Section~sec-longcot-discussion). Another downside of this fine-tuning method is its tendency to default to long CoT reasoning mode even for simpler problems (See Example~example-long-cot). To better manage reasoning behavior, it is essential to explore systematic approaches that integrate both long CoT reasoning mode and standard response mode. Scaling RL Training. Although OpenAI has not disclosed technical details about the o-series models, training methods have been published through initiatives that implement long chain-of-thought (CoT) reasoning systems, such as DeepSeek-R1~Deepseek-arxiv-2025-DeepSeek and Kimi-K1.5~Kimi-arxiv-2025-Kimi, which have demonstrated performance comparable to o1. The technical methods employed converge on the approach of scaling RL training to enhance the complex reasoning capabilities of LLMs. In the following part, we introduce the detailed RL method through three components: the policy model, the reward model, and the RL training algorithm. $\\bullet$ Policy model. The policy model refers to the LLM that needs to be enhanced by the complex reasoning capacities. Typically, it should be warmed up through supervised fine-tuning with long CoT data, as outlined in the aforementioned method. The main purpose of this warm-up is to activate the long CoT reasoning mode, enabling the policy model to conduct appropriate explorations using a long-form thought process. It is also recommended that the policy model possesses strong foundational capabilities, as this is crucial for eliciting high-reward actions in a more efficient way. An interesting attempt taken by DeepSeek-R1-Zero is to omit the supervised fine-tuning step. Instead, it leverages its strong instruction-following capacity to adhere to the response format and reasoning mode, guiding the model to generate formatted responses comprising two parts: thought and answer. This method uses a format reward to reinforce the correct reasoning mode. $\\bullet$ Reward model. To effectively guide the policy model, it is necessary to set an appropriate reward model in RL algorithms. As discussed in Section~sec-alignment, RLHF employs a specially trained reward model to instruct the learning of the policy model. However, this approach has become less effective for long CoT reasoning, given the difficulty of training reliable reward models to assess the quality of long CoT reasoning processes. Consequently, existing approaches typically employ a verifiable reward model primarily built on reference answers (\\eg mathematical problems) or test samples (\\eg coding problems). Typically, the mathematical domain serves as the major source of training data, where problems with specific answers are selected. The ground-truth answer is used to derive the reward scores, such as 1 for a correct solution and 0 for an incorrect solution. This might seem counterintuitive: how can a complex reasoning system be effectively developed using such a simple reward model? The explanation lies in the essence of RL: unlike supervised fine-tuning, it encourages the autonomous explorations of models through simple yet appropriate incentives. In this way, the complex reasoning capability can be well internalized within the model. In addition to the accuracy reward, other simple rewards can be considered, including completeness, avoidance of excessively long texts, and other formatting issues like repetition. OpenAI has proposed the reinforcement fine-tuning (ReFT)~OpenAI-openai-2024-Reinforcement approach for tuning the o-series models to build domain-specific models, which also uses a simple accuracy reward to guide the training. One limitation of this reward model is that it can only utilize problems with definite and concise answers for training. More general task data, such as summarization, cannot be directly used for training. In such cases, incorporating a trainable reward model becomes necessary. However, as we have discussed, once this reasoning mode is elicited in specific domains, it can naturally generalize well across different domains. $\\bullet$ RL Training. After configuring the policy and reward models, suitable RL algorithms are selected to train the policy model~zeng2024scaling, chen-2025-still3. In Section~sec-alignment, we provide a detailed implementation of the PPO algorithm, which can be applied directly for training such models. Nonetheless, PPO requires the maintenance and updating of an additional value model, which leads to high training costs, especially when scaling RL training. As a result, existing approaches~Deepseek-arxiv-2025-DeepSeek often prefer more simplified RL algorithms, such as GRPO~Shao-arxiv-2024-Deepseekmath and RLOO~Kool-ICLR-2019-Buy, which use heuristic methods to eliminate the need of a value model. These algorithms typically exhibit higher efficiency and demonstrate strong training performance, especially in long CoT reasoning. A critical factor to monitor during RL training is the response length of the reasoning models, as a longer average response length often corresponds to enhanced reasoning capabilities. Therefore, it is important to track the trends in average response lengths. With appropriate training, the model should show progressively longer response lengths, accompanied by simultaneous performance improvements. In fact, response length is directly connected to the test-time scaling law demonstrated by OpenAI\\url{https://openai.com/index/learning-to-reason-with-llms/}. This law suggests that as more output tokens are generated, a model's reasoning performance can improve substantially. Nonetheless, achieving stable and effective RL training is challenging and necessitates consideration of various factors, such as the selection of query problems (\\eg choosing problems that are challenging yet solvable by the model), the updating of the reference model (\\eg continually updating it as training progresses), and the enhancement of exploration strategies (\\eg sampling more responses with higher temperature settings).",
      "origin_cites_number": 12
    },
    {
      "section_title": "Extended Discussion",
      "level": "3",
      "content": "In the preceding discussions, we have introduced the long CoT reasoning in technical detail. Actually, it can be considered a specific approach to achieve test-time scaling (\\aka inference-time scaling), which is the focus of this subsection. From a broader perspective, test-time scaling encompasses various approaches that enhance model performance by increasing the outputs or computations from LLMs. In this way, many methods can be considered test-time scaling techniques. For example, Self-Consistency~Wang-arxiv-2022-Self-Consistency generates multiple responses and then aggregates the solutions using majority vote, resulting in higher inference costs due to the increased number of rollouts. Additionally, planning techniques (Section~subsec-planning) and their agentic instantiations (Section~sec:llm_based_agent) can also be considered test-time scaling approaches, as they involve prompting LLMs multiple times and utilizing tools or memory components. Therefore, the essence of test-time scaling is to trade additional inference costs for performance gains. Unlike previous approaches, long CoT reasoning directly searches for solutions within the natural language space, notably within a single response. When comparing different test-time scaling methods, two critical factors require careful examination: token efficiency (the performance improvement per token cost) and performance ceiling (the maximum attainable performance). Research has shown that scaling test-time computation can effectively enhance model performance~Charlie-arxiv-2024-Scaling,Deepseek-arxiv-2025-DeepSeek through the use of simple aggregation methods or specially trained models, though token efficiency may vary. Overall, scaling RL training tends to exhibit higher token efficiency compared to existing test-time scaling methods~Deepseek-arxiv-2025-DeepSeek. Additionally, both heuristic methods and supervised fine-tuning often exhibit a relatively limited performance ceiling that cannot be substantially elevated once scaling reaches a certain level~Min-arxiv-2024-Imitate,Charlie-arxiv-2024-Scaling. In contrast, scaling RL training can lead to continuous performance improvements in reasoning models as training time increases. For example, DeepSeek-R1-Zero demonstrates a consistent upward trend in performance even after more than 8,000 training steps~Deepseek-arxiv-2025-DeepSeek. These scaling effects are crucial for solving complex tasks. Notably, a potential advantage of long CoT reasoning models is that they make it feasible to develop expert-level models in specialized domains or for specific tasks, which could significantly impact the advancement of scientific research challenges. Moreover, as inference methods and hardware techniques improve, the deployment and use cost of these models will be significantly reduced, enhancing the contribution of these highly intelligent models to real-world applications. Additionally, addressing security issues in long CoT reasoning models is crucial. Given their unique reasoning mode, specialized alignment strategies should be developed to ensure safer use of these models.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Conclusion and Future Directions",
      "level": "1",
      "content": "In this survey, we have reviewed the recent progress of large language models~(LLMs), and introduced the key concepts, findings, and techniques for understanding and utilizing LLMs. We focus on the large-sized models (\\ie having a size larger than 10B) while excluding the contents of early pre-trained language models (\\eg BERT and GPT-2) that have been well covered in the existing literature. In particular, our survey has discussed four important aspects of LLMs, \\ie pre-training, adaptation, utilization, and evaluation. For each aspect, we highlight the techniques or findings that are key to the success of LLMs. Furthermore, we also summarize the available resources for developing LLMs and discuss important implementation guidelines for reproducing LLMs. This survey tries to cover the most recent literature about LLMs and provides a good reference resource on this topic for both researchers and engineers. Next, we summarize the discussions of this survey, and introduce the challenges and future directions for LLMs, in the following aspects. Basics and Principles. Instead of training on specific task goals, LLMs learn from unsupervised pre-training on large-scale text data. This is quite different from previous multi-task learning approaches, which aim to extend the training tasks as possible to achieve sufficient generalization. Thus, it is essential to reveal the basic principles or elements that establish the foundation of the abilities of LLMs. Although the basic idea of language models is intuitive, it is still challenging to formally explain why LLMs trained by simple language modeling objectives (\\eg next token prediction) can become capable of solving various real-world tasks. To investigate this problem, a promising approach is to study the capacity learning (or selection) mechanism based on unsupervised pre-training, since the model capacity of LLMs strongly depends on pre-training data. In addition, scaling plays an important role in improving the capacity of LLMs~Brown-NeurIPS-2020-Language,Wei-arxiv-2022-Emergent,Rae-arxiv-2021-Scaling, and it is very useful to conduct more theoretical analysis about how the behaviors of large models relate to those of small models, \\eg what behaviors of large models can be inferred from small models and what can't be predicted indeed. Another research direction is to explore more deep analysis on model generalization for LLMs, since increasing concerns have been raised about whether LLMs can generalize beyond the knowledge encoded by pre-training data. Furthermore, data contamination has become a severe issue for fairly assessing the performance of LLMs~zhou-arxiv-2023-dont, and thus setting appropriate evaluation protocol will be the basis to investigate and analyze the model capacity of LLMs. Model Architecture. Due to the scalability and effectiveness, Transformer has become the de facto architecture for building LLMs. % Various strategies have been proposed to improve the performance of this architecture, such as neural network configuration and scalable parallel training (see discussions in Section~sec:configuration). However, Transformer still suffers from high training costs and slow inference rates. More efforts~peng-2023-arxiv-rwkv,sun-2023-arxiv-retnet are still in need to develop improved model architectures for large-scale pre-training. Specially, system-level or hardware-level optimization (\\eg FlashAttention~Dao-2023-arxiv-flashattention2) is worth more exploration to improve the efficiency of Transformer architectures. % In addition, as an important basic capacity, existing LLMs typically maintain a long context window. For example, the most recent GPT-4 Turbo enables a long context of 128K tokens, and Claude 2.1 also supports the input up to 200K tokens. Although many efforts have been made to enhance the long context modeling ability of LLMs~su-online-2023-Rerope,Press-ICLR-2022-Train, the resulting models still can't well process the information in the context window~Liu-arxiv-2023-Lost. To address this issue, specific architecture adaptations or algorithms might be needed to enhance the modeling and utilization of long context information. Another worrying concern is that existing work mostly focuses on training LLMs with decoder-only Transformers. Despite the effectiveness, it severely limits the more wide, diverse explorations on alternative model architectures. Model Training. For pre-training, it is essential to establish a data-centric infrastructure and training procedure for LLM optimization, which can effectively support a systematic process of data collection, data cleaning, data mixture, and data curriculum. Furthermore, it also calls for more flexible mechanisms of hardware support or resource schedule, so as to better organize and utilize the resources in a computing cluster. In practice, it is very challenging to pre-train capable LLMs, due to the huge compute consumption and the sensitivity to data quality and training tricks~Zeng-arxiv-2022-GLM,Scao-arxiv-2022-BLOOM. Thus, it becomes particularly important to develop systemic, economical pre-training approaches for optimizing LLMs, \\eg predictable scaling~OpenAI-OpenAI-2023-GPT-4 and proxy model training~Xie-arxiv-2023-doremi. % More training recipes or principles should be investigated and shared to reduce the potential risk of degradation or failure in large-scale model optimization. Although increasingly more model checkpoints and cleaned datasets have been released, there still lacks reproducible work on pre-training data preparation (\\eg detailed cleaning strategies) and data scheduling (\\eg data mixture and curriculum). % Since it is very costly to pre-train a LLM from scratch, it is important to design suitable mechanisms for continually pre-training or fine-tuning the LLM based on publicly available model checkpoints (\\eg LLaMA~Touvron-arxiv-2023-LLaMA and Flan-T5~Chung-arxiv-2022-Scaling). For this purpose, a number of technical issues have to be resolved, \\eg catastrophic forgetting and task specialization. {Furthermore, it is also useful to develop effective tuning strategies that effectively inject or edit specific knowledge~Yao-arxiv-2023-Editing, \\eg correcting the outdated facts.} Model Utilization. Based on the natural language interface, prompting has become the prominent approach for using LLMs to solving various tasks. % By combining task descriptions and demonstration examples into prompts, in-context learning~(ICL) endows LLMs with the ability to perform well on new tasks, even outperforming full-data fine-tuned models in some cases. To enhance the ability of complex reasoning, advanced prompting techniques have been proposed, exemplified by the chain-of-thought~(CoT) strategy, which includes the intermediate reasoning steps into prompts. Furthermore, planning is a promising approach for solving complex tasks, which iteratively invokes LLMs by leveraging tool use capacities. Despite these efforts, several basic problems related to prompting are still under-explored: why a good prompt can elicit the correct answer but a bad prompt cannot, how to reveal the working principles of advanced prompting methods (\\eg ICL and CoT) and further improve these existing approaches, and how to efficiently find the effective prompts for LLMs on specific tasks. However, existing prompting approaches still have several deficiencies described as follows. Firstly, it involves considerable human efforts in the design of prompts. It would be quite useful to automatically generate effective prompts for solving various tasks. Secondly, some complex tasks (\\eg formal proof and numerical computation) require specific knowledge or logic rules, which may not be well expressed in natural language or demonstrated by examples. Thus, it is important to develop more informative, flexible task formatting methods for prompts\\footnote{It seems that an alternative approach to this issue is to invoke external tools, \\eg the plugins for ChatGPT, when the task is difficult to solve via text generation.. Thirdly, existing prompting strategies mainly focus on single-turn performance. It is useful to develop interactive prompting mechanisms (\\eg through natural language conversations) for solving complex tasks, which have been demonstrated to be very useful by ChatGPT. % } Furthermore, from a practical perspective, it has become a fundamental challenge to reduce the inference cost of LLMs, especially in large-scale deployment. Another popular research direction is retrieval-augmented generation, where retrieved contexts from supporting sources are included into prompts for task solving. It has been shown that retrieval augmentation can extend the knowledge boundary and improve the question answering capacity~Ren-arxiv-2023-Investigating, but may suffer from the effectiveness of long context utilization by LLMs~Liu-arxiv-2023-Lost. % Safety and Alignment. Despite the capacities, LLMs are faced with great safety challenges in practical use. As a fundamental issue of probabilistic modeling nature, LLMs exhibit a tendency to generate hallucinations~Bang-arxiv-2023-A, referring to texts that seem plausible but may be factually incorrect~OpenAI-OpenAI-2023-GPT-4. What is worse, LLMs might be elicited by intentional instructions to produce harmful, biased, or toxic texts for malicious systems, leading to the potential risks of misuse~Brown-NeurIPS-2020-Language,Ouyang-arxiv-2022-Training. To have a detailed discussion of the safety issues of LLMs (\\eg privacy, overreliance, disinformation, and influence operations), the readers can refer to the {GPT-3/4 technical reports~OpenAI-OpenAI-2023-GPT-4,Brown-NeurIPS-2020-Language. % As the major technical approach to averting these issues, alignment methods (\\eg RLHF)~Ouyang-arxiv-2022-Training,Glaese-arxiv-2022-Improving have been widely used by leveraging human feedback for developing well-aligned LLMs. However, RLHF heavily relies on high-quality human feedback data from professional labelers, which is costly and time-consuming to recruit qualified human annotators. % Therefore, it is necessary to improve the RLHF framework for reducing the efforts of human labelers and seek a more efficient annotation approach with guaranteed data quality, \\eg LLMs can be employed to assist the labeling work. Furthermore, it is also suggested to develop simplified optimization algorithms for alignment~Rafailov-arxiv-2023-Direct,Guo-arxiv-2023-Beyond, to reduce the training difficulty and unstability of RLHF. As another practical approach, red teaming~Ganguli-arxiv-2022-Red,Perez-EMNLP-2022-Red has been adopted for improving the model safety of LLMs, which utilizes the collected adversarial prompts to refine the LLMs (\\ie avoiding the attacks from red teaming). In addition, privacy concerns are also important to consider when fine-tuning LLMs with domain-specific data, and thus federated based learning~Kuang-2023-arxiv-FederatedScope can be useful in privacy-restricted scenarios. Application and Ecosystem. As LLMs have shown strong capacities in solving various tasks, they can be applied in a broad range of real-world applications (\\ie following task-specific natural language instructions). % As a remarkable progress, ChatGPT has potentially changed the way how humans access information, which has been additionally integrated in the release of New Bing. Generally, in the near future, it can be foreseen that LLMs would have a significant impact on information-seeking techniques, including both search engines and recommender systems. Furthermore, LLMs make it possible to develop more intelligent systems (\\eg autonomous AI agents) to tackle various complex tasks in real-world scenarios. Specially, Assistants API has been launched by OpenAI (featured by instructions, knowledge and tool use), enabling rapid development of agent-like assistants within the applications. This wave of technical innovation would lead to an ecosystem of LLM-empowered applications (\\eg OpenAIâ€™s GPT Store), which has a close connection with human life. Lastly, the rise of LLMs sheds light on the exploration of artificial general intelligence~(AGI). It is promising to develop more smart AI systems than ever. However, in this development process, AI safety should be one of the primary concerns, \\ie making AI lead to good for humanity but not bad~OpenAI-blog-2023-Planning.",
      "origin_cites_number": 23
    },
    {
      "section_title": "\\textsc{Coda",
      "level": "1",
      "content": "} It is not an easy job to write this long survey and update its content with timely work. First of all, we would like to sincerely thank the support from the readers and our team members. We work very hard on this survey, and hope that it can present a comprehensive, timely reference for LLMs. Survey Writing. This survey was planned during a discussion meeting held by our research team, and we aimed to summarize the recent advances of large language models as a highly readable report for our team members. The first draft was finished on March 13, 2023, in which our team members tried their best to include the related studies about LLMs in a relatively objective, comprehensive way. Then, we have extensively revised the writing and contents in several passes. Due to the space limit, we can only include a fraction of existing LLMs in Figure~fig:llms_timeline and Table~tab:resource_model, by setting the selection criterion. However, we set a more relaxed criterion for model selection on our GitHub page (https://github.com/RUCAIBox/LLMSurvey), which will be regularly maintained. We release the initial version on March 31, 2023, the major revision on June 29, 2023, and second version on September 10, 2023, and this latest version (major revision) on November 23, 2023. Seeking for Advice. Despite all our efforts, this survey is still far from perfect: we are likely to miss important references or topics, and might also have non-rigorous expressions or discussions. We will continuously update this survey, and improve the quality as much as we can. For us, survey writing is also a learning process for LLMs by ourselves. For readers with constructive suggestions to improve this survey, you are welcome to leave comments on the GitHub page of our survey or directly email our authors. We will make revisions following the received comments or suggestions in a future version, and acknowledge the readers who have contributed constructive suggestions in our survey. Update log. In this part, we regularly maintain an update log for the submissions of this survey to arXiv: itemize \\item First release on March 31, 2023: the initial version. \\item Update on April 9, 2023: add the affiliation information, revise Figure~fig:llms_timeline and Table~tab:resource_model and clarify the corresponding selection criterion for LLMs, improve the writing, and correct some minor errors. \\item Update on April 11, 2023: correct the errors for library resources. \\item Update on April 12, 2023: revise Figure~fig:llms_timeline and Table~tab:resource_model, and clarify the release date of LLMs. \\item Update on April 16, 2023: add a new Section~sec-GPT-series about the technical evolution of GPT-series models. \\item Update on April 24, 2023: add the discussion about scaling laws and add some explanations about the model sizes for emergent abilities (Section~sec-background); add an illustrative figure for the attention patterns for different architectures in Figure~fig:architectures, and add the detailed formulas in Table~tab:detailed_configuration. \\item Update on April 25, 2023: revise some copy errors in figures and tables. \\item Update on April 27, 2023: add efficient tuning in Section~sec-PEFT. \\item Update on April 28, 2023: revise Section~sec-PEFT. \\item Update on May 7, 2023: revise Table~tab:resource_model, Table~tab:corpora, and some minor points. \\item {Update on June 29, 2023 (major revision): } itemize \\item Section~sec:introduction: add Figure~fig:paper_number for the trends of published LLM papers in arXiv; \\item Section~sec-overview: add Figure~fig:openai for GPT's evolution and the corresponding discussion; \\item Section~sec-resource: add Figure~fig:llama_family for LLaMA family and the corresponding discussion; \\item Section~sec-adaptation: add latest discussion about the synthetic data formatting of instruction tuning in Section~sec-instruction-formatted, the empirical analysis for instruction tuning in Section~instruction-results, parameter-efficient model adaptation in Section~sec-PEFT and memory-efficient adaptation in Section~sec-PEFT; \\item Section~sec-utilization: add latest discussion about the underlying mechanism of ICL~sec-ICL-mechanism, planning for complex task solving in Section~subsec-planning; \\item Section~sec-evaluation: update Table~tab:dataset for representative datasets for evaluating advanced abilities of LLMs, and empirical ability evaluation in Section~sec-empirical; \\item Section~subsec:promptdesign: add prompt design;% \\item Section~sec-application: add the discussions on applications of LLMs in finance and scientific research domains; itemize \\item {Update on September 10, 2023 (major revision): } itemize \\item {Claim the copyrights of the figures and tables in this paper.} \\item % {Add latest LLMs, techniques and their descriptions in Section~sec-resource, Section~sec-pretraining, Section~sec-adaptation, Section~sec-utilization and Section~sec-evaluation;} \\item {Section~sec-pretraining: add latest discussion about the decoding strategy in Section~sec-decoding;} \\item {Section~sec-adaptation: add latest discussion about the practical tricks for instruction tuning in Section~sec-ituning-strategy, the empirical analysis on LLaMA (13B) for instruction tuning in Section~instruction-results, practical strategies for RLHF in Section~sub:RLHF, alignment without RLHF in Section~sec-alignment-withoutRL and remarks on SFT and RLHF in Section~sec-remarks-SFTRL;} \\item {Section~sec-utilization: update the content about the planning for complex task solving in Section~subsec-planning;} \\item {Section~sec-evaluation: add discussions about evaluation approaches in Section~subsec-evaapp, Table~tab-category-evaluation for the category of existing evaluation work, and update empirical ability evaluation in Section~sec-empirical and the results on Table~tab-experimental-res;} \\item {Section~subsec:promptdesign: add new prompt examples in Table~tab-tips;}% itemize \\item {Update on November 23, 2023 (major revision): } itemize \\item % { Section~sec:introduction: add Figure~fig:task_solvers for the evolution process of four generations of language models;} \\item % {Section~sec-overview: add more discussion about scaling laws and how emergent abilities relate to scaling laws;} \\item {Section~sec-resource: add latest LLMs in Figure~fig:llms_timeline and Table~tab:resource_model, latest APIs in Section~sec:apis_for_llms, commonly used datasets for instruction tuning and alignment tuning in Section~sec:commonly_used_fituning, and several libraries in Section~sec:library;} \\item {Section~sec-pretraining: add latest discussion about the data scheduling, including data mixtures and data curriculum in Section~sec:data_scheduling; add summary of data preparation in Section~sec:data_prepare_sug; add discussion about modeling long context in Section~sec:long_context; add discussion about decoding efficiency issues and add latest decoding strategies in Section~sec-decoding;} \\item {Section~sec-adaptation: add latest discussion about instance construction and tuning strategies in Section~sec-instruction; add latest discussion about process-supervised RLHF in Section~sub:RLHF, and the empirical study on quantized LLaMA models (7B and 13B) in Section~sec:quantization_empirical;} \\item {Section~sec-utilization: add latest discussion about prompt optimization in Section~sec:prompt_opt, and update the content about chain-of-thought prompting in Section~subsec-cot;} \\item {Section~sec-application: add latest discussion about LLM for research directions in Section~sec:llm4community;} \\item {Section~sec-con: revise the content in the several aspects.} itemize \\item {Update on September 25, 2024: } itemize \\item {Section~sec-resource: reorganize the content of ``public available model checkpoints'' into multiple series; add the latest LLMs in Figure~fig:llms_timeline.} \\item {Section~sec-pretraining: add LLM-based data filtering and selection methods in Section~sec:data_pre_processing; update Section~sec:archs, ``Emergent Architectures'' to include more discussions about SSM-based architectures; add Table~tab-new-architectures to compare parallelism and complexity of different architectures.} \\item {Section~sec-adaptation: add latest discussion about instruction quality improvement and instruction selection in Section~sec-instruction-formatted; add latest discussion about practical strategies for RLHF and process-supervised RLHF in Section~sub:RLHF; update the content about supervised alignment tuning in Section~sec-alignment-withoutRL.} \\item {Section~sec-utilization: add latest papers about discrete prompt optimization in Section~sec:prompt_opt.} \\item {Section~sec-advanced-topics: add latest discussion about advanced topics, including long context modeling, LLM-based agent, analysis and optimization for training and inference, model inference, model compression, retrieval-augmented generation, and hallucination.} itemize \\item {Update on October 12, 2024: } itemize \\item Section~sec-KGLLM: correct the errors. itemize \\item {Update on March 11, 2025: } itemize \\item Section~sec-long-cot: add latest papers about long CoT reasoning, including the analysis of reasoning patterns and advantages, construction of long CoT data (\\ie distillation, search-based, and multi-agent collaboration), and training methods (\\ie instruction tuning and reinforcement learning). itemize itemize Clarifications on Experiments. In this version, we have included a number experiments on instruction-tuning (Table~tab-instruction-tuning-res), overall ability evaluation (Table~tab-experimental-res), and prompt engineering (Table~tab-instructions). Due to the limit of computational resources, our experiments are not complete, limited to small-sized models or a few comparisons. Despite that, we feel that it might be meaningful to share the partial results to the public. We will try to include the missing results of larger models or more comparisons in the future versions. We also call for support of computing power for conducting more comprehensive experiments. Chinese Book. We also released a Chinese book based on this survey article, at the link: https://llmbook-zh.github.io. This book is in the publication process. \\ifCLASSOPTIONcompsoc",
      "origin_cites_number": 0
    },
    {
      "section_title": "Acknowledgments",
      "level": "1",
      "content": "\\else",
      "origin_cites_number": 0
    },
    {
      "section_title": "Acknowledgment",
      "level": "1",
      "content": "\\fi The authors would like to thank Yankai Lin and Yutao Zhu for proofreading this paper. Since the first release of this paper, we have received a number of valuable comments from the readers. We sincerely thank the readers who have written to us with constructive suggestions and comments: Tyler Suard, Damai Dai, Liang Ding, Stella Biderman, Kevin Gray, Jay Alammar, Yubo Feng, Mark Holmstrom, Xingdong Liu, Il-Seok Oh, Yiting Liu, Shaojun Wang, Gaoyan Ou, Todd Morrill, Hao Liu, Zhenyu Zhang, and Xinlin Zhuang. \\\\ Since the v11 version (June 29, 2023), we have been adding a large number of experiments and prompt practices. These new contents are completed by a number of volunteers in our team. Here, we add a special part to thank all the students who have worked very hard on this part (also including the ones on our author list). Contribution on Experiments. We would like to sincerely thank the following people for their hard work involved in experiments shown in Table~tab-experimental-res. $\\bullet$ Xiaoxue Cheng: implement the experiments for evaluation on Language Generation and HaluEval tasks. $\\bullet$ Yuhao Wang: implement the experiments for evaluation on interaction with environment tasks. $\\bullet$ Bowen Zheng: implement the experiments for evaluation on tool manipulation tasks. Contribution on Tips. We list the following guys for their contributions on the corresponding numbers of provided tips for designing prompts in Table~tab-tips. $\\bullet$ Xiaolei Wang: T3, O3 $\\bullet$ Beichen Zhang: D2, D5 $\\bullet$ Zhipeng Chen: D3, D4 $\\bullet$ Junjie Zhang: D6 $\\bullet$ Bowen Zheng: D7 $\\bullet$ Zican Dong: D8 $\\bullet$ Xinyu Tang: C2 $\\bullet$ Yifan Du: T4 $\\bullet$ Tianyi Tang: O6, O7, D9 $\\bullet$ Yupeng Hou: O8, C3 $\\bullet$ Salvatore Raieli: C4 \\ifCLASSOPTIONcaptionsoff \\newpage \\fi IEEEtran",
      "origin_cites_number": 0
    }
  ],
  "literature_review_id": 257900969,
  "meta_info": {
    "cite_counts": 1076,
    "Conference_journal_name": "ArXiv",
    "influentialcitationcount": 173,
    "Author_info": {
      "Publicationsh": 225,
      "h_index": 69,
      "Citations": 26801,
      "Highly Influential Citations": 0
    },
    "all_cites_title": [
      "A neural probabilistic language model",
      "Natural language processing (almost) from scratch",
      "The Language Instinct: How the Mind Creates Language",
      "The faculty of language: what is it, who has it, and how did it evolve?\" science",
      "Computing machinery and intelligence",
      "Statistical Methods for Speech Recognition",
      "Introduction to the special issue on statistical language modeling",
      "Two decades of statistical language modeling: Where do we go from here?",
      "Srilm-an extensible language modeling toolkit",
      "Statistical language modeling for information retrieval",
      "Statistical Language Models for Information Retrieval, ser. Synthesis Lectures on Human Language Technologies",
      "A second-order hidden markov model for part-of-speech tagging",
      "A tree-based statistical language model for natural language speech recognition",
      "Large language models in machine translation",
      "Estimation of probabilities from sparse data for the language model component of a speech recognizer",
      "Good-turing frequency estimation without tears",
      "Recurrent neural network based language model",
      "Recurrent neural network based language modeling in meeting recognition",
      "Distributed representations of words and phrases and their compositionality",
      "Efficient estimation of word representations in vector space",
      "Deep contextualized word representations",
      "Attention is all you need",
      "BERT: pre-training of deep bidirectional transformers for language understanding",
      "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
      "Language models are unsupervised multitask learners",
      "Roberta: A robustly optimized BERT pretraining approach",
      "Multitask prompted training enables zero-shot task generalization",
      "What language model architecture and pretraining objective works best for zero-shot generalization",
      "Scaling laws for neural language models",
      "Emergent abilities of large language models",
      "Talking about large language models",
      "Chain of thought prompting elicits reasoning in large language models",
      "Training compute-optimal large language models",
      "Galactica: A large language model for science",
      "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "A comprehensive survey on pretrained foundation models: A history from BERT to chatgpt",
      "Pre-trained models: Past, present and future",
      "Pre-trained models for natural language processing: A survey",
      "Planning for agi and beyond",
      "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "Language is not all you need: Aligning perception with language models",
      "A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt",
      "Palm-e: An embodied multimodal language model",
      "Visual chatgpt: Talking, drawing and editing with visual foundation models",
      "Gpt-4 technical report",
      "How does gpt obtain its ability? tracing emergent abilities of language models to their sources",
      "Pretrained language model for text generation: A survey",
      "A survey of deep learning for mathematical reasoning",
      "A survey for incontext learning",
      "Towards reasoning in large language models: A survey",
      "Reasoning with language model prompting: A survey",
      "Chatgpt: potential, prospects, and limitations",
      "Dense text retrieval based on pretrained language models: A survey",
      "Language models are few-shot learners",
      "A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Is- ard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghe- mawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Do- han, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier- Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel, \"Palm: Scaling language modeling with pathways,\" CoRR, vol. abs/2204.02311, 2022.",
      "Llama: Open and efficient foundation language models",
      "Scaling laws for autoregressive generative modeling",
      "Doremi: Optimizing data mixtures speeds up language model pretraining",
      "Will we run out of data? an analysis of the limits of scaling datasets in machine learning",
      "Scaling data-constrained language models",
      "The inverse scaling prize",
      "Phase transitions in artificial intelligence systems",
      "J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoff- mann, H. F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A. Hendricks, M. Rauh, P. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Ue- sato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. M. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Ne- matzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J. Lespiau, M. Tsimpoukelli, N. Grig- orev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d'Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. J. Johnson, B. A. Hechtman, L. Weidinger, I. Gabriel, W. S. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving, \"Scaling language models: Methods, analysis & insights from training gopher,\" CoRR, vol. abs/2112.11446, 2021.",
      "Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers",
      "Training language models to follow instructions with human feedback",
      "Finetuned language models are zero-shot learners",
      "Lamda: Language models for dialog applications",
      "Scaling instruction-finetuned language models",
      "A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray, A. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv, A. Xiang, A. Parrish, A. Nie, A. Hussain, A. Askell, A. Dsouza, A. Rahane, A. S. Iyer, A. Andreassen, A. Santilli, A. Stuhlm Ã¼ller, A. M. Dai, A. La, A. Lampinen, A. Zou, A. Jiang, A. Chen, A. Vuong, A. Gupta, A. Gottardi, A. Norelli, A. Venkatesh, A. Gholamidavoodi, A. Tabassum, A. Menezes, A. Kirubarajan, A. Mullokandov, A. Sab- harwal, A. Herrick, A. Efrat, A. Erdem, A. Karakas, and et al., \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language mod- els,\" CoRR, vol. abs/2206.04615, 2022.",
      "Are emergent abilities of large language models a mirage?",
      "Unlock predictable scaling from emergent abilities",
      "Grokking: Generalization beyond overfitting on small algorithmic datasets",
      "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters",
      "Megatron-lm: Training multi-billion parameter language models using model parallelism",
      "Efficient large-scale language model training on GPU clusters using megatron-lm",
      "Reducing activation recomputation in large transformer models",
      "BLOOM: A 176b-parameter open-access multilingual language model",
      "Deep reinforcement learning from human preferences",
      "Toolformer: Language models can teach themselves to use tools",
      "Webgpt: Browser-assisted question-answering with human feedback",
      "Exploring the limits of transfer learning with a unified textto-text transformer",
      "mt5: A massively multilingual pre-trained text-to-text transformer",
      "Pangu-Î±: Largescale autoregressive pretrained chinese language models with auto-parallel computation",
      "CPM-2: large-scale costeffective pre-trained language models",
      "Codegen: An open large language model for code with mtulti-turn program synthesis",
      "Gptneox-20b: An open-source autoregressive language model",
      "Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks",
      "Ul2: Unifying language learning paradigms",
      "OPT: open pre-trained transformer language models",
      "No language left behind: Scaling human-centered machine translation",
      "Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x",
      "GLM-130B: an open bilingual pre-trained model",
      "Crosslingual generalization through multitask finetuning",
      "OPT-IML: scaling language model instruction meta learning through the lens of generalization",
      "Pythia: A suite for analyzing large language models across training and scaling",
      "Codegen2: Lessons for training llms on programming and natural languages",
      "Starcoder: may the source be with you",
      "Llama 2: Open foundation and finetuned chat models",
      "Baichuan 2: Open large-scale language models",
      "Qwen technical report",
      "Flm-101b: An open llm and how to train it with $100 k budget",
      "Skywork: A more open bilingual foundation model",
      "Gshard: Scaling giant models with conditional computation and automatic sharding",
      "M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, \"Evaluating large language models trained on code,\" CoRR, vol. abs/2107.03374, 2021.",
      "Large-scale knowledge enhanced pretraining for language understanding and generation",
      "Jurassic-1: Technical details and evaluation",
      "What changes can largescale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers",
      "Yuan 1.0: Largescale pre-trained language model in zero-shot and few-shot learning",
      "A general language assistant as a laboratory for alignment",
      "titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation",
      "Glam: Efficient scaling of language models with mixture-of-experts",
      "Using deepspeed and megatron to train megatronturing NLG 530b, A large-scale generative language model",
      "Competition-level code generation with alphacode",
      "Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model",
      "Improving alignment of dialogue agents via targeted human judgements",
      "Welm: A well-read pre-trained language model for chinese",
      "Transcending scaling laws with 0.1% extra compute",
      "Pangu-Î£: Towards trillion parameter language model with sparse heterogeneous computing",
      "Palm 2 technical report",
      "Learning to generate reviews and discovering sentiment",
      "Improving language understanding by generative pre-training",
      "The natural language decathlon: Multitask learning as question answering",
      "DIALOGPT : Large-scale generative pre-training for conversational response generation",
      "End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2",
      "A neural network solves and generates mathematics problems by program synthesis: Calculus, differential equations, linear algebra, and more",
      "Text and code embeddings by contrastive pre-training",
      "Proximal policy optimization algorithms",
      "Learning to summarize from human feedback",
      "Our approach to alignment research",
      "Introducing chatgpt",
      "D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K. Ndousse, A. Jones, S. Bowman, A. Chen, T. Con- erly, N. DasSarma, D. Drain, N. Elhage, S. E. Showk, S. Fort, Z. Hatfield-Dodds, T. Henighan, D. Hernan- dez, T. Hume, J. Jacobson, S. Johnston, S. Kravec, C. Olsson, S. Ringer, E. Tran-Johnson, D. Amodei, T. Brown, N. Joseph, S. McCandlish, C. Olah, J. Ka- plan, and J. Clark, \"Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned,\" CoRR, vol. abs/2209.07858, 2022.",
      "Gpt-4v(ision) system card",
      "Lessons learned on language model safety and misuse",
      "Introducing meta llama 3: The most capable openly available llm to date",
      "Introducing Llama 3.1: Our most capable models to date",
      "A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam- ford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.- A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, \"Mistral 7b,\" 2023.",
      "Mixtral of experts",
      "Gemma: Open models based on gemini research and technology",
      "M. RiviÃ¨re, S. Pathak, P. G. Sessa, C. Hardin, S. Bhu- patiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. RamÃ©, J. Ferret, P. Liu, P. Tafti, A. Friesen, M. Cas- bon, S. Ramos, R. Kumar, C. L. Lan, S. Jerome, A. Tsit- sulin, N. Vieillard, P. Stanczyk, S. Girgin, N. Mom- chev, M. Hoffman, S. Thakoor, J. Grill, B. Neyshabur, O. Bachem, A. Walton, A. Severyn, A. Parrish, A. Ah- mad, A. Hutchison, A. Abdagic, A. Carl, A. Shen, A. Brock, A. Coenen, A. Laforge, A. Paterson, B. Bas- tian, B. Piot, B. Wu, B. Royal, C. Chen, C. Kumar, C. Perry, C. Welty, C. A. Choquette-Choo, D. Sinopal- nikov, D. Weinberger, D. Vijaykumar, D. Rogozin- ska, D. Herbison, E. Bandy, E. Wang, E. Noland, E. Moreira, E. Senter, E. Eltyshev, F. Visin, G. Rasskin, G. Wei, G. Cameron, G. Martins, H. Hashemi, H. Klimczak-Plucinska, H. Batra, H. Dhand, I. Nar- dini, J. Mein, J. Zhou, J. Svensson, J. Stanway, J. Chan, J. P. Zhou, J. Carrasqueira, J. Iljazi, J. Becker, J. Fer- nandez, J. van Amersfoort, J. Gordon, J. Lipschultz, J. Newlan, J. Ji, K. Mohamed, K. Badola, K. Black, K. Millican, K. McDonell, K. Nguyen, K. Sodhia, K. Greene, L. L. Sj Ã¶sund, L. Usui, L. Sifre, L. Heuer- mann, L. Lago, and L. McNealus, \"Gemma 2: Im- proving open language models at a practical size,\" CoRR, vol. abs/2408.00118, 2024.",
      "Qwen2 technical report",
      "Qwen2.5: A party of foundation models",
      "T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Rojas, G. Feng, H. Zhao, H. Lai, H. Yu, H. Wang, J. Sun, J. Zhang, J. Cheng, J. Gui, J. Tang, J. Zhang, J. Li, L. Zhao, L. Wu, L. Zhong, M. Liu, M. Huang, P. Zhang, Q. Zheng, R. Lu, S. Duan, S. Zhang, S. Cao, S. Yang, W. L. Tam, W. Zhao, X. Liu, X. Xia, X. Zhang, X. Gu, X. Lv, X. Liu, X. Liu, X. Yang, X. Song, X. Zhang, Y. An, Y. Xu, Y. Niu, Y. Yang, Y. Li, Y. Bai, Y. Dong, Z. Qi, Z. Wang, Z. Yang, Z. Du, Z. Hou, and Z. Wang, \"Chatglm: A family of large language models from glm-130b to glm-4 all tools,\" 2024.",
      "JEC-QA: A legal-domain question answering dataset",
      "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
      "Stanford alpaca: An instruction-following llama model",
      "Self-instruct: Aligning language model with self generated instructions",
      "Instruct-tune llama on consumer hardware",
      "Lora: Low-rank adaptation of large language models",
      "Koala: A dialogue model for academic research",
      "Belle: Be everyone's large language model engine",
      "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "Visual instruction tuning",
      "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "Instructblip: Towards general-purpose vision-language models with instruction tuning",
      "Pandagpt: One model to instruction-follow them all",
      "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "Project gutenberg",
      "A simple method for commonsense reasoning",
      "Defending against neural fake news",
      "Openwebtext corpus",
      "The pushshift reddit dataset",
      "Bigquery dataset",
      "The pile: An 800gb dataset of diverse text for language modeling",
      "The bigscience roots corpus: A 1.6 tb composite multilingual dataset",
      "Ccnet: Extracting high quality monolingual datasets from web crawl data",
      "Redpajama: an open dataset for training large language models",
      "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only",
      "On the use of arxiv as a dataset",
      "S2ORC: The semantic scholar open research corpus",
      "peS2o (Pretraining Efficiently on S2ORC) Dataset",
      "The stack: 3 tb of permissively licensed source code",
      "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
      "Dolma: an open corpus of three trillion tokens for language model pretraining research",
      "Olmo: Accelerating the science of language models",
      "Cross-task generalization via natural language crowdsourcing instructions",
      "Promptsource: An integrated development environment and repository for natural language prompts",
      "MVP: multitask supervised pre-training for natural language generation",
      "The oig dataset",
      "Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
      "Free dolly: Introducing the world's first truly open instruction-tuned llm",
      "Openassistant conversationsdemocratizing large language model alignment",
      "Stanford alpaca: An instruction-following llama model",
      "Guanaco -generative universal assistant for natural-language adaptive context-aware omnilingual outputs",
      "Baize: An open-source chat model with parameterefficient tuning on self-chat data",
      "Towards better instruction following language models for chinese: Investigating the im-pact of training data and evaluation",
      "Understanding dataset difficulty with V-usable information",
      "Huggingface h4 stack exchange preference dataset",
      "Training socially aligned language models in simulated human society",
      "Cvalues: Measuring the values of chinese large language models from safety to responsibility",
      "Safe rlhf: Safe reinforcement learning from human feedback",
      "Multitask prompted training enables zero-shot task generalization",
      "The flan collection: Designing data and methods for effective instruction tuning",
      "Training verifiers to solve math word problems",
      "Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies",
      "Make up your mind! adversarial generation of inconsistent natural language explanations",
      "Transformers: State-of-the-art natural language processing",
      "JAX: composable transformations of Python+NumPy programs",
      "Colossal-ai: A unified deep learning system for large-scale parallel training",
      "Patrickstar: Parallel training of pre-trained models via a chunk-based memory management",
      "Colossalchat: An open-source solution for cloning chatgpt with a complete rlhf pipeline",
      "Bmtrain: Effient training for big models",
      "Fastmoe: A fast mixture-of-expert training system",
      "Efficient memory management for large language model serving with pagedattention",
      "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales",
      "Pytorch: An imperative style, high-performance deep learning library",
      "Tensorflow: A system for large-scale machine learning",
      "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems",
      "Paddlepaddle: An open-source deep learning platform from industrial practice",
      "Huawei mindspore ai development framework",
      "Oneflow: Redesign the distributed deep learning framework from scratch",
      "Recipes for building an open-domain chatbot",
      "Solving quantitative reasoning problems with language models",
      "unarxive 2022: All arxiv publications pre-processed for nlp, including structured full-text and citation network",
      "Experiments with a heuristic compiler",
      "Toward automatic program synthesis",
      "Codebert: A pre-trained model for programming and natural languages",
      "Program synthesis with large language models",
      "GPT-Neo: Large Scale Autoregressive Lan-guage Modeling with Mesh-Tensorflow",
      "A systematic evaluation of large language models of code",
      "Language models of code are few-shot commonsense learners",
      "A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity",
      "Data-juicer: A one-stop data processing system for large language models",
      "Phi-3 technical report: A highly capable language model locally on your phone",
      "The fineweb datasets: Decanting the web for the finest text data at scale",
      "Rephrasing the web: A recipe for compute and data-efficient language modeling",
      "When less is more: Investigating data pruning for pretraining llms at scale",
      "How to train data-efficient llms",
      "Scaling laws and interpretability of learning from repeated data",
      "The curious case of neural text degeneration",
      "Deduplicating training data makes language models better",
      "Quantifying memorization across neural language models",
      "Deduplicating training data mitigates privacy risks in language models",
      "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "A new algorithm for data compression",
      "Neural machine translation of rare words with subword units",
      "Japanese and korean voice search",
      "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "Subword regularization: Improving neural network translation models with multiple subword candidates",
      "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "Unicode normalization forms",
      "Deep double descent: Where bigger models and more data hurt",
      "Improving llm pretraining via document de-duplication and diversification",
      "Slimpajama-dc: Understanding data combinations for llm training",
      "Data selection for language models via importance resampling",
      "Farewell to aimless large-scale pretraining: Influential subset selection for language model",
      "The LAMBADA dataset: Word prediction requiring a broad discourse context",
      "Skill-it! a data-driven skills framework for understanding and training language models",
      "Code llama: Open foundation models for code",
      "Curriculum learning",
      "Contrastive post-training large language models on data curriculum",
      "Focused transformer: Contrastive training for context scaling",
      "Llemma: An open language model for mathematics",
      "Extending context window of large language models via positional interpolation",
      "Ccnet: Extracting high quality monolingual datasets from web crawl data",
      "Bag of tricks for efficient text classification",
      "Data-juicer: A one-stop data processing system for large language models",
      "Examining scaling and transfer of language model architectures for machine translation",
      "Unified language model pre-training for natural language understanding and generation",
      "Unified scaling laws for routed language models",
      "Efficiently modeling long sequences with structured state spaces",
      "Simplified state space layers for sequence modeling",
      "Resurrecting recurrent neural networks for long sequences",
      "Hyena hierarchy: Towards larger convolutional language models",
      "RWKV: reinventing rnns for the transformer era",
      "Retentive network: A successor to transformer for large language models",
      "Mamba: Linear-time sequence modeling with selective state spaces",
      "Rwkv: Reinventing rnns for the transformer era",
      "Cogview: Mastering text-to-image generation via transformers",
      "Layer normalization",
      "Root mean square layer normalization",
      "Deepnet: Scaling transformers to 1, 000 layers",
      "Rectified linear units improve restricted boltzmann machines",
      "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "Searching for activation functions",
      "GLU variants improve transformer",
      "Roformer: Enhanced transformer with rotary position embedding",
      "Train short, test long: Attention with linear biases enables input length extrapolation",
      "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "Do transformer modifications transfer across implementations and applications",
      "On layer normalization in the transformer architecture",
      "Adaptive input representations for neural language modeling",
      "Understanding the difficulty of training transformers",
      "Gaussian error linear units (gelus)",
      "Language modeling with gated convolutional networks",
      "What language model to train if you have one million GPU hours",
      "Selfattention with relative position representations",
      "Transformer-xl: Attentive language models beyond a fixed-length context",
      "Xlnet: Generalized autoregressive pretraining for language understanding",
      "Yarn: Efficient context window extension of large language models",
      "A length-extrapolatable transformer",
      "Random feature attention",
      "Big bird: Transformers for longer sequences",
      "Generating long sequences with sparse transformers",
      "Fast transformer decoding: One writehead is all you need",
      "Gqa: Training generalized multi-query transformer models from multihead checkpoints",
      "Flashattention: Fast and memory-efficient exact attention with IO-awareness",
      "Flashattention-2: Faster attention with better parallelism and work partitioning",
      "vllm: Easy, fast, and cheap llm serving with pagedattention",
      "Correcting length bias in neural machine translation",
      "The curious case of neural text degeneration",
      "Speech Understanding Systems",
      "Six challenges for neural machine translation",
      "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "A deep reinforced model for abstractive summarization",
      "Diverse beam search: Decoding diverse solutions from neural sequence models",
      "Hierarchical neural story generation",
      "Truncation sampling as language model desmoothing",
      "A contrastive framework for neural text generation",
      "Locally typical sampling",
      "Contrastive decoding: Open-ended text generation as optimization",
      "Dola: Decoding by contrasting layers improves factuality in large language models",
      "Adam: A method for stochastic optimization",
      "Fixing weight decay regularization in adam",
      "Adafactor: Adaptive learning rates with sublinear memory cost",
      "Gpipe: Efficient training of giant neural networks using pipeline parallelism",
      "Pipedream: Fast and efficient pipeline parallel DNN training",
      "Mixed precision training",
      "An efficient 2d method for training super-large deep learning models",
      "Tesseract: Parallelize the tensor parallelism efficiently",
      "Maximizing parallelism in distributed training for huge neural networks",
      "Sequence parallelism: Long sequence training from system perspective",
      "Alpa: Automating inter-and {Intra-Operator} parallelism for distributed deep learning",
      "Training deep nets with sublinear memory cost",
      "Fairscale: A general purpose modular pytorch library for high performance and large scale training",
      "Is prompt all you need? no. A comprehensive and broader view of instruction learning",
      "Multi-task deep neural networks for natural language understanding",
      "Muppet: Massive multi-task representations with pre-finetuning",
      "The flan collection: Designing data and methods for effective instruction tuning",
      "Wizardlm: Empowering large language models to follow complex instructions",
      "Principle-driven self-alignment of language models from scratch with minimal human supervision",
      "Selfalignment with instruction backtranslation",
      "Lima: Less is more for alignment",
      "Alpagasus: Training A better alpaca with fewer data",
      "Orca: Progressive learning from complex explanation traces of GPT-4",
      "Yulan-chat: An open-source bilingual chatbot",
      "Key-point-driven data synthesis with its enhancement on mathematical reasoning",
      "Enhancing chat language models by scaling high-quality instructional conversations",
      "Jiuzhang3.0: Efficiently improving mathematical reasoning by training small data synthesis models",
      "Instruction mining: High-quality instruction data selection for large language models",
      "From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning",
      "Active learning for convolutional neural networks: A core-set approach",
      "LESS: selecting influential data for targeted instruction tuning",
      "Understanding black-box predictions via influence functions",
      "How far can camels go? exploring the state of instruction tuning on open resources",
      "Scaling laws of rope-based extrapolation",
      "Instruction tuning with GPT-4",
      "Efficient sequence packing without crosscontamination: Accelerating large language models without impacting performance",
      "Large language models encode clinical knowledge",
      "Recommendation as instruction following: A large language model empowered recommendation approach",
      "Huatuo: Tuning llama model with chinese medical knowledge",
      "Lawyer llama technical report",
      "Bloomberggpt: A large language model for finance",
      "Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks",
      "Moss: Training conversational language models from synthetic data",
      "Alpacafarm: A simulation framework for methods that learn from human feedback",
      "Measuring massive multitask language understanding",
      "Challenging big-bench tasks and whether chain-of-thought can solve them",
      "Alignment of language agents",
      "Fine-tuning language models from human preferences",
      "A general language assistant as a laboratory for alignment",
      "Red teaming language models with language models",
      "Teaching language models to support answers with verified quotes",
      "Constitutional AI: harmlessness from AI feedback",
      "RLAIF: scaling reinforcement learning from human feedback with AI feedback",
      "RAFT: reward ranked finetuning for generative foundation model alignment",
      "A general language assistant as a laboratory for alignment",
      "Secrets of rlhf in large language models part i: Ppo",
      "Solving math word problems with process-and outcome-based feedback",
      "Let's verify step by step",
      "Measuring coding challenge competence with APPS",
      "Shepherd: A critic for language model generation",
      "Alphamath almost zero: process supervision without process",
      "Let's reward step by step: Step-level reward model as the navigators for reasoning",
      "Improving large language models via fine-grained reinforcement learning with minimum editing constraint",
      "Training large language models for reasoning through reverse curriculum reinforcement learning",
      "Mastering the game of go without human knowledge",
      "Thinking fast and slow with deep learning and tree search",
      "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
      "Second thoughts are best: Learning to re-align with human values from text edits",
      "QUARK: controllable text generation with reinforced unlearning",
      "Training language models with language feedback at scale",
      "Beyond imitation: Leveraging finegrained quality signals for alignment",
      "Socially situated artificial intelligence enables learning from human interaction",
      "Chain of hindsight aligns language models with feedback",
      "Direct preference optimization: Your language model is secretly a reward model",
      "KTO: model alignment as prospect theoretic optimization",
      "Simpo: Simple preference optimization with a reference-free reward",
      "Towards analyzing and understanding the limitations of DPO: A theoretical perspective",
      "Learn your reference model for real good alignment",
      "sdpo: Don't use your data all at once",
      "RRHF: rank responses to align language models with human feedback without tears",
      "Slic-hf: Sequence likelihood calibration with human feedback",
      "Robust preference optimization through reward model distillation",
      "The wisdom of hindsight makes language models better instruction followers",
      "Imitation learning: A survey of learning methods",
      "Should i imitate or reinforce",
      "Reinforcement learning from human feedback: Progress and challenges",
      "Prefix-tuning: Optimizing continuous prompts for generation",
      "The power of scale for parameter-efficient prompt tuning",
      "Parameter-efficient transfer learning for NLP",
      "Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models",
      "Towards a unified view of parameterefficient transfer learning",
      "Ptuning v2: Prompt tuning can be comparable to finetuning universally across scales and tasks",
      "GPT understands, too",
      "Ppt: Pretrained prompt tuning for few-shot learning",
      "How can we know what language models know?",
      "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
      "Adaptive budget allocation for parameter-efficient fine-tuning",
      "Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation",
      "Parameter-efficient fine-tuning of large-scale pre-trained language models",
      "Llama-adapter: Efficient finetuning of language models with zero-init attention",
      "MAD-X: an adapter-based framework for multi-task crosslingual transfer",
      "Peft: State-of-the-art parameterefficient fine-tuning methods",
      "What makes good in-context examples for gpt-3?",
      "Learning to retrieve prompts for in-context learning",
      "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator",
      "Large language models are human-level prompt engineers",
      "Structured prompting: Scaling in-context learning to 1, 000 examples",
      "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
      "Complexity-based prompting for multi-step reasoning",
      "Automatic chain of thought prompting in large language models",
      "Selection-inference: Exploiting large language models for interpretable logical reasoning",
      "Self-consistency improves chain of thought reasoning in language models",
      "On the advance of making language models better reasoners",
      "Rationale-augmented ensembles in language models",
      "Least-to-most prompting enables complex reasoning in large language models",
      "Decomposed prompting: A modular approach for solving complex tasks",
      "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
      "Faithful chain-of-thought reasoning",
      "PAL: program-aided language models",
      "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
      "Adaplanner: Adaptive planning from feedback with language models",
      "Multimodal procedural planning via dual text-image prompting",
      "Reasoning with language model is planning with world model",
      "Chatcot: Tool-augmented chainof-thought reasoning on chat-based large language models",
      "React: Synergizing reasoning and acting in language models",
      "Reflexion: Language agents with verbal reinforcement learning",
      "Tree of thoughts: Deliberate problem solving with large language models",
      "Design guidelines for prompt engineering text-to-image generative models",
      "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "Teler: A general taxonomy of LLM prompts for benchmarking complex tasks",
      "Gpt best practices",
      "Awesome chatgpt prompts",
      "Structgpt: A general framework for large language model to reason over structured data",
      "Prompting is programming: A query language for large language models",
      "Chameleon: Plugand-play compositional reasoning with large language models",
      "Investigating the factual knowledge boundary of large language models with retrieval augmentation",
      "Prompt design and engineering: Introduction and advanced methods",
      "Large language models are zeroshot rankers for recommender systems",
      "How to prompt llms for text-to-sql: A study in zero-shot, singledomain, and cross-domain settings",
      "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
      "Making pre-trained language models better few-shot learners",
      "Instructzero: Efficient instruction optimization for black-box large language models",
      "Use your INSTINCT: instruction optimization using neural bandits coupled with transformers",
      "Rlprompt: Optimizing discrete text prompts with reinforcement learning",
      "TEMPERA: test-time prompt editing via reinforcement learning",
      "Morl-prompt: An empirical analysis of multiobjective reinforcement learning for discrete prompt optimization",
      "Prewrite: Prompt rewriting with re-inforcement learning",
      "GPS: genetic prompt search for efficient few-shot learning",
      "Grips: Gradient-free, edit-based instruction search for prompting large language models",
      "Large language models are human-level prompt engineers",
      "Automatic prompt optimization with \"gradient descent\" and beam search",
      "Large language models as optimizers",
      "Prompt engineering a prompt engineer",
      "Unleashing the potential of large language models as prompt optimizers: An analogical analysis with gradient-based model optimizers",
      "Instoptima: Evolutionary multi-objective instruction optimization via large language model-based instruction operators",
      "Connecting large language models with evolutionary algorithms yields powerful prompt optimizers",
      "Prompt optimization via adversarial in-context learning",
      "Promptagent: Strategic planning with language models enables expert-level prompt optimization",
      "Contexttuning: Learning contextualized prompts for natural language generation",
      "Spot: Better frozen model adaptation through soft prompt transfer",
      "Learning to transfer prompts for text generation",
      "Rethinking the role of demonstrations: What makes in-context learning work",
      "Calibrate before use: Improving few-shot performance of language models",
      "Does GPT-3 generate empathetic dialogues? A novel in-context example selection method and automatic evaluation metric for empathetic dialogue generation",
      "Diverse demonstrations improve in-context compositional generalization",
      "Selective annotation makes language models better few-shot learners",
      "Complementary explanations for effective in-context learning",
      "Finding supporting examples for in-context learning",
      "Active example selection for in-context learning",
      "Chatgpt outperforms crowd-workers for text-annotation tasks",
      "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator",
      "An explanation of in-context learning as implicit bayesian inference",
      "Self-adaptive incontext learning",
      "Pre-training to learn in context",
      "Metaicl: Learning to learn in context",
      "A theory of emergent in-context learning as implicit structure induction",
      "What incontext learning \"learns\" in-context: Disentangling task recognition and task learning",
      "The learnability of in-context learning",
      "Do prompt-based models really understand the meaning of their prompts?",
      "Transformers learn in-context by gradient descent",
      "C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Gan- guli, Z. Hatfield-Dodds, D. Hernandez, S. John- ston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCan- dlish, and C. Olah, \"In-context learning and induc- tion heads,\" CoRR, vol. abs/2209.11895, 2022.",
      "What learning algorithm is in-context learning? investigations with linear models",
      "Larger language models do in-context learning differently",
      "Meta-in-context learning in large language models",
      "Symbol tuning improves in-context learning in language models",
      "A survey of chain of thought reasoning: Advances, frontiers and future",
      "A diverse corpus for evaluating and developing english math word problem solvers",
      "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
      "Large language models are zero-shot reasoners",
      "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
      "PAL: program-aided language models",
      "Automatic model selection with large language models for reasoning",
      "Making large language models better reasoners with step-aware verifier",
      "Answering questions by metareasoning over multiple chains of thought",
      "Deductive verification of chain-ofthought reasoning",
      "RCOT: detecting and rectifying factual inconsistency in reasoning by reversing chain-of-thought",
      "Large language models are better reasoners with self-verification",
      "Forward-backward reasoning in large language models for mathematical verification",
      "Large language model guided tree-ofthought",
      "Tree of uncertain thoughts reasoning for large language models",
      "Graph of thoughts: Solving elaborate problems with large language models",
      "Boosting logical reasoning in large language models through a new framework: The graph of thought",
      "Everything of thoughts: Defying the law of penrose triangle for thought generation",
      "Holistic evaluation of language models",
      "When do program-of-thoughts work for reasoning?",
      "Text and patterns: For effective chain of thought, it takes two to tango",
      "Multimodal chain-of-thought reasoning in language models",
      "Language models are multilingual chain-of-thought reasoners",
      "Limitations of language models in arithmetic and symbolic induction",
      "ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models",
      "Tree of thoughts: Deliberate problem solving with large language models",
      "Voyager: An open-ended embodied agent with large language models",
      "Self-planning code generation with large language model",
      "Progprompt: Generating situated robot task plans using large language models",
      "LLM+P: empowering large language models with optimal planning proficiency",
      "High-resolution image synthesis with latent diffusion models",
      "Generative agents: Interactive simulacra of human behavior",
      "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
      "Milvus: A purposebuilt vector data management system",
      "Memorybank: Enhancing large language models with long-term memory",
      "Building a large annotated corpus of english: The penn treebank",
      "Pointer sentinel mixture models",
      "Findings of the 2014 workshop on statistical machine translation",
      "Findings of the 2016 conference on machine translation",
      "Findings of the 2019 conference on machine translation (WMT19)",
      "Findings of the 2020 conference on machine translation (WMT20)",
      "Findings of the 2021 conference on machine translation (WMT21),\" in Proceedings of the Sixth Conference on Machine Translation, WMT@EMNLP 2021",
      "Findings of the 2022 conference on machine translation (WMT22)",
      "The flores-101 evaluation benchmark for low-resource and multilingual machine translation",
      "Diabla: a corpus of bilingual spontaneous written dialogues for machine translation",
      "Abstractive text summarization using sequence-to-sequence rnns and beyond",
      "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "Wikilingua: A new benchmark dataset for crosslingual abstractive summarization",
      "Opendialkg: Explainable conversational reasoning with attention-based walks over knowledge graphs",
      "DS-1000: A natural and reliable benchmark for data science code generation",
      "Execution-based evaluation for open-domain code generation",
      "Natural questions: a benchmark for question answering research",
      "Think you have solved question answering? try arc, the AI2 reasoning challenge",
      "Truthfulqa: Measuring how models mimic human falsehoods",
      "Semantic parsing on freebase from question-answer pairs",
      "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "PIQA: reasoning about physical commonsense in natural language",
      "Lc-quad 2.0: A large dataset for complex question answering over wikidata and dbpedia",
      "Beyond I.I.D.: three levels of generalization for question answering on knowledge bases",
      "KQA pro: A dataset with explicit compositional programs for complex question answering over knowledge base",
      "Logical form generation via multi-task learning for complex question answering over knowledge bases",
      "MKQA: A linguistically diverse benchmark for multilingual open domain question answering",
      "Scienceqa: a novel resource for question answering on scholarly articles",
      "Can a suit of armor conduct electricity? A new dataset for open book question answering",
      "MS MARCO: A human generated machine reading comprehension dataset",
      "QASC: A dataset for question answering via sentence composition",
      "Squad: 100, 000+ questions for machine comprehension of text",
      "Key-value memory networks for directly reading documents",
      "Assessing the factual accuracy of generated text",
      "Observed versus latent features for knowledge base and text inference",
      "Freebase: a collaboratively created graph database for structuring human knowledge",
      "Convolutional 2d knowledge graph embeddings",
      "Wordnet: A lexical database for english",
      "Language models as knowledge bases",
      "YAGO3: A knowledge base from multilingual wikipedias",
      "Yago: a core of semantic knowledge",
      "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
      "Boolq: Exploring the surprising difficulty of natural yes/no questions",
      "Socialiqa: Commonsense reasoning about social interactions",
      "Hellaswag: Can a machine really finish your sentence",
      "Winogrande: An adversarial winograd schema challenge at scale",
      "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
      "proscript: Partially ordered scripts generation",
      "Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension",
      "Explagraphs: An explanation graph generation task for structured commonsense reasoning",
      "Proofwriter: Generating implications, proofs, and abductive statements over natural language",
      "Explaining answers with entailment trees",
      "Language models are greedy reasoners: A systematic formal analysis of chain-ofthought",
      "C. Anil, Y. Wu, A. Andreassen, A. Lewkowycz, V. Misra, V. V. Ramasesh, A. Slone, G. Gur-Ari,",
      "Exploring length generalization in large language models",
      "Are NLP models really able to solve simple math word problems?",
      "Solving general arithmetic word problems",
      "Mathqa: Towards interpretable math word problem solving with operationbased formalisms",
      "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
      "Mawps: A math word problem repository",
      "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "Naturalproofs: Mathematical theorem proving in natural language",
      "Lisa: Language models of isabelle proofs",
      "minif2f: a crosssystem benchmark for formal olympiad-level mathematics",
      "Proofnet: Autoformalizing and formally proving undergraduate-level mathematics",
      "Halueval: A large-scale hallucination evaluation benchmark for large language models",
      "Crows-pairs: A challenge dataset for measuring social biases in masked language models",
      "Gender bias in coreference resolution",
      "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
      "Virtualhome: Simulating household activities via programs",
      "BEHAVIOR: benchmark for everyday household activities in virtual, interactive, and ecological environments",
      "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks",
      "Alfworld: Aligning text and embodied environments for interactive learning",
      "Webshop: Towards scalable real-world web interaction with grounded language agents",
      "Mind2web: To-wards a generalist agent for the web",
      "Minerl: A large-scale dataset of minecraft demonstrations",
      "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
      "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
      "Evaluating and improving tool-augmented computation-intensive math reasoning",
      "Gpt4tools: Teaching large language model to use tools via self-instruction",
      "Gorilla: Large language model connected with massive apis",
      "The value of semantic parse labeling for knowledge base question answering",
      "Metaqa: Combining expert agents for multi-skill question answering",
      "Compositional semantic parsing on semi-structured tables",
      "Seq2sql: Generating structured queries from natural language using reinforcement learning",
      "Tabfact: A largescale dataset for table-based fact verification",
      "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task",
      "Neural machine translation by jointly learning to align and translate",
      "Bleu: a method for automatic evaluation of machine translation",
      "ROUGE: A package for automatic evaluation of summaries",
      "Is chatgpt a good translator? a preliminary study",
      "Benchmarking large language models for news summarization",
      "News summarization and evaluation in the era of GPT-3",
      "Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text",
      "Is chatgpt a good NLG evaluator? A preliminary study",
      "G-eval: NLG evaluation using GPT-4 with better human alignment",
      "Re3: Generating longer stories with recursive reprompting and revision",
      "Recurrentgpt: Interactive generation of (arbitrarily) long text",
      "Program synthesis",
      "Planning with large language models for code generation",
      "The end of programming",
      "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
      "Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation",
      "Summeval: Reevaluating summarization evaluation",
      "Not all metrics are guilty: Improving NLG evaluation with LLM paraphrasing",
      "Rethinking the evaluation for conversational recommendation in the era of large language models",
      "Human-like summarization evaluation with chatgpt",
      "Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences",
      "Benchmarking foundation models with languagemodel-as-an-examiner",
      "Evaluate what you can't evaluate: Unassessable generated responses quality",
      "Large language models are not fair evaluators",
      "A comprehensive capability analysis of gpt-3 and gpt-3.5 series models",
      "Catastrophic interference in connectionist networks: The sequential learning problem",
      "Measuring catastrophic forgetting in neural networks",
      "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models",
      "How much knowledge can you pack into the parameters of a language model",
      "Few-shot learning with retrieval augmented language models",
      "Retrieval augmented language model pre-training",
      "Retrievalaugmented generation for knowledge-intensive NLP tasks",
      "Complex knowledge base question answering: A survey",
      "Improving language models by retrieving from trillions of tokens",
      "Search-in-the-chain: Towards accurate, credible and traceable large language models for knowledgeintensive tasks",
      "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
      "Active retrieval augmented generation",
      "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
      "Evaluating object hallucination in large vision-language models",
      "Language models (mostly) know what they know",
      "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
      "S. Agarwal, I. Akkaya, V. Balcom, M. Bavarian, G. Bernadett-Shapiro, G. Brockman, M. Brundage, J. Chan, F. Chantzis, N. Deutsch, B. Eastman, A. Eleti, N. Felix, S. P. Fishman, I. Fulford, C. Gibson, J. Gross, M. Heaton, J. Hilton, X. Hu, S. Jain, H. Jin, L. Kil- patrick, C. Kim, M. Kolhede, A. Mayne, P. McMil- lan, D. Medina, J. Menick, A. Mishchenko, A. Nair, R. Nayak, A. Neelakantan, R. Nuttall, J. Parish, A. T. Passos, A. Perelman, F. de Avila Belbute Peres, V. Pong, J. Schulman, E. Sigler, N. Staudacher, N. Tur- ley, J. Tworek, R. Greene, A. Vijayvergiya, C. Voss, J. Weng, M. Wiethoff, S. Yoo, K. Yu, W. Zaremba, S. Zhao, W. Zhuk, and B. Zoph, \"Chatgpt plugins,\" OpenAI Blog, March 2023.",
      "Internet-augmented language models through few-shot prompting for open-domain question answering",
      "Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus",
      "RETA-LLM: A retrieval-augmented large language model toolkit",
      "Knowledge neurons in pretrained transformers",
      "Locating and editing factual associations in gpt",
      "Transformer feed-forward layers are key-value memories",
      "Editing large language models: Problems, methods, and opportunities",
      "Easyedit: An easy-to-use knowledge editing framework for large language models",
      "Synthetic prompting: Generating chain-ofthought demonstrations for large language models",
      "Mind meets machine: Unravelling gpt-4's cognitive psychology",
      "Show your work: Scratchpads for intermediate computation with language models",
      "Limitations of language models in arithmetic and symbolic induction",
      "Jiuzhang: A chinese pre-trained language model for mathematical problem understanding",
      "First experiments with neural translation of informal to formal mathematics",
      "Generative language modeling for automated theorem proving",
      "Thor: Wielding hammers to integrate language models and automated theorem provers",
      "Formal mathematics statement curriculum learning",
      "Autoformalization with large language models",
      "Draft, sketch, and prove: Guiding formal theorem provers with informal proofs",
      "Self-refine: Iterative refinement with self-feedback",
      "Reflexion: an autonomous agent with dynamic memory and selfreflection",
      "CRITIC: large language models can self-correct with tool-interactive critiquing",
      "Solving math word problems with process-and outcome-based feedback",
      "Let's verify step by step",
      "How well do large language models perform in arithmetic tasks?",
      "Reasoning like program executors",
      "Teaching algorithmic reasoning via in-context learning",
      "TALM: tool augmented language models",
      "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
      "Grounding large language models in interactive environments with online reinforcement learning",
      "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory",
      "Voyager: An open-ended embodied agent with large language models",
      "M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Ir- pan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Ser- manet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, and M. Yan, \"Do as I can, not as I say: Grounding language in robotic affordances,\" CoRR, vol. abs/2204.01691, 2022.",
      "Code as policies: Language model programs for embodied control",
      "Improving language model negotiation with self-play and in-context learning from AI feedback",
      "Improving grounded language understanding in a collaborative environment by interacting with agents through help feedback",
      "Gorilla: Large language model connected with massive apis",
      "Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings",
      "Taskmatrix.ai: Completing tasks by connecting foundation models with millions of apis",
      "Large language models as tool makers",
      "Large language models can selfimprove",
      "Open llm leaderboard",
      "Agieval: A humancentric benchmark for evaluating foundation models",
      "Measuring massive multitask chinese understanding",
      "M3KE: A massive multi-level multisubject knowledge evaluation benchmark for chinese large language models",
      "C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models",
      "Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation",
      "Opencompass: A universal evaluation platform for foundation models",
      "Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance",
      "Kola: Carefully benchmarking world knowledge of large language models",
      "ARB: advanced reasoning benchmark for large language models",
      "Revisiting, benchmarking and exploring API recommendation: How far are we?",
      "Api-bank: A benchmark for tool-augmented llms",
      "Toolalpaca: Generalized tool learning for language models with 3000 simulated cases",
      "On the tool manipulation capability of open-source large language models",
      "Toolllm: Facilitating large language models to master 16000+ real-world apis",
      "BOLAA: benchmarking and orchestrat-ing llm-augmented autonomous agents",
      "X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, S. Zhang, X. Deng, A. Zeng, Z. Du, C. Zhang, S. Shen, T. Zhang, Y. Su, H. Sun, M. Huang, Y. Dong, and J. Tang, \"Agentbench: Evaluating llms as agents,\" CoRR, vol. abs/2308.03688, 2023.",
      "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts",
      "WHEN FLUE MEETS FLANG: benchmarks and large pre-trained language model for financial domain",
      "Legalbench: Prototyping a collaborative benchmark for legal reasoning",
      "Judging llm-as-ajudge with mt-bench and chatbot arena",
      "Scibench: Evaluating college-level scientific problem-solving abilities of large language models",
      "Alpacaeval: An automatic evaluator of instructionfollowing models",
      "Trustgpt: A benchmark for trustworthy and responsible large language models",
      "Benchmarking foundation models with languagemodel-as-an-examiner",
      "Chateval: Towards better llm-based evaluators through multi-agent debate",
      "A survey on evaluation of large language models",
      "Through the lens of core competency: Survey on evaluation of large language models",
      "Tydi QA: A benchmark for information-seeking question an-swering in typologically diverse languages",
      "A framework for few-shot language model evaluation",
      "When flue meets flang: Benchmarks and large pretrained language model for financial domain",
      "Don't make your llm an evaluation benchmark cheater",
      "Vegamt: The JD explore academy machine translation system for WMT22",
      "Calibrating sequence likelihood improves conditional language generation",
      "Unifiedqa: Crossing format boundaries with a single QA system",
      "Solving math word problem via cooperative reasoning induced language models",
      "Meet in the middle: A new pre-training paradigm",
      "RESDSQL: decoupling schema linking and skeleton parsing for text-to-sql",
      "Self-attentive sequential recommendation",
      "Improv-ing conversational recommendation systems' quality with context-aware item meta-information",
      "Falcon-40B: an open large language model with state-of-the-art performance",
      "Algorithms for bigram and trigram word clustering",
      "Word sense disambiguation: A survey",
      "A survey of text similarity approaches",
      "Deep learning-based text classification: a comprehensive review",
      "RAFT: A real-world few-shot text classification benchmark",
      "Is chatgpt a general-purpose natural language processing task solver?",
      "X. Chen, J. Ye, C. Zu, N. Xu, R. Zheng, M. Peng, J. Zhou, T. Gui, Q. Zhang, and X. Huang, \"How robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks,\" 2023.",
      "A survey of named entity recognition and classification",
      "A maximum entropy model for part-of-speech tagging",
      "A survey on recent advances in named entity recognition from deep learning models",
      "Portuguese named entity recognition using bert-crf",
      "Relation extraction: A survey",
      "Ace 2005 multilingual training corpus ldc",
      "Exploring the feasibility of chatgpt for event extraction",
      "Large language model is not a good few-shot information extractor, but a good reranker for hard samples",
      "Does synthetic data generation of llms help clinical text mining?",
      "Zero-shot information extraction via chatting with chatgpt",
      "Tensor2tensor for neural machine translation",
      "Prompting large language model for machine translation: A case study",
      "Dictionary-based phrase-level prompting of large language models for machine translation",
      "Document-level machine translation with large language models",
      "Parrot: Translating during chat using large language models",
      "Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages",
      "Chatgpt: Jack of all trades, master of none",
      "Can chatgpt understand too? A comparative study on chatgpt and fine-tuned BERT",
      "Uprise: Universal prompt retrieval for improving zero-shot evaluation",
      "Rocketqav2: A joint training method for dense passage retrieval and passage re-ranking",
      "Is chatgpt good at search? investigating large language models as re-ranking agent",
      "Large language models are effective text rankers with pairwise ranking prompting",
      "Discrete prompt optimization via constrained generation for zero-shot re-ranker",
      "Found in the middle: Permutation self-consistency improves listwise ranking in large language models",
      "Zero-shot listwise document reranking with a large language model",
      "A setwise approach for effective and highly efficient zero-shot ranking with large language models",
      "Beyond yes and no: Improving zero-shot llm rankers via scoring fine-grained relevance labels",
      "Large language models are built-in autoregressive search engines",
      "Finetuning llama for multi-stage text retrieval",
      "Rankvicuna: Zero-shot listwise document reranking with open-source large language models",
      "Transformer memory as a differentiable search index",
      "TOME: A two-stage approach for model-based retrieval",
      "Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering",
      "Pair: Leveraging passage-centric similarity relation for improving dense passage retrieval",
      "Soft prompt tuning for augmenting dense retrieval with large language models",
      "Promptagator: Few-shot dense retrieval from 8 examples",
      "Generating synthetic documents for crossencoder re-rankers: A comparative study of chatgpt and human experts",
      "Large language models know your contextual search intent: A prompting framework for conversational search",
      "Precise zeroshot dense retrieval without relevance labels",
      "Query2doc: Query expansion with large language models",
      "Pretraining with large language model-based document expansion for dense passage retrieval",
      "Instruction distillation makes large language models efficient zero-shot rankers",
      "Large search model: Redefining search stack in the era of llms",
      "Multimodal foundation models: From specialists to general-purpose assistants",
      "Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms",
      "S3-rec: Selfsupervised learning for sequential recommendation with mutual information maximization",
      "Recbole 2.0: Towards a more up-to-date recommendation library",
      "Towards a more user-friendly and easy-to-use benchmark library for recommender systems",
      "BPR: bayesian personalized ranking from implicit feedback",
      "Recommender systems in the era of large language models (llms)",
      "A survey on large language models for recommendation",
      "Chat-rec: Towards interactive and explainable llms-augmented recommender system",
      "Uncovering chatgpt's capabilities in recommender systems",
      "Large language models are zeroshot rankers for recommender systems",
      "Is chatgpt a good recommender? A preliminary study",
      "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
      "Collaborative large language model for recommender systems",
      "Adapting large language models by integrating collaborative semantics for recommendation",
      "Towards openworld recommendation with knowledge augmentation from large language models",
      "A first look at llm-powered generative news recommendation",
      "Exploring the upper limits of text-based collaborative filtering using large language models: Discoveries and insights",
      "Llmrec: Large language models with graph augmentation for recommendation",
      "Ctrl: Connect tabular and language model for ctr prediction",
      "Ctr-bert: Cost-effective knowledge distillation for billion-parameter teacher models",
      "A survey on large language model based autonomous agents",
      "Recagent: A novel simulation paradigm for recommender systems",
      "Recsim: A configurable simulation platform for recommender systems",
      "Agentcf: Collaborative learning with autonomous language agents for recommender systems",
      "On generative agents in recommendation",
      "A survey of vision-language pre-trained models",
      "Vision-language pre-training: Basics, recent advances, and future trends",
      "Audiopalm: A large language model that can speak and listen",
      "Flamingo: a visual language model for few-shot learning",
      "LAION-5B: an open large-scale dataset for training next generation image-text models",
      "Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts",
      "mplug-owl: Modularization empowers large language models with multimodality",
      "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "Improved baselines with visual instruction tuning",
      "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition",
      "Shikra: Unleashing multimodal llm's referential dialogue magic",
      "Aligning large multi-modal model with robust instruction tuning",
      "What makes for good visual instructions? synthesizing complex visual reasoning instructions for visual instruction tuning",
      "Vizwiz grand challenge: Answering visual questions from blind people",
      "Top-down and bottom-up cues for scene text recognition",
      "Mmbench: Is your multi-modal model an all-around player?",
      "MME: A comprehensive evaluation benchmark for multimodal large language models",
      "Siren's song in the AI ocean: A survey on hallucination in large language models",
      "Detecting and preventing hallucinations in large vision language models",
      "Evaluation and mitigation of agnosia in multimodal large language models",
      "Object hallucination in image captioning",
      "Evaluating object hallucination in large vision-language models",
      "GQA: A new dataset for real-world visual reasoning and compositional question answering",
      "Learn to explain: Multimodal reasoning via thought chains for science question answering",
      "Towards vqa models that can read",
      "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v(ision), llava-1.5, and other multi-modality models",
      "VQA: visual question answering",
      "Cider: Consensus-based image description evaluation",
      "Visual instruction tuning",
      "Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models",
      "Reform-eval: Evaluating large vision language models via unified re-formulation of taskoriented benchmarks",
      "Seed-bench: Benchmarking multimodal llms with generative comprehension",
      "Mm-vet: Evaluating large multimodal models for integrated capabilities",
      "To see is to believe: Prompting GPT-4V for better visual instruction tuning",
      "Llavar: Enhanced visual instruction tuning for text-rich image understanding",
      "Visual adversarial examples jailbreak aligned large language models",
      "Analyzing and mitigating object hallucination in large vision-language models",
      "Aligning large multimodal models with factually augmented rlhf",
      "Semtab 2019: Resources to benchmark tabular data to knowledge graph matching systems",
      "Unifying large language models and knowledge graphs: A roadmap",
      "Largescale knowledge enhanced pre-training for language understanding and generation",
      "ERNIE: enhanced language representation with informative entities",
      "KEPLER: A unified model for knowledge embedding and pre-trained language representation",
      "Subgraph retrieval enhanced model for multi-hop knowledge base question answering",
      "Jointgt: Graph-text joint representation learning for text generation from knowledge graphs",
      "Large scale knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training",
      "KGPT: knowledge-grounded pre-training for data-to-text generation",
      "Don't generate, discriminate: A proposal for grounding language models to real-world environments",
      "Reasoning on graphs: Faithful and interpretable large language model reasoning",
      "Query graph generation for answering multi-hop complex questions from knowledge bases",
      "Easyedit: An easy-to-use knowledge editing framework for large language models",
      "Editing large language models: Problems, methods, and opportunities",
      "KCTS: knowledge-constrained tree search decoding with token-level hallucination detection",
      "Mitigating language model hallucination with interactive question-knowledge alignment",
      "Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities",
      "The perils of using mechanical turk to evaluate open-ended text generation",
      "RLAIF: scaling reinforcement learning from human feedback with AI feedback",
      "Ultrafeedback: Boosting language models with high-quality feedback",
      "MINT: evaluating llms in multi-turn interaction with tools and language feedback",
      "Branch-solve-merge improves large language model evaluation and generation",
      "Wider and deeper LLM networks are fairer LLM evaluators",
      "Chateval: Towards better llm-based evaluators through multi-agent debate",
      "PRD: peer rank and discussion improve large language model based evaluations",
      "Judgelm: Fine-tuned large language models are scalable judges",
      "Evaluating large language models at evaluating instruction following",
      "Benchmarking cognitive biases in large language models as evaluators",
      "The generative AI paradox: \"what it can create, it may not understand",
      "Large language models cannot self-correct reasoning yet",
      "GPT-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems",
      "Putting chatgpt's medical advice to the (turing) test",
      "On the evaluations of chatgpt and emotionenhanced prompting for mental health analysis",
      "Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports",
      "Towards expert-level medical question answering with large language models",
      "Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue",
      "The utility of chatgpt for cancer treatment information",
      "On the educational impact of chatgpt: Is artificial intelligence ready to obtain a university degree?",
      "Chatgpt: The end of online exam integrity?",
      "Towards applying powerful large ai models in classroom teaching: Opportunities, challenges and prospects",
      "A new era of artificial intelligence in education: A multifaceted revolution",
      "Chatgpt for good? on opportunities and challenges of large language models for education",
      "Can GPT-3 perform statutory reasoning?",
      "Legal prompt engineering for multilingual legal judgement prediction",
      "Chatgpt goes to law school",
      "Law informs code: A legal informatics approach to aligning artificial intelligence with humans",
      "Legal prompting: Teaching a language model to think like a lawyer",
      "Legal prompt engineering for multilingual legal judgement prediction",
      "Understanding the capabilities, limitations, and societal impact of large language models",
      "A short survey of viewing large language models in legal aspect",
      "Persistent antimuslim bias in large language models",
      "Zero is not hero yet: Benchmarking zero-shot performance of llms for financial tasks",
      "Finbert: Financial sentiment analysis with pre-trained language models",
      "Domain adaption of named entity recognition to support credit risk assessment",
      "Beyond classification: Financial reasoning in state-of-the-art language models",
      "Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters",
      "Fingpt: Opensource financial large language models",
      "Pubmedqa: A dataset for biomedical research question answering",
      "Bioasq-qa: A manually curated corpus for biomedical question answering",
      "Oceangpt: A large language model for ocean science tasks",
      "One small step for generative ai, one giant leap for AGI: A complete survey on chatgpt in AIGC era",
      "Using chatgpt to conduct a literature review",
      "Openai chatgpt generated literature review: Digital twin in healthcare",
      "Can chatgpt be used to generate scientific hypotheses?",
      "Chatgpt as your personal data scientist",
      "Is GPT-4 a good data analyst?",
      "Artificial hallucinations in chatgpt: Implications in scientific writing",
      "Chatgpt is a remarkable tool -for experts",
      "Academic writing with GPT-3.5: reflections on practices, efficacy and transparency",
      "Reviewergpt? an exploratory study on using large language models for paper reviewing",
      "Theory of mind may have spontaneously emerged in large language models",
      "Will affective computing emerge from foundation models and general ai? A first evaluation on chatgpt",
      "Chatgpt: A study on its utility for ubiquitous software engineering tasks",
      "Automatic code summarization via chatgpt: How far are we?",
      "Conversational automated program repair",
      "The impact of positional encoding on length generalization in transformers",
      "Effective long-context scaling of foundation models",
      "Things I'm learning while training superhot",
      "BAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models",
      "Transformer upgrade path: 12, infinite extrapolation of rerope?",
      "Giraffe: Adventures in expanding context lengths in llms",
      "Leveraging passage retrieval with generative models for open domain question answering",
      "Parallel context windows for large language models",
      "Longformer: The long-document transformer",
      "Efficient streaming language models with attention sinks",
      "Lost in the middle: How language models use long contexts",
      "Lm-infinite: Simple on-the-fly length generalization for large language models",
      "Unlimiformer: Long-range transformers with unlimited length input",
      "Memorizing transformers",
      "Longheads: Multi-head attention is secretly a long context processor",
      "Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory",
      "Data engineering for scaling language models to 128k context",
      "Longwanjuan: Towards systematic measurement for long text quality",
      "Walking down the memory maze: Beyond context limit through interactive reading",
      "Recurrentgpt: Interactive generation of (arbitrarily) long text",
      "Memgpt: Towards llms as operating systems",
      "Retrieval meets long context large language models",
      "Artificial Intelligence: A Modern Approach",
      "Building machines that learn and think like people",
      "React: Synergizing reasoning and acting in language models",
      "Xagent: An autonomous agent for complex task solving",
      "CAMEL: communicative agents for \"mind\" exploration of large scale language model society",
      "Metagpt: Meta programming for multi-agent collaborative framework",
      "Let models speak ciphers: Multiagent debate through embeddings",
      "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
      "Autogen: Enabling next-gen llm applications via multi-agent conversation framework",
      "Improving factuality and reasoning in language models through multiagent debate",
      "Character-llm: A trainable agent for role-playing",
      "Trustagent: Towards safe and trustworthy llmbased agents through agent constitution",
      "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
      "Decoupled weight decay regularization",
      "Reducing activation recomputation in large transformer models",
      "Zero: memory optimizations toward training trillion parameter models",
      "Zero-offload: Democratizing billion-scale model training",
      "Zero-infinity: breaking the GPU memory wall for extreme scale deep learning",
      "Flashattention: Fast and memory-efficient exact attention with io-awareness",
      "Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models",
      "Ring attention with blockwise transformers for near-infinite context",
      "Towards coarse-to-fine evaluation of inference efficiency for large language models",
      "Flexgen: High-throughput generative inference of large language models with a single GPU",
      "PMLR, 2023, pp. 31 094-31 116.",
      "Flashdecoding for long-context inference",
      "Deepspeedfastgen: High-throughput text generation for llms via MII and deepspeed-inference",
      "Fast inference from transformers via speculative decoding",
      "Accelerating large language model decoding with speculative sampling",
      "Specinfer: Accelerating generative LLM serving with speculative inference and token tree verification",
      "Accelerating LLM inference with staged speculative decoding",
      "Frugalgpt: How to use large language models while reducing cost and performance",
      "Large language model cascades with mixture of thoughts representations for cost-efficient reasoning",
      "Non-autoregressive neural machine translation",
      "Semiautoregressive neural machine translation",
      "Medusa: Simple LLM inference acceleration framework with multiple decoding heads",
      "Branchynet: Fast inference via early exiting from deep neural networks",
      "Multi-scale dense networks for resource efficient image classification",
      "Mixtureof-depths: Dynamically allocating compute in transformer-based language models",
      "Efficient large language models: A survey",
      "A survey of quantization methods for efficient neural network inference",
      "-bit matrix multiplication for transformers at scale",
      "Awq: Activation-aware weight quantization for llm compression and acceleration",
      "PB-LLM: partially binarized large language models",
      "Spqr: A sparsequantized representation for near-lossless LLM weight compression",
      "APTQ: attention-aware post-training mixedprecision quantization for large language models",
      "OWQ: outlier-aware weight quantization for efficient finetuning and inference of large language models",
      "Smoothquant: Accurate and efficient posttraining quantization for large language models",
      "Zeroquant: Efficient and affordable posttraining quantization for large-scale transformers",
      "Gptq: Accurate post-training quantization for generative pre-trained transformers",
      "Optimal brain compression: A framework for accurate post-training quantization and pruning",
      "Qlora: Efficient finetuning of quantized llms",
      "Llm-qat: Data-free quantization aware training for large language models",
      "Zeroquantv2: Exploring post-training quantization in llms from comprehensive study to low rank compensation",
      "The case for 4-bit precision: k-bit inference scaling laws",
      "Do emergent abilities exist in quantized large language models: An empirical study",
      "Qa-lora: Quantization-aware low-rank adaptation of large language models",
      "Loftq: Lora-fine-tuningaware quantization for large language models",
      "Knowledge distillation of large language models",
      "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
      "Sparsegpt: Massive language models can be accurately pruned in oneshot",
      "Llm-pruner: On the structural pruning of large language models",
      "Sheared llama: Accelerating language model pre-training via structured pruning",
      "8-bit optimizers via block-wise quantization",
      "A survey on rag meets llms: Towards retrieval-augmented large language models",
      "Retrieval-augmented generation for large language models: A survey",
      "The probabilistic relevance framework",
      "Rear: A relevance-aware retrieval-augmented framework for open-domain question answering",
      "Context embeddings for efficient answer generation in rag",
      "Recomp: Improving retrieval-augmented lms with context compression and selective augmentation",
      "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy",
      "Dense x retrieval: What retrieval granularity should we use?",
      "Question decomposition tree for answering complex questions over knowledge bases",
      "Learning to rewrite queries",
      "Query rewriting via large language models",
      "Enhancing conversational search: Large language model-aided informative query rewriting",
      "Adaptive-rag: Learning to adapt retrievalaugmented large language models through question complexity",
      "Llmlingua: Compressing prompts for accelerated inference of large language models",
      "Sayself: Teaching llms to express confidence with self-reflective rationales",
      "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
      "Sail: Searchaugmented instruction learning",
      "Ra-dit: Retrieval-augmented dual instruction tuning",
      "Retrieval augmented language model pre-training",
      "Latent retrieval for weakly supervised open domain question answering",
      "The dawn after the dark: An empirical study on factuality hallucination in large language models",
      "Survey of hallucination in natural language generation",
      "Siren's song in the AI ocean: A survey on hallucination in large language models",
      "Scheduled sampling for sequence prediction with recurrent neural networks",
      "Towards understanding sycophancy in language models",
      "Exploring the relationship between LLM hallucinations and prompt linguistic nuances: Readability, formality, and concreteness",
      "Chain-of-verification reduces hallucination in large language models",
      "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
      "A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation",
      "In search of truth: An interrogation approach to hallucination detection",
      "Factscore: Fine-grained atomic evaluation of factual precision in long form text generation",
      "Factool: Factuality detection in generative AI -A tool augmented framework for multi-task and multi-domain scenarios",
      "Small agent can also rock! empowering small language models as hallucination detector",
      "Towards understanding sycophancy in language models",
      "Simple synthetic data reduces sycophancy in large language models",
      "RARR: researching and revising what language models say, using language models",
      "Verifyand-edit: A knowledge-enhanced chain-of-thought framework",
      "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
      "Inference-time intervention: Eliciting truthful answers from a language model",
      "Trusting your evidence: Hallucinate less with context-aware decoding",
      "Thinking, fast and slow",
      "A comparative study on reasoning patterns of openai's o1 model",
      "Evaluation of openai o1: Opportunities and challenges of agi",
      "Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems",
      "Deepseek-r1-lite-preview is now live: unleashing supercharged reasoning power",
      "Qwq: Reflect deeply on the boundaries of the unknown",
      "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "Enhancing llm reasoning with reward-guided tree search",
      "Encouraging divergent thinking in large language models through multiagent debate",
      "Virgo: A preliminary exploration on reproducing o1-like mllm",
      "Kimi k1.5: Scaling reinforcement learning with llms",
      "Openai's reinforcement fine-tuning research program",
      "Scaling of search and learning: A roadmap to reproduce o1 from reinforcement learning perspective",
      "An empirical study on eliciting and improving r1-like reasoning models",
      "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
      "Buy 4 REIN-FORCE samples, get a baseline for free",
      "Scaling llm test-time compute optimally can be more effective than scaling model parameters",
      "Federatedscopellm: A comprehensive package for fine-tuning large language models in federated learning"
    ]
  }
}