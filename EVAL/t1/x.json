{
  "survey": "Large language models (LLMs) have revolutionized natural language processing (NLP), offering significant advancements in understanding, generating, and manipulating human language at scale. This survey provides a comprehensive overview of LLMs, focusing on their transformative impact through the integration of neural networks and transformer models. Key developments include sophisticated neural architectures that enhance context-aware learning and bidirectional comprehension, enabling efficient language processing across diverse applications. The integration of multimodal capabilities, such as visual encoders, further expands the scope of LLMs, facilitating richer human-machine interactions. Recent advancements in data utilization and training methodologies, including strategic dataset curation and innovative sampling techniques, have optimized LLM performance, challenging traditional practices and paving the way for future improvements. Despite these advancements, challenges remain in ensuring model robustness, addressing computational resource constraints, and enhancing data transparency and diversity. Future directions emphasize the need for strategic data selection, improved training methodologies, and robust evaluation metrics to ensure the scalability and practical application of LLMs across varied linguistic contexts. Overall, the ongoing evolution of LLMs continues to enhance the efficiency and scope of language-based applications, underscoring their pivotal role in shaping the future of NLP and addressing longstanding challenges in AI-driven communication, reasoning, and understanding.\n\nIntroduction Significance of Large Language Models in NLP Large language models (LLMs) represent a transformative advancement in natural language processing (NLP), significantly enhancing the understanding, generation, and manipulation of human language through transformer-based architectures. These models exhibit remarkable improvements in context-awareness and bidirectional comprehension, overcoming limitations of traditional NLP methods [1]. The development of LaMDA exemplifies LLMs' crucial role in conversational AI, facilitating coherent and contextually relevant interactions within dialog systems [1]. The scalability of models like GLM-130B has expanded computational capabilities, though challenges remain in optimizing training efficiency and aligning large-scale models with user intent. Refinements in training methodologies, such as reinforcement learning with human feedback (RLHF), address conflicting objectives of performance and safety, enhancing model reliability [2]. Moreover, advancements in policy gradient methods have improved sample efficiency, further broadening the applicability of LLMs across diverse AI tasks [3]. Integrating LLMs with multimodal systems, including Visual Foundation Models, has extended their functionality beyond text, enabling tasks such as image processing and generation, thereby enriching human-machine interactions [4]. This versatility positions LLMs to tackle complex cross-modal challenges and fosters innovative applications in visual reasoning and image captioning. Benchmarks for language understanding, reasoning, and safety are critical for evaluating and enhancing LLM performance. Initiatives like GEMMA underscore the necessity for standardized metrics to ensure robust deployment across varied domains [5]. Concurrently, the demand for efficient text classification methods has spurred research into lightweight models capable of handling large datasets effectively [6]. Despite these advancements, challenges persist in aligning LLM outputs with user intent, particularly concerning untruthful or toxic responses [7]. Commonsense reasoning remains a significant hurdle, especially in applying neural networks to complex datasets such as the Winograd Schema [8]. These limitations highlight the ongoing need for innovation in model design, training methodologies, and evaluation metrics. By processing vast datasets, adapting to diverse tasks, and integrating multimodal capabilities, LLMs have fundamentally reshaped the NLP landscape. Their evolution not only enhances the efficiency and scope of language-based applications but also opens new avenues for addressing longstanding challenges in AI-driven communication, reasoning, and understanding [9]. Purpose of the Survey This survey consolidates recent advancements in large language models, focusing on architectural innovations, training methodologies, and diverse applications. It provides a thorough analysis of pre-training technologies, addressing limitations of early static methods and highlighting progress in pretrained foundation models across various data modalities. The exploration of training methodologies and model architecture integration is vital for enhancing efficiency and performance in large language models [10]. Additionally, the survey aims to bridge knowledge gaps in leveraging LLMs for complex reasoning tasks, examining their capacity to mimic human reasoning processes and contribute to commonsense reasoning. It assesses the implications of advanced models like GPT-4 in understanding artificial general intelligence and identifies areas for further exploration in AI development [9]. The application of technology in educational contexts, such as using NLP models like ChatGPT to enhance museum education, illustrates the practical benefits of these models in enriching user engagement and learning experiences [11]. The significance of benchmarks designed to evaluate language model performance in tasks requiring high accuracy and efficiency, including mathematics, code generation, and multilingual understanding, is also highlighted [12]. By systematically reviewing advancements in training methodologies and exploring the integration of human feedback, the survey aims to refine model alignment with user intent and improve performance across various tasks [7]. The discussion of the open-sourced GLM-130B model as a solution to bilingual model challenges emphasizes its potential to perform comparably to established models like GPT-3 [13]. This survey serves as a comprehensive resource synthesizing existing research on LLMs and their NLP applications, highlighting advancements in pretrained models for text generation and reasoning tasks. It identifies knowledge gaps, such as limitations in current pre-training technologies and reasoning accuracy challenges, while suggesting future directions like enhancing reasoning capabilities through reinforcement learning and test-time scaling. Innovative paradigms, including the use of \"thought\" sequences for reasoning and the impact of dynamic pre-training technologies, are also reviewed, providing a thorough overview and guidance for further research in the field [14,15,16]. Focus on Transformer Models and Neural Networks This survey emphasizes transformer models and neural networks due to their crucial roles in enhancing the performance and capabilities of large language models. Transformer models, characterized by attention mechanisms and parallel processing, have revolutionized NLP by enabling efficient context-aware learning and bidirectional language comprehension [17]. A comprehensive review of the history and evolution of generative models, including basic components and recent advances in Artificial Intelligence Generated Content (AIGC), is provided, highlighting their contributions to both unimodal and multimodal interactions [18]. The survey encompasses prompt-based learning methods integral to pre-trained language models, exploring various prompting functions and tuning strategies that enhance adaptability and performance across diverse tasks [19]. The integration of neural networks with transformer models amplifies their effectiveness, particularly in complex reasoning tasks and multilingual processing, as exemplified by advancements in models like GLM-130B, which represents a significant progression in making large-scale bilingual models accessible and efficient [13]. Practical applications of these models are underscored by their use in interactive environments, such as employing ChatGPT to improve communication and engagement between museum visitors and staff, demonstrating their potential to enhance user interactions and educational experiences [11]. This survey elucidates the transformative impact of transformer models and neural networks on large language models, offering insights into their architectural innovations, training methodologies, and diverse applications in NLP. Structure of the Survey The survey is meticulously organized to provide a comprehensive examination of large language models and their role in NLP. It begins with an introduction that establishes the significance of large language models, particularly emphasizing transformer models and neural networks, setting the stage for understanding their transformative impact on NLP [1]. Following the introduction, the survey delves into background and definitions, clarifying core concepts such as large language models, NLP, transformer models, and neural networks, while tracing the historical development of these technologies and highlighting key milestones. The survey then explores recent advancements in large language models, focusing on architectural innovations, training methodologies, and scalability enhancements, examining how improvements in data utilization and benchmarking contribute to the emergent abilities and applications of these models. An in-depth analysis of transformer models follows, detailing their architecture and capabilities, investigating core components and mechanisms, innovative architectures, cross-modal capabilities, and training techniques that have propelled these models to the forefront of NLP research. Subsequently, the role of neural networks in NLP is analyzed, emphasizing their integration with transformer models and their contributions to language processing and generation, while evaluating performance using established metrics [13]. Applications of large language models are thoroughly examined, showcasing their diverse uses in multilingual contexts, conversational agents, sentiment analysis, and more, with real-world examples and case studies illustrating their practical impact [11]. Finally, the survey addresses challenges and future directions for large language models, discussing issues such as bias, ethics, computational resources, and model robustness, while proposing potential avenues for future research and development to ensure continued technological advancement [9]. This structured approach offers an in-depth examination of the progress, methodologies, and applications of large language models (LLMs), highlighting their transformative impact on NLP. It underscores the shift from task-specific engineering to versatile neural network architectures that leverage vast amounts of unlabeled data, as seen in systems that efficiently perform tasks like part-of-speech tagging and semantic role labeling. The evolution from static pre-training techniques, such as Word2Vec and GLoVe, to dynamic methods exemplified by BERT, has resolved issues like polysemy and significantly advanced NLP capabilities. Furthermore, the introduction of reasoning processes within LLMs, including reinforcement learning and test-time scaling, has expanded their ability to tackle complex reasoning tasks, marking a new frontier in NLP research and development [20,14,15].The following sections are organized as shown in . Background and Definitions Definitions and Core Concepts Large language models (LLMs) represent sophisticated neural network architectures designed to process and generate human language across diverse tasks, showcasing adaptability in comprehension and production. These models utilize extensive datasets comprising billions of tokens to achieve superior performance across multiple languages and applications [21]. Models like InstructBLIP illustrate advancements in integrating visual inputs, enhancing vision-language instruction tuning for multimodal processing [22]. However, limitations persist, notably in models like ChatGPT, which face challenges in image processing and generation, indicating areas for further development in visual tasks [4]. Natural language processing (NLP) involves methodologies enabling machines to interpret, generate, and manipulate human language, aiming to align fine-grained and high-level semantic information for improved understanding and contextual relevance. Benchmarks such as LaMDA underscore the importance of safety and factual grounding in dialogue applications, highlighting core LLM concepts in NLP [1]. Evaluations of GPT-4 suggest its potential as an early version of artificial general intelligence, defining critical issues in NLP [9]. Bilingual benchmarks like GLM-130B enhance NLP capabilities in multilingual contexts, particularly in English and Chinese [13]. Transformer models, characterized by attention mechanisms, have transformed NLP by enabling efficient context-aware learning and bidirectional comprehension. They are vital for advancing tasks requiring code generation and understanding across programming languages, as demonstrated by benchmarks like Codellama [23]. The LAMBADA benchmark assesses a model's ability to predict the last word of narrative passages based on broader context, emphasizing the importance of context-aware comprehension in transformer models [24]. Neural networks are integral to LLMs and transformer models, demonstrating efficacy in tasks like sentiment analysis, even with limited training data. The unified neural network architecture (UNNA) proposes a generalized NLP approach by learning internal representations directly from data, rather than relying on predefined features [15]. Neural networks significantly enhance human language processing and generation, with activation functions underscoring their importance in NLP. Historical Development The historical evolution of LLMs is marked by milestones that have shaped NLP development. Initially, recurrent neural networks (RNNs) offered foundational insights into sequential data processing. However, the advent of transformer models marked a crucial shift, enabling efficient context-aware learning and overcoming RNN limitations [23]. This transition was driven by the demand for models capable of managing long-range dependencies and parallelizing training processes. Benchmarks like S2ORC have facilitated access to extensive academic papers for text mining tasks, essential for advancing LLM capabilities [25]. Frameworks such as PyTorch emphasize a balance between usability and performance, crucial for deploying LLMs in real-world scenarios without sacrificing computational efficiency [23]. Unified frameworks and benchmarks like Transforme109 address the need for standardized metrics to evaluate model performance across varied domains [19]. Domain-specific benchmarks, such as JEC-QA, assess models' abilities to answer legal questions requiring logical reasoning based on comprehensive legal materials [12]. Despite advancements, LLM development has been dominated by resource-rich organizations, limiting broader research community accessibility [13]. Initiatives like BLOOM aim to democratize LLM technology access, fostering inclusivity in research. Benchmarks like Pythia provide insights into training dynamics and performance characteristics as model sizes scale, emphasizing the importance of understanding model behavior during this process [26]. The historical development of LLMs is intertwined with progress in cross-modal understanding, exemplified by text-to-image generation models like Cogview, requiring robust generative models and cross-modal capabilities [22]. This integration underscores LLMs' versatility and potential in addressing complex AI challenges. Throughout their evolution, LLMs have faced challenges related to anthropomorphization, where human tendencies to attribute human-like characteristics to technology can obscure their true operational mechanics [27]. Addressing these challenges is vital for ensuring accurate understanding and deployment of LLMs across various applications. The trajectory of LLM development illustrates significant innovation and adaptation, driven by the need to overcome traditional NLP limitations and expand its scope. Initially, static pre-training techniques like Word2Vec and GLoVe faced challenges, including polysemy due to a lack of context consideration. The introduction of dynamic pre-training models like BERT marked a significant advancement, improving various downstream NLP tasks. Furthermore, LLMs have evolved to tackle complex reasoning tasks by integrating concepts like \"thought\" sequences and employing reinforcement learning to enhance reasoning capabilities. These developments have collectively expanded NLP technologies' capabilities, paving the way for future innovations in large reasoning models and automated data construction [14,15,16]. As research progresses, the focus remains on refining model architectures, enhancing scalability, and ensuring accessibility for diverse research communities. Limitations of Traditional Models Traditional NLP models, such as recurrent neural networks (RNNs) and early transformer architectures, face limitations that LLMs aim to address. RNNs exhibit inefficiencies in managing long-range dependencies crucial for context-aware learning, hindering their ability to process complex language nuances [28]. Additionally, traditional transformers struggle with long sequences due to their quadratic complexity in time and memory usage, limiting efficiency with extensive datasets [29]. Sequence-to-sequence models often suffer from inefficient pretraining methods, as evidenced by the BART model, which fails to maintain high performance across various NLP tasks [30]. Moreover, static models struggle to capture language nuances, resulting in limitations in understanding context and meaning [14]. The computational demands of existing methods to learn effective word representations present a core obstacle, restricting their applicability to large datasets [31]. Existing benchmarks primarily focus on unimodal tasks, limiting the evaluation of models integrating multiple data types and hindering the development of models capable of processing diverse inputs effectively [32]. The inadequacy of benchmarks in addressing challenges such as global versus local deduplication and the effects of varying proportions of highly-deduplicated datasets further restricts understanding of model performance [21]. Additionally, existing benchmarks inadequately address quantitative reasoning challenges in technical subjects, limiting evaluation of model performance in this critical area [33]. The high costs associated with data and compute resources limit traditional models' performance, particularly in efficiently training models with billions of parameters without significant performance degradation. These constraints are exacerbated by the inefficiencies and costs of full fine-tuning, which requires retraining all model parameters, posing substantial computational resource challenges [34]. The limitations outlined emphasize the critical need for LLMs that leverage advanced architectures and scalable methodologies to transcend traditional models' constraints. By integrating innovative reasoning techniques, such as \"thought\" sequences and reinforcement learning, LLMs can mimic complex human reasoning processes, including tree search and reflective thinking. The unification of model architectures and learning methods, as demonstrated in the CodeGen2 project, illustrates LLMs' potential to efficiently handle both programming and natural language tasks. These advancements collectively enable LLMs to achieve state-of-the-art performance across a diverse array of language processing tasks, marking significant progress in the field [10,15]. Advancements in Large Language Models The evolution of large language models (LLMs) has been marked by significant progress in data utilization, training methodologies, and architectural designs, enhancing performance and broadening applicability in natural language processing tasks. As illustrated in , the hierarchical structure of advancements in LLMs categorizes key innovations into primary categories and subcategories, including data utilization, training methodologies, architectural designs, scalability, benchmarking, and emergent abilities. Table presents a comprehensive comparison of various methodologies in data utilization, architectural innovations, and training strategies, underscoring their significance in enhancing the capabilities of large language models. This figure emphasizes the significant contributions to LLM evolution and performance enhancement across various natural language processing tasks. The following subsections delve into these advancements, showcasing how they have been pivotal in shaping LLM capabilities and addressing challenges in this rapidly evolving field. Advancements in Data and Training Recent innovations in data utilization and training methodologies have greatly enhanced LLM capabilities, addressing computational and data efficiency challenges. Table provides a detailed comparison of various methods in data utilization, training methodologies, and optimization strategies, illustrating the advancements in large language model capabilities. The use of comprehensive datasets, such as those exceeding 170 billion tokens, has been crucial in improving performance across tasks, exemplified by SlimPajama-DC's empirical analyses of data combinations and deduplication strategies [21]. Training methodologies have also evolved, with Safe Reinforcement Learning from Human Feedback (Safe RLHF) introducing algorithms that align LLMs with human values by distinguishing reward and cost model training [2]. InstructGPT further exemplifies this by incorporating human feedback for fine-tuning [7]. Minerva's approach of pretraining on general language data followed by fine-tuning on technical content sets it apart from previous benchmarks [33]. Optimization efforts are highlighted by models like GPT-4, which use novel methods and infrastructure to ensure predictable performance scaling [9]. Nucleus sampling enhances text generation quality by focusing on the most probable subset of the probability distribution [35]. These advancements underscore the significance of data properties on model performance and the need for data-efficient pre-training approaches, given LLM training costs. The evolution of LLMs reflects transformative shifts in data utilization and training methodologies, with dynamic pre-training techniques like BERT enhancing contextual understanding and addressing polysemy. Innovative reasoning paradigms, such as \"thought\" sequences and reinforcement learning, expand LLM reasoning capabilities, paving the way for efficient and adaptable AI systems capable of tackling complex linguistic and reasoning challenges [36,14,15]. Architectural Innovations Architectural innovations have significantly enhanced LLM capabilities through advanced self-attention mechanisms and model scalability frameworks. Sparse attention mechanisms, as seen in BigBird, reduce computational complexity from quadratic to \\(O(n )\\), enabling efficient processing of longer sequences [29]. Sequence parallelism and selective activation recomputation further optimize computational resources, enhancing efficiency [37]. XLNet integrates ideas from Transformer-XL to surpass BERT by addressing its limitations through novel pretraining strategies. Shared experimental settings emphasize evaluating architectural modifications' effectiveness across applications [38]. This approach aids in identifying innovations that optimize performance and scalability, ensuring LLMs' practical applicability. Developing architectures that surpass existing techniques in accuracy and computational cost is crucial for LLM evolution. Integrating verifier models with state-of-the-art transformers enhances model reliability and robustness [39]. Frameworks for analyzing scaling laws, as demonstrated by [40], provide insights into empirical power-law relationships, informing architectural innovations and training methodologies. Architectural innovations, including advanced data selection techniques and unified neural network architectures, significantly enhance LLM capabilities, ensuring continued evolution and applicability across natural language processing applications [20,36,41,10,15]. Training Methodologies Advancements in training methodologies have redefined neural network learning dynamics, improving LLM efficiency and performance. Reinforcement Learning from Human Feedback (RLHF) fine-tunes models using human feedback, enhancing alignment with user expectations and improving coherence [7]. Nucleus Sampling optimizes generation by truncating unreliable distribution tails while maintaining diversity [35]. Continuous Skip-gram models with negative sampling advance vector representation learning [42]. Unsupervised contrastive learning on large datasets refines token representations, optimizing model performance [43]. Large-scale training initiatives emphasize robust configurations to achieve state-of-the-art results [44]. GLaM's ability to selectively activate components conserves resources while delivering superior performance [45]. Ask-LLM evaluates training example quality using instruction-tuned reasoning capabilities, while density sampling models data distribution for diverse selections [46]. Systematic evaluations across configurations identify optimal training methodologies for diverse NLP tasks [47]. Models like Llemma, which excel in mathematical reasoning, exemplify advancements in training methodologies [48]. These methodologies, encompassing dynamic pre-training technologies like BERT and reinforcement learning-driven thought processes, drive LLM evolution. Addressing static pre-training limitations and expanding reasoning capabilities through automated data construction and test-time scaling, these approaches enhance LLM applicability across diverse tasks, improving accuracy and tackling complex reasoning challenges [14,15]. Table provides a comprehensive summary of key training methodologies that have significantly influenced the learning dynamics, optimization strategies, and resource management in large language models (LLMs). Scalability and Efficiency Table presents a detailed comparison of different methods that contribute to the scalability and efficiency of large language models, illustrating their respective optimization techniques, resource utilization, and scalability strategies. Advancements in scalability and efficiency have enhanced LLM capabilities, addressing computational resource challenges. Sparse Transformers reduce time and memory requirements through sparse attention matrix factorizations, optimizing resource utilization [29]. Instruction-tuned LLMs in Ask-LLM evaluate data quality and employ density sampling for efficient data utilization [46]. DeepSpeed's optimizations enhance training speed and reduce resource requirements, enabling models with hundreds of billions of parameters [49]. Mixtral's superior performance compared to Llama 2 70B highlights optimized training methodologies' impact on scalability [12]. S4 manages long sequences efficiently, underscoring efficient architectural designs' importance for scalable deployment [50]. LoRA captures task-specific information through low-rank updates, leveraging pre-trained models' general knowledge [34]. Colossal-AI improves training speed, achieving up to 2.76 times speedup on large-scale models [51]. GLaM employs a mixture-of-experts framework to enhance capabilities without dense models' prohibitive costs [45]. Adafactor optimizes large neural networks with less memory than Adam, contributing to training efficiency [52]. Experiments with state-of-the-art models reveal a power-law relationship between model size, dataset size, and performance, with larger models achieving lower cross-entropy loss. These advancements enable robust, adaptable, and computationally efficient AI systems, addressing linguistic challenges across applications. Data Utilization and Benchmarking Data utilization and benchmarking are crucial in advancing LLMs, influencing training efficiency, scalability, and performance. Comprehensive datasets characterized by size and diversity train LLMs for a wide range of tasks [40]. CodeLlama integrates code sequences from real-world repositories and synthetic samples [53]. Data-Juicer enhances data processing with fine-grained abstraction and built-in operators, facilitating efficient handling and evaluation [54]. The Qwen2 benchmark offers a broader task range, reflecting state-of-the-art models' capabilities in multilingual contexts [26]. Benchmarking frameworks like GLUE integrate diverse tasks and auxiliary datasets for comprehensive evaluations [55]. Automatic pipelines in benchmarks like CCNet enhance data quality by deduplicating documents and filtering for proximity to high-quality corpora [56]. Optimus demonstrates significant improvements over previous models, achieving notable speedups in training and inference [57]. Efficient data utilization and benchmarking are vital for optimizing LLM training resource demands. Table provides a detailed overview of various benchmarks employed in the assessment of large language models, highlighting their size, domain, task format, and the metrics used for evaluation. Strategic use of diverse, high-quality datasets and robust benchmarking frameworks like GLUE is crucial for advancing LLMs. Enhanced by dynamic pre-training techniques like BERT, these models address modern natural language processing tasks' complex demands, handling a wide range of linguistic tasks while maintaining pre-training corpora quality and size for effective knowledge transfer and sample-efficient learning [55,14,56]. Emergent Abilities and Applications Emergent abilities in LLMs reflect complex capabilities arising from model scaling, significantly enhancing performance across domains. The LAMBADA benchmark emphasizes context-aware processing [24]. OLMo promotes transparency and reproducibility by releasing model weights, training data, and evaluation code [58]. Frameworks improving coherence and diversity, like Nucleus Sampling, enhance machine-generated text quality [35]. The Gemma benchmark evaluates lightweight models in real-world applications, showcasing emergent capabilities [5]. As illustrated in , emergent abilities in large language models (LLMs) can be categorized into several key areas: benchmarks and frameworks, reasoning and safety, and advanced models. Each category highlights significant contributions and innovations that enhance the performance and capabilities of LLMs across various tasks. Toolformer integrates external tools to improve task performance, showcasing emergent abilities in reasoning and content generation. Train-time and test-time scaling significantly enhance reasoning accuracy, marking new frontiers in Large Reasoning Models [15]. Minerva improves quantitative reasoning capabilities, highlighting benchmark potential [33]. LaMDA introduces safety and factual grounding metrics, integrating external knowledge sources, showcasing dialog systems' emergent abilities [1]. GPT-4 achieves human-level performance in complex tasks, underscoring LLMs' potential in advancing AI [9]. These developments illustrate dynamic evolution, driven by training methodologies, data optimization, and architectural innovations, enhancing LLM performance and capabilities across tasks, including conversational agents, knowledge reasoning, and domain-specific processing. Transformer Models: Architecture and Capabilities Core Components and Mechanisms Transformer models, defined by their core components and mechanisms, excel in efficient language processing and generation across various tasks. Central to these models is the self-attention mechanism, which enables parallel processing of input sequences, overcoming the limitations of traditional sequential models like RNNs [28]. The RWKV model exemplifies this adaptability, functioning effectively as both a Transformer and an RNN. XLNet's autoregressive approach captures bidirectional context without input token masking, enhancing the model's ability to understand complex linguistic structures [59]. Positional encoding mechanisms maintain sequence information, with techniques aimed at enhancing extrapolation capabilities to manage long-range dependencies effectively [60]. Innovative designs, such as the S5 layer, integrate multiple inputs and outputs within a single state space framework, achieving high efficiency [61]. The GELU activation function's probabilistic approach allows smoother transitions, reducing dead neurons and enhancing network performance [62]. Gating mechanisms interleaved with long convolutions in models like Hyena offer efficient alternatives to traditional attention mechanisms, optimizing computational resources [63]. Experiments on various state-of-the-art models emphasize implementation details' impact on performance, highlighting architectural modifications' importance [38]. RFA replaces conventional softmax attention, introducing a gating mechanism for learning recency bias, enhancing adaptability and efficiency [64]. Visual Foundation Models, integrated with systems like Visual ChatGPT, illustrate transformers' potential in multimodal applications, enabling interaction through both language and images [4]. LaMDA models exemplify state-of-the-art architectures in dialogue applications, enhancing transformers' capabilities in understanding and generating human language [1]. As illustrated in , these core components and mechanisms highlight the adaptability and efficiency of Transformer models across diverse tasks in language processing and generation. This figure underscores the dynamic interplay of architectural innovations and optimization strategies, ensuring their evolution and applicability in natural language processing tasks. Innovations like Transformer-XL and Focused Transformer address limitations such as fixed-length contexts, enhancing the management of long-term dependencies. The success of models like GPT-4 demonstrates transformers' potential to achieve human-level performance on various benchmarks, showcasing their applicability in both academic and professional contexts [65,38,66,32,67]. Innovative Architectures and Extensions Innovative architectures and extensions of transformer models have significantly advanced their capabilities, enabling efficient processing of complex linguistic tasks and facilitating multimodal interactions. Sparse attention mechanisms, as employed by models like BigBird, exemplify architectural innovations that reduce computational complexity from quadratic to \\(O(n )\\), enhancing scalability and efficiency [29]. The RWKV model showcases adaptability, functioning effectively as both a Transformer and an RNN [28]. Gating mechanisms interleaved with long convolutions in models like Hyena optimize resources, enhancing performance [63]. Architectural extensions like the S5 layer, integrating multiple inputs and outputs within a single state space framework, contribute significantly to enhanced capabilities [61]. Visual Foundation Models, integrated with systems like Visual ChatGPT, highlight transformers' potential in multimodal applications, allowing interaction through both language and images [4]. These extensions broaden applicability beyond text-based tasks, facilitating richer human-machine interactions. Despite numerous proposed modifications, only a few have achieved widespread adoption due to marginal performance improvements, often dependent on specific implementation details. Nonetheless, transformers have revolutionized NLP by enabling high-capacity models and dynamic pre-training techniques that address challenges like polysemy and context fragmentation. Libraries like Transformers facilitate access to these advancements, supporting diverse NLP tasks with efficient, extensible, and robust solutions. Notable innovations such as Transformer-XL further extend model capabilities by capturing longer-term dependencies, significantly outperforming previous models in speed and accuracy [14,65,20,38,67]. Cross-Modal and Multimodal Capabilities Cross-modal and multimodal capabilities of transformer models have been significantly enhanced through the integration of advanced language models with visual encoders, unlocking new abilities not previously demonstrated in earlier models [68]. This integration facilitates processing and generation of diverse data inputs, including text and images, expanding applicability across tasks and applications. Models like MiniGPT-4 exemplify the transformative impact of aligning language models with visual encoders, effectively processing and generating multimodal content, enhancing user experiences in applications such as image captioning and visual reasoning [68]. The Kosmos-1 Multimodal Large Language Model allows seamless processing of multimodal data, synthesizing information from text, images, and image-caption pairs, enhancing performance across tasks like language understanding, generation, multimodal dialogue, and visual question answering [69,38]. This capability is crucial for advancing AI systems requiring comprehensive understanding of complex data inputs, particularly in real-world scenarios where text and visual information are interdependent. Exploration of cross-modal and multimodal capabilities signifies a dynamic evolution in design and functionality, evidenced by advancements such as Kosmos-1, which excels in diverse perception-language tasks and showcases cross-modal transfer. Despite numerous proposed modifications to the original architecture, many have not achieved widespread adoption, underscoring the importance of implementation details. These developments ensure continued applicability across tasks, from NLP to vision-related applications, supported by open-source libraries like Transformers that facilitate access to state-of-the-art architectures and pretrained models [69,38,67]. These advancements underscore the transformative impact of multimodal integration in enhancing capabilities and efficiency. Training and Optimization Techniques Training and optimization techniques are pivotal in enhancing performance and efficiency, addressing challenges related to computational demands and model stability. Layer normalization, applied on a per-layer basis, ensures consistent performance across training and testing by stabilizing the learning process and improving convergence rates [70]. Admin enhances model initialization, improving stability and performance during training [71]. FlashAttention-2 optimizes attention computations, maximizing GPU occupancy and minimizing communication overhead, allowing faster execution of essential attention mechanisms [72]. GShard facilitates efficient scaling by expressing complex parallel computation patterns with minimal code changes [73]. Optimization techniques reduce computational demands while enhancing word vectors' quality, ensuring efficient resource utilization crucial for training large-scale models without compromising performance [31]. Self-instruct methods involve generating, filtering, and utilizing synthetic instructions and outputs to fine-tune pre-trained models, enhancing adaptability and performance across diverse tasks [74]. Experiments comparing Pre-LN Transformers without the warm-up stage to baseline models reveal insights into performance metrics and training time across different configurations, underscoring optimization techniques' importance [75]. Integration of training and optimization techniques has significantly propelled development, enhancing relevance and performance across diverse NLP tasks. Despite numerous proposed modifications, few have been widely adopted due to limited impact on performance. However, advancements in architecture and pretraining have enabled higher-capacity models that effectively utilize potential across applications. Open-source libraries like Transformers provide access to state-of-the-art architectures and pretrained models, fostering innovation and practicality for researchers and practitioners. The shift from static to dynamic pretraining technologies, exemplified by models like BERT, has further advanced the field by addressing challenges such as polysemy, setting the stage for future developments in NLP [38,67,14]. Challenges and Future Directions in Architecture Transformer architecture faces challenges that necessitate ongoing research and development to ensure effectiveness across diverse NLP tasks. Large amounts of unlabeled text for pre-training, as exemplified by BERT, may not be readily available for all languages or domains [76], highlighting the need for innovative data acquisition strategies and architectures that leverage limited data effectively. Emergent abilities are often contingent upon evaluation frameworks, suggesting these abilities may not be inherent but influenced by specific metrics [77], calling for comprehensive evaluation metrics that accurately capture capabilities across varied applications. Computational complexity associated with traditional attention mechanisms limits scalability. FlashAttention offers substantial speed improvements and memory efficiency, enabling training of larger models and handling longer sequences [78]. The Switch Transformer maintains constant computational cost while leveraging parameters through sparse activation [79], suggesting sparse activation techniques could be pivotal in scaling without incurring prohibitive computational costs. RWKV offers a resource-efficient alternative for NLP tasks, efficiently scaling with sequence length while maintaining competitive performance [80], indicating potential avenues for developing architectures that balance performance and resource efficiency. Admin's effectiveness in balancing residual dependency and minimizing amplification effects suggests robust training methodologies that ensure model stability [71]. The GLU variant highlights the need for careful selection of activation functions to avoid overfitting, suggesting future research into optimizing activation functions for diverse architectures [81]. Addressing challenges and exploring future directions involves leveraging high-capacity building capabilities and effective pretraining, as highlighted by the Transformers library. While numerous modifications have been proposed, few have achieved widespread adoption, with performance improvements often hinging on specific implementation details. The shift from static to dynamic pre-training techniques, exemplified by models like BERT, has significantly enhanced handling of polysemy and contextual understanding, marking a pivotal advancement in NLP technology [38,67,14]. Neural Networks in Natural Language Processing Integration with Transformer Models The integration of neural networks with transformer models has markedly enhanced language processing capabilities, boosting performance across diverse applications. This synergy combines deep contextualized word representations from bi-directional language models with transformers' efficient attention mechanisms, refining word usage understanding and improving language model performance [1]. Transformers, when paired with neural networks, benefit from optimized attention computations, enabling the handling of longer sequences while maintaining computational efficiency, essential for large-scale language processing [29]. The integration of LaMDA into dialog systems illustrates how neural networks enhance human language processing and generation, ensuring responses are safe and factually accurate [1]. Moreover, incorporating web-browsing capabilities through imitation learning and human feedback exemplifies collaboration between neural networks and transformers in NLP. This integration allows models to process real-time web information, leveraging datasets like CommonCrawl. Models trained on refined web data, such as those utilizing the RefinedWeb dataset and Ccnet pipeline, outperform those trained on traditional curated corpora, enhancing contextual understanding and response generation while addressing static pre-training limitations [82,14,56]. As illustrated in , the collaboration between neural networks and transformers represents a significant advancement in NLP, highlighting enhanced language processing, web browsing capabilities, and advancements in the field, supported by various datasets and techniques. Utilizing comprehensive datasets like arXiv, S2ORC, and unarXive fosters innovation and accessibility by providing standardized, machine-readable academic texts and extensive citation networks, facilitating advanced bibliometric analyses and developing next-generation models for text mining and NLP across scientific disciplines [25,83,84]. Evaluation Metrics and Model Performance Evaluation metrics are vital for assessing neural networks' performance in natural language processing (NLP), offering insights into their effectiveness across tasks. Metrics such as precision, recall, and F1-score provide a comprehensive view of model accuracy and the balance between false positives and negatives [7]. These metrics ensure models effectively distinguish between categories and generate accurate predictions. Benchmarks like GLUE have integrated diverse tasks, including sentiment analysis and question answering, providing a standardized framework for evaluating model performance across varied NLP applications [55]. Innovative frameworks like Data-Juicer enhance performance assessment by facilitating fine-grained abstraction and visualization, allowing detailed understanding of model capabilities [54]. Perplexity is crucial for evaluating language models, reflecting fluency and coherence of generated text. Lower perplexity values indicate better predictive accuracy and language generation capabilities, showcasing the model's ability to produce contextually relevant and grammatically correct outputs [12]. Incorporating human feedback in evaluation processes, as seen in models like InstructGPT, ensures alignment with user expectations and enhances model output reliability [7]. Metrics such as training speed and memory usage are critical for evaluating neural networks' scalability in NLP. Techniques like FlashAttention optimize attention computations, maximizing GPU occupancy and minimizing communication overhead, enhancing model performance and scalability [72]. These advancements underscore the importance of efficient resource utilization in training large-scale models without compromising performance. The strategic use of evaluation metrics and benchmarking frameworks continues to drive neural networks' advancement in NLP, enabling them to meet complex demands of modern NLP tasks and applications. These metrics provide insights into model performance by evaluating aspects like memorization, summary quality, and sentiment analysis, guiding the development of robust and adaptable language processing systems. Understanding models' memorization of training data helps mitigate privacy and fairness issues, while optimizing for human feedback rather than traditional metrics like ROUGE can enhance summary quality. Discovering sentiment units within byte-level recurrent models showcases efficient sentiment analysis potential, even with limited labeled data, contributing to pre-training technologies' advancement and creating high-quality monolingual datasets from web-sourced data [85,14,56,86,87]. Applications of Large Language Models Large language models (LLMs) are transforming various domains by leveraging extensive datasets and sophisticated algorithms. This section explores their multifaceted roles, particularly their impact on multilingual and domain-specific contexts. Multilingual and Domain-Specific Applications LLMs demonstrate exceptional capabilities in multilingual and domain-specific contexts, utilizing diverse datasets to enhance performance. The Data-Juicer system facilitates data-centric research, promoting LLM applications in multilingual environments [54]. Models like PaLM 2 exhibit significant improvements across tasks, emphasizing their multilingual potential [88,32]. The BLOOM dataset, with 46 natural languages and 13 programming languages, provides a robust foundation for evaluating multilingual models [89]. WeLM excels in monolingual Chinese tasks, showcasing strong zero-shot learning capabilities [90]. CodeBERT integrates programming and natural languages, enhancing domain-specific task performance [91]. Aligning models with human intent through fine-tuning with human feedback ensures coherent and contextually relevant outputs [7]. These advancements reflect LLMs' dynamic evolution, ensuring effectiveness across diverse linguistic contexts. Applications in Conversational Agents and Sentiment Analysis LLMs are integral to developing conversational agents, enabling natural interactions that mimic human conversation. DialoGPT generates contextually appropriate responses, enhancing user experience [92]. LaMDA focuses on safety and factual grounding, ensuring helpful interactions aligned with user expectations [1,93]. Sparrow integrates structured human feedback and rule adherence to refine interaction quality [94]. Visual ChatGPT's multimodal approach enriches user experience by processing textual and visual information [4]. DeepSpeed optimizes RLHF training, enabling sophisticated conversational agents tuned to user preferences [95]. These advancements underscore LLMs' transformative impact on conversational agent capabilities, facilitating natural interactions and expanding applications in diverse fields [36,41,15,11,96]. Privacy and Security Applications LLMs enhance privacy and security applications through advanced data processing and secure communication capabilities. Despite challenges like potential data memorization, LLMs tackle complex reasoning tasks using techniques like reinforcement learning [36,86,15]. Integrating privacy-preserving techniques, such as differential privacy, ensures sensitive information protection while maintaining performance. In cybersecurity, LLMs utilize NLP to analyze complex data patterns, improving threat detection and response [32]. They contribute to secure communication systems by enhancing cryptographic algorithm development [38,97,98]. LLMs also educate users about security risks and best practices, enhancing information quality and addressing privacy issues [86,41,46]. These advancements highlight LLMs' critical role in addressing security challenges and protecting sensitive information. Translation and Multilingual Processing LLMs advance translation and multilingual processing, demonstrating capabilities in understanding and generating language across diverse contexts. PaLM 2 exemplifies improvements in multilingual processing, emphasizing broader applications [88]. The BLOOM dataset supports effective multilingual model performance [89]. WeLM's zero-shot learning capabilities underscore LLMs' potential in domain-specific applications [90]. CodeBERT enhances domain-specific task performance by integrating programming and natural languages [91]. Evaluation metrics and benchmarking frameworks drive advancements in multilingual processing [55]. These applications underscore LLMs' transformative impact on translation, facilitating seamless communication in a globalized world [36,38]. Reasoning and Information Retrieval LLMs enhance reasoning and information retrieval tasks, processing complex queries and providing accurate responses. LLaVA's multimodal capabilities set a new standard in Science QA [99]. Extending context windows enables comprehensive understanding in real-world scenarios [100]. Self-Instruct broadens applicability across reasoning tasks [74]. PanGu- exemplifies strong zero-shot learning capabilities [101]. Contrastive pre-training on unsupervised data improves text and code embeddings [43]. Safe RLHF aligns LLMs with human values, ensuring ethical reasoning [2]. Integrating LLMs into reasoning tasks highlights their dynamic evolution, expanding applications in diverse fields [36,96,15]. Creative Content Generation LLMs revolutionize creative content generation, producing diverse outputs across media formats. As illustrated in , the hierarchical structure of creative content generation using large language models (LLMs) categorizes key aspects into multimodal capabilities, diversity enhancement, and interactive environments. This figure highlights the transformative impact of LLMs across various formats, demonstrating how GPT-4 achieves human-level performance in generating coherent narratives [32]. Furthermore, its multimodal capabilities enhance creative potential by synthesizing text and visual elements [68]. Nucleus Sampling improves content diversity and quality [35], while self-instruct methods align content generation with creative guidelines [74]. LLMs also contribute to interactive environments like virtual reality and gaming, enhancing user engagement [36,41,15]. These applications underscore LLMs' transformative impact, enabling high-quality content creation across formats [69,16,36,15,87]. Mobile and Accessibility Applications LLMs advance mobile and accessibility applications by enhancing user experience across platforms. Their integration in mobile applications facilitates real-time language processing [32]. In accessibility contexts, LLMs develop tools aiding individuals with disabilities, enhancing communication [68]. Optimization techniques support LLM deployment in mobile applications, ensuring efficient resource utilization [72]. Models like LaMDA enhance user interaction and accessibility, addressing challenges related to safety and factual grounding [36,93,1,15]. These advancements underscore LLMs' importance in addressing mobile technology and accessibility challenges, facilitating seamless communication for diverse users. Challenges and Future Directions The evolution of large language models (LLMs) encounters several challenges, notably bias and ethical considerations, which influence their societal acceptance and technological integrity. This section explores these complexities, focusing on their implications for the training, deployment, and future development of LLMs. Addressing these issues is crucial for fostering responsible AI practices and ensuring equitable outcomes across diverse applications. Bias and Ethical Concerns Bias and ethical concerns significantly affect the fairness, reliability, and applicability of LLMs. A major challenge is the quality of training data, which can introduce biases in model outputs. The variability introduced by human feedback datasets in Reinforcement Learning from Human Feedback (RLHF) raises ethical questions regarding consistency and reliability [2]. Despite improvements in models like InstructGPT, persistent errors highlight the need for enhanced bias mitigation strategies during training [7]. Proprietary restrictions limit access to powerful language models, hindering scientific evaluation of biases and risks, emphasizing the importance of open access to support comprehensive assessments and mitigation strategies [58]. Responsible release practices are crucial for innovation while addressing bias and ethical issues [5]. Benchmarks like LaMDA underscore the necessity of aligning model outputs with human values and factual accuracy for responsible AI interactions [1]. Current research often lacks robust frameworks capturing human-like reasoning complexities, indicating a need for more comprehensive evaluation metrics addressing bias and ethical implications [15]. The ethical implications of artificial general intelligence (AGI) and the societal impacts of advancing AI technologies remain critical areas for exploration [9]. Addressing bias and ethical concerns requires a multifaceted approach, including enhancing dataset transparency, diversifying training data sources, and developing comprehensive evaluation metrics. Advanced data selection techniques and innovative reasoning paradigms are essential for improving fairness and reliability across applications. Strategies such as document de-duplication, intelligent data repetition, and leveraging pre-trained model embeddings can enhance training efficiency and downstream accuracy. Integrating reasoning capabilities through reinforcement learning and test-time scaling broadens LLM applicability for complex reasoning tasks, contributing to sustainable and high-performing model development [46,41,15]. Computational Resources and Efficiency LLMs demand substantial computational resources due to extensive datasets and complex architectures. Traditional attention mechanisms, with quadratic scaling in time and space, impede real-time processing and elevate memory usage, challenging efficient model deployment [29]. The lack of a unified approach for high-level abstraction and low-level optimization complicates performance in heterogeneous distributed systems [23]. Innovative solutions like sequence parallelism and selective activation recomputation effectively reduce computational overhead and improve training efficiency [37]. Sparse attention mechanisms, exemplified by models like BigBird, reduce computational complexity, enabling efficient processing of longer sequences [102]. However, implementation complexity and tuning requirements may introduce additional challenges [29]. Training data quality and diversity remain critical limitations affecting LLM generalization, emphasizing the need for benchmarks that reflect real-world data constraints. Optimizing training under these constraints is vital for future language model development, advocating scalable solutions enhancing resource utilization without sacrificing performance [103]. Advanced hardware configurations, such as the Cerebras 16 CS-2 cluster with 80 PFLOP/s in bf16 mixed precision, address computational resource challenges while ensuring high performance [21]. A comprehensive strategy should incorporate architectural innovations to minimize memorization, employ data-efficient training methods like Ask-LLM and Density sampling, and prioritize high-quality, diverse data inputs. Addressing memorization concerns is crucial for maintaining privacy and fairness, ultimately enhancing LLM output quality and utility [86,41,46]. Data Quality and Diversity Data quality and diversity are crucial for LLM effectiveness, directly influencing generalization capabilities across varied linguistic contexts. Curated datasets, like those used by WeLM, reveal limitations in capturing linguistic nuances, potentially impairing performance in underrepresented language settings [90]. Expanding datasets to encompass a wider array of linguistic phenomena and application contexts is essential for comprehensive real-world coverage. Future research should refine skill identification processes and apply frameworks like Skill-It across diverse datasets and models to enhance linguistic skill capture [104]. Optimizing training processes and exploring new model architectures, as suggested by DeepSpeed, can improve usability and performance across applications [95]. Expanding benchmarks like Mixtral to include additional tasks and refine model architectures for greater efficiency is crucial for ensuring LLM applicability across domains [12]. Ensuring data quality remains a challenge, as benchmarks like CCNet may contain low-quality data due to filtration difficulties [56]. This highlights the necessity for robust evaluation frameworks accurately reflecting diverse data scenarios. Efforts to mitigate data scarcity and refine scaling laws are vital for optimizing resource utilization and enhancing model performance [103]. Utilizing diverse and high-quality datasets continues to propel LLM advancements, enabling them to meet complex demands of modern natural language processing tasks. Addressing data quality and diversity involves refining datasets through de-duplication and careful selection, incorporating heterogeneous data sources like books and web content, and developing robust evaluation frameworks accounting for temporal shifts and toxicity filters. Prioritizing diverse and high-quality data is essential for improved performance and generalization capabilities, as evidenced by studies demonstrating curated web data advantages and intelligent data selection methods [82,41,105]. illustrates the hierarchical categorization of strategies to enhance data quality and diversity in large language models (LLMs). It highlights three primary areas: dataset expansion, benchmark refinement, and data selection strategies, each supported by relevant research papers. This visual representation underscores the multifaceted approach required to address the complexities of data quality and diversity in LLM training and application. Generalization and Robustness Generalization and robustness challenges in LLMs involve model architecture, data quality, and evaluation metrics. Model architecture and pretraining objectives significantly impact zero-shot generalization, underscoring thoughtful design importance for optimizing performance across applications [13]. Despite advancements, many models struggle to effectively utilize external tools for enhancing basic functionalities, indicating a need for innovative integration approaches to broaden task applicability [4]. Model architecture intricacy is crucial for determining varied training settings performance, suggesting sophisticated models may yield better generalization capabilities. Strategic layer normalization placement can significantly influence training outcomes, necessitating careful optimization for robust performance. While fastText is efficient, it may not capture complex text patterns as effectively as deeper models, potentially limiting generalization [6]. Existing datasets may hinder generalization, as indicated by GEMMA's benchmark emphasizing task alignment that may not generalize across applications [5]. Exploring diverse data combinations and their long-term performance effects remains an area for further investigation [21]. Existing benchmarks may not fully encompass quantitative reasoning aspects, impacting generalizability [33]. Future research should focus on enhancing language capabilities and optimizing efficiency for broader applications [13]. Unanswered questions regarding effective reasoning training strategies and their scalability across tasks persist [15]. Current studies do not fully address GPT-4 limitations, particularly concerning decision-making processes [9]. Nucleus Sampling, while enhancing diversity, may struggle with generating coherent long-form text due to variability introduced by the dynamic nucleus, challenging consistency in longer sequences [35]. Future research could investigate sparse attention mechanism optimizations and their applicability to longer sequences or different data types [29]. A holistic approach addressing these challenges should encompass architectural innovation, optimized training strategies, and robust evaluation metrics. Techniques like document de-duplication, intelligent data selection, and reasoning enhancement via reinforcement learning are essential for improving LLM scalability and real-world applicability. By optimizing data efficiency and enhancing reasoning capabilities, these strategies enable LLMs to perform effectively across diverse tasks and linguistic contexts while promoting resource-efficient training and expanding their potential for complex reasoning [46,41,36,15]. Conclusion Large language models (LLMs) have redefined natural language processing (NLP) by leveraging advanced neural network architectures and transformer models, significantly enhancing the processing and generation of human language. Their integration with visual encoders and multimodal capabilities extends their functionality, facilitating richer human-machine interactions and expanding the scope of NLP applications. Recent advancements in dataset curation and strategic data selection have propelled LLM pre-training forward, challenging traditional approaches and paving the way for future innovations. The pivotal role of transformer models in enabling efficient, context-aware learning and bidirectional comprehension has revolutionized language processing and generation. Their synergy with neural networks augments the complexity of reasoning and supports multilingual capabilities. Furthermore, novel architectures, such as sparse attention and gating mechanisms, optimize computational efficiency and scalability, ensuring the robust performance of LLMs. Future research should prioritize enhancing model robustness and efficiency, addressing challenges related to computational resources, and improving dataset transparency and diversity. The potential for performance enhancement through high-quality dataset curation is substantial, highlighting the importance of strategic data selection and innovative training methodologies. Additionally, benchmarks provide essential frameworks for advancing language model research, offering structured approaches to evaluate and improve cross-task generalization and scientific exploration. The ongoing evolution of LLMs is broadening the efficiency and scope of language-based applications, facilitating solutions to enduring challenges in AI-driven communication, reasoning, and understanding. These advancements underscore the vital role of LLMs in shaping the future of NLP, emphasizing the transformative impact of neural networks and transformer models in this ever-evolving field.",
  "reference": {
    "1": "2201.08239v3",
    "2": "2310.12773v1",
    "3": "1707.06347v2",
    "4": "2303.04671v1",
    "5": "2403.08295v4",
    "6": "1607.01759v3",
    "7": "2203.02155v1",
    "8": "1806.02847v2",
    "9": "2303.12712v5",
    "10": "2305.02309v2",
    "11": "2403.07082v1",
    "12": "2401.04088v1",
    "13": "2210.02414v2",
    "14": "2003.08271v4",
    "15": "2212.10403v2",
    "16": "2201.05273v4",
    "17": "2210.15191v1",
    "18": "2303.04226v1",
    "19": "2107.13586v1",
    "20": "1103.0398v1",
    "21": "2309.10818v3",
    "22": "2305.06500v2",
    "23": "1512.01274v1",
    "24": "1606.06031v1",
    "25": "1905.00075v1",
    "26": "2407.10671v4",
    "27": "2301.13688v2",
    "28": "2305.13048v2",
    "29": "1904.10509v1",
    "30": "1910.13461v1",
    "31": "1301.3781v3",
    "32": "2303.08774v6",
    "33": "2206.14858v2",
    "34": "2106.09685v2",
    "35": "1904.09751v2",
    "36": "2212.03551v5",
    "37": "2205.05198v1",
    "38": "2102.11972v2",
    "39": "2110.14168v2",
    "40": "2001.08361v1",
    "41": "2308.12284v1",
    "42": "1310.4546v1",
    "43": "2201.10005v1",
    "44": "2302.13971v1",
    "45": "2112.06905v2",
    "46": "2402.09668v1",
    "47": "1706.03872v1",
    "48": "2310.10631v3",
    "49": "1909.08053v4",
    "50": "2111.00396v3",
    "51": "2110.14883v3",
    "52": "1804.04235v1",
    "53": "2308.12950v3",
    "54": "2309.02033v3",
    "55": "1804.07461v3",
    "56": "1911.00359v2",
    "57": "2104.05343v1",
    "58": "2402.00838v4",
    "59": "1906.08237v2",
    "60": "2212.10554v1",
    "61": "2208.04933v3",
    "62": "1606.08415v5",
    "63": "2302.10866v3",
    "64": "2103.02143v2",
    "65": "1901.02860v3",
    "66": "2307.03170v2",
    "67": "1910.03771v5",
    "68": "2304.10592v2",
    "69": "2302.14045v2",
    "70": "1607.06450v1",
    "71": "2004.08249v3",
    "72": "2307.08691v1",
    "73": "2006.16668v1",
    "74": "2212.10560v2",
    "75": "2002.04745v2",
    "76": "1810.04805v2",
    "77": "2304.15004v2",
    "78": "2205.14135v2",
    "79": "2101.03961v3",
    "80": "2305.13048v2",
    "81": "2002.05202v1",
    "82": "2306.01116v1",
    "83": "1911.02782v3",
    "84": "2303.14957v1",
    "85": "2009.01325v3",
    "86": "2202.07646v3",
    "87": "1704.01444v2",
    "88": "2305.10403v3",
    "89": "2211.05100v4",
    "90": "2209.10372v5",
    "91": "2002.08155v4",
    "92": "1911.00536v3",
    "93": "2112.00861v3",
    "94": "2209.14375v1",
    "95": "2308.01320v1",
    "96": "2212.09597v8",
    "97": "1905.12616v3",
    "98": "2202.06539v3",
    "99": "2304.08485v2",
    "100": "2306.15595v2",
    "101": "2303.10845v1",
    "102": "2007.14062v2",
    "103": "2305.16264v5",
    "104": "2307.14430v1",
    "105": "2305.13169v2"
  },
  "chooseref": {
    "1": "2303.04226v1",
    "2": "2302.09419v3",
    "3": "2202.06417v3",
    "4": "1705.04304v3",
    "5": "2112.00861v3",
    "6": "2212.10554v1",
    "7": "2305.13169v2",
    "8": "1806.02847v2",
    "9": "2212.10535v2",
    "10": "2202.13169v3",
    "11": "1804.04235v1",
    "12": "1412.6980v9",
    "13": "1809.10853v3",
    "14": "2208.01448v2",
    "15": "1506.06724v1",
    "16": "2104.05343v1",
    "17": "2304.15004v2",
    "18": "1706.03762v7",
    "19": "1910.13461v1",
    "20": "1810.04805v2",
    "21": "2211.05100v4",
    "22": "1607.01759v3",
    "23": "2309.10305v4",
    "24": "1502.03167v3",
    "25": "2007.14062v2",
    "26": "1911.00359v2",
    "27": "2201.11903v6",
    "28": "2308.12950v3",
    "29": "2002.08155v4",
    "30": "2305.02309v2",
    "31": "2105.13290v3",
    "32": "2110.14883v3",
    "33": "2203.07814v1",
    "34": "2210.15097v2",
    "35": "1808.10006v2",
    "36": "2104.08773v4",
    "37": "2211.01786v2",
    "38": "2307.09705v1",
    "39": "1911.00536v3",
    "40": "2302.03169v3",
    "41": "2309.02033v3",
    "42": "2107.06499v2",
    "43": "2202.06539v3",
    "44": "1802.05365v2",
    "45": "1912.02292v1",
    "46": "1706.03741v4",
    "47": "2308.01320v1",
    "48": "1905.12616v3",
    "49": "2211.14876v1",
    "50": "2101.02235v1",
    "51": "1310.4546v1",
    "52": "1610.02424v2",
    "53": "2102.11972v2",
    "54": "2309.03883v2",
    "55": "2402.00159v2",
    "56": "2305.10429v4",
    "57": "1301.3781v3",
    "58": "2104.04473v5",
    "59": "2309.06180v1",
    "60": "2111.00396v3",
    "61": "2206.07682v2",
    "62": "2306.15595v2",
    "63": "2305.12816v1",
    "64": "2103.13262v1",
    "65": "2109.01652v5",
    "66": "2307.08691v1",
    "67": "2205.14135v2",
    "68": "2307.03170v2",
    "69": "2210.02414v2",
    "70": "2002.05202v1",
    "71": "1804.07461v3",
    "72": "2211.09085v1",
    "73": "1606.08415v5",
    "74": "2403.08295v4",
    "75": "1904.10509v1",
    "76": "2112.06905v2",
    "77": "1609.08144v2",
    "78": "2303.08774v6",
    "79": "2201.02177v1",
    "80": "2006.16668v1",
    "81": "2301.07597v1",
    "82": "2402.09668v1",
    "83": "2302.10866v3",
    "84": "2209.14375v1",
    "85": "2308.12284v1",
    "86": "2305.06500v2",
    "87": "2403.07082v1",
    "88": "1911.12011v1",
    "89": "2201.08239v3",
    "90": "2302.14045v2",
    "91": "2005.14165v4",
    "92": "2210.07128v3",
    "93": "1607.06450v1",
    "94": "1704.01444v2",
    "95": "2009.01325v3",
    "96": "2302.13971v1",
    "97": "2310.10631v3",
    "98": "2202.00666v6",
    "99": "2106.09685v2",
    "100": "1910.03065v3",
    "101": "2312.00752v2",
    "102": "1909.08053v4",
    "103": "2304.10592v2",
    "104": "2401.04088v1",
    "105": "2110.08207v3",
    "106": "1512.01274v1",
    "107": "1103.0398v1",
    "108": "1508.07909v5",
    "109": "2212.12017v3",
    "110": "2402.00838v4",
    "111": "2002.04745v2",
    "112": "1905.00075v1",
    "113": "2110.15032v6",
    "114": "2305.10403v3",
    "115": "2303.03378v1",
    "116": "2305.16355v1",
    "117": "2303.10845v1",
    "118": "2404.14219v4",
    "119": "1806.03377v1",
    "120": "2107.13586v1",
    "121": "2003.08271v4",
    "122": "2201.05273v4",
    "123": "2202.01279v3",
    "124": "1707.06347v2",
    "125": "2304.01373v2",
    "126": "1912.01703v1",
    "127": "2202.07646v3",
    "128": "2309.16609v1",
    "129": "2407.10671v4",
    "130": "2305.13048v2",
    "131": "2103.02143v2",
    "132": "2212.09597v8",
    "133": "2004.13637v2",
    "134": "2205.05198v1",
    "135": "2401.16380v1",
    "136": "2303.06349v1",
    "137": "2307.08621v4",
    "138": "1907.11692v1",
    "139": "1910.07467v1",
    "140": "2305.13048v2",
    "141": "1911.02782v3",
    "142": "2310.12773v1",
    "143": "2305.16264v5",
    "144": "2210.11416v5",
    "145": "2205.10487v1",
    "146": "2010.14701v2",
    "147": "2001.08361v1",
    "148": "2212.10560v2",
    "149": "1808.06226v1",
    "150": "2208.04933v3",
    "151": "1706.03872v1",
    "152": "2307.14430v1",
    "153": "2310.19341v1",
    "154": "2309.10818v3",
    "155": "2206.14858v2",
    "156": "2303.12712v5",
    "157": "2305.06161v2",
    "158": "1804.10959v1",
    "159": "2101.03961v3",
    "160": "2212.03551v5",
    "161": "2201.10005v1",
    "162": "1606.06031v1",
    "163": "2306.01116v1",
    "164": "1904.09751v2",
    "165": "2406.17557v2",
    "166": "2301.13688v2",
    "167": "2101.00027v1",
    "168": "2001.08435v1",
    "169": "2211.15533v1",
    "170": "2302.04761v1",
    "171": "2212.10403v2",
    "172": "2108.12409v2",
    "173": "2204.05862v1",
    "174": "2203.15556v1",
    "175": "2203.02155v1",
    "176": "2110.14168v2",
    "177": "2210.11399v2",
    "178": "1901.02860v3",
    "179": "1910.03771v5",
    "180": "2210.15191v1",
    "181": "2004.08249v3",
    "182": "1905.03197v3",
    "183": "2202.01169v2",
    "184": "2303.04671v1",
    "185": "2304.08485v2",
    "186": "2112.09332v3",
    "187": "2209.10372v5",
    "188": "2009.13081v1",
    "189": "2204.05832v1",
    "190": "2210.15424v2",
    "191": "2309.04564v1",
    "192": "1906.08237v2",
    "193": "2309.00071v2",
    "194": "2112.12731v1",
    "195": "2303.14957v1"
  }
}