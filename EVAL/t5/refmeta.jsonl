{"paper_id": 258865657, "title": "LAraBench: Benchmarking Arabic AI with Large Language Models", "author_names": ["Ahmed Abdelali", "Hamdy Mubarak", "Shammur A. Chowdhury", "Maram Hasanain", "Basel Mousi", "Sabri Boughorbel", "Yassine El Kheir", "Daniel Izham", "Fahim Dalvi", "Majd Hawasly", "Nizi Nazar", "Y. Elshahawy", "Ahmed M. Ali", "Nadir Durrani", "Natasa Milic-Frayling", "Firoj Alam"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperformed LLMs in zero-shot learning, with a few exceptions. Notably, larger computational models with few-shot learning techniques managed to reduce these performance gaps. Our findings provide valuable insights into the applicability of LLMs for Arabic NLP and speech processing tasks.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.18653/v1/2024.eacl-long.30"}, "doi_lower": "10.18653/v1/2024.eacl-long.30"}
{"paper_id": 257663467, "title": "MEGA: Multilingual Evaluation of Generative AI", "author_names": ["Kabir Ahuja", "Rishav Hada", "Millicent Ochieng", "Prachi Jain", "Harshita Diddee", "Krithika Ramesh", "Samuel C. Maina", "Tanuja Ganu", "Sameer Segal", "Maxamed Axmed", "Kalika Bali", "Sunayana Sitaram"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.", "year": 2023, "publicationdate": "2023-03-22", "externalids": {"DOI": "10.18653/v1/2023.emnlp-main.258"}, "doi_lower": "10.18653/v1/2023.emnlp-main.258"}
{"paper_id": 258866000, "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models", "author_names": ["Daman Arora", "H. Singh", "Mausam"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.15074"}, "doi_lower": "10.48550/arxiv.2305.15074"}
{"paper_id": 244799619, "title": "A General Language Assistant as a Laboratory for Alignment", "author_names": ["Amanda Askell", "Yuntao Bai", "Anna Chen", "Dawn Drain", "Deep Ganguli", "T. Henighan", "Andy Jones", "Nicholas Joseph", "Benjamin Mann", "Nova Dassarma", "Nelson Elhage", "Zac Hatfield-Dodds", "Danny Hernandez", "John Kernion", "Kamal Ndousse", "Catherine Olsson", "Dario Amodei", "Tom B. Brown", "Jack Clark", "Sam McCandlish", "Chris Olah", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.", "year": 2021, "publicationdate": "2021-12-01", "externalids": {}, "doi_lower": null}
{"paper_id": 259095491, "title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner", "author_names": ["Yushi Bai", "Jiahao Ying", "Yixin Cao", "Xin Lv", "Yuze He", "Xiaozhi Wang", "Jifan Yu", "Kaisheng Zeng", "Yijia Xiao", "Haozhe Lyu", "Jiayin Zhang", "Juanzi Li", "Lei Hou"], "venue": "Neural Information Processing Systems", "abstract": "Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains to probe for a broad acquisition, and raise follow-up questions to engage in a more in-depth assessment. (2) Upon evaluation, the examiner combines both scoring and ranking measurements, providing a reliable result as it aligns closely with human annotations. (3) We additionally propose a decentralized Peer-examination method to address the biases in a single examiner. Our data and benchmarking results are available at: https://lmexam.com.", "year": 2023, "publicationdate": "2023-06-07", "externalids": {"DOI": "10.48550/arXiv.2306.04181"}, "doi_lower": "10.48550/arxiv.2306.04181"}
{"paper_id": 256662612, "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity", "author_names": ["Yejin Bang", "Samuel Cahyawijaya", "Nayeon Lee", "Wenliang Dai", "Dan Su", "Bryan Wilie", "Holy Lovenia", "Ziwei Ji", "Tiezheng Yu", "Willy Chung", "Quyet V. Do", "Yan Xu", "Pascale Fung"], "venue": "International Joint Conference on Natural Language Processing", "abstract": "This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn\"prompt engineering\"fashion. We also release codebase for evaluation set extraction.", "year": 2023, "publicationdate": "2023-02-08", "externalids": {"DOI": "10.18653/v1/2023.ijcnlp-main.45"}, "doi_lower": "10.18653/v1/2023.ijcnlp-main.45"}
{"paper_id": 10438447, "title": "Comparing Automatic and Human Evaluation of NLG Systems", "author_names": ["A. Belz", "Ehud Reiter"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": null, "year": 2006, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 201099722, "title": "Daniel Callahan (1930-2019).", "author_names": [], "venue": "The Hastings center report", "abstract": "Daniel Callahan, cofounder of The Hastings Center, prodigious author, and pioneer in bioethics, died on July 16, 2019, three days before his eighty-ninth birthday. Callahan created The Hastings Center with Willard Gaylin in 1969. He served as its director from 1969 to 1983 and president from 1984 to 1996, and he continued as a scholar and president emeritus until his death, publishing books and essays and leading research projects. Tributes published in the days following Callahan's death celebrated him for his role in creating bioethics, for his challenging questions and unconventional thinking, and for his ability to do incisive scholarship in a way that had a public impact.", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.1002/hast.1037"}, "doi_lower": "10.1002/hast.1037"}
{"paper_id": 257804619, "title": "ChatGPT Is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models", "author_names": ["Ning Bian", "Xianpei Han", "Le Sun", "Hongyu Lin", "Yaojie Lu", "Ben He"], "venue": "International Conference on Language Resources and Evaluation", "abstract": "Large language models (LLMs) have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point. In this paper, we specifically focus on ChatGPT, a widely used and easily accessible LLM, and ask the following questions: (1) Can ChatGPT effectively answer commonsense questions? (2) Is ChatGPT aware of the underlying commonsense knowledge for answering a specific question? (3) Is ChatGPT knowledgeable in commonsense? (4) Can ChatGPT effectively leverage commonsense for answering questions? We conduct a series of experiments on 11 datasets to evaluate ChatGPT’s commonsense abilities, including answering commonsense questions, identifying necessary knowledge, generating knowledge descriptions, and using knowledge descriptions to answer questions again. Experimental results show that: (1) ChatGPT can achieve good QA accuracies in commonsense tasks, while still struggling with certain domains of datasets. (2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question. These findings raise the need to explore improved mechanisms for effectively incorporating commonsense into LLMs like ChatGPT, such as better instruction following and commonsense guidance.", "year": 2023, "publicationdate": "2023-03-29", "externalids": {}, "doi_lower": null}
{"paper_id": 259095544, "title": "Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results", "author_names": ["Bojana Bodroža", "Bojana M. Dinić", "Ljubisa Bojic Department of Psychology", "Faculty of Philosophy", "Universityof Novi Sad", "Serbia", "Digital Society Lab", "Institute for Philosophy", "Social Theory", "U. Belgrade"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.04308"}, "doi_lower": "10.48550/arxiv.2306.04308"}
{"paper_id": 237091588, "title": "On the Opportunities and Risks of Foundation Models", "author_names": ["Rishi Bommasani", "Drew A. Hudson", "E. Adeli", "R. Altman", "Simran Arora", "Sydney von Arx", "Michael S. Bernstein", "Jeannette Bohg", "Antoine Bosselut", "E. Brunskill", "Erik Brynjolfsson", "S. Buch", "Dallas Card", "Rodrigo Castellon", "Niladri S. Chatterji", "Annie S. Chen", "Kathleen A. Creel", "Jared Davis", "Dora Demszky", "Chris Donahue", "M. Doumbouya", "Esin Durmus", "Stefano Ermon", "J. Etchemendy", "Kawin Ethayarajh", "L. Fei-Fei", "Chelsea Finn", "Trevor Gale", "Lauren Gillespie", "Karan Goel", "Noah D. Goodman", "S. Grossman", "Neel Guha", "Tatsunori Hashimoto", "Peter Henderson", "John Hewitt", "Daniel E. Ho", "Jenny Hong", "Kyle Hsu", "Jing Huang", "Thomas F. Icard", "Saahil Jain", "Dan Jurafsky", "Pratyusha Kalluri", "Siddharth Karamcheti", "G. Keeling", "Fereshte Khani", "O. Khattab", "Pang Wei Koh", "M. Krass", "Ranjay Krishna", "Rohith Kuditipudi", "Ananya Kumar", "Faisal Ladhak", "Mina Lee", "Tony Lee", "J. Leskovec", "Isabelle Levent", "Xiang Lisa Li", "Xuechen Li", "Tengyu Ma", "Ali Malik", "Christopher D. Manning", "Suvir Mirchandani", "E. Mitchell", "Zanele Munyikwa", "Suraj Nair", "A. Narayan", "D. Narayanan", "Benjamin Newman", "Allen Nie", "Juan Carlos Niebles", "H. Nilforoshan", "Julian Nyarko", "Giray Ogut", "Laurel J. Orr", "Isabel Papadimitriou", "J. Park", "C. Piech", "Eva Portelance", "Christopher Potts", "Aditi Raghunathan", "Robert Reich", "Hongyu Ren", "Frieda Rong", "Yusuf H. Roohani", "Camilo Ruiz", "Jack Ryan", "Christopher R'e", "Dorsa Sadigh", "Shiori Sagawa", "Keshav Santhanam", "Andy Shih", "K. Srinivasan", "Alex Tamkin", "Rohan Taori", "A. Thomas", "Florian Tramèr", "Rose E. Wang", "William Wang", "Bohan Wu", "Jiajun Wu", "Yuhuai Wu", "Sang Michael Xie", "Michihiro Yasunaga", "Jiaxuan You", "M. Zaharia", "Michael Zhang", "Tianyi Zhang", "Xikun Zhang", "Yuhui Zhang", "Lucia Zheng", "Kaitlyn Zhou", "Percy Liang"], "venue": "arXiv.org", "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.", "year": 2021, "publicationdate": "2021-08-16", "externalids": {}, "doi_lower": null}
{"paper_id": 145433581, "title": "Intelligence? What intelligence?", "author_names": ["R. Colom"], "venue": "Behavioral and Brain Sciences", "abstract": null, "year": 2007, "publicationdate": "2007-04-01", "externalids": {"DOI": "10.1017/S0140525X07001215"}, "doi_lower": "10.1017/s0140525x07001215"}
{"paper_id": 10986188, "title": "Class-Based n-gram Models of Natural Language", "author_names": ["P. Brown", "V. D. Pietra", "P. D. Souza", "J. Lai", "R. Mercer"], "venue": "International Conference on Computational Logic", "abstract": null, "year": 1992, "publicationdate": "1992-12-01", "externalids": {}, "doi_lower": null}
{"paper_id": 218971783, "title": "Language Models are Few-Shot Learners", "author_names": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "R. Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Ma-teusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "I. Sutskever", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "year": 2020, "publicationdate": "2020-05-28", "externalids": {}, "doi_lower": null}
{"paper_id": 257663729, "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4", "author_names": ["Sébastien Bubeck", "Varun Chandrasekaran", "Ronen Eldan", "J. Gehrke", "Eric Horvitz", "Ece Kamar", "Peter Lee", "Y. Lee", "Yuan-Fang Li", "Scott M. Lundberg", "Harsha Nori", "Hamid Palangi", "Marco Tulio Ribeiro", "Yi Zhang"], "venue": "arXiv.org", "abstract": "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.", "year": 2023, "publicationdate": "2023-03-22", "externalids": {}, "doi_lower": null}
{"paper_id": 257833897, "title": "Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study", "author_names": ["Yong Cao", "Li Zhou", "Seolhwa Lee", "Laura Cabello", "Min Chen", "Daniel Hershcovich"], "venue": "C3NLP", "abstract": "The recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like conversations. Given its usage by users from various nations and its training on a vast multilingual corpus that includes diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation. In this paper, we investigate the underlying cultural background of ChatGPT by analyzing its responses to questions designed to quantify human cultural differences. Our findings suggest that, when prompted with American context, ChatGPT exhibits a strong alignment with American culture, but it adapts less effectively to other cultural contexts. Furthermore, by using different prompts to probe the model, we show that English prompts reduce the variance in model responses, flattening out cultural differences and biasing them towards American culture. This study provides valuable insights into the cultural implications of ChatGPT and highlights the necessity of greater diversity and cultural awareness in language technologies.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {"DOI": "10.48550/arXiv.2303.17466"}, "doi_lower": "10.48550/arxiv.2303.17466"}
{"paper_id": 257312905, "title": "Evaluating the Feasibility of ChatGPT in Healthcare: An Analysis of Multiple Clinical and Research Scenarios", "author_names": ["M. Cascella", "J. Montomoli", "Valentina Bellini", "E. Bignami"], "venue": "Journal of medical systems", "abstract": "This paper aims to highlight the potential applications and limits of a large language model (LLM) in healthcare. ChatGPT is a recently developed LLM that was trained on a massive dataset of text for dialogue with users. Although AI-based language models like ChatGPT have demonstrated impressive capabilities, it is uncertain how well they will perform in real-world scenarios, particularly in fields such as medicine where high-level and complex thinking is necessary. Furthermore, while the use of ChatGPT in writing scientific articles and other scientific outputs may have potential benefits, important ethical concerns must also be addressed. Consequently, we investigated the feasibility of ChatGPT in clinical and research scenarios: (1) support of the clinical practice, (2) scientific production, (3) misuse in medicine and research, and (4) reasoning about public health topics. Results indicated that it is important to recognize and promote education on the appropriate use and potential pitfalls of AI-based LLMs in medicine.", "year": 2023, "publicationdate": "2023-03-04", "externalids": {"DOI": "10.1007/s10916-023-01925-4"}, "doi_lower": "10.1007/s10916-023-01925-4"}
{"paper_id": 257581728, "title": "Do Large Language Models Understand Chemistry? A Conversation with ChatGPT", "author_names": ["Cayque Monteiro Castro Nascimento", "A. S. Pimentel"], "venue": "Journal of Chemical Information and Modeling", "abstract": "Large language models (LLMs) have promised a revolution in answering complex questions using the ChatGPT model. Its application in chemistry is still in its infancy. This viewpoint addresses the question of how well ChatGPT understands chemistry by posing five simple tasks in different subareas of chemistry.", "year": 2023, "publicationdate": "2023-03-16", "externalids": {"DOI": "10.1021/acs.jcim.3c00285"}, "doi_lower": "10.1021/acs.jcim.3c00285"}
{"paper_id": 235755472, "title": "Evaluating Large Language Models Trained on Code", "author_names": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Pondé", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "Scott Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mo Bavarian", "Clemens Winter", "P. Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "Jan Leike", "Josh Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "I. Sutskever", "Wojciech Zaremba"], "venue": "arXiv.org", "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.", "year": 2021, "publicationdate": "2021-07-07", "externalids": {}, "doi_lower": null}
{"paper_id": 257913780, "title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study", "author_names": ["Yi Chen", "Rui Wang", "Haiyun Jiang", "Shuming Shi", "Rui-Lan Xu"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 258825860, "title": "The promise and peril of using a large language model to obtain clinical information: ChatGPT performs strongly as a fertility counseling tool with limitations.", "author_names": ["J. Chervenak", "H. Lieman", "M. Blanco-Breindel", "S. Jindal"], "venue": "Fertility and Sterility", "abstract": null, "year": 2023, "publicationdate": "2023-05-01", "externalids": {"DOI": "10.1016/j.fertnstert.2023.05.151"}, "doi_lower": "10.1016/j.fertnstert.2023.05.151"}
{"paper_id": 259108199, "title": "InstructEval: Towards Holistic Evaluation of Instruction-Tuned Large Language Models", "author_names": ["Yew Ken Chia", "Pengfei Hong", "Lidong Bing", "Soujanya Poria"], "venue": "SCALELLM", "abstract": "Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as GPT-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. However, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and lack of holistic evaluation. To address these challenges, we present InstructEval, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and training methods. Our findings reveal that the quality of instruction data is a crucial factor in scaling model performance. While open-source models demonstrate impressive writing abilities, there is substantial room for improvement in problem-solving and alignment.", "year": 2023, "publicationdate": "2023-06-07", "externalids": {"DOI": "10.48550/arXiv.2306.04757"}, "doi_lower": "10.48550/arxiv.2306.04757"}
{"paper_id": 258865939, "title": "Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark", "author_names": ["Minje Choi", "Jiaxin Pei", "Sagar Kumar", "Chang Shu", "David Jurgens"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand \\textit{social} language. Here, we introduce a new theory-driven benchmark, SocKET, that contains 58 NLP tasks testing social knowledge which we group into five categories: humor&sarcasm, offensiveness, sentiment&emotion, and trustworthiness. In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory. Through zero-shot evaluations, we show that pretrained models already possess some innate but limited capabilities of social language understanding and training on one category of tasks can improve zero-shot testing on others. Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs. The associated resources are released at https://github.com/minjechoi/SOCKET.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.18653/v1/2023.emnlp-main.699"}, "doi_lower": "10.18653/v1/2023.emnlp-main.699"}
{"paper_id": 247951931, "title": "PaLM: Scaling Language Modeling with Pathways", "author_names": ["A. Chowdhery", "Sharan Narang", "Jacob Devlin", "Maarten Bosma", "Gaurav Mishra", "Adam Roberts", "P. Barham", "Hyung Won Chung", "Charles Sutton", "Sebastian Gehrmann", "Parker Schuh", "Kensen Shi", "Sasha Tsvyashchenko", "Joshua Maynez", "Abhishek Rao", "Parker Barnes", "Yi Tay", "Noam Shazeer", "Vinodkumar Prabhakaran", "Emily Reif", "Nan Du", "Ben Hutchinson", "Reiner Pope", "James Bradbury", "Jacob Austin", "M. Isard", "Guy Gur-Ari", "Pengcheng Yin", "Toju Duke", "Anselm Levskaya", "S. Ghemawat", "Sunipa Dev", "H. Michalewski", "Xavier García", "Vedant Misra", "Kevin Robinson", "L. Fedus", "Denny Zhou", "Daphne Ippolito", "D. Luan", "Hyeontaek Lim", "Barret Zoph", "A. Spiridonov", "Ryan Sepassi", "David Dohan", "Shivani Agrawal", "Mark Omernick", "Andrew M. Dai", "Thanumalayan Sankaranarayana Pillai", "Marie Pellat", "Aitor Lewkowycz", "Erica Moreira", "R. Child", "Oleksandr Polozov", "Katherine Lee", "Zongwei Zhou", "Xuezhi Wang", "Brennan Saeta", "Mark Díaz", "Orhan Firat", "Michele Catasta", "Jason Wei", "K. Meier-Hellstern", "D. Eck", "J. Dean", "Slav Petrov", "Noah Fiedel"], "venue": "Journal of machine learning research", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "year": 2022, "publicationdate": "2022-04-05", "externalids": {}, "doi_lower": null}
{"paper_id": 4787508, "title": "Deep Reinforcement Learning from Human Preferences", "author_names": ["P. Christiano", "Jan Leike", "Tom B. Brown", "Miljan Martic", "S. Legg", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.", "year": 2017, "publicationdate": "2017-06-12", "externalids": {}, "doi_lower": null}
{"paper_id": 257496827, "title": "Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification", "author_names": ["Benjamin Clavié", "Alexandru Ciceu", "Frederick Naylor", "Guillaume Souli'e", "Thomas Brightwell"], "venue": "International Conference on Applications of Natural Language to Data Bases", "abstract": "This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language job posting is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance. Our results show that, with a well-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all other models, achieving a 6% increase in Precision@95% Recall compared to the best supervised approach. Furthermore, we observe that the wording of the prompt is a critical factor in eliciting the appropriate\"reasoning\"in the model, and that seemingly minor aspects of the prompt significantly affect the model's performance.", "year": 2023, "publicationdate": "2023-03-13", "externalids": {"DOI": "10.48550/arXiv.2303.07142"}, "doi_lower": "10.48550/arxiv.2303.07142"}
{"paper_id": 259063728, "title": "Evaluating Language Models for Mathematics through Interactions", "author_names": ["Katherine M. Collins", "Albert Qiaochu Jiang", "Simon Frieder", "L. Wong", "Miri Zilka", "Umang Bhatt", "Thomas Lukasiewicz", "Yuhuai Wu", "J. Tenenbaum", "William Hart", "T. Gowers", "Wenda Li", "Adrian Weller", "M. Jamnik"], "venue": "arXiv.org", "abstract": "The standard methodology of evaluating large language models (LLMs) based on static pairs of inputs and outputs is insufficient for developing assistants: this kind of assessments fails to take into account the essential interactive element in their deployment, and therefore limits how we understand language model capabilities. We introduce CheckMate, an adaptable prototype platform for humans to interact with and evaluate LLMs. We conduct a study with CheckMate to evaluate three language models~(InstructGPT, ChatGPT, and GPT-4) as assistants in proving undergraduate-level mathematics, with a mixed cohort of participants from undergraduate students to professors of mathematics. We release the resulting interaction and rating dataset, MathConverse. By analysing MathConverse, we derive a preliminary taxonomy of human behaviours and uncover that despite a generally positive correlation, there are notable instances of divergence between correctness and perceived helpfulness in LLM generations, amongst other findings. Further, we identify useful scenarios and existing issues of GPT-4 in mathematical reasoning through a series of case studies contributed by expert mathematicians. We conclude with actionable takeaways for ML practitioners and mathematicians: models which communicate uncertainty, respond well to user corrections, are more interpretable and concise may constitute better assistants; interactive evaluation is a promising way to continually navigate the capability of these models; humans should be aware of language models' algebraic fallibility, and for that reason discern where they should be used.", "year": 2023, "publicationdate": "2023-06-02", "externalids": {"DOI": "10.48550/arXiv.2306.01694"}, "doi_lower": "10.48550/arxiv.2306.01694"}
{"paper_id": 206787478, "title": "Support-vector networks", "author_names": ["Corinna Cortes", "V. Vapnik"], "venue": "Machine-mediated learning", "abstract": null, "year": 2004, "publicationdate": null, "externalids": {"DOI": "10.1007/BF00994018"}, "doi_lower": "10.1007/bf00994018"}
{"paper_id": 258461170, "title": "Uncovering ChatGPT’s Capabilities in Recommender Systems", "author_names": ["Sunhao Dai", "Ninglu Shao", "Haiyuan Zhao", "Weijie Yu", "Zihua Si", "Chen Xu", "ZhongXiang Sun", "Xiao Zhang", "Jun Xu"], "venue": "ACM Conference on Recommender Systems", "abstract": "The debut of ChatGPT has recently attracted significant attention from the natural language processing (NLP) community and beyond. Existing studies have demonstrated that ChatGPT shows significant improvement in a range of downstream NLP tasks, but the capabilities and limitations of ChatGPT in terms of recommendations remain unclear. In this study, we aim to enhance ChatGPT’s recommendation capabilities by aligning it with traditional information retrieval (IR) ranking capabilities, including point-wise, pair-wise, and list-wise ranking. To achieve this goal, we re-formulate the aforementioned three recommendation policies into prompt formats tailored specifically to the domain at hand. Through extensive experiments on four datasets from different domains, we analyze the distinctions among the three recommendation policies. Our findings indicate that ChatGPT achieves an optimal balance between cost and performance when equipped with list-wise ranking. This research sheds light on a promising direction for aligning ChatGPT with recommendation tasks. To facilitate further explorations in this area, the full code and detailed original results are open-sourced at https://github.com/rainym00d/LLM4RS.", "year": 2023, "publicationdate": "2023-05-03", "externalids": {"DOI": "10.1145/3604915.3610646"}, "doi_lower": "10.1145/3604915.3610646"}
{"paper_id": 263231102, "title": "Can Large Language Models Provide Feedback to Students? A Case Study on ChatGPT", "author_names": ["Wei Dai", "Jionghao Lin", "Hua Jin", "Tongguang Li", "Yi-Shan Tsai", "D. Gašević", "Guanliang Chen"], "venue": "International Conference on Advanced Learning Technologies", "abstract": "Educational feedback has been widely acknowledged as an effective approach to improving student learning. However, scaling effective practices can be laborious and costly, which motivated researchers to work on automated feedback systems (AFS). Inspired by the recent advancements in the pre-trained language models (e.g., ChatGPT), we posit that such models might advance the existing knowledge of textual feedback generation in AFS because of their capability to offer natural-sounding and detailed responses. Therefore, we aimed to investigate the feasibility of using ChatGPT to provide students with feedback to help them learn better. Our results show that i) ChatGPT is capable of generating more detailed feedback that fluently and coherently summarizes students' performance than human instructors; ii) ChatGPT achieved high agreement with the instructor when assessing the topic of students' assignments; and iii) ChatGPT could provide feedback on the process of students completing the task, which might benefit students developing learning skills.", "year": 2023, "publicationdate": "2023-07-01", "externalids": {"DOI": "10.1109/ICALT58122.2023.00100"}, "doi_lower": "10.1109/icalt58122.2023.00100"}
{"paper_id": 259137620, "title": "Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem Solving: Evidence from the Vietnamese National High School Graduation Examination", "author_names": ["Xuan-Quy Dao", "Ngoc-Bich Le"], "venue": "arXiv.org", "abstract": "This study offers a complete analysis of ChatGPT's mathematics abilities in responding to multiple-choice questions for the Vietnamese National High School Graduation Examination (VNHSGE) on a range of subjects and difficulty levels. The dataset included 250 questions divided into four levels: knowledge (K), comprehension (C), application (A), and high application (H), and it included ten themes that covered diverse mathematical concepts. The outcomes demonstrate that ChatGPT's performance varies depending on the difficulty level and subject. It performed best on questions at Level (K), with an accuracy rate of $83\\%$; but, as the difficulty level rose, it scored poorly, with an accuracy rate of $10\\%$. The study has also shown that ChatGPT significantly succeeds in providing responses to questions on subjects including exponential and logarithmic functions, geometric progression, and arithmetic progression. The study found that ChatGPT had difficulty correctly answering questions on topics including derivatives and applications, spatial geometry, and Oxyz spatial calculus. Additionally, this study contrasted ChatGPT outcomes with Vietnamese students in VNHSGE and in other math competitions. ChatGPT dominated in the SAT Math competition with a success rate of $70\\%$, followed by VNHSGE mathematics ($58.8\\%)$. However, its success rates were lower on other exams, such as AP Statistics, the GRE Quantitative, AMC 10, AMC 12, and AP Calculus BC. These results suggest that ChatGPT has the potential to be an effective teaching tool for mathematics, but more work is needed to enhance its handling of graphical data and address the challenges presented by questions that are getting more challenging.", "year": 2023, "publicationdate": "2023-06-10", "externalids": {"DOI": "10.48550/arXiv.2306.06331"}, "doi_lower": "10.48550/arxiv.2306.06331"}
{"paper_id": 261851724, "title": "Can ChatGPT Pass High School Exams on English Language Comprehension?", "author_names": ["J. D. de Winter"], "venue": "International Journal of Artificial Intelligence in Education", "abstract": "Launched in late November 2022, ChatGPT, a large language model chatbot, has garnered considerable attention. However, ongoing questions remain regarding its capabilities. In this study, ChatGPT was used to complete national high school exams in the Netherlands on the topic of English reading comprehension. In late December 2022, we submitted the exam questions through the ChatGPT web interface (GPT-3.5). According to official norms, ChatGPT achieved a mean grade of 7.3 on the Dutch scale of 1 to 10—comparable to the mean grade of all students who took the exam in the Netherlands, 6.99. However, ChatGPT occasionally required re-prompting to arrive at an explicit answer; without these nudges, the overall grade was 6.5. In March 2023, API access was made available, and a new version of ChatGPT, GPT-4, was released. We submitted the same exams to the API, and GPT-4 achieved a score of 8.3 without a need for re-prompting. Additionally, employing a bootstrapping method that incorporated randomness through ChatGPT’s ‘temperature’ parameter proved effective in self-identifying potentially incorrect answers. Finally, a re-assessment conducted with the GPT-4 model updated as of June 2023 showed no substantial change in the overall score. The present findings highlight significant opportunities but also raise concerns about the impact of ChatGPT and similar large language models on educational assessment.", "year": 2023, "publicationdate": "2023-09-13", "externalids": {"DOI": "10.1007/s40593-023-00372-z"}, "doi_lower": "10.1007/s40593-023-00372-z"}
{"paper_id": 57246310, "title": "ImageNet: A large-scale hierarchical image database", "author_names": ["Jia Deng", "Wei Dong", "R. Socher", "Li-Jia Li", "K. Li", "Li Fei-Fei"], "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition", "abstract": null, "year": 2009, "publicationdate": "2009-06-20", "externalids": {"DOI": "10.1109/CVPR.2009.5206848"}, "doi_lower": "10.1109/cvpr.2009.5206848"}
{"paper_id": 259064225, "title": "How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?", "author_names": ["Aniket Deroy", "Kripabandhu Ghosh", "Saptarshi Ghosh"], "venue": "LegalAIIA@ICAIL", "abstract": "Automatic summarization of legal case judgements has traditionally been attempted by using extractive summarization methods. However, in recent years, abstractive summarization models are gaining popularity since they can generate more natural and coherent summaries. Legal domain-specific pre-trained abstractive summarization models are now available. Moreover, general-domain pre-trained Large Language Models (LLMs), such as ChatGPT, are known to generate high-quality text and have the capacity for text summarization. Hence it is natural to ask if these models are ready for off-the-shelf application to automatically generate abstractive summaries for case judgements. To explore this question, we apply several state-of-the-art domain-specific abstractive summarization models and general-domain LLMs on Indian court case judgements, and check the quality of the generated summaries. In addition to standard metrics for summary quality, we check for inconsistencies and hallucinations in the summaries. We see that abstractive summarization models generally achieve slightly higher scores than extractive models in terms of standard summary evaluation metrics such as ROUGE and BLEU. However, we often find inconsistent or hallucinated information in the generated abstractive summaries. Overall, our investigation indicates that the pre-trained abstractive summarization models and LLMs are not yet ready for fully automatic deployment for case judgement summarization; rather a human-in-the-loop approach including manual checks for inconsistencies is more suitable at present.", "year": 2023, "publicationdate": "2023-06-02", "externalids": {"DOI": "10.48550/arXiv.2306.01248"}, "doi_lower": "10.48550/arxiv.2306.01248"}
{"paper_id": 258060002, "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models", "author_names": ["A. Deshpande", "Vishvak Murahari", "Tanmay Rajpurohit", "A. Kalyan", "Karthik Narasimhan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and harmful to an unsuspecting user. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.", "year": 2023, "publicationdate": "2023-04-11", "externalids": {"DOI": "10.48550/arXiv.2304.05335"}, "doi_lower": "10.48550/arxiv.2304.05335"}
{"paper_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "author_names": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/N19-1423"}, "doi_lower": "10.18653/v1/n19-1423"}
{"paper_id": 231719337, "title": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation", "author_names": ["J. Dhamala", "Tony Sun", "Varun Kumar", "Satyapriya Krishna", "Yada Pruksachatkun", "Kai-Wei Chang", "Rahul Gupta"], "venue": "Conference on Fairness, Accountability and Transparency", "abstract": "Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.", "year": 2021, "publicationdate": "2021-01-27", "externalids": {"DOI": "10.1145/3442188.3445924"}, "doi_lower": "10.1145/3442188.3445924"}
{"paper_id": 258865545, "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback", "author_names": ["Yann Dubois", "Xuechen Li", "Rohan Taori", "Tianyi Zhang", "Ishaan Gulrajani", "Jimmy Ba", "Carlos Guestrin", "Percy Liang", "Tatsunori Hashimoto"], "venue": "Neural Information Processing Systems", "abstract": "Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their strong instruction-following abilities. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following requires tackling three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 50x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, DPO, best-of-n, expert iteration, and more) that learn from pairwise feedback. Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of real human feedback and show that rankings of models trained in AlpacaFarm match rankings of models trained on human data. As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a +10% improvement in win-rate against Davinci003. We release all components of AlpacaFarm at https://github.com/tatsu-lab/alpaca_farm.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.48550/arXiv.2305.14387"}, "doi_lower": "10.48550/arxiv.2305.14387"}
{"paper_id": 258959776, "title": "Analysis of large-language model versus human performance for genetics questions", "author_names": ["Dat Van Duong", "B. Solomon"], "venue": "European Journal of Human Genetics", "abstract": null, "year": 2023, "publicationdate": "2023-05-29", "externalids": {"DOI": "10.1038/s41431-023-01396-8"}, "doi_lower": "10.1038/s41431-023-01396-8"}
{"paper_id": 259342486, "title": "Recommender Systems in the Era of Large Language Models (LLMs)", "author_names": ["Wenqi Fan", "Zihuai Zhao", "Jiatong Li", "Yunqing Liu", "Xiaowei Mei", "Yiqi Wang", "Jiliang Tang", "Qing Li"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "abstract": "With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an indispensable and important component, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have achieved significant advancements in enhancing recommender systems, these DNN-based methods still exhibit some limitations, such as inferior capabilities to effectively capture textual side information about users and items, difficulties in generalization to various recommendation scenarios, and reasoning on their predictions, etc. Meanwhile, the development of Large Language Models (LLMs), such as ChatGPT and GPT-4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization capabilities and reasoning skills. As a result, recent studies have actively attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems. Therefore, in this survey, we comprehensively review LLM-empowered recommender systems from various perspectives including pre-training, fine-tuning, and prompting paradigms. More specifically, we first introduce the representative methods to learn user and item representations, leveraging LLMs as feature encoders. Then, we systematically review the emerging advanced techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss the promising future directions in this emerging field.", "year": 2023, "publicationdate": "2023-07-05", "externalids": {"DOI": "10.1109/TKDE.2024.3392335"}, "doi_lower": "10.1109/tkde.2024.3392335"}
{"paper_id": 248887532, "title": "DDXPlus: A new Dataset for Medical Automatic Diagnosis", "author_names": ["Arsène Fansi Tchango", "Zhi Wen", "Rishab Goel", "J. Ghosn"], "venue": "arXiv.org", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2205.09148"}, "doi_lower": "10.48550/arxiv.2205.09148"}
{"paper_id": 258041203, "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models", "author_names": ["Emilio Ferrara"], "venue": "First Monday", "abstract": "As generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI.", "year": 2023, "publicationdate": "2023-04-07", "externalids": {"DOI": "10.5210/fm.v28i11.13346"}, "doi_lower": "10.5210/fm.v28i11.13346"}
{"paper_id": 228954221, "title": "GPT-3: Its Nature, Scope, Limits, and Consequences", "author_names": ["L. Floridi", "Massimo Chiriatti"], "venue": "Minds and Machines", "abstract": "In this commentary, we discuss the nature of reversible and irreversible questions, that is, questions that may enable one to identify the nature of the source of their answers. We then introduce GPT-3, a third-generation, autoregressive language model that uses deep learning to produce human-like texts, and use the previous distinction to analyse it. We expand the analysis to present three tests based on mathematical, semantic (that is, the Turing Test), and ethical questions and show that GPT-3 is not designed to pass any of them. This is a reminder that GPT-3 does not do what it is not supposed to do, and that any interpretation of GPT-3 as the beginning of the emergence of a general form of artificial intelligence is merely uninformed science fiction. We conclude by outlining some of the significant consequences of the industrialisation of automatic and cheap production of good, semantic artefacts.", "year": 2020, "publicationdate": "2020-11-01", "externalids": {"DOI": "10.1007/s11023-020-09548-1"}, "doi_lower": "10.1007/s11023-020-09548-1"}
{"paper_id": 259713140, "title": "Baby steps in evaluating the capacities of large language models", "author_names": ["Michael C. Frank"], "venue": "Nature Reviews Psychology", "abstract": null, "year": 2023, "publicationdate": "2023-06-27", "externalids": {"DOI": "10.1038/s44159-023-00211-x"}, "doi_lower": "10.1038/s44159-023-00211-x"}
{"paper_id": 256415984, "title": "Mathematical Capabilities of ChatGPT", "author_names": ["Simon Frieder", "Luca Pinchetti", "Ryan-Rhys Griffiths", "Tommaso Salvatori", "Thomas Lukasiewicz", "Philipp Petersen", "Alexis Chevalier", "J. Berner"], "venue": "Neural Information Processing Systems", "abstract": "We investigate the mathematical capabilities of two iterations of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel methodology. In contrast to formal mathematics, where large databases of formal proofs are available (e.g., the Lean Mathematical Library), current datasets of natural-language mathematics, used to benchmark language models, either cover only elementary mathematics or are very small. We address this by publicly releasing two new datasets: GHOSTS and miniGHOSTS. These are the first natural-language datasets curated by working researchers in mathematics that (1) aim to cover graduate-level mathematics, (2) provide a holistic overview of the mathematical capabilities of language models, and (3) distinguish multiple dimensions of mathematical reasoning. These datasets also test whether ChatGPT and GPT-4 can be helpful assistants to professional mathematicians by emulating use cases that arise in the daily professional activities of mathematicians. We benchmark the models on a range of fine-grained performance metrics. For advanced mathematics, this is the most detailed evaluation effort to date. We find that ChatGPT can be used most successfully as a mathematical assistant for querying facts, acting as a mathematical search engine and knowledge base interface. GPT-4 can additionally be used for undergraduate-level mathematics but fails on graduate-level difficulty. Contrary to many positive reports in the media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of selection bias), their overall mathematical performance is well below the level of a graduate student. Hence, if your goal is to use ChatGPT to pass a graduate-level math exam, you would be better off copying from your average peer!", "year": 2023, "publicationdate": "2023-01-31", "externalids": {}, "doi_lower": null}
{"paper_id": 259243928, "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models", "author_names": ["Chaoyou Fu", "Peixian Chen", "Yunhang Shen", "Yulei Qin", "Mengdan Zhang", "Xu Lin", "Zhenyu Qiu", "Wei Lin", "Jinrui Yang", "Xiawu Zheng", "Ke Li", "Xing Sun", "Rongrong Ji"], "venue": "arXiv.org", "abstract": "Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data are released at the project page https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.", "year": 2023, "publicationdate": "2023-06-23", "externalids": {"DOI": "10.48550/arXiv.2306.13394"}, "doi_lower": "10.48550/arxiv.2306.13394"}
{"paper_id": 258959433, "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance", "author_names": ["Yao Fu", "Litu Ou", "Mingyu Chen", "Yuhao Wan", "Hao-Chun Peng", "Tushar Khot"], "venue": "arXiv.org", "abstract": "As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and PaLM-2 are the only two models that are comparable with GPT-4, while open-sourced models still lag behind; (3) LLaMA-65B performs closely to code-davinci-002, indicating that with successful further development such as reinforcement learning from human feedback (RLHF), it has great potential to be close to GPT-3.5-Turbo. Our results also suggest that for the open-source efforts to catch up, the community may focus more on building better base models and exploring RLHF.", "year": 2023, "publicationdate": "2023-05-26", "externalids": {"DOI": "10.48550/arXiv.2305.17306"}, "doi_lower": "10.48550/arxiv.2305.17306"}
{"paper_id": 21933061, "title": "Estimation of prediction error by using K-fold cross-validation", "author_names": ["Tadayoshi Fushiki"], "venue": "Statistics and computing", "abstract": null, "year": 2011, "publicationdate": "2011-04-01", "externalids": {"DOI": "10.1007/s11222-009-9153-8"}, "doi_lower": "10.1007/s11222-009-9153-8"}
{"paper_id": 31763947, "title": "Perceptron-based learning algorithms", "author_names": ["S. I. Gallant"], "venue": "IEEE Trans. Neural Networks", "abstract": null, "year": 1990, "publicationdate": "1990-06-01", "externalids": {"DOI": "10.1109/72.80230"}, "doi_lower": "10.1109/72.80230"}
{"paper_id": 254274920, "title": "Adaptive Testing of Computer Vision Models", "author_names": ["Irena Gao", "Gabriel Ilharco", "Scott M. Lundberg", "Marco Tulio Ribeiro"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Vision models often fail systematically on groups of data that share common semantic characteristics (e.g., rare objects or unusual scenes), but identifying these failure modes is a challenge. We introduce AdaVision, an interactive process for testing vision models which helps users identify and fix coherent failure modes. Given a natural language description of a coherent group, AdaVision retrieves relevant images from LAION-5B with CLIP. The user then labels a small amount of data for model correctness, which is used in successive retrieval rounds to hill-climb towards high-error regions, refining the group definition. Once a group is saturated, AdaVision uses GPT-3 to suggest new group descriptions for the user to explore. We demonstrate the usefulness and generality of AdaVision in user studies, where users find major bugs in state-of-the-art classification, object detection, and image captioning models. These user-discovered groups have failure rates 2-3x higher than those surfaced by automatic error clustering methods. Finally, finetuning on examples found with AdaVision fixes the discovered bugs when evaluated on unseen examples, without degrading in-distribution accuracy, and while also improving performance on out-of-distribution datasets.", "year": 2022, "publicationdate": "2022-12-06", "externalids": {"DOI": "10.1109/ICCV51070.2023.00370"}, "doi_lower": "10.1109/iccv51070.2023.00370"}
{"paper_id": 322920, "title": "Introduction to the special issue on statistical language modeling", "author_names": ["Jianfeng Gao", "Chin-Yew Lin"], "venue": "TALIP", "abstract": null, "year": 2004, "publicationdate": "2004-06-01", "externalids": {"DOI": "10.1145/1034780.1034781"}, "doi_lower": "10.1145/1034780.1034781"}
{"paper_id": 229923710, "title": "Making Pre-trained Language Models Better Few-shot Learners", "author_names": ["Tianyu Gao", "Adam Fisch", "Danqi Chen"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF—better few-shot fine-tuning of language models—a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.", "year": 2021, "publicationdate": "2021-01-01", "externalids": {"DOI": "10.18653/v1/2021.acl-long.295"}, "doi_lower": "10.18653/v1/2021.acl-long.295"}
{"paper_id": 221878771, "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models", "author_names": ["Samuel Gehman", "Suchin Gururangan", "Maarten Sap", "Yejin Choi", "Noah A. Smith"], "venue": "Findings", "abstract": "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning “bad” words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.", "year": 2020, "publicationdate": "2020-09-24", "externalids": {"DOI": "10.18653/v1/2020.findings-emnlp.301"}, "doi_lower": "10.18653/v1/2020.findings-emnlp.301"}
{"paper_id": 491127, "title": "Selective Classification for Deep Neural Networks", "author_names": ["Yonatan Geifman", "Ran El-Yaniv"], "venue": "Neural Information Processing Systems", "abstract": "Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2% error in top-5 ImageNet classification can be guaranteed with probability 99.9%, and almost 60% test coverage.", "year": 2017, "publicationdate": "2017-05-01", "externalids": {}, "doi_lower": null}
{"paper_id": 258762340, "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models", "author_names": ["Zorik Gekhman", "Jonathan Herzig", "Roee Aharoni", "C. Elkind", "Idan Szpektor"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.", "year": 2023, "publicationdate": "2023-05-18", "externalids": {"DOI": "10.48550/arXiv.2305.11171"}, "doi_lower": "10.48550/arxiv.2305.11171"}
{"paper_id": 258988045, "title": "Large Language Models Are Not Strong Abstract Reasoners", "author_names": ["Gaël Gendron", "Qiming Bao", "M. Witbrock", "G. Dobbie"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Large Language Models have shown tremendous performance on a large variety of natural language processing tasks, ranging from text comprehension to common sense reasoning. \n\nHowever, the mechanisms responsible for this success remain opaque, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally circumscribed. Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data. Evaluating deep neural architectures on this task could give insight into their potential limitations regarding reasoning and their broad generalisation abilities, yet this is currently an under-explored area. In this paper, we introduce a new benchmark for evaluating language models beyond memorization on abstract reasoning tasks. We perform extensive evaluations of state-of-the-art LLMs, showing that they currently achieve very limited performance in contrast with other natural language tasks, even when applying techniques that have been shown to improve performance on other NLP tasks. We argue that guiding LLM generation to follow causal paths could help improve the generalisation and reasoning abilities of LLMs.", "year": 2023, "publicationdate": "2023-05-31", "externalids": {"DOI": "10.24963/ijcai.2024/693"}, "doi_lower": "10.24963/ijcai.2024/693"}
{"paper_id": 256663603, "title": "How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment", "author_names": ["Aidan Gilson", "Conrad W. Safranek", "Thomas Huang", "V. Socrates", "Ling Chi", "R. Taylor", "David Chartash"], "venue": "JMIR Medical Education", "abstract": "Background Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods We used 2 sets of multiple-choice questions to evaluate ChatGPT’s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT’s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT’s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT’s capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning.", "year": 2023, "publicationdate": "2023-02-08", "externalids": {"DOI": "10.2196/45312"}, "doi_lower": "10.2196/45312"}
{"paper_id": 2570757, "title": "Moral Foundations Theory: The Pragmatic Validity of Moral Pluralism", "author_names": ["J. Graham", "J. Haidt", "S. Koleva", "Matt Motyl", "Ravi Iyer", "S. Wojcik", "Peter H. Ditto"], "venue": "", "abstract": null, "year": 2012, "publicationdate": "2012-11-28", "externalids": {"DOI": "10.1016/B978-0-12-407236-7.00002-4"}, "doi_lower": "10.1016/b978-0-12-407236-7.00002-4"}
{"paper_id": 259129613, "title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation", "author_names": ["Zhouhong Gu", "Xiaoxuan Zhu", "Haoning Ye", "Lin Zhang", "Jianchen Wang", "Sihang Jiang", "Zhuozhi Xiong", "Zihan Li", "Qi He", "Rui Xu", "Wenhao Huang", "Weiguo Zheng", "Hongwei Feng", "Yanghua Xiao"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "New Natural Langauge Process~(NLP) benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present Xiezhi, the most comprehensive evaluation suite designed to assess holistic domain knowledge.Xiezhi comprises multiple-choice questions across 516 diverse disciplines ranging from 13 different subjects with 249,587 questions and accompanied by Xiezhi-Specialty with 14,041 questions and Xiezhi-Interdiscipline with 10,746 questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results indicate that LLMs exceed average performance of humans in science, engineering, agronomy, medicine, and art, but fall short in economics, jurisprudence, pedagogy, literature, history, and management. All the evaluation code and data are open sourced in https://github.com/MikeGu721/XiezhiBenchmark", "year": 2023, "publicationdate": "2023-06-09", "externalids": {"DOI": "10.48550/arXiv.2306.05783"}, "doi_lower": "10.48550/arxiv.2306.05783"}
{"paper_id": 28671436, "title": "On Calibration of Modern Neural Networks", "author_names": ["Chuan Guo", "Geoff Pleiss", "Yu Sun", "Kilian Q. Weinberger"], "venue": "International Conference on Machine Learning", "abstract": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.", "year": 2017, "publicationdate": "2017-06-14", "externalids": {}, "doi_lower": null}
{"paper_id": 263875917, "title": "What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks", "author_names": ["Taicheng Guo", "Kehan Guo", "B. Nan", "Zhengwen Liang", "Zhichun Guo", "N. Chawla", "Olaf Wiest", "Xiangliang Zhang"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.18365"}, "doi_lower": "10.48550/arxiv.2305.18365"}
{"paper_id": 263828908, "title": "Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models - and Disappeared in GPT-4", "author_names": ["Thilo Hagendorff", "Sarah Fabi"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.07622"}, "doi_lower": "10.48550/arxiv.2306.07622"}
{"paper_id": 259076294, "title": "Evaluation of AI Chatbots for Patient-Specific EHR Questions", "author_names": ["Alaleh Hamidi", "Kirk Roberts"], "venue": "arXiv.org", "abstract": "This paper investigates the use of artificial intelligence chatbots for patient-specific question answering (QA) from clinical notes using several large language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google Bard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and coherence of the answers generated by each model using a 5-point Likert scale on a set of patient-specific questions.", "year": 2023, "publicationdate": "2023-06-05", "externalids": {"DOI": "10.48550/arXiv.2306.02549"}, "doi_lower": "10.48550/arxiv.2306.02549"}
{"paper_id": 7567061, "title": "Equality of Opportunity in Supervised Learning", "author_names": ["Moritz Hardt", "Eric Price", "N. Srebro"], "venue": "Neural Information Processing Systems", "abstract": "We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.", "year": 2016, "publicationdate": "2016-10-07", "externalids": {}, "doi_lower": null}
{"paper_id": 255440573, "title": "The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation", "author_names": ["Jochen Hartmann", "Jasper Schwenzow", "Maximilian Witte"], "venue": "Social Science Research Network", "abstract": "Conversational artificial intelligence (AI) disrupts how humans interact with technology. Recently, OpenAI introduced ChatGPT, a state-of-the-art dialogue model that can converse with its human counterparts with unprecedented capabilities. ChatGPT has witnessed tremendous attention from the media, academia, industry, and the general public, attracting more than a million users within days of its release. However, its explosive adoption for information search and as an automated decision aid underscores the importance to understand its limitations and biases. This paper focuses on one of democratic society's most important decision-making processes: political elections. Prompting ChatGPT with 630 political statements from two leading voting advice applications and the nation-agnostic political compass test in three pre-registered experiments, we uncover ChatGPT's pro-environmental, left-libertarian ideology. For example, ChatGPT would impose taxes on flights, restrict rent increases, and legalize abortion. In the 2021 elections, it would have voted most likely for the Greens both in Germany (B\\\"undnis 90/Die Gr\\\"unen) and in the Netherlands (GroenLinks). Our findings are robust when negating the prompts, reversing the order of the statements, varying prompt formality, and across languages (English, German, Dutch, and Spanish). We conclude by discussing the implications of politically biased conversational AI on society.", "year": 2023, "publicationdate": "2023-01-05", "externalids": {"DOI": "10.48550/arXiv.2301.01768"}, "doi_lower": "10.48550/arxiv.2301.01768"}
{"paper_id": 262043773, "title": "Can Large Language Models Understand Real-World Complex Instructions?", "author_names": ["Qi He", "Jie Zeng", "Wenhao Huang", "Lina Chen", "Jin Xiao", "Qianxi He", "Xunzhe Zhou", "Lida Chen", "Xintao Wang", "Yuncheng Huang", "Haoning Ye", "Zihan Li", "Shisong Chen", "Yikai Zhang", "Zhouhong Gu", "Jiaqing Liang", "Yanghua Xiao"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Large language models (LLMs) can understand human instructions, showing their potential for pragmatic applications beyond traditional NLP tasks. However, they still struggle with complex instructions, which can be either complex task descriptions that require multiple tasks and constraints, or complex input that contains long context, noise, heterogeneous information and multi-turn format. Due to these features, LLMs often ignore semantic constraints from task descriptions, generate incorrect formats, violate length or sample count constraints, and be unfaithful to the input text. Existing benchmarks are insufficient to assess LLMs’ ability to understand complex instructions, as they are close-ended and simple. To bridge this gap, we propose CELLO, a benchmark for evaluating LLMs' ability to follow complex instructions systematically. We design eight features for complex instructions and construct a comprehensive evaluation dataset from real-world scenarios. We also establish four criteria and develop corresponding metrics, as current ones are inadequate, biased or too strict and coarse-grained. We compare the performance of representative Chinese-oriented and English-oriented models in following complex instructions through extensive experiments. Resources of CELLO are publicly available at https://github.com/Abbey4799/CELLO.", "year": 2023, "publicationdate": "2023-09-17", "externalids": {"DOI": "10.48550/arXiv.2309.09150"}, "doi_lower": "10.48550/arxiv.2309.09150"}
{"paper_id": 259129577, "title": "Exploring the Responses of Large Language Models to Beginner Programmers’ Help Requests", "author_names": ["Arto Hellas", "Juho Leinonen", "Sami Sarsa", "Charles Koutcheme", "Lilja Kujanpää", "Juha Sorva"], "venue": "International Computing Education Research Workshop", "abstract": "Background and Context: Over the past year, large language models (LLMs) have taken the world by storm. In computing education, like in other walks of life, many opportunities and threats have emerged as a consequence. Objectives: In this article, we explore such opportunities and threats in a specific area: responding to student programmers’ help requests. More specifically, we assess how good LLMs are at identifying issues in problematic code that students request help on. Method: We collected a sample of help requests and code from an online programming course. We then prompted two different LLMs (OpenAI Codex and GPT-3.5) to identify and explain the issues in the students’ code and assessed the LLM-generated answers both quantitatively and qualitatively. Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently find at least one actual issue in each student program (GPT-3.5 in 90% of the cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57% of the time). False positives are common (40% chance for GPT-3.5). The advice that the LLMs provide on the issues is often sensible. The LLMs perform better on issues involving program logic rather than on output formatting. Model solutions are frequently provided even when the LLM is prompted not to. LLM responses to prompts in a non-English language are only slightly worse than responses to English prompts. Implications: Our results continue to highlight the utility of LLMs in programming education. At the same time, the results highlight the unreliability of LLMs: LLMs make some of the same mistakes that students do, perhaps especially when formatting output as required by automated assessment systems. Our study informs teachers interested in using LLMs as well as future efforts to customize LLMs for the needs of programming education.", "year": 2023, "publicationdate": "2023-06-09", "externalids": {"DOI": "10.1145/3568813.3600139"}, "doi_lower": "10.1145/3568813.3600139"}
{"paper_id": 234790100, "title": "Measuring Coding Challenge Competence With APPS", "author_names": ["Dan Hendrycks", "Steven Basart", "Saurav Kadavath", "Mantas Mazeika", "Akul Arora", "Ethan Guo", "Collin Burns", "Samir Puranik", "Horace He", "D. Song", "J. Steinhardt"], "venue": "NeurIPS Datasets and Benchmarks", "abstract": "While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we then evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an important measure for tracking advancements.", "year": 2021, "publicationdate": "2021-05-20", "externalids": {}, "doi_lower": null}
{"paper_id": 220968818, "title": "Aligning AI With Shared Human Values", "author_names": ["Dan Hendrycks", "Collin Burns", "Steven Basart", "Andrew Critch", "J. Li", "D. Song", "J. Steinhardt"], "venue": "International Conference on Learning Representations", "abstract": "We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete understanding of basic ethical knowledge. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.", "year": 2020, "publicationdate": "2020-08-05", "externalids": {}, "doi_lower": null}
{"paper_id": 221516475, "title": "Measuring Massive Multitask Language Understanding", "author_names": ["Dan Hendrycks", "Collin Burns", "Steven Basart", "Andy Zou", "Mantas Mazeika", "D. Song", "J. Steinhardt"], "venue": "International Conference on Learning Representations", "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.", "year": 2020, "publicationdate": "2020-09-07", "externalids": {}, "doi_lower": null}
{"paper_id": 232170369, "title": "CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review", "author_names": ["Dan Hendrycks", "Collin Burns", "Anya Chen", "Spencer Ball"], "venue": "NeurIPS Datasets and Benchmarks", "abstract": "Many specialized domains remain untouched by deep learning, as large labeled datasets require expensive expert annotators. We address this bottleneck within the legal domain by introducing the Contract Understanding Atticus Dataset (CUAD), a new dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review. We find that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size. Despite these promising results, there is still substantial room for improvement. As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.", "year": 2021, "publicationdate": "2021-03-10", "externalids": {}, "doi_lower": null}
{"paper_id": 232134851, "title": "Measuring Mathematical Problem Solving With the MATH Dataset", "author_names": ["Dan Hendrycks", "Collin Burns", "Saurav Kadavath", "Akul Arora", "Steven Basart", "Eric Tang", "D. Song", "J. Steinhardt"], "venue": "NeurIPS Datasets and Benchmarks", "abstract": "Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.", "year": 2021, "publicationdate": "2021-03-05", "externalids": {}, "doi_lower": null}
{"paper_id": 257921233, "title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics", "author_names": ["J. Holmes", "Zheng Liu", "Lian-Cheng Zhang", "Yuzhen Ding", "T. Sio", "L. Mcgee", "J. Ashman", "Xiang Li", "Tianming Liu", "Jiajian Shen", "W. Liu"], "venue": "Frontiers in Oncology", "abstract": "Purpose We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs. Methods We developed an exam consisting of 100 radiation oncology physics questions based on our expertise. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. The performance of ChatGPT (GPT-4) was further explored by being asked to explain first, then answer. The deductive reasoning capability of ChatGPT (GPT-4) was evaluated using a novel approach (substituting the correct answer with “None of the above choices is the correct answer.”). A majority vote analysis was used to approximate how well each group could score when working together. Results ChatGPT GPT-4 outperformed all other LLMs and medical physicists, on average, with improved accuracy when prompted to explain before answering. ChatGPT (GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices across a number of trials, whether correct or incorrect, a characteristic that was not observed in the human test groups or Bard (LaMDA). In evaluating deductive reasoning ability, ChatGPT (GPT-4) demonstrated surprising accuracy, suggesting the potential presence of an emergent ability. Finally, although ChatGPT (GPT-4) performed well overall, its intrinsic properties did not allow for further improvement when scoring based on a majority vote across trials. In contrast, a team of medical physicists were able to greatly outperform ChatGPT (GPT-4) using a majority vote. Conclusion This study suggests a great potential for LLMs to work alongside radiation oncology experts as highly knowledgeable assistants.", "year": 2023, "publicationdate": "2023-04-01", "externalids": {"DOI": "10.3389/fonc.2023.1219326"}, "doi_lower": "10.3389/fonc.2023.1219326"}
{"paper_id": 247694170, "title": "TRUE: Re-evaluating Factual Consistency Evaluation", "author_names": ["Or Honovich", "Roee Aharoni", "Jonathan Herzig", "Hagai Taitelbaum", "Doron Kukliansy", "Vered Cohen", "Thomas Scialom", "Idan Szpektor", "Avinatan Hassidim", "Yossi Matias"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2204.04991"}, "doi_lower": "10.48550/arxiv.2204.04991"}
{"paper_id": 262084258, "title": "Choice-75: A Dataset on Decision Branching in Script Learning", "author_names": ["Zhaoyi Hou", "Li Zhang", "Chris Callison-Burch"], "venue": "International Conference on Language Resources and Evaluation", "abstract": "Script learning studies how daily events unfold. It enables machines to reason about narratives with implicit information. Previous works mainly consider a script as a linear sequence of events while ignoring the potential branches that arise due to people’s circumstantial choices. We hence propose Choice-75, the first benchmark that challenges intelligent systems to make decisions given descriptive scenarios, containing 75 scripts and more than 600 scenarios. We also present preliminary results with current large language models (LLM). Although they demonstrate overall decent performances, there is still notable headroom in hard scenarios.", "year": 2023, "publicationdate": "2023-09-21", "externalids": {"DOI": "10.48550/arXiv.2309.11737"}, "doi_lower": "10.48550/arxiv.2309.11737"}
{"paper_id": 260682960, "title": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench", "author_names": ["Jen-Tse Huang", "Man Ho Lam", "E. Li", "Shujie Ren", "Wenxuan Wang", "Wenxiang Jiao", "Zhaopeng Tu", "Michael R. Lyu"], "venue": "arXiv.org", "abstract": "Evaluating Large Language Models' (LLMs) anthropomorphic capabilities has become increasingly important in contemporary discourse. Utilizing the emotion appraisal theory from psychology, we propose to evaluate the empathy ability of LLMs, i.e., how their feelings change when presented with specific situations. After a careful and comprehensive survey, we collect a dataset containing over 400 situations that have proven effective in eliciting the eight emotions central to our study. Categorizing the situations into 36 factors, we conduct a human evaluation involving more than 1,200 subjects worldwide. With the human evaluation results as references, our evaluation includes seven LLMs, covering both commercial and open-source models, including variations in model sizes, featuring the latest iterations, such as GPT-4, Mixtral-8x22B, and LLaMA-3.1. We find that, despite several misalignments, LLMs can generally respond appropriately to certain situations. Nevertheless, they fall short in alignment with the emotional behaviors of human beings and cannot establish connections between similar situations. Our collected dataset of situations, the human evaluation results, and the code of our testing framework, i.e., EmotionBench, are publicly available at https://github.com/CUHK-ARISE/EmotionBench.", "year": 2023, "publicationdate": "2023-08-07", "externalids": {"DOI": "10.48550/arXiv.2308.03656"}, "doi_lower": "10.48550/arxiv.2308.03656"}
{"paper_id": 257219775, "title": "Language Is Not All You Need: Aligning Perception with Language Models", "author_names": ["Shaohan Huang", "Li Dong", "Wenhui Wang", "Y. Hao", "Saksham Singhal", "Shuming Ma", "Tengchao Lv", "Lei Cui", "O. Mohammed", "Qiang Liu", "Kriti Aggarwal", "Zewen Chi", "Johan Bjorck", "Vishrav Chaudhary", "Subhojit Som", "Xia Song", "Furu Wei"], "venue": "Neural Information Processing Systems", "abstract": "A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.", "year": 2023, "publicationdate": "2023-02-27", "externalids": {"DOI": "10.48550/arXiv.2302.14045"}, "doi_lower": "10.48550/arxiv.2302.14045"}
{"paper_id": 258685666, "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models", "author_names": ["Yuzhen Huang", "Yuzhuo Bai", "Zhihao Zhu", "Junlei Zhang", "Jinghan Zhang", "Tangjun Su", "Junteng Liu", "Chuancheng Lv", "Yikai Zhang", "Jiayi Lei", "Fanchao Qi", "Yao Fu", "Maosong Sun", "Junxian He"], "venue": "Neural Information Processing Systems", "abstract": "New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users.", "year": 2023, "publicationdate": "2023-05-15", "externalids": {"DOI": "10.48550/arXiv.2305.08322"}, "doi_lower": "10.48550/arxiv.2305.08322"}
{"paper_id": 259202452, "title": "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models", "author_names": ["Yue Huang", "Qihui Zhang", "Philip S. Yu", "Lichao Sun"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) such as ChatGPT, have gained significant attention due to their impressive natural language processing capabilities. It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs. Therefore, this study aims to address these gaps by introducing a new benchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in three crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT examines toxicity in language models by employing toxic prompt templates derived from social norms. It then quantifies the extent of bias in models by measuring quantifiable toxicity values across different groups. Lastly, TrustGPT assesses the value of conversation generation models from both active value-alignment and passive value-alignment tasks. Through the implementation of TrustGPT, this research aims to enhance our understanding of the performance of conversation generation models and promote the development of language models that are more ethical and socially responsible.", "year": 2023, "publicationdate": "2023-06-20", "externalids": {"DOI": "10.48550/arXiv.2306.11507"}, "doi_lower": "10.48550/arxiv.2306.11507"}
{"paper_id": 269009978, "title": "The Hallucinations Leaderboard - An Open Effort to Measure Hallucinations in Large Language Models", "author_names": ["Giwon Hong", "Aryo Pradipta Gema", "Rohit Saxena", "Xiaotang Du", "Ping Nie", "Yu Zhao", "Laura Perez-Beltrachini", "Max Ryabinin", "Xuanli He", "Pasquale Minervini"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) have transformed the Natural Language Processing (NLP) landscape with their remarkable ability to understand and generate human-like text. However, these models are prone to ``hallucinations'' -- outputs that do not align with factual reality or the input context. This paper introduces the Hallucinations Leaderboard, an open initiative to quantitatively measure and compare the tendency of each model to produce hallucinations. The leaderboard uses a comprehensive set of benchmarks focusing on different aspects of hallucinations, such as factuality and faithfulness, across various tasks, including question-answering, summarisation, and reading comprehension. Our analysis provides insights into the performance of different models, guiding researchers and practitioners in choosing the most reliable models for their applications.", "year": 2024, "publicationdate": "2024-04-08", "externalids": {"DOI": "10.48550/arXiv.2404.05904"}, "doi_lower": "10.48550/arxiv.2404.05904"}
{"paper_id": 259096053, "title": "Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers", "author_names": ["Israt Jahan", "Md Tahmid Rahman Laskar", "Chun Peng", "J. Huang"], "venue": "Workshop on Biomedical Natural Language Processing", "abstract": "ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT’s pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that lack large annotated data.", "year": 2023, "publicationdate": "2023-06-07", "externalids": {"DOI": "10.48550/arXiv.2306.04504"}, "doi_lower": "10.48550/arxiv.2306.04504"}
{"paper_id": 259243643, "title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models", "author_names": ["Neel Jain", "Khalid Saifullah", "Yuxin Wen", "John Kirchenbauer", "Manli Shu", "Aniruddha Saha", "Micah Goldblum", "Jonas Geiping", "T. Goldstein"], "venue": "arXiv.org", "abstract": "With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence, in addition to sensitivity to grammatical structure and tokenization errors. When comparisons to similar human-labeled benchmarks are available, we find strong correlations between self-supervised and human-supervised evaluations. The self-supervised paradigm complements current evaluation strategies that rely on labeled data.", "year": 2023, "publicationdate": "2023-06-23", "externalids": {"DOI": "10.48550/arXiv.2306.13651"}, "doi_lower": "10.48550/arxiv.2306.13651"}
{"paper_id": 236278033, "title": "Online question and answer sessions: How students support their own and other students' processes of inquiry in a text-based learning environment", "author_names": ["Malin Jansson", "Stefan Hrastinski", "Stefan Stenbom", "Fredrik Enoksson"], "venue": "Internet and Higher Education", "abstract": null, "year": 2021, "publicationdate": "2021-06-12", "externalids": {"DOI": "10.1016/J.IHEDUC.2021.100817"}, "doi_lower": "10.1016/j.iheduc.2021.100817"}
{"paper_id": 259095915, "title": "ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models", "author_names": ["Sophie F. Jentzsch", "K. Kersting"], "venue": "Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis", "abstract": "Humor is a central aspect of human communication that has not been solved for artificial agents so far. Large language models (LLMs) are increasingly able to capture implicit and contextual information. Especially, OpenAI’s ChatGPT recently gained immense public attention. The GPT3-based model almost seems to communicate on a human level and can even tell jokes. Humor is an essential component of human communication. But is ChatGPT really funny?We put ChatGPT’s sense of humor to the test. In a series of exploratory experiments around jokes, i.e., generation, explanation, and detection, we seek to understand ChatGPT’s capability to grasp and reproduce human humor. Since the model itself is not accessible, we applied prompt-based experiments. Our empirical evidence indicates that jokes are not hard-coded but mostly also not newly generated by the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system accurately explains valid jokes but also comes up with fictional explanations for invalid jokes. Joke-typical characteristics can mislead ChatGPT in the classification of jokes. ChatGPT has not solved computational humor yet but it can be a big leap toward “funny” machines.", "year": 2023, "publicationdate": "2023-06-07", "externalids": {"DOI": "10.48550/arXiv.2306.04563"}, "doi_lower": "10.48550/arxiv.2306.04563"}
{"paper_id": 259501579, "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset", "author_names": ["Jiaming Ji", "Mickel Liu", "Juntao Dai", "Xuehai Pan", "Chi Zhang", "Ce Bian", "Ruiyang Sun", "Yizhou Wang", "Yaodong Yang"], "venue": "Neural Information Processing Systems", "abstract": "In this paper, we introduce the \\textsc{BeaverTails} dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 30,207 question-answer (QA) pairs and 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails. Warning: this paper contains example data that may be offensive or harmful.", "year": 2023, "publicationdate": "2023-07-10", "externalids": {"DOI": "10.48550/arXiv.2307.04657"}, "doi_lower": "10.48550/arxiv.2307.04657"}
{"paper_id": 258714753, "title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data", "author_names": ["Jinhao Jiang", "Kun Zhou", "Zican Dong", "Keming Ye", "Wayne Xin Zhao", "Ji-rong Wen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "In this paper, we study how to improve the zero-shot reasoning ability of large language models~(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \\emph{Iterative Reading-then-Reasoning~(IRR)} approach for solving question answering tasks based on structured data, called \\textbf{StructGPT}. In our approach, we construct the specialized function to collect relevant evidence from structured data (\\ie \\emph{reading}), and let LLMs concentrate the reasoning task based on the collected information (\\ie \\emph{reasoning}). Specially, we propose an \\emph{invoking-linearization-generation} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at~\\url{https://github.com/RUCAIBox/StructGPT}.", "year": 2023, "publicationdate": "2023-05-16", "externalids": {"DOI": "10.48550/arXiv.2305.09645"}, "doi_lower": "10.48550/arxiv.2305.09645"}
{"paper_id": 257437276, "title": "Assessing the Accuracy and Reliability of AI-Generated Medical Responses: An Evaluation of the Chat-GPT Model", "author_names": ["Douglas B. Johnson", "Rachel S Goodman", "J. Patrinely", "Cosby A Stone", "Eli Zimmerman", "R. Donald", "Sam S Chang", "Sean T Berkowitz", "Avni P Finn", "Eiman Jahangir", "Elizabeth A Scoville", "Tyler Reese", "Debra E. Friedman", "Julie A. Bastarache", "Yuri F van der Heijden", "Jordan Wright", "Nicholas Carter", "Matthew R Alexander", "Jennifer H Choe", "Cody A Chastain", "J. Zic", "Sara Horst", "Isik Turker", "R. Agarwal", "E. Osmundson", "K. Idrees", "C. Kiernan", "Chandrasekhar Padmanabhan", "C. Bailey", "Cameron Schlegel", "L. Chambless", "Mike Gibson", "Travis J. Osterman", "Lee E. Wheless"], "venue": "Research Square", "abstract": "Background: Natural language processing models such as ChatGPT can generate text-based content and are poised to become a major information source in medicine and beyond. The accuracy and completeness of ChatGPT for medical queries is not known. Methods: Thirty-three physicians across 17 specialties generated 284 medical questions that they subjectively classified as easy, medium, or hard with either binary (yes/no) or descriptive answers. The physicians then graded ChatGPT-generated answers to these questions for accuracy (6-point Likert scale; range 1 – completely incorrect to 6 – completely correct) and completeness (3-point Likert scale; range 1 – incomplete to 3 - complete plus additional context). Scores were summarized with descriptive statistics and compared using Mann-Whitney U or Kruskal-Wallis testing. Results: Across all questions (n=284), median accuracy score was 5.5 (between almost completely and completely correct) with mean score of 4.8 (between mostly and almost completely correct). Median completeness score was 3 (complete and comprehensive) with mean score of 2.5. For questions rated easy, medium, and hard, median accuracy scores were 6, 5.5, and 5 (mean 5.0, 4.7, and 4.6; p=0.05). Accuracy scores for binary and descriptive questions were similar (median 6 vs. 5; mean 4.9 vs. 4.7; p=0.07). Of 36 questions with scores of 1-2, 34 were re-queried/re-graded 8-17 days later with substantial improvement (median 2 vs. 4; p<0.01). Conclusions: ChatGPT generated largely accurate information to diverse medical queries as judged by academic physician specialists although with important limitations. Further research and model development are needed to correct inaccuracies and for validation.", "year": 2023, "publicationdate": "2023-02-28", "externalids": {"DOI": "10.21203/rs.3.rs-2566942/v1"}, "doi_lower": "10.21203/rs.3.rs-2566942/v1"}
{"paper_id": 26501419, "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "author_names": ["Mandar Joshi", "Eunsol Choi", "Daniel S. Weld", "Luke Zettlemoyer"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study.", "year": 2017, "publicationdate": "2017-05-01", "externalids": {"DOI": "10.18653/v1/P17-1147"}, "doi_lower": "10.18653/v1/p17-1147"}
{"paper_id": 250451161, "title": "Language Models (Mostly) Know What They Know", "author_names": ["Saurav Kadavath", "Tom Conerly", "Amanda Askell", "T. Henighan", "Dawn Drain", "Ethan Perez", "Nicholas Schiefer", "Z. Dodds", "Nova Dassarma", "Eli Tran-Johnson", "Scott Johnston", "S. El-Showk", "Andy Jones", "Nelson Elhage", "Tristan Hume", "Anna Chen", "Yuntao Bai", "Sam Bowman", "Stanislav Fort", "Deep Ganguli", "Danny Hernandez", "Josh Jacobson", "John Kernion", "Shauna Kravec", "Liane Lovitt", "Kamal Ndousse", "Catherine Olsson", "Sam Ringer", "Dario Amodei", "Tom B. Brown", "Jack Clark", "Nicholas Joseph", "Benjamin Mann", "Sam McCandlish", "Chris Olah", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability\"P(True)\"that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict\"P(IK)\", the probability that\"I know\"the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.", "year": 2022, "publicationdate": "2022-07-11", "externalids": {"DOI": "10.48550/arXiv.2207.05221"}, "doi_lower": "10.48550/arxiv.2207.05221"}
{"paper_id": 248496374, "title": "MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning", "author_names": ["Ehud Karpas", "Omri Abend", "Yonatan Belinkov", "Barak Lenz", "Opher Lieber", "Nir Ratner", "Y. Shoham", "Hofit Bata", "Yoav Levine", "Kevin Leyton-Brown", "Dor Muhlgay", "N. Rozen", "Erez Schwartz", "Gal Shachaf", "Shai Shalev-Shwartz", "A. Shashua", "Moshe Tenenholtz"], "venue": "arXiv.org", "abstract": "Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced\"miracle\") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.", "year": 2022, "publicationdate": "2022-05-01", "externalids": {"DOI": "10.48550/arXiv.2205.00445"}, "doi_lower": "10.48550/arxiv.2205.00445"}
{"paper_id": 257445349, "title": "ChatGPT for good? On opportunities and challenges of large language models for education", "author_names": ["Enkelejda Kasneci", "Kathrin Seßler", "S. Küchemann", "M. Bannert", "Daryna Dementieva", "F. Fischer", "Urs Gasser", "G. Groh", "Stephan Günnemann", "Eyke Hüllermeier", "Stephan Krusche", "Gitta Kutyniok", "Tilman Michaeli", "Claudia Nerdel", "J. Pfeffer", "Oleksandra Poquet", "Michael Sailer", "Albrecht Schmidt", "T. Seidel", "Matthias Stadler", "J. Weller", "Jochen Kuhn", "Gjergji Kasneci"], "venue": "Learning and Individual Differences", "abstract": null, "year": 2023, "publicationdate": "2023-04-01", "externalids": {"DOI": "10.1016/j.lindif.2023.102274"}, "doi_lower": "10.1016/j.lindif.2023.102274"}
{"paper_id": 145433581, "title": "Intelligence? What intelligence?", "author_names": ["R. Colom"], "venue": "Behavioral and Brain Sciences", "abstract": null, "year": 2007, "publicationdate": "2007-04-01", "externalids": {"DOI": "10.1017/S0140525X07001215"}, "doi_lower": "10.1017/s0140525x07001215"}
{"paper_id": 259108337, "title": "covLLM: Large Language Models for COVID-19 Biomedical Literature", "author_names": ["Y. Khan", "Clarisse Hokia", "Jennifer Xu", "Ben Ehlert"], "venue": "arXiv.org", "abstract": "The COVID-19 pandemic led to 1.1 million deaths in the United States, despite the explosion of coronavirus research. These new findings are slow to translate to clinical interventions, leading to poorer patient outcomes and unnecessary deaths. One reason is that clinicians, overwhelmed by patients, struggle to keep pace with the rate of new coronavirus literature. A potential solution is developing a tool for evaluating coronavirus literature using large language models (LLMs) -- neural networks that are deployed for natural language processing. LLMs can be used to summarize and extract user-specified information. The greater availability and advancement of LLMs and pre-processed coronavirus literature databases provide the opportunity to assist clinicians in evaluating coronavirus literature through a coronavirus literature specific LLM (covLLM), a tool that directly takes an inputted research article and a user query to return an answer. Using the COVID-19 Open Research Dataset (CORD-19), we produced two datasets: (1) synCovid, which uses a combination of handwritten prompts and synthetic prompts generated using OpenAI, and (2) real abstracts, which contains abstract and title pairs. covLLM was trained with LLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca and synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real abstract datasets. These models were evaluated by two human evaluators and ChatGPT. Results demonstrate that training covLLM on the synCovid and abstract pairs datasets performs competitively with ChatGPT and outperforms covLLM trained primarily using the Alpaca dataset.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.48550/arXiv.2306.04926"}, "doi_lower": "10.48550/arxiv.2306.04926"}
{"paper_id": 233444226, "title": "Dynabench: Rethinking Benchmarking in NLP", "author_names": ["Douwe Kiela", "Max Bartolo", "Yixin Nie", "Divyansh Kaushik", "Atticus Geiger", "Zhengxuan Wu", "Bertie Vidgen", "Grusha Prasad", "Amanpreet Singh", "Pratik Ringshia", "Zhiyi Ma", "Tristan Thrush", "Sebastian Riedel", "Zeerak Talat", "Pontus Stenetorp", "Robin Jia", "Mohit Bansal", "Christopher Potts", "Adina Williams"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.", "year": 2021, "publicationdate": "2021-04-07", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.324"}, "doi_lower": "10.18653/v1/2021.naacl-main.324"}
{"paper_id": 2702042, "title": "A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection", "author_names": ["Ron Kohavi"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": null, "year": 1995, "publicationdate": "1995-08-20", "externalids": {}, "doi_lower": null}
{"paper_id": 39718, "title": "Recurrent Neural Network Based Language Modeling in Meeting Recognition", "author_names": ["Stefan Kombrink", "Tomas Mikolov", "M. Karafiát", "L. Burget"], "venue": "Interspeech", "abstract": null, "year": 2011, "publicationdate": null, "externalids": {"DOI": "10.21437/Interspeech.2011-720"}, "doi_lower": "10.21437/interspeech.2011-720"}
{"paper_id": 254876189, "title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models", "author_names": ["Tiffany H. Kung", "Morgan Cheatham", "Arielle Medenilla", "Czarina Sillos", "Lorie De Leon", "Camille Elepaño", "Maria Madriaga", "Rimel Aggabao", "Giezel Diaz-Candido", "James Maningo", "Victor Tseng"], "venue": "medRxiv", "abstract": "We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, even clinical decision-making.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.1371/journal.pdig.0000198"}, "doi_lower": "10.1371/journal.pdig.0000198"}
{"paper_id": 86611921, "title": "Natural Questions: A Benchmark for Question Answering Research", "author_names": ["T. Kwiatkowski", "J. Palomaki", "Olivia Redfield", "Michael Collins", "Ankur P. Parikh", "Chris Alberti", "D. Epstein", "I. Polosukhin", "Jacob Devlin", "Kenton Lee", "Kristina Toutanova", "Llion Jones", "Matthew Kelcey", "Ming-Wei Chang", "Andrew M. Dai", "Jakob Uszkoreit", "Quoc V. Le", "Slav Petrov"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.", "year": 2019, "publicationdate": "2019-08-01", "externalids": {"DOI": "10.1162/tacl_a_00276"}, "doi_lower": "10.1162/tacl_a_00276"}
{"paper_id": 257485880, "title": "Evaluating the use of large language model in identifying top research questions in gastroenterology", "author_names": ["A. Lahat", "E. Shachar", "B. Avidan", "Zina Shatz", "Benjamin S. Glicksberg", "E. Klang"], "venue": "Scientific Reports", "abstract": "The field of gastroenterology (GI) is constantly evolving. It is essential to pinpoint the most pressing and important research questions. To evaluate the potential of chatGPT for identifying research priorities in GI and provide a starting point for further investigation. We queried chatGPT on four key topics in GI: inflammatory bowel disease, microbiome, Artificial Intelligence in GI, and advanced endoscopy in GI. A panel of experienced gastroenterologists separately reviewed and rated the generated research questions on a scale of 1–5, with 5 being the most important and relevant to current research in GI. chatGPT generated relevant and clear research questions. Yet, the questions were not considered original by the panel of gastroenterologists. On average, the questions were rated 3.6 ± 1.4, with inter-rater reliability ranging from 0.80 to 0.98 ( p  < 0.001). The mean grades for relevance, clarity, specificity, and originality were 4.9 ± 0.1, 4.6 ± 0.4, 3.1 ± 0.2, 1.5 ± 0.4, respectively. Our study suggests that Large Language Models (LLMs) may be a useful tool for identifying research priorities in the field of GI, but more work is needed to improve the novelty of the generated research questions.", "year": 2023, "publicationdate": "2023-03-13", "externalids": {"DOI": "10.1038/s41598-023-31412-2"}, "doi_lower": "10.1038/s41598-023-31412-2"}
{"paper_id": 258079179, "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning", "author_names": ["Viet Dac Lai", "Nghia Trung Ngo", "Amir Pouran Ben Veyseh", "Hieu Man", "Franck Dernoncourt", "Trung Bui", "Thien Huu Nguyen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current research. Our work aims to fill this gap for the evaluation of ChatGPT and similar LLMs to provide more comprehensive information for multilingual NLP applications. While this work will be an ongoing effort to include additional experiments in the future, our current paper evaluates ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources. We also focus on the zero-shot learning setting for ChatGPT to improve reproducibility and better simulate the interactions of general users. Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.", "year": 2023, "publicationdate": "2023-04-12", "externalids": {"DOI": "10.48550/arXiv.2304.05613"}, "doi_lower": "10.48550/arxiv.2304.05613"}
{"paper_id": 257364995, "title": "ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design", "author_names": ["P. Lanzi", "D. Loiacono"], "venue": "Annual Conference on Genetic and Evolutionary Computation", "abstract": "Large language models (LLMs) have taken the scientific world by storm, changing the landscape of natural language processing and human-computer interaction. These powerful tools can answer complex questions and, surprisingly, perform challenging creative tasks (e.g., generate code and applications to solve problems, write stories, pieces of music, etc.). In this paper, we present a collaborative game design framework that combines interactive evolution and large language models to simulate the typical human design process. We use the former to exploit users' feedback for selecting the most promising ideas and large language models for a very complex creative task---the recombination and variation of ideas. In our framework, the process starts with a brief and a set of candidate designs, either generated using a language model or proposed by the users. Next, users collaborate on the design process by providing feedback to an interactive genetic algorithm that selects, recombines, and mutates the most promising designs. We evaluated our framework on three game design tasks with human designers who collaborated remotely.", "year": 2023, "publicationdate": "2023-02-09", "externalids": {"DOI": "10.1145/3583131.3590351"}, "doi_lower": "10.1145/3583131.3590351"}
{"paper_id": 258967462, "title": "A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets", "author_names": ["Md Tahmid Rahman Laskar", "M Saiful Bari", "Mizanur Rahman", "Md Amran Hossen Bhuiyan", "Shafiq R. Joty", "J. Huang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instructions that we mostly found in ChatGPT and other instruction-tuned models. Our extensive evaluation shows that even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks. By providing a thorough assessment of ChatGPT's performance across diverse NLP tasks, this paper sets the stage for a targeted deployment of ChatGPT-like LLMs in real-world applications.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {"DOI": "10.48550/arXiv.2305.18486"}, "doi_lower": "10.48550/arxiv.2305.18486"}
{"paper_id": 263887216, "title": "An Evaluation of Log Parsing with ChatGPT", "author_names": ["Van-Hoang Le", "Hongyu Zhang"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.01590"}, "doi_lower": "10.48550/arxiv.2306.01590"}
{"paper_id": 259129342, "title": "Can Large Language Models Infer Causation from Correlation?", "author_names": ["Zhijing Jin", "Jiarui Liu", "Zhiheng Lyu", "Spencer Poff", "Mrinmaya Sachan", "Rada Mihalcea", "Mona T. Diab", "B. Scholkopf"], "venue": "International Conference on Learning Representations", "abstract": "Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.", "year": 2023, "publicationdate": "2023-06-09", "externalids": {"DOI": "10.48550/arXiv.2306.05836"}, "doi_lower": "10.48550/arxiv.2306.05836"}
{"paper_id": 204960716, "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "author_names": ["M. Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdel-rahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.", "year": 2019, "publicationdate": "2019-10-29", "externalids": {"DOI": "10.18653/v1/2020.acl-main.703"}, "doi_lower": "10.18653/v1/2020.acl-main.703"}
{"paper_id": 260334888, "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension", "author_names": ["Bohao Li", "Rui Wang", "Guangzhi Wang", "Yuying Ge", "Yixiao Ge", "Ying Shan"], "venue": "arXiv.org", "abstract": "Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.", "year": 2023, "publicationdate": "2023-07-30", "externalids": {"DOI": "10.48550/arXiv.2307.16125"}, "doi_lower": "10.48550/arxiv.2307.16125"}
{"paper_id": 259164635, "title": "CMMLU: Measuring massive multitask language understanding in Chinese", "author_names": ["Haonan Li", "Yixuan Zhang", "Fajri Koto", "Yifei Yang", "Hai Zhao", "Yeyun Gong", "Nan Duan", "Tim Baldwin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "As the capabilities of large language models (LLMs) continue to advance, evaluating their performance becomes increasingly crucial and challenging. This paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural science, social sciences, engineering, and humanities. We conduct a thorough evaluation of 18 advanced multilingual- and Chinese-oriented LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%. This highlights significant room for improvement in LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models within the Chinese context.", "year": 2023, "publicationdate": "2023-06-15", "externalids": {"DOI": "10.48550/arXiv.2306.09212"}, "doi_lower": "10.48550/arxiv.2306.09212"}
{"paper_id": 276344352, "title": "API-Bank: A Benchmark for Tool-Augmented LLMs", "author_names": ["Minghao Li", "Feifan Song", "Bowen Yu", "Haiyang Yu", "Zhoujun Li", "Fei Huang", "Yongbin Li"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2304.08244"}, "doi_lower": "10.48550/arxiv.2304.08244"}
{"paper_id": 258822859, "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights", "author_names": ["Ruyu Li", "Wenhao Deng", "Yu Cheng", "Zheng Yuan", "Jiaqi Zhang", "Fajie Yuan"], "venue": "Proceedings of the 34th ACM International Conference on Information and Knowledge Management", "abstract": "Text-based collaborative filtering (TCF) has emerged as the prominent technique for text and news recommendation, employing language models (LMs) as text encoders to represent items. However, the current landscape of TCF models mainly relies on the utilization of relatively small or medium-sized LMs. The potential impact of using larger, more powerful language models (such as these with over 100 billion parameters) as item encoders on recommendation performance remains uncertain. Can we anticipate unprecedented results and discover new insights? To address this question, we undertake a comprehensive series of experiments aimed at exploring the performance limits of the TCF paradigm. Specifically, we progressively augment the scale of item encoders, ranging from one hundred million to one hundred billion parameters, in order to reveal the scaling limits of the TCF paradigm. Moreover, we investigate whether these exceptionally large LMs have the potential to establish a universal item representation for the recommendation task, thereby revolutionizing the traditional ID paradigm, which is considered a significant obstacle to developing transferable ''one model fits all'' recommender models. Our study not only demonstrates positive results but also uncovers unexpected negative outcomes, illuminating the current state of the TCF paradigm within the community. These findings will evoke deep reflection and inspire further research on text-based recommender systems.", "year": 2023, "publicationdate": "2023-05-19", "externalids": {"DOI": "10.1145/3746252.3761429"}, "doi_lower": "10.1145/3746252.3761429"}
{"paper_id": 259261931, "title": "A Survey on Out-of-Distribution Evaluation of Neural NLP Models", "author_names": ["Xinzhe Li", "Ming Liu", "Shang Gao", "Wray L. Buntine"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Adversarial robustness, domain generalization and dataset biases are three active lines of research contributing to out-of-distribution (OOD) evaluation on neural NLP models.\n\n\n\nHowever, a comprehensive, integrated discussion of the three research lines is still lacking in the literature. This survey will 1) compare the three lines of research under a unifying definition; 2) summarize their data-generating processes and evaluation protocols for each line of research; and 3) emphasize the challenges and opportunities for future work.", "year": 2023, "publicationdate": "2023-06-27", "externalids": {"DOI": "10.24963/ijcai.2023/749"}, "doi_lower": "10.24963/ijcai.2023/749"}
{"paper_id": 269302417, "title": "Automatic Layout Planning for Visually-Rich Documents with Instruction-Following Models", "author_names": ["Wanrong Zhu", "Jennifer Healey", "Ruiyi Zhang", "William Yang Wang", "Tong Sun"], "venue": "ALVR", "abstract": "Recent advancements in instruction-following models have made user interactions with models more user-friendly and efficient, broadening their applicability. In graphic design, non-professional users often struggle to create visually appealing layouts due to limited skills and resources. In this work, we introduce a novel multimodal instruction-following framework for layout planning, allowing users to easily arrange visual elements into tailored layouts by specifying canvas size and design purpose, such as for book covers, posters, brochures, or menus. We developed three layout reasoning tasks to train the model in understanding and executing layout instructions. Experiments on two benchmarks show that our method not only simplifies the design process for non-professionals but also surpasses the performance of few-shot GPT-4V models, with mIoU higher by 12% on Crello. This progress highlights the potential of multimodal instruction-following models to automate and simplify the design process, providing an approachable solution for a wide range of design tasks on visually-rich documents.", "year": 2024, "publicationdate": "2024-04-23", "externalids": {"DOI": "10.48550/arXiv.2404.15271"}, "doi_lower": "10.48550/arxiv.2404.15271"}
{"paper_id": 258740697, "title": "Evaluating Object Hallucination in Large Vision-Language Models", "author_names": ["Yifan Li", "Yifan Du", "Kun Zhou", "Jinpeng Wang", "Wayne Xin Zhao", "Ji-rong Wen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructions and generation styles of LVLMs. Thus, we further design an improved evaluation method for object hallucination by proposing a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way. Our codes and data are publicly available at https://github.com/RUCAIBox/POPE.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {"DOI": "10.48550/arXiv.2305.10355"}, "doi_lower": "10.48550/arxiv.2305.10355"}
{"paper_id": 263423935, "title": "Holistic Evaluation of Language Models", "author_names": ["Percy Liang", "Rishi Bommasani", "Tony Lee", "Dimitris Tsipras", "Dilara Soylu", "Michihiro Yasunaga", "Yian Zhang", "Deepak Narayanan", "Yuhuai Wu", "Ananya Kumar", "Benjamin Newman", "Binhang Yuan", "Bobby Yan", "Ce Zhang", "Christian Cosgrove", "Christopher D. Manning", "Christopher Ré", "Diana Acosta-Navas", "Drew A. Hudson", "E. Zelikman", "Esin Durmus", "Faisal Ladhak", "Frieda Rong", "Hongyu Ren", "Huaxiu Yao", "Jue Wang", "Keshav Santhanam", "Laurel J. Orr", "Lucia Zheng", "Mert Yüksekgönül", "Mirac Suzgun", "Nathan Kim", "Neel Guha", "Niladri S. Chatterji", "O. Khattab", "Peter Henderson", "Qian Huang", "Ryan Chi", "Sang Michael Xie", "Shibani Santurkar", "Surya Ganguli", "Tatsunori Hashimoto", "Thomas Icard", "Tianyi Zhang", "Vishrav Chaudhary", "William Wang", "Xuechen Li", "Yifan Mai", "Yuhui Zhang", "Yuta Koreeda"], "venue": "arXiv.org", "abstract": "Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.", "year": 2022, "publicationdate": "2022-11-16", "externalids": {"DOI": "10.48550/arXiv.2211.09110"}, "doi_lower": "10.48550/arxiv.2211.09110"}
{"paper_id": 264828822, "title": "Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models", "author_names": ["Tian Liang", "Zhiwei He", "Jen-Tse Huang", "Wenxuan Wang", "Wenxiang Jiao", "Rui Wang", "Yujiu Yang", "Zhaopeng Tu", "Shuming Shi", "Xing Wang"], "venue": "arXiv.org", "abstract": "The automatic evaluation of LLM-based agent intelligence is critical in developing advanced LLM-based agents. Although considerable effort has been devoted to developing human-annotated evaluation datasets, such as AlpacaEval, existing techniques are costly, time-consuming, and lack adaptability. In this paper, inspired by the popular language game ``Who is Spy'', we propose to use the word guessing game to assess the intelligence performance of LLMs. Given a word, the LLM is asked to describe the word and determine its identity (spy or not) based on its and other players' descriptions. Ideally, an advanced agent should possess the ability to accurately describe a given word using an aggressive description while concurrently maximizing confusion in the conservative description, enhancing its participation in the game. To this end, we first develop DEEP to evaluate LLMs' expression and disguising abilities. DEEP requires LLM to describe a word in aggressive and conservative modes. We then introduce SpyGame, an interactive multi-agent framework designed to assess LLMs' intelligence through participation in a competitive language-based board game. Incorporating multi-agent interaction, SpyGame requires the target LLM to possess linguistic skills and strategic thinking, providing a more comprehensive evaluation of LLMs' human-like cognitive abilities and adaptability in complex communication situations. The proposed evaluation framework is very easy to implement. We collected words from multiple sources, domains, and languages and used the proposed evaluation framework to conduct experiments. Extensive experiments demonstrate that the proposed DEEP and SpyGame effectively evaluate the capabilities of various LLMs, capturing their ability to adapt to novel situations and engage in strategic communication.", "year": 2023, "publicationdate": "2023-10-31", "externalids": {"DOI": "10.48550/arXiv.2310.20499"}, "doi_lower": "10.48550/arxiv.2310.20499"}
{"paper_id": 265456112, "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation", "author_names": ["Xun Liang", "Shichao Song", "Simin Niu", "Zhiyu Li", "Feiyu Xiong", "Bo Tang", "Zhaohui Wy", "Dawei He", "Peng Cheng", "Zhonghao Wang", "Haiying Deng"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large language models (LLMs) have emerged as pivotal contributors in contemporary natural language processing and are increasingly being applied across a diverse range of industries. However, these large-scale probabilistic statistical models cannot currently ensure the requisite quality in professional content generation. These models often produce hallucinated text, compromising their practical utility in professional contexts. To assess the authentic reliability of LLMs in text generation, numerous initiatives have developed benchmark evaluations for hallucination phenomena. Nevertheless, these benchmarks frequently utilize constrained generation techniques due to cost and temporal constraints. These techniques encompass the use of directed hallucination induction and strategies that deliberately alter authentic text to produce hallucinations. These approaches are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations in text generation is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, designed to compile outputs produced with minimal restrictions by LLMs. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also executed extensive experiments, evaluating prominent Chinese language models and the GPT series models to derive professional performance insights regarding hallucination challenges.", "year": 2023, "publicationdate": "2023-11-26", "externalids": {"DOI": "10.18653/v1/2024.acl-long.288"}, "doi_lower": "10.18653/v1/2024.acl-long.288"}
{"paper_id": 250627547, "title": "Can large language models reason about medical questions?", "author_names": ["Valentin Li'evin", "C. Hother", "A. Motzfeldt", "O. Winther"], "venue": "Patterns", "abstract": null, "year": 2022, "publicationdate": "2022-07-17", "externalids": {"DOI": "10.1016/j.patter.2024.100943"}, "doi_lower": "10.1016/j.patter.2024.100943"}
{"paper_id": 964287, "title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author_names": ["Chin-Yew Lin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": null, "year": 2004, "publicationdate": "2004-07-25", "externalids": {}, "doi_lower": null}
{"paper_id": 237532606, "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods", "author_names": ["Stephanie C. Lin", "Jacob Hilton", "Owain Evans"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.", "year": 2021, "publicationdate": "2021-09-08", "externalids": {"DOI": "10.18653/v1/2022.acl-long.229"}, "doi_lower": "10.18653/v1/2022.acl-long.229"}
{"paper_id": 14113767, "title": "Microsoft COCO: Common Objects in Context", "author_names": ["Tsung-Yi Lin", "M. Maire", "Serge J. Belongie", "James Hays", "P. Perona", "Deva Ramanan", "Piotr Dollár", "C. L. Zitnick"], "venue": "European Conference on Computer Vision", "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.", "year": 2014, "publicationdate": "2014-05-01", "externalids": {"DOI": "10.1007/978-3-319-10602-1_48"}, "doi_lower": "10.1007/978-3-319-10602-1_48"}
{"paper_id": 258841681, "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models", "author_names": ["Yen-Ting Lin", "Yun-Nung (Vivian) Chen"], "venue": "NLP4CONVAI", "abstract": "We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.13711"}, "doi_lower": "10.48550/arxiv.2305.13711"}
{"paper_id": 258740849, "title": "M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models", "author_names": ["Chuang Liu", "Renren Jin", "Yuqi Ren", "Linhao Yu", "Tianyu Dong", "Xia Peng", "Shuting Zhang", "Jianxiang Peng", "Peiyi Zhang", "Qingqing Lyu", "Xiaowen Su", "Qun Liu", "Deyi Xiong"], "venue": "arXiv.org", "abstract": "Large language models have recently made tremendous progress in a variety of aspects, e.g., cross-task generalization, instruction following. Comprehensively evaluating the capability of large language models in multiple tasks is of great importance. In this paper, we propose M3KE, a Massive Multi-Level Multi-Subject Knowledge Evaluation benchmark, which is developed to measure knowledge acquired by Chinese large language models by testing their multitask accuracy in zero- and few-shot settings. We have collected 20,477 questions from 71 tasks. Our selection covers all major levels of Chinese education system, ranging from the primary school to college, as well as a wide variety of subjects, including humanities, history, politics, law, education, psychology, science, technology, art and religion. All questions are multiple-choice questions with four options, hence guaranteeing a standardized and unified assessment process. We've assessed a number of state-of-the-art open-source Chinese large language models on the proposed benchmark. The size of these models varies from 335M to 130B parameters. Experiment results demonstrate that they perform significantly worse than GPT-3.5 that reaches an accuracy of ~ 48% on M3KE. The dataset is available at https://github.com/tjunlp-lab/M3KE.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {"DOI": "10.48550/arXiv.2305.10263"}, "doi_lower": "10.48550/arxiv.2305.10263"}
{"paper_id": 259251834, "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning", "author_names": ["Fuxiao Liu", "Kevin Lin", "Linjie Li", "Jianfeng Wang", "Y. Yacoob", "Lijuan Wang"], "venue": "International Conference on Learning Representations", "abstract": "Despite the promising progress in multi-modal tasks, current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions. Moreover, we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving performance on several public datasets compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data are available at https://github.com/FuxiaoLiu/LRV-Instruction.", "year": 2023, "publicationdate": "2023-06-26", "externalids": {}, "doi_lower": null}
{"paper_id": 258041354, "title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4", "author_names": ["Hanmeng Liu", "Ruoxi Ning", "Zhiyang Teng", "Jian Liu", "Qiji Zhou", "Yuexin Zhang"], "venue": "arXiv.org", "abstract": "Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as\"advanced\"at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. With early access to the GPT-4 API we are able to conduct intense experiments on the GPT-4 model. The results show GPT-4 yields even higher performance on most logical reasoning datasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known datasets like LogiQA and ReClor. However, the performance drops significantly when handling newly released and out-of-distribution datasets. Logical reasoning remains challenging for ChatGPT and GPT-4, especially on out-of-distribution and natural language inference datasets. We release the prompt-style logical reasoning datasets as a benchmark suite and name it LogiEval.", "year": 2023, "publicationdate": "2023-04-07", "externalids": {"DOI": "10.48550/arXiv.2304.03439"}, "doi_lower": "10.48550/arxiv.2304.03439"}
{"paper_id": 258437095, "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation", "author_names": ["Jiawei Liu", "Chun Xia", "Yuyao Wang", "Lingming Zhang"], "venue": "Neural Information Processing Systems", "abstract": "Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.", "year": 2023, "publicationdate": "2023-05-02", "externalids": {}, "doi_lower": null}
{"paper_id": 259837088, "title": "MMBench: Is Your Multi-modal Model an All-around Player?", "author_names": ["Yuanzhan Liu", "Haodong Duan", "Yuanhan Zhang", "Bo Li", "Songyang Zhang", "Wangbo Zhao", "Yike Yuan", "Jiaqi Wang", "Conghui He", "Ziwei Liu", "Kai Chen", "Dahua Lin"], "venue": "European Conference on Computer Vision", "abstract": "Large vision-language models (VLMs) have recently achieved remarkable progress, exhibiting impressive multimodal perception and reasoning abilities. However, effectively evaluating these large VLMs remains a major challenge, hindering future development in this domain. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but lack fine-grained ability assessment and robust evaluation metrics. Meanwhile, subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, which is not scalable and may display significant bias. In response to these challenges, we propose MMBench, a bilingual benchmark for assessing the multi-modal capabilities of VLMs. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of the following key features: 1. MMBench is meticulously curated with well-designed quality control schemes, surpassing existing similar benchmarks in terms of the number and variety of evaluation questions and abilities; 2. MMBench introduces a rigorous CircularEval strategy and incorporates large language models to convert free-form predictions into pre-defined choices, which helps to yield accurate evaluation results for models with limited instruction-following capabilities. 3. MMBench incorporates multiple-choice questions in both English and Chinese versions, enabling an apples-to-apples comparison of VLMs' performance under a bilingual context. To summarize, MMBench is a systematically designed objective benchmark for a robust and holistic evaluation of vision-language models. We hope MMBench will assist the research community in better evaluating their models and facilitate future progress in this area. The evalutation code of MMBench has been integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.", "year": 2023, "publicationdate": "2023-07-12", "externalids": {"DOI": "10.48550/arXiv.2307.06281"}, "doi_lower": "10.48550/arxiv.2307.06281"}
{"paper_id": 263893278, "title": "Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models", "author_names": ["Yi-Hsueh Liu", "Tianle Han", "Siyuan Ma", "Jiayue Zhang", "Yuanyuan Yang", "Jiaming Tian", "Haoyang He", "Antong Li", "Mengshen He", "Zheng Liu", "Zihao Wu", "Dajiang Zhu", "Xiang Li", "Ning Qiang", "Dinggang Shen", "Tianming Liu", "Bao Ge"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2304.01852"}, "doi_lower": "10.48550/arxiv.2304.01852"}
{"paper_id": 275212089, "title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings", "author_names": ["Shanghaoran Quan", "Jiaxin Yang", "Bowen Yu", "Bo Zheng", "Dayiheng Liu", "An Yang", "Xuancheng Ren", "Bofei Gao", "Yibo Miao", "Yunlong Feng", "Zekun Wang", "Jian Yang", "Zeyu Cui", "Yang Fan", "Yichang Zhang", "Binyuan Hui", "Junyang Lin"], "venue": "arXiv.org", "abstract": "With the increasing code reasoning capabilities of existing large language models (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3, there is a growing need to develop more challenging and comprehensive benchmarks that effectively test their sophisticated competition-level coding abilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to the unavailability of private test cases, lack of support for special judges, and misaligned execution environments. To bridge this gap, we introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. We compile the recent six months of contest problems on CodeForces with detailed information such as contest divisions, problem difficulty ratings, and problem algorithm tags. We introduce a unique judging method in which problems are submitted directly to the platform and develop a reliable Elo rating calculation system that aligns with the platform and is comparable with human participants but has lower variance. By testing on our CodeElo, we provide the Elo ratings of 30 existing popular open-source and 3 proprietary LLMs for the first time. The results show that o1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of 1578 and 1261, respectively, while other models struggle even with the easiest problems, placing in the lowest 25 percent among all human participants. Detailed analysis experiments are also conducted to provide insights into performance across algorithms and comparisons between using C++ and Python, which can suggest directions for future studies.", "year": 2025, "publicationdate": "2025-01-02", "externalids": {"DOI": "10.48550/arXiv.2501.01257"}, "doi_lower": "10.48550/arxiv.2501.01257"}
{"paper_id": 258071542, "title": "Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models", "author_names": ["Alejandro Lopez-Lira", "Yuehua Tang"], "venue": "Social Science Research Network", "abstract": "We document the capability of large language models (LLMs) like ChatGPT to predict stock market reactions from news headlines without direct financial training. Using post-knowledge-cutoff headlines, GPT-4 captures initial market responses, achieving approximately 90% portfolio-day hit rates for the non-tradable initial reaction. GPT-4 scores also significantly predict the subsequent drift, especially for small stocks and negative news. Forecasting ability generally increases with model size, suggesting that financial reasoning is an emerging capacity of complex LLMs. Strategy returns decline as LLM adoption rises, consistent with improved price efficiency. To rationalize these findings, we develop a theoretical model that incorporates LLM technology, information-processing capacity constraints, underreaction, and limits to arbitrage.", "year": 2023, "publicationdate": "2023-04-15", "externalids": {"DOI": "10.2139/ssrn.4412788"}, "doi_lower": "10.2139/ssrn.4412788"}
{"paper_id": 276961699, "title": "New Trends for Modern Machine Translation with Large Reasoning Models", "author_names": ["Sinuo Liu", "Chenyang Lyu", "Minghao Wu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Zifu Shang"], "venue": "arXiv.org", "abstract": "Recent advances in Large Reasoning Models (LRMs), particularly those leveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility for Machine Translation (MT). This position paper argues that LRMs substantially transformed traditional neural MT as well as LLMs-based MT paradigms by reframing translation as a dynamic reasoning task that requires contextual, cultural, and linguistic understanding and reasoning. We identify three foundational shifts: 1) contextual coherence, where LRMs resolve ambiguities and preserve discourse structure through explicit reasoning over cross-sentence and complex context or even lack of context; 2) cultural intentionality, enabling models to adapt outputs by inferring speaker intent, audience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can perform self-reflection during the inference time to correct the potential errors in translation especially extremely noisy cases, showing better robustness compared to simply mapping X->Y translation. We explore various scenarios in translation including stylized translation, document-level translation and multimodal translation by showcasing empirical examples that demonstrate the superiority of LRMs in translation. We also identify several interesting phenomenons for LRMs for MT including auto-pivot translation as well as the critical challenges such as over-localisation in translation and inference efficiency. In conclusion, we think that LRMs redefine translation systems not merely as text converters but as multilingual cognitive agents capable of reasoning about meaning beyond the text. This paradigm shift reminds us to think of problems in translation beyond traditional translation scenarios in a much broader context with LRMs - what we can achieve on top of it.", "year": 2025, "publicationdate": "2025-03-13", "externalids": {"DOI": "10.48550/arXiv.2503.10351"}, "doi_lower": "10.48550/arxiv.2503.10351"}
{"paper_id": 258745593, "title": "Translating radiology reports into plain language using ChatGPT and GPT-4 with prompt learning: results, limitations, and potential", "author_names": ["Qing Lyu", "Josh Tan", "M. Zapadka", "Janardhana Ponnatapura", "Chuang Niu", "Kyle J. Myers", "Ge Wang", "Chris Whitlow"], "venue": "Visual Computing for Industry, Biomedicine, and Art", "abstract": "The large language model called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities. In this study, we investigate the feasibility of using ChatGPT in experiments on translating radiology reports into plain language for patients and healthcare providers so that they are educated for improved healthcare. Radiology reports from 62 low-dose chest computed tomography lung cancer screening scans and 76 brain magnetic resonance imaging metastases screening scans were collected in the first half of February for this study. According to the evaluation by radiologists, ChatGPT can successfully translate radiology reports into plain language with an average score of 4.27 in the five-point system with 0.08 places of information missing and 0.07 places of misinformation. In terms of the suggestions provided by ChatGPT, they are generally relevant such as keeping following-up with doctors and closely monitoring any symptoms, and for about 37% of 138 cases in total ChatGPT offers specific suggestions based on findings in the report. ChatGPT also presents some randomness in its responses with occasionally over-simplified or neglected information, which can be mitigated using a more detailed prompt. Furthermore, ChatGPT results are compared with a newly released large model GPT-4, showing that GPT-4 can significantly improve the quality of translated reports. Our results show that it is feasible to utilize large language models in clinical education, and further efforts are needed to address limitations and maximize their potential.", "year": 2023, "publicationdate": "2023-03-16", "externalids": {"DOI": "10.1186/s42492-023-00136-5"}, "doi_lower": "10.1186/s42492-023-00136-5"}
{"paper_id": 235399978, "title": "Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking", "author_names": ["Zhiyi Ma", "Kawin Ethayarajh", "Tristan Thrush", "Somya Jain", "Ledell Yu Wu", "Robin Jia", "Christopher Potts", "Adina Williams", "Douwe Kiela"], "venue": "Neural Information Processing Systems", "abstract": "We introduce Dynaboard, an evaluation-as-a-service framework for hosting benchmarks and conducting holistic model comparison, integrated with the Dynabench platform. Our platform evaluates NLP models directly instead of relying on self-reported metrics or predictions on a single dataset. Under this paradigm, models are submitted to be evaluated in the cloud, circumventing the issues of reproducibility, accessibility, and backwards compatibility that often hinder benchmarking in NLP. This allows users to interact with uploaded models in real time to assess their quality, and permits the collection of additional metrics such as memory use, throughput, and robustness, which -- despite their importance to practitioners -- have traditionally been absent from leaderboards. On each task, models are ranked according to the Dynascore, a novel utility-based aggregation of these statistics, which users can customize to better reflect their preferences, placing more/less weight on a particular axis of evaluation or dataset. As state-of-the-art NLP models push the limits of traditional benchmarks, Dynaboard offers a standardized solution for a more diverse and comprehensive evaluation of model quality.", "year": 2021, "publicationdate": "2021-05-21", "externalids": {}, "doi_lower": null}
{"paper_id": 257557820, "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models", "author_names": ["Potsawee Manakul", "Adian Liusie", "M. Gales"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.", "year": 2023, "publicationdate": "2023-03-16", "externalids": {"DOI": "10.48550/arXiv.2303.08896"}, "doi_lower": "10.48550/arxiv.2303.08896"}
{"paper_id": 256389726, "title": "MQAG: Multiple-choice Question Answering and Generation for Assessing Information Consistency in Summarization", "author_names": ["Potsawee Manakul", "Adian Liusie", "M. Gales"], "venue": "International Joint Conference on Natural Language Processing", "abstract": "State-of-the-art summarization systems can generate highly fluent summaries. These summaries, however, may contain factual inconsistencies and/or information not present in the source. Hence, an important component of assessing the quality of summaries is to determine whether there is information consistency between the source and the summary. Existing approaches are typically based on lexical matching or representation-based methods. In this work, we introduce an alternative scheme based on standard information-theoretic measures in which the information present in the source and summary is directly compared. We propose a Multiple-choice Question Answering and Generation framework, MQAG, which approximates the information consistency by computing the expected statistical distance between summary and source answer distributions over automatically generated multiple-choice questions. This approach exploits multiple-choice answer probabilities, as predicted answer distributions can be compared. We conduct experiments on four summary evaluation datasets: QAG-CNNDM/XSum, XSum-Hallucination, Podcast Assessment, and SummEval. Experiments show that MQAG, using models trained on SQuAD or RACE, outperforms existing evaluation methods on the majority of tasks.", "year": 2023, "publicationdate": "2023-01-28", "externalids": {"DOI": "10.48550/arXiv.2301.12307"}, "doi_lower": "10.48550/arxiv.2301.12307"}
{"paper_id": 257206033, "title": "Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift with Multiple Views", "author_names": ["Katerina Margatina", "Shuai Wang", "Yogarshi Vyas", "Neha Ann John", "Yassine Benajiba", "Miguel Ballesteros"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Temporal concept drift refers to the problem of data changing over time. In the field of NLP, that would entail that language (e.g. new expressions, meaning shifts) and factual knowledge (e.g. new concepts, updated facts) evolve over time. Focusing on the latter, we benchmark 11 pretrained masked language models (MLMs) on a series of tests designed to evaluate the effect of temporal concept drift, as it is crucial that widely used language models remain up-to-date with the ever-evolving factual updates of the real world. Specifically, we provide a holistic framework that (1) dynamically creates temporal test sets of any time granularity (e.g. month, quarter, year) of factual data from Wikidata, (2) constructs fine-grained splits of tests (e.g. updated, new, unchanged facts) to ensure comprehensive analysis, and (3) evaluates MLMs in three distinct ways (single-token probing, multi-token generation, MLM scoring). In contrast to prior work, our framework aims to unveil how robust an MLM is over time and thus to provide a signal in case it has become outdated, by leveraging multiple views of evaluation.", "year": 2023, "publicationdate": "2023-02-23", "externalids": {"DOI": "10.48550/arXiv.2302.12297"}, "doi_lower": "10.48550/arxiv.2302.12297"}
{"paper_id": 272337302, "title": "What Is Artificial Intelligence?", "author_names": ["Virginia Dignum"], "venue": "Responsible Artificial Intelligence", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.1007/978-3-030-30371-6_2"}, "doi_lower": "10.1007/978-3-030-30371-6_2"}
{"paper_id": 258841470, "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation", "author_names": ["Sewon Min", "Kalpesh Krishna", "Xinxi Lyu", "M. Lewis", "Wen-tau Yih", "Pang Wei Koh", "Mohit Iyyer", "Luke Zettlemoyer", "Hannaneh Hajishirzi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via `pip install factscore`.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14251"}, "doi_lower": "10.48550/arxiv.2305.14251"}
{"paper_id": 259138525, "title": "Large language models as tax attorneys: a case study in legal capabilities emergence", "author_names": ["John J. Nay", "David Karamardian", "Sarah Lawsky", "Wenting Tao", "Meghana Moorthy Bhat", "Raghav Jain", "Aaron Travis Lee", "Jonathan H. Choi", "Jungo Kasai"], "venue": "Philosophical Transactions of the Royal Society A", "abstract": "Better understanding of Large Language Models' (LLMs) legal analysis abilities can contribute to improving the efficiency of legal services, governing artificial intelligence and leveraging LLMs to identify inconsistencies in law. This paper explores LLM capabilities in applying tax law. We choose this area of law because it has a structure that allows us to set up automated validation pipelines across thousands of examples, requires logical reasoning and maths skills, and enables us to test LLM capabilities in a manner relevant to real-world economic lives of citizens and companies. Our experiments demonstrate emerging legal understanding capabilities, with improved performance in each subsequent OpenAI model release. We experiment with retrieving and using the relevant legal authority to assess the impact of providing additional legal context to LLMs. Few-shot prompting, presenting examples of question–answer pairs, is also found to significantly enhance the performance of the most advanced model, GPT-4. The findings indicate that LLMs, particularly when combined with prompting enhancements and the correct legal texts, can perform at high levels of accuracy but not yet at expert tax lawyer levels. As LLMs continue to advance, their ability to reason about law autonomously could have significant implications for the legal profession and AI governance. This article is part of the theme issue ‘A complexity science approach to law and governance’.", "year": 2023, "publicationdate": "2023-06-12", "externalids": {"DOI": "10.1098/rsta.2023.0159"}, "doi_lower": "10.1098/rsta.2023.0159"}
{"paper_id": 207756753, "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "author_names": ["Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "J. Weston", "Douwe Kiela"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.", "year": 2019, "publicationdate": "2019-10-31", "externalids": {"DOI": "10.18653/v1/2020.acl-main.441"}, "doi_lower": "10.18653/v1/2020.acl-main.441"}
{"paper_id": 252668917, "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis", "author_names": ["Erik Nijkamp", "Bo Pang", "Hiroaki Hayashi", "Lifu Tu", "Haiquan Wang", "Yingbo Zhou", "S. Savarese", "Caiming Xiong"], "venue": "International Conference on Learning Representations", "abstract": "Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.", "year": 2022, "publicationdate": "2022-03-25", "externalids": {}, "doi_lower": null}
{"paper_id": 1929239, "title": "Why We Need New Evaluation Metrics for NLG", "author_names": ["Jekaterina Novikova", "Ondrej Dusek", "A. C. Curry", "Verena Rieser"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.", "year": 2017, "publicationdate": "2017-07-21", "externalids": {"DOI": "10.18653/v1/D17-1238"}, "doi_lower": "10.18653/v1/d17-1238"}
{"paper_id": 258518128, "title": "ChatGPT goes to the operating room: evaluating GPT-4 performance and its potential in surgical education and training in the era of large language models", "author_names": ["N. Oh", "G. Choi", "W. Lee"], "venue": "Annals of Surgical Treatment and Research", "abstract": "Purpose This study aimed to assess the performance of ChatGPT, specifically the GPT-3.5 and GPT-4 models, in understanding complex surgical clinical information and its potential implications for surgical education and training. Methods The dataset comprised 280 questions from the Korean general surgery board exams conducted between 2020 and 2022. Both GPT-3.5 and GPT-4 models were evaluated, and their performances were compared using McNemar test. Results GPT-3.5 achieved an overall accuracy of 46.8%, while GPT-4 demonstrated a significant improvement with an overall accuracy of 76.4%, indicating a notable difference in performance between the models (P < 0.001). GPT-4 also exhibited consistent performance across all subspecialties, with accuracy rates ranging from 63.6% to 83.3%. Conclusion ChatGPT, particularly GPT-4, demonstrates a remarkable ability to understand complex surgical clinical information, achieving an accuracy rate of 76.4% on the Korean general surgery board exam. However, it is important to recognize the limitations of large language models and ensure that they are used in conjunction with human expertise and judgment.", "year": 2023, "publicationdate": "2023-04-28", "externalids": {"DOI": "10.4174/astr.2023.104.5.269"}, "doi_lower": "10.4174/astr.2023.104.5.269"}
{"paper_id": 262070493, "title": "Generating Multiple Choice Questions from a Textbook: LLMs Match Human Performance on Most Metrics", "author_names": ["Andrew M. Olney"], "venue": "LLM@AIED", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 263327561, "title": "https://inplasy.com/inplasy-2023-9-0109", "author_names": ["Zheng Hong", "H. Fang", "Hongchun Zhang"], "venue": "", "abstract": null, "year": 2023, "publicationdate": "2023-09-30", "externalids": {"DOI": "10.37766/inplasy2023.9.0109"}, "doi_lower": "10.37766/inplasy2023.9.0109"}
{"paper_id": 257532815, "title": "GPT-4 Technical Report", "author_names": ["OpenAI Josh Achiam", "Steven Adler", "Sandhini Agarwal", "Lama Ahmad", "Ilge Akkaya", "Florencia Leoni Aleman", "Diogo Almeida", "Janko Altenschmidt", "Sam Altman", "Shyamal Anadkat", "Red Avila", "Igor Babuschkin", "S. Balaji", "Valerie Balcom", "Paul Baltescu", "Haim-ing Bao", "Mo Bavarian", "J. Belgum", "Irwan Bello", "Jake Berdine", "Gabriel Bernadett-Shapiro", "Christopher Berner", "Lenny Bogdonoff", "Oleg Boiko", "Made-laine Boyd", "Anna-Luisa Brakman", "Greg Brockman", "Tim Brooks", "Miles Brundage", "Kevin Button", "Trevor Cai", "Rosie Campbell", "Andrew Cann", "Brittany Carey", "Chelsea Carlson", "Rory Carmichael", "Brooke Chan", "Che Chang", "Fotis Chantzis", "Derek Chen", "Sully Chen", "Ruby Chen", "Jason Chen", "Mark Chen", "Benjamin Chess", "Chester Cho", "Casey Chu", "Hyung Won Chung", "Dave Cummings", "Jeremiah Currier", "Yunxing Dai", "Cory Decareaux", "Thomas Degry", "Noah Deutsch", "Damien Deville", "Arka Dhar", "David Dohan", "Steve Dowling", "Sheila Dunning", "Adrien Ecoffet", "Atty Eleti", "Tyna Eloundou", "David Farhi", "L. Fedus", "Niko Felix", "Sim'on Posada Fishman", "Juston Forte", "Is-abella Fulford", "Leo Gao", "Elie Georges", "C. Gibson", "Vik Goel", "Tarun Gogineni", "Gabriel Goh", "Raphael Gontijo-Lopes", "Jonathan Gordon", "Morgan Grafstein", "Scott Gray", "Ryan Greene", "Joshua Gross", "S. Gu", "Yufei Guo", "Chris Hallacy", "Jesse Han", "Jeff Harris", "Yuchen He", "Mike Heaton", "Johannes Heidecke", "Chris Hesse", "Alan Hickey", "W. Hickey", "Peter Hoeschele", "Brandon Houghton", "Kenny Hsu", "Shengli Hu", "Xin Hu", "Joost Huizinga", "Shantanu Jain", "Shawn Jain", "Joanne Jang", "Angela Jiang", "Roger Jiang", "Haozhun Jin", "Denny Jin", "Shino Jomoto", "B. Jonn", "Heewoo Jun", "Tomer Kaftan", "Lukasz Kaiser", "Ali Kamali", "I. Kanitscheider", "N. Keskar", "Tabarak Khan", "Logan Kilpatrick", "Jong Wook Kim", "Christina Kim", "Yongjik Kim", "Hendrik Kirchner", "J. Kiros", "Matthew Knight", "Daniel Kokotajlo", "Lukasz Kondraciuk", "Andrew Kondrich", "Aris Konstantinidis", "Kyle Kosic", "Gretchen Krueger", "Vishal Kuo", "Michael Lampe", "Ikai Lan", "Teddy Lee", "Jan Leike", "Jade Leung", "Daniel Levy", "Chak Li", "Rachel Lim", "Molly Lin", "Stephanie Lin", "Ma-teusz Litwin", "Theresa Lopez", "Ryan Lowe", "Patricia Lue", "A. Makanju", "Kim Malfacini", "Sam Manning", "Todor Markov", "Yaniv Markovski", "Bianca Martin", "Katie Mayer", "Andrew Mayne", "Bob McGrew", "S. McKinney", "Christine McLeavey", "Paul McMillan", "Jake McNeil", "David Medina", "Aalok Mehta", "Jacob Menick", "Luke Metz", "An-drey Mishchenko", "Pamela Mishkin", "Vinnie Monaco", "Evan Morikawa", "Daniel P. Mossing", "Tong Mu", "Mira Murati", "O. Murk", "David M'ely", "Ashvin Nair", "Reiichiro Nakano", "Rajeev Nayak", "Arvind Neelakantan", "Richard Ngo", "Hyeonwoo Noh", "Ouyang Long", "Cullen O'Keefe", "J. Pachocki", "A. Paino", "Joe Palermo", "Ashley Pantuliano", "Giambattista Parascandolo", "J. Parish", "Emy Parparita", "Alexandre Passos", "Mikhail Pavlov", "Andrew Peng", "Adam Perelman", "Filipe de Avila Belbute Peres", "Michael Petrov", "Henrique Pondé de Oliveira Pinto", "Michael Pokorny", "Michelle Pokrass", "Vitchyr H. Pong", "Tolly Powell", "Alethea Power", "Boris Power", "Elizabeth Proehl", "Raul Puri", "Alec Radford", "Jack W. Rae", "Aditya Ramesh", "Cameron Raymond", "Francis Real", "Kendra Rimbach", "Carl Ross", "Bob Rotsted", "Henri Roussez", "N. Ryder", "M. Saltarelli", "Ted Sanders", "Shibani Santurkar", "Girish Sastry", "Heather Schmidt", "David Schnurr", "John Schulman", "Daniel Selsam", "Kyla Sheppard", "Toki Sherbakov", "Jessica Shieh", "Sarah Shoker", "Pranav Shyam", "Szymon Sidor", "Eric Sigler", "Maddie Simens", "Jordan Sitkin", "Katarina Slama", "Ian Sohl", "Benjamin Sokolowsky", "Yang Song", "Natalie Staudacher", "F. Such", "Natalie Summers", "I. Sutskever", "Jie Tang", "N. Tezak", "Madeleine Thompson", "P. Tillet", "Amin Tootoonchian", "Elizabeth Tseng", "Preston Tuggle", "Nick Turley", "Jerry Tworek", "Juan Felipe Cer'on Uribe", "Andrea Vallone", "Arun Vijayvergiya", "Chelsea Voss", "Carroll L. Wainwright", "Justin Jay Wang", "Alvin Wang", "Ben Wang", "Jonathan Ward", "Jason Wei", "CJ Weinmann", "Akila Welihinda", "Peter Welinder", "Jiayi Weng", "Lilian Weng", "Matt Wiethoff", "Dave Willner", "Clemens Winter", "Samuel Wolrich", "Hannah Wong", "Lauren Workman", "Sherwin Wu", "Jeff Wu", "Michael Wu", "Kai Xiao", "Tao Xu", "Sarah Yoo", "Kevin Yu", "Qim-ing Yuan", "Wojciech Zaremba", "Rowan Zellers", "Chong Zhang", "Marvin Zhang", "Shengjia Zhao", "Tianhao Zheng", "Juntang Zhuang", "William Zhuk", "Barret Zoph"], "venue": "", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {}, "doi_lower": null}
{"paper_id": 258846153, "title": "Human-like problem-solving abilities in large language models using ChatGPT", "author_names": ["G. Orrú", "Andrea Piarulli", "C. Conversano", "A. Gemignani"], "venue": "Frontiers in Artificial Intelligence", "abstract": "Backgrounds The field of Artificial Intelligence (AI) has seen a major shift in recent years due to the development of new Machine Learning (ML) models such as Generative Pre-trained Transformer (GPT). GPT has achieved previously unheard-of levels of accuracy in most computerized language processing tasks and their chat-based variations. Aim The aim of this study was to investigate the problem-solving abilities of ChatGPT using two sets of verbal insight problems, with a known performance level established by a sample of human participants. Materials and methods A total of 30 problems labeled as “practice problems” and “transfer problems” were administered to ChatGPT. ChatGPT's answers received a score of “0” for each incorrectly answered problem and a score of “1” for each correct response. The highest possible score for both the practice and transfer problems was 15 out of 15. The solution rate for each problem (based on a sample of 20 subjects) was used to assess and compare the performance of ChatGPT with that of human subjects. Results The study highlighted that ChatGPT can be trained in out-of-the-box thinking and demonstrated potential in solving verbal insight problems. The global performance of ChatGPT equalled the most probable outcome for the human sample in both practice problems and transfer problems as well as upon their combination. Additionally, ChatGPT answer combinations were among the 5% of most probable outcomes for the human sample both when considering practice problems and pooled problem sets. These findings demonstrate that ChatGPT performance on both set of problems was in line with the mean rate of success of human subjects, indicating that it performed reasonably well. Conclusions The use of transformer architecture and self-attention in ChatGPT may have helped to prioritize inputs while predicting, contributing to its potential in verbal insight problem-solving. ChatGPT has shown potential in solving insight problems, thus highlighting the importance of incorporating AI into psychological research. However, it is acknowledged that there are still open challenges. Indeed, further research is required to fully understand AI's capabilities and limitations in verbal problem-solving.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.3389/frai.2023.1199350"}, "doi_lower": "10.3389/frai.2023.1199350"}
{"paper_id": 256358591, "title": "ThoughtSource: A central hub for large language model reasoning data", "author_names": ["Simon Ott", "Konstantin Hebenstreit", "Valentin Li'evin", "C. Hother", "M. Moradi", "Maximilian Mayrhauser", "Robert Praas", "O. Winther", "M. Samwald"], "venue": "Scientific Data", "abstract": "Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to ‘hallucinate’ facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates seven scientific/medical, three general-domain and five math word question answering datasets.", "year": 2023, "publicationdate": "2023-01-27", "externalids": {"DOI": "10.1038/s41597-023-02433-3"}, "doi_lower": "10.1038/s41597-023-02433-3"}
{"paper_id": 246426909, "title": "Training language models to follow instructions with human feedback", "author_names": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "John Schulman", "Jacob Hilton", "Fraser Kelton", "Luke E. Miller", "Maddie Simens", "Amanda Askell", "Peter Welinder", "P. Christiano", "Jan Leike", "Ryan J. Lowe"], "venue": "Neural Information Processing Systems", "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.", "year": 2022, "publicationdate": "2022-03-04", "externalids": {}, "doi_lower": null}
{"paper_id": 258887822, "title": "Understanding the Capabilities of Large Language Models for Automated Planning", "author_names": ["Vishal Pallagani", "Bharath Muppasani", "K. Murugesan", "F. Rossi", "Biplav Srivastava", "L. Horesh", "F. Fabiano", "A. Loreggia"], "venue": "arXiv.org", "abstract": "Automated planning is concerned with developing efficient algorithms to generate plans or sequences of actions to achieve a specific goal in a given environment. Emerging Large Language Models (LLMs) can answer questions, write high-quality programming code, and predict protein folding, showcasing their versatility in solving various tasks beyond language-based problems. In this paper, we aim to explore how LLMs can also be used for automated planning. To do so, we seek to answer four key questions. Firstly, we want to understand the extent to which LLMs can be used for plan generation. Secondly, we aim to identify which pre-training data is most effective in facilitating plan generation. Thirdly, we investigate whether fine-tuning or prompting is a more effective approach for plan generation. Finally, we explore whether LLMs are capable of plan generalization. By answering these questions, the study seeks to shed light on the capabilities of LLMs in solving complex planning problems and provide insights into the most effective approaches for using LLMs in this context.", "year": 2023, "publicationdate": "2023-05-25", "externalids": {"DOI": "10.48550/arXiv.2305.16151"}, "doi_lower": "10.48550/arxiv.2305.16151"}
{"paper_id": 259165563, "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap", "author_names": ["Shirui Pan", "Linhao Luo", "Yufei Wang", "Chen Chen", "Jiapu Wang", "Xindong Wu"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia, and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and, simultaneously, leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely: 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.", "year": 2023, "publicationdate": "2023-06-14", "externalids": {"DOI": "10.1109/TKDE.2024.3352100"}, "doi_lower": "10.1109/tkde.2024.3352100"}
{"paper_id": 249017698, "title": "TALM: Tool Augmented Language Models", "author_names": ["Aaron Parisi", "Yao Zhao", "Noah Fiedel"], "venue": "arXiv.org", "abstract": "Transformer based language models (LMs) demonstrate increasing performance with scale across a wide variety of tasks. Scale alone however cannot enable models to solve tasks that require access to ephemeral, changing, or private data that was unavailable at training time. Many useful tasks may also benefit from LMs being able to access APIs that read or modify state. In this work, we present Tool Augmented Language Models (TALM), combining a text-only approach to augment language models with non-differentiable tools, and an iterative\"self-play\"technique to bootstrap performance starting from few tool demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA task and a reasoning oriented math task with simple tools. At a given model scale, TALM significantly outperforms non-augmented LMs. We further demonstrate that TALM successfully performs out-of-distribution inferences on both QA and math tasks, where non-augmented LMs fail. Our results suggest that Tool Augmented Language Models are a promising direction to enrich LMs' capabilities, with less dependence on scale.", "year": 2022, "publicationdate": "2022-05-24", "externalids": {"DOI": "10.48550/arXiv.2205.12255"}, "doi_lower": "10.48550/arxiv.2205.12255"}
{"paper_id": 239010011, "title": "BBQ: A hand-built bias benchmark for question answering", "author_names": ["Alicia Parrish", "Angelica Chen", "Nikita Nangia", "Vishakh Padmakumar", "Jason Phang", "Jana Thompson", "Phu Mon Htut", "Sam Bowman"], "venue": "Findings", "abstract": "It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model’s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model’s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.", "year": 2021, "publicationdate": "2021-10-15", "externalids": {"DOI": "10.18653/v1/2022.findings-acl.165"}, "doi_lower": "10.18653/v1/2022.findings-acl.165"}
{"paper_id": 259076261, "title": "Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs", "author_names": ["Alejandro Peña", "A. Morales", "Julian Fierrez", "Ignacio Serna", "J. Ortega-Garcia", "Iñigo Puente", "Jorge Cordova", "Gonzalo Cordova"], "venue": "ICDAR Workshops", "abstract": "The analysis of public affairs documents is crucial for citizens as it promotes transparency, accountability, and informed decision-making. It allows citizens to understand government policies, participate in public discourse, and hold representatives accountable. This is crucial, and sometimes a matter of life or death, for companies whose operation depend on certain regulations. Large Language Models (LLMs) have the potential to greatly enhance the analysis of public affairs documents by effectively processing and understanding the complex language used in such documents. In this work, we analyze the performance of LLMs in classifying public affairs documents. As a natural multi-label task, the classification of these documents presents important challenges. In this work, we use a regex-powered tool to collect a database of public affairs documents with more than 33K samples and 22.5M tokens. Our experiments assess the performance of 4 different Spanish LLMs to classify up to 30 different topics in the data in different configurations. The results shows that LLMs can be of great use to process domain-specific documents, such as those in the domain of public affairs.", "year": 2023, "publicationdate": "2023-06-05", "externalids": {"DOI": "10.1007/978-3-031-41498-5_2"}, "doi_lower": "10.1007/978-3-031-41498-5_2"}
{"paper_id": 45639063, "title": "Validity problems comparing values across cultures and possible solutions.", "author_names": ["K. Peng", "R. Nisbett", "N. Wong"], "venue": "", "abstract": null, "year": 1997, "publicationdate": "1997-12-01", "externalids": {"DOI": "10.1037/1082-989X.2.4.329"}, "doi_lower": "10.1037/1082-989x.2.4.329"}
{"paper_id": 259138387, "title": "Measuring and Modifying Factual Knowledge in Large Language Models", "author_names": ["Pouya Pezeshkpour"], "venue": "International Conference on Machine Learning and Applications", "abstract": "Large Language Models (LLMs) store an extensive amount of factual knowledge obtained from vast collections of text. To effectively utilize these models for downstream tasks, it is crucial to have reliable methods for measuring their knowledge. However, existing approaches for knowledge measurement have certain limitations, and despite recent efforts, they fail to provide accurate measurements and the necessary insights for modifying the knowledge within LLMs. In this work, we employ information theory-based measurements to provide a framework estimating the factual knowledge contained within large language models. More specifically, we measure knowledge by analyzing the LLM's prediction probability distribution before and after instilling the target knowledge, employing metrics such as entropy and KL-divergence. Introducing our metrics, we first assess their accuracy in comparison to previous ranking-based methods, surpassing them by around 30% in a synthetic experiment. Then, we explore two prominent methods of knowledge instillation, discovering that LLMs exhibit limitations in capturing new knowledge under specific circumstances for one of these methods. Lastly, we demonstrate the applicability of our methods in extracting unlearned and mislearned facts in LLMs through their application to in-context learning.", "year": 2023, "publicationdate": "2023-06-09", "externalids": {"DOI": "10.1109/ICMLA58977.2023.00122"}, "doi_lower": "10.1109/icmla58977.2023.00122"}
{"paper_id": 244129853, "title": "Adversarially Constructed Evaluation Sets Are More Challenging, but May Not Be Fair", "author_names": ["Jason Phang", "Angelica Chen", "William Huang", "Samuel R. Bowman"], "venue": "DADC", "abstract": "Large language models increasingly saturate existing task benchmarks, in some cases outperforming humans, leaving little headroom with which to measure further progress. Adversarial dataset creation, which builds datasets using examples that a target system outputs incorrect predictions for, has been proposed as a strategy to construct more challenging datasets, avoiding the more serious challenge of building more precise benchmarks by conventional means. In this work, we study the impact of applying three common approaches for adversarial dataset creation: (1) filtering out easy examples (AFLite), (2) perturbing examples (TextFooler), and (3) model-in-the-loop data collection (ANLI and AdversarialQA), across 18 different adversary models. We find that all three methods can produce more challenging datasets, with stronger adversary models lowering the performance of evaluated models more. However, the resulting ranking of the evaluated models can also be unstable and highly sensitive to the choice of adversary model. Moreover, we find that AFLite oversamples examples with low annotator agreement, meaning that model comparisons hinge on the examples that are most contentious for humans. We recommend that researchers tread carefully when using adversarial methods for building evaluation datasets.", "year": 2021, "publicationdate": "2021-11-16", "externalids": {"DOI": "10.18653/v1/2022.dadc-1.8"}, "doi_lower": "10.18653/v1/2022.dadc-1.8"}
{"paper_id": 259145296, "title": "ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer", "author_names": ["Dongqi Liu", "Vera Demberg"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "ChatGPT Analysis", "year": 2023, "publicationdate": "2023-06-13", "externalids": {"DOI": "10.48550/arXiv.2306.07799"}, "doi_lower": "10.48550/arxiv.2306.07799"}
{"paper_id": 256827430, "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?", "author_names": ["Chengwei Qin", "Aston Zhang", "Zhuosheng Zhang", "Jiaao Chen", "Michihiro Yasunaga", "Diyi Yang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.", "year": 2023, "publicationdate": "2023-02-08", "externalids": {"DOI": "10.18653/v1/2023.emnlp-main.85"}, "doi_lower": "10.18653/v1/2023.emnlp-main.85"}
{"paper_id": 258179336, "title": "Tool Learning with Foundation Models", "author_names": ["Yujia Qin", "Shengding Hu", "Yankai Lin", "Weize Chen", "Ning Ding", "Ganqu Cui", "Zheni Zeng", "Yufei Huang", "Chaojun Xiao", "Chi Han", "Y. Fung", "Yusheng Su", "Huadong Wang", "Cheng Qian", "Runchu Tian", "Kunlun Zhu", "Shi Liang", "Xingyu Shen", "Bokai Xu", "Zhen Zhang", "Yining Ye", "Bowen Li", "Ziwei Tang", "Jing Yi", "Yu Zhu", "Zhenning Dai", "Lan Yan", "Xin Cong", "Ya-Ting Lu", "Weilin Zhao", "Yuxiang Huang", "Junxi Yan", "Xu Han", "Xian Sun", "Dahai Li", "Jason Phang", "Cheng Yang", "Tongshuang Wu", "Heng Ji", "Zhiyuan Liu", "Maosong Sun"], "venue": "ACM Computing Surveys", "abstract": "Humans possess an extraordinary ability to create and utilize tools. With the advent of foundation models, artificial intelligence systems have the potential to be equally adept in tool use as humans. This paradigm, which is dubbed as tool learning with foundation models, combines the strengths of tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. This article presents a systematic investigation and comprehensive review of tool learning. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research and formulate a general framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate generalization in tool learning. Finally, we discuss several open problems that require further investigation, such as ensuring trustworthy tool use, enabling tool creation with foundation models, and addressing personalization challenges. Overall, we hope this article could inspire future research in integrating tools with foundation models.", "year": 2023, "publicationdate": "2023-04-17", "externalids": {"DOI": "10.1145/3704435"}, "doi_lower": "10.1145/3704435"}
{"paper_id": 260334759, "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "author_names": ["Yujia Qin", "Shi Liang", "Yining Ye", "Kunlun Zhu", "Lan Yan", "Ya-Ting Lu", "Yankai Lin", "Xin Cong", "Xiangru Tang", "Bill Qian", "Sihan Zhao", "Runchu Tian", "Ruobing Xie", "Jie Zhou", "Marc H. Gerstein", "Dahai Li", "Zhiyuan Liu", "Maosong Sun"], "venue": "International Conference on Learning Representations", "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.", "year": 2023, "publicationdate": "2023-07-31", "externalids": {"DOI": "10.48550/arXiv.2307.16789"}, "doi_lower": "10.48550/arxiv.2307.16789"}
{"paper_id": 49313245, "title": "Improving Language Understanding by Generative Pre-Training", "author_names": ["Alec Radford", "Karthik Narasimhan"], "venue": "", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 261696947, "title": "A Survey of Hallucination in Large Foundation Models", "author_names": ["Vipula Rawte", "A. Sheth", "Amitava Das"], "venue": "arXiv.org", "abstract": "Hallucination in a foundation model (FM) refers to the generation of content that strays from factual reality or includes fabricated information. This survey paper provides an extensive overview of recent efforts that aim to identify, elucidate, and tackle the problem of hallucination, with a particular focus on ``Large'' Foundation Models (LFMs). The paper classifies various types of hallucination phenomena that are specific to LFMs and establishes evaluation criteria for assessing the extent of hallucination. It also examines existing strategies for mitigating hallucination in LFMs and discusses potential directions for future research in this area. Essentially, the paper offers a comprehensive examination of the challenges and solutions related to hallucination in LFMs.", "year": 2023, "publicationdate": "2023-09-12", "externalids": {"DOI": "10.48550/arXiv.2309.05922"}, "doi_lower": "10.48550/arxiv.2309.05922"}
{"paper_id": 248779886, "title": "Adaptive Testing and Debugging of NLP Models", "author_names": ["Marco Tulio Ribeiro", "Scott M. Lundberg"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor, or only work for a very restrictive class of bugs. We present AdaTest, a process which uses large scale language models (LMs) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model. Such bugs are then addressed through an iterative text-fix-retest loop, inspired by traditional software development. In experiments with expert and non-expert users and commercial / research models for 8 different tasks, AdaTest makes users 5-10x more effective at finding bugs than current approaches, and helps users effectively fix bugs without adding new bugs.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.acl-long.230"}, "doi_lower": "10.18653/v1/2022.acl-long.230"}
{"paper_id": 218551201, "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList", "author_names": ["Marco Tulio Ribeiro", "Tongshuang Sherry Wu", "Carlos Guestrin", "Sameer Singh"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.", "year": 2020, "publicationdate": "2020-05-08", "externalids": {"DOI": "10.18653/v1/2020.acl-main.442"}, "doi_lower": "10.18653/v1/2020.acl-main.442"}
{"paper_id": 272692262, "title": "The Two Word Test as a semantic benchmark for large language models", "author_names": ["Nicholas Riccardi", "Xuan Yang", "Rutvik H. Desai"], "venue": "Scientific Reports", "abstract": "Large language models (LLMs) have shown remarkable abilities recently, including passing advanced professional exams and demanding benchmark tests. This performance has led many to suggest that they are close to achieving humanlike or “true” understanding of language, and even artificial general intelligence (AGI). Here, we provide a new open-source benchmark, the Two Word Test (TWT), that can assess semantic abilities of LLMs using two-word phrases in a task that can be performed relatively easily by humans without advanced training. Combining multiple words into a single concept is a fundamental linguistic and conceptual operation routinely performed by people. The test requires meaningfulness judgments of 1768 noun-noun combinations that have been rated as meaningful (e.g., baby boy) or as having low meaningfulness (e.g., goat sky) by human raters. This novel test differs from existing benchmarks that rely on logical reasoning, inference, puzzle-solving, or domain expertise. We provide versions of the task that probe meaningfulness ratings on a 0–4 scale as well as binary judgments. With both versions, we conducted a series of experiments using the TWT on GPT-4, GPT-3.5, Claude-3-Optus, and Gemini-1-Pro-001. Results demonstrated that, compared to humans, all models performed relatively poorly at rating meaningfulness of these phrases. GPT-3.5-turbo, Gemini-1.0-Pro-001 and GPT-4-turbo were also unable to make binary discriminations between sensible and nonsense phrases, with these models consistently judging nonsensical phrases as making sense. Claude-3-Opus made a substantial improvement in binary discrimination of combinatorial phrases but was still significantly worse than human performance. The TWT can be used to understand and assess the limitations of current LLMs, and potentially improve them. The test also reminds us that caution is warranted in attributing “true” or human-level understanding to LLMs based only on tests that are challenging for humans.", "year": 2024, "publicationdate": "2024-09-16", "externalids": {"DOI": "10.1038/s41598-024-72528-3"}, "doi_lower": "10.1038/s41598-024-72528-3"}
{"paper_id": 258180220, "title": "The Self-Perception and Political Biases of ChatGPT", "author_names": ["Jérôme Rutinowski", "Sven Franke", "Jan Endendyk", "Ina Dormuth", "Markus Pauly"], "venue": "Human Behavior and Emerging Technologies", "abstract": "This contribution analyzes the self-perception and political biases of OpenAI’s Large Language Model ChatGPT. Considering the first small-scale reports and studies that have emerged, claiming that ChatGPT is politically biased towards progressive and libertarian points of view, this contribution is aimed at providing further clarity on this subject. Although the concept of political bias and affiliation is hard to define, lacking an agreed-upon measure for its quantification, this contribution attempts to examine this issue by having ChatGPT respond to questions on commonly used measures of political bias. In addition, further measures for personality traits that have previously been linked to political affiliations were examined. More specifically, ChatGPT was asked to answer the questions posed by the political compass test as well as similar questionnaires that are specific to the respective politics of the G7 member states. These eight tests were repeated ten times each and indicate that ChatGPT seems to hold a bias towards progressive views. The political compass test revealed a bias towards progressive and libertarian views, supporting the claims of prior research. The political questionnaires for the G7 member states indicated a bias towards progressive views but no significant bias between authoritarian and libertarian views, contradicting the findings of prior reports. In addition, ChatGPT’s Big Five personality traits were tested using the OCEAN test, and its personality type was queried using the Myers-Briggs Type Indicator (MBTI) test. Finally, the maliciousness of ChatGPT was evaluated using the Dark Factor test. These three tests were also repeated ten times each, revealing that ChatGPT perceives itself as highly open and agreeable, has the Myers-Briggs personality type ENFJ, and is among the test-takers with the least pronounced dark traits.", "year": 2023, "publicationdate": "2023-04-14", "externalids": {"DOI": "10.48550/arXiv.2304.07333"}, "doi_lower": "10.48550/arxiv.2304.07333"}
{"paper_id": 259317218, "title": "Personality Traits in Large Language Models", "author_names": ["Mustafa Safdari", "Gregory Serapio-Garc'ia", "Clé-ment Crepy", "Stephen Fitz", "P. Romero", "Luning Sun", "Marwa Abdulhai", "Aleksandra Faust", "Maja Matari'c"], "venue": "arXiv.org", "abstract": "The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly powerconversational agents used by the general public world-wide, the synthetic personality traits embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we present a novel and comprehensive psychometrically valid and reliable methodology for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method to 18 LLMs, we found: 1) personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss the application and ethical implications of the measurement and shaping method, in particular regarding responsible AI.", "year": 2023, "publicationdate": "2023-07-01", "externalids": {"DOI": "10.48550/arXiv.2307.00184"}, "doi_lower": "10.48550/arxiv.2307.00184"}
{"paper_id": 258376846, "title": "Assessing the Accuracy of Responses by the Language Model ChatGPT to Questions Regarding Bariatric Surgery", "author_names": ["Jamil S. Samaan", "Y. Yeo", "N. Rajeev", "L. Hawley", "Stuart A. Abel", "Wee Han Ng", "N. Srinivasan", "Justin J. Park", "M. Burch", "R. Watson", "O. Liran", "K. Samakar"], "venue": "Obesity Surgery", "abstract": "Purpose ChatGPT is a large language model trained on a large dataset covering a broad range of topics, including the medical literature. We aim to examine its accuracy and reproducibility in answering patient questions regarding bariatric surgery. Materials and methods Questions were gathered from nationally regarded professional societies and health institutions as well as Facebook support groups. Board-certified bariatric surgeons graded the accuracy and reproducibility of responses. The grading scale included the following: (1) comprehensive, (2) correct but inadequate, (3) some correct and some incorrect, and (4) completely incorrect. Reproducibility was determined by asking the model each question twice and examining difference in grading category between the two responses. Results In total, 151 questions related to bariatric surgery were included. The model provided “comprehensive” responses to 131/151 (86.8%) of questions. When examined by category, the model provided “comprehensive” responses to 93.8% of questions related to “efficacy, eligibility and procedure options”; 93.3% related to “preoperative preparation”; 85.3% related to “recovery, risks, and complications”; 88.2% related to “lifestyle changes”; and 66.7% related to “other”. The model provided reproducible answers to 137 (90.7%) of questions. Conclusion The large language model ChatGPT often provided accurate and reproducible responses to common questions related to bariatric surgery. ChatGPT may serve as a helpful adjunct information resource for patients regarding bariatric surgery in addition to standard of care provided by licensed healthcare professionals. We encourage future studies to examine how to leverage this disruptive technology to improve patient outcomes and quality of life. Graphical Abstract", "year": 2023, "publicationdate": "2023-04-27", "externalids": {"DOI": "10.1007/s11695-023-06603-5"}, "doi_lower": "10.1007/s11695-023-06603-5"}
{"paper_id": 258865898, "title": "Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples", "author_names": ["Abulhair Saparov", "Richard Yuanzhe Pang", "Vishakh Padmakumar", "Nitish Joshi", "Seyed Mehran Kazemi", "Najoung Kim", "He He"], "venue": "Neural Information Processing Systems", "abstract": "Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.15269"}, "doi_lower": "10.48550/arxiv.2305.15269"}
{"paper_id": 260155126, "title": "ARB: Advanced Reasoning Benchmark for Large Language Models", "author_names": ["Tomohiro Sawada", "Daniel Paleka", "Alexander Havrilla", "Pranav Tadepalli", "Paula Vidas", "Alexander Kranias", "John J. Nay", "Kshitij Gupta", "Aran Komatsuzaki"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 50% on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps. Further, we conduct a human evaluation of the symbolic subset of ARB, finding promising agreement between annotators and GPT-4 rubric evaluation scores.", "year": 2023, "publicationdate": "2023-07-25", "externalids": {"DOI": "10.48550/arXiv.2307.13692"}, "doi_lower": "10.48550/arxiv.2307.13692"}
{"paper_id": 256697342, "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "author_names": ["Timo Schick", "Jane Dwivedi-Yu", "Roberto Dessì", "R. Raileanu", "M. Lomeli", "Luke Zettlemoyer", "Nicola Cancedda", "Thomas Scialom"], "venue": "Neural Information Processing Systems", "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.", "year": 2023, "publicationdate": "2023-02-09", "externalids": {"DOI": "10.48550/arXiv.2302.04761"}, "doi_lower": "10.48550/arxiv.2302.04761"}
{"paper_id": 211224290, "title": "Prem Khanal", "author_names": ["P. Khanal", "Prem Khanal CV.doc"], "venue": "", "abstract": null, "year": 2020, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 257833781, "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face", "author_names": ["Yongliang Shen", "Kaitao Song", "Xu Tan", "Dongsheng Li", "Weiming Lu", "Y. Zhuang"], "venue": "Neural Information Processing Systems", "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2303.17580"}, "doi_lower": "10.48550/arxiv.2303.17580"}
{"paper_id": 234337004, "title": "Societal Biases in Language Generation: Progress and Challenges", "author_names": ["Emily Sheng", "Kai-Wei Chang", "P. Natarajan", "Nanyun Peng"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.", "year": 2021, "publicationdate": "2021-05-10", "externalids": {"DOI": "10.18653/v1/2021.acl-long.330"}, "doi_lower": "10.18653/v1/2021.acl-long.330"}
{"paper_id": 252531526, "title": "Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity", "author_names": ["Gabriel Simmons"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "This paper studies whether LLMs moral preferences based on prompted political ideology replicate known results obtained in social science studies, using tools from Moral Foundations Theory", "year": 2022, "publicationdate": "2022-09-24", "externalids": {"DOI": "10.48550/arXiv.2209.12106"}, "doi_lower": "10.48550/arxiv.2209.12106"}
{"paper_id": 255124952, "title": "Large language models encode clinical knowledge", "author_names": ["K. Singhal", "Shekoofeh Azizi", "T. Tu", "S. Mahdavi", "Jason Wei", "Hyung Won Chung", "Nathan Scales", "A. Tanwani", "H. Cole-Lewis", "S. Pfohl", "P. Payne", "Martin G. Seneviratne", "P. Gamble", "C. Kelly", "Nathaneal Scharli", "A. Chowdhery", "P. A. Mansfield", "B. A. Y. Arcas", "D. Webster", "Greg S. Corrado", "Yossi Matias", "K. Chou", "Juraj Gottweis", "Nenad Tomašev", "Yun Liu", "A. Rajkomar", "J. Barral", "Christopher Semturs", "A. Karthikesalingam", "Vivek Natarajan"], "venue": "Nature", "abstract": "Med-PaLM, a state-of-the-art large language model for medicine, is introduced and evaluated across several medical question answering tasks, demonstrating the promise of these models in this domain. Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model^ 1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM^ 2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA^ 3 , MedMCQA^ 4 , PubMedQA^ 5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics^ 6 ), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.", "year": 2022, "publicationdate": "2022-12-26", "externalids": {"DOI": "10.1038/s41586-023-06291-2"}, "doi_lower": "10.1038/s41586-023-06291-2"}
{"paper_id": 255124952, "title": "Large language models encode clinical knowledge", "author_names": ["K. Singhal", "Shekoofeh Azizi", "T. Tu", "S. Mahdavi", "Jason Wei", "Hyung Won Chung", "Nathan Scales", "A. Tanwani", "H. Cole-Lewis", "S. Pfohl", "P. Payne", "Martin G. Seneviratne", "P. Gamble", "C. Kelly", "Nathaneal Scharli", "A. Chowdhery", "P. A. Mansfield", "B. A. Y. Arcas", "D. Webster", "Greg S. Corrado", "Yossi Matias", "K. Chou", "Juraj Gottweis", "Nenad Tomašev", "Yun Liu", "A. Rajkomar", "J. Barral", "Christopher Semturs", "A. Karthikesalingam", "Vivek Natarajan"], "venue": "Nature", "abstract": "Med-PaLM, a state-of-the-art large language model for medicine, is introduced and evaluated across several medical question answering tasks, demonstrating the promise of these models in this domain. Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model^ 1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM^ 2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA^ 3 , MedMCQA^ 4 , PubMedQA^ 5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics^ 6 ), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.", "year": 2022, "publicationdate": "2022-12-26", "externalids": {"DOI": "10.1038/s41586-023-06291-2"}, "doi_lower": "10.1038/s41586-023-06291-2"}
{"paper_id": 246411325, "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model", "author_names": ["Shaden Smith", "M. Patwary", "Brandon Norick", "P. LeGresley", "Samyam Rajbhandari", "J. Casper", "Zhun Liu", "Shrimai Prabhumoye", "George Zerveas", "V. Korthikanti", "Elton Zhang", "R. Child", "Reza Yazdani Aminabadi", "J. Bernauer", "Xia Song", "M. Shoeybi", "Yuxiong He", "Michael Houston", "Saurabh Tiwary", "Bryan Catanzaro"], "venue": "arXiv.org", "abstract": "Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 258866061, "title": "Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs", "author_names": ["Xiaoyang Song", "Akshat Gupta", "Kiyan Mohebbizadeh", "Shujie Hu", "Anant Singh"], "venue": "arXiv.org", "abstract": "Have Large Language Models (LLMs) developed a personality? The short answer is a resounding\"We Don't Know!\". In this paper, we show that we do not yet have the right tools to measure personality in language models. Personality is an important characteristic that influences behavior. As LLMs emulate human-like intelligence and performance in various tasks, a natural question to ask is whether these models have developed a personality. Previous works have evaluated machine personality through self-assessment personality tests, which are a set of multiple-choice questions created to evaluate personality in humans. A fundamental assumption here is that human personality tests can accurately measure personality in machines. In this paper, we investigate the emergence of personality in five LLMs of different sizes ranging from 1.5B to 30B. We propose the Option-Order Symmetry property as a necessary condition for the reliability of these self-assessment tests. Under this condition, the answer to self-assessment questions is invariant to the order in which the options are presented. We find that many LLMs personality test responses do not preserve option-order symmetry. We take a deeper look at LLMs test responses where option-order symmetry is preserved to find that in these cases, LLMs do not take into account the situational statement being tested and produce the exact same answer irrespective of the situation being tested. We also identify the existence of inherent biases in these LLMs which is the root cause of the aforementioned phenomenon and makes self-assessment tests unreliable. These observations indicate that self-assessment tests are not the correct tools to measure personality in LLMs. Through this paper, we hope to draw attention to the shortcomings of current literature in measuring personality in LLMs and call for developing tools for machine personality measurement.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.14693"}, "doi_lower": "10.48550/arxiv.2305.14693"}
{"paper_id": 258947110, "title": "ChatGPT: A Study on its Utility for Ubiquitous Software Engineering Tasks", "author_names": ["G. Sridhara", "Ranjani H.G.", "Sourav Mazumdar"], "venue": "arXiv.org", "abstract": "ChatGPT (Chat Generative Pre-trained Transformer) is a chatbot launched by OpenAI on November 30, 2022. OpenAI's GPT-3 family of large language models serve as the foundation for ChatGPT. ChatGPT is fine-tuned with both supervised and reinforcement learning techniques and has received widespread attention for its articulate responses across diverse domains of knowledge. In this study, we explore how ChatGPT can be used to help with common software engineering tasks. Many of the ubiquitous tasks covering the breadth of software engineering such as ambiguity resolution in software requirements, method name suggestion, test case prioritization, code review, log summarization can potentially be performed using ChatGPT. In this study, we explore fifteen common software engineering tasks using ChatGPT. We juxtapose and analyze ChatGPT's answers with the respective state of the art outputs (where available) and/or human expert ground truth. Our experiments suggest that for many tasks, ChatGPT does perform credibly and the response from it is detailed and often better than the human expert output or the state of the art output. However, for a few other tasks, ChatGPT in its present form provides incorrect answers and hence is not suited for such tasks.", "year": 2023, "publicationdate": "2023-05-26", "externalids": {"DOI": "10.48550/arXiv.2305.16837"}, "doi_lower": "10.48550/arxiv.2305.16837"}
{"paper_id": 263625818, "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models", "author_names": ["Aarohi Srivastava", "Abhinav Rastogi", "Abhishek Rao", "Abu Awal Md Shoeb", "Abubakar Abid", "Adam Fisch", "Adam R. Brown", "Adam Santoro", "Aditya Gupta", "Adrià Garriga-Alonso", "Agnieszka Kluska", "Aitor Lewkowycz", "Akshat Agarwal", "Alethea Power", "Alex Ray", "Alex Warstadt", "Alexander W. Kocurek", "Ali Safaya", "Ali Tazarv", "Alice Xiang", "Alicia Parrish", "Allen Nie", "Aman Hussain", "Amanda Askell", "A. Dsouza", "Ambrose Slone", "Ameet Rahane", "Anantharaman S. Iyer", "Anders Andreassen", "Andrea Madotto", "Andrea Santilli", "Andreas Stuhlmuller", "Andrew M. Dai", "A. La", "Andrew Kyle Lampinen", "Andy Zou", "Angela Jiang", "Angelica Chen", "Anh Vuong", "Animesh Gupta", "Anna Gottardi", "Antonio Norelli", "Anu Venkatesh", "Arash Gholamidavoodi", "A. Tabassum", "Arul Menezes", "Arun Kirubarajan", "A. Mullokandov", "Ashish Sabharwal", "Austin Herrick", "Avia Efrat", "Aykut Erdem", "Ayla Karakacs", "B. R. Roberts", "B. S. Loe", "Barret Zoph", "Bartlomiej Bojanowski", "Batuhan Ozyurt", "Behnam Hedayatnia", "Behnam Neyshabur", "Benjamin Inden", "Benno Stein", "Berk Ekmekci", "Bill Yuchen Lin", "B. Howald", "Bryan Orinion", "Cameron Diao", "Cameron Dour", "Catherine Stinson", "Cedrick Argueta", "C'esar Ferri Ram'irez", "Chandan Singh", "Charles Rathkopf", "Chenlin Meng", "Chitta Baral", "Chiyu Wu", "Chris Callison-Burch", "Chris Waites", "Christian Voigt", "Christopher D. Manning", "Christopher Potts", "Cindy Ramirez", "Clara E. Rivera", "Clemencia Siro", "Colin Raffel", "Courtney Ashcraft", "Cristina Garbacea", "Damien Sileo", "Dan Garrette", "Dan Hendrycks", "D. Kilman", "Dan Roth", "Daniel Freeman", "Daniel Khashabi", "Daniel Levy", "D. Gonz'alez", "Danielle R. Perszyk", "Danny Hernandez", "Danqi Chen", "Daphne Ippolito", "Dar Gilboa", "David Dohan", "D. Drakard", "David Jurgens", "Debajyoti Datta", "Deep Ganguli", "Denis Emelin", "Denis Kleyko", "Deniz Yuret", "Derek Chen", "Derek Tam", "Dieuwke Hupkes", "Diganta Misra", "Dilyar Buzan", "Dimitri Coelho Mollo", "Diyi Yang", "Dong-Ho Lee", "Dylan Schrader", "Ekaterina Shutova", "E. D. Cubuk", "Elad Segal", "Eleanor Hagerman", "Elizabeth Barnes", "Elizabeth Donoway", "Ellie Pavlick", "Emanuele Rodolà", "Emma Lam", "Eric Chu", "Eric Tang", "Erkut Erdem", "Ernie Chang", "Ethan A. Chi", "Ethan Dyer", "E. Jerzak", "Ethan Kim", "Eunice Engefu Manyasi", "Evgenii Zheltonozhskii", "Fanyue Xia", "Fatemeh Siar", "Fernando Mart'inez-Plumed", "Francesca Happ'e", "François Chollet", "Frieda Rong", "Gaurav Mishra", "Genta Indra Winata", "Gerard de Melo", "Germán Kruszewski", "Giambattista Parascandolo", "Giorgio Mariani", "Gloria Xinyue Wang", "Gonzalo Jaimovitch-L'opez", "Gregor Betz", "Guy Gur-Ari", "Hana Galijasevic", "Hannah Kim", "Hannah Rashkin", "Hannaneh Hajishirzi", "Harsh Mehta", "H. Bogar", "Henry Shevlin", "Hinrich Schutze", "H. Yakura", "Hongming Zhang", "Hugh Mee Wong", "Ian Ng", "Isaac Noble", "Jaap Jumelet", "Jack Geissinger", "John Kernion", "Jacob Hilton", "Jaehoon Lee", "J. Fisac", "James B. Simon", "James Koppel", "James Zheng", "James Zou", "Jan Koco'n", "Jana Thompson", "Janelle Wingfield", "Jared Kaplan", "Jarema Radom", "Jascha Narain Sohl-Dickstein", "Jason Phang", "Jason Wei", "J. Yosinski", "Jekaterina Novikova", "Jelle Bosscher", "Jennifer Marsh", "Jeremy Kim", "Jeroen Taal", "Jesse Engel", "Jesujoba Oluwadara Alabi", "Jiacheng Xu", "Jiaming Song", "Jillian Tang", "Jane W Waweru", "John Burden", "John Miller", "John U. Balis", "Jonathan Batchelder", "Jonathan Berant", "Jorg Frohberg", "Jos Rozen", "J. Hernández-Orallo", "Joseph Boudeman", "J. Guerr", "Joseph Jones", "Joshua B. Tenenbaum", "Joshua S. Rule", "Joyce Chua", "Kamil Kanclerz", "Karen Livescu", "K. Krauth", "Karthik Gopalakrishnan", "Katerina Ignatyeva", "K. Markert", "Kaustubh D. Dhole", "Kevin Gimpel", "Kevin Omondi", "K. Mathewson", "Kristen Chiafullo", "Ksenia Shkaruta", "Kumar Shridhar", "Kyle McDonell", "Kyle Richardson", "Laria Reynolds", "Leo Gao", "Li Zhang", "Liam Dugan", "Lianhui Qin", "Lidia Contreras-Ochando", "Louis-philippe Morency", "Luca Moschella", "Luca Lam", "Lucy Noble", "Ludwig Schmidt", "Luheng He", "Luis Oliveros Col'on", "Luke Metz", "Lutfi Kerem cSenel", "Maarten Bosma", "Maarten Sap", "Maartje ter Hoeve", "Maheen Farooqi", "Manaal Faruqui", "Mantas Mazeika", "Marco Baturan", "Marco Marelli", "Marco Maru", "Maria Jose Ram’irez Quintana", "M. Tolkiehn", "Mario Giulianelli", "Martha Lewis", "Martin Potthast", "Matthew L. Leavitt", "Matthias Hagen", "M. Schubert", "Medina Baitemirova", "Melody Arnaud", "M. McElrath", "Michael A. Yee", "Michael Cohen", "Michael Gu", "Michael Ivanitskiy", "Michael Starritt", "M. Strube", "Michal Swkedrowski", "Michele Bevilacqua", "Michihiro Yasunaga", "Mihir Kale", "Mike Cain", "Mimee Xu", "Mirac Suzgun", "Mitch Walker", "Monica Tiwari", "Mohit Bansal", "Moin Aminnaseri", "Mor Geva", "Mozhdeh Gheini", "T. MukundVarma", "Nanyun Peng", "Nathan A. Chi", "Nayeon Lee", "Neta Gur-Ari Krakover", "Nicholas Cameron", "Nicholas Roberts", "Nick Doiron", "Nicole Martinez", "Nikita Nangia", "Niklas Deckers", "Niklas Muennighoff", "N. Keskar", "Niveditha Iyer", "Noah Constant", "Noah Fiedel", "Nuan Wen", "Oliver Zhang", "Omar Agha", "Omar Elbaghdadi", "Omer Levy", "Owain Evans", "Pablo Antonio Moreno Casares", "P. Doshi", "Pascale Fung", "Paul Pu Liang", "Paul Vicol", "Pegah Alipoormolabashi", "Peiyuan Liao", "Percy Liang", "Peter Chang", "P. Eckersley", "Phu Mon Htut", "P. Hwang", "P. Milkowski", "P. Patil", "Pouya Pezeshkpour", "Priti Oli", "Qiaozhu Mei", "Qing Lyu", "Qinlang Chen", "Rabin Banjade", "Rachel Etta Rudolph", "Raefer Gabriel", "Rahel Habacker", "Ramon Risco", "Raphael Milliere", "Rhythm Garg", "Richard Barnes", "R. Saurous", "Riku Arakawa", "Robbe Raymaekers", "Robert Frank", "Rohan Sikand", "Roman Novak", "Roman Sitelew", "Ronan Le Bras", "Rosanne Liu", "Rowan Jacobs", "Rui Zhang", "R. Salakhutdinov", "Ryan Chi", "Ryan Lee", "Ryan Stovall", "R. Teehan", "Rylan Yang", "Sahib Singh", "Saif Mohammad", "Sajant Anand", "Sam Dillavou", "Sam Shleifer", "Sam Wiseman", "Samuel Gruetter", "Samuel R. Bowman", "S. Schoenholz", "Sanghyun Han", "Sanjeev Kwatra", "Sarah A. Rous", "Sarik Ghazarian", "Sayan Ghosh", "Sean Casey", "Sebastian Bischoff", "Sebastian Gehrmann", "Sebastian Schuster", "Sepideh Sadeghi", "Shadi S. Hamdan", "Sharon Zhou", "Shashank Srivastava", "Sherry Shi", "Shikhar Singh", "Shima Asaadi", "S. Gu", "Shubh Pachchigar", "Shubham Toshniwal", "Shyam Upadhyay", "Shyamolima Debnath", "Siamak Shakeri", "Simon Thormeyer", "S. Melzi", "Siva Reddy", "S. Makini", "Soo-Hwan Lee", "Spencer Bradley Torene", "Sriharsha Hatwar", "S. Dehaene", "Stefan Divic", "Stefano Ermon", "Stella Biderman", "Stephanie Lin", "Stephen Prasad", "Steven T Piantadosi", "Stuart M. Shieber", "Summer Misherghi", "S. Kiritchenko", "Swaroop Mishra", "Tal Linzen", "Tal Schuster", "Tao Li", "Tao Yu", "Tariq Ali", "Tatsunori Hashimoto", "Te-Lin Wu", "T. Desbordes", "Theodore Rothschild", "Thomas Phan", "Tianle Wang", "Tiberius Nkinyili", "Timo Schick", "T. Kornev", "T. Tunduny", "Tobias Gerstenberg", "T. Chang", "Trishala Neeraj", "Tushar Khot", "Tyler Shultz", "Uri Shaham", "Vedant Misra", "Vera Demberg", "Victoria Nyamai", "Vikas Raunak", "V. Ramasesh", "Vinay Uday Prabhu", "Vishakh Padmakumar", "Vivek Srikumar", "W. Fedus", "W. Saunders", "William Zhang", "Wout Vossen", "Xiang Ren", "Xiaoyu Tong", "Xinran Zhao", "Xinyi Wu", "Xudong Shen", "Yadollah Yaghoobzadeh", "Yair Lakretz", "Yangqiu Song", "Yasaman Bahri", "Yejin Choi", "Yichi Yang", "Yiding Hao", "Yifu Chen", "Yonatan Belinkov", "Yu Hou", "Yu Hou", "Yuntao Bai", "Zachary Seid", "Zhuoye Zhao", "Zijian Wang", "Zijie J. Wang", "Zirui Wang", "Ziyi Wu"], "venue": "arXiv.org", "abstract": "Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit\"breakthrough\"behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.", "year": 2022, "publicationdate": "2022-06-09", "externalids": {}, "doi_lower": null}
{"paper_id": 258212638, "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent", "author_names": ["Weiwei Sun", "Lingyong Yan", "Xinyu Ma", "Pengjie Ren", "Dawei Yin", "Z. Ren"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model's ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.", "year": 2023, "publicationdate": "2023-04-19", "externalids": {"DOI": "10.48550/arXiv.2304.09542"}, "doi_lower": "10.48550/arxiv.2304.09542"}
{"paper_id": 258866165, "title": "EvEval: A Comprehensive Evaluation of Event Semantics for Large Language Models", "author_names": ["Zhengwei Tao", "Zhi Jin", "Xiaoying Bai", "Haiyan Zhao", "Yanlin Feng", "Jia Li", "Wenpeng Hu"], "venue": "arXiv.org", "abstract": "Events serve as fundamental units of occurrence within various contexts. The processing of event semantics in textual information forms the basis of numerous natural language processing (NLP) applications. Recent studies have begun leveraging large language models (LLMs) to address event semantic processing. However, the extent that LLMs can effectively tackle these challenges remains uncertain. Furthermore, the lack of a comprehensive evaluation framework for event semantic processing poses a significant challenge in evaluating these capabilities. In this paper, we propose an overarching framework for event semantic processing, encompassing understanding, reasoning, and prediction, along with their fine-grained aspects. To comprehensively evaluate the event semantic processing abilities of models, we introduce a novel benchmark called EVEVAL. We collect 8 datasets that cover all aspects of event semantic processing. Extensive experiments are conducted on EVEVAL, leading to several noteworthy findings based on the obtained results.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.15268"}, "doi_lower": "10.48550/arxiv.2305.15268"}
{"paper_id": 233296016, "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models", "author_names": ["Nandan Thakur", "Nils Reimers", "Andreas Ruckl'e", "Abhishek Srivastava", "Iryna Gurevych"], "venue": "NeurIPS Datasets and Benchmarks", "abstract": "Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir.", "year": 2021, "publicationdate": "2021-04-17", "externalids": {}, "doi_lower": null}
{"paper_id": 258096420, "title": "Trialling a Large Language Model (ChatGPT) in General Practice With the Applied Knowledge Test: Observational Study Demonstrating Opportunities and Limitations in Primary Care", "author_names": ["A. J. Thirunavukarasu", "Refaat Hassan", "Shathar Mahmood", "Rohan Sanghera", "Kara Barzangi", "Mohanned El Mukashfi", "Sachin Shah"], "venue": "JMIR Medical Education", "abstract": "Background Large language models exhibiting human-level performance in specialized tasks are emerging; examples include Generative Pretrained Transformer 3.5, which underlies the processing of ChatGPT. Rigorous trials are required to understand the capabilities of emerging technology, so that innovation can be directed to benefit patients and practitioners. Objective Here, we evaluated the strengths and weaknesses of ChatGPT in primary care using the Membership of the Royal College of General Practitioners Applied Knowledge Test (AKT) as a medium. Methods AKT questions were sourced from a web-based question bank and 2 AKT practice papers. In total, 674 unique AKT questions were inputted to ChatGPT, with the model’s answers recorded and compared to correct answers provided by the Royal College of General Practitioners. Each question was inputted twice in separate ChatGPT sessions, with answers on repeated trials compared to gauge consistency. Subject difficulty was gauged by referring to examiners’ reports from 2018 to 2022. Novel explanations from ChatGPT—defined as information provided that was not inputted within the question or multiple answer choices—were recorded. Performance was analyzed with respect to subject, difficulty, question source, and novel model outputs to explore ChatGPT’s strengths and weaknesses. Results Average overall performance of ChatGPT was 60.17%, which is below the mean passing mark in the last 2 years (70.42%). Accuracy differed between sources (P=.04 and .06). ChatGPT’s performance varied with subject category (P=.02 and .02), but variation did not correlate with difficulty (Spearman ρ=–0.241 and –0.238; P=.19 and .20). The proclivity of ChatGPT to provide novel explanations did not affect accuracy (P>.99 and .23). Conclusions Large language models are approaching human expert–level performance, although further development is required to match the performance of qualified primary care physicians in the AKT. Validated high-performance models may serve as assistants or autonomous clinical tools to ameliorate the general practice workforce crisis.", "year": 2023, "publicationdate": "2023-02-20", "externalids": {"DOI": "10.2196/46599"}, "doi_lower": "10.2196/46599"}
{"paper_id": 246063428, "title": "LaMDA: Language Models for Dialog Applications", "author_names": ["R. Thoppilan", "Daniel De Freitas", "Jamie Hall", "Noam Shazeer", "Apoorv Kulshreshtha", "Heng-Tze Cheng", "Alicia Jin", "Taylor Bos", "Leslie Baker", "Yu Du", "Yaguang Li", "Hongrae Lee", "H. Zheng", "Amin Ghafouri", "Marcelo Menegali", "Yanping Huang", "M. Krikun", "Dmitry Lepikhin", "James Qin", "Dehao Chen", "Yuanzhong Xu", "Zhifeng Chen", "Adam Roberts", "Maarten Bosma", "Yanqi Zhou", "Chung-Ching Chang", "I. Krivokon", "W. Rusch", "Marc Pickett", "K. Meier-Hellstern", "M. Morris", "Tulsee Doshi", "Renelito Delos Santos", "Toju Duke", "J. Søraker", "Ben Zevenbergen", "Vinodkumar Prabhakaran", "Mark Díaz", "Ben Hutchinson", "Kristen Olson", "Alejandra Molina", "Erin Hoffman-John", "Josh Lee", "Lora Aroyo", "Ravi Rajakumar", "Alena Butryna", "Matthew Lamm", "V. Kuzmina", "Joseph Fenton", "Aaron Cohen", "R. Bernstein", "R. Kurzweil", "Blaise Aguera-Arcas", "Claire Cui", "M. Croak", "Ed H. Chi", "Quoc Le"], "venue": "arXiv.org", "abstract": "We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.", "year": 2022, "publicationdate": "2022-01-20", "externalids": {}, "doi_lower": null}
{"paper_id": 247958140, "title": "Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks", "author_names": ["Tristan Thrush", "Kushal Tirumala", "Anmol Gupta", "Max Bartolo", "Pedro Rodriguez", "Tariq Kane", "W. G. Rojas", "Peter Mattson", "Adina Williams", "Douwe Kiela"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We introduce Dynatask: an open source system for setting up custom NLP tasks that aims to greatly lower the technical knowledge and effort required for hosting and evaluating state-of-the-art NLP models, as well as for conducting model in the loop data collection with crowdworkers. Dynatask is integrated with Dynabench, a research platform for rethinking benchmarking in AI that facilitates human and model in the loop data collection and evaluation. To create a task, users only need to write a short task configuration file from which the relevant web interfaces and model hosting infrastructure are automatically generated. The system is available at https://dynabench.org/ and the full library can be found at https://github.com/facebookresearch/dynabench.", "year": 2022, "publicationdate": "2022-04-05", "externalids": {"DOI": "10.48550/arXiv.2204.01906"}, "doi_lower": "10.48550/arxiv.2204.01906"}
{"paper_id": 258865733, "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback", "author_names": ["Katherine Tian", "E. Mitchell", "Allan Zhou", "Archit Sharma", "Rafael Rafailov", "Huaxiu Yao", "Chelsea Finn", "Christopher D. Manning"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.14975"}, "doi_lower": "10.48550/arxiv.2305.14975"}
{"paper_id": 4055261, "title": "DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars", "author_names": ["Yuchi Tian", "Kexin Pei", "S. Jana", "Baishakhi Ray"], "venue": "International Conference on Software Engineering", "abstract": "Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads. However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases. In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.", "year": 2017, "publicationdate": "2017-08-28", "externalids": {"DOI": "10.1145/3180155.3180220"}, "doi_lower": "10.1145/3180155.3180220"}
{"paper_id": 215878961, "title": "Open-source tools and benchmarks for code-clone detection", "author_names": ["WalkerAndrew", "CernyTomas", "SongEungee"], "venue": "", "abstract": null, "year": 2020, "publicationdate": "2020-01-28", "externalids": {"DOI": "10.1145/3381307.3381310"}, "doi_lower": "10.1145/3381307.3381310"}
{"paper_id": 257219404, "title": "LLaMA: Open and Efficient Foundation Language Models", "author_names": ["Hugo Touvron", "Thibaut Lavril", "Gautier Izacard", "Xavier Martinet", "M. Lachaux", "Timothée Lacroix", "Baptiste Rozière", "Naman Goyal", "Eric Hambro", "Faisal Azhar", "Aur'elien Rodriguez", "Armand Joulin", "Edouard Grave", "Guillaume Lample"], "venue": "arXiv.org", "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.", "year": 2023, "publicationdate": "2023-02-27", "externalids": {}, "doi_lower": null}
{"paper_id": 14636783, "title": "Computing Machinery and Intelligence", "author_names": ["A. Turing"], "venue": "The Philosophy of Artificial Intelligence", "abstract": null, "year": 1950, "publicationdate": "1950-10-01", "externalids": {"DOI": "10.1093/MIND/LIX.236.433"}, "doi_lower": "10.1093/mind/lix.236.433"}
{"paper_id": 260440590, "title": "On the Planning Abilities of Large Language Models - A Critical Investigation", "author_names": ["Karthik Valmeekam", "Matthew Marquez", "S. Sreedharan", "Subbarao Kambhampati"], "venue": "Neural Information Processing Systems", "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation.", "year": 2023, "publicationdate": "2023-05-25", "externalids": {"DOI": "10.48550/arXiv.2305.15771"}, "doi_lower": "10.48550/arxiv.2305.15771"}
{"paper_id": 249889477, "title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change", "author_names": ["Karthik Valmeekam", "Alberto Olmo", "S. Sreedharan", "Subbarao Kambhampati"], "venue": "Neural Information Processing Systems", "abstract": "Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.", "year": 2022, "publicationdate": "2022-06-21", "externalids": {}, "doi_lower": null}
{"paper_id": 209333890, "title": "Best practices for the human evaluation of automatically generated text", "author_names": ["Chris van der Lee", "Albert Gatt", "Emiel van Miltenburg", "Sander Wubben", "E. Krahmer"], "venue": "International Conference on Natural Language Generation", "abstract": "Currently, there is little agreement as to how Natural Language Generation (NLG) systems should be evaluated. While there is some agreement regarding automatic metrics, there is a high degree of variation in the way that human evaluation is carried out. This paper provides an overview of how human evaluation is currently conducted, and presents a set of best practices, grounded in the literature. With this paper, we hope to contribute to the quality and consistency of human evaluations in NLG.", "year": 2019, "publicationdate": "2019-10-01", "externalids": {"DOI": "10.18653/v1/W19-8643"}, "doi_lower": "10.18653/v1/w19-8643"}
{"paper_id": 233210249, "title": "Not All Attention Is All You Need", "author_names": ["Hongqiu Wu", "Hai Zhao", "Min Zhang"], "venue": "arXiv.org", "abstract": "Beyond the success story of pre-trained language models (PrLMs) in recent natural language processing, they are susceptible to over-fitting due to unusual large model size. To this end, dropout serves as a therapy. However, existing methods like random-based, knowledge-based and search-based dropout are more general but less effective onto self-attention based models, which are broadly chosen as the fundamental architecture of PrLMs. In this paper, we propose a novel dropout method named AttendOut to let self-attention empowered PrLMs capable of more robust task-specific tuning. We demonstrate that state-of-the-art models with elaborate training design may achieve much stronger results. We verify the universality of our approach on extensive natural language processing tasks.", "year": 2021, "publicationdate": "2021-04-10", "externalids": {}, "doi_lower": null}
{"paper_id": 263672149, "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation", "author_names": ["Tu Vu", "Mohit Iyyer", "Xuezhi Wang", "Noah Constant", "Jerry Wei", "Jason Wei", "C. Tar", "Yun-Hsuan Sung", "Denny Zhou", "Quoc Le", "Thang Luong"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.", "year": 2023, "publicationdate": "2023-10-05", "externalids": {"DOI": "10.48550/arXiv.2310.03214"}, "doi_lower": "10.48550/arxiv.2310.03214"}
{"paper_id": 143424870, "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "author_names": ["Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "venue": "Neural Information Processing Systems", "abstract": "In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at this http URL.", "year": 2019, "publicationdate": "2019-05-02", "externalids": {}, "doi_lower": null}
{"paper_id": 5034059, "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "author_names": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "venue": "BlackboxNLP@EMNLP", "abstract": "Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.", "year": 2018, "publicationdate": "2018-04-20", "externalids": {"DOI": "10.18653/v1/W18-5446"}, "doi_lower": "10.18653/v1/w18-5446"}
{"paper_id": 259202782, "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models", "author_names": ["Boxin Wang", "Weixin Chen", "Hengzhi Pei", "Chulin Xie", "Mintong Kang", "Chenhui Zhang", "Chejian Xu", "Zidi Xiong", "Ritik Dutta", "Rylan Schaeffer", "Sang Truong", "Simran Arora", "Mantas Mazeika", "Dan Hendrycks", "Zinan Lin", "Yuk-Kit Cheng", "Sanmi Koyejo", "D. Song", "Bo Li"], "venue": "Neural Information Processing Systems", "abstract": "Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance -- where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives -- including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially because GPT-4 follows (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/ ; our dataset can be previewed at https://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version of this work is at https://openreview.net/pdf?id=kaHpo8OZw2 .", "year": 2023, "publicationdate": "2023-06-20", "externalids": {"DOI": "10.48550/arXiv.2306.11698"}, "doi_lower": "10.48550/arxiv.2306.11698"}
{"paper_id": 258832435, "title": "GPT-SW3: An Autoregressive Language Model for the Nordic Languages", "author_names": ["Ariel Ekgren", "Amaru Cuba Gyllensten", "F. Stollenwerk", "Joey Öhman", "T. Isbister", "Evangelia Gogoulou", "F. Carlsson", "Alice Heiman", "Judit Casademont", "Magnus Sahlgren"], "venue": "arXiv.org", "abstract": "This paper details the process of developing the first native large generative language model for the Nordic languages, GPT-SW3. We cover all parts of the development process, from data collection and processing, training configuration and instruction finetuning, to evaluation and considerations for release strategies. We hope that this paper can serve as a guide and reference for other researchers that undertake the development of large generative models for smaller languages.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.48550/arXiv.2305.12987"}, "doi_lower": "10.48550/arxiv.2305.12987"}
{"paper_id": 242757097, "title": "Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models", "author_names": ["Boxin Wang", "Chejian Xu", "Shuohang Wang", "Zhe Gan", "Yu Cheng", "Jianfeng Gao", "A. Awadallah", "B. Li"], "venue": "NeurIPS Datasets and Benchmarks", "abstract": "Large-scale pre-trained language models have achieved tremendous success across a wide range of natural language understanding (NLU) tasks, even surpassing human performance. However, recent studies reveal that the robustness of these models can be challenged by carefully crafted textual adversarial examples. While several individual datasets have been proposed to evaluate model robustness, a principled and comprehensive benchmark is still missing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations. Our findings are summarized as follows. (i) Most existing adversarial attack algorithms are prone to generating invalid or ambiguous adversarial examples, with around 90% of them either changing the original semantic meanings or misleading human annotators as well. Therefore, we perform a careful filtering process to curate a high-quality benchmark. (ii) All the language models and robust training methods we tested perform poorly on AdvGLUE, with scores lagging far behind the benign accuracy. We hope our work will motivate the development of new adversarial attacks that are more stealthy and semantic-preserving, as well as new robust language models against sophisticated adversarial attacks. AdvGLUE is available at https://adversarialglue.github.io.", "year": 2021, "publicationdate": "2021-11-04", "externalids": {}, "doi_lower": null}
{"paper_id": 258833523, "title": "Evaluating Open Question Answering Evaluation", "author_names": ["Cunxiang Wang", "Sirui Cheng", "Zhikun Xu", "Bowen Ding", "Yidong Wang", "Yue Zhang"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.12421"}, "doi_lower": "10.48550/arxiv.2305.12421"}
{"paper_id": 258822931, "title": "Chain-of-thought prompting for responding to in-depth dialogue questions with LLM", "author_names": ["Hongru Wang", "Rui Wang", "Fei Mi", "Zezhong Wang", "Rui-Lan Xu", "Kam-Fai Wong"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.11792"}, "doi_lower": "10.48550/arxiv.2305.11792"}
{"paper_id": 257102461, "title": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective", "author_names": ["Jindong Wang", "Xixu Hu", "Wenxin Hou", "Hao Chen", "Runkai Zheng", "Yidong Wang", "Linyi Yang", "Haojun Huang", "Weirong Ye", "Xiubo Geng", "Binxing Jiao", "Yue Zhang", "Xingxu Xie"], "venue": "IEEE Data Engineering Bulletin", "abstract": "ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat to foundation models. Moreover, ChatGPT shows astounding performance in understanding dialogue-related texts and we find that it tends to provide informal suggestions for medical tasks instead of definitive answers. Finally, we present in-depth discussions of possible research directions.", "year": 2023, "publicationdate": "2023-02-22", "externalids": {"DOI": "10.48550/arXiv.2302.12095"}, "doi_lower": "10.48550/arxiv.2302.12095"}
{"paper_id": 232110832, "title": "Generalizing to Unseen Domains: A Survey on Domain Generalization", "author_names": ["Jindong Wang", "Cuiling Lan", "Chang Liu", "Yidong Ouyang", "Tao Qin"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "abstract": "Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.", "year": 2021, "publicationdate": "2021-03-02", "externalids": {"DOI": "10.1109/TKDE.2022.3178128"}, "doi_lower": "10.1109/tkde.2022.3178128"}
{"paper_id": 274505042, "title": "LARGE LANGUAGE MODELS AND MACHINE TRANSLATION", "author_names": ["Maryana Tomenchuk", "Kseniia Popovych"], "venue": "Věda a perspektivy", "abstract": null, "year": 2024, "publicationdate": "2024-12-02", "externalids": {"DOI": "10.52058/2695-1592-2024-11(42)-422-431"}, "doi_lower": "10.52058/2695-1592-2024-11(42)-422-431"}
{"paper_id": 258960339, "title": "Large Language Models are not Fair Evaluators", "author_names": ["Peiyi Wang", "Lei Li", "Liang Chen", "Dawei Zhu", "Binghuai Lin", "Yunbo Cao", "Qi Liu", "Tianyu Liu", "Zhifang Sui"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the\"win/tie/lose\"outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \\url{https://github.com/i-Eval/FairEval} to facilitate future research.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {"DOI": "10.48550/arXiv.2305.17926"}, "doi_lower": "10.48550/arxiv.2305.17926"}
{"paper_id": 261030527, "title": "CMB: A Comprehensive Medical Benchmark in Chinese", "author_names": ["Xidong Wang", "Guiming Hardy Chen", "Dingjie Song", "Zhiyi Zhang", "Zhihong Chen", "Qingying Xiao", "Feng Jiang", "Jianquan Li", "Xiang Wan", "Benyou Wang", "Haizhou Li"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in contextual incongruities to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. We hope this benchmark provide first-hand experience in existing LLMs for medicine and also facilitate the widespread adoption and enhancement of medical LLMs within China. Our data and code are publicly available at https://github.com/FreedomIntelligence/CMB.", "year": 2023, "publicationdate": "2023-08-17", "externalids": {"DOI": "10.48550/arXiv.2308.08833"}, "doi_lower": "10.48550/arxiv.2308.08833"}
{"paper_id": 259951557, "title": "Emotional intelligence of Large Language Models", "author_names": ["Xuena Wang", "Xueting Li", "Zi Yin", "Yue Wu", "Liu Jia Department of PsychologyTsinghua Laboratory of Brain", "Intelligence", "Tsinghua University", "Departmentof Psychology", "Renmin University"], "venue": "Journal of Pacific Rim Psychology", "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI. This test is an objective, performance-driven, and text-based evaluation, which requires evaluating complex emotions in realistic scenarios, providing a consistent assessment for both human and LLM capabilities. With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achieved above-average Emotional Quotient (EQ) scores, with GPT-4 exceeding 89% of human participants with an EQ of 117. Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not rely on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans. In addition, we discussed the impact of factors such as model size, training method, and architecture on LLMs' EQ. In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence. Project website: https://emotional-intelligence.github.io/", "year": 2023, "publicationdate": "2023-01-01", "externalids": {"DOI": "10.1177/18344909231213958"}, "doi_lower": "10.1177/18344909231213958"}
{"paper_id": 237386541, "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation", "author_names": ["Yue Wang", "Weishi Wang", "Shafiq R. Joty", "S. Hoi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.", "year": 2021, "publicationdate": "2021-09-02", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.685"}, "doi_lower": "10.18653/v1/2021.emnlp-main.685"}
{"paper_id": 257921626, "title": "Exploring Vision-Language Models for Imbalanced Learning", "author_names": ["Yidong Wang", "Zhuohao Yu", "Jindong Wang", "Qiang Heng", "Haoxing Chen", "Wei Ye", "Rui Xie", "Xingxu Xie", "Shi-Bo Zhang"], "venue": "International Journal of Computer Vision", "abstract": "Vision-language models (VLMs) that use contrastive language-image pre-training have shown promising zero-shot classification performance. However, their performance on imbalanced dataset is relatively poor, where the distribution of classes in the training dataset is skewed, leading to poor performance in predicting minority classes. For instance, CLIP achieved only 5% accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder to VLMs to avoid out of memory problem caused by large number of classes and capture nuanced features for tail classes. Then, we explore improvements of VLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms such as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments demonstrate that the performance of VLMs can be further boosted when used with decoder and imbalanced methods. Specifically, our improved VLMs significantly outperforms zero-shot classification by an average accuracy of 6.58%, 69.82%, and 6.17%, on ImageNet-LT, iNaturalist18, and Places-LT, respectively. We further analyze the influence of pre-training data size, backbones, and training cost. Our study highlights the significance of imbalanced learning algorithms in face of VLMs pre-trained by huge data. We release our code at https://github.com/Imbalance-VLM/Imbalance-VLM.", "year": 2023, "publicationdate": "2023-04-04", "externalids": {"DOI": "10.1007/s11263-023-01868-w"}, "doi_lower": "10.1007/s11263-023-01868-w"}
{"paper_id": 259064252, "title": "Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today", "author_names": ["Zhuo Wang", "R. Li", "Bowen Dong", "Jie Wang", "Xiuxing Li", "Ning Liu", "C. Mao", "Wei Zhang", "L. Dong", "Jing Gao", "Jianyong Wang"], "venue": "arXiv.org", "abstract": "Recent investigations show that large language models (LLMs), specifically GPT-4, not only have remarkable capabilities in common Natural Language Processing (NLP) tasks but also exhibit human-level performance on various professional and academic benchmarks. However, whether GPT-4 can be directly used in practical applications and replace traditional artificial intelligence (AI) tools in specialized domains requires further experimental validation. In this paper, we explore the potential of LLMs such as GPT-4 to outperform traditional AI tools in dementia diagnosis. Comprehensive comparisons between GPT-4 and traditional AI tools are conducted to examine their diagnostic accuracy in a clinical setting. Experimental results on two real clinical datasets show that, although LLMs like GPT-4 demonstrate potential for future advancements in dementia diagnosis, they currently do not surpass the performance of traditional AI tools. The interpretability and faithfulness of GPT-4 are also evaluated by comparison with real doctors. We discuss the limitations of GPT-4 in its current state and propose future research directions to enhance GPT-4 in dementia diagnosis.", "year": 2023, "publicationdate": "2023-06-02", "externalids": {"DOI": "10.48550/arXiv.2306.01499"}, "doi_lower": "10.48550/arxiv.2306.01499"}
{"paper_id": 258048703, "title": "Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study", "author_names": ["Zengzhi Wang", "Qiming Xie", "Zixiang Ding", "Yi Feng", "Rui Xia"], "venue": "arXiv.org", "abstract": "Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly interested in whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of \\emph{opinions}, \\emph{sentiments}, and \\emph{emotions} contained in the text. Specifically, we evaluate it in three settings, including \\emph{standard} evaluation, \\emph{polarity shift} evaluation and \\emph{open-domain} evaluation. We conduct an evaluation on 7 representative sentiment analysis tasks covering 17 benchmark datasets and compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on them. We also attempt several popular prompting techniques to elicit the ability further. Moreover, we conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.", "year": 2023, "publicationdate": "2023-04-10", "externalids": {"DOI": "10.48550/arXiv.2304.04339"}, "doi_lower": "10.48550/arxiv.2304.04339"}
{"paper_id": 249674500, "title": "Emergent Abilities of Large Language Models", "author_names": ["Jason Wei", "Yi Tay", "Rishi Bommasani", "Colin Raffel", "Barret Zoph", "Sebastian Borgeaud", "Dani Yogatama", "Maarten Bosma", "Denny Zhou", "Donald Metzler", "Ed H. Chi", "Tatsunori Hashimoto", "O. Vinyals", "P. Liang", "J. Dean", "W. Fedus"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.", "year": 2022, "publicationdate": "2022-06-15", "externalids": {"DOI": "10.48550/arXiv.2206.07682"}, "doi_lower": "10.48550/arxiv.2206.07682"}
{"paper_id": 249674500, "title": "Emergent Abilities of Large Language Models", "author_names": ["Jason Wei", "Yi Tay", "Rishi Bommasani", "Colin Raffel", "Barret Zoph", "Sebastian Borgeaud", "Dani Yogatama", "Maarten Bosma", "Denny Zhou", "Donald Metzler", "Ed H. Chi", "Tatsunori Hashimoto", "O. Vinyals", "P. Liang", "J. Dean", "W. Fedus"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.", "year": 2022, "publicationdate": "2022-06-15", "externalids": {"DOI": "10.48550/arXiv.2206.07682"}, "doi_lower": "10.48550/arxiv.2206.07682"}
{"paper_id": 257079092, "title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT", "author_names": ["Jules White", "Quchen Fu", "Sam Hays", "Michael Sandborn", "Carlos Olea", "Henry Gilbert", "Ashraf Elnashar", "Jesse Spencer-Smith", "Douglas C. Schmidt"], "venue": "arXiv.org", "abstract": "Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.", "year": 2023, "publicationdate": "2023-02-21", "externalids": {}, "doi_lower": null}
{"paper_id": 262906910, "title": "Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting", "author_names": ["Patrick Y. Wu", "Joshua A. Tucker", "Jonathan Nagler", "Solomon Messing"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2303.12057"}, "doi_lower": "10.48550/arxiv.2303.12057"}
{"paper_id": 260900008, "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification", "author_names": ["Aojun Zhou", "Ke Wang", "Zimu Lu", "Weikang Shi", "Sichun Luo", "Zipeng Qin", "Shaoqing Lu", "Anya Jia", "Linqi Song", "Mingjie Zhan", "Hongsheng Li"], "venue": "International Conference on Learning Representations", "abstract": "Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \\textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \\uline{c}ode-based \\uline{s}elf-\\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset \\textbf{(53.9\\% $\\to$ 84.3\\%)}.", "year": 2023, "publicationdate": "2023-08-15", "externalids": {"DOI": "10.48550/arXiv.2308.07921"}, "doi_lower": "10.48550/arxiv.2308.07921"}
{"paper_id": 249063032, "title": "Autoformalization with Large Language Models", "author_names": ["Yuhuai Wu", "Albert Qiaochu Jiang", "Wenda Li", "M. Rabe", "Charles Staats", "M. Jamnik", "Christian Szegedy"], "venue": "Neural Information Processing Systems", "abstract": "Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion ($25.3\\%$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from $29.6\\%$ to $35.2\\%$.", "year": 2022, "publicationdate": "2022-05-25", "externalids": {"DOI": "10.48550/arXiv.2205.12615"}, "doi_lower": "10.48550/arxiv.2205.12615"}
{"paper_id": 259341893, "title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks", "author_names": ["Zhaofeng Wu", "Linlu Qiu", "Alexis Ross", "Ekin Akyürek", "Boyuan Chen", "Bailin Wang", "Najoung Kim", "Jacob Andreas", "Yoon Kim"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on “counterfactual” task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects.", "year": 2023, "publicationdate": "2023-07-05", "externalids": {"DOI": "10.48550/arXiv.2307.02477"}, "doi_lower": "10.48550/arxiv.2307.02477"}
{"paper_id": 263608964, "title": "Ask Again, Then Fail: Large Language Models' Vacillations in Judgement", "author_names": ["Qiming Xie", "Zengzhi Wang", "Yi Feng", "Rui Xia"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We observe that current conversational language models often waver in their judgments when faced with follow-up questions, even if the original judgment was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a \\textsc{Follow-up Questioning Mechanism} along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework \\textsc{Unwavering-FQ} that teaches language models to maintain their originally correct judgments through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models.", "year": 2023, "publicationdate": "2023-10-03", "externalids": {"DOI": "10.48550/arXiv.2310.02174"}, "doi_lower": "10.48550/arxiv.2310.02174"}
{"paper_id": 259188006, "title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views", "author_names": ["Fangzhi Xu", "Qika Lin", "Jiawei Han", "Tianzhe Zhao", "Jun Liu", "E. Cambria"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.09841"}, "doi_lower": "10.48550/arxiv.2306.09841"}
{"paper_id": 259983087, "title": "CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility", "author_names": ["Guohai Xu", "Jiayi Liu", "Mingshi Yan", "Haotian Xu", "Jinghui Si", "Zhuoran Zhou", "Peng Yi", "Xing Gao", "Jitao Sang", "Rong Zhang", "Ji Zhang", "Chao Peng", "Feiyan Huang", "Jingren Zhou"], "venue": "arXiv.org", "abstract": "With the rapid evolution of large language models (LLMs), there is a growing concern that they may pose risks or have negative social impacts. Therefore, evaluation of human values alignment is becoming increasingly important. Previous work mainly focuses on assessing the performance of LLMs on certain knowledge and reasoning abilities, while neglecting the alignment to human values, especially in a Chinese context. In this paper, we present CValues, the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria. As a result, we have manually collected adversarial safety prompts across 10 scenarios and induced responsibility prompts from 8 domains by professional experts. To provide a comprehensive values evaluation of Chinese LLMs, we not only conduct human evaluation for reliable comparison, but also construct multi-choice prompts for automatic evaluation. Our findings suggest that while most Chinese LLMs perform well in terms of safety, there is considerable room for improvement in terms of responsibility. Moreover, both the automatic and human evaluation are important for assessing the human values alignment in different aspects. The benchmark and code is available on ModelScope and Github.", "year": 2023, "publicationdate": "2023-07-19", "externalids": {"DOI": "10.48550/arXiv.2307.09705"}, "doi_lower": "10.48550/arxiv.2307.09705"}
{"paper_id": 259165040, "title": "LVLM-EHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models", "author_names": ["Peng Xu", "Wenqi Shao", "Kaipeng Zhang", "Peng Gao", "Shuo Liu", "Meng Lei", "Fanqing Meng", "Siyuan Huang", "Y. Qiao", "Ping Luo"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building an LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of 13 representative LVLMs such as InstructBLIP and LLaVA, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates five categories of multimodal capabilities of LVLMs such as visual question answering and object hallucination on 42 in-domain text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study investigates how specific features of LVLMs such as model configurations, modality alignment mechanisms, and training data affect the multimodal understanding. By conducting a comprehensive comparison of these features on quantitative and arena evaluation, our study uncovers several innovative findings, which establish a fundamental framework for the development and evaluation of innovative strategies aimed at enhancing multimodal techniques.", "year": 2023, "publicationdate": "2023-06-15", "externalids": {"DOI": "10.1109/TPAMI.2024.3507000"}, "doi_lower": "10.1109/tpami.2024.3507000"}
{"paper_id": 257913006, "title": "Accuracy and Political Bias of News Source Credibility Ratings by Large Language Models", "author_names": ["Kai-Cheng Yang", "F. Menczer"], "venue": "Web Science Conference", "abstract": "Search engines increasingly leverage large language models (LLMs) to generate direct answers, and AI chatbots now access the Internet for fresh data. As information curators for billions of users, LLMs must assess the accuracy and reliability of different sources. This paper audits nine widely used LLMs from three leading providers—OpenAI, Google, and Meta—to evaluate their ability to discern credible and high-quality information sources from low-credibility ones. We find that while LLMs can rate most tested news outlets, larger models more frequently refuse to provide ratings due to insufficient information, whereas smaller models are more prone to making errors in their ratings. For sources where ratings are provided, LLMs exhibit a high level of agreement among themselves (average Spearman’s ρ = 0.79), but their ratings align only moderately with human expert evaluations (average ρ = 0.50). Analyzing news sources with different political leanings in the US, we observe a liberal bias in credibility ratings yielded by all LLMs in default configurations. Additionally, assigning partisan roles to LLMs consistently induces strong politically congruent bias in their ratings. These findings have important implications for the use of LLMs in curating news and political information.", "year": 2023, "publicationdate": "2023-04-01", "externalids": {"DOI": "10.1145/3717867.3717903"}, "doi_lower": "10.1145/3717867.3717903"}
{"paper_id": 253523094, "title": "GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective", "author_names": ["Linyi Yang", "Shuibai Zhang", "Libo Qin", "Yafu Li", "Yidong Wang", "Hanmeng Liu", "Jindong Wang", "Xingxu Xie", "Yue Zhang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named GLUE-X for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.", "year": 2022, "publicationdate": "2022-11-15", "externalids": {"DOI": "10.48550/arXiv.2211.08073"}, "doi_lower": "10.48550/arxiv.2211.08073"}
{"paper_id": 259138958, "title": "LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark", "author_names": ["Zhen-fei Yin", "Jiong Wang", "Jianjian Cao", "Zhelun Shi", "Dingning Liu", "Mukai Li", "Lu Sheng", "Lei Bai", "Xiaoshui Huang", "Zhiyong Wang", "Wanli Ouyang", "Jing Shao"], "venue": "Neural Information Processing Systems", "abstract": "Large language models have become a potential pathway toward achieving artificial general intelligence. Recent works on multi-modal large language models have demonstrated their effectiveness in handling visual modalities. In this work, we extend the research of MLLMs to point clouds and present the LAMM-Dataset and LAMM-Benchmark for 2D image and 3D point cloud understanding. We also establish an extensible framework to facilitate the extension of MLLMs to additional modalities. Our main contribution is three-fold: 1) We present the LAMM-Dataset and LAMM-Benchmark, which cover almost all high-level vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our dataset and benchmark. 2) We demonstrate the detailed methods of constructing instruction-tuning datasets and benchmarks for MLLMs, which will enable future research on MLLMs to scale up and extend to other domains, tasks, and modalities faster. 3) We provide a primary but potential MLLM training framework optimized for modalities' extension. We also provide baseline models, comprehensive experimental observations, and analysis to accelerate future research. Codes and datasets are now available at https://github.com/OpenLAMM/LAMM.", "year": 2023, "publicationdate": "2023-06-11", "externalids": {"DOI": "10.48550/arXiv.2306.06687"}, "doi_lower": "10.48550/arxiv.2306.06687"}
{"paper_id": 259165244, "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models", "author_names": ["Jifan Yu", "Xiaozhi Wang", "Shangqing Tu", "S. Cao", "Daniel Zhang-Li", "Xin Lv", "Hao Peng", "Zijun Yao", "Xiaohan Zhang", "Hanming Li", "Chun-yan Li", "Zheyuan Zhang", "Yushi Bai", "Yantao Liu", "Amy Xin", "Nianyi Lin", "Kaifeng Yun", "Linlu Gong", "Jianhui Chen", "Zhili Wu", "Yunjia Qi", "Weikai Li", "Yong Guan", "Kaisheng Zeng", "Ji Qi", "Hailong Jin", "Jinxin Liu", "Yuxian Gu", "Yu Gu", "Yuan Yao", "Ning Ding", "Lei Hou", "Zhiyuan Liu", "Bin Xu", "Jie Tang", "Juanzi Li"], "venue": "International Conference on Learning Representations", "abstract": "The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For \\textbf{ability modeling}, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering $19$ tasks. (2) For \\textbf{data}, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For \\textbf{evaluation criteria}, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate $28$ open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset and open-participation leaderboard are publicly released at https://kola.xlore.cn and will be continuously updated to provide references for developing LLMs and knowledge-related systems.", "year": 2023, "publicationdate": "2023-06-15", "externalids": {"DOI": "10.48550/arXiv.2306.09296"}, "doi_lower": "10.48550/arxiv.2306.09296"}
{"paper_id": 262084051, "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models", "author_names": ["L. Yu", "Weisen Jiang", "Han Shi", "Jincheng Yu", "Zhengying Liu", "Yu Zhang", "James T. Kwok", "Zheng Li", "Adrian Weller", "Weiyang Liu"], "venue": "International Conference on Learning Representations", "abstract": "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.", "year": 2023, "publicationdate": "2023-09-21", "externalids": {}, "doi_lower": null}
{"paper_id": 259096157, "title": "Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations", "author_names": ["Lifan Yuan", "Yangyi Chen", "Ganqu Cui", "Hongcheng Gao", "Fangyuan Zou", "Xingyi Cheng", "Heng Ji", "Zhiyuan Liu", "Maosong Sun"], "venue": "Neural Information Processing Systems", "abstract": "This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at \\url{https://github.com/lifan-yuan/OOD_NLP}.", "year": 2023, "publicationdate": "2023-06-07", "externalids": {"DOI": "10.48550/arXiv.2306.04618"}, "doi_lower": "10.48550/arxiv.2306.04618"}
{"paper_id": 257756994, "title": "Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited", "author_names": ["Zheng Yuan", "Fajie Yuan", "Yu Song", "Youhua Li", "Junchen Fu", "Fei Yang", "Yunzhu Pan", "Yongxin Ni"], "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "abstract": "Recommendation models that utilize unique identities (IDs for short) to represent distinct users and items have been state-of-the-art (SOTA) and dominated the recommender systems (RS) literature for over a decade. Meanwhile, the pre-trained modality encoders, such as BERT [9] and Vision Transformer [11], have become increasingly powerful in modeling the raw modality features of an item, such as text and images. Given this, a natural question arises: can a purely modality-based recommendation model (MoRec) outperforms or matches a pure ID-based model (IDRec) by replacing the itemID embedding with a SOTA modality encoder? In fact, this question was answered ten years ago when IDRec beats MoRec by a strong margin in both recommendation accuracy and efficiency. We aim to revisit this 'old' question and systematically study MoRec from several aspects. Specifically, we study several sub-questions: (i) which recommendation paradigm, MoRec or IDRec, performs better in practical scenarios, especially in the general setting and warm item scenarios where IDRec has a strong advantage? does this hold for items with different modality features? (ii) can the latest technical advances from other communities (i.e., natural language processing and computer vision) translate into accuracy improvement for MoRec? (iii) how to effectively utilize item modality representation, can we use it directly or do we have to adjust it with new data? (iv) are there any key challenges that MoRec needs to address in practical applications? To answer them, we conduct rigorous experiments for item recommendations with two popular modalities, i.e., text and vision. We provide the first empirical evidence that MoRec is already comparable to its IDRec counterpart with an expensive end-to-end training method, even for warm item recommendation. Our results potentially imply that the dominance of IDRec in the RS field may be greatly challenged in the future. We release our code and other materials at https://github.com/westlake-repl/IDvs.MoRec.", "year": 2023, "publicationdate": "2023-03-24", "externalids": {"DOI": "10.1145/3539618.3591932"}, "doi_lower": "10.1145/3539618.3591932"}
{"paper_id": 257952500, "title": "How well do Large Language Models perform in Arithmetic tasks?", "author_names": ["Zheng Yuan", "Hongyi Yuan", "Chuanqi Tan", "Wei Wang", "Songfang Huang"], "venue": "arXiv.org", "abstract": "Large language models have emerged abilities including chain-of-thought to answer math word problems step by step. Solving math word problems not only requires abilities to disassemble problems via chain-of-thought but also needs to calculate arithmetic expressions correctly for each step. To the best of our knowledge, there is no work to focus on evaluating the arithmetic ability of large language models. In this work, we propose an arithmetic dataset MATH 401 to test the latest large language models including GPT-4, ChatGPT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provide a detailed analysis of the ability of large language models. MATH 401 and evaluation codes are released at \\url{https://github.com/GanjinZero/math401-llm}.", "year": 2023, "publicationdate": "2023-03-16", "externalids": {"DOI": "10.48550/arXiv.2304.02015"}, "doi_lower": "10.48550/arxiv.2304.02015"}
{"paper_id": 252715691, "title": "GLM-130B: An Open Bilingual Pre-trained Model", "author_names": ["Aohan Zeng", "Xiao Liu", "Zhengxiao Du", "Zihan Wang", "Hanyu Lai", "Ming Ding", "Zhuoyi Yang", "Yifan Xu", "Wendi Zheng", "Xiao Xia", "W. Tam", "Zixuan Ma", "Yufei Xue", "Jidong Zhai", "Wenguang Chen", "P. Zhang", "Yuxiao Dong", "Jie Tang"], "venue": "International Conference on Learning Representations", "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{https://github.com/THUDM/GLM-130B/}.", "year": 2022, "publicationdate": "2022-10-05", "externalids": {"DOI": "10.48550/arXiv.2210.02414"}, "doi_lower": "10.48550/arxiv.2210.02414"}
{"paper_id": 259075484, "title": "Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning", "author_names": ["Beichen Zhang", "Kun Zhou", "Xilin Wei", "Wayne Xin Zhao", "Jing Sha", "Shijin Wang", "Ji-rong Wen"], "venue": "Neural Information Processing Systems", "abstract": "Chain-of-thought prompting~(CoT) and tool augmentation have been validated in recent work as effective practices for improving large language models~(LLMs) to perform step-by-step reasoning on complex math-related tasks. However, most existing math reasoning datasets may be not able to fully evaluate and analyze the ability of LLMs in manipulating tools and performing reasoning, as they may only require very few invocations of tools or miss annotations for evaluating intermediate reasoning steps. To address the issue, we construct \\textbf{CARP}, a new Chinese dataset consisting of 4,886 computation-intensive algebra problems with formulated annotations on intermediate steps. In CARP, we test four LLMs with CoT prompting, and find that they are all prone to make mistakes at the early steps of the solution, leading to wrong answers. Based on this finding, we propose a new approach that can deliberate the reasoning steps with tool interfaces, namely \\textbf{DELI}. In DELI, we first initialize a step-by-step solution based on retrieved exemplars, then iterate two deliberation procedures that check and refine the intermediate steps of the generated solution, from the perspectives of tool manipulation and natural language reasoning, until obtaining converged solutions or reaching the maximum turn. Experimental results on CARP and six other datasets show that the proposed DELI mostly outperforms competitive baselines, and can further boost the performance of existing CoT methods. Our data and code are available in \\url{https://github.com/RUCAIBox/CARP}.", "year": 2023, "publicationdate": "2023-06-04", "externalids": {"DOI": "10.48550/arXiv.2306.02408"}, "doi_lower": "10.48550/arxiv.2306.02408"}
{"paper_id": 258676079, "title": "Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation", "author_names": ["Jizhi Zhang", "Keqin Bao", "Yang Zhang", "Wenjie Wang", "Fuli Feng", "Xiangnan He"], "venue": "ACM Conference on Recommender Systems", "abstract": "The remarkable achievements of Large Language Models (LLMs) have led to the emergence of a novel recommendation paradigm — Recommendation via LLM (RecLLM). Nevertheless, it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation. To avoid the potential risks of RecLLM, it is imperative to evaluate the fairness of RecLLM with respect to various sensitive attributes on the user side. Due to the differences between the RecLLM paradigm and the traditional recommendation paradigm, it is problematic to directly use the fairness benchmark of traditional recommendation. To address the dilemma, we propose a novel benchmark called Fairness of Recommendation via LLM (FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios: music and movies. By utilizing our FaiRLLM benchmark, we conducted an evaluation of ChatGPT and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations. Our code and dataset can be found at https://github.com/jizhi-zhang/FaiRLLM.", "year": 2023, "publicationdate": "2023-05-12", "externalids": {"DOI": "10.1145/3604915.3608860"}, "doi_lower": "10.1145/3604915.3608860"}
{"paper_id": 248496292, "title": "OPT: Open Pre-trained Transformer Language Models", "author_names": ["Susan Zhang", "Stephen Roller", "Naman Goyal", "Mikel Artetxe", "Moya Chen", "Shuohui Chen", "Christopher Dewan", "Mona T. Diab", "Xian Li", "Xi Victoria Lin", "Todor Mihaylov", "Myle Ott", "Sam Shleifer", "Kurt Shuster", "Daniel Simig", "Punit Singh Koura", "Anjali Sridhar", "Tianlu Wang", "Luke Zettlemoyer"], "venue": "arXiv.org", "abstract": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.", "year": 2022, "publicationdate": "2022-05-02", "externalids": {}, "doi_lower": null}
{"paper_id": 127986044, "title": "BERTScore: Evaluating Text Generation with BERT", "author_names": ["Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi"], "venue": "International Conference on Learning Representations", "abstract": "We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.", "year": 2019, "publicationdate": "2019-04-21", "externalids": {}, "doi_lower": null}
{"paper_id": 259108959, "title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models", "author_names": ["Wenxuan Zhang", "Sharifah Mahani Aljunied", "Chang Gao", "Yew Ken Chia", "Lidong Bing"], "venue": "Neural Information Processing Systems", "abstract": "Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels. In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23\\% of the questions require processing images for successful solving. We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions. We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development. Data and evaluation code is available at \\url{https://github.com/DAMO-NLP-SG/M3Exam}.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.48550/arXiv.2306.05179"}, "doi_lower": "10.48550/arxiv.2306.05179"}
{"paper_id": 258866189, "title": "Sentiment Analysis in the Era of Large Language Models: A Reality Check", "author_names": ["Wenxuan Zhang", "Yue Deng", "Bing-Quan Liu", "Sinno Jialin Pan", "Lidong Bing"], "venue": "NAACL-HLT", "abstract": "Sentiment analysis (SA) has been a long-standing research area in natural language processing. It can offer rich insights into human sentiments and opinions and has thus seen considerable interest from both academia and industry. With the advent of large language models (LLMs) such as ChatGPT, there is a great potential for their employment on SA problems. However, the extent to which existing LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring deeper understanding or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs' SA abilities and propose a novel benchmark, \\textsc{SentiEval}, for a more comprehensive and realistic evaluation. Data and code during our investigations are available at \\url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.15005"}, "doi_lower": "10.48550/arxiv.2305.15005"}
{"paper_id": 260438863, "title": "Wider and Deeper LLM Networks are Fairer LLM Evaluators", "author_names": ["Xinghua Zhang", "Yu Bowen", "Haiyang Yu", "Yangyu Lv", "Tingwen Liu", "Fei Huang", "Hongbo Xu", "Yongbin Li"], "venue": "arXiv.org", "abstract": "Measuring the quality of responses generated by LLMs is a challenging task, particularly when it comes to evaluating whether the response is aligned with human preference. A novel approach involves using the LLM itself to make evaluation and stabilizing the results through multiple independent evaluations, similar to a single-layer narrow LLM network. This network consists of a fixed number of neurons, with each neuron being the same LLM. In this paper, we draw upon the extensive research on deep neural networks to explore whether deeper and wider networks can lead to fairer evaluations. Specifically, inspired by the observation that different neurons in a neural network are responsible for detecting different concepts, we first adaptively generate as many neuron roles as possible for each evaluation sample. Each perspective corresponds to the role of a specific LLM neuron in the first layer. In subsequent layers, we follow the idea that higher layers in deep networks are responsible for more comprehensive features, each layer receives representations from all neurons in the previous layer, integrating the locally learned evaluation information to obtain a more comprehensive evaluation result. Interestingly, this network design resembles the process of academic paper reviewing. To validate the effectiveness of our method, we construct the largest and most diverse English evaluation benchmark LLMEval$^2$ for LLM evaluators, comprising 15 tasks, 8 abilities, and 2,553 samples. Experimental results demonstrate that a wider network (involving many reviewers) with 2 layers (one round of discussion) performs the best, improving kappa correlation coefficient from 0.28 to 0.34. We also leverage WideDeep to aid in the assessment of Chinese LLMs, which has accelerated the evaluation time by 4.6 times, resulting in a 60% cost saving. WideDeep achieves a remarkable 93% agreement level among humans.", "year": 2023, "publicationdate": "2023-08-03", "externalids": {"DOI": "10.48550/arXiv.2308.01862"}, "doi_lower": "10.48550/arxiv.2308.01862"}
{"paper_id": 261706197, "title": "SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions", "author_names": ["Zhexin Zhang", "Leqi Lei", "Lindong Wu", "Rui Sun", "Yongkang Huang", "Chong Long", "Xiao Liu", "Xuanyu Lei", "Jie Tang", "Minlie Huang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "With the rapid development of Large Language Models (LLMs), increasing attention has been paid to their safety concerns. Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs. Nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of LLMs. In this work, we present SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages. Our extensive tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot settings reveal a substantial performance advantage for GPT-4 over its counterparts, and there is still significant room for improving the safety of current LLMs. We also demonstrate that the measured safety understanding abilities in SafetyBench are correlated with safety generation abilities. Data and evaluation guidelines are available at \\url{https://github.com/thu-coai/SafetyBench}{https://github.com/thu-coai/SafetyBench}. Submission entrance and leaderboard are available at \\url{https://llmbench.ai/safety}{https://llmbench.ai/safety}.", "year": 2023, "publicationdate": "2023-09-13", "externalids": {"DOI": "10.48550/arXiv.2309.07045"}, "doi_lower": "10.48550/arxiv.2309.07045"}
{"paper_id": 261823391, "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning", "author_names": ["Haozhe Zhao", "Zefan Cai", "Shuzheng Si", "Xiaojian Ma", "Kaikai An", "Liang Chen", "Zixuan Liu", "Sheng Wang", "Wenjuan Han", "Baobao Chang"], "venue": "International Conference on Learning Representations", "abstract": "Since the resurgence of deep learning, vision-language models (VLMs) enhanced by large language models (LLMs) have grown exponentially in popularity. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks. In this paper, we address the limitation above by 1) introducing vision-language Model with Multi-Modal In-Context Learning(MMICL), a new approach to allow the VLM to deal with multi-modal inputs efficiently; 2) proposing a novel context scheme to augment the in-context learning ability of the VLM; 3) constructing the Multi-modal In-Context Learning (MIC) dataset, designed to enhance the VLM's ability to understand complex multi-modal prompts. Our experiments confirm that MMICL achieves new state-of-the-art zero-shot performance on a wide range of general vision-language tasks, especially for complex benchmarks, including MME and MMBench. Our analysis demonstrates that MMICL effectively tackles the challenge of complex multi-modal prompt understanding and emerges the impressive ICL ability. Furthermore, we observe that MMICL successfully alleviates language bias in VLMs, a common issue for VLMs that often leads to hallucination when faced with extensive textual context. Our code, dataset, dataset tool, and model are available at https://github.com/PKUnlp-icler/MIC", "year": 2023, "publicationdate": "2023-09-14", "externalids": {"DOI": "10.48550/arXiv.2309.07915"}, "doi_lower": "10.48550/arxiv.2309.07915"}
{"paper_id": 258823380, "title": "CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models", "author_names": ["Jiaxu Zhao", "Meng Fang", "Zijing Shi", "Yitong Li", "Ling Chen", "Mykola Pechenizkiy"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "redWarning: This paper contains content that may be offensive or upsetting.Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. However, there are still limited bias categories in current research, and most of them only focus on English. In this paper, we introduce a new Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese conversational language models.Apart from those previous well-explored bias categories, CHBias includes under-explored bias categories, such as ageism and appearance biases, which received less attention. We evaluate two popular pretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias. Furthermore, to mitigate different biases, we apply several debiasing methods to the Chinese pretrained models. Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases, and debiasing methods using the proposed dataset can make response generation less biased while preserving the models’ conversational capabilities.", "year": 2023, "publicationdate": "2023-05-18", "externalids": {"DOI": "10.48550/arXiv.2305.11262"}, "doi_lower": "10.48550/arxiv.2305.11262"}
{"paper_id": 257900969, "title": "A Survey of Large Language Models", "author_names": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Z. Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "J. Nie", "Ji-rong Wen"], "venue": "arXiv.org", "abstract": "Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.", "year": 2023, "publicationdate": "2023-03-31", "externalids": {}, "doi_lower": null}
{"paper_id": 262084217, "title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset", "author_names": ["Lianmin Zheng", "Wei-Lin Chiang", "Ying Sheng", "Tianle Li", "Siyuan Zhuang", "Zhanghao Wu", "Yonghao Zhuang", "Zhuohan Li", "Zi Lin", "Eric P. Xing", "Joseph E. Gonzalez", "Ion Stoica", "Haotong Zhang"], "venue": "International Conference on Learning Representations", "abstract": "Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.", "year": 2023, "publicationdate": "2023-09-21", "externalids": {"DOI": "10.48550/arXiv.2309.11998"}, "doi_lower": "10.48550/arxiv.2309.11998"}
{"paper_id": 259129398, "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena", "author_names": ["Lianmin Zheng", "Wei-Lin Chiang", "Ying Sheng", "Siyuan Zhuang", "Zhanghao Wu", "Yonghao Zhuang", "Zi Lin", "Zhuohan Li", "Dacheng Li", "E. Xing", "Haotong Zhang", "Joseph E. Gonzalez", "Ion Stoica"], "venue": "Neural Information Processing Systems", "abstract": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.", "year": 2023, "publicationdate": "2023-06-09", "externalids": {}, "doi_lower": null}
{"paper_id": 252873117, "title": "Towards a Unified Multi-Dimensional Evaluator for Text Generation", "author_names": ["Ming Zhong", "Yang Liu", "Da Yin", "Yuning Mao", "Yizhu Jiao", "Peng Liu", "Chenguang Zhu", "Heng Ji", "Jiawei Han"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Multi-dimensional evaluation is the dominant paradigm for human evaluation in Natural Language Generation (NLG), i.e., evaluating the generated text from multiple explainable dimensions, such as coherence and fluency. However, automatic evaluation in NLG is still dominated by similarity-based metrics, and we lack a reliable framework for a more comprehensive evaluation of advanced models. In this paper, we propose a unified multi-dimensional evaluator UniEval for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task, and by guiding the model with different questions, we can use one evaluator to evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean QA format, we are able to introduce an intermediate learning phase that enables UniEval to incorporate external knowledge from multiple related tasks and gain further improvement. Experiments on three typical NLG tasks show that UniEval correlates substantially better with human judgments than existing metrics. Specifically, compared to the top-performing unified evaluators, UniEval achieves a 23% higher correlation on text summarization, and over 43% on dialogue response generation. Also, UniEval demonstrates a strong zero-shot learning ability for unseen evaluation dimensions and tasks. Source code, data, and all pre-trained evaluators are available at https://github.com/maszhongming/UniEval.", "year": 2022, "publicationdate": "2022-10-13", "externalids": {"DOI": "10.18653/v1/2022.emnlp-main.131"}, "doi_lower": "10.18653/v1/2022.emnlp-main.131"}
{"paper_id": 258108259, "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models", "author_names": ["Wanjun Zhong", "Ruixiang Cui", "Yiduo Guo", "Yaobo Liang", "Shuai Lu", "Yanlin Wang", "A. Saied", "Weizhu Chen", "Nan Duan"], "venue": "NAACL-HLT", "abstract": "Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal these models' strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models' performance in real-world scenarios. The data, code, and all model outputs are released in https://github.com/ruixiangcui/AGIEval.", "year": 2023, "publicationdate": "2023-04-13", "externalids": {"DOI": "10.48550/arXiv.2304.06364"}, "doi_lower": "10.48550/arxiv.2304.06364"}
{"paper_id": 253265328, "title": "Large Language Models Are Human-Level Prompt Engineers", "author_names": ["Yongchao Zhou", "Andrei Ioan Muresanu", "Ziwen Han", "Keiran Paster", "Silviu Pitis", "Harris Chan", "Jimmy Ba"], "venue": "International Conference on Learning Representations", "abstract": "By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the\"program,\"optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.", "year": 2022, "publicationdate": "2022-11-03", "externalids": {"DOI": "10.48550/arXiv.2211.01910"}, "doi_lower": "10.48550/arxiv.2211.01910"}
{"paper_id": 271953982, "title": "Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective", "author_names": ["Zhuang Yan", "Qi Liu", "Yuting Ning", "Weizhe Huang", "Rui Lv", "Zhenya Huang", "Guanhao Zhao", "Zheng Zhang", "Qingyang Mao", "Shijin Wang", "Enhong Chen"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.10512"}, "doi_lower": "10.48550/arxiv.2306.10512"}
{"paper_id": 256390238, "title": "Exploring AI Ethics of ChatGPT: A Diagnostic Analysis", "author_names": ["Terry Yue Zhuo", "Yujin Huang", "Chunyang Chen", "Zhenchang Xing"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2301.12867"}, "doi_lower": "10.48550/arxiv.2301.12867"}
{"paper_id": 256389762, "title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex", "author_names": ["Terry Yue Zhuo", "Zhuang Li", "Yujin Huang", "Yuan-Fang Li", "Weiqing Wang", "Gholamreza Haffari", "Fatemeh Shiri"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Semantic parsing is a technique aimed at constructing a structured representation of the meaning of a natural-language question. Recent advances in language models trained on code have shown superior performance in generating these representations compared to language models trained solely on natural language text. The existing fine-tuned neural semantic parsers are vulnerable to adversarial attacks on natural-language inputs. While it has been established that the robustness of smaller semantic parsers can be enhanced through adversarial training, this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data. This paper presents the first empirical study on the adversarial robustness of a prompt-based semantic parser based on CODEX, a stateof-the-art (SOTA) language model trained on code. Our results demonstrate that the large language model of code is vulnerable to carefully crafted adversarial examples. To overcome this challenge, we propose methods for enhancing robustness without requiring substantial amounts of labelled data or intensive computational resources.", "year": 2023, "publicationdate": "2023-01-30", "externalids": {"DOI": "10.48550/arXiv.2301.12868"}, "doi_lower": "10.48550/arxiv.2301.12868"}
{"paper_id": 202660943, "title": "Fine-Tuning Language Models from Human Preferences", "author_names": ["Daniel M. Ziegler", "Nisan Stiennon", "Jeff Wu", "Tom B. Brown", "Alec Radford", "Dario Amodei", "Paul Christiano", "G. Irving"], "venue": "arXiv.org", "abstract": "Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.", "year": 2019, "publicationdate": "2019-09-18", "externalids": {}, "doi_lower": null}
