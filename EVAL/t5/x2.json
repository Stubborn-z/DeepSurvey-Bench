{
  "survey": "A Survey on Evaluation of Large Language Models (LLMs) in Natural Language Processing (NLP) explores the methodologies and metrics for assessing LLM performance, emphasizing the development of standardized benchmarks to evaluate their effectiveness and reliability across diverse linguistic tasks. LLMs have revolutionized NLP by enhancing capabilities in translating complex information, commonsense knowledge representation, and cultural adaptability. Despite advancements, challenges remain in evaluating LLMs, particularly in assessing generative outputs, biases, and ethical considerations. The survey highlights the need for comprehensive evaluation frameworks to address these challenges, facilitating the cost-effective deployment of LLMs in various applications. Innovative benchmarking approaches, such as UniEval and BeaverTails, offer nuanced assessments of text quality and ethical dimensions, while advancements in training and fine-tuning processes enhance model adaptability. The survey underscores the importance of aligning AI with human values and expanding benchmarks and datasets to improve LLM evaluations. As LLM technology evolves, robust evaluation frameworks are critical for driving innovation and ensuring ethical deployment in real-world scenarios.\n\nIntroduction Significance of Large Language Models in NLP Large Language Models (LLMs) play a crucial role in advancing Natural Language Processing (NLP), transforming applications across various domains. Their capabilities in translating complex information, such as simplifying intricate radiology reports for improved patient understanding, underscore their significance in enhancing communication between healthcare providers and patients [1]. Models like ChatGPT exhibit proficiency in commonsense knowledge representation, a vital aspect for enriching conversational AI systems [2]. ChatGPT's adaptability to diverse cultural contexts further emphasizes its importance in fostering nuanced interactions within NLP applications [3]. The rise of Large Vision-Language Models (LVLMs) in multimodal learning necessitates comprehensive evaluation approaches, broadening the horizons of NLP technologies [4]. Moreover, integrating human preferences into reinforcement learning systems marks a pivotal shift toward effectively communicating complex goals [5]. BERT's innovative pre-training of deep bidirectional representations from unlabeled text has set a benchmark for fine-tuning with minimal task-specific adjustments, illustrating LLMs' transformative impact on NLP methodologies [6]. However, challenges persist in evaluating LLMs against benchmark academic datasets, particularly in assessing generative outputs relative to ground truth [7]. Additionally, the investigation of political biases within conversational AI, such as ChatGPT, reveals LLMs' influence on decision-making in democratic contexts [8]. The alignment of moral preferences with prompted political ideologies in LLMs highlights the need for ethical considerations in model development [9]. The introduction of GLM-130B, an open-source bilingual model, addresses the demand for alternatives to proprietary models like GPT-3, fostering accessibility and innovation [10]. As LLMs evolve, their role in NLP becomes increasingly critical, driving advancements in language processing technologies. The development of LaMDA responds to the growing need for advanced dialog systems prioritizing safety and factual accuracy [11]. Furthermore, the survey evaluates LLMs' empathy by examining their responses in emotionally charged situations [12]. The exploration of GPT-4 as a precursor to artificial general intelligence (AGI) further underscores LLMs' transformative potential in NLP [13]. Need for Evaluation of LLMs Evaluating Large Language Models (LLMs) is essential for assessing their performance and reliability across various applications, including Natural Language Generation (NLG), which traditionally relies on similarity-based metrics [14]. This evaluation is crucial for understanding biases within these models, which can significantly impact their outputs and societal implications [15]. Additionally, it is vital to discern whether LLMs' moral outputs genuinely reflect political ideologies or merely represent artifacts of their training data [9]. Benchmarks for bilingual language models, such as GLM-130B, underscore the necessity for frameworks that effectively assess performance in multilingual contexts, particularly in English and Chinese [10]. Evaluating models like LaMDA is critical to ensure alignment with human values and factual accuracy, essential for responsible AI deployment [11]. The limited literature on assessing GPT models emphasizes the need for reliable evaluation in real-world scenarios [16]. Furthermore, distinguishing helpful from harmful responses is fundamental in LLM evaluation, ensuring that models contribute positively to decision-making processes [17]. The evaluation of GPT-4's capabilities and its implications for AGI development highlights LLMs' transformative potential in NLP [13]. Understanding LLMs' empathy is crucial for assessing their comprehension of human emotional behaviors, particularly in conversational AI applications [12]. Comprehensive evaluation frameworks are vital for ensuring the cost-effective and adaptable deployment of LLMs across various applications, addressing the multifaceted challenges inherent in language model assessment [18]. Challenges in Establishing Standardized Benchmarks Table provides a detailed comparison of existing benchmarks aimed at evaluating the performance of Large Language Models (LLMs) across distinct domains, illustrating the diversity in linguistic challenges and evaluation criteria. Establishing standardized benchmarks for evaluating Large Language Models (LLMs) presents numerous challenges, complicating comprehensive assessments across various linguistic and functional domains. A significant issue is the inadequacy of current benchmarks in testing LLMs' ability to handle rapidly evolving knowledge and queries based on false premises, limiting their applicability in dynamic contexts [19]. Additionally, intrinsic limitations of AI systems in grasping context and the subtleties of human expression complicate the fine-tuning of models for specific outputs, thereby affecting evaluation accuracy [20]. Assessing LLMs' capabilities in generating plans and understanding best practices exemplifies challenges in benchmark development [21]. Limited access to model weights and infrastructure details hinders researchers' ability to study and replicate LLMs, essential for validating performance and reliability [22]. Existing benchmarks also face infrastructural and methodological limitations in evaluating large models, undermining the accuracy of performance assessments [23]. Current benchmarks frequently fail to capture qualitative improvements and novel capabilities of larger language models, leading to evaluation gaps that do not reflect the models' full potential [24]. The breadth of knowledge required to assess modern LLMs is inadequately covered, resulting in incomplete evaluations across various fields [25]. Furthermore, benchmarks do not specifically address unique requirements of certain language tasks, such as those in Arabic, limiting LLM effectiveness in these contexts [26]. The potential for misinterpretation of outputs, especially in politically sensitive areas, presents additional challenges, as users may develop skewed perceptions based on LLM-generated content [8]. Bilingual models face unique training challenges, such as loss spikes and divergence, inadequately addressed by existing benchmarks [10]. These multifaceted challenges highlight the need for robust and adaptable benchmarking frameworks that can accurately evaluate LLMs across diverse applications and linguistic contexts. Recent research emphasizes the importance of understanding current models like GPT-4's limitations and the necessity for new paradigms beyond traditional next-word prediction [13]. Structure of the Survey This survey is meticulously structured to provide an in-depth examination of the evaluation methodologies and benchmarks applied to Large Language Models (LLMs). It begins with an Introduction that delineates the significance of LLMs in Natural Language Processing (NLP), emphasizing their transformative impact and the necessity of evaluation for efficacy and reliability. This section also addresses challenges in establishing standardized benchmarks, setting the stage for a detailed exploration. Following the introduction, the survey delves into the Background and Core Concepts, offering an overview of LLMs, NLP, and AI model assessment while defining key terms essential for understanding language model benchmarking. The discussion progresses to \"Methodologies for Evaluating Large Language Models,\" where a comprehensive examination of evaluation strategies is undertaken, including qualitative and quantitative approaches. This section focuses on task-specific frameworks, innovative techniques, ethical considerations, and domain-specific evaluations. It encompasses a holistic view, as demonstrated by initiatives like HELM, which standardizes metrics such as accuracy, calibration, robustness, fairness, and bias across diverse scenarios. It also addresses systematic biases in LLM evaluations, proposing calibration frameworks to align machine assessments with human judgments. The evaluation of domain-specific knowledge, such as in medical applications through benchmarks like MultiMedQA, highlights the need for human evaluation alongside automated methods to identify gaps in factuality, comprehension, and reasoning. Additionally, information theory-based measurements provide new insights into the factual knowledge stored within LLMs, offering methods to measure and modify this knowledge effectively [27,28,29,30]. The survey then transitions to examining the Challenges in Language Model Benchmarking, focusing on the limitations of existing benchmarks, such as testing leakage and evaluation automation issues, as well as challenges posed by data availability and quality highlighted in the KoLA benchmark. The dynamic nature of language complicates reliable evaluation, necessitating diverse and evolving datasets, as seen in initiatives like BIG-bench and LAraBench, which address gaps in measuring model performance across various languages and tasks. These initiatives emphasize the importance of scalable evaluation frameworks like Language-Model-as-an-Examiner for comprehensive and equitable assessments [24,26,31,32]. Advancements in Standardized Benchmarks are subsequently explored, highlighting innovative approaches and diverse task coverage that address emergent properties and interdisciplinary research. The next section, Case Studies and Comparative Analysis, presents empirical evaluations of LLMs across various benchmarks and tasks, including code generation, multilingual capabilities, and commonsense planning. The survey concludes with Future Directions in LLM Evaluation, proposing expansions of benchmarks and datasets, refinements of evaluation metrics, integration of emerging technologies, and considerations of ethical and social implications.The following sections are organized as shown in . Background and Core Concepts Overview of Large Language Models Large Language Models (LLMs) represent a significant advancement in natural language processing, characterized by sophisticated architectures that enable complex linguistic tasks with high precision and flexibility. The Pathways Language Model (PaLM) exemplifies this progress, utilizing advanced architectures for few-shot learning, akin to Transformer-based models like BERT, which leverage deep bidirectional contexts for nuanced understanding across applications such as translation and conversational AI [33,6]. LLMs demonstrate versatility in domain-specific applications, necessitating large-scale, cross-dataset evaluations. Domain-specific benchmarks, such as C-Eval, assess knowledge and reasoning capabilities in the Chinese language context [34]. LLMs like ChatGPT exhibit zero-shot and few-shot learning proficiency, effectively handling biomedical tasks and general NLP functions without specialized training. Challenges persist in structured reasoning domains like mathematics and programming. MetaMath evaluates LLMs' mathematical problem-solving abilities, highlighting complexities in logical transformations [35]. While LLMs show potential in generating Python code from natural language, existing methodologies often fall short of capturing their full knowledge scope. Research in academic contexts, such as MIT's Mathematics and EECS curriculum, underscores the need for robust metrics in educational content evaluation [36]. Benchmarks like Dynatask facilitate custom NLP task creation, aiding researchers in assessing model adaptability and performance in novel problem spaces [37]. LLMs, including those evaluated in the MultiMedQA benchmark, are transforming healthcare by enabling precise medical question answering [30]. As these models evolve, they reshape the NLP landscape, driving innovation and paving the way for more intelligent systems. The CUAD dataset, with extensive annotations for legal contract review, highlights LLMs' diverse applications in specialized fields [38]. Benchmarks addressing public affairs document classification underscore the complexity LLMs navigate in processing intricate language [39]. The adversarial robustness of large vision-language models, such as GPT-4, in multimodal generation tasks reflects advancements in LLM architectures [40]. LaMDA, designed for dialog generation with a Transformer-based architecture, showcases expanding capabilities in conversational contexts [11]. Evaluations in emotional contexts demonstrate adaptability across different model sizes and types [12]. The survey analyzes GPT-4's performance across fields like mathematics, coding, vision, medicine, law, and psychology, illustrating the broad spectrum of LLM applications [13]. Defining Key Terms in Model Evaluation Understanding key terms is essential for evaluating Large Language Models (LLMs) and interpreting benchmarks and methodologies. 'Mathematical problem solving' is assessed through benchmarks like MATH, which includes numerous problems with detailed solutions, providing a framework for evaluating logical reasoning [41]. 'Multilingual evaluation tasks' and 'zero-shot learning setting' are crucial for assessing LLMs' ability to generalize across languages and tasks without prior exposure, highlighting their adaptability [42]. 'Social language understanding' involves LLMs' proficiency in recognizing and generating language related to humor, sarcasm, sentiment, and trustworthiness, as demonstrated by specialized benchmarks [43]. 'Emotional intelligence' benchmarks evaluate LLMs' ability to recognize and interpret emotions, vital for effective communication [44]. 'Emergent abilities' refer to the unpredictable emergence of capabilities within LLMs that cannot be anticipated based on smaller models, posing challenges for comprehensive evaluation [45]. The AlpacaFarm benchmark focuses on 'instruction-following,' simulating human feedback to enhance understanding and model behavior [46]. 'Generalization across domains' is evaluated by the GLUE benchmark, which assesses models on various natural language understanding tasks, emphasizing consistent performance [47]. The need for 'deep in-domain knowledge' is highlighted by benchmarks requiring rigorous evaluation beyond surface-level reasoning [48]. 'Automatic metrics for natural language generation' are defined by benchmarks evaluating new metrics independent of specific systems and datasets, aiming for objective assessment of LLM outputs [49]. The CUAD benchmark emphasizes 'salient portion identification' within legal contracts, addressing the challenge of pinpointing sections needing human review, enhancing LLM evaluations in legal contexts [38]. Benchmarks assessing ChatGPT's effectiveness in software engineering tasks, such as ambiguity resolution and code review, underscore the importance of domain-specific evaluations [50]. These terms collectively form the foundation for understanding the diverse evaluation criteria and benchmarks applied to LLMs, facilitating comprehensive assessments of their capabilities across various applications. Methodologies for Evaluating Large Language Models Evaluating Large Language Models (LLMs) requires a comprehensive approach that includes both qualitative and quantitative methodologies. This section delves into qualitative evaluation methods, which provide nuanced insights into model performance through expert reviews, user studies, and innovative frameworks. These evaluations are crucial for understanding how LLMs interpret complex information and engage users, influencing their applicability in real-world scenarios. Table presents a comprehensive comparison of different evaluation methods for large language models, illustrating the diverse approaches employed in qualitative and quantitative assessments, as well as task-specific evaluations. As illustrated in , the structured overview of methodologies for evaluating LLMs categorizes these approaches into qualitative and quantitative frameworks, task-specific assessments, novel techniques, as well as ethical and bias evaluations. Each category is further divided into subcategories, detailing specific methods and frameworks employed in the evaluation process. This comprehensive categorization not only emphasizes the importance of qualitative evaluation methodologies but also guides future advancements in the field. The following subsection will detail various qualitative evaluation methodologies, highlighting their significance in assessing LLM capabilities. Qualitative Evaluation Approaches Qualitative evaluation approaches for LLMs employ diverse methodologies to offer in-depth insights into model performance through expert reviews, user studies, and innovative frameworks. A central focus is on LLMs' ability to render complex information into accessible formats, as demonstrated by studies on ChatGPT and GPT-4's translation of radiology reports into plain language, with expert evaluations of clarity and accuracy [1]. The Iterative Reading-then-Reasoning (IRR) method exemplifies qualitative approaches, enabling LLMs to extract relevant evidence from structured data and perform reasoning based on that evidence, underscoring the significance of qualitative assessments in understanding reasoning capabilities [51]. TruthfulQA evaluates LLMs' capacity to generate truthful responses, emphasizing the avoidance of falsehoods, providing a qualitative measure of truthfulness in outputs [52]. User interaction and feedback are integral to qualitative evaluations, as static assessments may not capture LLMs' dynamic capabilities. The DELI framework introduces a structured approach to deliberating reasoning steps using tool interfaces, enhancing solution accuracy and exemplifying qualitative methods' role in refining reasoning processes [53]. Employing human preferences in training reinforcement learning agents highlights qualitative feedback's significance in enhancing model adaptability and performance [5]. Evaluating political biases in LLMs, such as those identified in ChatGPT, involves pre-registered experiments with diverse political statements, ensuring valid assessments of ideological biases [8]. The Dynatask framework, which automatically generates web interfaces and hosting infrastructure from simple task configuration files, further illustrates the integration of user feedback in qualitative evaluations [37]. These qualitative approaches contribute to understanding LLM capabilities by examining performance in specialized domains, such as legal analysis and public affairs, assessing factual knowledge and emotional intelligence, and exploring autonomous reasoning abilities. This facilitates nuanced assessments extending beyond traditional quantitative metrics, offering insights into impacts on legal services efficiency, public document analysis, AI governance, and social interactions [54,28,39,44]. As illustrated in , the qualitative evaluation approaches for large language models (LLMs) highlight key methodologies, focus areas, and the role of user interaction. The figure emphasizes the importance of expert reviews, user studies, and innovative frameworks in assessing translation clarity, reasoning capabilities, and truthfulness. Additionally, it underscores the integration of user feedback, evaluation of biases, and reinforcement learning in refining LLM performance. By incorporating diverse evaluation contexts and user interactions, qualitative methods yield valuable insights into the complex behaviors and potential applications of LLMs. Quantitative Evaluation Metrics Quantitative evaluation metrics are essential for systematically assessing LLM performance across diverse natural language processing tasks. Metrics such as Accuracy and F1-score provide a comprehensive view of a model's ability to generate precise and contextually relevant responses, particularly in evaluating ChatGPT's performance across various NLP tasks [7]. The F1-score, balancing precision and recall, is vital in multi-label classification tasks where identifying key portions in complex datasets is essential. In the medical field, Accuracy and F1-score reflect model effectiveness in generating reliable evaluations, highlighting their importance in contexts where precision is critical [55]. In software engineering tasks, these metrics assess the correctness and relevance of LLM responses, ensuring effective contributions to technical domains [50]. Evaluating models like LaMDA incorporates metrics such as Safety and Groundedness, measuring alignment of responses with human values and factual accuracy, broadening quantitative assessments [11]. Beyond traditional metrics, evaluating hallucination in language models presents challenges, necessitating effective measures to accurately assess performance [56]. In dynamic settings, Description Accuracy and Strategic Engagement reflect LLMs' ability to describe concepts effectively and engage strategically [18]. DELI's performance is evaluated through experimental results measuring accuracy and improvement over competitive baselines, underscoring quantitative metrics' role in refining capabilities [53]. Standard benchmarking protocols, as applied in evaluating models like GLM-130B, ensure reliability and validity across tasks, emphasizing robust evaluation standards [10]. By integrating diverse quantitative metrics, researchers are equipped with comprehensive tools to evaluate LLMs, facilitating continuous enhancement of language model technologies and adaptation to complex linguistic challenges. Task-Specific Evaluation Frameworks Task-specific evaluation frameworks are crucial for assessing LLM capabilities in specialized natural language processing tasks, allowing detailed understanding of performance across distinct domains. JEEBench introduces a novel set of problems from the IIT JEE-Advanced exam, emphasizing long-horizon reasoning and deep understanding, providing rigorous evaluation of mathematical reasoning abilities [48]. MetaMath addresses the challenge of mathematical reasoning, specifically evaluating LLMs' capability to solve complex problems accurately [35]. The EVEVAL framework offers structured evaluation of event semantics, encompassing various aspects often fragmented in existing benchmarks, providing comprehensive evaluation of understanding event-related language tasks [57]. In commonsense reasoning, a benchmark for ChatGPT assesses its ability to answer commonsense questions, emphasizing frameworks addressing cognitive aspects of language understanding [2]. KoLA provides structured evaluation of world knowledge capabilities, focusing on knowledge-related tasks to assess LLMs' ability to incorporate and utilize extensive factual information [31]. The LVLM-eHub benchmark evaluates multimodal capabilities of Large Vision-Language Models (LVLMs), including tasks like visual question answering and object hallucination, underscoring the necessity for frameworks integrating visual and linguistic data [4]. Benchmarks designed to evaluate abstract reasoning capabilities test LLMs' ability to identify and apply general patterns from limited data, providing insights into cognitive flexibility and generalization skills [58]. The multitask benchmark assesses reasoning capabilities, hallucination tendencies, and interactivity of ChatGPT compared to other LLMs, offering holistic view of performance across dimensions [59]. LaMDA's benchmark evaluates Transformer-based neural language models, focusing on generating safe and factually grounded responses, critical for responsible AI deployment [11]. Collectively, these frameworks contribute to understanding LLM capabilities, facilitating targeted assessments driving improvements in technologies across applications. By concentrating on specialized tasks, these benchmarks enable researchers to identify strengths and weaknesses, paving the way for more effective and contextually aware language processing systems. Novel Evaluation Techniques Innovative evaluation techniques for LLMs are essential for transcending traditional metrics, providing comprehensive methodologies enhancing assessment of capabilities across diverse contexts. LVLM-eHub introduces systematic framework for evaluating Large Vision-Language Models (LVLMs) across dimensions, including quantitative capabilities and user-level performance in open-world scenarios [4]. This approach facilitates holistic evaluation of multimodal capabilities, addressing integration of visual and linguistic data. Amultitask5's multimodal dataset represents advancement over previous benchmarks, offering novel evaluation of models on multiple tasks within multilingual context [59]. This framework facilitates assessment of adaptability and performance across languages and modalities, highlighting versatility in linguistic environments. UniEval introduces unified Boolean Question Answering format for multi-dimensional evaluation from single evaluator, incorporating external knowledge and demonstrating improved correlation with human judgments [14]. This innovative approach streamlines evaluation process, providing comprehensive insights into performance across dimensions, enhancing reliability and validity of assessments. SpyGame's interactive multi-agent framework combines linguistic skills and strategic thinking, offering novel method for evaluating interactive capabilities [18]. This framework challenges models to engage in strategic interactions, reflecting adaptability in dynamic scenarios. Through advanced methodologies, novel evaluation techniques extend boundaries of traditional metrics, enabling thorough assessments tailored to multifaceted capabilities of LLMs. These techniques enhance applicability and impact across domains by improving ability to process complex language in domain-specific documents, advancing evaluation of factual consistency in generative tasks. This drives innovation by enabling more accurate topic classification and robust multilingual evaluations, setting new standards for applications and practices [39,60]. Ethical and Bias Evaluation Assessing ethical considerations and bias in LLMs is crucial for ensuring outputs align with societal values and ethical standards. The BeaverTails dataset introduces metrics designed to evaluate safety alignment, focusing on measuring helpfulness and harmlessness in responses [17]. This dataset underscores importance of developing frameworks effectively assessing ethical dimensions of outputs. Trustworthiness in GPT models is examined through benchmark addressing vulnerabilities, including toxicity, bias, robustness, and privacy issues [16]. This comprehensive approach is vital for understanding multifaceted ethical challenges and developing strategies to mitigate vulnerabilities. Cultural bias remains significant concern, as LLMs exhibit preference for American culture, reducing cultural variance in responses [3]. This highlights necessity for methodologies capable of identifying and mitigating biases, ensuring culturally sensitive outputs. RealToxicityPrompts investigates relationship between persona assignment and toxicity levels, providing insights into how personas influence generation of toxic language [61]. Complementarily, BOLD introduces automated metrics evaluating social biases from multiple perspectives, facilitating multifaceted assessment of biases [62]. Alignment of outputs with moral judgments and ethical concepts is explored through benchmarks providing insights into capabilities in understanding and generating text aligned with human values [63]. Evaluation metrics, including Human Evaluation and Automatic Evaluation, focus on alignment with human values regarding safety and responsibility, ensuring ethically sound outputs [64]. BBQ benchmark addresses evaluating social biases' influence on outputs in question-answering tasks, particularly in scenarios with varying contextual information [65]. This approach is critical for understanding impact of biases on performance and developing strategies to mitigate biases. Research on hallucinations introduces taxonomies for categorizing hallucinations and evaluation benchmarks to assess mitigation strategies, emphasizing importance of understanding and addressing phenomenon in outputs [66]. BOSS introduces structured protocol for evaluating OOD robustness, focusing on performance amid distribution shifts, essential for ensuring reliability in contexts [67]. Evaluation of vision-language models in realistic adversarial settings is introduced by benchmark accounting for black-box attack scenarios, providing novel approach to assessing robustness and ethical considerations [40]. Identification and mitigation of biases in outputs are discussed, emphasizing importance of ethical considerations in development [15]. Alignment of outputs with Moral Foundations Theory is evaluated, addressing ethical considerations and potential biases, ensuring models accurately reflect human moral values [9]. Collectively, these approaches contribute to nuanced understanding of ethical and bias evaluation, ensuring development and deployment in alignment with societal values and ethical standards. Multi-Modal and Domain-Specific Evaluation Evaluating LLMs in multi-modal and domain-specific contexts is vital for understanding adaptability and performance across applications. Languageis48 benchmark contributes by offering structured methodology for assessing multimodal reasoning capabilities, crucial for advancing AI technologies [68]. This benchmark facilitates evaluation of ability to integrate and process information across modalities, such as text and images, highlighting versatility in handling complex data inputs. In domain-specific evaluations, frameworks like MultiMedQA focus on assessing proficiency in medical question answering, providing insights into ability to navigate specialized knowledge areas [30]. Similarly, CUAD dataset, with extensive annotations for legal contract review tasks, underscores importance of domain-specific benchmarks in evaluating capabilities in legal contexts [38]. These evaluations are critical for ensuring effective contributions to specialized fields, where precision and domain knowledge are paramount. Moreover, evaluating in multilingual contexts, as demonstrated by GLM-130B benchmark, highlights importance of assessing performance across languages, ensuring adaptability in linguistic environments [10]. Integration of multimodal data, as seen in LVLM-eHub benchmark, expands scope of evaluations, allowing comprehensive assessment of ability to process and generate outputs across data types [4]. Through multi-modal and domain-specific evaluation methods, researchers gain deeper understanding of capabilities, facilitating development of models attuned to complexities of real-world applications. These evaluations play crucial role in advancing technologies by providing comprehensive assessments across scenarios and metrics, such as topic classification in public affairs, holistic evaluation of transparency, and event semantic processing. This enhances applicability in domains, ensuring effective deployment in specialized contexts like public policy analysis and event semantics understanding [39,57,29]. Challenges in Language Model Benchmarking Limitations of Existing Benchmarks Current benchmarking methods for Large Language Models (LLMs) face significant challenges, resulting in incomplete assessments of model capabilities across applications. High costs and time constraints limit the adaptability and scalability of these benchmarks [18]. They often fail to distinguish between helpfulness and harmlessness, complicating the evaluation of model safety [17]. Moreover, benchmarks inadequately address trustworthiness, leaving gaps in understanding vulnerabilities and risks [16]. In software engineering, benchmarks do not effectively assess models like ChatGPT, restricting insights into technical capabilities [50]. Additionally, English-centric medical benchmarks may not reflect healthcare nuances in diverse contexts, such as China [55]. Model alignment with human values is another area where benchmarks fall short, posing risks in dialog applications due to insufficient measurement [11]. Research struggles with unseen domain complexity, impeding model robustness and applicability [69]. Many math reasoning datasets lack tool use and intermediate reasoning annotations, hindering comprehensive LLM evaluations [53]. Misalignments between LLM responses and human emotional behaviors, alongside limited recognition of similar emotional situations, present further challenges [12]. Despite GLM-130B's superior performance across benchmarks compared to GPT-3, existing methodologies lack frameworks to capture the full range of capabilities [10]. As illustrated in , the limitations of existing benchmarks for large language models can be categorized into assessment challenges, domain-specific issues, and evaluation limitations. Each category highlights specific shortcomings such as incomplete assessments, domain-specific inadequacies, and gaps in evaluation methods. These limitations underscore the urgent need for robust benchmarking frameworks to accurately evaluate LLMs across diverse applications. Data Availability and Quality Data availability and quality are pivotal for accurate LLM evaluations. Eliminating discrimination while maintaining prediction accuracy is closely tied to data quality [70]. The predominance of English-centric benchmarks limits insights into generative models' performance across other languages, reducing evaluation comprehensiveness [71]. Despite AI-generated text advancements, models often produce formulaic responses and struggle with contextually rich content, exposing deficiencies in training and evaluation data [20]. Accurately identifying hallucinations complicates the evaluation process, necessitating sophisticated metrics [56]. Datasets for adversarial examples aim to challenge models and reveal incorrect predictions [72]. However, capturing variability in unseen domains remains challenging, as models often rely on identical training and testing distributions, limiting generalization [69]. Standardized benchmarks and new evaluation metrics are needed to assess model performance under out-of-distribution conditions [73]. Improved data quality and availability are crucial for accurate LLM assessments across diverse linguistic and contextual scenarios. Enhanced data quality ensures effective processing of complex domain-specific documents, promoting transparency and accountability. TrueTeacher's synthetic data generation illustrates potential for LLMs in multilingual and factual consistency tasks. In legal contexts, LLMs show promise in applying tax law, emphasizing relevant legal context and few-shot prompting. Innovative benchmarking approaches like KoLA highlight the need for careful design in evaluating world knowledge, ensuring effective management of unseen data and evolving knowledge. Addressing these challenges is vital for leveraging LLMs across fields, contributing to informed decision-making and AI governance [39,60,54,28,31]. Dynamic Nature of Language and Emergent Abilities The dynamic nature of language and emergent abilities of LLMs present significant benchmarking challenges, as these models exhibit unpredictable performance improvements that defy linear extrapolation of smaller model capabilities [45]. As model size increases, emergent abilities become more pronounced, resulting in unexpected enhancements in language processing tasks that challenge traditional metrics [74]. This unpredictability necessitates adaptive evaluation frameworks to accommodate evolving LLM capabilities. SuperGLUE provides a more rigorous framework for language understanding systems, addressing complexities associated with language's dynamic nature [75]. However, the variability in language and emergent properties of LLMs require continuous innovation in benchmarking to ensure accurate assessments. Emergent abilities underscore the need for benchmarks that dynamically adjust to expanding LLM capabilities, capturing nuanced improvements as models grow in complexity. Self-assessment tests in LLMs have proven unreliable for measuring personality traits, failing to account for inherent biases and situational factors [76]. This limitation highlights the inadequacy of traditional methods in capturing the full spectrum of LLM behavior, emphasizing the need for adaptive and comprehensive frameworks. As language evolves, developing innovative techniques to address LLMs' dynamic and emergent nature is crucial for advancing natural language processing. Advancements in Standardized Benchmarks Innovative Benchmarking Approaches Innovative benchmarking approaches are pivotal in refining the evaluation of Large Language Models (LLMs), addressing traditional metric shortcomings and enabling comprehensive assessments across diverse applications. Recent advancements have introduced benchmarks tailored to specific challenges, establishing new criteria for evaluating LLM capabilities. As illustrated in , these diverse innovative benchmarking approaches emphasize the problem-solving capabilities, safety evaluations, and applications of LLMs in specialized contexts. For instance, JEEBench rigorously assesses LLM problem-solving skills through challenging pre-engineering problems, revealing limitations in models like GPT-4 regarding algebraic manipulation and domain-specific knowledge retrieval. In legal contexts, LLMs show improved performance in tax law analysis through few-shot prompting, yet their expertise remains below expert levels. Additionally, LLMs adeptly classify public affairs documents, managing complex language and multi-label tasks, thus promoting transparency and accountability [54,48,39]. UniEval exemplifies an innovative benchmark by assessing generated text across multiple explainable dimensions, such as coherence and fluency, providing a multi-faceted framework for evaluating text quality [14]. The DEEP benchmark facilitates cost-effective evaluations of LLM intelligence across various linguistic scenarios [18]. BeaverTails distinguishes itself by separating annotations of helpfulness and harmlessness, allowing for nuanced safety evaluations in LLM outputs [17]. In software engineering, a specialized benchmark for ChatGPT targets tasks like ambiguity resolution and code review, addressing unique challenges in technical domains [50]. The DELI benchmark introduces iterative deliberation processes that refine reasoning steps through tool manipulation and natural language reasoning, enhancing LLM cognitive capabilities [53]. LaMDA sets new standards for dialog systems by incorporating fine-tuning methods with annotated data and external knowledge sources, positioning itself as a benchmark for conversational model evaluation [11]. EmotionBench advances the assessment of emotional intelligence in LLMs by providing a comprehensive dataset and evaluation framework for empathy [12]. The trustworthiness assessment benchmark expands traditional evaluations to encompass vulnerabilities like toxicity, bias, robustness, and privacy, offering a holistic view of GPT models [16]. These diverse methodologies yield deeper insights into LLM strengths and limitations, paving the way for future research and development in natural language processing. Such advancements enhance LLM applicability across various domains, improving public affairs document analysis and legal analysis capabilities, while ensuring ethical deployment through domain-specific processing and logical reasoning [39,54]. Diverse Task Coverage Diverse task coverage is essential for evaluating Large Language Models (LLMs) across a wide range of applications and capabilities. Recent advancements have led to comprehensive datasets that challenge models and provide insights into their performance across various domains. For instance, the dataset by Widerandde161 contains 2,553 samples across 15 tasks and 8 abilities, establishing itself as the largest and most diverse benchmark for LLMs [77]. This extensive coverage facilitates thorough assessments of model capabilities, enabling nuanced understanding of strengths and weaknesses. The dataset described by Beyondthei115 includes 204 tasks spanning numerous topics, designed to test language models and push the limits of their linguistic and cognitive abilities [24]. This diversity allows researchers to explore LLM adaptability and generalization, ensuring evaluations encompass a broad set of criteria. In vision models, the AdaptiveTe28 dataset features images representing coherent groups defined by user input, allowing targeted testing of vision models and enhancing multimodal evaluation [78]. Integrating diverse modalities in benchmarking is vital for assessing models' capabilities across different data types. The survey by Generalizi130 covers various aspects of domain generalization, including definitions, theories, algorithms, datasets, and applications, underscoring the need for evaluating LLMs across diverse domains to ensure robustness in real-world scenarios [69]. By incorporating a wide range of tasks, these benchmarking efforts provide a holistic view of LLM capabilities, driving innovation in language model technologies and ensuring effective deployment across diverse applications. Challenging models with complex tasks reveals insights into their performance, highlighting abstract reasoning capabilities and reliance on narrow, non-transferable procedures. This understanding, informed by evaluations across counterfactual variants and a broad spectrum of 57 tasks, emphasizes the necessity for significant improvements before models can achieve expert-level accuracy in areas like morality and law [79,80]. Emergent Properties and Interdisciplinary Research Emergent properties in Large Language Models (LLMs) represent a transformative aspect of understanding model performance, particularly regarding model size and complexity. These properties suggest that LLMs can exhibit capabilities not linearly extrapolated from smaller models, indicating that further scaling may unveil additional functionalities [74]. This shift necessitates revisiting evaluation methods, as traditional benchmarks may not adequately capture these emergent abilities [45]. Recent advancements in benchmarking methodologies address these emergent properties by providing insights into LLM functioning and improvement with increased scale. Benchmarks like JEEBench and Language-Model-as-an-Examiner challenge models to demonstrate unexpected capabilities, such as solving complex pre-engineering problems and engaging in open-ended question answering. These benchmarks introduce scenarios requiring long-horizon reasoning and deep in-domain knowledge, while addressing common failure modes in models, including algebraic manipulation errors and difficulties in grounding abstract concepts. This evolution is crucial for understanding LLM potential and ensuring evaluations reflect their dynamic nature [48,32]. Interdisciplinary research integrates insights from fields such as public affairs, law, and neural network evaluation to enhance LLM evaluation and application. By leveraging these diverse perspectives, researchers can improve document classification, enhance legal analysis capabilities, and develop fairer evaluation methods for LLMs, ultimately contributing to greater transparency, accountability, and efficiency across sectors [39,54,77]. Integrating cognitive science, linguistics, and computer science perspectives allows for the development of benchmarks that better capture LLM capabilities, fostering innovation across multiple domains and facilitating broader application deployment in natural language processing and beyond. Case Studies and Comparative Analysis The exploration of Large Language Models (LLMs) reveals diverse applications, notably in code generation and human evaluation. This section examines LLMs using benchmarks like HumanEval to assess their code generation performance, focusing on models such as Codex and GPT-3. By analyzing their capabilities through specific tasks, we gain insights into their strengths, weaknesses, and the implications of their training methodologies, essential for understanding LLM performance in code generation contexts. Evaluation Using HumanEval and Code Generation Tasks Evaluating LLMs in code generation and human-centric tasks provides critical insights into their capabilities and limitations, highlighting their transformative potential in AI development. This analysis covers their proficiency in language comprehension, planning abilities, topic classification, programming education, and synthetic personality traits. While LLMs show advancements in understanding complex language structures and improving public affairs document analysis, challenges persist in autonomous planning, accurately identifying programming issues, and maintaining consistent personality traits [39,81,82,83,84]. The HumanEval benchmark highlights significant differences in code generation accuracy between Codex and models like GPT-3 and GPT-J, emphasizing the role of specialized training. Experiments with CODEGEN demonstrate its competitiveness in zero-shot Python code generation, indicating LLMs' potential to tackle complex programming tasks without prior exposure. Large-scale LLMs, including ChatGPT, GPT-4, and dedicated Chinese models, were assessed using the CMB dataset, revealing insights into their multilingual performance [55]. Comparative analysis indicates that while LLMs can respond to emotional situations, they often misalign with human emotional responses, underscoring the need for improved metrics capturing emotional intelligence and empathy [12]. Evaluations using the Chain-of-Thought Hub benchmark tested models like GPT-4, Claude-v1.3, PaLM-2, and LLaMA-65B on reasoning capabilities, showcasing diverse LLM applications beyond code generation. The CMMLU benchmark assessed 18 advanced multilingual LLMs, revealing struggles to exceed an average accuracy of 50 State-of-the-art models like InstructGPT, ChatGPT, and PerplexityAI underwent extensive evaluations, revealing limited creativity in joke generation, with over 90\\ Assessments of GPT-4 and GPT-3.5 in semantic analysis tasks demonstrated proficiency in evaluating noun-noun combinations, yet they performed poorly compared to humans in distinguishing meaningful phrases. This limitation is evident in the Two Word Test (TWT), which challenges models with tasks easily performed by humans without advanced training [85,16,86,79]. Experiments with generative LLMs like ChatGPT and GPT-4 against supervised ranking models using the NovelEval dataset revealed varying performance levels, indicating potential in sentiment analysis tasks. The LAMM benchmark further assessed performance improvements using the LAMM-Dataset, contributing to understanding LLM capabilities in code generation and human evaluation tasks. These case studies collectively provide a comprehensive overview of LLM performance, offering valuable insights into their strengths and areas for future development. Comparative Analysis of MLLMs Comparative analysis of multilingual Large Language Models (MLLMs) across various benchmarks provides significant insights into their capabilities and limitations in handling diverse linguistic tasks. The CMMLU benchmark highlights challenges faced by advanced multilingual and Chinese-oriented LLMs in achieving high accuracy, with many struggling to exceed an average of 50\\ The Xiezhi benchmark comprehensively evaluates 47 cutting-edge LLMs, focusing on their ability to answer multiple-choice questions across languages [25]. Results indicate that while some models excel in specific tasks, there is a general need for improvement in language comprehension and reasoning capabilities, emphasizing the importance of developing sophisticated MLLMs that generalize effectively across diverse linguistic environments. Multilingual evaluation tasks conducted with ChatGPT and other baseline models demonstrate LLMs' adaptability in various linguistic contexts [2]. These tasks reveal that while MLLMs perform well in certain scenarios, they often struggle with consistency and accuracy across different languages, necessitating continued research in multilingual language processing to enhance cross-linguistic capabilities. The GLM-130B benchmark evaluates bilingual models, showing their effectiveness compared to monolingual counterparts like GPT-3 [10]. This suggests that incorporating multilingual training data can significantly enhance LLM performance, though challenges like loss spikes and divergence during training highlight the need for refined techniques and evaluation metrics. Comparative analysis of MLLMs across different benchmarks provides valuable insights into their strengths and areas for improvement. By thoroughly analyzing LLM limitations and potential, researchers can devise effective strategies to enhance multilingual language processing, improving classification of complex public affairs documents, optimizing planning capabilities, and refining reasoning skills. Exploring embedded synthetic personality traits in LLMs can lead to more nuanced communication, crucial for responsible AI deployment, ultimately contributing to the evolution of AI algorithms across multiple fields [39,82,80,83,84]. Commonsense Planning and Decision-Making Tasks Evaluating LLMs in commonsense planning and decision-making tasks provides critical insights into their ability to process complex scenarios and generate coherent outputs. A prominent case study involves assessing ChatGPT's performance in commonsense reasoning tasks, scrutinizing its ability to answer questions and identify necessary knowledge, revealing strengths and limitations in cognitive language understanding [2]. The multitask benchmark evaluates ChatGPT's reasoning capabilities, hallucination tendencies, and interactivity compared to other LLMs, offering a holistic view of model performance [59]. The DELI framework exemplifies the evaluation of LLMs in decision-making tasks, introducing iterative deliberation procedures to refine reasoning through tool manipulation and natural language reasoning, contributing to robust assessments of model performance [53]. Additionally, the LVLM-eHub benchmark evaluates Large Vision-Language Models (LVLMs) on tasks like visual question answering, emphasizing the necessity for frameworks integrating visual and linguistic data in decision-making scenarios [4]. These case studies provide insights into LLM capabilities and limitations in commonsense planning and decision-making tasks, revealing that while models like GPT-4 show promise in heuristic guidance and domain-specific document processing, they struggle with autonomous plan generation and abstract reasoning, achieving limited success and requiring further advancements to approach expert-level performance in complex cognitive tasks [54,83,39,58]. Identifying areas for improvement, such as enhancing cognitive flexibility and reasoning processes, contributes to advancing language model technologies for effective deployment in real-world scenarios. Multilingual and Multimodal Capabilities Evaluating LLMs in multilingual and multimodal contexts is pivotal for understanding their adaptability and performance across diverse applications. The GLM-130B benchmark highlights the effectiveness of bilingual models, outperforming monolingual counterparts like GPT-3 on multiple tasks [10]. This suggests that incorporating multilingual training data can significantly enhance LLM performance, though challenges like loss spikes and divergence during training highlight the need for refined techniques and evaluation metrics. In multimodal capabilities, the LVLM-eHub benchmark provides a comprehensive framework for evaluating LVLMs across multiple dimensions, including quantitative capabilities and user-level performance in open-world scenarios [4]. This approach facilitates holistic evaluation, addressing the integration of visual and linguistic data, crucial for advancing AI technologies. The Languageis48 benchmark contributes to this field by offering a structured methodology for assessing multimodal reasoning capabilities, ensuring models can effectively process and generate outputs across various data types [68]. Integrating diverse modalities in benchmarking efforts is essential for enabling models to handle complex data inputs. Through these evaluations, researchers gain valuable insights into LLM strengths and limitations in multilingual and multimodal contexts, driving innovation in language model technologies and ensuring effective deployment across diverse applications. These evaluations enhance LLM applicability in fields such as public affairs, legal analysis, and open-domain conversations, ensuring reliable, efficient, and ethical deployment in real-world scenarios. They enable LLMs to process complex documents, improve legal reasoning capabilities, and streamline conversation evaluations, ultimately contributing to transparency, accountability, and informed decision-making [39,54,87,77]. Future Directions in LLM Evaluation The evolving complexity of Large Language Models (LLMs) necessitates advancements in evaluation methodologies. This section discusses future directions for LLM evaluation, emphasizing the expansion of benchmarks and datasets to assess performance across diverse linguistic and functional domains. Expansion of Benchmarks and Datasets Enhancing benchmarks and datasets is essential for advancing LLM evaluations, enabling comprehensive assessments across varied domains. Future research should refine benchmarks to address structured data and complex reasoning challenges, such as optimizing BOSS for out-of-distribution (OOD) evaluations [67]. Developing robust metrics and techniques to mitigate LLM hallucinations will further improve assessments [56]. In healthcare, diversifying benchmarks and improving datasets can enhance LLM applications in medical contexts [55]. Expanding benchmarks to incorporate additional languages will strengthen multilingual evaluations [10], while frameworks like UniEval, which refine evaluation dimensions, will enhance LLM assessments [14]. In software engineering, expanding benchmarks to cover complex tasks will improve LLM applicability [50], while diverse dialog scenarios, as seen with LaMDA, will advance conversational model evaluations [11]. Enhancing deliberation processes and integrating varied datasets will broaden frameworks like DELI [53]. Addressing limitations in current models and exploring new paradigms for artificial general intelligence (AGI) development are crucial, alongside examining societal implications [13]. Expanding datasets and refining metrics, coupled with integrating cognitive tasks, will yield comprehensive assessments [18]. Future work should enhance LLMs' emotional alignment and explore factors influencing empathy [12]. Focusing on these areas will lead to effective benchmarks and datasets that capture LLM capabilities, advancing natural language processing and ensuring reliable deployment. Refinement of Evaluation Metrics Refining evaluation metrics is vital for enhancing LLM benchmarks. The UHGEval benchmark, which evaluates user-generated content, can benefit from nuanced metrics capturing user interaction complexity [88]. Future research should focus on metrics reflecting language dynamics and LLM emergent abilities. Refining metrics to assess ethical considerations and biases in LLM outputs is crucial for societal alignment. Developing sophisticated measures for evaluating safety, trustworthiness, and cultural sensitivity involves creating benchmarks like CValues and TrustGPT, assessing alignment with human values [89,64]. Enhancing these metrics will provide insights into ethical dimensions, facilitating responsible deployment. Expanding metrics to encompass multimodal capabilities will improve understanding of LLM performance in integrating visual and linguistic data. The LVLM-eHub benchmark demonstrates how multimodal reasoning metrics can provide a holistic view of model capabilities [4]. These refinements will enable frameworks to capture the full spectrum of LLM capabilities, fostering innovation in natural language processing and ensuring effective deployment. Integration of Emerging Technologies Integrating emerging technologies into LLM evaluation offers promising avenues for enhancing assessment accuracy. Technologies like blockchain, edge computing, and quantum computing provide novel methodologies for addressing evaluation challenges, enhancing legal analysis capabilities, topic classification, and robust benchmarks [39,77,54,48,57]. Blockchain can ensure secure and transparent evaluation data storage, facilitating collaborative benchmarking. Edge computing enhances evaluation efficiency by enabling localized data processing, reducing latency, and improving scalability. Quantum computing's potential for processing complex computations at unprecedented speeds offers transformative capabilities for evaluating emergent LLM properties, such as legal reasoning [39,44,83,54,28]. Integrating AI technologies, such as machine learning algorithms for automated metric refinement, can streamline evaluation processes by dynamically adjusting benchmarks. Innovative frameworks like Language-Model-as-an-Examiner and methods like TrueTeacher enhance assessment precision [60,32]. These approaches ensure benchmarks remain relevant and reflective of advancements in language model research. Leveraging emerging technologies will enable robust evaluation frameworks addressing multifaceted LLM assessment challenges, enhancing accuracy and comprehensiveness, driving innovation in natural language processing, and facilitating the development of contextually aware models [60,47,39]. Ethical and Social Implications Evaluating and deploying LLMs entail multifaceted ethical and social implications, necessitating responsible AI development strategies. Understanding and measuring toxicity is crucial for ensuring LLM safety, highlighting ethical evaluation implications [61]. Future research should explore these implications, emphasizing ethical and social factors in evaluation [33]. Aligning AI with human values is critical, as social biases significantly impact outputs, necessitating benchmarks like BBQ [63,65]. Emerging trends indicate a need for interdisciplinary approaches combining linguistics, psychology, and machine learning to manage LLM hallucinations [66]. Questions remain regarding AI biases' long-term effects on political engagement and conversational AI evolution [8]. Findings reveal vulnerabilities of large VLMs to adversarial attacks, emphasizing research into robustness and security [40]. Addressing these challenges ensures comprehensive evaluation frameworks aligned with societal values, facilitating responsible deployment. Ethical implications include LLM-generated moral rationalizations' influence on societal perceptions [9]. Trustworthiness gaps in GPT models highlight the necessity for comprehensive frameworks [16]. Addressing these challenges ensures evaluation frameworks align with societal values, promoting responsible deployment across real-world contexts. Advancements in Training and Fine-Tuning Emerging trends in LLM training and fine-tuning emphasize enhancing contextual understanding and developing ethical frameworks for AI text generation [20]. As LLMs evolve, refining training methodologies is crucial for improving adaptability and performance. Future research should focus on expanding datasets, like BeaverTails, with diverse examples and refining metrics [17]. Integrating diverse linguistic and contextual scenarios into training datasets will improve LLMs' real-world language processing capabilities, such as classifying public affairs documents, applying tax law, and evaluating factual consistency in multilingual tasks. This supports transparency and accountability in public discourse while enhancing legal service efficiency [39,54,60]. Advancements in fine-tuning, including user feedback integration and iterative learning techniques, improve output precision and reliability. These improvements contribute to robust evaluation frameworks, facilitating comprehensive LLM capability assessments and ensuring effective deployment across domains. Conclusion The evaluation of Large Language Models (LLMs) plays a pivotal role in the evolution of natural language processing (NLP) technologies, as highlighted throughout this survey. The exploration of diverse evaluation methodologies and metrics has led to notable progress in establishing standardized benchmarks, which are crucial for ensuring reliable and comprehensive assessments of LLM performance. These benchmarks, such as UniEval, provide a structured approach to evaluating natural language generation tasks, facilitating assessments that resonate with human evaluators and enhancing the precision of text quality evaluations. Despite these advancements, challenges persist, particularly in areas requiring abstract reasoning, underscoring the necessity for continued research and model refinement. The survey further underscores the significance of cultural diversity in language technologies, advocating for enhancements that improve adaptability across various cultural landscapes. While LLMs have shown proficiency in translating complex information into accessible language, especially within clinical environments, further advancements are needed to optimize these applications fully. The integration of structured data tasks has markedly enhanced LLM performance, achieving outcomes on par with fully supervised models and expanding their functional scope. Moreover, aligning artificial intelligence with human values remains imperative, as current models exhibit an emerging yet incomplete understanding of essential ethical principles, highlighting the need for progress in machine ethics. The effective evaluation of LLMs, coupled with the development of standardized benchmarks, is essential for the advancement of NLP, ensuring the dependable deployment of language models across diverse applications. As LLM technologies continue to evolve, the importance of robust evaluation frameworks becomes increasingly pronounced, driving innovation and ensuring ethical, efficient deployment in practical scenarios.",
  "reference": {
    "1": "2303.09038v3",
    "2": "2303.16421v3",
    "3": "2303.17466v2",
    "4": "2306.09265v1",
    "5": "1706.03741v4",
    "6": "1810.04805v2",
    "7": "2305.18486v4",
    "8": "2301.01768v1",
    "9": "2209.12106v2",
    "10": "2210.02414v2",
    "11": "2201.08239v3",
    "12": "2308.03656v6",
    "13": "2303.12712v5",
    "14": "2210.07197v1",
    "15": "2304.03738v3",
    "16": "2306.11698v5",
    "17": "2307.04657v3",
    "18": "2310.20499v2",
    "19": "2310.03214v2",
    "20": "2306.07799v2",
    "21": "2305.16151v1",
    "22": "2205.01068v4",
    "23": "2201.11990v3",
    "24": "2206.04615v3",
    "25": "2306.05783v3",
    "26": "2305.14982v2",
    "27": "2305.17926v2",
    "28": "2306.06264v1",
    "29": "2211.09110v2",
    "30": "2212.13138v1",
    "31": "2306.09296v3",
    "32": "2306.04181v2",
    "33": "2204.02311v5",
    "34": "2305.08322v3",
    "35": "2309.12284v4",
    "36": "2306.08997v2",
    "37": "2204.01906v1",
    "38": "2103.06268v2",
    "39": "2306.02864v2",
    "40": "2305.16934v2",
    "41": "2103.03874v2",
    "42": "2304.05613v1",
    "43": "2305.14938v2",
    "44": "2307.09042v2",
    "45": "2206.07682v2",
    "46": "2305.14387v4",
    "47": "1804.07461v3",
    "48": "2305.15074v3",
    "49": "1707.06875v1",
    "50": "2305.16837v1",
    "51": "2305.09645v2",
    "52": "2109.07958v2",
    "53": "2306.02408v1",
    "54": "2306.07075v1",
    "55": "2308.08833v2",
    "56": "2309.05922v1",
    "57": "2305.15268v1",
    "58": "2508.05614v1",
    "59": "2302.04023v4",
    "60": "2305.11171v3",
    "61": "2304.05335v1",
    "62": "2101.11718v1",
    "63": "2008.02275v6",
    "64": "2307.09705v1",
    "65": "2110.08193v2",
    "66": "2309.01219v3",
    "67": "2306.04618v2",
    "68": "2302.14045v2",
    "69": "2103.03097v7",
    "70": "1610.02413v1",
    "71": "2303.12528v4",
    "72": "2111.08181v1",
    "73": "2306.15261v1",
    "74": "2206.07682v2",
    "75": "1905.00537v3",
    "76": "2305.14693v1",
    "77": "2308.01862v1",
    "78": "2212.02774v2",
    "79": "2009.03300v3",
    "80": "2307.02477v3",
    "81": "2306.05715v1",
    "82": "2307.00184v4",
    "83": "2305.15771v2",
    "84": "2303.18223v16",
    "85": "2306.09212v2",
    "86": "2306.04610v1",
    "87": "2304.03439v3",
    "88": "2305.13711v1",
    "89": "2311.15296v3",
    "90": "2306.11507v1"
  },
  "chooseref": {
    "1": "2309.05922v1",
    "2": "2306.15261v1",
    "3": "2305.18486v4",
    "4": "2112.00861v3",
    "5": "2302.04023v4",
    "6": "2302.11382v1",
    "7": "2303.18223v16",
    "8": "2307.13692v2",
    "9": "2212.02774v2",
    "10": "1910.14599v2",
    "11": "2111.02840v2",
    "12": "2111.08181v1",
    "13": "2304.06364v2",
    "14": "2008.02275v6",
    "15": "2305.14387v4",
    "16": "2303.17466v2",
    "17": "1706.03762v7",
    "18": "2205.12615v1",
    "19": "2110.08193v2",
    "20": "2307.04657v3",
    "21": "2104.08663v4",
    "22": "2305.14982v2",
    "23": "2306.04181v2",
    "24": "1810.04805v2",
    "25": "1904.09675v3",
    "26": "2005.04118v1",
    "27": "2206.04615v3",
    "28": "2101.11718v1",
    "29": "2306.13651v2",
    "30": "2305.08322v3",
    "31": "2305.11262v1",
    "32": "2306.16636v1",
    "33": "2308.08833v2",
    "34": "2306.09212v2",
    "35": "2307.09705v1",
    "36": "2306.01499v1",
    "37": "2304.07619v6",
    "38": "2305.17306v1",
    "39": "2304.05613v1",
    "40": "2306.04563v1",
    "41": "2306.07799v2",
    "42": "2307.01135v1",
    "43": "2305.16837v1",
    "44": "2303.02155v2",
    "45": "2303.16421v3",
    "46": "2309.11737v2",
    "47": "2203.13474v5",
    "48": "2103.06268v2",
    "49": "2306.11698v5",
    "50": "1706.03741v4",
    "51": "1910.13461v1",
    "52": "2305.14938v2",
    "53": "2106.06052v1",
    "54": "2302.12297v1",
    "55": "2204.01906v1",
    "56": "2206.07682v2",
    "57": "2206.07682v2",
    "58": "2307.09042v2",
    "59": "2308.03656v6",
    "60": "1610.02413v1",
    "61": "2305.15268v1",
    "62": "2306.01694v2",
    "63": "2306.02408v1",
    "64": "2304.01938v1",
    "65": "2107.03374v2",
    "66": "2305.10355v3",
    "67": "2304.03439v3",
    "68": "2306.02549v1",
    "69": "2306.04504v3",
    "70": "2304.01457v2",
    "71": "2306.08997v2",
    "72": "2306.05715v1",
    "73": "2305.11700v2",
    "74": "2305.14251v2",
    "75": "1909.08593v2",
    "76": "2310.03214v2",
    "77": "1804.07461v3",
    "78": "2103.03097v7",
    "79": "2210.02414v2",
    "80": "2211.08073v4",
    "81": "2305.15074v3",
    "82": "2305.14693v1",
    "83": "2211.09110v2",
    "84": "2306.06331v3",
    "85": "2304.09542v3",
    "86": "2304.04339v2",
    "87": "2306.03090v1",
    "88": "2302.06476v3",
    "89": "2306.05685v4",
    "90": "2305.14975v2",
    "91": "2306.09296v3",
    "92": "2306.06687v3",
    "93": "2305.13711v1",
    "94": "2309.11998v4",
    "95": "2306.09265v1",
    "96": "2201.08239v3",
    "97": "2207.05221v4",
    "98": "2302.14045v2",
    "99": "2005.14165v4",
    "100": "2508.05614v1",
    "101": "2306.07075v1",
    "102": "2303.07142v3",
    "103": "2211.01910v2",
    "104": "2305.17926v2",
    "105": "2212.13138v1",
    "106": "2306.02864v2",
    "107": "2310.20499v2",
    "108": "2307.06281v5",
    "109": "2306.13394v5",
    "110": "2309.07915v3",
    "111": "2301.12307v2",
    "112": "2205.00445v1",
    "113": "2012.15723v2",
    "114": "2301.13867v2",
    "115": "2306.06264v1",
    "116": "2105.09938v3",
    "117": "2009.03300v3",
    "118": "2103.03874v2",
    "119": "2303.12528v4",
    "120": "2309.12284v4",
    "121": "1405.0312v3",
    "122": "2306.14565v4",
    "123": "2308.02490v4",
    "124": "2209.12106v2",
    "125": "2305.16934v2",
    "126": "2301.12868v3",
    "127": "1706.04599v2",
    "128": "2305.15771v2",
    "129": "2108.07258v3",
    "130": "2302.12095v5",
    "131": "2205.01068v4",
    "132": "2204.02311v5",
    "133": "2306.05087v2",
    "134": "2307.00184v4",
    "135": "2009.11462v2",
    "136": "2307.02477v3",
    "137": "2307.02046v6",
    "138": "2306.04618v2",
    "139": "2307.16125v2",
    "140": "1705.08500v2",
    "141": "2303.08896v3",
    "142": "2304.03738v3",
    "143": "2309.01219v3",
    "144": "2105.04054v3",
    "145": "2303.12712v5",
    "146": "2305.09645v2",
    "147": "1905.00537v3",
    "148": "2205.12255v1",
    "149": "2305.15269v3",
    "150": "2304.07333v1",
    "151": "2306.04610v1",
    "152": "2301.01768v1",
    "153": "2301.11596v5",
    "154": "2304.08354v3",
    "155": "2307.16789v2",
    "156": "2302.04761v1",
    "157": "2210.07197v1",
    "158": "2304.05335v1",
    "159": "2303.09038v3",
    "160": "1705.03551v2",
    "161": "2305.11171v3",
    "162": "2306.11507v1",
    "163": "2109.07958v2",
    "164": "2311.15296v3",
    "165": "2305.02182v3",
    "166": "2305.16151v1",
    "167": "2306.08302v3",
    "168": "2201.11990v3",
    "169": "2303.13835v4",
    "170": "1707.06875v1",
    "171": "2308.01862v1",
    "172": "2306.05783v3",
    "173": "2306.04926v1"
  }
}