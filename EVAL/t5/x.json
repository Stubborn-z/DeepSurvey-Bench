{
  "survey": "This survey paper provides a comprehensive review of methodologies and metrics for evaluating large language models (LLMs) in natural language processing (NLP), focusing on accuracy, efficiency, ethical considerations, and standardized benchmarks for consistent evaluation across AI models. It underscores the significance of robust evaluation frameworks to ensure LLMs meet human expectations, particularly in high-stakes domains like healthcare and finance. The paper explores opportunities and risks associated with LLMs, highlighting their potential for personalization and the challenges of scalability and efficiency. It emphasizes the importance of addressing biases and ethical implications in model deployment, advocating for innovative benchmarking approaches and user-centric evaluation frameworks. The survey suggests future directions for research, including the expansion and refinement of datasets, integration of dynamic evaluation frameworks, and development of robust metrics to capture emergent abilities and generalization capabilities. It concludes by stressing the need for ongoing advancements in evaluation practices to ensure the effective and ethical use of LLMs in diverse applications, aligning with human values and societal norms.\n\nIntroduction Importance of Evaluating LLMs Evaluating large language models (LLMs) is critical for advancing natural language processing (NLP) and artificial intelligence (AI). The integration of LLMs into applications such as conversational agents and recommendation systems necessitates robust evaluation frameworks to ensure alignment with human expectations [1]. Advanced benchmarks that reflect the cognitive abilities of LLMs are essential for accurately assessing their performance in real-world scenarios. In high-stakes domains like healthcare and finance, where errors can have severe consequences, effective evaluation is crucial to differentiate between beneficial and harmful outputs [2]. Models such as ChatGPT require careful assessment to determine their effectiveness in specific fields, including software engineering [3]. The evaluation of LLMs in specialized areas, such as traditional Chinese medicine, is equally vital, as culturally specific contexts demand precise and reliable outputs [4]. Furthermore, evaluating hallucinations in large foundation models (LFMs) is essential for ensuring the accuracy and reliability of generated content in NLP applications [5]. This evaluation process also addresses the limitations of current automatic methods, which often rely on simplistic similarity metrics [2]. This is particularly important in computation-intensive tasks where reasoning capabilities may be limited [6]. Moreover, understanding LLMs' emotional alignment with human responses is vital, as empathy is a key factor in human-computer interaction [7]. Evaluating effectiveness in bilingual processing [8] and aligning outputs with human values [9] are crucial components of the evaluation process. As models like GPT-4 evolve, rigorous evaluation is necessary to grasp their capabilities and limitations for safe deployment in real-world applications [10]. Thorough evaluation is indispensable for enhancing the effectiveness, reliability, and ethical deployment of LLMs across various domains, ensuring they can meet the complex demands of modern AI applications. Opportunities and Risks of LLMs Large language models (LLMs) offer numerous opportunities, enhancing personalization and user preference understanding in recommender systems [11]. Their evolution from statistical to neural models has facilitated advancements in user-centric AI solutions, yet this progress introduces operational complexities that challenge scalability and efficiency [12]. The integration of multimodal large language models (MLLMs) expands interaction capabilities across different modalities, necessitating comprehensive evaluation frameworks to capture these benefits beyond traditional unimodal benchmarks [13]. The HELM benchmark aims to enhance transparency in language models through structured evaluations that address inherent risks, including bias and ethical concerns [14]. Tool Augmented Language Models (TALMs) significantly improve task performance via effective API utilization, contrasting with traditional models that lack dynamic data handling capabilities [15]. However, challenges persist in ensuring trustworthy tool use and addressing personalization issues [16]. The societal impact of LLMs is profound, offering opportunities to bridge cultural gaps and improve representation in language generation [17]. Nonetheless, risks related to cultural alignment and potential bias necessitate ongoing scrutiny to ensure equitable outcomes [18]. The efficiency and scalability of LLMs in content generation must be balanced against risks associated with quality and creativity [19]. Evaluating human values alignment in LLMs is crucial amid growing concerns about their potential risks and negative social impacts [20]. Moreover, the survey highlights risks associated with hallucination in LFMs, including the generation of factually inaccurate or fabricated information [5]. Understanding the origins of biases and their ethical implications is essential for addressing these challenges [21]. The implications of Moral Foundations Theory in understanding moral preferences in LLMs underscore the need for benchmarks that evaluate moral judgments in AI systems. The exploration of political ideology in conversational AI, such as ChatGPT, reveals biases and limitations in democratic decision-making contexts [22]. Additionally, while LLMs may respond to emotional cues, risks associated with inconsistent emotional alignment remain [7]. As research progresses, addressing adversarial robustness, domain generalization, and dataset biases will be crucial for ensuring LLMs can reliably operate across diverse contexts [23]. Structure of the Survey This survey is structured to provide a comprehensive examination of LLM evaluation within NLP. It begins with an introduction that emphasizes the significance of evaluating LLMs in advancing NLP and AI applications [1], addressing the necessity of robust evaluation frameworks to ensure alignment with human expectations, particularly in high-stakes domains like healthcare and finance [2]. Following the introduction, the survey offers background and definitions, providing an overview of LLMs and elucidating key terms essential for understanding AI model assessment, including 'linguistic model benchmarking' [4]. This section sets the stage for discussing methodologies employed in LLM evaluation, highlighting both traditional techniques and emerging innovations [13]. The importance of user-centric evaluation frameworks is emphasized, recognizing the need to align model outputs with human values and expectations [9]. The survey transitions to metrics for performance assessment, detailing various criteria used to measure LLM capabilities, including accuracy, efficiency, and ethical considerations [12]. It examines the role of standardized benchmarks in ensuring consistent and fair evaluation across different AI models, reviewing existing benchmarks and their impact on evaluation practices [14]. Innovative benchmarking approaches are explored, suggesting future directions for enhancing evaluation standards [15]. Challenges and ethical considerations are thoroughly examined, identifying issues such as bias, fairness, and the ethical implications of deploying LLMs in real-world applications [21]. The survey also addresses challenges in multimodal and multilingual evaluation, emphasizing the need for comprehensive frameworks to capture diverse interaction capabilities [11]. Finally, the paper proposes future research directions in LLM evaluation, advocating for the expansion and refinement of datasets, integration and enhancement of evaluation frameworks, and the development of robust evaluation metrics [23]. Advancements in model training and optimization are highlighted, along with the exploration of new domains and applications for LLM evaluation [24]. The conclusion reiterates the importance of ongoing research and development to ensure the effective and ethical use of LLMs in NLP applications [19].The following sections are organized as shown in . Background and Definitions Overview of Large Language Models (LLMs) Large Language Models (LLMs) have revolutionized artificial intelligence (AI) and natural language processing (NLP) by generating text that mimics human communication [25]. Transitioning from statistical models to neural architectures, exemplified by ChatGPT, has empowered these models to handle diverse tasks without task-specific fine-tuning [3]. Their integration into multimodal applications, such as LVLMs, which blend vision and language learning, underscores their importance [26]. Benchmarks like TruthfulQA assess the accuracy and reliability of LLM-generated content, highlighting the need for truthfulness in AI [1]. Despite their capabilities, challenges persist in abstract reasoning and complex problem-solving, necessitating benchmarks that evaluate beyond rote memorization [6]. In technical domains, LLMs bridge natural language and programming languages, generating Python code from natural language specifications, advancing software development and automation [5]. The dynamic nature of language and evolving factual knowledge require benchmarks addressing temporal concept drift to keep LLMs updated [4]. Models like GLM-130B demonstrate bilingual processing, aiming to match proprietary models like GPT-3, broadening research across languages [8]. Exploring moral preferences in LLMs, reflecting political identities in prompts, underscores the need to align AI outputs with human values and ethics [24]. As LLMs evolve, integrating advanced reasoning, semantic understanding, and multilingual capabilities will shape AI applications in creative, technical, and scientific domains. Developing robust evaluation frameworks is crucial for reliable, accurate, and ethical deployment, paving the way for AI advancements [9]. Definitions and Key Terms Understanding key terms is crucial for evaluating LLMs and their implications. 'Benchmarking' involves systematic comparison against standards, assessing capabilities across languages and tasks [8]. Frameworks like LAraBench exemplify this by providing structured assessments for specific linguistic contexts [27]. 'Zero-shot performance' evaluates LLMs' adaptability without prior task-specific training, vital in educational and multidisciplinary contexts [28,29]. 'Evaluation of empathy ability' and 'emotion appraisal theory' assess LLMs' emotional alignment, crucial for human-computer interaction [7]. 'Human values alignment' and 'safety and responsibility criteria' ensure outputs align with societal norms and ethics [20,30]. Understanding 'social biases' is integral to evaluating model outputs, mitigating stereotypes and inaccuracies [31]. Political bias in conversational AI highlights the importance of assessing ideological orientations [22]. 'Generalization' emphasizes developing models capable of adapting to unseen domains, requiring robust knowledge transfer [32]. 'Language representation models' and 'bidirectional context' relate to LLM architecture, essential for evaluating language processing and contextual comprehension [33]. 'Human preferences' play a role in teaching reinforcement learning systems complex behaviors, emphasizing alignment with user expectations [34]. These definitions highlight the multifaceted nature of LLM evaluation, encompassing technical, ethical, and human-centric aspects for comprehensive assessments [35]. AI Model Assessment Comprehensive frameworks are essential for assessing LLMs across domains. Traditional benchmarks often fail to capture nuanced capabilities, such as sentiment analysis and complex reasoning, constrained by reliance on human input and multiple model prompts [1]. Issues like testing leakage and lack of automated methods challenge current frameworks [10]. Specialized benchmarks address challenges like mathematical problem-solving, providing standardized approaches for LLM evaluation [1]. 'TruthfulQA' targets hallucination in text generation, assessing reliability in authentic content production. Frameworks like 'Dynabench' evaluate temporal concept drift impacts on masked language models [1]. Evaluation in structured data contexts, especially zero-shot scenarios, remains underexplored. Benchmarks like 'LVLM-eHub' enhance reasoning assessments in question-answering tasks involving structured data. The 'Amultitask' benchmark evaluates coding problem performance, showcasing code generation capabilities [1]. Educational benchmarks assess human-level problem-solving replication, crucial for understanding LLM generalization across disciplines. Measuring personality in LLMs presents gaps in evaluative methodologies, as traditional tests are ineffective [1]. Existing benchmarks lack granularity in assessing safety attributes, limiting guidance for safe deployment. The GLM-130B benchmark evaluates bilingual model performance, addressing limitations in metrics. Benchmarks for GPT models assess trustworthiness in terms of toxicity, stereotype bias, adversarial robustness, and privacy concerns [10]. Evaluating moral rationalizations in politically motivated prompts is crucial for AI assessment [10]. Diverse assessments ensure rigorous LLM evaluation, addressing technical capabilities and alignment with human-centric tasks. This comprehensive approach fosters effective, ethical integration across applications, including public affairs, legal analysis, and emotional intelligence assessment, promoting transparency, accountability, and informed decision-making in AI technology [36,30,37,38,39]. Methodologies for Evaluating LLMs Table presents a comprehensive comparison of evaluation methodologies for large language models, illustrating the transition from traditional techniques to innovative and user-focused evaluation frameworks. Evaluating large language models (LLMs) requires an understanding of methodologies that have evolved alongside these models. Traditional techniques have provided foundational insights into model performance; however, as LLM complexity increases, these methods face limitations. This section explores traditional evaluation methods, their strengths and weaknesses, and transitions to emerging techniques that are reshaping evaluation paradigms. Table offers a detailed summary of the various methodologies utilized in evaluating large language models, emphasizing the evolution from traditional techniques to emerging innovations and robustness testing. illustrates the hierarchical structure of methodologies for evaluating LLMs, categorizing traditional techniques, emerging innovations, user-centric frameworks, and robustness testing. This figure highlights the primary categories, subcategories, and key concepts involved in assessing LLM performance across various domains, thereby providing a visual representation that complements the discussion of both established and novel evaluation approaches. Traditional Evaluation Techniques Traditional evaluation techniques for LLMs use deterministic frameworks and standardized benchmarks, enabling structured quantitative assessments and facilitating model comparisons [37]. Metrics like BLEU are often system- and data-specific, potentially yielding unreliable evaluations by failing to capture nuanced capabilities in natural language generation tasks [40]. These methods typically use small, domain-specific datasets with human-curated labels, which can mislead evaluations, particularly in complex reasoning and decision-making tasks where metrics may not adequately reflect the capabilities of advanced models like GPT-4 [19]. In tool learning contexts, traditional evaluations assess task decomposition and tool integration effectiveness, emphasizing the need for frameworks that can evaluate these capabilities [16]. Benchmarks like MoRec against IDRec leverage NLP and computer vision advancements to systematically evaluate models across practical scenarios [41]. However, traditional methods often inadequately assess LLMs' effectiveness in specialized translations, such as radiology reports, necessitating specialized evaluation frameworks [42]. Existing benchmarks predominantly use exact matches for evaluation, which inadequately reflect semantic quality, leading to poor correlations with human assessments [43]. Benchmarks like GLUE offer a diverse array of tasks and auxiliary datasets for probing linguistic knowledge representation, facilitating a more comprehensive evaluation of LLMs [44]. Additionally, traditional methods often overlook the manipulation of response order, resulting in skewed evaluation outcomes [45]. The Personality Measurement and Shaping Methodology (PMSM) offers a novel approach for assessing and modifying personality traits in LLM outputs [46]. Traditional techniques often neglect the specific context of educational assessments, which dedicated benchmarks aim to address by focusing on educational domain requirements [47]. Debiasing methods in models like CDial-GPT and EVA2.0 integrate bias reduction techniques into traditional evaluation frameworks, optimizing conversational quality while minimizing bias [48]. Frameworks for understanding cultural biases in ChatGPT's responses categorize biases based on prompt context, highlighting the need for culturally aware evaluation methodologies [17]. Traditional evaluation techniques provide essential benchmarks for LLM performance, offering structured methodologies for quantitative analysis across diverse applications. As the field evolves, these methods integrate new dimensions, enhancing evaluative effectiveness [49]. The Information Theory-Based Knowledge Measurement (ITK) method exemplifies this evolution by employing information theory metrics to measure and modify knowledge within LLMs, providing insights into model understanding depth and breadth [50]. The LVLM-eHub benchmark enhances the traditional evaluation landscape by combining quantitative assessments with user-level evaluations [51]. Traditional evaluation methods often fail to adequately measure the impact of social biases on model performance [31]. With 140 tasks analyzing 255,000 responses, the benchmark represents the largest evaluation of ChatGPT in NLP [52]. Experiments testing Transformer models on the CUAD dataset evaluate effectiveness in identifying salient portions of legal contracts [25]. Methodologies for hallucination detection have been surveyed, categorizing methods based on effectiveness and real-world applicability [26]. A framework for categorizing the political ideology of conversational AI has been introduced, focusing on pro-environmental and left-libertarian tendencies [22]. Current research has developed initial strategies for mitigating hallucination phenomena, contributing positively to LFM evaluation [5]. Integrating social science theory with AI research provides insights into traditional evaluation techniques by analyzing LLM moral outputs [24]. Traditional methods struggle with bilingual models and often fail to capture language processing nuances [8]. Current studies often lack comprehensive evaluations across diverse datasets, and there are limitations in model robustness to extreme variations in unseen domains [32]. Traditional evaluation methods frequently overlook vulnerabilities related to toxic outputs and privacy leaks, emphasizing the need for comprehensive trustworthiness benchmarks [53]. These methods have not adequately measured the performance of models like ChatGPT in software engineering contexts [3]. Current evaluation methods are organized into stages reflecting the progression of language models from basic tasks to complex problem-solving [10]. The evaluation framework introduces stages based on emotional responses, categorizing existing research into a structured approach focusing on empathy [7]. Emerging Techniques and Innovations Emerging techniques in LLM evaluation are redefining traditional methodologies, introducing innovative approaches that enhance understanding of model capabilities across applications. The Prompt Pattern Catalog represents a significant advancement, providing reusable prompt patterns that improve evaluation effectiveness and adaptability [54]. This framework addresses complexities in LLM outputs, enabling dynamic and flexible evaluation. The LAMM-Benchmark integrates instruction-tuning across multiple modalities, a feature previously absent in benchmarks, facilitating comprehensive assessments of model performance, especially in tasks requiring external knowledge sources [55]. UniEval introduces a Boolean Question Answering format for evaluating text generation models, significantly improving correlation with human assessments and addressing traditional benchmark limitations [2]. StructGPT employs an Iterative Reading-then-Reasoning (IRR) approach, focusing on evidence collection from structured data before reasoning tasks, challenging models to demonstrate higher-order reasoning capabilities [56]. LaMDA enhances assessment by integrating fine-tuning with annotated data and enabling external knowledge sources, improving safety and factual grounding [9]. The DELI method enhances reasoning processes by incorporating tool interfaces and iterative deliberation of solution steps, offering deeper insights into model reasoning capabilities, particularly in complex tasks [6]. The benchmark introduced by Leveraging70 incorporates DEEP and SpyGame, focusing on multi-agent interaction and strategic communication, thus providing insights into LLM dynamic processing capabilities [1]. APE represents an innovative approach by automatically searching over a pool of instruction candidates generated by an LLM to maximize performance [57]. The Amultitask5 benchmark introduces a multimodal dataset, evaluating LLMs in multitask and multilingual contexts, targeting abstract reasoning tasks [58]. Collectively, these emerging techniques signify a transformative shift in LLM evaluation, introducing methodologies that enhance understanding of model performance across various domains, including public affairs document analysis and legal reasoning. By leveraging regex-powered tools and holistic evaluation frameworks like HELM, these approaches provide comprehensive insights into LLM capabilities, including topic classification, legal analysis, and multi-metric assessments of accuracy, robustness, and fairness. These advancements improve transparency and accountability, paving the way for effective governance of artificial intelligence [37,14,39]. User-Centric Evaluation Frameworks User-centric evaluation frameworks are crucial for assessing LLMs, emphasizing user satisfaction and alignment with human expectations. These frameworks provide a holistic view of model performance by integrating user experience metrics alongside traditional evaluative criteria, offering insights into the efficacy of information-seeking tools like ChatGPT [59]. Frameworks such as MultiMedQA underscore the importance of human evaluation across multiple dimensions, ensuring model responses meet diverse user needs [60]. AdaVision introduces a novel interactive testing framework, enabling users to identify and address failure modes overlooked by traditional benchmarks [61]. This approach emphasizes the dynamic relationship between user instructions and effective tool selection planning, highlighting adaptability's importance in user-centric evaluations [16]. The BBQ benchmark provides a structured framework for evaluating biases in NLP models, focusing on user satisfaction and alignment with human expectations [31]. The survey discusses how biases in models like ChatGPT can influence user interactions and perceptions, stressing the need for frameworks that consider cultural context in responses. This cultural awareness is vital for ensuring equitable and accurate model outputs across diverse user demographics. Future research should aim to develop user-centric evaluation frameworks that better integrate user feedback into AI models, enhancing their responsiveness and relevance [19]. The emphasis on standardized evaluation benchmarks for hallucination detection highlights the necessity of frameworks that maintain the accuracy and reliability of generated content, aligning with user expectations [26]. Collectively, these user-centric frameworks represent a transformative approach to LLM evaluation, ensuring models are assessed not only for technical proficiency but also for their ability to meet the nuanced needs and expectations of users across various contexts. Robustness and Adversarial Testing Evaluating the robustness of LLMs against adversarial inputs is critical for ensuring reliability and safety in diverse, particularly safety-critical, applications [62]. The robustness of models like ChatGPT is assessed through benchmarks designed to test performance against adversarial and out-of-distribution inputs, providing insights into their ability to handle unexpected scenarios [62]. The Robust Prompt-based Semantic Parsing (RPSP) method exemplifies advancements in enhancing prompt-based semantic parsers' robustness, allowing them to withstand adversarial attacks without extensive labeled data or computational resources [63]. Despite the significance of robustness evaluation, comprehensive benchmarks specifically designed to assess language models against adversarial attacks are lacking [64]. This gap has led to proposed adversarial dataset creation strategies, aiming to construct challenging datasets that better test model robustness limits [65]. The complexity of data-generating processes and evaluation protocols across different research lines poses significant challenges for comparative analysis, hindering field progress [23]. Innovative approaches like the Information Theory-Based Knowledge Measurement (ITK) method utilize metrics such as entropy and KL-divergence to analyze prediction probability distributions before and after knowledge instillation, providing a quantitative measure of robustness [50]. Additionally, reinforcement learning systems are being explored for their ability to develop complex behaviors with minimal supervision, offering a new approach to robustness in learning environments [34]. The BOSS benchmark represents a structured and challenging evaluation protocol covering multiple tasks and datasets, significantly improving out-of-distribution (OOD) robustness assessment in models [66]. Vision-language models (VLMs) undergo benchmarks testing resilience against adversarial attacks, especially when visual inputs are manipulated to deceive the model [67]. These diverse methodologies and benchmarks collectively contribute to a comprehensive understanding of LLM robustness, ensuring models can effectively operate in adversarial contexts and maintain reliability across varied applications. Metrics for Performance Assessment Accuracy and Precision Metrics Accuracy and precision metrics are pivotal in evaluating large language models (LLMs), providing insights into their ability to produce reliable outputs across diverse tasks. These metrics are crucial in contexts demanding precise decision-making and information retrieval, ensuring alignment with human expectations and values. In sensitive domains like medical evaluations, accuracy and F1-score are standard metrics used to ensure output reliability [4]. Similarly, these metrics are vital in assessing linguistic proficiency in models such as GLM-130B [8]. For models like ChatGPT, accuracy and F1-score measure the correctness and relevance of responses, ensuring factual integrity and coherence [3]. In the evaluation of LaMDA, metrics such as 'Groundedness' and 'Safety' complement accuracy metrics, aligning model responses with human values [9]. The DELI method further evaluates performance by comparing generated solutions against competitive baselines [6]. In adversarial contexts, metrics like Success Rate and Evasion Rate assess the effectiveness of adversarial examples against vision-language models (VLMs), highlighting robustness under challenging conditions [67]. The correlation with human judgments is crucial for evaluation accuracy, reinforcing the necessity of aligning automatic assessments with human evaluations [2]. Metrics such as Expression Ability and Disguising Ability reflect cognitive and linguistic skills in strategic communication [1]. The BeaverTails benchmark incorporates safety metrics assessing helpfulness and harmlessness, ensuring beneficial and non-harmful outputs [68]. Addressing harmful language and bias, metrics like toxicity and stereotype bias are essential for identifying and mitigating biased outputs [53]. Collectively, these accuracy and precision metrics, including advanced measures like entropy and KL-divergence for factual knowledge assessment, provide a comprehensive framework for evaluating LLMs. They ensure models meet user expectations and application requirements across diverse domains, addressing broader considerations such as calibration, robustness, and fairness in holistic evaluations like HELM [37,14,50]. Efficiency and Task Completion Metrics Efficiency and task completion metrics are essential for evaluating LLM capabilities, particularly in scenarios requiring rapid processing and accurate task execution. These metrics assess operational effectiveness, measuring the ability to complete tasks within specified constraints and timeframes. In computational tasks, model performance is evaluated by comparing generated code against expected outputs, ensuring efficiency in producing correct and functional code [69]. This evaluation is crucial for understanding LLMs' proficiency in handling complex programming tasks and translating natural language specifications into executable code. In question-answering contexts, models are assessed on their ability to match correct answers with posed questions, using provided evidence to ensure accuracy and relevance [70]. Efficiency metrics often involve measuring computational resources required for task execution, including time complexity and memory usage, which are critical for understanding scalability in real-world applications. Task completion metrics emphasize not only performance efficiency but also precision and thoroughness of outputs. They evaluate a model's ability to handle various tasks accurately, identify critical failures, avoid generating falsehoods, and manage factual knowledge effectively. These metrics also help identify limitations, such as struggles with expert-level accuracy, truthfulness, or social biases, crucial for improving model capabilities and reliability [71,72,50,73,74]. Together, these metrics provide a comprehensive framework for evaluating LLM efficiency and task completion capabilities, offering valuable insights into their operational effectiveness. They highlight potential deployment across diverse domains, enhancing document analysis and promoting transparency and accountability. Additionally, these metrics facilitate holistic assessments, encompassing aspects like accuracy, robustness, and fairness, as demonstrated by comprehensive benchmarks revealing both limitations and optimization potential [37,14,75]. User Satisfaction and Human Alignment Metrics User satisfaction and human alignment metrics are crucial for evaluating LLMs, ensuring outputs align with human values and societal norms. These metrics provide a comprehensive understanding of the ethical implications of model-generated content, focusing on bias, toxicity, and emotional understanding. Metrics like the Emotional Quotient (EQ) highlight LLMs' capacity to interpret and respond to emotional cues, enhancing human-computer interaction [7]. Metrics such as BERTScore offer reliable evaluations of text generation quality, aligning closely with human judgments and demonstrating robustness in challenging examples [43]. These metrics are vital for assessing how well LLM outputs correspond to user expectations and societal norms, ensuring AI-generated text is coherent and culturally relevant. Evaluation processes must consider both human and automatic assessments, focusing on alignment with human values [20]. This dual approach integrates subjective user feedback with objective performance metrics. Metrics like 'Accuracy' and 'Bias Difference' assess user satisfaction and ethical implications, ensuring equitable and unbiased content [31]. Assessing discrimination levels and classification accuracy before and after adjustments is vital for integrating ethical considerations into evaluations, emphasizing fairness and unbiased outputs. These metrics encompass diverse aspects such as justice, well-being, and commonsense morality, providing a nuanced understanding of AI and human value interactions. By leveraging benchmarks like the ETHICS dataset and AGIEval, which evaluate language models' grasp of ethical concepts and performance on human-centric tasks, researchers can develop AI models that are both effective and ethically sound. This approach fosters alignment with shared human values, addressing opportunities and risks associated with foundation models, guiding the creation of systems that navigate complex societal impacts while maintaining ethical integrity [30,76,77,18]. Emergent Abilities and Generalization Metrics Emergent abilities in LLMs refer to capabilities that manifest as models scale, which are not predictable from smaller models' performance. The ability of LLMs to exhibit emergent capabilities—those appearing in larger models but absent in smaller ones—has significant implications for generalization metrics evaluation. Scaling models not only enhances performance and sample efficiency across tasks but also unlocks unpredictable, qualitative improvements. These emergent abilities suggest that further scaling could substantially expand model capabilities, impacting fields such as linguistics, mathematics, and social bias mitigation. Understanding these developments is crucial for addressing potential transformative impacts and social implications [74,78,79]. This phenomenon underscores the need for specific performance metrics that effectively capture and assess these emergent capabilities. Generalization metrics are essential for evaluating how well LLMs apply learned knowledge to novel, out-of-distribution (OOD) scenarios. This survey highlights the comparative effectiveness of different research lines regarding OOD evaluation, identifying strengths and weaknesses that inform the development of robust generalization metrics [23]. These metrics are vital for understanding LLM adaptability and flexibility, particularly in contexts requiring reliable performance in unfamiliar settings. LLM-Eval provides a streamlined framework for assessing LLMs across multiple dimensions using a single prompt, enhancing evaluation by simplifying emergent abilities and generalization capabilities assessment [38]. This approach ensures comprehensive evaluations, confirming models' capabilities in both in-domain (ID) tasks and OOD instances. Recent studies indicate that while fine-tuning domain-specific models is effective for ID tasks, LLMs with in-context learning outperform on OOD instances [66]. This underscores the necessity for generalization metrics that accurately assess LLMs' ability to transfer knowledge across domains, ensuring applicability in diverse real-world applications. Collectively, these emergent abilities and generalization metrics provide deeper insights into LLM capabilities, highlighting the critical need for scalable and adaptable evaluation frameworks. Such frameworks are essential for accurately capturing the full spectrum of model performance, especially as emergent abilities continue to expand potential applications across diverse contexts [78,79]. Benchmarking and Standardization Role of Standardized Benchmarks Standardized benchmarks are essential for evaluating large language models (LLMs), offering consistent frameworks for systematic comparisons across various tasks and domains. Table illustrates the role of standardized benchmarks in assessing the performance and reliability of large language models, highlighting their diverse applications across multiple domains and tasks. They help identify model performance strengths and weaknesses, facilitating advancements in AI applications. The BeaverTails benchmark exemplifies a structured approach to safety alignment, emphasizing the need for models to generate beneficial and non-harmful outputs for ethical AI deployment [68]. The DecodingTrust benchmark evaluates trustworthiness by addressing vulnerabilities in GPT models, including toxicity, stereotype bias, adversarial robustness, and privacy concerns, providing insights into LLM reliability and security [53]. Leveraging70's benchmark assesses LLM intelligence in strategic communication, highlighting dynamic processing capabilities and evaluation of diverse input scenarios [1]. Recent methodologies, such as the Language-Model-as-an-Examiner framework, enhance benchmarking by enabling language models to generate and evaluate questions across multiple domains, ensuring comprehensive and unbiased assessments. The KoLA benchmark emphasizes world knowledge by mimicking human cognitive abilities and utilizing diverse data sources for thorough evaluations of LLMs. These strategies tackle issues like testing leakage and evaluation automation, incorporating decentralized peer-examination methods to reduce biases and align outcomes with human annotations, enhancing understanding of model adaptability and generalization capabilities, especially in cross-linguistic contexts [80,81]. Integrating datasets like ETHICS and foundation models into benchmarks fosters the development of AI systems that are reliable, accurate, and aligned with human values, advancing AI technology in fields such as law, healthcare, and education. The ETHICS dataset evaluates language models' understanding of moral concepts, while foundation models enhance adaptability across tasks, promoting AI's effectiveness and trustworthiness in real-world scenarios [30,18,16]. Existing Benchmarks and Their Impact Existing benchmarks significantly influence the evaluation of large language models (LLMs), providing structured frameworks for comprehensive assessments across domains. The C-eval dataset offers multiple-choice questions across four difficulty levels, spanning 52 disciplines, enabling detailed evaluations of LLM capabilities in educational contexts [29]. This benchmark ensures models can address varying complexities and subject breadth, revealing areas for improvement, as only GPT-4 has surpassed 60 Innovative Benchmarking Approaches Innovative benchmarking approaches are reshaping evaluation standards for large language models (LLMs), introducing methodologies that enhance the depth and breadth of model assessments. Techniques like the Prompt Pattern Catalog and LAMM-Benchmark emphasize instruction-tuning across modalities, facilitating detailed evaluations of Multimodal Large Language Models (MLLMs). The MME benchmark assesses perception and cognition across 14 subtasks, using instruction-answer pairs to prevent data leakage, highlighting areas for MLLM improvement [55,75]. UniEval and StructGPT advance benchmarking by introducing formats and reasoning approaches that challenge models to demonstrate higher-order reasoning capabilities. The Language-Model-as-an-Examiner framework uses LLMs as examiners to formulate and evaluate responses across domains, enhancing assessment depth and reliability. JEEBench tests models' problem-solving abilities with pre-engineering problems, revealing common failure modes like algebraic manipulation errors [81,82]. LaMDA and DELI enhance benchmarking by incorporating external knowledge sources and iterative deliberation, improving safety and factual grounding in model evaluations. LaMDA addresses safety through fine-tuning and response filtering, ensuring alignment with human values, while DELI integrates comprehensive frameworks for medical knowledge assessments, emphasizing factuality and reasoning [60,9]. The Leveraging70 framework introduces strategic communication evaluations, highlighting LLM adaptability in complex tasks [1]. APE and Amultitask5 benchmarks underscore multilingual and multitask evaluations, focusing on abstract reasoning tasks and cross-cultural nuances, advancing evaluation standards by integrating domain-specific analysis and multi-layered neural network techniques [37,83]. Future Directions in Benchmarking The future of benchmarking for large language models (LLMs) involves developing frameworks that are comprehensive and adaptable to evolving AI technologies. Integrating dynamic benchmarks that adapt to model updates ensures evaluations remain relevant and rigorous [1]. Cross-linguistic and cross-modal benchmarks are vital for capturing LLM capabilities across modalities like text, image, and audio, fostering advancements in model generalization and adaptability [55]. Developing benchmarks focusing on ethical considerations and societal impacts is critical, incorporating metrics assessing bias, fairness, and alignment with human values for responsible AI deployment [30]. User-centric evaluations are crucial for aligning model outputs with human expectations, incorporating user feedback into benchmark criteria to enhance AI applications [19]. Assessing emergent abilities and generalization is vital for understanding LLM scalability and adaptability, developing metrics that capture these abilities for novel and complex tasks [79]. Addressing challenges like testing leakage and evaluation automation, as seen in the MME benchmark, is crucial. By creating manual annotations for instruction-answer pairs, MME avoids data leakage and facilitates fair comparisons. The Language-Model-as-an-Examiner framework enhances assessments by generating diverse questions and evaluating responses, emphasizing the need for benchmarks that evolve with AI technology, revealing areas for model optimization and ensuring reliable evaluation outcomes aligned with human judgments [75,81]. Challenges and Ethical Considerations Challenges in Evaluating LLMs Evaluating large language models (LLMs) presents several complexities due to their advanced capabilities and diverse applications. A significant issue is the reliance on specific datasets that often fail to capture linguistic nuances, constraining benchmark relevance [8]. Current methodologies cannot fully encapsulate the subtleties of human judgment, necessitating continual improvements in evaluation frameworks [2]. Performance inconsistencies, particularly in tasks like software engineering with models such as ChatGPT, erode user trust [3]. Challenges also stem from the initialization quality in frameworks like DELI, where exemplar selection impacts performance [6]. Evaluating models like GPT-4 demands understanding their limitations and identifying necessary advancements toward artificial general intelligence (AGI) [10]. Additionally, evaluating multi-agent interactions and complex word selection complicates the assessment process [1]. A persistent challenge is the misalignment of LLMs with human emotional behaviors, which affects their ability to relate similar situations, impacting user satisfaction [7]. These issues underscore the urgent need for advanced evaluation methodologies incorporating new metrics, addressing biases, and ensuring generalizability to promote the ethical and effective deployment of LLMs across domains. Bias and Fairness in Evaluation Addressing bias and fairness in LLM evaluation is critical for ethical deployment and reliability. Bias, including ideological biases observed in models such as ChatGPT, can influence user interactions [22]. This necessitates evaluation frameworks promoting ideological diversity and fairness [24]. The persistence of false beliefs in human texts replicated by models further complicates bias issues, highlighting the need for processes that mitigate these biases [72]. Subjective safety assessments emphasize developing standardized benchmarks for unbiased output evaluation [68]. Existing benchmarks like BBQ reveal the importance of addressing social biases, as they significantly influence outcomes [31]. Despite intervention efforts, bias complexity presents ethical dilemmas in evaluations [21]. Trustworthiness assessments should consider bias and fairness, particularly in contexts where outputs shape societal norms [53]. Dataset specificity, such as those for Spanish documents, may limit generalizability, underscoring the need for frameworks embracing linguistic diversity [37]. Understanding adversarial vulnerabilities in vision-language models (VLMs) is essential to ensure reliability and safety, as biases can impair practical application performance [67]. In dialog systems, addressing bias and fairness is paramount due to direct user interactions, necessitating equitable evaluations [9]. LLM misalignment with human evaluations also indicates limited empathetic capabilities [7]. These considerations highlight the need for advanced methodologies integrating diverse perspectives, mitigating biases, enhancing fairness, and ensuring ethical LLM deployment. Ethical Implications of Model Deployment Deploying LLMs in real-world contexts presents complex ethical implications requiring careful consideration to ensure responsible use and societal impact. A key concern is misinformation and hallucinations undermining AI trust, necessitating robust solutions for enhancing reliability [26]. Political biases in models like ChatGPT can influence societal decisions, highlighting the importance of evaluations considering ideological diversity [22]. Deploying biased models in virtual assistants necessitates frameworks for equitable interactions [21]. The BOSS benchmark offers a rigorous framework for evaluating out-of-distribution (OOD) robustness, tackling ethical considerations of reliability and safety [66]. Societal impacts from AI advancements urge re-evaluation of ethical and practical deployment considerations [10]. Ethical implications underscore the need for comprehensive frameworks integrating considerations for responsible deployment. Utilizing resources like the ETHICS dataset to assess morality understanding and employing broader networks for fair evaluations can advance AI technology aligned with societal values. This enhances value alignment, fairness, and LLM evaluation efficiency, contributing to responsible AI progression [30,83]. Challenges in Multimodal and Multilingual Evaluation Evaluating LLMs in multimodal and multilingual contexts presents unique challenges requiring sophisticated frameworks to capture diverse inputs and linguistic variations. Multimodal integration, such as text, images, and audio, needs benchmarks evaluating LLM performance across varied data types for real-world application reflection [84]. Multilingual evaluation complexities arise from varying model performance across languages, emphasizing benchmarks accommodating linguistic diversity for equitable assessments [84]. This requires frameworks evaluating LLM adaptability and generalization for linguistic and cultural contexts proficiency. Evaluating multimodal models requires a comprehensive understanding of modality interactions impacting performance. Benchmarks assessing large multimodal models (LMMs) and large vision-language models (LVLMs) focus on integrated capabilities. Examples include MM-Vet's capability structure, MME's perception-cognition evaluation, LVLM-eHub's modality alignment analysis, and MMBench's multi-modal strategies, providing insights into core vision-language capabilities and improvement areas [85,51,75,38,86]. Dynamic multimodal inputs necessitate evaluation methodologies capturing model capability depth and breadth for robust task assessments. Recent research emphasizes innovative evaluation frameworks addressing multimodal and multilingual input complexities, essential for accurate LLM assessments meeting evolving AI technology demands. For instance, MME benchmarks perception and cognition across subtasks, LLM-Eval streamlines open-domain conversation evaluations, MM-Vet offers integrated multimodal capabilities insights, CMMLU assesses Chinese context performance across subjects, and LVLM-eHub analyzes multimodal capabilities and user-level performance. Collectively, these frameworks reveal potential improvement areas and suggest model capability and evaluation strategy optimization directions [87,51,75,38,86]. Future Directions The rapid evolution of large language models (LLMs) demands exploration of future directions to guide their development and evaluation. Key areas for innovation include dataset expansion and refinement, which are vital for enhancing assessments that accurately reflect the diverse capabilities of LLMs. This section examines how expanding datasets can inform robust evaluation practices and promote LLM technology advancement. Expansion and Refinement of Datasets Enhancing datasets is crucial for advancing LLM evaluation methodologies, ensuring assessments capture diverse model capabilities. Future research should adopt interdisciplinary approaches, integrating insights from adversarial robustness, domain generalization, and dataset biases to inform more thorough evaluations. Expanding datasets to encompass broader cultural contexts will enhance models' adaptability in cross-cultural communication, capturing the full spectrum of LLM capabilities [1]. Refining methodologies like the Personality Measurement and Shaping Methodology (PMSM) is essential for exploring applications across various architectures while addressing ethical implications. Enhancing commonsense knowledge integration, especially in instruction following, can significantly improve evaluation frameworks. Emphasis should be placed on enhancing LLMs' emotional alignment and empathetic responses [7]. Expanding the LLM-Eval schema to cover additional conversational dimensions and testing on varied datasets will enhance evaluation comprehensiveness. Similarly, expanding datasets like MetaMath and UHGEval will contribute to more robust benchmarks, refining question generation processes to enhance LLM performance. Addressing limitations in GPT-4 and exploring paradigms beyond next-word prediction are essential for future research [10]. Increasing question diversity in evaluation frameworks can provide nuanced understanding of LLM proficiency across domains. Future work should refine ChatGPT's capabilities across diverse software engineering tasks [3] and expand dimensions of trustworthiness in practical applications [53]. These efforts underscore the importance of diverse datasets and refined metrics in fostering AI innovation. Integration and Enhancement of Evaluation Frameworks Integrating and enhancing evaluation frameworks for LLMs is critical for advancing their reliability across diverse domains. Future research should improve accuracy in identifying issues, particularly in programming education, where models must adapt to specific instructional needs [88]. This involves refining methodologies to meet domain-specific requirements, ensuring comprehensive assessments of LLM proficiency in specialized tasks. Emerging trends indicate a focus on developing adaptive models, necessitating enhanced benchmarks to assess practical reliability [26]. This includes integrating dynamic criteria that adjust to evolving capabilities, providing robust frameworks for real-world contexts. Refining frameworks requires prioritizing user-centric metrics that align assessments with human values. Moving beyond traditional metrics, such as BLEU, a holistic approach incorporating diverse metrics—accuracy, fairness, bias, and ethical considerations—will enable evaluations to reflect human-centric concerns, ensuring transparency and comprehensiveness [30,14,2,40]. Integrating subjective feedback with objective metrics fosters balanced evaluation processes, enhancing relevance and responsiveness. Continuous innovation in evaluation practices is essential for understanding emerging capabilities, enhancing performance in complex document classification, and adapting methods to align with human preferences and emotional intelligence [36,39,37,38,83]. Development of Robust Evaluation Metrics Developing robust evaluation metrics is essential for capturing LLM capabilities' complexities across domains. Future research should refine existing frameworks and integrate advanced prompt patterns with machine learning techniques to enhance adaptability [54], yielding deeper insights into LLMs' adaptability in diverse contexts, especially in educational applications [28]. Exploring new metrics that align closely with human judgments is crucial for enhancing evaluations of natural language generation systems [40]. Future directions could involve developing metrics that assess academic problem-solving complexities and incorporate diverse question types for nuanced understanding of performance [89]. Refining knowledge instillation methods and exploring additional metrics can further enhance understanding of LLMs' knowledge depth [50]. Emerging trends suggest a focus on adaptable models and exploring novel datasets to enhance domain generalization capabilities [32]. Refining metrics and exploring transformations to enhance understanding of LLM performance, particularly in reasoning tasks, is essential. Developing new metrics for moral reasoning, focusing on reducing bias and ensuring balanced representation, is vital for ethical model outputs [24]. These efforts underscore the critical role of innovative metrics in comprehensively assessing capabilities, advancing AI technology, and ensuring responsible deployment across diverse applications, including legal analysis and public affairs [37,50,39]. Advancements in Model Training and Optimization Recent advancements in model training and optimization have significantly enhanced LLM capabilities across applications. A promising direction involves refining the deliberation process in models like DELI, exploring applicability beyond mathematical reasoning [6]. This refinement could improve robustness and versatility, enabling LLMs to tackle a broader range of complex tasks. Developing hybrid models integrating in-domain and modality-based approaches holds promise for enhancing adaptability. By leveraging domain-specific knowledge in public affairs and legal analysis with multimodal capabilities, hybrid models can improve performance in complex tasks like topic classification, legal reasoning, and clinical decision-making [90,37,60,86,39]. Optimizing noising techniques and model architectures offers promising avenues for refining methodologies and enhancing robustness. Integrating complex tools into Tool Augmented Language Models (TALMs) represents significant advancement, with future work aimed at expanding capabilities to encompass additional task types. This expansion could enhance versatility and effectiveness, enabling broader functional performance with greater precision. For instance, LLMs are demonstrating capabilities in legal analysis, applying tax law with logical reasoning, and improving document analysis by classifying complex language into multiple topics. While LLMs show limited success in autonomously generating executable plans, they offer promise as heuristic guides for AI planners. Wider and deeper networks can lead to fairer evaluations, improving alignment with human preferences and accelerating processes [37,90,83,39]. In educational contexts, enhancing performance in complex mathematical reasoning is crucial for improving effectiveness as educational tools. Leveraging datasets like MATH, which provide challenging problems with solutions, can train models in generating answer derivations and explanations. Despite advancements, models struggle with accuracy, indicating a need for new innovations beyond scaling. Approaches such as Chain-of-Thought prompting and tool augmentation show promise in refining reasoning steps, although early-stage errors remain common. Interactive platforms reveal that while AI models can assist in problem-solving, they often diverge in correctness and perceived helpfulness, emphasizing human oversight. Studies on performance across curricula underscore AI's potential in educational settings while highlighting the necessity for models to communicate uncertainty and respond to corrections effectively [91,92,93,6,89]. This involves developing techniques to reduce bias and improve safety measures, focusing on persona usage trends and output toxicity effects. Reducing resource requirements for pre-training models like BERT and exploring applications in low-resource languages could significantly enhance accessibility across domains, including public affairs analysis, legal services, and recommender systems. This can improve transparency, accountability, and informed decision-making, increase legal analysis efficiency and accuracy, and enhance personalized recommendations by leveraging language understanding and generation capabilities [90,37,11,12,39]. Refining evaluation methods for generative models and exploring deployment in practical applications is vital for advancing training and optimization. These advancements underscore the critical role of ongoing innovation in model training and optimization, essential for developing robust and adaptable LLMs. These models increasingly handle complex tasks across various domains, such as legal analysis, document classification, and enhanced recommender systems. The continuous evolution of LLMs not only improves performance in specific applications but also enhances generalization and reasoning abilities, addressing the dynamic demands of AI technology and its applications [90,37,11,12,39]. Exploration of New Domains and Applications Exploring new domains and applications for LLM evaluation presents opportunities for advancing capabilities and versatility. Future research could expand benchmarks to include diverse data types and subtasks, providing robust frameworks that capture the full spectrum of capabilities [75]. This ensures comprehensive assessments across varied contexts, enhancing adaptability in complex task handling. Exploring diverse programming languages and more complex coding scenarios is crucial for advancing evaluation in technical domains [69]. By expanding benchmarks to encompass a wider variety of languages, researchers can gain insights into models' ability to generalize and apply knowledge across different environments, fostering innovation in software development. Applying collaborative game design frameworks to other creative domains offers promising avenues for expanding use in creative industries [94]. This exploration could enhance models' ability to generate innovative solutions and facilitate creative processes across artistic and design contexts. Further research should explore additional modalities and tasks, refining instruction-tuning methods to enhance standards [55]. This emphasizes integrating multimodal inputs and diverse task requirements into frameworks, ensuring comprehensive assessments of capabilities. Expanding benchmarks to include diverse tasks and models in evaluating LVLMs is essential for capturing multimodal application intricacies [51]. By broadening scope, researchers can better understand interactions between modalities and their impact on performance, ensuring robust assessments reflecting real-world complexity. The application of ChatGPT in diverse biomedical tasks presents opportunities for enhancing performance and expanding use in critical domains [95]. Future research should investigate ways to optimize outputs for biomedical applications, ensuring LLMs effectively address unique challenges and requirements. Exploring additional question categories to improve evaluations of truthfulness is vital for ensuring AI-generated content reliability [72]. By diversifying question types, researchers can gain deeper insights into models' ability to produce truthful responses across contexts. These explorations emphasize the need to broaden scope and application areas for evaluating LLMs, highlighting the importance of comprehensive assessments to meet evolving demands. LLMs have shown promise in enhancing document analysis, improving legal analysis capabilities in tax law, and advancing clinical knowledge applications. Innovative frameworks like LLM-Eval and WideDeep underscore the necessity of developing robust, multi-dimensional methods to align LLMs with human preferences and effectively meet diverse requirements [39,37,60,38,83]. Conclusion The examination of large language models (LLMs) underscores the imperative for developing robust and ethically grounded evaluation frameworks that support their deployment in natural language processing (NLP) applications. This survey highlights the importance of standardized benchmarks and the adoption of innovative methodologies to comprehensively evaluate the diverse capabilities of LLMs across different sectors. Significant insights demonstrate that refined benchmarking techniques can mitigate evaluation biases, leading to more accurate assessments of model performance. The exploration of emergent abilities and generalization metrics reveals the scalability and adaptability inherent in LLMs, emphasizing the need for continual advancement in evaluation strategies. Furthermore, addressing concerns related to biases, fairness, and ethical considerations remains vital for ensuring the responsible integration of LLMs into real-world applications. The survey advocates for ongoing research and development efforts in LLM evaluation, aiming to align these models with human values and societal expectations, thereby contributing to the advancement of AI technology in a manner that is both effective and ethically sound.",
  "reference": {
    "1": "2310.20499v2",
    "2": "2210.07197v1",
    "3": "2305.16837v1",
    "4": "2308.08833v2",
    "5": "2309.05922v1",
    "6": "2306.02408v1",
    "7": "2308.03656v6",
    "8": "2210.02414v2",
    "9": "2201.08239v3",
    "10": "2303.12712v5",
    "11": "2307.02046v6",
    "12": "2303.18223v16",
    "13": "2302.14045v2",
    "14": "2211.09110v2",
    "15": "2307.16789v2",
    "16": "2304.08354v3",
    "17": "2303.17466v2",
    "18": "2108.07258v3",
    "19": "2306.07799v2",
    "20": "2307.09705v1",
    "21": "2304.03738v3",
    "22": "2301.01768v1",
    "23": "2306.15261v1",
    "24": "2209.12106v2",
    "25": "2103.06268v2",
    "26": "2309.01219v3",
    "27": "2305.14982v2",
    "28": "2306.03090v1",
    "29": "2305.08322v3",
    "30": "2008.02275v6",
    "31": "2110.08193v2",
    "32": "2103.03097v7",
    "33": "1810.04805v2",
    "34": "1706.03741v4",
    "35": "2211.08073v4",
    "36": "2307.09042v2",
    "37": "2306.02864v2",
    "38": "2305.13711v1",
    "39": "2306.07075v1",
    "40": "1707.06875v1",
    "41": "2303.13835v4",
    "42": "2303.09038v3",
    "43": "1904.09675v3",
    "44": "1804.07461v3",
    "45": "2305.17926v2",
    "46": "2307.00184v4",
    "47": "2306.06331v3",
    "48": "2305.11262v1",
    "49": "2306.08302v3",
    "50": "2306.06264v1",
    "51": "2306.09265v1",
    "52": "2305.18486v4",
    "53": "2306.11698v5",
    "54": "2302.11382v1",
    "55": "2306.06687v3",
    "56": "2305.09645v2",
    "57": "2211.01910v2",
    "58": "2302.04023v4",
    "59": "2307.01135v1",
    "60": "2212.13138v1",
    "61": "2212.02774v2",
    "62": "2302.12095v5",
    "63": "2301.12868v3",
    "64": "2111.02840v2",
    "65": "2111.08181v1",
    "66": "2306.04618v2",
    "67": "2305.16934v2",
    "68": "2307.04657v3",
    "69": "2107.03374v2",
    "70": "1705.03551v2",
    "71": "2005.04118v1",
    "72": "2109.07958v2",
    "73": "2009.03300v3",
    "74": "2206.04615v3",
    "75": "2306.13394v5",
    "76": "2306.02549v1",
    "77": "2304.06364v2",
    "78": "2206.07682v2",
    "79": "2206.07682v2",
    "80": "2306.09296v3",
    "81": "2306.04181v2",
    "82": "2305.15074v3",
    "83": "2308.01862v1",
    "84": "2303.12528v4",
    "85": "2307.06281v5",
    "86": "2308.02490v4",
    "87": "2306.09212v2",
    "88": "2306.05715v1",
    "89": "2306.08997v2",
    "90": "2305.15771v2",
    "91": "2103.03874v2",
    "92": "2306.01694v2",
    "93": "2508.05614v1",
    "94": "2303.02155v2",
    "95": "2306.04504v3"
  },
  "chooseref": {
    "1": "2309.05922v1",
    "2": "2306.15261v1",
    "3": "2305.18486v4",
    "4": "2112.00861v3",
    "5": "2302.04023v4",
    "6": "2302.11382v1",
    "7": "2303.18223v16",
    "8": "2307.13692v2",
    "9": "2212.02774v2",
    "10": "1910.14599v2",
    "11": "2111.02840v2",
    "12": "2111.08181v1",
    "13": "2304.06364v2",
    "14": "2008.02275v6",
    "15": "2305.14387v4",
    "16": "2303.17466v2",
    "17": "1706.03762v7",
    "18": "2205.12615v1",
    "19": "2110.08193v2",
    "20": "2307.04657v3",
    "21": "2104.08663v4",
    "22": "2305.14982v2",
    "23": "2306.04181v2",
    "24": "1810.04805v2",
    "25": "1904.09675v3",
    "26": "2005.04118v1",
    "27": "2206.04615v3",
    "28": "2101.11718v1",
    "29": "2306.13651v2",
    "30": "2305.08322v3",
    "31": "2305.11262v1",
    "32": "2306.16636v1",
    "33": "2308.08833v2",
    "34": "2306.09212v2",
    "35": "2307.09705v1",
    "36": "2306.01499v1",
    "37": "2304.07619v6",
    "38": "2305.17306v1",
    "39": "2304.05613v1",
    "40": "2306.04563v1",
    "41": "2306.07799v2",
    "42": "2307.01135v1",
    "43": "2305.16837v1",
    "44": "2303.02155v2",
    "45": "2303.16421v3",
    "46": "2309.11737v2",
    "47": "2203.13474v5",
    "48": "2103.06268v2",
    "49": "2306.11698v5",
    "50": "1706.03741v4",
    "51": "1910.13461v1",
    "52": "2305.14938v2",
    "53": "2106.06052v1",
    "54": "2302.12297v1",
    "55": "2204.01906v1",
    "56": "2206.07682v2",
    "57": "2206.07682v2",
    "58": "2307.09042v2",
    "59": "2308.03656v6",
    "60": "1610.02413v1",
    "61": "2305.15268v1",
    "62": "2306.01694v2",
    "63": "2306.02408v1",
    "64": "2304.01938v1",
    "65": "2107.03374v2",
    "66": "2305.10355v3",
    "67": "2304.03439v3",
    "68": "2306.02549v1",
    "69": "2306.04504v3",
    "70": "2304.01457v2",
    "71": "2306.08997v2",
    "72": "2306.05715v1",
    "73": "2305.11700v2",
    "74": "2305.14251v2",
    "75": "1909.08593v2",
    "76": "2310.03214v2",
    "77": "1804.07461v3",
    "78": "2103.03097v7",
    "79": "2210.02414v2",
    "80": "2211.08073v4",
    "81": "2305.15074v3",
    "82": "2305.14693v1",
    "83": "2211.09110v2",
    "84": "2306.06331v3",
    "85": "2304.09542v3",
    "86": "2304.04339v2",
    "87": "2306.03090v1",
    "88": "2302.06476v3",
    "89": "2306.05685v4",
    "90": "2305.14975v2",
    "91": "2306.09296v3",
    "92": "2306.06687v3",
    "93": "2305.13711v1",
    "94": "2309.11998v4",
    "95": "2306.09265v1",
    "96": "2201.08239v3",
    "97": "2207.05221v4",
    "98": "2302.14045v2",
    "99": "2005.14165v4",
    "100": "2508.05614v1",
    "101": "2306.07075v1",
    "102": "2303.07142v3",
    "103": "2211.01910v2",
    "104": "2305.17926v2",
    "105": "2212.13138v1",
    "106": "2306.02864v2",
    "107": "2310.20499v2",
    "108": "2307.06281v5",
    "109": "2306.13394v5",
    "110": "2309.07915v3",
    "111": "2301.12307v2",
    "112": "2205.00445v1",
    "113": "2012.15723v2",
    "114": "2301.13867v2",
    "115": "2306.06264v1",
    "116": "2105.09938v3",
    "117": "2009.03300v3",
    "118": "2103.03874v2",
    "119": "2303.12528v4",
    "120": "2309.12284v4",
    "121": "1405.0312v3",
    "122": "2306.14565v4",
    "123": "2308.02490v4",
    "124": "2209.12106v2",
    "125": "2305.16934v2",
    "126": "2301.12868v3",
    "127": "1706.04599v2",
    "128": "2305.15771v2",
    "129": "2108.07258v3",
    "130": "2302.12095v5",
    "131": "2205.01068v4",
    "132": "2204.02311v5",
    "133": "2306.05087v2",
    "134": "2307.00184v4",
    "135": "2009.11462v2",
    "136": "2307.02477v3",
    "137": "2307.02046v6",
    "138": "2306.04618v2",
    "139": "2307.16125v2",
    "140": "1705.08500v2",
    "141": "2303.08896v3",
    "142": "2304.03738v3",
    "143": "2309.01219v3",
    "144": "2105.04054v3",
    "145": "2303.12712v5",
    "146": "2305.09645v2",
    "147": "1905.00537v3",
    "148": "2205.12255v1",
    "149": "2305.15269v3",
    "150": "2304.07333v1",
    "151": "2306.04610v1",
    "152": "2301.01768v1",
    "153": "2301.11596v5",
    "154": "2304.08354v3",
    "155": "2307.16789v2",
    "156": "2302.04761v1",
    "157": "2210.07197v1",
    "158": "2304.05335v1",
    "159": "2303.09038v3",
    "160": "1705.03551v2",
    "161": "2305.11171v3",
    "162": "2306.11507v1",
    "163": "2109.07958v2",
    "164": "2311.15296v3",
    "165": "2305.02182v3",
    "166": "2305.16151v1",
    "167": "2306.08302v3",
    "168": "2201.11990v3",
    "169": "2303.13835v4",
    "170": "1707.06875v1",
    "171": "2308.01862v1",
    "172": "2306.05783v3",
    "173": "2306.04926v1"
  }
}