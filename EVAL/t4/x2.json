{
  "survey": "This survey paper provides a comprehensive review of memory mechanisms in large language model-based agents, neural memory networks, cognitive architectures, and artificial intelligence agents. It highlights the critical role memory systems play in enhancing cognitive capabilities and performance, addressing limitations related to context length, information retention, and adaptability in AI systems. The paper systematically explores architectural frameworks, applications, and techniques that integrate memory mechanisms to optimize AI functionality across diverse domains. Key advancements such as parameter-efficient fine-tuning, model compression techniques, and Retrieval-Augmented Generation (RAG) are discussed, emphasizing their transformative impact on AI systems. The survey identifies significant challenges, including computational intensity, hallucinations in natural language generation, privacy concerns, and biases, and suggests future directions for research to address these issues. Ethical and social considerations are examined, underscoring the importance of responsible and equitable AI deployment. The paper concludes by reflecting on the ongoing advancements in memory mechanisms, paving the way for more sophisticated, contextually aware AI applications that emulate human-like intelligence and functionality.\n\nIntroduction Importance of Memory in Enhancing Cognitive Capabilities Memory mechanisms are essential for advancing cognitive functions and enhancing the performance of AI systems. The absence of long-term memory in Large Language Models (LLMs) limits their effectiveness in contexts requiring sustained interaction, such as personal companionship and psychological counseling [1]. Users often lack control over what agents remember, resulting in inadequate mental models and conversational breakdowns [2]. The need for conversational agents to maintain long-term consistency and flexibility underscores the importance of memory [3]. Mainstream LLMs struggle to utilize memory for complex multi-hop reasoning tasks, highlighting memory's role in cognitive enhancement [4]. Furthermore, memory mechanisms are crucial for improving the efficiency and effectiveness of generative agents in real-time social interactions [5]. Utilizing LLMs for financial tasks reveals challenges closely linked to memory capabilities [6], while the effective application of LLMs in decision-making tasks further emphasizes memory's significance [7]. Traditional recommender systems often lack sophisticated functionalities such as explanations and conversational abilities, which could be improved through memory integration [8]. Ensuring that LLM outputs align with human intentions and societal standards necessitates robust memory systems [9]. Memory systems can also help integrate external knowledge to mitigate limitations posed by outdated data and domain-specific constraints [10]. The transition from single-agent to multi-agent systems for complex problem-solving illustrates the necessity for effective memory integration [11]. Enhancing language models with reasoning skills and tool usage reveals gaps in understanding how these augmentations can enhance LMs [12]. Addressing social biases captured by LLMs from unprocessed training data is vital for fairness, where memory plays a crucial role [13]. The integration of LLMs with AI agents improves performance, showcasing advantages in natural language processing, knowledge storage, and reasoning capabilities compared to traditional AI agents [14]. The limited context windows of LLMs restrict their effectiveness in tasks requiring extensive memory [15]. RecMind illustrates the role of memory mechanisms in enhancing cognitive functions by leveraging external knowledge for zero-shot personalized recommendations [16]. The challenge of developing computational models that simulate believable human behavior in interactive environments remains significant [17], compounded by the inability of current methods to store, synthesize, and dynamically retrieve comprehensive experience records [17]. Personal LLM Agents seek to overcome the limitations of existing intelligent personal assistants, exploring how LLMs can enhance their functionalities [18]. The constraints of current Transformer-based LLMs in processing long-context inputs and outputs are critical for practical applications [19]. Maintaining consistency in long-range open-domain conversations with LLMs emphasizes memory's importance in improving conversational flow [20]. The phenomenon of hallucination in Natural Language Generation (NLG), where models produce unintended or nonsensical text, impacts the quality and reliability of generated content, highlighting the need for robust memory mechanisms [21]. The lack of effective methods for customizing conversational AI characters to meet users' social and emotional needs poses a significant challenge [22]. CharacterGLM offers advantages such as high customization, enhanced user engagement, and the ability to address diverse emotional requirements [22]. The development of LLM-based intelligent agents is viewed as a pathway toward artificial general intelligence (AGI), emphasizing memory's role in this evolution [23]. Enhancing information-seeking capabilities in machines by leveraging LLMs to mimic human-like cognitive processes, including critical thinking and planning, is crucial for efficient information retrieval [24]. This survey addresses the limitations of prior research on autonomous agents, examining how LLMs can enhance their capabilities to achieve human-like decision-making [25]. The role of memory in evaluating language models' ability to simulate human behavior underscores the importance of accurately assessing cognitive functions [26]. Enhancing language agents for effective operation in open-world game environments requires adaptability and long-term memory, addressing existing method limitations [27]. Structure of the Survey This survey is structured to comprehensively explore memory mechanisms in large language model-based agents, neural memory networks, cognitive architectures, and artificial intelligence agents. The paper begins with an Introduction that emphasizes the significance of memory in enhancing cognitive capabilities, followed by a detailed Background section tracing the evolution of large language models and the development of neural memory networks and cognitive architectures. The Definitions and Core Concepts section defines key terms and elucidates their interrelations and relevance to the survey. Subsequent sections delve into specific areas, starting with Memory Mechanisms in Large Language Models, which examines techniques for storing, retrieving, and utilizing information, focusing on compression and efficient inference methods. This is followed by a section on Neural Memory Networks, analyzing frameworks and architectures while discussing applications and techniques that enhance memory and cognitive functions. In Cognitive Architectures, we explore theoretical foundations and practical applications, emphasizing memory integration to enhance agent intelligence and decision-making. The section on Artificial Intelligence Agents investigates applying memory mechanisms to improve interaction, learning, and adaptability, focusing on enhancing interaction capabilities, adaptability, learning, and social interaction. The survey includes a Comparative Analysis of various memory mechanisms, highlighting strengths, weaknesses, and areas for improvement. Finally, we address Challenges and Future Directions, identifying key challenges and potential research directions, including ethical and social considerations. The paper concludes with a Conclusion summarizing key findings and reflecting on the future of memory mechanisms in AI systems. Throughout the survey, we emphasize Transformer-based LLM architectures, particularly those improving long-context processing, discussing pre-training, inference, evaluation necessities, and optimization toolkits [19].The following sections are organized as shown in . Background Evolution of Large Language Models Large language models (LLMs) have revolutionized artificial intelligence by enabling sophisticated text processing and generation across diverse applications. Early iterations of LLMs were limited by context length, affecting their performance in complex domains like biomedical applications and autonomous systems [28]. To address these constraints, instruction tuning has been employed using specialized datasets to enhance adaptability and domain performance [29]. The substantial computational demands of LLMs have driven the adoption of model compression techniques such as quantization, pruning, and knowledge distillation, facilitating scalability [30]. This transition from statistical models to neural architectures marks a significant advancement in AI capabilities. Despite these improvements, LLMs continue to face challenges, including generating toxic or inaccurate content and hallucinations that mislead users [31]. Benchmarks like SafeEdit aim to bolster trustworthiness in terms of reliability and safety [32]. Privacy concerns and the risk of inadvertently revealing sensitive information necessitate robust privacy-preserving methods and ethical frameworks [33]. Moreover, LLMs often struggle with reasoning and planning tasks, failing to incorporate environment-specific rewards [34]. Enhancing knowledge retrieval and augmentation is crucial to overcoming these limitations, especially in the absence of dedicated memory units [35]. The application of LLMs as autonomous agents with human-like decision-making capabilities underscores their growing significance across various sectors, necessitating improvements in reasoning abilities [29]. FinMem exemplifies the integration of layered memory structures within LLMs, prioritizing financial data for decision-making and demonstrating adaptability in specialized contexts [36]. Comparative analyses, such as those by [37], highlight the advancements of LLMs over prior pre-trained language models (PLMs), showcasing significant improvements in capabilities and performance. These developments underscore the rapid progression of LLMs, which are set to transform numerous fields by leveraging enhanced cognitive and interactive capabilities. As AI systems evolve, integrating advanced LLMs into domains like psychological practice expands their application scope, necessitating rigorous validation in real-world contexts [29]. Furthermore, LLMs are increasingly utilized in software engineering activities, including coding, design, requirements, repair, refactoring, performance improvement, documentation, and analytics, illustrating their extensive impact [30]. Neural Memory Networks and Cognitive Architectures Neural memory networks and cognitive architectures are central to advancing artificial intelligence systems, enhancing learning, adaptability, and decision-making capabilities. The integration of neural memory networks is crucial for tackling complex tasks, as demonstrated by embodied lifelong learning agents that continuously explore and acquire skills in dynamic environments like Minecraft [38]. This underscores the importance of memory networks in facilitating adaptive learning processes. In software engineering, LLMs utilize neural memory networks to address technical challenges [39]. These networks improve efficiency and effectiveness in recommendations and decision-making processes within AI systems while addressing ethical considerations [40]. Cognitive architectures incorporating memory processing and decision-making enhance language agents' functionality in gaming environments, showcasing the versatility of these systems [27]. Beyond dynamic environments and software engineering, neural memory networks are vital for managing diverse tasks in open-world settings [41]. The structured typology of adversarial attack concepts categorizes research based on learning structures and attack methodologies, emphasizing the need for robust memory systems within cognitive architectures [42]. Training strategies introduced by GLM-130B enhance stability and efficiency, playing a vital role in developing neural memory networks and cognitive architectures in AI systems [30]. These advancements are fundamental for optimizing algorithmic and system-level processes, as outlined in frameworks categorizing research in these domains [32]. Moreover, categorizing Knowledge Management and Enhancement (KME) techniques into various strategies provides insights into the advantages and limitations of neural memory networks, highlighting their metrics and applications [43]. Recent advancements in neural memory networks and cognitive architectures have significantly reshaped artificial intelligence, leading to improved cognitive capabilities and innovative applications across diverse domains. Integrating large language models and AI agents enhances natural language processing, knowledge storage, and reasoning abilities. Notably, AI's application in psychology promises to accelerate progress in the field while reducing labor costs and social stress. Innovative classification schemes for AI memory systems offer new perspectives on agent design, establishing a solid foundation for future advancements. However, challenges persist, particularly in adapting to paradigm shifts and upgrading medical instrumentation for clinical and real-world applications [44,14]. Definitions and Core Concepts Definitions and Core Concepts Understanding AI's memory mechanisms involves defining key terms: memory mechanisms, neural memory networks, cognitive architectures, and AI agents. These concepts provide a framework for comprehending AI's emulation of cognitive processes, enhancing functionality and integrating advanced capabilities such as reasoning, tool use, and task decomposition. Leveraging large language models (LLMs) and foundation models, AI significantly impacts psychology, natural language processing, and complex problem-solving, promising reductions in labor costs and alleviation of social stress while addressing interpretability, consistency, and scalability challenges [44,45,12,14]. Memory mechanisms in AI encompass processes for storing, retrieving, and utilizing information, essential for executing complex tasks. Innovations like the RET-LLM framework incorporate scalable read-write memory units, enabling LLMs to extract, store, and recall knowledge as triplets, enhancing temporal performance. Developments such as MemoryBank provide LLMs with long-term memory, adapting to user personalities and improving interactions based on human memory theories like the Ebbinghaus Forgetting Curve. These advancements highlight the evolving AI memory landscape, enhancing performance in areas like question answering and psychological counseling while distinguishing LLM-based agents through superior natural language handling and reasoning abilities [46,1,14]. Such mechanisms are crucial for overcoming limitations related to context length and information retention, facilitating sophisticated interactions and decision-making. Neural memory networks are specialized architectures that enrich AI systems with memory capabilities, enabling continuous learning and adaptation. They facilitate information storage and retrieval, enhancing task performance requiring long-term memory and context awareness. In dynamic environments, LLMs excel by integrating external knowledge, adapting through efficient knowledge editing, and enhancing information retrieval via advanced natural language understanding and generation [47,48,49,10,50]. Cognitive architectures model human-like cognitive processes within AI systems, integrating functions such as perception, memory, and decision-making to create agents capable of complex reasoning and problem-solving. The incorporation of memory mechanisms within these architectures enhances their ability to simulate human cognition accurately. AI agents utilize memory mechanisms, neural memory networks, and cognitive architectures to autonomously perform tasks. These agents leverage memory to enhance interaction capabilities, learning processes, and adaptability across various environments, from social interactions to complex decision-making scenarios. The interplay of these concepts is central to this survey, exploring how memory mechanisms enhance AI systems' cognitive capabilities. The integration of pre-training, fine-tuning, prompting, and text generation techniques further underscores the sophistication of large pre-trained language models [51]. Understanding these core concepts is vital for advancing AI systems toward more human-like intelligence and functionality. In recent years, the exploration of memory mechanisms in large language models (LLMs) has garnered significant attention within the field of artificial intelligence. A comprehensive understanding of these mechanisms is crucial for improving model efficiency and application. illustrates the hierarchical structure of AI memory mechanisms, categorizing them into memory mechanisms, neural memory networks, and cognitive architectures. Each category highlights key components and innovations, emphasizing their role in enhancing AI systems' cognitive capabilities. This figure also highlights the ongoing integration efforts that enhance functionality and facilitate specialized applications, providing a visual representation that complements the discussion presented in this review. By examining these elements, we can better appreciate the advancements in LLM architectures and their implications for future research. Memory Mechanisms in Large Language Models Compression and Efficient Inference Advanced compression techniques and efficient inference processes are crucial for managing the computational and memory demands of large language models (LLMs) [47]. Methods such as pruning, quantization, knowledge distillation, and low-rank approximation enhance LLM performance while mitigating resource constraints [52]. Bayesian approaches like the Laplace-LoRA method improve model calibration by estimating parameter uncertainty through Laplace approximation, thereby refining fine-tuned LLMs [53]. Inference challenges in pretrained language models, including high energy consumption and latency, impede performance optimization [47]. Techniques such as Retrieval-Augmented Generation (RAG), exemplified by RTLFixer, enhance inference efficiency by optimizing error correction and debugging processes [54]. The Meta-Agent Tuning (MAT) framework further optimizes smaller LLMs for information-seeking tasks, surpassing traditional systems in inference efficiency [24]. Innovative methods like ReAct integrate reasoning and acting, enabling LLMs to interact with external sources and generate task-specific actions, thereby improving inference processes [55]. Benchmarks such as LongBench standardize datasets for comprehensive LLM evaluations, aiding in the refinement of inference techniques [28]. The GLM-130B model demonstrates significant performance enhancements over predecessors like GPT-3, reflecting advancements in inference efficiency within LLMs [30]. Despite these advancements, challenges in understanding user intent, task planning, tool usage, and personal data management continue to hinder the scalability of intelligent personal assistants (IPAs) [18]. A proposed benchmark introduces a novel metric of additivity to measure perturbation effects, indicating areas for further investigation in inference efficiency [31]. The integration of compression techniques such as quantization, pruning, and knowledge distillation, along with advancements in efficient inference strategies, underscores the importance of optimizing LLM architecture. These approaches address the substantial computational demands and memory requirements of LLMs, particularly in resource-constrained environments. Enhancing efficiency and adaptability through these methods promotes the scalability and applicability of LLMs across diverse domains while mitigating environmental impacts and operational costs associated with deploying large models. Furthermore, the exploration of tuning-free algorithms and the emphasis on preserving model versatility and generalization post-compression reflect ongoing efforts to refine LLMs for broader and more sustainable use [47,56,57,58,52]. Integration with Large Language Models Integrating memory mechanisms into large language models (LLMs) enhances their functionality, enabling improved management of complex tasks with greater efficiency and adaptability. This integration is evident in applications such as debugging, communication, and personality adaptation. For instance, RTLFixer employs interactive feedback to enhance LLM debugging capabilities, allowing iterative learning and refinement of problem-solving strategies [54]. Maintaining context within conversational models is critical, and MemoChat utilizes structured memos to store and retrieve past dialogues, significantly improving conversational coherence over extended interactions [20]. This ability to recall previous interactions enhances LLMs' capacity to provide contextually relevant responses, boosting user engagement and satisfaction. The integration of retrieval, generation, and augmentation techniques within Retrieval-Augmented Generation (RAG) systems optimizes the retrieval and application of previously acquired knowledge [59]. In specialized domains, the incorporation of domain-specific knowledge is exemplified by Radiology-GPT, which utilizes instruction-tuning to refine radiological diagnosis and communication [60]. In personality-focused applications, tools like PersonalityEdit leverage Social Psychology theories to guide the modeling of personality traits, facilitating more personalized interactions [61]. This enables LLMs to tailor responses based on specific personality traits, enhancing the relevance and personalization of interactions. The ReAct framework integrates reasoning and acting capabilities, allowing LLMs to generate comprehensive reasoning traces alongside task-specific actions, providing a nuanced approach to problem-solving [55]. This dual integration ensures that LLMs not only execute tasks but also elucidate their decision-making processes, enhancing transparency and user understanding. Despite these advancements, traditional fine-tuning methods incur significant computational costs, risking the loss of valuable pre-trained knowledge and creating inefficiencies in frequent updates [43]. Nonetheless, ongoing refinement and integration of memory mechanisms into LLMs foster improved adaptability, efficiency, and scalability, paving the way for more sophisticated and contextually aware AI applications. As illustrated in , the integration of memory mechanisms, domain-specific applications, and personality and reasoning frameworks into LLMs highlights key advancements in each category. The LARP framework facilitates the interaction of language agents within open-world games by incorporating memory processing and decision-making capabilities, showcasing potential for enhanced agent functionality in dynamic environments [27]. Additionally, JARVIS-1 illustrates the use of multimodal memory for planning and self-improvement, integrating memory mechanisms to enhance LLM capabilities [41]. In collaborative settings, DMC employs dialogue among robots equipped with LLMs to generate task strategies and waypoint paths, facilitating enhanced coordination and planning [35]. Furthermore, FinMem's three-module system customizes agent characteristics, hierarchically processes financial data, and converts insights into actionable investment decisions, highlighting the adaptability of LLMs in specialized domains [36]. These integration efforts underscore the evolving role of memory-enhanced LLMs in achieving advanced cognitive functions across diverse domains. Neural Memory Networks Frameworks and Architectures Neural memory network architectures are foundational for enhancing AI systems' cognitive performance and adaptability. The Voyager architecture exemplifies this by incorporating a dynamic skill library, facilitating continuous learning and adaptation through the recall of diverse experiences [38]. Integrating traditional software engineering techniques with large language models (LLMs) creates a hybrid framework that significantly boosts reliability, efficiency, and effectiveness in LLM-based applications [39]. This fusion leverages the strengths of both domains, enabling robust AI systems to manage complex tasks efficiently. These architectures excel in handling billion-scale datasets, offering substantial improvements in speed and memory efficiency, crucial for processing extensive data inputs [62]. Such capabilities ensure sustained performance and scalability in data-intensive applications. In generative AI systems, initializing agents with memory systems for storing experiences and employing planning algorithms based on these memories are crucial components [17]. These elements allow AI systems to utilize past experiences for informed decision-making, enhancing autonomous operation. A unified framework for LLM-based autonomous agents illustrates the integration of diverse methodologies into a cohesive perspective [25]. This framework categorizes existing research, offering insights into enhancing AI systems' functionality and cognitive capabilities. As illustrated in , the figure presents the hierarchical structure of frameworks and architectures in AI systems, emphasizing neural memory networks, hybrid frameworks integrating large language models, and performance optimization strategies for handling billion-scale data. It highlights key methodologies and their contributions to enhancing AI's adaptability, reliability, and efficiency across various applications. Collectively, these frameworks and architectures highlight the transformative potential of neural memory networks in advancing AI systems. By integrating LLMs with innovative memory units, these systems can efficiently learn, adapt, and perform complex tasks across diverse applications, including psychological analysis, natural language processing, and AI agent development. This progress enhances AI's capabilities in language handling, knowledge storage, and reasoning while addressing challenges related to service efficiency, latency, and throughput, reshaping AI deployment in real-world scenarios [46,44,32,14]. Applications and Techniques Neural memory networks are pivotal in enhancing AI systems' memory and cognitive functions, offering diverse applications and techniques that improve performance and adaptability. The Character-LLM application trains LLMs to simulate historical figures with depth and emotional complexity, enriching models' ability to engage in nuanced interactions [63]. This technique demonstrates how memory systems enhance AI agents' emotional and contextual understanding. The Synapse framework utilizes exemplars to improve decision-making processes, underscoring the critical role of exemplar memory in advancing cognitive functions through trajectory prompting, enhancing generalization to novel tasks [64]. This highlights memory systems' importance in facilitating adaptive learning and decision-making in dynamic environments. The TPTU framework enhances API selection and task planning accuracy, essential for improving cognitive functions in AI systems [65]. By optimizing task planning, TPTU contributes to developing AI systems capable of efficiently navigating complex scenarios, leveraging memory networks to refine strategies. The RTLFixer framework exemplifies autonomous debugging processes, utilizing memory systems to correct errors and enhance cognitive functions within AI systems [54]. This technique underscores neural memory networks' capacity for continuous learning and improvement, enabling AI systems to autonomously adapt and optimize performance. JARVIS-1 showcases neural memory networks' integration in handling a wide range of tasks, emphasizing self-improvement through lifelong learning [41]. This framework demonstrates memory-enhanced AI systems' potential to operate autonomously and adaptively, continuously refining capabilities and expanding application scope. These applications and techniques highlight neural memory networks' transformative impact in advancing AI systems, enabling complex task performance with enhanced efficiency across diverse domains. This progression is evident in integrating LLMs with AI agents, improving natural language processing and reasoning capabilities. Incorporating psychological principles into AI models unlocks unprecedented capabilities in social media analytics and clinical insights, promising accelerated progress while reducing labor costs and social stress. Despite challenges like computational intensity and domain-specific limitations, innovative solutions in memory systems and knowledge integration pave the way for efficient LLM deployment, reshaping AI's future across various sectors [44,32,10,14]. Cognitive Architectures Theoretical Foundations of Cognitive Architectures Cognitive architectures form the structural basis for AI systems, simulating human cognition by integrating perception, memory, and decision-making. Embodied cognition, a key theoretical framework, suggests that cognitive processes are influenced by the body's interactions with the environment, vital for AI's continuous learning in dynamic settings [38]. Memory systems are central to these architectures, enabling effective information storage, retrieval, and application. For example, JARVIS-1 employs multimodal memory to enhance planning and self-improvement, facilitating complex reasoning akin to human cognition [41]. Neural computation principles further guide the creation of architectures that emulate human neural processes, as seen in neural memory networks that support adaptive learning and contextual decision-making [39]. As illustrated in , the core components of cognitive architectures are highlighted, emphasizing embodied cognition, memory systems, and social interaction as fundamental categories. The relationships between these elements underscore the integration of continuous learning, multimodal memory, and task strategies to enhance AI's cognitive capabilities. Social interaction theories contribute to cognitive architectures by emphasizing communication's role in cognitive processes. The DMC framework demonstrates how robots with large language models (LLMs) can collaborate to devise task strategies, highlighting social dynamics in boosting cognitive capabilities [35]. Integrating psychological principles with large-scale AI models positions cognitive architectures to revolutionize AI, enhancing intelligence and functionality in domains like clinical applications, social media analytics, and tool learning. This comprehensive approach reduces labor costs, alleviates social stress, and addresses challenges related to trustworthy tool use and personalization, resulting in AI systems that are more context-aware and capable of meaningful interactions [44,24,14,66,45]. Practical Applications of Cognitive Architectures Cognitive architectures significantly enhance AI systems' ability to perform complex tasks, with applications in robotics, autonomous agents, and interactive gaming. In robotics, these architectures enable autonomous systems to interact with dynamic environments. The DMC framework, for instance, uses dialogue among robots with LLMs to collaboratively generate task strategies, improving coordination and planning [35]. In gaming, cognitive architectures allow AI agents to adaptively interact with complex systems, utilizing memory mechanisms to enhance decision-making. The LARP framework illustrates this by enabling language agents to function effectively in open-world games, showcasing cognitive architectures' role in adaptive learning and strategic planning [27]. Cognitive architectures also empower autonomous agents to make human-like decisions. JARVIS-1, for example, uses multimodal memory for planning and self-improvement, enhancing AI systems' adaptability across tasks [41]. In finance, cognitive architectures support agent customization and hierarchical processing of domain-specific data. The FinMem framework exemplifies this by converting insights into actionable investment decisions, showcasing AI's adaptability in specialized applications [36]. Advances in psychological AI models and multimodal interaction systems highlight cognitive architectures' transformative impact, enhancing AI efficiency and effectiveness across domains like social media analytics and clinical insights. By incorporating psychological principles and multimodal interactions, AI systems become increasingly adaptive and context-aware, capable of processing diverse data types. This evolution accelerates progress in fields like psychology, reduces labor costs, and mitigates social stress. Embodied agent AI systems further enhance interactions by predicting actions and incorporating external knowledge, addressing challenges like the hallucinations of large foundation models [44,66]. Integration of Memory Mechanisms in Cognitive Architectures Integrating memory mechanisms within cognitive architectures is crucial for enhancing AI systems' efficiency and functionality, simulating human-like decision-making processes. In economic simulations, advanced LLM capabilities replicate human decision-making, improving economic models' realism [67]. Current methods for integrating memory mechanisms follow stages: architecture design, capability enhancement, efficiency optimization, and security measures [18]. This structured approach ensures seamless memory mechanism incorporation, leading to robust and secure AI systems. Dynamic information storage and retrieval enabled by memory integration facilitate continuous learning and adaptation, addressing LLMs' traditional lack of long-term memory necessary for sustained interactions in applications like personal companionship and psychological counseling. Novel frameworks, such as MemoryBank and RET-LLM, introduce scalable memory units that synthesize and recall relevant knowledge, enhancing user experience by adapting to individual preferences and managing time-dependent information [46,24,1,14,4]. These systems draw inspiration from biological and symbolic memory, enabling complex reasoning and enriched interactions. Furthermore, memory mechanisms support AI systems in complex reasoning and problem-solving tasks. Advanced memory systems enhance AI agents' ability to process and analyze large information volumes, improving natural language handling and reasoning abilities. Recent LLM advancements enable AI agents to emulate human cognitive functions, efficiently understanding queries, referencing external documents, and executing time-aware actions. This progression equips AI agents to tackle complex tasks across domains, contributing to AI technology's future advancement [44,24,14]. These advancements underscore memory mechanisms' critical role in enhancing cognitive architectures. Leveraging multimodal interactions and grounded environments, AI systems achieve sophisticated, contextually aware operations, paving the way for intelligent agents capable of human-like cognitive functions, such as reasoning and executing actions based on multi-sensory inputs. This evolution enhances agents' interactions in physical and virtual environments and mitigates hallucinations in large models, advancing AI technology across fields [44,24,66,14]. Artificial Intelligence Agents Enhancing Interaction Capabilities Integrating memory mechanisms into artificial intelligence agents is crucial for enhancing their interaction capabilities, allowing for more effective engagement in complex interactions. These systems enable AI agents to store and recall past interactions, fostering coherent and contextually relevant responses. This is particularly significant in role-playing tasks, where metrics like Accuracy and F1-score evaluate character emulation and interaction quality [68]. Enhanced memory capabilities permit AI agents to emulate characters with greater precision, enriching interaction depth. illustrates the key aspects of enhancing interaction capabilities in AI, focusing on role-playing tasks, memory mechanisms, and AI applications in various fields. Memory systems also ensure consistency and coherence in prolonged conversations, vital for applications demanding sustained engagement, such as personal companions and interactive gaming. Systems like NarrativePlay exemplify this by enabling users to role-play fictional characters, generating human-like responses through Large Language Models (LLMs) [69]. These advancements highlight AI's transformative potential in clinical nursing and social media analytics, emphasizing performance validation and adaptive AI technologies' role in reducing labor costs and alleviating social stress [44]. The ability to recall and integrate past experiences significantly boosts user satisfaction and engagement. Beyond conversational capabilities, memory mechanisms are vital for AI agents undertaking complex reasoning and decision-making tasks. Recent research shows these systems enable LLM-based AI agents to manage natural language more effectively, store and retrieve knowledge, and apply sophisticated reasoning. Innovations like MemoryBank equip AI systems with human-like long-term memory, allowing adaptation to user interactions, recall of relevant information, and understanding of user personalities [24,1,12,14]. This supports the development of empathetic AI companions capable of planning, reflecting, and executing actions with external tools, addressing traditional language models' limitations and paving the way for more generalized and autonomous AI systems. These advancements underscore memory mechanisms' transformative impact on AI agents' interaction capabilities, enabling more effective engagement across diverse applications. By integrating large language models with sophisticated memory systems like MemoryBank, AI agents can manage natural language processing, knowledge storage, and reasoning more efficiently. These systems facilitate long-term interactions in applications such as personal companionship and psychological counseling, where AI agents recall relevant memories, adapt to user personalities, and provide empathetic responses. Innovations like Memory Sandbox enhance user experience by offering interactive memory management, ensuring contextually relevant interactions and minimizing conversational breakdowns. Such developments improve AI agents' performance across various fields, including psychology and social media analytics, providing valuable insights for future research and technological advancements [44,2,1,14]. Adaptability and Learning Memory mechanisms are pivotal for enhancing artificial intelligence agents' adaptability and learning processes, enabling continuous refinement in dynamic environments. Integrating memory systems allows AI agents to store and retrieve past experiences, facilitating adaptive learning through knowledge accumulation and application over time. The Voyager architecture exemplifies this by incorporating a growing skill library for behavior storage, enabling learning and adaptation in complex settings like Minecraft [38]. Leveraging memory systems for adaptive learning is crucial for AI agents across diverse domains, including software engineering and interactive gaming. In software engineering, combining traditional methodologies with LLMs enhances AI systems' reliability and effectiveness, allowing navigation through intricate problem-solving scenarios with greater precision [39]. This highlights memory mechanisms' importance in facilitating adaptive learning processes that improve AI agents' efficiency and effectiveness. Memory systems also support developing AI agents capable of complex reasoning and decision-making. By dynamically storing and retrieving relevant information, AI agents can analyze inputs more effectively, facilitating informed and strategic learning processes. Integrating LLMs enhances their ability to tackle complex problem-solving and continuous adaptation, making them indispensable for applications requiring autonomous and efficient operation across diverse environments. These agents benefit from advanced natural language processing, robust knowledge storage, and sophisticated reasoning capabilities, allowing effective navigation and interaction within both physical and virtual spaces. By leveraging multimodal interactions and incorporating external knowledge, AI agents can predict and execute actions, adapt to user intent, manage personal data, and mitigate inaccuracies in environmental understanding, providing promising solutions in fields ranging from psychology to personal assistance [18,44,66,14]. Integrating memory mechanisms enhances AI agents' ability to simulate human-like cognitive processes, improving adaptability and learning capabilities. By incorporating advanced memory systems and LLMs, cognitive architectures can enhance AI agents to exhibit human-like intelligence and functionality. This integration improves natural language processing, knowledge storage, and reasoning abilities and enables planning, reflection, and interaction with the world in a more contextually aware manner. These advancements pave the way for sophisticated applications, such as the KwaiAgents system, which efficiently understands user queries, utilizes external documents, and employs time-aware tools for comprehensive responses. These innovations lay a solid foundation for future AI agent technology, offering valuable insights and directions for further research in the field [24,14]. These advancements highlight memory mechanisms' transformative impact on AI agents' adaptability and learning processes, particularly those based on LLMs. By integrating innovative memory systems like MemoryBank, inspired by psychological theories such as the Ebbinghaus Forgetting Curve, AI agents can continuously refine their capabilities and respond effectively to dynamic environments. This enables handling complex tasks across diverse applications, including psychological counseling and long-term companionship, while enhancing natural language processing, knowledge storage, and reasoning capabilities. Such progress underscores AI's potential to revolutionize fields like psychology and social media analytics, setting the stage for future advancements in AI agent technology [44,1,14]. Social Interaction and Engagement Integrating memory mechanisms within artificial intelligence agents is crucial for enhancing social interaction and engagement capabilities, allowing these agents to emulate human-like communication and establish meaningful connections with users. Memory systems enable AI agents to recall past interactions, facilitating coherent and contextually relevant dialogues essential for maintaining long-term engagement in social settings [20]. This capability is particularly beneficial in personal companions and interactive gaming environments, where sustained interaction and adaptability are crucial for user satisfaction. In role-playing tasks, memory mechanisms significantly enhance character emulation, allowing AI agents to engage users with greater depth and emotional complexity [68]. By leveraging stored experiences and character traits, AI agents can deliver personalized interactions, enhancing user engagement and satisfaction, vital for applications requiring emotional intelligence and nuanced communication. Maintaining consistency in long-range conversations is essential for AI agents in social domains. Memory systems facilitate the recall of previous dialogues, ensuring contextually relevant responses over extended interactions, thereby improving conversational coherence and user engagement [20]. This capability is crucial for applications requiring continuous dialogue and adaptive learning, enabling AI agents to operate effectively in dynamic social environments. Integrating advanced memory mechanisms, such as MemoryBank and symbolic databases, significantly enhances AI agents' capabilities in complex reasoning and decision-making tasks within social contexts. These mechanisms enable AI agents to store, retrieve, and update information effectively, allowing adaptation to user personalities and providing empathetic responses in long-term interactions. By leveraging LLMs as cognitive cores, AI agents can mimic human-like memory behaviors, perform multi-hop reasoning, and utilize external tools to deliver comprehensive and informed responses. This advancement supports psychological applications and empowers AI agents to operate efficiently in diverse social environments, offering promising solutions to societal challenges [44,24,1,14,4]. By storing and retrieving relevant information, AI agents can analyze and interpret social cues more effectively, facilitating informed and strategic interactions crucial for applications requiring sophisticated problem-solving and adaptive learning. These advancements underscore memory mechanisms' transformative impact on AI agents' social interaction and engagement capabilities, enabling them to emulate human-like communication and establish meaningful connections across various applications. Integrating memory systems into AI agents significantly enhances their ability to process and interpret multimodal data, such as visual stimuli, language inputs, and environmental context, crucial for achieving sophisticated and contextually aware social interactions. This advancement leverages LLMs' strengths and innovative memory mechanisms to enable AI systems to engage in human-like communication, recall relevant information over time, and adapt to user personalities, broadening their application in personal companionship, psychological counseling, and interactive virtual environments. Addressing traditional AI agents' limitations and incorporating elements like MemoryBank and agentic AI frameworks, these systems pave the way for more empathetic, intelligent, and dynamic interactions that mimic human cognitive capabilities and social behaviors [44,24,1,14,66]. Comparative Analysis Comparative Analysis of Memory Mechanisms The comparative analysis of memory mechanisms in AI systems reveals diverse approaches, each with distinct advantages and limitations. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a promising alternative to full fine-tuning, offering comparable performance while reducing resource demands [70]. This highlights the potential for optimizing memory mechanisms to enhance AI systems' efficiency and scalability. Model compression techniques, including quantization, pruning, and knowledge distillation, exhibit varying effectiveness, each presenting unique benefits and trade-offs [58,56]. While these methods aim to reduce computational overhead, their impact on model accuracy and efficiency varies, necessitating careful selection based on specific application requirements. The integration of memory mechanisms within AI systems also shows variability in effectiveness, with different knowledge integration methods yielding diverse outcomes [10]. Some approaches excel in particular contexts, outperforming others in adaptability and functionality, underscoring the need for context-aware memory integration strategies. Evaluations using financial NLP benchmarks demonstrate the competitive performance of models like GPT-3.5, GPT-4, and Claude-2 against specialized models such as InvestLM [71]. This comparison highlights the adaptability of memory mechanisms in specialized domains, showcasing their potential to enhance decision-making in financial applications. These insights collectively underscore the transformative impact of memory mechanisms on AI systems, emphasizing the importance of optimizing these mechanisms to improve performance across diverse applications. Integrating large language models with AI agents offers significant advantages in natural language processing, knowledge storage, and reasoning, highlighting the necessity for innovative memory system designs. Moreover, advancements in efficient serving methodologies for generative models are crucial to addressing challenges related to computational intensity and memory consumption, reshaping AI's role in fields like psychology, social media analytics, and clinical applications [44,32,14]. This comparative analysis provides a comprehensive understanding of the strengths and limitations of various memory mechanisms, paving the way for more sophisticated and contextually aware AI systems. Challenges and Future Directions Challenges in Current Memory Mechanisms The integration of memory mechanisms in AI systems presents challenges impacting scalability, efficiency, and adaptability. A significant issue is the dependency on training data quality and volume, which can hinder the performance of memory-enhanced models. Computational challenges during Knowledge Management and Enhancement (KME) processes risk knowledge degradation during updates, highlighting the need for robust data generation techniques [47]. Effective memorization and retrieval of past dialogues remain problematic, causing inconsistencies in conversational flow [35]. Syntax errors in code produced by large language models (LLMs) complicate debugging, necessitating advanced error correction methodologies. Hallucinations—where models generate seemingly factual but ungrounded content—pose reliability challenges, with unresolved issues concerning their causes and proposed solutions [31]. The negative perception of hallucinations may deter exploration of their creative applications, suggesting a paradigm shift in understanding these phenomena. Privacy and data security concerns arise in personal assistance applications, as existing methods often lack transparency and validation, impairing trust in autonomous systems [37]. Current studies frequently fail to address privacy complexities comprehensively, potentially overlooking emerging threats. Challenges also exist in assessing personality traits across diverse contexts, affecting benchmarks for personality-driven interactions. Multi-role collaboration and communication issues emphasize the need for robust methods to utilize environmental feedback effectively. The performance of memory mechanisms in LLMs varies with query complexity and model capabilities, underscoring the importance of context-aware integration strategies. Conventional neural memory mechanisms struggle with complex reasoning tasks due to their approximate nature and error accumulation. Enhancements such as symbolic memory frameworks or hierarchical memory management techniques can improve handling of extended contexts and complex reasoning [46,72,4,15,73]. Current benchmarks inadequately assess long-context understanding, limiting evaluation of models designed for longer sequences. The inability of current methods to integrate reasoning traces with action generation results in significant issues like hallucination and error propagation, critical in real-world applications such as medical record summarization or financial analysis, where accuracy is paramount. Techniques like Retrieval Augmented Generation and ReAct enhance human interpretability and trustworthiness while mitigating hallucination and error propagation [55,74,75,21]. The rapid proliferation of algorithms such as pruning, quantization, and knowledge distillation advances the field but obscures emerging trends and fundamental concepts. Standardized evaluation strategies are essential for assessing the real-world applicability and efficiency of these techniques, particularly for LLMs demanding significant computational resources and presenting integration challenges in resource-constrained environments [56,10,58,52]. Existing methods focusing on individual simulations fail to capture human behavior variability and complexity in group settings. Researchers face ongoing challenges such as continuous knowledge updates, domain-specific information integration, and hallucination management in generated content. Training large-scale models reveals complexities like loss spikes and divergence, reflecting current memory mechanism challenges within AI systems. Studies often fall short in addressing data biases and operational challenges of LLMs, particularly concerning latency and throughput trade-offs and scalability across diverse deployment environments. These challenges collectively highlight the urgent need for advancements in memory mechanisms to enhance computational efficiency, adaptability, and evaluation capabilities in LLMs. The RET-LLM framework addresses these needs by integrating a scalable and interpretable read-write memory unit, improving knowledge storage and retrieval for tasks like temporal-based question answering. The survey on efficient generative LLM serving emphasizes optimizing system designs and algorithms to overcome computational intensity and memory consumption challenges, ensuring low latency and high throughput in AI deployments [46,32]. Overcoming these obstacles is vital for developing robust, scalable, and contextually aware AI systems that effectively leverage memory mechanisms across diverse applications. Future research could further refine existing methods to enhance their applicability to various models or datasets. Future Directions The future of memory mechanisms in AI systems offers promising research avenues to improve interpretability, efficiency, and applicability across diverse domains. Developing robust retrieval techniques and transparent reasoning processes in LLMs is crucial for creating systems that are both interpretable and reliable [59]. Expanding benchmarks to encompass additional languages and dialects broadens the applicability of memory mechanisms [30]. Research should focus on establishing robust evaluation platforms integrating capabilities, alignment, and safety frameworks, while exploring new applications of LLMs in economic contexts to enhance effectiveness in financial domains [29]. Addressing gaps in reasoning and tool usage integration within AI systems can significantly bolster cognitive capabilities [29]. Future studies might refine agent interaction accuracy and expand frameworks to accommodate more complex user behaviors [34]. Creating robust debiasing strategies and evaluation frameworks is essential for ensuring fairness and reliability in LLMs, particularly to mitigate biases in real-world applications [29]. Optimizing cognitive architectures for greater efficiency, investigating applications like the LARP framework in varied simulation contexts, and enhancing alignment mechanisms for personality-driven applications are key research areas [27]. Developing adaptive defense strategies and evaluation frameworks will be critical for addressing safety concerns, including potential adversarial attacks [32]. Expanding frameworks like TPTU to accommodate a wider range of tasks and APIs enhances AI systems' versatility and efficiency [32]. Improving adaptability of systems like JARVIS-1 for complex tasks and integrating additional sensory modalities to bolster memory mechanisms are important research directions [41]. In multi-agent environments, enhancing LLM training for specific tasks and improving environmental feedback mechanisms represent key exploration avenues [35]. In the biomedical domain, expanding datasets for QA instances, refining fine-tuning processes, and broadening application exploration are pivotal for advancing AI model capabilities [31]. Future efforts should prioritize creating frameworks for ethical AI usage in healthcare and addressing limitations in current data-centered methodologies [37]. Hybrid approaches combining algorithmic and system-level optimizations, focusing on adaptive techniques responsive to varying operational demands, represent a strategic direction for future research [32]. In financial applications, future research could emphasize enhancing agent adaptability to dynamic market conditions and integrating additional features to refine decision-making processes [36]. These future directions underscore the transformative potential of memory mechanisms in AI, paving the way for innovative, contextually aware applications across numerous fields. Ethical and Social Considerations The implementation of memory mechanisms in AI systems raises significant ethical and social considerations that must be addressed to ensure responsible and equitable deployment. A primary concern is the potential for memory mechanisms to inadvertently perpetuate biases, particularly in LLMs relying on extensive datasets for training [50]. These biases can influence AI systems' outputs and decisions, impacting fairness and reliability in real-world applications. Ethical implications extend to knowledge editing strategies and their long-term effects on LLM performance [50]. Unanswered questions remain regarding optimal knowledge management approaches and their consequences on AI systems' adaptability and accuracy. Addressing these concerns requires developing frameworks prioritizing transparency, accountability, and fairness in AI operations. Social considerations encompass memory mechanisms' impact on privacy and data security, particularly in applications involving personal data and sensitive information. Integrating memory systems in AI agents demands stringent measures to protect user privacy and ensure data integrity, as current methods often lack transparency and validation [50]. Establishing ethical guidelines and security protocols is crucial for fostering trust in AI systems and preventing misuse of stored information. Deploying AI systems with advanced memory capabilities can influence societal dynamics, particularly in employment, communication, and decision-making. AI agents' burgeoning capabilities to mimic human-like interactions and decision-making processes prompt examination of balancing automation and human involvement. This necessitates understanding societal implications of AI integration, especially as these systems become more prevalent in physical and virtual environments. Multi-modal AI systems, leveraging foundation models, enhance interactivity by processing visual and contextual data, informing agent responses. Advancements in LLMs in psychological applications underscore AI's potential to transform fields like social media analytics and clinical insights. However, integrating AI into these domains requires careful consideration of performance validation and societal impacts, alongside challenges posed by paradigm shifts in medical instrumentation and related applications [44,66]. These ethical and social considerations underscore the need for comprehensive evaluation and regulation of memory mechanisms in AI systems, particularly concerning privacy risks, integration of psychological principles, and potential LLM risks. Addressing privacy risks, ensuring compliance with privacy regulations, and incorporating innovative memory systems like MemoryBank to enhance long-term interaction capabilities are crucial. Rigorous performance validation and alignment evaluation maximize societal benefits while minimizing potential risks [33,44,1,14,76]. Ensuring ethical and socially responsible operation of these systems is essential for advancing AI technologies to benefit society while safeguarding individual rights and promoting equitable outcomes. Conclusion The survey underscores the pivotal role of memory mechanisms in enhancing cognitive capabilities and the performance of large language model-based agents, neural memory networks, cognitive architectures, and artificial intelligence agents. Advanced memory systems effectively tackle limitations related to context length and information retention, exemplified by RecMind's success in recommendation systems through sophisticated memory utilization. Additionally, generative agents demonstrate the ability to produce credible individual behaviors and emergent social interactions, validating the efficacy of memory-enhanced architectures. Architectural modifications and evaluation frameworks significantly enhance the long-context capabilities of large language models (LLMs), facilitating improved performance in practical applications. These advancements are crucial for increasing AI systems' adaptability and efficiency across diverse environments. The exploration of Knowledge Management and Enhancement (KME) highlights practical challenges and suggests promising research directions for future advancements in memory mechanisms. The survey identifies considerable challenges in measuring and mitigating hallucinations in natural language generation (NLG), emphasizing the need for further research to develop robust solutions. Privacy concerns remain significant, with notable risks identified in LLMs, necessitating ongoing efforts to devise effective privacy-preserving techniques. The emergence of Retrieval-Augmented Generation (RAG) as a significant advancement illustrates its potential to address common issues associated with traditional language models, thereby enhancing LLM capabilities. Efforts aimed at improving LLM efficiency through innovative techniques highlight the importance of a structured approach to advancing research in this field. Collectively, these advancements reveal the transformative potential of memory mechanisms in AI systems, paving the way for more sophisticated, contextually aware applications. The future of memory mechanisms in large language model-based agents promises continued progress in cognitive capabilities, adaptability, and performance across diverse domains, fostering the development of intelligent systems that emulate human-like intelligence and functionality. Future research may focus on enhancing model robustness and generalization across varied contexts, addressing both technical and ethical challenges in AI deployment.",
  "reference": {
    "1": "2305.10250v3",
    "2": "2308.01542v1",
    "3": "2305.04533v1",
    "4": "2306.03901v2",
    "5": "2310.02172v1",
    "6": "2311.10723v2",
    "7": "2308.10144v3",
    "8": "2308.16505v3",
    "9": "2308.05374v2",
    "10": "2311.05876v3",
    "11": "2402.01680v2",
    "12": "2302.07842v1",
    "13": "2308.10149v2",
    "14": "2309.14365v1",
    "15": "2310.08560v2",
    "16": "2308.14296v3",
    "17": "2304.03442v2",
    "18": "2401.05459v2",
    "19": "2311.12351v2",
    "20": "2308.08239v2",
    "21": "2202.03629v7",
    "22": "2311.16832v1",
    "23": "2401.03428v1",
    "24": "2312.04889v3",
    "25": "2308.11432v7",
    "26": "2208.10264v5",
    "27": "2312.17653v1",
    "28": "2308.14508v2",
    "29": "2304.13712v2",
    "30": "2210.02414v2",
    "31": "2401.17623v2",
    "32": "2312.15234v2",
    "33": "2312.06717v4",
    "34": "2310.09233v1",
    "35": "2307.04738v1",
    "36": "2311.13743v2",
    "37": "2310.05694v3",
    "38": "2305.16291v2",
    "39": "2310.03533v4",
    "40": "2306.05817v6",
    "41": "2311.05997v3",
    "42": "2310.10844v1",
    "43": "2310.16218v4",
    "44": "2312.04578v1",
    "45": "2304.08354v3",
    "46": "2305.14322v2",
    "47": "2312.03863v4",
    "48": "2312.17617v3",
    "49": "2308.07107v5",
    "50": "2401.01286v5",
    "51": "2111.01243v1",
    "52": "2401.15347v1",
    "53": "2308.13111v5",
    "54": "2311.16543v3",
    "55": "2210.03629v3",
    "56": "2308.07633v4",
    "57": "2402.09748v1",
    "58": "2202.07105v2",
    "59": "2312.10997v5",
    "60": "2306.08666v2",
    "61": "2310.02168v4",
    "62": "1702.08734v1",
    "63": "2310.10158v2",
    "64": "2306.07863v3",
    "65": "2311.11315v1",
    "66": "2401.03568v2",
    "67": "2310.10436v4",
    "68": "2310.00746v3",
    "69": "2310.01459v1",
    "70": "2312.12148v1",
    "71": "2309.13064v1",
    "72": "2307.03172v3",
    "73": "2310.01424v2",
    "74": "2309.06794v1",
    "75": "2401.01313v3",
    "76": "2310.19736v3"
  },
  "chooseref": {
    "1": "2401.01286v5",
    "2": "2401.15347v1",
    "3": "2401.01313v3",
    "4": "2309.05922v1",
    "5": "2311.01043v4",
    "6": "2311.10372v2",
    "7": "2310.05694v3",
    "8": "2311.05112v7",
    "9": "2303.18223v16",
    "10": "2307.03109v9",
    "11": "2308.10149v2",
    "12": "2308.11432v7",
    "13": "2402.06647v1",
    "14": "2202.07105v2",
    "15": "2308.07633v4",
    "16": "2311.12320v1",
    "17": "2306.13549v4",
    "18": "2311.12351v2",
    "19": "2401.03568v2",
    "20": "2310.09233v1",
    "21": "2010.03768v2",
    "22": "2309.14365v1",
    "23": "2402.09283v3",
    "24": "2302.07842v1",
    "25": "2306.03901v2",
    "26": "2401.00625v4",
    "27": "2309.00770v3",
    "28": "1702.08734v1",
    "29": "2310.10158v2",
    "30": "2401.07339v2",
    "31": "2309.06794v1",
    "32": "2311.16832v1",
    "33": "2403.14472v5",
    "34": "2307.04738v1",
    "35": "2309.16292v3",
    "36": "2304.01097v2",
    "37": "2308.07269v3",
    "38": "2104.08164v2",
    "39": "2305.13172v3",
    "40": "2310.02168v4",
    "41": "2312.03863v4",
    "42": "2305.10250v3",
    "43": "2310.19736v3",
    "44": "2308.10144v3",
    "45": "2309.01029v3",
    "46": "2401.03428v1",
    "47": "2307.16789v2",
    "48": "2110.11309v2",
    "49": "2311.13743v2",
    "50": "2307.03170v2",
    "51": "2308.14921v1",
    "52": "2312.04889v3",
    "53": "2304.03442v2",
    "54": "2308.10882v1",
    "55": "2210.02414v2",
    "56": "2304.13712v2",
    "57": "2306.05817v6",
    "58": "2310.01424v2",
    "59": "2308.10792v10",
    "60": "2309.13064v1",
    "61": "2311.05997v3",
    "62": "2310.16218v4",
    "63": "2309.04175v1",
    "64": "2302.04761v1",
    "65": "2309.15025v1",
    "66": "2402.01680v2",
    "67": "2308.14296v3",
    "68": "2310.10436v4",
    "69": "2312.17617v3",
    "70": "2309.01157v2",
    "71": "2308.07107v5",
    "72": "2311.07226v2",
    "73": "2310.03533v4",
    "74": "2311.10723v2",
    "75": "2312.17653v1",
    "76": "2402.03659v3",
    "77": "2308.14508v2",
    "78": "2307.03172v3",
    "79": "2308.13111v5",
    "80": "2310.02172v1",
    "81": "2310.07197v1",
    "82": "2308.08239v2",
    "83": "2308.01542v1",
    "84": "2402.09748v1",
    "85": "2310.01459v1",
    "86": "2401.17623v2",
    "87": "2302.13971v1",
    "88": "2312.12148v1",
    "89": "2401.05459v2",
    "90": "2110.05006v4",
    "91": "2312.06717v4",
    "92": "2305.04533v1",
    "93": "2402.03755v1",
    "94": "2306.08666v2",
    "95": "2310.16340v3",
    "96": "2210.03629v3",
    "97": "2111.01243v1",
    "98": "2308.16505v3",
    "99": "2303.11366v4",
    "100": "2305.14322v2",
    "101": "2312.10997v5",
    "102": "2308.02151v3",
    "103": "2308.09597v1",
    "104": "2310.00746v3",
    "105": "2311.16543v3",
    "106": "2307.14984v3",
    "107": "2402.00888v2",
    "108": "2309.01219v3",
    "109": "2307.07221v3",
    "110": "2202.03629v7",
    "111": "2310.10844v1",
    "112": "2306.07863v3",
    "113": "2401.07872v1",
    "114": "2311.08719v1",
    "115": "2304.08354v3",
    "116": "2309.17452v4",
    "117": "2312.04578v1",
    "118": "2312.15234v2",
    "119": "2310.08560v2",
    "120": "2311.11315v1",
    "121": "2311.05876v3",
    "122": "2308.05374v2",
    "123": "2304.06975v1",
    "124": "2208.10264v5",
    "125": "2305.16291v2",
    "126": "2311.17227v2",
    "127": "2401.02705v2",
    "128": "2305.14196v3"
  }
}