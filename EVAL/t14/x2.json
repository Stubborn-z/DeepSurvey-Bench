{
  "survey": "Deep neural network pruning is a critical technique for optimizing model efficiency and performance, particularly in resource-constrained environments. This survey provides a comprehensive examination of pruning methodologies, focusing on model compression, sparsity, and network optimization. It categorizes pruning techniques into structured and unstructured approaches, magnitude-based, gradient-based, and innovative methodologies, evaluating their effectiveness in reducing computational complexity while preserving accuracy. The survey highlights the role of pruning in enhancing computational efficiency and inference speed, noting its significance in deploying models in real-world applications such as IoT and edge devices. Furthermore, it explores the trade-offs involved in different pruning strategies, emphasizing the need for standardized benchmarks and metrics to ensure consistent evaluation across diverse architectures. Recommendations for future research include enhancing pruning algorithms, integrating compression techniques, and exploring adaptive strategies to optimize model performance. By providing informed insights into the applicability of pruning across various neural network architectures, the survey aims to guide future research and practical applications, ensuring the continued advancement of deep learning models. Ultimately, this paper serves as a foundational resource for leveraging pruning techniques to achieve superior model efficiency and performance.\n\nIntroduction Importance of Deep Neural Network Pruning Pruning is a vital technique in deep neural networks, enabling model compression and optimization [1]. As deep models grow, they face challenges such as high computational and storage demands, which pruning effectively alleviates [2]. This reduction in complexity is crucial for deploying models in resource-constrained environments, such as Internet of Things (IoT) applications, where energy and storage limitations are significant [3]. Furthermore, pruning enhances the efficiency of deep neural networks by eliminating redundant filters that do not substantially contribute to model performance, thus minimizing unnecessary computational overhead [4]. This is particularly important for convolutional neural networks (CNNs), which often struggle with deployment due to their size and resource requirements [5]. By facilitating compression and acceleration, pruning makes it feasible to deploy CNNs on edge devices [6]. Structured pruning methods are particularly relevant for large-scale models like Vision Transformers (ViTs), which necessitate specialized approaches to optimize performance in resource-limited contexts [7]. Pruning techniques also play a critical role in reducing computational complexity in deep models, ensuring their practicality in real-world applications with limited resources [8]. Scope and Objectives of the Survey This survey provides an extensive examination of deep neural network pruning, emphasizing the enhancement of model efficiency while maintaining performance across diverse tasks and architectures [2]. It encompasses a thorough literature review, addressing both empirical and theoretical aspects of pruning techniques, while excluding unrelated optimization strategies [5]. The survey aims to tackle challenges associated with neural network pruning, such as scheduling and accurately assessing weight importance, while exploring sparsification techniques for both inference and training. A primary objective is to critically assess pruning techniques in comparison to training models from scratch, questioning the conventional reliance on pre-training over-parameterized models. Recent findings indicate that pruning can be effectively applied to randomly initialized weights, potentially yielding superior performance without the computational burden of pre-training. This challenges the perceived value of inherited \"important\" weights and underscores the significance of the pruned architecture itself. The absence of standardized benchmarks and metrics complicates the comparison of pruning methods, highlighting the need for frameworks like ShrinkBench to enable consistent evaluations. Techniques such as Contrastive Pruning aim to preserve both task-agnostic and task-specific knowledge, enhancing the generalization capability of pruned models, even at high sparsity levels. These insights advocate for a reevaluation of existing pruning paradigms and the exploration of sparsity as a means to improve model efficiency and performance [9,10,11,12,13]. The survey also investigates the evolution of sparse topologies, which are increasingly supplanting dense architectures with efficient configurations, a trend essential for compact network design. Additionally, structured pruning techniques, including filter ranking methods and the lottery ticket hypothesis, are examined for their relevance to hardware compatibility and optimal compression ratios. The survey further addresses specific challenges in deploying deep neural networks on resource-constrained devices, proposing innovative approaches such as dynamic feedback mechanisms for optimization. It underscores the significance of pruning as a model compression strategy, particularly for reducing CNN sizes for edge applications, and discusses its applicability across various architectures. Informed recommendations for future research are provided, advocating for the integration of pruning with other compression techniques like quantization and knowledge distillation to enhance performance and efficiency. Additionally, the survey delves into the challenges faced by the model compression community and explores promising avenues for advancing compression algorithms to improve the deployment of large language models and deep neural networks in resource-constrained settings [3,14,5,15]. Structure of the Survey This survey is systematically structured to provide a comprehensive examination of deep neural network pruning, commencing with an introduction that highlights the importance of pruning for model compression and network optimization [3]. The introductory section establishes the scope and objectives of the survey, followed by an overview of the paper's organization. Section 2 delves into the background and definitions, providing foundational knowledge of deep neural networks and pruning. Key terms such as model compression, sparsity, and network optimization are defined, emphasizing their relevance in mitigating energy and performance costs associated with deep learning. Model compression includes strategies like pruning, which reduces the size of neural networks by eliminating non-critical or redundant neurons, making them more suitable for deployment in resource-constrained environments. Sparsity involves selectively reducing network components, decreasing memory footprint and training time while enhancing generalization capabilities. Network optimization focuses on refining neural network architecture and parameters to improve efficiency and performance, particularly for edge applications and large language models. These concepts are essential for addressing the computational, energy, and storage challenges of deploying deep neural networks in practical scenarios [9,3,5,14]. Section 3 presents a detailed taxonomy of pruning methods, categorizing various approaches into structured and unstructured pruning, magnitude-based pruning, and gradient-based pruning. This section also highlights innovative methodologies, including the introduction of a scaling factor for effectively pruning unimportant components [16] and the Bi-level Optimization Pruning (BiP) method, which enhances the identification of winning tickets in deep learning models [17]. Section 4 compares pruning techniques, assessing their effectiveness in terms of model compression and sparsity while evaluating their impacts on network optimization and performance. Performance metrics and benchmarks support this comparative analysis. In Section 5, the survey analyzes the effects of pruning on network optimization, concentrating on computational efficiency, inference speed, model accuracy, and generalization. The exploration of various pruning strategies reveals intricate trade-offs, offering insights into the compromises required for effective model compression and acceleration. This includes understanding how pruning affects model accuracy, the benefits of using pruned architectures for efficient inference on edge devices, and the role of pruning in architecture search. The findings underscore the necessity of standardized benchmarks and metrics for evaluating pruning methods, the potential of sparsity to enhance model performance while lowering resource costs, and innovative frameworks like \"prune, then distill\" for optimizing knowledge transfer in neural networks [9,10,18,5,13]. Section 6 provides recommendations for future research and applications, suggesting improvements to pruning algorithms, standardization of benchmarks and metrics, and the exploration of pruning applicability across various architectures. The integration of pruning with other compression techniques is also proposed to achieve superior results [14]. Finally, Section 7 concludes the survey by summarizing key findings on pruning in deep neural networks, emphasizing its critical role in model compression and acceleration for edge applications. This section discusses the paper's contributions to the field, examining the advantages and drawbacks of various pruning strategies, including structured and unstructured approaches, and their impact on computational efficiency and hardware implementation. It also reflects on the challenges faced by the model compression community, such as the need for standardized benchmarks and metrics, and suggests potential research opportunities, proposing that pruning can serve as an architecture search paradigm while highlighting the importance of careful baseline evaluations in future research [19,9,10,5,13].The following sections are organized as shown in . Background and Definitions Overview of Deep Neural Networks Deep neural networks (DNNs) are pivotal in artificial intelligence, excelling in complex tasks such as image recognition, natural language processing, and speech recognition. Their success is attributed to automated feature extraction and the ability to learn detailed patterns from extensive datasets. Despite challenges related to computational, energy, and storage demands, compression techniques like pruning, sparse representation, and knowledge distillation have emerged to mitigate these issues [20,21,22,3,23]. Innovations in architectures, including deep convolutional networks and residual learning frameworks, have enhanced model accuracy and efficiency, facilitating large-scale applications. Convolutional neural networks (CNNs) are prominent for image processing, utilizing convolutional layers to capture spatial hierarchies, excelling in classification and detection tasks [24]. However, lightweight CNNs may suffer performance degradation due to computational constraints [25]. Despite these limitations, CNNs remain foundational in computer vision, often as the backbone for complex models. In natural language processing, Transformer architectures have gained prominence for modeling long-range dependencies through self-attention mechanisms, showing exceptional performance in language understanding and generation [26]. However, training large Transformers is computationally intensive, requiring substantial resources [27]. Emerging methodologies like Differentiable Architecture Search (DARTS) optimize neural architecture search, discovering efficient architectures with reduced computational overhead [28]. Frameworks like MoCo advance contrastive learning, enhancing DNN capabilities in unsupervised scenarios [29]. Despite their capabilities, DNNs demand significant computational resources, especially as model complexity increases. Research aims to optimize architectures to balance efficiency with performance [30]. Understanding DNN characteristics is crucial for effective application across various tasks and industries. Concept of Pruning in Neural Networks Pruning enhances neural network efficiency by removing non-essential components, such as weights, neurons, or filters, increasing sparsity and reducing computational load [31]. This process minimizes memory usage and accelerates inference, beneficial for deploying large models in resource-constrained environments [5]. Pruning identifies efficient subnetworks within expansive networks, maintaining accuracy while significantly reducing model size [3]. Various methodologies focus on reducing network size by eliminating redundant parameters, optimizing CNNs for edge applications, and enhancing generative model efficiency. Despite diverse techniques, challenges persist, such as the lack of standardized benchmarks, complicating comparisons. Advancements include frameworks like ShrinkBench for standardized evaluation, strategies like Globally Unique Movement (GUM) to improve neuron uniqueness, and insights suggesting pruning can function as architecture search rather than preserving 'important' weights [5,32,10,13]. Techniques like Outlier Weighed Layerwise Sparsity (OWL) introduce non-uniform layerwise sparsity ratios, optimizing pruning across layers, while Feature Boosting and Suppression (FBS) dynamically adjusts channel usage for adaptability. Layer-adaptive magnitude-based pruning (LAMP) automates layerwise sparsity selection, streamlining optimal performance. Structured pruning methods like ThiNet focus on filter-level pruning, optimizing CNN models by removing less important filters based on their influence on subsequent layers [33]. The SOSP technique uses second-order approximations to evaluate saliency, capturing correlations across layers and enhancing accuracy. In deep compression, pruning often combines with weight quantization and Huffman coding to reduce storage requirements significantly without sacrificing accuracy. This pipeline identifies essential connections, reduces bit representation, and applies Huffman coding for compression, enabling models like AlexNet and VGG-16 to fit into on-chip SRAM cache [14,15,34,35,12]. This approach intersects with Knowledge Distillation, where knowledge from multiple models is distilled into a single, efficient model. Methods like QLoRA optimize memory usage through techniques like 4-bit quantization, expanding applications. Despite advancements, challenges remain, particularly in structured pruning methods requiring backpropagation, leading to high memory and computational costs [7]. Pruning remains essential for reducing deployment costs while maintaining model efficacy, addressing theoretical and practical challenges [36]. Definitions of Key Terms Understanding neural network pruning requires familiarity with key terms essential for grasping methodologies and implications. Pruning aims to reduce network size by removing parameters, enhancing efficiency without sacrificing accuracy. It addresses computational, energy, and memory costs, particularly for edge applications, using strategies like random, magnitude-based, and structural pruning. Despite advancements, the field grapples with challenges like the lack of standardized benchmarks, complicating comparisons. Initiatives like ShrinkBench aim to standardize evaluations and mitigate pitfalls. Pruning accelerates models while maintaining or improving generalization, critical for deploying networks on resource-constrained devices [9,37,38,5,13]. Model compression reduces network size while preserving performance, lowering memory and computational costs crucial for deployment in resource-limited environments like mobile and IoT devices. Sparsity, achieved by removing less significant weights, creates efficient models with fewer parameters [35], decreasing storage requirements and enhancing computational efficiency during inference [39]. Layerwise sparsities measure optimal non-zero weight distribution across layers to maximize performance [39]. Network optimization improves efficiency and effectiveness, often through pruning techniques. This includes formulating weight pruning as a nonconvex optimization problem, solvable via methods like alternating direction method of multipliers (ADMM) [40], aiming to enhance performance while minimizing computational costs, complicated by inconsistent benchmarks [13]. Accuracy evaluates pruning success, indicating how well models replicate human-level understanding in complex tasks [41]. Benchmarks for assessing pruning methods at initialization are essential for evaluating effectiveness [42]. Existing filter pruning methods often rely on simple heuristics, highlighting the need for sophisticated strategies [33]. These definitions underscore the intricate nature of pruning, emphasizing its role in model compression and network optimization. Pruning selectively removes non-essential components to enhance efficiency, resulting in reduced memory footprint, faster training, and improved performance on edge devices. Despite potential, challenges persist, such as the need for standardized benchmarks and robust methods preserving accuracy and robustness. Evolving understanding suggests pruning could serve as architecture search, providing insights into designing compact and efficient networks [9,2,10,5,13]. Relevance of Pruning in Deep Neural Networks Pruning is crucial in deep neural networks, optimizing computational efficiency and resource management, particularly in large-scale applications like CNNs and GNNs [5]. By systematically reducing model size and complexity, pruning addresses the challenge of maintaining accuracy while enhancing performance, making CNNs suitable for edge deployments [5]. Innovative methodologies like the Elastic Lottery Ticket Hypothesis (E-LTH) offer efficient means of identifying winning tickets, reducing the need for repeated evaluations, and allowing flexibility in architecture [43]. This aligns with the lottery ticket hypothesis, suggesting smaller subnetworks can perform as effectively as larger models. Techniques like movement pruning show adaptability in pruning weights while preserving performance, surpassing traditional methods in transfer learning scenarios. This adaptability is vital for scaling across layers without per-layer sensitivity analysis, enhancing versatility and efficiency. The Dependency Graph (DepGraph) method exemplifies this by providing automatic structural pruning across architectures—CNNs, RNNs, GNNs, and Transformers—by modeling inter-layer dependencies and grouping parameters for pruning, maintaining performance while avoiding architecture-specific limitations [9,18,44,45]. In transformer models, benchmarks efficiently train and infer while balancing size and resources, highlighting pruning's significance in optimizing large-scale models. Larger models, despite initial demands, can be trained quickly and are amenable to compression techniques like pruning and quantization. Heavily compressed large models achieve higher accuracy and efficiency than lightly compressed small models, bridging training efficiency and inference performance. Methods integrating weight pruning and model distillation allow sparse pre-trained models to maintain performance across NLP tasks while achieving significant compression ratios with minimal accuracy loss [46,47]. Metrics like Block Influence simplify layer removal, enhancing efficiency. Pruning improves deep CNN efficiency by removing non-essential components, reducing computational and storage costs while maintaining accuracy. This is advantageous for deploying models on edge devices with limited resources. Structured pruning, focusing on eliminating filters, reduces inference costs and facilitates realistic acceleration by maintaining dense connectivity patterns, leveraging efficient libraries for matrix operations. Advanced techniques preserve network robustness against adversarial examples, ensuring compact networks retain high benign and robust accuracy. Pruning is thus a critical compression strategy addressing energy and performance demands, paving the way for efficient and scalable deployments [48,2,9,5,19]. It addresses poor gradient flow at initialization in sparse networks, enhancing performance. Dynamic strategies leverage varying importance of channels based on inputs, enhancing efficiency. Techniques like those in QLoRA enable deep networks to function efficiently on single GPU setups, selectively removing non-essential components, reducing memory footprint, and accelerating training without compromising performance. Inspired by biological sparsity, these methods maintain and can enhance generalization, suitable for deployment on resource-constrained devices [9,5]. Predicting errors in pruned networks provides a framework for understanding pruning across scales, crucial as networks grow larger and more complex. Pruning enhances scalability and adaptability of networks, reducing energy and performance costs by removing non-critical components. This decreases memory footprint, enabling deployment on mobile and edge devices, and accelerates training for large-scale networks. Maintaining or improving generalization compared to dense networks facilitates efficient inference and training across architectures and tasks. Recent research highlights predictability and interchangeability across sparsities, providing a framework for optimizing performance in diverse domains [9,5,10,49]. As research evolves, innovative strategies will be vital for unlocking models' potential, supporting deployment in resource-constrained scenarios while maintaining competitive performance. Taxonomy of Pruning Methods The optimization of neural network architectures is vital for improving computational efficiency and performance in deep learning. This section categorizes pruning methods into structured and unstructured approaches, with a focus on structured pruning in the subsequent subsection, which systematically removes entire structures within neural networks to enhance efficiency while maintaining performance. Table offers a comprehensive comparison of various pruning methods, elucidating their optimization targets, performance impacts, and application contexts. illustrates the taxonomy of pruning methods in neural network optimization, categorizing them into structured, unstructured, magnitude-based, gradient-based, and innovative methodologies. Each category highlights key techniques, their applications, and benefits, thereby emphasizing the role of pruning in enhancing model efficiency, performance, and deployment in resource-constrained environments. Structured Pruning Approaches Structured pruning targets entire components such as neurons, channels, or layers, enhancing hardware compatibility and inference speed [50]. These methods are particularly beneficial for large-scale models like Transformers and LLMs, where computational complexity reduction is crucial [1]. Single Shot Structured Pruning (SSSP) optimizes both training and inference times by removing entire channels and hidden units before training [1]. The Bonsai method simplifies the process by eliminating backpropagation, using forward-pass-only perturbative pruning to enhance model efficiency [50]. Movement pruning, primarily a weight pruning technique, leverages first-order information to selectively prune weights, showing sensitivity to pretrained model fine-tuning [51]. Innovative methodologies like EigenDamage use the Kronecker-factored eigenbasis for efficient pruning, improving inference speed and reducing storage needs. The FLuctuation-based Adaptive Structured Pruning (FLAP) method introduces a retraining-free framework that adapts to performance fluctuations, enhancing inference speed and storage efficiency. FLAP's structured importance metrics and global model structure determination outperform state-of-the-art methods like LLM-Pruner across various language benchmarks. The code for FLAP is available at https://github.com/CASIA-IVA-Lab/FLAP [52,53,54,55,56]. Structured pruning has also been applied to GPT-type models, evaluating methods such as magnitude, random, and movement pruning for performance optimization. The categorization of knowledge distillation into algorithm, skill, and verticalization underscores structured pruning's significance in enhancing neural network efficiency [57]. Structured pruning plays a crucial role in efficiently deploying neural networks, especially in resource-constrained environments, by reducing model complexity while maintaining performance. As deep learning research progresses, structured pruning remains a key tool for optimizing models across various applications, significantly lowering computational and storage costs while enhancing performance. Unlike unstructured pruning, structured pruning offers realistic acceleration benefits, improving hardware compatibility. Recent findings highlight the pruned architecture's importance for efficiency, potentially serving as an architecture search paradigm, emphasizing the need for careful evaluation of pruning methods to maximize their potential in optimizing deep learning models [9,5,10,19]. Unstructured Pruning Techniques Unstructured pruning focuses on removing individual parameters or connections within neural networks, offering a fine-grained model optimization approach. These methods are particularly relevant for large-scale models, where precise control over sparsity can significantly improve computational efficiency and performance. The 'Random Tickets' concept exemplifies this approach, applying simple, data-independent prune ratios to create subnetworks, challenging traditional structured pruning paradigms [58]. illustrates key unstructured pruning techniques in neural networks, highlighting the Random Tickets concept, LLM Surgeon technique, and Unstructured Magnitude Pruning. Each technique offers unique methods for optimizing neural network efficiency through selective parameter removal and dynamic allocation, showcasing their applicability in various computational scenarios. The LLM surgeon technique dynamically allocates structures for removal in LLMs, enhancing adaptability and efficiency by selectively pruning individual parameters [59]. This dynamic allocation is essential for managing LLM complexity, which often demands considerable computational resources for training and inference. Unstructured magnitude pruning has also been successfully applied in transfer learning scenarios, identifying sparse sub-networks to enhance model performance [60]. This technique leverages the inherent sparsity within neural networks to optimize transfer learning tasks, showcasing the versatility of unstructured pruning methods in various learning environments. Unstructured pruning provides a flexible framework for optimizing neural networks, enabling precise control over model sparsity and enhancing computational efficiency. As research advances, unstructured pruning remains critical for refining deep learning models by reducing computational and storage costs while maintaining or improving generalization across diverse applications. Unlike structured pruning, which emphasizes hardware-friendly model acceleration, unstructured pruning allows flexible removal of non-critical or redundant neurons, optimizing neural network efficiency for deployment in resource-constrained settings. This approach is essential for achieving efficient inference and training, especially in large, over-parameterized models, where the pruned architecture often plays a more significant role in efficiency than inherited weights [9,10,5,32,19]. Magnitude-Based Pruning Magnitude-based pruning optimizes neural networks by selectively removing weights based on their magnitude, reducing the number of parameters. This approach primarily targets fully connected layers, where substantial parameter reduction is achievable, though it may not effectively address computational costs in convolutional layers due to irregular sparsity [48]. Iterative Magnitude Pruning (IMP) exemplifies this by identifying subnetworks within a neural network that can train to full accuracy in isolation, enhancing model efficiency [61]. The method proposed by [35] employs three distinct magnitude-based pruning schemes: class-blind, class-uniform, and class-distribution. These schemes utilize weight magnitudes to effectively reduce parameters, providing a versatile framework for pruning across various neural network architectures and achieving optimal sparsity levels while maintaining performance. Additionally, the Layer-Adaptive Magnitude-based Pruning (LAMP) method introduces a dynamic adjustment of weight magnitudes based on the model-level distortion from pruning [39]. This adjustment optimizes the trade-off between sparsity and accuracy by informing pruning decisions with the overall impact on model performance. Magnitude-based pruning offers a robust mechanism for enhancing neural network efficiency, particularly in scenarios where reducing parameter count is critical for deployment in resource-constrained environments. As research progresses, innovative magnitude-based strategies, including network pruning and knowledge distillation, continue to play a vital role in refining neural network architectures and optimizing computational efficiency. These strategies are essential for minimizing storage and computation demands, making deep neural networks more feasible for deployment on resource-constrained devices, such as those used in Internet of Things (IoT) applications. By focusing on techniques that minimize accuracy compromise, these approaches enhance performance and deployability, particularly for edge applications [3,5,21]. Gradient-Based Pruning Gradient-based pruning techniques leverage gradient information to identify and eliminate less significant parameters within neural networks, optimizing model efficiency and performance. These methods capitalize on the principle that gradients reveal the importance of individual weights, facilitating the selective removal of those contributing minimally to the network's output [5]. A prominent approach is the Gradient Signal Preservation (GSP) method, which focuses on maintaining the integrity of gradient signals during pruning, ensuring essential learning pathways remain intact [50]. Gradient information is particularly beneficial for large-scale models, where computational efficiency is paramount. By analyzing gradient magnitudes, these techniques effectively identify and prune weights that minimally influence the model's loss function, thereby reducing computational overhead without sacrificing accuracy [1]. The Gradient-based Importance Score (GIS) method exemplifies this by calculating importance scores based on gradient information, enabling precise targeting of weights for removal [2]. Moreover, gradient-based pruning can be combined with other optimization strategies, such as sparsity-inducing regularization, to enhance the robustness and adaptability of neural networks. This integration allows for dynamic adjustment of pruning criteria based on real-time gradient feedback, optimizing the balance between sparsity and model performance [31]. The synergy between gradient-based techniques and regularization methods underscores the versatility of gradient information in refining neural network architectures. Gradient-based pruning represents a sophisticated approach to model optimization, leveraging gradient properties to enhance neural network efficiency and effectiveness. As model compression research evolves, gradient-based strategies remain crucial for developing advanced pruning methodologies. These techniques are vital for deploying high-performance models in resource-constrained environments, such as edge devices, where reducing model size and inference latency is critical. Despite the emergence of gradient-free structured pruning methods using unlabeled data, gradient-based approaches continue to offer significant advantages in optimizing neural networks by selectively removing non-critical components, thus supporting efficient inference and training of sparse models while minimizing computational costs and memory footprints, all while maintaining or enhancing model accuracy [55,5,9]. Innovative Pruning Methodologies Innovative pruning methodologies have advanced neural network optimization by introducing novel strategies that enhance model efficiency and performance across various applications. The ThiNet approach utilizes next-layer statistics to assess filter importance, distinguishing itself from traditional pruning methods that often rely solely on layer-specific metrics [4]. This method emphasizes accurately estimating individual filter contributions to overall model output, a central challenge in pruning. The Elastic Lottery Ticket Hypothesis (E-LTH) introduces a paradigm shift by allowing manipulation of winning tickets across different network depths without the extensive computational burden associated with Iterative Magnitude Pruning (IMP) [43]. This innovation aligns with the trend of leveraging flexible network architectures to optimize performance in resource-constrained settings. Channel gating represents another innovative methodology that contrasts with traditional static pruning methods, performing run-time optimization based on input-specific characteristics [31]. This dynamic approach enables significant computational savings without compromising accuracy, underscoring the potential of adaptive pruning strategies. The Prune-Adjust-Re-Prune (PARP) method effectively discovers and fine-tunes subnetworks to achieve better performance with reduced computational effort [36]. By iteratively adjusting pruning decisions, PARP enhances pruning precision, ensuring essential components are retained while optimizing computational efficiency. GOHSP integrates graph-based ranking of attention heads with an optimization-based approach to implement heterogeneous structured pruning in Vision Transformer (ViT) models [7]. This method enhances adaptability and efficiency in ViT models, ensuring optimal performance across diverse tasks. ManiDP involves dynamically removing redundant filters by embedding manifold information of all instances into the pruned network space [8]. This technique enhances the representational capacity of pruned networks, ensuring that critical information is preserved while optimizing model efficiency. The novel filter pruning method proposed by [33] utilizes subspace clustering of feature maps to retain representative information while eliminating redundant filters, enhancing the precision of pruning decisions and preserving essential learning pathways. The introduction of pruning methods that effectively maintain both benign accuracy and robustness, as highlighted by [2], represents a significant advancement in pruning methodologies. These methods differ from existing approaches that typically prioritize one aspect over the other, offering a more balanced optimization strategy. These innovative methodologies collectively advance the field of neural network pruning, providing novel strategies that enhance model efficiency and performance across diverse applications. As research progresses, innovations in deep learning, including neural network compression techniques like network pruning, sparse representation, and knowledge distillation, will be pivotal in enhancing model performance and efficiency. These advancements will ensure effective deployment of deep learning models in resource-constrained environments, such as IoT devices, by reducing computational, energy, and storage demands without significantly compromising accuracy [3,62,9,21]. Comparison of Pruning Techniques Performance Metrics and Benchmarks Performance metrics and benchmarks are essential for evaluating the effectiveness of pruning techniques in neural networks, offering insights into model efficiency and accuracy post-pruning. Accuracy is a primary metric, as demonstrated by studies comparing pruned subnetworks with full models, such as the MobileVLM V2 evaluated using standard VLM benchmarks [6]. Inference speed and computational efficiency are critical, with methods like channel gating assessing improvements in floating-point operations (FLOPs) and off-chip memory accesses alongside accuracy [31]. These metrics ensure models maintain performance while reducing complexity. Table provides a detailed overview of key benchmarks and performance metrics relevant to the evaluation of pruning techniques in neural networks, emphasizing their role in maintaining model performance while reducing computational complexity. Compression ratios are crucial for assessing model size reduction, with techniques like pruning, quantization, and knowledge distillation achieving significant reductions while preserving accuracy. These strategies are vital for deploying large-scale language models, addressing challenges like increased carbon emissions and high maintenance costs. Optimizing compression sensitivity and employing collaborative compression strategies can substantially decrease computational requirements, enabling deployment on resource-constrained devices without significant performance loss [63,14,64,15]. Evaluating pruning techniques involves accuracy metrics and FLOPs measurements, facilitating a trade-off analysis between model complexity and computational efficiency. A comprehensive analysis of neural network pruning literature establishes a standardized framework for evaluating techniques, ensuring models maintain performance while reducing computational complexity. Addressing the historical lack of standardized benchmarks, tools like ShrinkBench enable reliable comparisons across pruning methods, preventing common pitfalls and enhancing deployment on resource-constrained devices. Techniques such as ContrAstive Pruning (CAP) illustrate the potential to retain task-agnostic and task-specific knowledge, achieving high performance even at extreme sparsity levels [14,5,12,13]. The development of standardized metrics and benchmarks is crucial for advancing neural network pruning. Structured vs. Unstructured Pruning Structured and unstructured pruning offer distinct neural network optimization approaches, each with unique advantages and limitations. Structured pruning removes entire structures, like neurons, channels, or layers, enhancing hardware compatibility and inference speed [50]. This method is effective for large-scale models, crucial for reducing computational complexity. For example, pruning an 8B LLaMA-3 model to 50 Conversely, unstructured pruning targets individual parameters or connections, allowing precise sparsity control and improving computational efficiency and model performance. Experimental comparisons across datasets highlight this approach's adaptability and effectiveness in achieving efficient model compression [58]. While structured pruning simplifies by eliminating entire components, unstructured pruning offers flexibility by dynamically adjusting sparsity levels based on specific model requirements. It requires sophisticated strategies to preserve accuracy, as removing individual parameters can create irregular sparsity patterns, impacting performance through inefficiencies in memory usage and training time. Despite challenges, unstructured pruning is valuable for architecture search, leading to models that generalize as well as, or better than, dense counterparts while reducing computational costs [9,32,10]. Both methods provide valuable strategies for optimizing neural networks, with structured pruning offering a straightforward means of reducing complexity and unstructured pruning offering adaptability and precision in sparsity control. Integrating knowledge distillation and network sparsification techniques enhances neural network efficiency. Knowledge distillation compresses ensemble knowledge into a single model, improving performance while reducing computational demands. Network sparsification involves pruning and optimizing connections, minimizing memory and energy requirements, facilitating deployment on resource-constrained devices [3,9,21,23]. State-of-the-Art Pruning Methods Recent advancements in pruning methodologies have significantly enhanced neural network efficiency and performance across diverse applications. The GOHSP method demonstrates substantial parameter reduction without accuracy loss on CIFAR-10 and improved accuracy on ImageNet, showcasing its effectiveness [7]. Similarly, the manifold-based pruning technique, benchmarked against state-of-the-art methods on architectures like ResNet-34 using ImageNet, highlights competitive performance [8]. Innovative filter pruning methods have emerged, indicating superior performance over existing techniques prior to fine-tuning and achieving state-of-the-art results post-fine-tuning [33]. These approaches underscore precise filter-level pruning's importance in refining architectures. Additionally, the Elastic Lottery Ticket Hypothesis (E-LTH) effectively adapts winning tickets, maintaining competitive effectiveness against those obtained through Iterative Magnitude Pruning (IMP) [43]. Surveys of pruning algorithms compare various techniques regarding effectiveness in reducing model size while maintaining accuracy, highlighting trade-offs involved [5]. These state-of-the-art methods represent significant progress, offering advanced strategies for enhancing efficiency and performance across diverse applications and architectures. As research evolves, these innovations will play a crucial role in advancing deep learning models, ensuring effective deployment in resource-constrained environments. Comparative Analysis of Pruning Techniques Comparative analysis of pruning techniques is essential for discerning their effectiveness in achieving model compression while retaining performance across diverse architectures. Research indicates Dynamic Sparse Allocation (DSA) notably outperforms iterative budgeted pruning methods, streamlining processes and reducing overall pruning time by at least 1.5x [65]. This efficiency is critical given neural networks' increasing size and complexity. Experiments on models like ResNet-50 and Inception-v3 on ImageNet, alongside MNIST, demonstrate subnetworks identified by Iterative Magnitude Pruning (IMP) achieving competitive performance [61]. Evidence suggests pruned networks can mirror larger counterparts' expressive power, supported by probabilistic bounds indicating pruned models can approach target networks' performance in expressiveness [38]. Research on the lottery ticket hypothesis shows pruned subnetworks, or \"winning tickets,\" achieving accuracy comparable to full pre-trained models. Studies indicate these subnetworks can be identified through various techniques, reducing parameter counts by over 90 Attention head pruning exemplifies potential for significant network simplification without detrimental impacts on performance, indicating new pruning efficiency possibilities [66]. Comparative studies reveal pruning remains potent for model downsizing, particularly when evaluated against alternative approaches [3]. Innovative methods like Collaborative Compression exhibit substantial computational demand reductions, underscoring pruning's comparative benefits in resource optimization [64]. Examinations of pruned networks at inference and training stages indicate distinctive advantages and highlight nuanced trade-offs between different strategies [67]. Evaluations across GLUE tasks, utilizing fine-tuned BERT models, underscore pruned networks' capability to sustain competitive performance benchmarks [68]. This accumulation of evidence emphasizes standardized benchmarks' critical role in inter-disciplinary evaluations and chronicling advancements within neural network pruning. As the landscape evolves, comprehensive analyses will guide future innovations. Analysis of Pruning Impacts Computational Efficiency and Inference Speed Pruning techniques significantly enhance computational efficiency and inference speed by reducing parameters and operations during model execution, crucial for deployment in resource-limited environments. Inspired by biological neural networks, pruning reduces computational costs and latency, proving effective across domains like speech recognition [6]. Structured pruning methods, such as Single Shot Structured Pruning (SSSP), deliver notable improvements, achieving 2x and 3x speedups for training and inference without compromising accuracy. Similarly, ThiNet efficiently prunes CNN filters, maintaining the original architecture while boosting computational efficiency [4]. The PARP method exemplifies structured pruning benefits, offering outputs twice as fast as semi-structured techniques [36]. Innovative strategies like Outlier Weighed Layerwise Sparsity (OWL) optimize inference speed, achieving a 2.6x speed-up, while the Feature Boosting and Suppression (FBS) technique achieves significant computational savings, reducing VGG-16 by 5x and ResNet-18 by 2x, highlighting dynamic channel usage's impact on efficiency [31]. The Elastic Lottery Ticket Hypothesis (E-LTH) demonstrates the transferability of winning tickets across architectures, achieving performance comparable to Iterative Magnitude-based Pruning (IMP) methods while reducing computational costs [43]. ParameterNet exemplifies pruning's ability to maintain high accuracy with reduced FLOPs, achieving 81.6\\ Deep compression techniques, combining pruning with weight quantization and Huffman coding, substantially reduce model size and enhance speed and energy efficiency during inference. QLoRA drastically reduces the memory footprint for finetuning large models, such as those with 65 billion parameters, on a single 48GB GPU, maintaining full 16-bit performance. This showcases the synergy between advanced quantization techniques, like 4-bit NormalFloat and double quantization, with low-rank adaptation, enabling finetuning of models otherwise infeasible due to resource constraints, achieving near state-of-the-art performance with reduced computational demands [14,63,30,15,59]. The SOSP method exemplifies scalability and efficiency gains through pruning, maintaining accuracy while eliminating architectural bottlenecks to enhance network performance. Collectively, these advancements underscore pruning's critical role in optimizing computational efficiency and inference speed, facilitating effective neural network deployment across diverse applications while maintaining high performance [33]. Ongoing research is essential for developing innovative pruning methodologies that optimize deep learning model deployment in resource-constrained environments. Model Accuracy and Generalization Pruning techniques significantly affect neural networks' accuracy and generalization by removing redundant components and optimizing model structures. Often, pruning results in minimal performance loss, with retraining potentially enhancing performance beyond the original model's capacity [2]. This highlights pruning's ability to refine architectures, preserving or improving efficacy despite reduced parameter counts. The Gate Decorator method exemplifies advanced pruning techniques by employing a global filter pruning algorithm that accelerates and compresses deep CNNs without introducing special operations. Utilizing channel-wise scaling factors and Taylor expansion for filter importance estimation, Gate Decorator achieves remarkable efficiency, reducing 70 Structured optimization enhances model generalization by reducing computational costs while maintaining or improving generalization across architectures and tasks. Strategic removal of redundant components and neural architecture search optimize pruned network structures, enabling efficient inference and training. These insights offer a framework for understanding pruned networks' scalability and interchangeability, highlighting pruning's potential as an effective architecture search paradigm [69,9,10,49,19]. Methods like QLoRA optimize memory usage while maintaining accuracy and generalization capabilities, showcasing the interplay between pruning and resource management. Larger models, when compressed, consistently achieve higher accuracy than smaller counterparts, indicating initial model size significantly influences post-pruning performance. The Elastic Lottery Ticket Hypothesis (E-LTH) explores winning ticket initializations' adaptability across architectures, datasets, and optimizers. E-LTH demonstrates that by strategically modifying layers within a network, winning tickets can be effectively transferred to other networks within the same model family while maintaining competitive performance. This reduces the computational burden associated with Iterative Magnitude-based Pruning (IMP) and reveals scalable sparse patterns that enhance generalization. Extensive experiments on datasets like CIFAR-10 and ImageNet validate the hypothesis, showing winning ticket initializations generalize robustly across diverse datasets and optimizers, often matching the performance of tickets generated specifically for those conditions [70,71,43,72]. Pruning also significantly impacts Vision Transformer (ViT) models' practical deployment. The GOHSP method achieves substantial model compression while maintaining or enhancing accuracy, demonstrating structured pruning's potential in optimizing large-scale ViT architectures for resource-constrained applications. By integrating graph-based ranking and optimization to impose heterogeneous structured sparsity patterns, GOHSP achieves a 40 Despite these advancements, challenges remain in fully understanding and generalizing self-supervised pruning methods across diverse tasks and environments. This complexity necessitates comprehensive theoretical exploration to leverage pruning's full potential in enhancing neural networks' accuracy and generalization capabilities across varied applications and architectures. Future research should focus on refining pruning strategies to optimize the compression and acceleration of Convolutional Neural Networks (CNNs), addressing challenges related to model size reduction while maintaining performance, leveraging sparsity to decrease energy and memory costs, and exploring structured pruning techniques for realistic acceleration and hardware compatibility [9,5,19]. Trade-offs in Pruning Strategies Pruning strategies inherently involve trade-offs between model compression and performance, where achieving optimal sparsity may lead to compromises in accuracy and generalization capabilities [5]. A significant limitation in the field is the inconsistency in benchmarking practices and the prevalence of sub-optimal hyper-parameters, which undermine the reliability of comparative results. These inconsistencies pose challenges in evaluating the true effectiveness of pruning techniques across different neural network architectures [3]. Current research highlights the challenges in implementing sparsification techniques across diverse architectures, presenting trade-offs in accuracy. Despite progress, many pruning techniques still struggle to maintain model accuracy post-pruning, and the generalizability of these methods across different CNN architectures remains limited [5]. This complexity is compounded by scalability limitations and ethical considerations regarding the use of proprietary models for knowledge transfer, necessitating a careful balance between optimizing model size and maintaining ethical research standards. The trade-offs in different pruning strategies are also influenced by evolving assumptions regarding the effectiveness of subnetworks. Future work should focus on extending theoretical results to a broader variety of network architectures and pruning techniques, along with empirical validation of proposed bounds. This exploration is crucial for understanding the long-term impacts of pruning on model performance and ensuring that compression rates do not adversely affect accuracy [3]. While numerous compression techniques—such as pruning, quantization, knowledge distillation, and efficient architecture design—have demonstrated significant progress in reducing the size of language models, unanswered questions remain regarding the optimal combination of these techniques and their long-term impact on performance. These methods aim to address challenges associated with the large size and high computational demands of language models, including increased carbon emissions and maintenance costs. However, the diversity of algorithms complicates the identification of emerging trends and fundamental concepts, underscoring the need for further research into benchmarking strategies and evaluation metrics to assess the efficacy of compressed models [14,15]. Future research should explore further optimizations in model training to mitigate environmental impact while maintaining performance. Addressing these trade-offs will be essential for advancing pruning methodologies and optimizing neural network efficiency across diverse applications and architectures. Recommendations for Future Research and Applications Enhancing Pruning Algorithms and Techniques Optimizing pruning algorithms is crucial for enhancing neural networks across various architectures. Investigating winning ticket initializations across diverse datasets and optimizers presents a promising avenue for assessing generalizability [72]. Expanding frameworks like GOHSP beyond Vision Transformers (ViTs) could yield significant improvements in sparsity patterns [7]. The PARP method offers a foundation for broader applications across pre-trained models, with potential optimizations refining its effectiveness [36]. Integrating manifold learning techniques may enhance efficiency when combined with existing pruning strategies [8]. Further research into filter pruning techniques could bolster robustness against diverse architectures and datasets, while innovations in MobileVLM V2 inform the development of more efficient algorithms [6]. The adaptability of the Elastic Lottery Ticket Hypothesis (E-LTH) presents opportunities for tailored pruning solutions across various model families [43]. Continued refinement of pruning algorithms is essential for maintaining neural network performance in resource-constrained environments, paving the way for sophisticated models like Large Audio Models and compressed Large Language Models for applications ranging from speech recognition to multilingual translation [62,73,15]. Standardizing Benchmarks and Metrics Establishing standardized benchmarks and metrics is vital for consistent evaluation of pruning methodologies across neural network architectures. Recognizing network trainability as a key factor influencing pruning outcomes emphasizes the need for consistency in experimental settings to accurately assess model performance [74]. Standardized benchmarks are particularly crucial for evaluating compressed large language models (LLMs), providing a reliable framework for assessing pruning efficacy [15]. The absence of standardized experimental settings in lottery ticket hypothesis (LTH) research presents challenges for reproducibility and comparison, underscoring the necessity for uniform evaluation criteria [57]. Implementing standardized benchmarks through frameworks like ShrinkBench will facilitate systematic comparisons, maximizing the benefits of pruning across diverse applications, particularly in resource-constrained settings [5,13]. Exploring Applicability Across Architectures Investigating the applicability of pruning techniques across various neural network architectures is essential for understanding their effectiveness. The Layer-Adaptive Magnitude-based Pruning (LAMP) method and channel gating offer promising directions for enhancing adaptability and performance across different architectures [39,31]. The LoRA method suggests avenues for optimizing rank decomposition and applying it to diverse tasks [75]. Research could focus on improving initial sparse topology selection through scaling factors and sparsity regularizations, optimizing architectures for efficient deployment. Expanding the applicability of pruning methods to reduce computational complexity and enhance performance across convolutional neural networks is vital for diverse applications, including mobile and edge devices [9,5,16]. The pruned architecture's design is more critical for efficiency than inherited weights, highlighting pruning's potential as an architecture search paradigm and the need for thorough baseline evaluations [5,10]. Integrating Compression Techniques Integrating pruning with other compression techniques, such as quantization, can significantly enhance model efficiency in resource-constrained environments. The Differentiable Hierarchical Pruning (DHP) method illustrates the benefits of combining pruning with quantization, resulting in substantial performance improvements [76]. This integration reduces precision in weights and activations, further lowering memory and computational costs. The synergy between pruning and quantization is especially advantageous for large-scale models where computational resources are limited. This approach utilizes advanced deep neural network compression techniques, including knowledge distillation, to enable efficient model deployment across platforms such as mobile and IoT devices [3,77,21]. Future research should explore the interplay between pruning and other compression techniques, such as weight clustering and Huffman coding, to optimize model efficiency and broaden their applicability across different domains [5,21]. Adaptive and Dynamic Pruning Strategies Adaptive and dynamic pruning strategies are crucial for optimizing neural network models, allowing real-time adjustments based on performance and environmental constraints. These strategies are particularly relevant in hybrid approaches that combine multiple compression techniques tailored for specific applications like IoT [3]. Future research should focus on developing adaptive pruning strategies that dynamically adjust based on performance, potentially enhancing unsupervised learning methods [29]. The AttendOut method presents a promising avenue for adaptive pruning, with future investigations focusing on its broader applications beyond self-attention [78]. Optimizing pruning criteria in methods like ThiNet could extend their applicability across various architectures [4]. Emerging trends should emphasize enhancing model generalization and multi-task learning approaches to foster innovation in adaptive pruning strategies [73]. Techniques such as knowledge distillation and sparsity will be essential for reducing computational, energy, and storage demands, facilitating deployment in resource-constrained environments. By employing methods like network pruning and adaptive parameter selection, researchers can minimize accuracy compromise while optimizing model efficiency, making it feasible to deploy robust neural networks in real-world applications [9,16,3,5,21]. Practical Applications and Deployment Pruning techniques have gained traction in real-world applications, particularly where computational resources are constrained. In the automotive sector, pruned models enable the efficient deployment of advanced driver-assistance systems (ADAS), reducing computational load and enhancing responsiveness [79]. In healthcare, pruning facilitates the deployment of diagnostic models on portable devices, improving access to services in remote areas by leveraging AI-generated content (AIGC) and deep neural network compression techniques [3,80,59]. The finance sector benefits from pruning techniques, allowing efficient models for high-frequency trading and risk assessment, which enhances processing speed and accuracy [79]. Practical deployment of pruned models is supported by benchmarks that evaluate trade-offs between model size, accuracy, and computational efficiency, guiding pruning strategy selection for specific applications [81]. Future research could investigate the implications of the Neural Tangent Kernel (NTK) in finite-width networks, translating theoretical insights into practical applications to develop more efficient models [82]. Pruning techniques represent a transformative approach for deploying neural networks efficiently across industries, particularly in resource-constrained environments. By systematically removing non-essential components, pruning accelerates inference, reduces memory footprint, and maintains predictive accuracy, facilitating advancements in fields like mobile computing and IoT. As research continues, these techniques will play a pivotal role in optimizing the performance and scalability of deep learning models, ensuring effective integration into real-world applications [83,9,10,38,5]. Conclusion This survey provides an in-depth examination of deep neural network pruning, highlighting its pivotal role in enhancing model efficiency and performance across a spectrum of applications and architectures. The findings underscore the significance of pretraining as a means to achieve precise performance estimation in long-sequence models, effectively narrowing the performance disparities across various architectures. Robust initialization strategies, such as the lottery ticket hypothesis, are crucial in identifying optimal configurations that maintain competitive performance post-retraining, thereby elucidating the impact of error landscape geometry in the pruning process. Pruning techniques are indispensable for boosting computational efficiency, accelerating inference, and facilitating deployment in environments with limited resources. They enable significant reductions in model complexity while maintaining accuracy, thus supporting neural networks' scalability and adaptability in practical settings. This survey offers a comprehensive taxonomy of pruning methods, juxtaposing their effects on model compression and sparsity, and evaluating their implications for network optimization. It also presents informed recommendations for advancing research, advocating for the integration of pruning with other compression techniques and the investigation of adaptive and dynamic strategies. As neural network architectures continue to advance, the insights garnered from this survey will guide future research endeavors, ensuring that pruning techniques remain central to model optimization strategies. By elucidating the contributions of diverse methodologies and their practical applications, this paper serves as a foundational resource for researchers and practitioners aiming to harness pruning for enhanced model efficiency and performance across varied domains.",
  "reference": {
    "1": "2007.00389v1",
    "2": "1906.06110v1",
    "3": "2010.03954v1",
    "4": "1707.06342v1",
    "5": "2005.04275v1",
    "6": "2402.03766v1",
    "7": "2301.05345v2",
    "8": "2103.05861v1",
    "9": "2102.00554v1",
    "10": "1810.05270v2",
    "11": "1909.12579v1",
    "12": "2112.07198v1",
    "13": "2003.03033v1",
    "14": "2401.15347v1",
    "15": "2308.07633v4",
    "16": "1707.01213v3",
    "17": "2210.04092v4",
    "18": "2109.14960v3",
    "19": "2303.00566v2",
    "20": "1512.03385v1",
    "21": "1503.02531v1",
    "22": "1409.1556v6",
    "23": "2110.02743v2",
    "24": "2010.11929v2",
    "25": "1912.03458v2",
    "26": "2205.01068v4",
    "27": "2012.12877v2",
    "28": "2106.10784v1",
    "29": "2003.04297v1",
    "30": "2305.14314v1",
    "31": "1805.12549v2",
    "32": "2302.03773v1",
    "33": "1803.05729v1",
    "34": "1510.00149v5",
    "35": "1606.09274v1",
    "36": "2106.05933v2",
    "37": "1906.10771v1",
    "38": "2105.10065v1",
    "39": "2010.07611v2",
    "40": "1804.03294v3",
    "41": "1905.10044v1",
    "42": "2009.08576v2",
    "43": "2103.16547v3",
    "44": "2301.12900v2",
    "45": "1911.09817v2",
    "46": "2111.05754v1",
    "47": "2002.11794v2",
    "48": "1608.08710v3",
    "49": "2006.10621v3",
    "50": "2402.05406v3",
    "51": "2005.07683v2",
    "52": "1909.12326v5",
    "53": "2204.09656v2",
    "54": "2312.11983v1",
    "55": "2303.04185v2",
    "56": "2006.07253v1",
    "57": "2403.04861v2",
    "58": "2009.11094v2",
    "59": "2312.17244v2",
    "60": "1905.07785v2",
    "61": "1912.05671v4",
    "62": "2402.13116v4",
    "63": "2402.09748v1",
    "64": "2105.11228v1",
    "65": "2004.02164v5",
    "66": "2409.05211v1",
    "67": "2007.12223v2",
    "68": "2002.00585v1",
    "69": "1803.03635v5",
    "70": "1905.10650v3",
    "71": "2103.06460v3",
    "72": "2005.00561v2",
    "73": "1909.08174v1",
    "74": "1905.09717v5",
    "75": "2107.00166v4",
    "76": "2010.02350v2",
    "77": "1906.02773v2",
    "78": "2110.11395v2",
    "79": "2003.08935v1",
    "80": "2308.12792v3",
    "81": "2301.05219v2",
    "82": "2106.09685v2",
    "83": "2003.13683v3",
    "84": "1404.0736v2",
    "85": "1706.03762v7",
    "86": "2402.17177v3",
    "87": "2304.06632v1",
    "88": "2111.13445v5",
    "89": "1806.07572v4",
    "90": "2002.08797v5"
  },
  "chooseref": {
    "1": "2401.15347v1",
    "2": "2204.09656v2",
    "3": "2009.11839v4",
    "4": "2105.10065v1",
    "5": "1906.06307v2",
    "6": "2306.11695v3",
    "7": "2002.05709v3",
    "8": "2403.04861v2",
    "9": "2010.03954v1",
    "10": "2402.13116v4",
    "11": "2308.07633v4",
    "12": "1804.03294v3",
    "13": "2102.06790v2",
    "14": "2101.10552v1",
    "15": "2304.06632v1",
    "16": "1802.03494v4",
    "17": "2210.04092v4",
    "18": "2010.11929v2",
    "19": "1905.09418v2",
    "20": "1902.01996v4",
    "21": "1905.10650v3",
    "22": "1706.03762v7",
    "23": "2403.14729v1",
    "24": "2105.06990v2",
    "25": "1905.10044v1",
    "26": "2203.04570v1",
    "27": "1805.12549v2",
    "28": "1707.06168v2",
    "29": "2106.04533v3",
    "30": "2102.07156v1",
    "31": "2003.02389v1",
    "32": "1606.09274v1",
    "33": "1806.09055v2",
    "34": "2003.13683v3",
    "35": "2005.03354v2",
    "36": "2004.02164v5",
    "37": "1707.01213v3",
    "38": "2404.13648v1",
    "39": "1510.00149v5",
    "40": "2106.14568v4",
    "41": "1512.03385v1",
    "42": "2301.12900v2",
    "43": "1503.02531v1",
    "44": "1909.11957v6",
    "45": "1909.11957v6",
    "46": "2203.04248v1",
    "47": "1810.05331v2",
    "48": "1912.03458v2",
    "49": "2006.07253v1",
    "50": "2005.06870v1",
    "51": "1905.05934v1",
    "52": "2402.05406v3",
    "53": "1312.6120v3",
    "54": "1812.04368v2",
    "55": "1404.0736v2",
    "56": "1803.05729v1",
    "57": "2110.08232v4",
    "58": "2312.11983v1",
    "59": "2112.07198v1",
    "60": "2008.11062v1",
    "61": "2106.00134v1",
    "62": "2301.05345v2",
    "63": "1909.08174v1",
    "64": "1909.12778v3",
    "65": "2110.04869v2",
    "66": "2003.01794v3",
    "67": "2303.08774v6",
    "68": "2010.03533v2",
    "69": "2303.04185v2",
    "70": "1911.09817v2",
    "71": "2108.00708v1",
    "72": "2003.08935v1",
    "73": "2002.10179v2",
    "74": "1905.07830v1",
    "75": "2108.00259v3",
    "76": "2111.13445v5",
    "77": "1409.0575v3",
    "78": "1906.10771v1",
    "79": "2003.04297v1",
    "80": "2105.12971v1",
    "81": "2305.11627v3",
    "82": "2302.13971v1",
    "83": "2402.11187v2",
    "84": "2010.07611v2",
    "85": "2202.10203v1",
    "86": "1506.02626v3",
    "87": "1708.06519v1",
    "88": "1608.03665v4",
    "89": "1912.05671v4",
    "90": "2106.09685v2",
    "91": "2306.11222v2",
    "92": "2006.12156v2",
    "93": "2409.05211v1",
    "94": "2103.05861v1",
    "95": "1903.10258v3",
    "96": "2306.08543v5",
    "97": "2402.03766v1",
    "98": "2402.09748v1",
    "99": "1909.12326v5",
    "100": "1703.08651v2",
    "101": "2005.07683v2",
    "102": "2201.00043v1",
    "103": "2012.00596v3",
    "104": "2304.02840v1",
    "105": "1711.02017v3",
    "106": "1905.09717v5",
    "107": "2012.09243v2",
    "108": "1806.07572v4",
    "109": "2310.02980v4",
    "110": "2205.01068v4",
    "111": "2308.13137v3",
    "112": "2006.10621v3",
    "113": "1906.02773v2",
    "114": "2310.05175v4",
    "115": "2106.05933v2",
    "116": "2305.11203v3",
    "117": "1911.11641v1",
    "118": "2206.12562v1",
    "119": "2204.02311v5",
    "120": "1902.05967v3",
    "121": "2306.14525v2",
    "122": "2002.07376v2",
    "123": "2104.11832v2",
    "124": "1609.07843v1",
    "125": "2006.09081v5",
    "126": "2002.00585v1",
    "127": "2111.05754v1",
    "128": "2109.14960v3",
    "129": "2005.04275v1",
    "130": "2001.03554v1",
    "131": "2009.14410v3",
    "132": "1608.08710v3",
    "133": "1909.12579v1",
    "134": "2009.08576v2",
    "135": "2006.05467v3",
    "136": "2111.11802v4",
    "137": "2305.14314v1",
    "138": "2103.06460v3",
    "139": "2101.03697v3",
    "140": "2305.02190v1",
    "141": "1810.05270v2",
    "142": "1911.11134v3",
    "143": "2002.08797v5",
    "144": "2010.10732v2",
    "145": "1810.02340v2",
    "146": "2110.11395v2",
    "147": "2107.00166v4",
    "148": "2009.11094v2",
    "149": "1707.04780v2",
    "150": "2006.08218v5",
    "151": "2310.06694v2",
    "152": "2403.03853v3",
    "153": "2007.00389v1",
    "154": "2401.15024v2",
    "155": "1808.06866v1",
    "156": "2402.17177v3",
    "157": "2308.12792v3",
    "158": "1907.04840v2",
    "159": "2106.10404v4",
    "160": "1905.07785v2",
    "161": "2301.00774v3",
    "162": "2402.17946v4",
    "163": "2102.00554v1",
    "164": "2305.10924v3",
    "165": "2303.00566v2",
    "166": "2204.00408v3",
    "167": "2312.17244v2",
    "168": "2103.16547v3",
    "169": "2007.12223v2",
    "170": "1803.03635v5",
    "171": "2012.06908v2",
    "172": "2203.07259v3",
    "173": "2202.02643v1",
    "174": "1707.06342v1",
    "175": "2105.11228v1",
    "176": "1901.07827v2",
    "177": "1906.06110v1",
    "178": "2110.02743v2",
    "179": "1904.12368v2",
    "180": "1903.09291v1",
    "181": "2002.11794v2",
    "182": "2012.12877v2",
    "183": "2301.13741v3",
    "184": "2203.08243v1",
    "185": "2210.03044v1",
    "186": "1409.1556v6",
    "187": "2312.07533v4",
    "188": "2104.08500v4",
    "189": "1906.04358v2",
    "190": "2003.03033v1",
    "191": "2302.03773v1",
    "192": "1911.13299v2",
    "193": "2005.00561v2",
    "194": "2107.01808v1",
    "195": "2301.05219v2",
    "196": "2402.11700v2",
    "197": "2110.05667v1",
    "198": "2010.02350v2",
    "199": "2303.04935v2",
    "200": "2106.10784v1"
  }
}