{
  "survey": "This survey paper presents a comprehensive examination of deep neural network pruning techniques, focusing on taxonomy, comparison, analysis, and recommendations for optimizing neural network models through compression and pruning strategies. The study systematically categorizes structured and unstructured pruning methods, highlighting their significance in enhancing model efficiency and performance, especially in large-scale image recognition tasks. It explores novel approaches, such as dynamic and adaptive methods, which maintain robustness and adaptability across diverse applications. The comparative analysis reveals significant trade-offs between model size reduction and accuracy retention, emphasizing the need for balanced strategies. The impact on computational efficiency and resource utilization is profound, with techniques like channel gating and sparse momentum demonstrating substantial improvements in training and inference speeds. Case studies illustrate practical benefits in deploying efficient AI models in resource-constrained environments. Despite advancements, challenges remain in ensuring generalizability across architectures and maintaining fairness and explainability in pruned models. Future research should refine hybrid approaches, explore the role of knowledge distillation, and address ethical considerations related to transparency and equity. This survey advances the field of neural network pruning towards more robust and efficient AI deployments, driving progress in artificial intelligence and machine learning.\n\nIntroduction Significance of Deep Neural Network Pruning Pruning is a crucial technique in deep neural networks (DNNs) that enhances model efficiency and performance by compressing and optimizing network architectures. By removing redundant weights or neurons, pruning reduces model complexity and size, significantly improving computational and storage efficiency, particularly in large-scale models, while maintaining robustness against adversarial examples [1,2]. The demand for efficient convolutional neural networks (CNNs) without sacrificing performance has spurred the development of innovative pruning methods [3]. These methods are essential for deploying large CNNs on resource-constrained edge devices [4]. Techniques like channel gating optimize inference efficiency by dynamically bypassing computations on less significant input channels, thereby enhancing overall model performance [5]. In vision language models (VLMs), there is a growing need for efficient models suitable for mobile devices, aligning with broader research aimed at improving performance while ensuring resource efficiency [6]. Pruning is vital in mitigating the computational costs associated with methods such as Iterative Magnitude-based Pruning (IMP) [7]. By effectively reducing model size and complexity without compromising accuracy, pruning remains an indispensable tool for optimizing DNNs across various applications, facilitating their deployment in resource-limited environments. Scope of the Survey This survey focuses on deep neural network pruning techniques, emphasizing experiments, hyper-parameters, fairness principles, and network trainability, while intentionally excluding non-pruning compression methods to maintain a concentrated examination of pruning strategies [4]. It provides a detailed analysis of model compression techniques, particularly pruning methods relevant to large language models [1]. The survey investigates frameworks like SparseLLM, which address inefficiencies in traditional global pruning methods [8], and explores neural architecture search for adaptable channel and layer sizes [9]. Additionally, the survey discusses the lottery ticket hypothesis (LTH) and its implications for pre-trained models in computer vision [10], as well as pruning at initialization (PaI) methods, including sparse training and selection [11]. It thoroughly examines structured pruning techniques, filter ranking, regularization methods, dynamic execution, and neural architecture search [12], while unstructured pruning is referenced mainly for comparative purposes. The need for benchmarks to evaluate various pruning methods against common standards is also emphasized [10]. The survey addresses visual representation learning through contrastive self-supervised learning, aiming to simplify existing methods that require specialized architectures or memory banks [13]. It also tackles the challenge of effectively pruning neural networks while preserving performance, highlighting limitations of current iterative methods [11]. The scope is defined to ensure a comprehensive understanding of key themes in neural network pruning, deliberately omitting detailed technical implementations and specific application case studies to focus on overarching trends. Traditional pruning approaches involve a resource-intensive three-step process of pre-training, pruning, and re-training [14]. Recognizing the high resource demands of replicating and studying large language models, the survey extends its focus to large audio models, particularly transformer-based architectures, and their applications in Automatic Speech Recognition, Text-To-Speech, and Music Generation [9]. The survey considers benchmarks designed to evaluate the performance and contributions of individual attention heads in multi-head self-attention mechanisms within Transformer models [15]. It explores the optimization of object detection frameworks by integrating Neural Architecture Search, pruning, and Knowledge Distillation into a cohesive system [16]. The necessity for benchmarks to investigate trainable subnetworks within these models is highlighted, aligning with the broader research landscape of efficient model deployment [10]. Discussions on existing research related to LTH, challenges within the field, and potential future directions are included [1]. The survey excludes unrelated neural network optimization techniques that do not directly pertain to LTH, concentrating instead on the limitations of existing visual language model pre-training methods and proposing a systematic examination of pre-training design options [13]. The need for advancements in the compactness and efficiency of DNN structures is underscored, covering various compression techniques, including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design [1]. Structure of the Survey The survey is systematically organized to provide an in-depth exploration of deep neural network pruning techniques, commencing with an introduction that highlights the significance and scope of pruning in enhancing model efficiency and performance [1]. This introductory section lays the groundwork for the detailed examination of pruning strategies presented in subsequent sections. Following the introduction, the survey delves into the background and core concepts, offering an overview of deep neural networks, model compression, and optimization techniques [4]. This section acts as a primer for readers, ensuring clarity in the fundamental principles underlying neural network pruning. The taxonomy section categorizes and describes various pruning methods, including structured and unstructured approaches, novel techniques, dynamic and adaptive methods, as well as hybrid and multi-objective strategies [12]. This classification provides a systematic framework for understanding the diverse pruning methodologies employed in deep neural networks. In the comparison section, the survey evaluates and contrasts the effectiveness of different pruning techniques based on defined criteria such as computational efficiency and accuracy retention [11]. This comparative analysis is essential for identifying the strengths and limitations of various approaches, guiding the selection of suitable methods for specific applications. The analysis section examines the impact of pruning on model performance, discussing trade-offs between model size and accuracy, computational efficiency, resource utilization, and overall performance [6]. Real-world case studies and examples illustrate the practical implications of pruning techniques in various scenarios. The recommendations section provides insights and strategies for optimizing neural networks through pruning, addressing best practices, challenges, and emerging trends [7]. It also explores the role of knowledge distillation and emphasizes the importance of explainability and fairness in applied pruning techniques. The survey concludes with a summary of key findings and suggestions for future research, emphasizing areas that require further investigation to advance the field of neural network pruning [1]. This structured approach ensures a thorough exploration of the topic, enabling readers to gain a comprehensive understanding of the complexities and opportunities associated with deep neural network pruning.The following sections are organized as shown in . Background and Core Concepts Overview of Deep Neural Networks Deep neural networks (DNNs) are integral to advanced artificial intelligence, characterized by their layered architectures that facilitate complex pattern recognition and data representation [17]. They support a wide range of AI applications, including image classification and natural language processing, by modeling intricate data relationships [18]. Convolutional layers, such as those in RepVGG models, utilize 3 × 3 convolutions and ReLU activations to improve inference efficiency [19]. Residual learning has enabled the development of deeper networks, maintaining computational efficiency even at 152 layers [20], which is crucial for large-scale AI challenges. Transformers have revolutionized DNN architectures, particularly for tasks requiring long-sequence modeling and self-attention mechanisms [21]. These models excel in processing sequential data but require careful tuning to prevent overfitting [22]. Innovations like Pointer Sentinel-LSTM improve neural sequence modeling by enhancing contextual word generation [23]. In AI-generated content (AIGC), DNNs expand AI's capabilities in creative domains, challenging traditional weight training methods and highlighting DNN adaptability [13]. Benchmarks such as BoolQ and PIQA assess large pretrained models against human performance, emphasizing the need for deep understanding and commonsense reasoning in AI systems [14]. Vision-language model benchmarks address mobile performance limitations under constrained computational resources [6]. DNNs are indispensable in modern AI, driving innovation and enabling advancements across various fields through robust architectures, tackling complex challenges, and supporting diverse applications. Model Compression Techniques Model compression techniques are vital for enhancing DNN efficiency, especially in resource-constrained environments. These methods aim to reduce model size and complexity while maintaining or improving performance. Pruning is a key strategy, specifically targeting CNNs by eliminating less significant components [4]. Magnitude-based pruning removes redundant weights based on their magnitude, achieving compression with minimal accuracy loss [24]. The Prune-Adjust-Re-Prune (PARP) method further optimizes performance by fine-tuning sparse subnetworks within pre-trained models [25]. Model quantization reduces parameter precision, with techniques like QLoRA utilizing a frozen 4-bit quantized pre-trained language model and Low Rank Adapters for efficient fine-tuning [26]. LoRA embeds trainable low-rank matrices within each layer of a pre-trained Transformer model, facilitating adaptation to downstream tasks without increasing complexity [27]. Knowledge distillation involves a smaller student model replicating the behavior of a larger teacher model, achieving comparable performance with reduced complexity [1]. This approach is advantageous in scenarios demanding high performance under resource constraints. ParameterNet illustrates the potential of strategically increasing model parameters to enhance low-FLOPs model performance while maintaining computational efficiency [28]. Benchmarks are crucial for evaluating compressed models' generalizability across datasets and optimizers [10]. These compression techniques ensure DNNs retain high performance and adaptability for diverse tasks, significantly reducing computational, energy, and storage demands. This is crucial for IoT devices and large language models, where methods like pruning, quantization, and knowledge distillation minimize operational footprints without compromising accuracy, contributing to reduced carbon emissions and maintenance costs [29,1]. Their integration fosters advancements in AI and machine learning, enabling efficient deployment in large-scale and resource-constrained scenarios. Pruning Techniques and Their Importance Pruning techniques optimize DNNs by systematically reducing redundant parameters and connections, enhancing model efficiency and performance [4]. These techniques include structured, unstructured, and dynamic approaches, each contributing uniquely to refining neural network architectures. Structured pruning methods, such as Single Shot Structured Pruning (SSSP) and Second-Order Structured Pruning (SOSP), focus on entire channels or hidden units, using statistical or second-order information to remove less important structures [7]. These methods improve training speed and inference efficiency without compromising overall architecture. Channel gating, a dynamic and hardware-efficient pruning scheme, optimizes CNN inference by bypassing computations for less significant input channels [5]. This approach enhances model performance and aligns with the demand for resource-efficient neural networks, especially in edge computing environments. Unstructured pruning provides granular control over model sparsity, addressing error rate unpredictability across architectures and tasks [4]. This method fine-tunes model complexity, ensuring compact networks robust against adversarial attacks. The Elastic Lottery Ticket Hypothesis (E-LTH) offers a framework for transferring winning tickets between networks of varying depths within the same model family, enhancing efficiency by leveraging sparse subnetworks [7]. This hypothesis highlights pruned models' adaptability, facilitating deployment across diverse applications while maintaining high performance. These pruning strategies are essential for deploying robust AI systems in resource-constrained settings, driving advancements in machine learning and AI by strategically reducing model complexity while preserving or enhancing performance. By elucidating key pruning concepts and their role in enhancing neural network efficiency, these techniques decrease DNN size and energy consumption while maintaining or improving generalization capabilities. This ensures DNNs remain effective across diverse tasks and environments, particularly in mobile and edge devices. Furthermore, pruning techniques contribute to shorter training times and provide insights into model sparsity, leading to significant performance gains on real hardware [8,4]. Taxonomy of Pruning Techniques The taxonomy of pruning techniques is crucial for optimizing deep neural networks (DNNs), providing a framework that categorizes diverse methodologies aimed at enhancing model performance and efficiency. Table outlines the comparison between various pruning techniques, emphasizing their distinctive features and implications for the optimization of deep neural network models. As illustrated in , this figure categorizes pruning methods into structured versus unstructured pruning, novel pruning approaches, dynamic and adaptive pruning techniques, and hybrid and multi-objective pruning methods. Each category highlights specific strategies and their contributions to optimizing model efficiency, performance, and adaptability. This section explores the dichotomy between structured and unstructured pruning, emphasizing their distinct characteristics and advantages, which are vital for model optimization and innovation. Structured vs. Unstructured Pruning Structured and unstructured pruning are essential strategies for optimizing DNNs, each offering unique methodologies for improving model efficiency. Structured pruning systematically removes entire neurons, filters, or layers, simplifying network architecture and reducing computational complexity [24]. This approach is advantageous for balancing model size and performance, as demonstrated by Single Shot Structured Pruning, which eliminates entire channels and hidden units, unlike traditional weight pruning methods [30]. Techniques like the Gate Decorator exemplify structured pruning by using scaling factors to effectively remove filters [31]. However, structured pruning faces challenges such as dependency on backpropagation, which increases memory requirements and computational costs, limiting its applicability on constrained hardware [31]. Conversely, unstructured pruning targets individual weights, providing fine-grained control over model sparsity [24]. This method can achieve substantial model size reductions while preserving accuracy. Techniques addressing the challenge of pruning channels without significantly impacting accuracy demonstrate the flexibility of unstructured pruning [16]. Notably, the unique characteristics of each layer in large language models (LLMs), particularly the presence of activation outliers, complicate uniform pruning approaches [32]. As illustrated in , the categorization of pruning strategies in deep neural networks (DNNs) highlights the distinctions between structured, unstructured, and hybrid approaches. Structured pruning focuses on removing entire neurons or channels, while unstructured pruning targets individual weights for fine-grained control. Hybrid approaches combine both strategies to optimize computational efficiency, particularly in self-attention mechanisms. Hybrid approaches combining structured and unstructured sparsity offer comprehensive solutions for optimizing self-attention heads in Transformer models, enhancing computational efficiency [15]. This systematic categorization underscores the adaptability of pruning methods across various model families, though challenges persist in maintaining model accuracy while achieving high sparsity levels [16]. Novel Pruning Approaches Emerging pruning techniques are transforming the efficiency and performance of DNNs. The ManiDP approach integrates manifold information from all instances to dynamically prune redundant filters, ensuring robust performance across diverse datasets [33]. By leveraging instance-level data, this method offers a holistic strategy compared to traditional methods. The Elastic Lottery Ticket Hypothesis (E-LTH) simplifies the identification of winning tickets across varying architectures without exhaustive re-pruning [7]. This enhances the transferability of sparse subnetworks between models of different depths, improving adaptability and efficiency for various applications. The Robust Pruning Method (RPM) aligns training objectives of pre-training and fine-tuning with model robustness goals, ensuring pruned models achieve reduced complexity while maintaining resilience against adversarial attacks [2]. Collectively, these novel approaches significantly advance DNN pruning, enhancing adaptability, scalability, and efficiency across applications. They address increasing model sizes of Convolutional Neural Networks (CNNs) for edge deployment while leveraging sparsity to reduce memory footprints and accelerate training and inference. Despite challenges like the lack of standardized benchmarks, initiatives such as ShrinkBench aim to establish consistent evaluation frameworks, promoting reliable comparisons and ongoing innovation in model compression techniques [8,34,4]. Dynamic and Adaptive Pruning Techniques Dynamic and adaptive pruning techniques are crucial for optimizing DNNs, tailoring network structures to the characteristics of input data and activations. These techniques reduce the size and energy consumption of overparameterized networks, enhancing training and inference efficiency through strategies like pruning at initialization and feedback mechanisms. By dynamically adjusting sparsity patterns and reactivating important weights, they achieve state-of-the-art performance comparable to dense models while maintaining or improving generalization capabilities. Recent advances in pruning methodologies underscore their potential for efficient deployment on low-end devices [35,8,36]. An example is the Dynamic Pruning with Self-Supervised Mask Prediction (DPSMP) method, which predicts a mask for a layer based on the previous layer's activations [37]. This technique employs self-supervised learning to adjust network parameters dynamically, retaining only the most relevant components for optimized computational efficiency and accuracy. Channel gating exemplifies dynamic and adaptive pruning by adapting to input-specific features, selectively processing channels to reduce unnecessary computations and enhance overall efficiency [5]. By dynamically skipping computations for less significant channels, channel gating meets the demand for resource-efficient neural networks, particularly in edge computing environments. These techniques are essential for deploying robust AI systems in real-world applications where input data varies significantly. By leveraging network pruning, sparse representation, bits precision, knowledge distillation, and adaptive sparse connectivity, these methods ensure DNNs remain efficient and flexible, maintaining high performance across diverse tasks and environments, even in resource-constrained devices like those in Internet of Things (IoT) applications [38,1,39]. Hybrid and Multi-Objective Pruning Methods Hybrid and multi-objective pruning methods represent a sophisticated approach to neural network optimization, integrating multiple strategies to enhance model efficiency and performance. These methods address various objectives simultaneously, such as reducing model size, improving computational efficiency, and maintaining accuracy. The integration of structured and unstructured pruning techniques facilitates comprehensive optimization [15], enabling the removal of entire neurons or layers while targeting individual weights for balanced complexity reduction. Multi-objective pruning often incorporates knowledge distillation, where smaller models learn from larger counterparts, achieving similar performance levels with reduced complexity [1]. The synergy between pruning and distillation enhances adaptability for deployment in resource-constrained environments. Furthermore, incorporating neural architecture search (NAS) allows dynamic adjustment of model structures based on performance metrics, optimizing the balance between size, speed, and accuracy [16]. The exploration of trainable subnetworks within these models, as highlighted in [10], emphasizes the importance of benchmarks in evaluating hybrid pruning strategies' effectiveness. By leveraging a combination of pruning objectives, these methods ensure that neural networks remain robust and efficient across diverse applications, driving progress in artificial intelligence and machine learning. The continuous evolution of hybrid and multi-objective pruning techniques reflects the growing demand for versatile and efficient AI models capable of addressing complex challenges in the modern technological landscape. Comparison of Pruning Methods Exploring various pruning methods is vital for enhancing the efficiency and performance of deep neural networks (DNNs). Establishing a robust evaluation framework is essential for systematically comparing different pruning strategies and identifying the criteria that underpin their effectiveness. The following subsection outlines specific criteria for evaluating pruning methods, focusing on computational efficiency, accuracy retention, and adaptability across diverse neural network architectures. Criteria for Evaluation Evaluating pruning methods in DNNs requires a comprehensive framework that integrates multiple criteria to assess computational efficiency, accuracy retention, and overall performance enhancements. A fundamental criterion is reducing computational costs while maintaining high accuracy, ensuring pruned models remain effective across various applications [33]. Techniques like RPM exemplify this by measuring benign accuracy, empirical robust accuracy, and verifiable robust accuracy post-pruning, highlighting the importance of model robustness [2]. Accuracy retention is crucial in precision-sensitive tasks such as classification and detection, where maintaining high accuracy levels is essential. The evaluation process involves standardized testing protocols measuring accuracy and F1-score on a held-out test set, ensuring a balance between false positives and false negatives [6]. Performance is often assessed before and after fine-tuning, analyzing metrics such as accuracy and model size to ensure pruning does not adversely affect performance [3]. Computational efficiency is evaluated through metrics like floating-point operations (FLOPs), parameter counts, and inference speed, which are crucial for understanding the trade-offs between reducing model size and maintaining performance [7]. Evaluating pruning methods often involves measuring FLOPs reduction, model compression ratios, and accuracy drops due to pruning, providing insights into the impact on model efficiency [7]. The adaptability of pruning methods across different architectures is significant. The evaluation of E-LTH compared to traditional methods like IMP underscores the need for criteria that account for computational efficiency and adaptability, ensuring pruned models can be effectively deployed across diverse architectures [7]. This adaptability is crucial for optimizing neural networks in varied contexts, enabling the deployment of efficient AI models in resource-constrained environments. This is illustrated in , which depicts the key criteria for evaluating pruning methods in deep neural networks, focusing on accuracy retention, computational efficiency, and adaptability across architectures. Each criterion is supported by relevant metrics and methodologies from the reviewed literature, highlighting the importance of maintaining model performance while optimizing efficiency and deployment capabilities. Additionally, Table illustrates the diversity of benchmarks utilized in assessing the effectiveness of pruning methods, emphasizing the importance of maintaining accuracy and computational efficiency across different domains and tasks. Collectively, these criteria form a robust framework for evaluating pruning methods, guiding the selection of optimal strategies to enhance neural network efficiency and performance across various applications. By incorporating a range of metrics such as accuracy, computational efficiency, model size, and neuron-level redundancy, researchers can ensure pruned models remain effective and adaptable across diverse deployment scenarios. This approach addresses the high computational demands of large language models like GPT-3 while leveraging advanced pruning techniques, such as Globally Unique Movement (GUM) and Kronecker-factored curvature approximations, to enhance model performance and efficiency. Furthermore, methods like quantization, knowledge distillation, and efficient architecture design optimize models for resource-limited environments, reducing carbon emissions and maintenance costs while maintaining state-of-the-art performance [40,41,42,29]. Performance Metrics Performance metrics are crucial in assessing the effectiveness of pruning methods applied to DNNs, providing insights into trade-offs between model size reduction and accuracy retention. Accuracy, often evaluated through Top-1 and Top-5 measurements, indicates the model's capability to correctly classify inputs among the top predicted classes, essential for understanding the impact of pruning on model performance, especially in precision-sensitive applications like image classification [43]. The evaluation of Wanda on LLaMA and LLaMA-2 across various language benchmarks exemplifies this approach, measuring pruning effectiveness in preserving model accuracy [44]. Reductions in FLOPs and parameters reflect computational efficiency gained through pruning. Techniques like channel gating illustrate significant reductions in FLOPs and memory accesses while maintaining accuracy, providing essential metrics for evaluating pruning effectiveness [5]. Similarly, MetaPruning assesses performance by measuring accuracy and efficiency of pruned networks generated through PruningNet compared to baseline methods, emphasizing computational savings [45]. The performance gap between pruned and unpruned models is another critical metric, indicating the extent to which pruning affects model accuracy and generalization. This gap is measured by comparing the loss of pruned subnetworks with networks trained directly using gradient descent, providing insights into trade-offs involved in model optimization [46]. Additionally, correlation between estimated contributions and true importance is tracked, measuring changes in accuracy and resource metrics post-pruning [47]. In channel pruning methods, performance is assessed by measuring accuracy and computational speed before and after applying pruning, demonstrating significant speed-ups with minimal increases in error, evidenced by a 5× speed-up with only a 0.3 These performance metrics form a comprehensive framework for assessing the effectiveness of pruning techniques, guiding the selection of optimal strategies to enhance neural network efficiency and performance across various applications. By incorporating a comprehensive set of metrics, including accuracy, computational efficiency, and impact on model size, researchers can ensure pruned models remain effective and adaptable across diverse deployment scenarios. This approach is informed by recent insights into structured pruning methods, revealing the importance of neuron-level redundancy and efficacy of techniques like Globally Unique Movement (GUM) for enhancing model uniqueness. Advancements in model compression techniques, such as quantization and knowledge distillation, support efficient deployment of large language models in resource-constrained environments while maintaining high performance [40,41,42,29]. Comparative Analysis of Pruning Techniques Comparative analysis of pruning techniques in DNNs is essential for understanding their relative efficacy and applicability across diverse applications. This section evaluates various methodologies, focusing on their ability to reduce resource requirements while maintaining or enhancing model performance. Recent experiments with SCOP have demonstrated significant reduction in parameters (57.8 Structured pruning techniques, as demonstrated by Bonsai, offer substantial improvements over traditional methods, enabling pruning of larger models on limited hardware and outperforming existing methods in terms of efficiency and performance [31]. This approach underscores adaptability and effectiveness of structured pruning in enhancing model performance across varied contexts. Furthermore, GraNet significantly boosts sparse-to-sparse training performance without extending training time, indicating notable advancement in pruning techniques compared to dense-to-sparse methods [48]. These findings emphasize importance of innovative approaches in optimizing neural network architectures. Unstructured pruning methods, such as movement pruning, have been assessed for their ability to maintain network performance under high-sparsity conditions, offering insights into their effectiveness relative to state-of-the-art algorithms [49]. Survey also compares different sparsification methods, highlighting that sparse networks can achieve comparable or better performance than dense networks while being more efficient in terms of resource usage [8]. These comparisons provide valuable benchmarks for understanding trade-offs involved in different pruning approaches. Survey conducts comparative analysis of various large audio models, highlighting differences in effectiveness and approach across tasks like speech recognition and music generation [19]. Additionally, comparative analysis of various studies on Lottery Ticket Hypothesis (LTH) notes differences in effectiveness, methodologies, and outcomes across different approaches [50]. These analyses underscore diverse landscape of pruning methodologies, highlighting need for comprehensive evaluations to guide selection of optimal strategies for enhancing neural network efficiency and performance. This comparative analysis underscores diverse landscape of pruning methodologies, emphasizing need for comprehensive evaluations to guide selection of optimal strategies for enhancing neural network efficiency and performance. By integrating standardized metrics and benchmarks, researchers can ensure pruned models remain viable and effective across a wide range of deployment scenarios [1], [4]. Analysis of Pruning Impact Exploring the impact of pruning techniques on deep neural networks (DNNs) is essential for optimizing model performance. This section examines the trade-offs between model size and accuracy, focusing on critical considerations for effective pruning strategies to achieve an optimal balance for practical applications. Trade-offs Between Model Size and Accuracy Pruning techniques require a careful balance between reducing model size and maintaining accuracy, especially in resource-constrained environments. Deploying DNNs on IoT devices faces challenges due to high computational, energy, and storage demands, necessitating efficient strategies to navigate these trade-offs [1]. RPM exemplifies this by achieving a 10x compression while preserving 93 As illustrated in , the trade-offs in DNN optimization are depicted, emphasizing key model pruning strategies, the balance between accuracy and robustness, and the evaluation of methods across various applications. Innovative approaches are notable for their independence from network structure and ability to retain critical data, ensuring models remain effective despite significant size reductions [3]. These insights reveal intricate trade-offs in DNN optimization, where strategic pruning can effectively reduce model size while preserving accuracy. Advanced methods facilitate compact DNNs that maintain both benign accuracy and robustness against adversarial examples, significantly reducing computational and storage overheads. A structured pruning process, including pre-training, weight pruning, and fine-tuning, ensures pruned networks remain effective across diverse applications. Recent findings suggest that the architecture of pruned networks may be more crucial for efficiency than inherited weights, advocating pruning as an architecture search paradigm and emphasizing careful baseline evaluations in future research [51,2]. Impact on Computational Efficiency and Resource Utilization Pruning techniques significantly enhance computational efficiency and resource utilization in DNNs, enabling effective deployment in environments with limited computational capacities. The Single Shot Structured Pruning (SSSP) method exemplifies this by achieving a 2x speedup in training and a 3x increase in inference speed, demonstrating notable improvements in computational efficiency [30]. The Sparse Momentum method illustrates potential reductions in computational demands, achieving state-of-the-art performance in sparse training with error rates reduced by up to 5.61x compared to conventional methods [52]. Advancements like Conserved Energy Quantized Networks, particularly with the 4-bit quantization method exemplified by QLoRA, highlight pruning's impact on computational efficiency by enabling expansive models to be fine-tuned on standard hardware with reduced demands while preserving model integrity [26]. The ManiDP approach further demonstrates effective resource utilization, achieving a 55.3 Pruning Techniques and Their Impact on Performance Pruning techniques significantly influence DNN performance, offering diverse strategies to optimize model efficiency while preserving accuracy. Each method presents unique advantages and challenges, impacting model performance in various contexts. Movement pruning, for instance, adapts effectively during fine-tuning, resulting in improved performance compared to traditional methods [49]. This adaptability highlights movement pruning's potential to sustain high performance during model optimization. The SOSP method effectively captures global correlations, enhancing performance on large-scale tasks [53]. This approach emphasizes leveraging global information within networks to improve scalability and efficacy. Predictions based on scaling laws provide insights into potential accuracy retention in pruned networks, influencing overall model performance [54]. Experiments with winning ticket initializations indicate that these often achieve performance comparable to tickets generated on the same dataset, suggesting strong generalizability and robustness across different models [10]. This supports the notion that strategic pruning can uncover performant subnetworks within larger models, facilitating efficient deployment. The performance of MobileVLM V2, which achieves results comparable to larger models, demonstrates pruning's effectiveness in vision-language processing [6]. This underscores pruning's potential to enhance complex reasoning tasks and improve computational efficiency in DNNs. The impact of pruning techniques on DNN performance is profound, as each method offers distinct pathways to enhance model efficiency and accuracy. Pruning algorithms are essential for compressing and accelerating convolutional neural networks (CNNs) for deployment on edge devices by removing non-critical neurons, thus reducing memory footprint and training time. Sparse networks, achieved through strategic pruning, often generalize as well as or better than their dense counterparts, making them suitable for mobile applications. Contrary to common beliefs, some studies suggest training a large model before pruning may not be necessary, as pruned architectures can effectively serve as search paradigms for efficient models. However, the field lacks standardized benchmarks for comparing pruning techniques, a gap addressed by the introduction of ShrinkBench, a framework for standardized evaluation. This highlights the need for careful baseline evaluations and pruning's potential to redefine future workloads [51,8,34,4]. By judiciously selecting and applying these techniques, researchers can achieve significant improvements in model performance, advancing DNN deployment across various applications. Case Studies and Examples Pruning techniques in DNNs are exemplified through various real-world case studies, showcasing their impact on model efficiency and performance. The MobileNet series demonstrates how structured pruning can significantly reduce model size and computational demands, enabling efficient operation on mobile devices without sacrificing accuracy [34]. This illustrates pruning's practical benefits in deploying complex AI models in resource-constrained settings. In large-scale vision tasks, the SCOP technique has been employed to prune ResNet-101, achieving substantial reductions in parameters and FLOPs with minimal accuracy loss [34]. This case study highlights structured pruning's effectiveness in maintaining high performance while improving computational efficiency for large-scale image classification tasks. In natural language processing, pruning techniques applied to transformer models like BERT showcase potential for reducing model complexity while retaining language understanding capabilities. Pruning methods have successfully decreased the number of parameters in BERT models, facilitating deployment in environments with limited computational resources [34]. These examples underscore pruning techniques' versatility in optimizing diverse models across various domains. Despite successes, current studies often lack standardized metrics, limiting comparability and reliability of findings [34]. The absence of uniform evaluation criteria poses challenges in assessing pruning techniques' true impact across different applications. Addressing this gap is crucial for advancing the field and ensuring pruning strategies are effectively tailored to specific use cases. These case studies collectively underscore pruning techniques' transformative impact in enhancing DNN efficiency and performance across diverse applications. By removing non-critical or redundant neurons, pruning is pivotal for deploying large CNNs on edge devices, addressing growing energy and performance costs associated with deep learning. Pruning reduces memory footprint and accelerates training times, facilitating sparsity and enabling DNNs to generalize as effectively as their dense counterparts. Despite challenges like the lack of standardized benchmarks and difficulties in training pruned networks, innovative approaches like pruning at initialization and structured pruning algorithms are being explored to optimize network architectures. Furthermore, pruning serves as a valuable paradigm for architecture search, highlighting its potential to redefine model efficiency without relying on inherited weights from larger models [34,4,51,35,8]. By leveraging these methods, researchers can achieve significant improvements in model deployment, driving progress in artificial intelligence and machine learning. Recommendations for Neural Network Optimization Best Practices in Neural Network Pruning Effective neural network pruning requires strategic methodologies to optimize model size reduction while preserving performance. Evaluating parameter importance based on their contribution to loss is recommended to retain crucial parameters, ensuring model accuracy during size reduction [25]. Utilizing Block Influence scores to identify and remove redundant layers further enhances model efficiency by optimizing resource allocation [15]. Movement pruning, focusing on weights with minimal impact on performance during training, is vital for maintaining accuracy [2]. Incorporating modified initialization techniques to enhance gradient flow and overall model performance fosters a conducive architecture for effective pruning [30]. Rapid post-training pruning frameworks facilitate quick model pruning without retraining, reducing inference costs while preserving accuracy [5]. The CoFi method achieves high-speed performance without reliance on unlabeled data, demonstrating comparable accuracy to distillation methods [33]. In neural machine translation (NMT) models, identifying weights, calculating pruning thresholds, and retraining the pruned model ensures retention of linguistic capabilities while optimizing efficiency [16]. BIRNN-T model optimization illustrates best practices by focusing on systematic training improvements [6]. Collectively, these practices advocate for comprehensive approaches integrating automated processes, adaptive techniques, and strategic fine-tuning. Employing advanced methodologies such as knowledge distillation and neural network compression significantly enhances performance while minimizing computational and storage demands, ensuring robust deployment across various AI applications, including resource-constrained IoT devices [38,1]. Challenges in Neural Network Pruning Despite its advantages, neural network pruning faces several challenges that may hinder its effectiveness. A primary obstacle is the dependency on specific architectures and weight distributions, limiting the generalizability of pruning methods [55]. Over-pruning risks performance decline, emphasizing the need for balanced strategies that reduce complexity while preserving accuracy [9]. Current literature often inadequately addresses the trade-offs between sparsity and model accuracy, lacking comprehensive guidelines for effective implementation [8]. Norm-based criteria may negatively affect performance, as they might not capture nuanced task requirements [56]. Methods such as OWL may struggle to accurately identify activation outliers, impacting applied sparsity ratios [32]. Implementing deep compression methods and retraining post-pruning and quantization can be resource-intensive, particularly in environments with limited computational capacity [24]. The assumption that winning tickets can be universally adapted across architectures may not universally apply, complicating adaptability [7]. This is illustrated in , which categorizes the primary challenges faced in neural network pruning into three main areas: architecture dependency, performance trade-offs, and resource intensity. Each category highlights specific issues such as the reliance on specific architectures and weight distributions, the risks of over-pruning and the trade-off between sparsity and accuracy, as well as the resource-intensive nature of deep compression methods. Addressing these challenges is crucial for enhancing the applicability and success of pruning techniques in real-world scenarios [4]. Challenges and Future Directions Neural network pruning presents challenges that require continued research to improve its applicability and effectiveness. Ensuring the generalizability of pruning methods across diverse architectures while maintaining representation transferability is a significant challenge. Future research could focus on optimizing rank selection processes and extending techniques like LoRA beyond Transformers [27]. Refining outlier detection and applying OWL to architectures beyond large language models could bolster pruning robustness [32]. Efficient pruning methods in large-scale models remain challenging; future efforts should refine quantization techniques and address challenges in applying QLoRA to various architectures [26]. Optimizing auxiliary connections and exploring Feature Boost Selection beyond CNNs presents promising directions [57]. Enhancing compute-aware scoring mechanisms and adapting Single Shot Structured Pruning to different architectures could guide future research [30]. Investigating further optimizations in Hessian approximation and applying Second-Order Structured Pruning to other architectures could yield significant advancements [53]. Refining ranking processes and exploring Gate-Oriented Hybrid Sparsity Pruning beyond Vision Transformers could enhance efficiency [15]. Examining the generalizability of winning ticket initializations in other domains and architectures, as proposed by the Elastic Lottery Ticket Hypothesis, is recommended [10]. Optimizing pruning and adjustment processes and applying Prune-Adjust-Re-Prune beyond speech recognition could broaden pruning applicability [25]. Enhancing Bonsai's applicability to different architectures could improve efficiency [31]. Future research could focus on improving manifold information extraction techniques to boost pruning effectiveness [33]. Refining clustering algorithms used in pruning methods and exploring their application beyond CNNs could provide valuable insights [3]. Emerging hybrid approaches combining pruning with other optimization techniques, along with adaptive pruning methods responsive to deployment conditions, are suggested for exploration [4]. Integrating multiple compression techniques and addressing limitations to enhance DNN deployment on IoT devices remain critical areas for future research [1]. Optimizing the pruning process and broadening RPM's application to various architectures and datasets present promising directions [2]. Enhancements to channel gating mechanisms and their integration with other optimization techniques warrant investigation [5]. Optimizing MobileVLM V2 and examining its performance across diverse tasks and datasets are recommended [6]. These directions emphasize the need for robust methodologies that tackle current challenges while optimizing model efficiency and performance, ensuring the ongoing advancement of neural network pruning in AI and machine learning [7]. Emerging Trends and Hybrid Approaches Emerging trends in neural network pruning highlight adaptive methods and integrating new architectures leveraging structured pruning [58]. Hybrid models combining pruning with compression techniques like quantization and knowledge distillation represent promising directions for improving efficiency and performance. New architectures supporting structured pruning streamline optimization, enhancing scalability across applications. Integrating neural architecture search with pruning methodologies offers potential for tailoring model structures to performance criteria, optimizing size-speed balance for diverse scenarios. NPAS exemplifies this by merging structured pruning with compiler-aware optimizations, utilizing reinforcement learning and Bayesian optimization for efficient search space navigation, improving inference times and accuracy on mobile platforms [59,4]. Hybrid strategies leveraging structured and unstructured sparsity principles significantly enhance computational efficiency by reducing size and resource demands without compromising accuracy. These strategies maximize performance gains by addressing neuron-level redundancies and optimizing neuron distinctiveness, as evidenced by evaluations in generative language models like GPT architectures [8,42]. Optimizing parameter distribution and learning dependencies within models, hybrid approaches are poised to influence machine learning's future landscape, paving the way for efficient AI deployment in resource-constrained environments. Continued exploration and refinement of these trends play a crucial role in advancing neural network optimization, ensuring AI technologies remain innovative and practically applicable. Role of Knowledge Distillation Knowledge Distillation (KD) complements pruning techniques, enhancing DNN capabilities by transferring knowledge from larger teacher models to smaller student models [60]. This process is advantageous when pruned models need guidance to maintain accuracy and generalization despite reduced complexity. KD enables pruned models to achieve performance levels comparable to unpruned counterparts, ensuring size reduction benefits do not compromise accuracy [61]. The synergy between KD and pruning enhances model transferability, preserving essential features and decision pathways potentially lost during pruning [61]. KD fosters a robust optimization framework, enabling pruned models to retain high performance while benefiting from reduced computational demands. KD enhances pruned models' adaptability across applications, ensuring distilled models mimic teacher models' behavior and insights, maintaining high performance in resource-constrained environments [60]. KD's role with pruning underscores its significance in developing efficient AI systems, driving advancements in machine learning by optimizing performance and efficiency. Explainability and Fairness in Pruning Integrating explainability and fairness in pruning techniques ensures DNNs operate transparently and equitably across applications. Explainability involves clarifying the rationale behind removing specific network components, enhancing model decision interpretability [4]. Transparency is vital for validating pruned models' integrity, especially in sensitive domains like healthcare and finance. Fairness addresses equitable treatment of data representations, ensuring pruning does not disproportionately affect certain classes or groups [1]. Fairness-aware techniques aim to balance minority class representation, mitigating biases from uneven pruning across data segments [7]. Incorporating fairness principles ensures pruned models maintain equitable performance across demographic and categorical data. The importance of explainability and fairness is underscored by the need for robust evaluation frameworks assessing pruning's impact on transparency and bias [6]. Such frameworks facilitate identifying biases introduced during pruning, guiding strategies to enhance fairness and accountability. Integrating explainability and fairness represents a critical advancement in neural network optimization, ensuring models achieve efficiency and performance gains while adhering to ethical standards. Focusing on cutting-edge AI-generated content technologies, improving commonsense reasoning in natural language processing, optimizing attention mechanisms, and incorporating biologically-inspired networks in speech recognition can advance AI systems that are effective and socially responsible [62,63,64,65,14]. Conclusion The exploration of deep neural network pruning within this survey has delineated various methodologies, underscoring their pivotal contribution to augmenting model efficiency and performance. Both structured and unstructured pruning approaches are essential in refining deep neural networks, particularly in extensive image recognition tasks, where deeper networks markedly enhance accuracy. The survey highlights innovative pruning strategies, such as dynamic and adaptive techniques, which hold potential for sustaining model resilience and flexibility across a wide range of applications. Through comparative analysis, the survey identifies significant trade-offs between reducing model size and maintaining accuracy, highlighting the need for balanced approaches to address these challenges. The impact of pruning on computational efficiency and resource utilization is evident, with techniques like channel gating and sparse momentum offering notable improvements in training and inference speeds. Practical case studies emphasize the benefits of pruning in deploying effective AI models in environments with limited resources. Despite these developments, ongoing challenges remain in ensuring the applicability of pruning methods across different architectures and in upholding fairness and transparency in pruned models. Future research should focus on enhancing hybrid strategies that combine multiple optimization techniques, exploring the contribution of knowledge distillation to improving pruned models, and considering ethical issues related to transparency and fairness in AI systems. Advancements in these areas will drive the field of neural network pruning towards more resilient and efficient AI implementations, facilitating progress in artificial intelligence and machine learning.",
  "reference": {
    "1": "2010.03954v1",
    "2": "1906.06110v1",
    "3": "1803.05729v1",
    "4": "2005.04275v1",
    "5": "1805.12549v2",
    "6": "2402.03766v1",
    "7": "2103.16547v3",
    "8": "2102.00554v1",
    "9": "2010.07611v2",
    "10": "1906.02773v2",
    "11": "2012.09243v2",
    "12": "2204.00408v3",
    "13": "2006.08218v5",
    "14": "2110.02743v2",
    "15": "2301.05345v2",
    "16": "1707.06342v1",
    "17": "2106.10784v1",
    "18": "2107.00166v4",
    "19": "2308.12792v3",
    "20": "2003.04297v1",
    "21": "2312.07533v4",
    "22": "2205.01068v4",
    "23": "2303.08774v6",
    "24": "1510.00149v5",
    "25": "2106.05933v2",
    "26": "2305.14314v1",
    "27": "2106.09685v2",
    "28": "2306.14525v2",
    "29": "2401.15347v1",
    "30": "2007.00389v1",
    "31": "2402.05406v3",
    "32": "2310.05175v4",
    "33": "2103.05861v1",
    "34": "2003.03033v1",
    "35": "2002.08797v5",
    "36": "2006.07253v1",
    "37": "2110.08232v4",
    "38": "1503.02531v1",
    "39": "1707.04780v2",
    "40": "2312.17244v2",
    "41": "2308.07633v4",
    "42": "2302.03773v1",
    "43": "1707.06168v2",
    "44": "2306.11695v3",
    "45": "1903.10258v3",
    "46": "2003.01794v3",
    "47": "1906.10771v1",
    "48": "2010.10732v2",
    "49": "2110.11395v2",
    "50": "2106.10404v4",
    "51": "2005.07683v2",
    "52": "2403.04861v2",
    "53": "1810.05270v2",
    "54": "1608.03665v4",
    "55": "1810.05331v2",
    "56": "2006.10621v3",
    "57": "2002.00585v1",
    "58": "2301.12900v2",
    "59": "2303.00566v2",
    "60": "2012.00596v3",
    "61": "2402.13116v4",
    "62": "2109.14960v3",
    "63": "1911.11641v1",
    "64": "2402.17177v3",
    "65": "2304.06632v1",
    "66": "1905.10650v3"
  },
  "chooseref": {
    "1": "2401.15347v1",
    "2": "2204.09656v2",
    "3": "2009.11839v4",
    "4": "2105.10065v1",
    "5": "1906.06307v2",
    "6": "2306.11695v3",
    "7": "2002.05709v3",
    "8": "2403.04861v2",
    "9": "2010.03954v1",
    "10": "2402.13116v4",
    "11": "2308.07633v4",
    "12": "1804.03294v3",
    "13": "2102.06790v2",
    "14": "2101.10552v1",
    "15": "2304.06632v1",
    "16": "1802.03494v4",
    "17": "2210.04092v4",
    "18": "2010.11929v2",
    "19": "1905.09418v2",
    "20": "1902.01996v4",
    "21": "1905.10650v3",
    "22": "1706.03762v7",
    "23": "2403.14729v1",
    "24": "2105.06990v2",
    "25": "1905.10044v1",
    "26": "2203.04570v1",
    "27": "1805.12549v2",
    "28": "1707.06168v2",
    "29": "2106.04533v3",
    "30": "2102.07156v1",
    "31": "2003.02389v1",
    "32": "1606.09274v1",
    "33": "1806.09055v2",
    "34": "2003.13683v3",
    "35": "2005.03354v2",
    "36": "2004.02164v5",
    "37": "1707.01213v3",
    "38": "2404.13648v1",
    "39": "1510.00149v5",
    "40": "2106.14568v4",
    "41": "1512.03385v1",
    "42": "2301.12900v2",
    "43": "1503.02531v1",
    "44": "1909.11957v6",
    "45": "1909.11957v6",
    "46": "2203.04248v1",
    "47": "1810.05331v2",
    "48": "1912.03458v2",
    "49": "2006.07253v1",
    "50": "2005.06870v1",
    "51": "1905.05934v1",
    "52": "2402.05406v3",
    "53": "1312.6120v3",
    "54": "1812.04368v2",
    "55": "1404.0736v2",
    "56": "1803.05729v1",
    "57": "2110.08232v4",
    "58": "2312.11983v1",
    "59": "2112.07198v1",
    "60": "2008.11062v1",
    "61": "2106.00134v1",
    "62": "2301.05345v2",
    "63": "1909.08174v1",
    "64": "1909.12778v3",
    "65": "2110.04869v2",
    "66": "2003.01794v3",
    "67": "2303.08774v6",
    "68": "2010.03533v2",
    "69": "2303.04185v2",
    "70": "1911.09817v2",
    "71": "2108.00708v1",
    "72": "2003.08935v1",
    "73": "2002.10179v2",
    "74": "1905.07830v1",
    "75": "2108.00259v3",
    "76": "2111.13445v5",
    "77": "1409.0575v3",
    "78": "1906.10771v1",
    "79": "2003.04297v1",
    "80": "2105.12971v1",
    "81": "2305.11627v3",
    "82": "2302.13971v1",
    "83": "2402.11187v2",
    "84": "2010.07611v2",
    "85": "2202.10203v1",
    "86": "1506.02626v3",
    "87": "1708.06519v1",
    "88": "1608.03665v4",
    "89": "1912.05671v4",
    "90": "2106.09685v2",
    "91": "2306.11222v2",
    "92": "2006.12156v2",
    "93": "2409.05211v1",
    "94": "2103.05861v1",
    "95": "1903.10258v3",
    "96": "2306.08543v5",
    "97": "2402.03766v1",
    "98": "2402.09748v1",
    "99": "1909.12326v5",
    "100": "1703.08651v2",
    "101": "2005.07683v2",
    "102": "2201.00043v1",
    "103": "2012.00596v3",
    "104": "2304.02840v1",
    "105": "1711.02017v3",
    "106": "1905.09717v5",
    "107": "2012.09243v2",
    "108": "1806.07572v4",
    "109": "2310.02980v4",
    "110": "2205.01068v4",
    "111": "2308.13137v3",
    "112": "2006.10621v3",
    "113": "1906.02773v2",
    "114": "2310.05175v4",
    "115": "2106.05933v2",
    "116": "2305.11203v3",
    "117": "1911.11641v1",
    "118": "2206.12562v1",
    "119": "2204.02311v5",
    "120": "1902.05967v3",
    "121": "2306.14525v2",
    "122": "2002.07376v2",
    "123": "2104.11832v2",
    "124": "1609.07843v1",
    "125": "2006.09081v5",
    "126": "2002.00585v1",
    "127": "2111.05754v1",
    "128": "2109.14960v3",
    "129": "2005.04275v1",
    "130": "2001.03554v1",
    "131": "2009.14410v3",
    "132": "1608.08710v3",
    "133": "1909.12579v1",
    "134": "2009.08576v2",
    "135": "2006.05467v3",
    "136": "2111.11802v4",
    "137": "2305.14314v1",
    "138": "2103.06460v3",
    "139": "2101.03697v3",
    "140": "2305.02190v1",
    "141": "1810.05270v2",
    "142": "1911.11134v3",
    "143": "2002.08797v5",
    "144": "2010.10732v2",
    "145": "1810.02340v2",
    "146": "2110.11395v2",
    "147": "2107.00166v4",
    "148": "2009.11094v2",
    "149": "1707.04780v2",
    "150": "2006.08218v5",
    "151": "2310.06694v2",
    "152": "2403.03853v3",
    "153": "2007.00389v1",
    "154": "2401.15024v2",
    "155": "1808.06866v1",
    "156": "2402.17177v3",
    "157": "2308.12792v3",
    "158": "1907.04840v2",
    "159": "2106.10404v4",
    "160": "1905.07785v2",
    "161": "2301.00774v3",
    "162": "2402.17946v4",
    "163": "2102.00554v1",
    "164": "2305.10924v3",
    "165": "2303.00566v2",
    "166": "2204.00408v3",
    "167": "2312.17244v2",
    "168": "2103.16547v3",
    "169": "2007.12223v2",
    "170": "1803.03635v5",
    "171": "2012.06908v2",
    "172": "2203.07259v3",
    "173": "2202.02643v1",
    "174": "1707.06342v1",
    "175": "2105.11228v1",
    "176": "1901.07827v2",
    "177": "1906.06110v1",
    "178": "2110.02743v2",
    "179": "1904.12368v2",
    "180": "1903.09291v1",
    "181": "2002.11794v2",
    "182": "2012.12877v2",
    "183": "2301.13741v3",
    "184": "2203.08243v1",
    "185": "2210.03044v1",
    "186": "1409.1556v6",
    "187": "2312.07533v4",
    "188": "2104.08500v4",
    "189": "1906.04358v2",
    "190": "2003.03033v1",
    "191": "2302.03773v1",
    "192": "1911.13299v2",
    "193": "2005.00561v2",
    "194": "2107.01808v1",
    "195": "2301.05219v2",
    "196": "2402.11700v2",
    "197": "2110.05667v1",
    "198": "2010.02350v2",
    "199": "2303.04935v2",
    "200": "2106.10784v1"
  }
}