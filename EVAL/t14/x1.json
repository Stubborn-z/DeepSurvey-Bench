{
  "survey": "Deep neural network pruning is a crucial technique for optimizing model efficiency, particularly in resource-constrained environments like IoT and mobile devices. This survey provides a comprehensive review of pruning methodologies, presenting a taxonomy that categorizes methods into structured and unstructured, static and dynamic, and novel and hybrid approaches. The survey evaluates these methods based on effectiveness and efficiency metrics, highlighting their impact on model performance, computational cost, and deployment feasibility. Key findings emphasize the significance of dynamic pruning strategies, such as those utilizing feedback signals and sparsity patterns, which optimize weight pruning in a single training pass. Additionally, the survey explores the role of pruning in specific applications, including NLP, computer vision, and federated learning, underscoring the importance of tailored strategies for different domains. Recommendations for optimizing network structures focus on selecting appropriate pruning methods and integrating advanced optimization techniques like Differentiable Architecture Search (DARTS) and neuroregeneration processes. Future research directions include developing hybrid approaches and enhancing pruning strategies to improve generalization and robustness. Overall, this survey highlights the indispensable role of pruning in advancing deep neural networks, advocating for sustainable and efficient AI systems.\n\nIntroduction Significance of Deep Neural Network Pruning Deep neural network pruning is essential for enhancing model efficiency and performance, especially in resource-constrained environments such as IoT and mobile devices, where large model sizes pose significant challenges [1,2]. By systematically reducing the complexity of neural networks, pruning facilitates deployment across various domains, including neural machine translation and automatic speech recognition (ASR), which often suffer from over-parameterization [3]. In convolutional neural networks (CNNs) and Vision Transformers (ViTs), pruning addresses high computational costs and storage overheads. The removal of non-critical neurons and the identification of sparse subnetworks enable substantial reductions in model size and computational demands, enhancing CNN efficiency without compromising accuracy [4]. Pruning in ViTs presents unique challenges due to their distinct architecture, necessitating innovative strategies for effective model compression [5]. Moreover, pruning optimizes models for specific applications, such as ASR, by identifying efficient subnetworks within pre-trained models, thereby improving performance in low-resource settings [3]. This process mitigates computational and storage overheads, crucial for enhancing the overall efficiency of deep neural networks [6]. By focusing on coarse-grained structures like feature maps and channels, pruning effectively reduces inference time and memory costs, making it a valuable strategy for optimizing deployment on standard hardware [7]. Deep neural network pruning is a vital strategy for optimizing model architectures, enabling efficient deployment on edge devices and mobile platforms by reducing model size and computational costs. This technique involves removing non-critical or redundant neurons, maintaining or enhancing performance levels similar to dense networks. Pruning decreases memory footprint, accelerates training and inference, and supports structured sparsity, which aligns better with hardware implementations. Consequently, pruning facilitates the deployment of neural networks across diverse applications while addressing challenges related to energy consumption and computational efficiency [8,9,4]. Objectives of the Survey This survey aims to comprehensively explore deep neural network pruning methodologies, addressing inefficiencies and complexities that hinder effectiveness across various tasks and architectures [10]. It presents a detailed taxonomy of pruning techniques, elucidating parameters that influence network structure to minimize extensive trial and error. A significant focus is on developing innovative model compression methods that leverage dynamic allocation of sparsity patterns and feedback signals, optimizing weight pruning in a single training pass [6]. The survey emphasizes unified frameworks, such as the Unified GNN Sparsification (UGS) framework, which simultaneously prunes graph adjacency matrices and model weights, enhancing inference on large-scale graphs [1]. Additionally, insights into the Lottery Ticket Hypothesis (LTH) are provided, establishing a platform for experiments and comparisons with updated baselines [11]. The survey also addresses the challenge of compressing large language models while maintaining accuracy, focusing on various compression techniques, including pruning [3]. By advancing the understanding of pruning strategies and their applications in modern AI systems, the survey contributes to the development of efficient, high-performance neural network models [4]. Notably, methods such as the Gate Decorator, which uses channel-wise scaling factors for effective filter pruning, are highlighted, setting unimportant filters to zero [12]. Structure of the Survey This survey is structured to guide readers through the multifaceted domain of deep neural network pruning, offering a comprehensive understanding of its methodologies and applications. It begins with an introduction to the significance of pruning in enhancing model efficiency and performance, particularly in resource-constrained environments [13]. Following this, the objectives are outlined, emphasizing the taxonomy of pruning techniques and the development of innovative model compression methods, such as dynamic allocation of sparsity patterns and feedback signals [14]. The background section provides foundational knowledge on deep neural networks, model compression, and network optimization strategies, defining key concepts such as pruning, sparsity, and efficiency. This sets the stage for a detailed taxonomy of pruning techniques, categorizing methods into structured and unstructured, static and dynamic approaches, alongside novel and hybrid techniques [14]. The comparison section evaluates pruning methods based on effectiveness and efficiency metrics, discussing trade-offs and the impact of different model architectures. Subsequent sections analyze pruning's impact on model performance, addressing accuracy, inference time, resource utilization, generalization, and robustness. Recommendations for optimizing network structures are provided, offering strategies for selecting pruning methods, exploring future research directions, and integrating advanced optimization techniques [13]. The conclusion synthesizes key findings, underscoring the role and future of pruning in advancing deep neural networks and AI technology. Through this structured approach, the survey facilitates a comprehensive exploration of deep neural network pruning, aiding researchers and practitioners in navigating this complex field.The following sections are organized as shown in . Background and Definitions Overview of Deep Neural Networks Deep neural networks (DNNs) emulate hierarchical information processing akin to human cognition, enabling complex pattern recognition across applications like image classification and language modeling [15,16]. Their architecture typically includes an input layer, multiple hidden layers, and an output layer, facilitating data transformation through non-linear functions [17]. Techniques such as residual connections and batch normalization are employed to stabilize training by addressing issues like vanishing gradients [15]. Specialized architectures, including recurrent neural networks (RNNs) and attention-based models, are critical for sequential data processing, enhancing tasks like time-series forecasting and natural language processing [16]. Differentiable architecture search methods and weight-agnostic models represent recent advancements, optimizing network structures and transforming custom architecture development [15]. Experimental findings reveal robust performance across AI tasks for both large Transformer models and smaller baselines, highlighting their importance in computational applications [15]. Initiatives such as Mobile Vision Language Models (MobileVLM) focus on improving DNN performance in resource-constrained environments [17]. DNNs are integral to AI systems, with ongoing optimizations addressing computational challenges, facilitating widespread integration across domains, including IoT applications [6]. However, the computational, energy, and storage demands pose challenges for resource-constrained IoT devices. Compression techniques like network pruning, sparse representation, and knowledge distillation mitigate these issues while maintaining accuracy [1]. Research emphasizes recognizing the heterogeneous nature of DNN layers, optimizing robustness and performance by distinguishing between \"ambient\" and \"critical\" layers [18]. In automatic speech recognition, biologically-inspired models are explored to enhance efficiency and reduce latency [6,16,19]. Model Compression Techniques Model compression is crucial for optimizing deep neural networks, reducing computational and memory requirements while preserving performance. Pruning, a fundamental method, eliminates redundant structures to decrease model complexity. Techniques like Global Structural Pruning (GSP) and Single Shot Structured Pruning (SSSP) exemplify systematic approaches by redistributing parameters and applying structured pruning pre-training [4,1]. Quantization reduces the precision of weights and activations, leading to smaller models and faster computations, while Deep Compression integrates pruning, quantization, and Huffman coding to reduce storage needs [1]. Knowledge distillation transfers knowledge from larger to smaller models, enhancing deployment efficiency [1]. Compact architecture designs and dynamic networks, such as Vision Transformers (ViTs) and Channel Gating, address computational costs by selectively pruning filters [7]. Frameworks like MobileVLM V2 improve performance through architectural enhancements and tailored training schemes [17]. The Robust Pruning Method (RPM) includes pre-training, weight pruning, and fine-tuning for compactness without performance loss [6]. Despite advancements, challenges persist in structured pruning for large language models, which require backward passes that complicate processes and increase resource consumption. Techniques like Channel Gating and RPM optimize CNNs for speed and size, enhancing network efficiency and hardware compatibility [20,7]. Model compression techniques are essential for optimizing neural networks, facilitating diverse applications while maintaining performance and efficiency. By employing methods like network pruning, sparse representation, bit precision, and knowledge distillation, these techniques alleviate computational costs and memory requirements, enabling deployment on resource-constrained devices and expanding applications across domains like IoT and edge computing [4,1]. Network Optimization Strategies Network optimization strategies enhance neural network performance by improving efficiency, reducing computational costs, and increasing accuracy. Layer-wise adaptive strategies prune or adjust network layers based on their contribution to performance, enabling dynamic optimization during training [10]. Feedback signals dynamically adjust sparsity patterns, optimizing weight pruning in a single training pass, reducing retraining cycles [6]. The Unified GNN Sparsification (UGS) framework exemplifies sophisticated optimization by simultaneously pruning graph adjacency matrices and model weights, enhancing inference on large-scale graphs [1]. The Lottery Ticket Hypothesis (LTH) identifies efficient subnetworks within over-parameterized models, offering experimental comparison platforms [11]. Channel-wise scaling factors in methods like Gate Decorator optimize performance by nullifying unimportant filters, improving speed and accuracy [12]. Structured pruning methods face challenges due to backward pass complexity, but advancements like Channel Gating address these by selectively skipping computations on less impactful channels [7]. Network optimization strategies maximize deep neural networks' performance, ensuring efficient deployment while maintaining accuracy and resource utilization. These strategies tackle deep learning models' complexities, facilitating continuous improvement and adaptation to evolving computational demands. Key Concepts: Pruning, Sparsity, and Efficiency Pruning, sparsity, and efficiency are foundational concepts in optimizing deep neural networks, enhancing computational performance and resource utilization. Pruning reduces network parameters to create sparse architectures, essential for minimizing computational, memory, and energy requirements without significant performance loss [6]. This technique is advantageous for deploying neural networks on resource-constrained devices [21]. Sparsity results from pruning, characterized by zero-valued parameters within a network, reducing redundancy and computational overhead. Structured pruning methods eliminate entire channels or hidden units, leveraging sparsity for substantial model compression [6]. The Elastic Lottery Ticket Hypothesis (E-LTH) transfers sparse patterns across architectures, supporting efficient adaptation through structural modifications [21]. Efficiency denotes the ability to perform tasks with minimal resources while maintaining accuracy, achieved through compute-aware scoring mechanisms and dynamic pruning strategies [6]. These methods ensure sparsity and pruning conserve resources and enhance network performance by dynamically adapting to computational needs [6]. Challenges remain in maximizing pruning and sparsity potential across diverse architectures. Dynamic pruning must overcome independent instance operation limitations to leverage input instance interplay [21]. Addressing these inefficiencies is crucial for advancing pruning's practical implementation in AI systems. Refining network structures and optimizing resource allocation provide pathways for adapting to evolving computational demands and ensuring sustainable advancement of neural network technologies. Importance in Modern AI Applications Integrating pruning, sparsity, and efficiency into modern AI applications enhances computational performance while minimizing resource consumption. These strategies mitigate computational demands and environmental impacts associated with training large-scale models [22]. Pruning contributes to sustainability, crucial for large-scale AI deployments, and optimizing models on devices with constrained resources is imperative in federated learning contexts [23]. Differentiating network layers into 'ambient' and 'critical' categories supports targeted pruning strategies, enhancing model efficiency during training and re-initialization phases [18]. Sparse networks improve performance in object recognition tasks, evidenced by benchmarks like CIFAR-10, SmallNORB, and FashionMNIST [24]. The adoption of Transformer models, reducing reliance on traditional convolutional networks, highlights a shift towards architectures managing complex tasks with lower computational loads [25]. Understanding and generalizing winning ticket initializations optimize model architectures and training processes [26]. The benchmark for assessing 'winning tickets' within major DNN architectures underscores efficient model architecture pursuit in deep learning research [27]. Sparse architectures exhibit versatility across domains, handling diverse audio tasks within Large Audio Models [28]. Efficient model structures enhance learning efficiencies in contrastive learning without large training batches [29]. On mobile devices, sparsity is critical for avoiding performance bottlenecks, demonstrated by benchmarks for mobile vision language models [17]. Evaluating AI models against human-level performance across professional tasks emphasizes model optimization importance through pruning and sparsity [30]. Leveraging unlabeled data to match supervised approaches highlights sparse and efficient models' potential in advancing self-supervised learning frameworks [31]. Pruning, sparsity, and efficiency are pivotal in developing robust AI models, enabling effective deployment across diverse environments and domains. Pruning selectively removes non-critical components, reducing neural networks' size and memory footprint, making them suitable for mobile and edge devices. Sparse networks often outperform dense counterparts, shortening training times and enhancing generalization. The relationships between sparsity and training, alongside techniques accelerating performance on real hardware, illustrate neural network optimization's evolving landscape. Pruning serves as an architecture search paradigm, where the pruned model's structure, rather than inherited weights, becomes crucial for efficiency, challenging beliefs about large, over-parameterized models' necessity [8,32,4]. Taxonomy of Pruning Techniques The examination of pruning techniques involves categorizing various methodologies based on their principles and applications. This section explores the distinction between structured and unstructured pruning, highlighting their respective strengths and challenges in optimizing neural network architectures. Table presents a detailed classification of pruning techniques, illustrating their methodologies and applications as discussed in the subsequent sections. Additionally, Table offers a comprehensive comparison of different pruning methods, detailing their characteristics and application contexts as discussed in the subsequent sections. illustrates the taxonomy of pruning techniques in neural networks, categorizing methodologies into structured and unstructured, static and dynamic, novel approaches, hybrid and specialized methods, and application-specific strategies. Understanding these foundational approaches, as depicted in the figure, is crucial for analyzing their effects on model performance and deployment strategies, as it highlights the distinct characteristics and applications of each pruning type, demonstrating their impact on model optimization, performance, and deployment across various computational environments. Structured vs. Unstructured Pruning Structured and unstructured pruning are key methodologies for optimizing deep neural networks, each offering unique benefits and limitations. Structured pruning removes entire neurons, filters, or layers, simplifying network architecture and enhancing compatibility with hardware environments, thus reducing computational overhead during inference. Notable methods include Single Shot Structured Pruning (SSSP) and CoFi, which integrate coarse and fine-grained pruning, optimizing model efficiency by balancing feature retention with computational demands [4,33]. LoRA employs low-rank decomposition matrices for structured pruning without compromising model integrity [34]. Unstructured pruning removes individual weights, resulting in heterogeneous sparsity patterns that often achieve higher compression ratios. This granularity allows greater model size reduction and decreased computational demands, although it requires specialized hardware or software optimizations [10]. Magnitude pruning effectively reduces model size in supervised learning environments but is less effective in transfer learning [35]. The irregularity of unstructured pruning complicates deployment across standard hardware setups. Recent advancements, such as the GOHSP approach, integrate graph-based ranking to develop heterogeneous structured sparsity within Vision Transformer models [5]. Combining quantization with pruning, methods like QLoRA utilize 4-bit quantization with Low Rank Adapters, achieving significant memory savings [36]. The PARP method exemplifies structured pruning by pinpointing and tuning subnetworks for computational efficiency [3]. The choice between structured and unstructured pruning depends on deployment objectives, hardware constraints, and the need to balance compression rate with execution efficiency. Structured pruning is preferred for hardware compatibility, while unstructured pruning offers granular network optimization. As methodologies evolve, innovative strategies harnessing both approaches are expected to enhance adaptability and efficiency across applications. ManiDP aligns manifold relationships between instances and pruned sub-networks during training, contrasting with methods treating each instance in isolation [37]. Static vs. Dynamic Pruning Static and dynamic pruning are distinct approaches to neural network optimization, defined by the timing and flexibility of the pruning process. Table presents an overview of the leading pruning methods employed in neural network optimization, providing insights into their adaptability and application contexts within static and dynamic pruning frameworks. Static pruning follows a predetermined schedule, reducing network parameters at specific stages, often before or during initial training phases. It relies on pre-defined criteria to eliminate redundant parameters, simplifying model architectures [38]. Despite achieving high compression ratios, static pruning faces challenges in hardware compatibility and adapting to network conditions [39]. Dynamic pruning introduces adaptability by making pruning decisions iteratively throughout the training process. This approach uses real-time feedback and performance metrics to adjust strategies, enabling responsive and efficient optimization [40]. Dynamic methods like SynFlow do not require training data, challenging assumptions about data-driven pruning [41]. By continuously evaluating performance, dynamic pruning accommodates budget constraints and maintains stability under extreme scenarios [39]. Integrating a gradient flow framework within dynamic pruning enhances the analysis of importance measures, assessing the impact of early pruning on training [40]. This adaptability benefits transfer learning scenarios, where sparse networks can match or exceed dense networks' accuracy [24]. Modeling layer dependencies, as proposed in the DepGraph framework, allows nuanced structural pruning strategies [42]. The decision to use static or dynamic pruning depends on application needs for flexibility, computational efficiency, and performance maintenance across conditions. Static pruning traditionally reduces resource usage but may leave redundancies. Dynamic pruning allows reactivation of prematurely pruned weights, achieving dense model performance without retraining, making it suitable for adaptable, efficient deployment on resource-constrained devices, like edge applications [43,44,4,8]. Hybrid approaches combining static and dynamic pruning strengths are anticipated to enhance optimization strategies. Novel Pruning Techniques Recent developments introduce innovative pruning methodologies enhancing efficiency, adaptability, and performance. Table provides a comprehensive overview of various novel pruning techniques, illustrating their contributions to efficiency, adaptability, and robustness in modern computational models. ParameterNet incorporates dynamic convolutions for parameter addition without elevating computational costs, maintaining performance gains [45]. CoFi integrates joint pruning of coarse and fine-grained modules, distinguishing itself from traditional techniques [33]. GraNet enhances flexibility through zero-cost neuroregeneration, preserving performance while supporting efficient training [46]. This integration demonstrates potential improvements in adaptability and robustness. In Vision Transformers, CP-ViT introduces layer-aware dynamic pruning ratio adjustment, optimizing efficiency through adaptive strategies [47]. SCFP performs filter pruning based on linear relationships within feature map subspaces, distinguishing itself from heuristic methods [2]. FBS adds auxiliary connections to convolutional layers, allowing dynamic skipping of less significant channels, enhancing flexibility and responsiveness [48]. Functional approximation models elucidate pruning dynamics in relation to network width, depth, and pruning level, offering predictive scaling laws [49]. These insights deepen understanding across architectural dimensions. Fast post-training techniques like mask search, rearrangement, and tuning improve pruning with reduced computational overhead [50]. The Robust Pruning Method balances benign and robust accuracy, ensuring model resilience throughout pruning [6]. This maintains performance and robustness, critical in modern applications. Collectively, these novel methodologies signify a shift towards sophisticated, adaptable, resource-efficient pruning strategies. Deployment capabilities are enhanced through advanced techniques like knowledge distillation, network pruning, and sparse connectivity, optimizing architectures for diverse environments. These methods compress models to reduce computational and storage demands, suitable for resource-constrained devices while preserving performance metrics [51,6,52,19,1]. Hybrid and Specialized Pruning Methods Hybrid and specialized pruning methods converge traditional and innovative approaches to optimize neural network architectures, enhancing efficiency and performance across applications. These techniques combine structured and unstructured pruning, leveraging strengths for superior compression and execution efficiency. Hybrid strategies integrate coarse-grained and fine-grained pruning, balancing computational savings with accuracy retention [33]. Specialized methods address unique challenges in complex environments. SCFP employs filter pruning based on linear relationships within feature map subspaces, optimizing convolutional layers [2]. CP-ViT introduces layer-aware dynamic pruning ratios, adapting strategies for Vision Transformers [47]. GraNet incorporates neuroregeneration processes, maintaining robustness and adaptability during training [46]. This supports dynamic adjustment of structures, ensuring pruned models respond effectively to varying conditions and constraints. Hybrid methods like CoFi integrate pruning with distillation techniques, optimizing size reduction and performance enhancement [33]. These strategies are beneficial where maintaining high accuracy is crucial despite compression. Frameworks facilitating real-time pruning adjustments, using feedback signals or dynamic channel adjustments, highlight hybrid and specialized methods' potential to enhance optimization [48]. These approaches enable models to adapt to changing environments, ensuring efficient resource utilization and sustained performance across applications. Hybrid and specialized pruning methods represent a critical evolution in optimization, providing tailored solutions for modern AI systems. By employing a range of strategies, these methods enhance efficiency in resource-limited environments while preserving accuracy and robustness. Removing non-critical neurons and exploiting sparsity reduces overheads, shortens training times, and maintains resilience against adversarial examples. Techniques like pruning at initialization and fine-tuning create compact models that perform reliably on edge devices, balancing energy efficiency with robust performance [8,6,4,53]. Table provides a detailed overview of various hybrid and specialized pruning methods, illustrating their optimization strategies and adaptability features crucial for enhancing neural network efficiency and performance. Pruning in Specific Applications Pruning strategies vary in effectiveness and challenges across domains, requiring tailored approaches for specific tasks. In Transformer architectures, pruning must consider sensitivity to feature removal, as shown in studies on BERT and related models [54]. Attention mechanisms require careful pruning to maintain integrity and performance, emphasizing domain-specific strategies. For NLP tasks, pruning focuses on redundancy within attention heads and feed-forward layers, streamlining complexity without sacrificing linguistic comprehension. Techniques like head pruning leverage modular attention mechanisms to optimize efficiency [54]. These strategies are crucial for deploying NLP models on resource-constrained devices. In computer vision, pruning addresses challenges in convolutional layers and image processing. Filter and channel pruning reduce computational load associated with high-dimensional data, enhancing applicability in real-time tasks [2]. For audio processing, pruning simplifies recurrent and convolutional structures for efficient handling of temporal data. Structured pruning optimizes performance for tasks like ASR, where real-time processing and efficiency are critical [3]. In federated learning, pruning accommodates decentralized training, ensuring models operate effectively across environments. Approaches reducing communication overhead and enhancing local efficiency are vital for performance in federated scenarios [23]. Pruning in specific applications requires understanding domain-specific requirements and challenges, developing tailored strategies to optimize performance while maintaining accuracy and efficiency. Removing non-critical neurons reduces model size, crucial for deployment on edge devices. Sparse networks generalize as well as or better than dense networks, reducing memory footprint and training time. Leveraging sparsity accelerates inference and training on hardware, guided by comprehensive understanding of techniques, criteria, and metrics for pruned parameter efficiency. This approach provides insights into early structure adaptation and the relationship between sparsity and training, contributing to improved workloads and addressing open challenges [8,4]. Integrating specialized approaches enhances AI models' deployment capabilities across computational environments. Comparison of Pruning Methods Effectiveness Metrics Pruning methods in deep neural networks are evaluated using metrics such as accuracy, computational efficiency, and operational performance across hardware platforms. Table provides a detailed overview of the benchmarks employed to evaluate the effectiveness of pruning methods in deep neural networks, highlighting their relevance in measuring accuracy, computational efficiency, and other performance metrics. Accuracy is a key indicator, often assessed against baseline models to gauge improvements in speed and resource usage [1,4]. SCFP has demonstrated superior accuracy and efficiency in optimizing convolutional layers [2], while ManiDP has achieved significant computational cost reductions on benchmarks like ImageNet, emphasizing the importance of evaluating pruning methods beyond model size reduction to include performance preservation [37]. The Robust Pruning Method underscores the necessity of evaluating various accuracy facets—benign, empirical robust, and verifiable robust accuracy—to understand pruning's impact on model robustness [6]. In resource-constrained environments, standardized testing on benchmark datasets assesses model performance through metrics like accuracy and F1-score, which are crucial for comparison against established baselines [17]. Inference time and memory costs are vital for evaluating pruning configurations, with methods like Single Shot Structured Pruning (SSSP) illustrating effectiveness through training time reduction and improved inference speed [4]. A comprehensive evaluation of effectiveness metrics—including accuracy, computational efficiency, latency, and transferability—provides insights into the practical impact of pruning strategies, especially for optimizing model performance on edge devices and addressing redundancy in generative language models. The lack of standardized benchmarks necessitates innovative frameworks like ShrinkBench for consistent assessments [14,55,44,4,32]. These metrics enable researchers to enhance neural network architectures for improved applicability and sustained high performance across diverse domains. Efficiency Metrics Efficiency metrics are essential for evaluating computational cost savings and resource optimization achieved through various pruning methods. The reduction in floating-point operations (FLOPs) serves as a primary indicator of computational efficiency. Channel gating methods significantly reduce FLOPs and off-chip memory access, demonstrating their effectiveness in minimizing computational costs [7]. Techniques like CP-ViT achieve over 40\\ Trade-offs in Pruning Methods Evaluating trade-offs in pruning methods is crucial for understanding the balance between computational efficiency and model performance. Techniques explored in this survey emphasize the trade-offs between compression levels and model accuracy, essential for assessing various pruning strategies [14]. Comparative analyses of sparsification methods reveal their effectiveness and inherent trade-offs; substantial model size reductions can degrade accuracy, necessitating a careful balance between resource savings and performance retention [8]. Training large models for fewer iterations can enhance performance and efficiency, addressing trade-offs in pruning methods [15]. The simplicity and effectiveness of ShortGPT in pruning redundant layers compared to more complex methods highlight the importance of evaluating trade-offs in terms of model simplicity and execution efficiency [56]. Additionally, the predictability of error in pruned networks varies across architectures and tasks, necessitating a nuanced understanding of trade-offs involved in selecting specific pruning techniques [49]. E-LTH's innovation lies in adapting winning tickets across architectures without exhaustive pruning, significantly reducing computational costs [21]. This adaptability minimizes trade-offs in efficiency and scalability, enhancing pruned models' deployment capabilities. Ultimately, choosing between pruning techniques requires a nuanced evaluation of trade-offs encompassing computational costs, training requirements, and model performance. Pruning is a critical strategy for reducing inference costs in low-resource settings by selectively removing redundant neurons while preserving accuracy. Recent studies suggest that training a large, over-parameterized model may not be necessary for achieving efficiency, positioning pruning as a potential architecture search paradigm. The lack of standardized benchmarks complicates the comparison of techniques, but pruning can enhance model deployment on edge devices by reducing memory footprint and training time. As the field evolves, establishing consistent evaluation frameworks and metrics will be essential for advancing pruning methods and their practical applications [8,32,4,55]. Understanding these trade-offs allows researchers to tailor pruning strategies effectively for optimizing neural network architectures across diverse application domains. Pruning Techniques in Different Model Architectures The choice and performance of pruning techniques are significantly influenced by neural network models' underlying architecture. Convolutional neural networks (CNNs) benefit from pruning techniques targeting redundancy in convolutional filters and channels, alleviating computational and memory burdens associated with larger models. By strategically removing filters with minimal impact on output accuracy, computational costs can be lowered without requiring sparse convolution libraries, reducing inference costs by up to 34\\ Analysis of Impact on Model Performance Analyzing model performance involves examining factors that influence the efficiency and effectiveness of deep neural networks. This section investigates the relationship between pruning techniques and their impact on performance metrics like accuracy and inference time, which are vital for optimizing models amid computational efficiency demands in AI applications. The following subsections delve into these techniques' implications for accuracy and inference time, offering insights into their operational effectiveness in real-world scenarios. Accuracy and Inference Time Pruning techniques significantly influence model accuracy and inference time, which are crucial for optimizing deep neural networks in various computational settings. Structured pruning methods, such as Single Shot Structured Pruning (SSSP), achieve substantial speedups in training and inference—demonstrating 2x and 3x increases respectively—while maintaining high performance [4]. This effectiveness is due to structured pruning's ability to identify non-critical neurons for removal, thereby reducing computational complexity without negatively impacting accuracy. In Vision Transformers (ViTs), techniques like GOHSP effectively reduce parameter size while maintaining performance, illustrating their deployment potential in resource-constrained environments [4]. Channel gating methods achieve a 2.7-8.0× reduction in FLOPs with minimal accuracy loss, reinforcing their effectiveness in optimizing computational efficiency [4]. Additionally, QLoRA allows for fine-tuning large models on a single 48GB GPU, preserving performance akin to traditional 16-bit fine-tuning, highlighting significant improvements in computational efficiency [4]. Dynamic pruning methods, such as PARP, show notable improvements in accuracy and inference time, achieving a significant decrease in Word Error Rate (WER) compared to full models, demonstrating their effectiveness in enhancing model performance [4]. The ManiDP approach leverages relationships between input instances to accurately identify redundant filters, leading to improved performance metrics [4]. Furthermore, methods achieving state-of-the-art results post fine-tuning effectively impact both accuracy and inference time [4]. Experiments with pruning methods show preservation of an average of 93 MobileVLM V2 models outperform larger models on standard benchmarks, emphasizing efficient pruning strategies' importance in enhancing performance [17]. Advanced pruning strategies, including structured and unstructured pruning, enhance computational efficiency by removing non-critical components from neural networks. These strategies maintain or improve model accuracy by preserving essential knowledge, even under high sparsity. The architecture itself often proves more crucial than inherited weights for efficiency, suggesting pruning's potential as an architecture search paradigm. Innovative approaches like Contrastive Pruning and Globally Unique Movement optimize neuron generalization and uniqueness, improving performance across diverse tasks and models [8,57,44,4,32]. These advancements ensure pruned models are well-suited for large-scale AI systems and real-world applications, presenting tangible benefits amid operational resource constraints. Resource Utilization Pruning techniques enhance resource utilization by substantially reducing computational demands while maintaining or improving model performance. Global Lottery Tickets (GLTs) achieve comparable accuracy to full models with substantial savings in multiply-accumulate operations (MACs), ranging from 20\\ LoRA exemplifies reductions in memory usage and training parameters, positively influencing resource utilization post-pruning [34]. By incorporating low-rank adaptation, LoRA enables efficient model compression, reducing the computational footprint required for training and inference, crucial for deployment in resource-constrained environments. Pruning an 8B LLaMA-3 model to 50\\ Deep Compression demonstrates storage requirement reduction of 35x to 49x without accuracy loss [11]. This emphasizes pruning's role in minimizing storage overheads while preserving performance, enabling neural network deployment in environments with limited computational resources. These advancements in pruning techniques optimize resource utilization, enabling high-performance neural networks across diverse applications while addressing environmental and computational constraints. Efficient pruning strategies selectively remove non-essential components, allowing models to operate effectively in resource-constrained environments like mobile and edge devices. This approach reduces memory footprint and accelerates training and inference times, supporting sustainable AI development by minimizing energy consumption and enhancing model generalization [8,4]. Generalization and Robustness Pruning techniques influence neural networks' generalization and robustness, affecting their ability to maintain consistent performance across diverse tasks and conditions. The method proposed in [58] enhances generalization and training efficiency in sparse networks by optimizing neuron activations and convergence dynamics, crucial for deploying models in varied environments with limited computational resources. Exploration of scaling laws in large-scale networks highlights the need to understand trade-offs between compression rates and performance, essential for developing robust pruning methods that maintain efficacy while significantly reducing computational demands [49]. Furthermore, experiments concluded that winning ticket initializations can generalize across datasets and optimizers, suggesting larger datasets provide better initialization methods applicable in diverse settings, enhancing model robustness [26]. The Elastic Lottery Ticket Hypothesis (E-LTH) facilitates winning ticket transfer across architectures, achieving performance nearly equivalent to traditionally identified tickets [21]. This underscores adaptable pruning strategies' importance in preserving performance across varying architectures and training conditions. Current studies often overlook compression rates versus performance trade-offs, indicating a need for more robust methods [1]. By optimizing layer characteristics and neuron activations, pruned models achieve sustained high performance and adaptability in AI applications, ensuring effective operation across varied domains and computational environments. Pruning techniques enhance neural networks' generalization and robustness, enabling adaptation to evolving input conditions without compromising performance. These advancements ensure pruned models effectively operate in resource-constrained settings like edge devices. By reducing model size and computational demands, these techniques facilitate efficient federated learning, shorten training times, and decrease memory footprints, promoting sustainable AI development and deployment. This approach maintains and often enhances model accuracy, making it pivotal for sustainable AI practices [8,23,4,59]. Impact on Specific Applications Pruning techniques influence neural networks' performance across application-specific scenarios, optimizing computational efficiency and resource utilization. In large language models like GPT-4, pruning enhances performance while reducing resource consumption, enabling human-level performance in specific tasks [30]. This capability demonstrates pruned models' efficiency in professional and academic settings, where high performance is essential. Sparse models in transfer learning scenarios reveal potential to match or exceed dense models' performance, underscoring efficiency gains through pruning [60]. This is crucial for deploying models in environments with limited computational resources, ensuring pruned networks maintain high accuracy and generalization across diverse tasks. The ShrinkBench framework provides a standardized approach for evaluating pruning techniques, emphasizing future research's need to focus on developing and adopting standardized benchmarks and metrics [55]. This framework facilitates different pruning methods' comparison, ensuring models are optimized for specific applications while maintaining robust performance. Identifying winning tickets within neural networks profoundly impacts optimizing model architectures, providing guidelines for future deep learning research [27]. Leveraging these insights allows researchers to develop more efficient pruning strategies that enhance model performance across various domains. Comprehensive analysis of pruning's impact on model performance highlights the importance of maintaining accuracy while reducing resource consumption [14]. This balance is critical for deploying AI models in application-specific scenarios, where efficient resource utilization and high performance are paramount. Pruning techniques offer advantages for optimizing neural networks in application-specific contexts, ensuring models operate effectively across diverse computational environments. By addressing identified gaps and exploring new methodologies related to the Lottery Ticket Hypothesis, future research can enhance pruning strategies' applicability and efficiency in modern AI systems [61]. Recommendations for Optimizing Network Structures Strategies for Selecting Pruning Methods Choosing suitable pruning methods is essential for tailoring neural networks to specific application needs. The OWL approach customizes layerwise pruning for large language models, boosting efficiency and performance [62]. Bonsai offers efficient pruning with reduced resource demands, beneficial in constrained environments [63]. Techniques in [11] focus on size reduction and energy efficiency, ideal for mobile applications. QLoRA efficiently fine-tunes large models on standard hardware, minimizing memory use [36]. PARP simplifies pruning selection, requiring a single ASR finetuning run, streamlining the process [3]. Channel gating optimizes resource use during inference by focusing on critical input features [7]. Collectively, these strategies form a robust framework for selecting pruning methods that optimize performance and resource use across applications. Insights from convolutional network evaluation, knowledge distillation, and structured pruning further enhance network performance by reducing computational demands and maintaining accuracy [64,32,19]. Future Research Directions Neural network pruning offers promising research avenues to enhance efficiency and adaptability across architectures and tasks. Developing hybrid pruning approaches tailored for IoT applications can refine compression strategies for resource-constrained environments [1]. Dynamic sparsity methods and movement pruning improvements present valuable research opportunities [10,48]. Exploring the Elastic Lottery Ticket Hypothesis across model families and datasets provides insights into pruning versatility [21]. Refining clustering algorithms for diverse architectures and optimizing channel gating across tasks offer further exploration avenues [2,7]. Future research should focus on adaptive pruning techniques and alternatives to traditional methods to complement model compression [4]. Enhancements in alignment and performance for large models across broader tasks are crucial for optimizing AI technologies [30]. Investigating optimization strategies for architecture and training methods, along with expanding benchmarks, could significantly boost performance [17]. Research into pruning at initialization, standardizing benchmarks, and leveraging sparsity is poised to enhance AI technologies' efficiency and robustness across applications [8,4,55,59]. Optimizing Network Structures in Specific Applications Optimizing network structures for specific applications requires aligning pruning methods with domain-specific demands. In computer vision, filter and channel pruning reduce computational overhead in convolutional layers, enhancing real-time image recognition efficiency [2]. These methods decrease inference time and memory usage while preserving accuracy. For NLP, pruning strategies target redundancy in attention heads and feed-forward layers, optimizing complexity without losing linguistic comprehension [54]. In audio processing, simplifying recurrent and convolutional architectures enhances performance for tasks like ASR, where real-time processing is crucial [3]. Federated learning requires pruning strategies that improve local model efficiency and minimize communication overhead [23]. Dynamic channel adjustment methods, as seen in VGG-16 and ResNet-18, offer computational savings and sustainable resource use [48]. Optimizing network structures necessitates understanding domain-specific requirements to maximize performance while maintaining high accuracy and efficiency, enabling efficient deployment on edge devices and mobile platforms [8,4]. Integrating Advanced Optimization Techniques Integrating advanced optimization techniques with pruning is crucial for enhancing neural network performance and efficiency. Differentiable Architecture Search (DARTS) aids in discovering architectures aligned with application needs [15]. Neuroregeneration processes, like GraNet, enhance pruning flexibility and robustness [46]. Dynamic channel adjustment methods offer computational savings and sustainable resource use [48]. Exploring scaling laws in neural networks provides insights into predictive pruning dynamics, facilitating optimization strategies that maintain accuracy and generalization [49]. Combining optimization strategies, such as knowledge distillation and sparse representation, enhances neural network efficiency in resource-constrained environments, compressing large models without compromising performance [6,19,1,65]. Conclusion The examination of deep neural network pruning methodologies underscores its pivotal role in refining model architectures to enhance efficiency and performance across diverse applications. The analysis suggests that conventional pruning strategies, especially structured methods, require a critical reassessment to address foundational assumptions that may impede their efficacy. This reevaluation is crucial for the evolution of pruning techniques to meet the growing demands of AI systems. Moreover, the survey highlights the importance of integrating sustainability into AI development, as demonstrated by models like OPT, which deliver competitive performance with reduced environmental impact. This emphasizes the need for pruning strategies that not only improve computational efficiency but also contribute to environmental sustainability. The future trajectory of pruning is presented as an essential component in the progression of deep neural networks, advocating for the development of innovative approaches that overcome current limitations and foster the creation of sustainable, high-performance AI systems.",
  "reference": {
    "1": "2010.03954v1",
    "2": "1803.05729v1",
    "3": "2106.05933v2",
    "4": "2005.04275v1",
    "5": "2301.05345v2",
    "6": "1906.06110v1",
    "7": "1805.12549v2",
    "8": "2102.00554v1",
    "9": "2303.00566v2",
    "10": "2010.07611v2",
    "11": "1510.00149v5",
    "12": "1909.08174v1",
    "13": "1909.12579v1",
    "14": "2401.15347v1",
    "15": "2002.11794v2",
    "16": "2110.02743v2",
    "17": "2402.03766v1",
    "18": "1902.01996v4",
    "19": "1503.02531v1",
    "20": "1905.09717v5",
    "21": "2103.16547v3",
    "22": "2205.01068v4",
    "23": "1909.12326v5",
    "24": "1905.07785v2",
    "25": "2010.11929v2",
    "26": "1906.02773v2",
    "27": "2107.00166v4",
    "28": "2308.12792v3",
    "29": "2003.04297v1",
    "30": "2303.08774v6",
    "31": "2006.08218v5",
    "32": "1810.05270v2",
    "33": "2204.00408v3",
    "34": "2106.09685v2",
    "35": "2005.07683v2",
    "36": "2305.14314v1",
    "37": "2103.05861v1",
    "38": "2009.14410v3",
    "39": "2102.07156v1",
    "40": "2009.11839v4",
    "41": "2006.05467v3",
    "42": "2301.12900v2",
    "43": "2006.07253v1",
    "44": "2302.03773v1",
    "45": "2306.14525v2",
    "46": "2106.10404v4",
    "47": "2203.04570v1",
    "48": "1810.05331v2",
    "49": "2006.10621v3",
    "50": "2204.09656v2",
    "51": "1707.01213v3",
    "52": "1707.04780v2",
    "53": "2002.08797v5",
    "54": "2105.06990v2",
    "55": "2003.03033v1",
    "56": "2201.00043v1",
    "57": "2112.07198v1",
    "58": "2403.03853v3",
    "59": "1608.08710v3",
    "60": "2102.06790v2",
    "61": "2402.05406v3",
    "62": "2010.03533v2",
    "63": "2103.06460v3",
    "64": "2111.13445v5",
    "65": "2403.04861v2",
    "66": "2310.05175v4",
    "67": "1404.0736v2",
    "68": "1506.02626v3"
  },
  "chooseref": {
    "1": "2401.15347v1",
    "2": "2204.09656v2",
    "3": "2009.11839v4",
    "4": "2105.10065v1",
    "5": "1906.06307v2",
    "6": "2306.11695v3",
    "7": "2002.05709v3",
    "8": "2403.04861v2",
    "9": "2010.03954v1",
    "10": "2402.13116v4",
    "11": "2308.07633v4",
    "12": "1804.03294v3",
    "13": "2102.06790v2",
    "14": "2101.10552v1",
    "15": "2304.06632v1",
    "16": "1802.03494v4",
    "17": "2210.04092v4",
    "18": "2010.11929v2",
    "19": "1905.09418v2",
    "20": "1902.01996v4",
    "21": "1905.10650v3",
    "22": "1706.03762v7",
    "23": "2403.14729v1",
    "24": "2105.06990v2",
    "25": "1905.10044v1",
    "26": "2203.04570v1",
    "27": "1805.12549v2",
    "28": "1707.06168v2",
    "29": "2106.04533v3",
    "30": "2102.07156v1",
    "31": "2003.02389v1",
    "32": "1606.09274v1",
    "33": "1806.09055v2",
    "34": "2003.13683v3",
    "35": "2005.03354v2",
    "36": "2004.02164v5",
    "37": "1707.01213v3",
    "38": "2404.13648v1",
    "39": "1510.00149v5",
    "40": "2106.14568v4",
    "41": "1512.03385v1",
    "42": "2301.12900v2",
    "43": "1503.02531v1",
    "44": "1909.11957v6",
    "45": "1909.11957v6",
    "46": "2203.04248v1",
    "47": "1810.05331v2",
    "48": "1912.03458v2",
    "49": "2006.07253v1",
    "50": "2005.06870v1",
    "51": "1905.05934v1",
    "52": "2402.05406v3",
    "53": "1312.6120v3",
    "54": "1812.04368v2",
    "55": "1404.0736v2",
    "56": "1803.05729v1",
    "57": "2110.08232v4",
    "58": "2312.11983v1",
    "59": "2112.07198v1",
    "60": "2008.11062v1",
    "61": "2106.00134v1",
    "62": "2301.05345v2",
    "63": "1909.08174v1",
    "64": "1909.12778v3",
    "65": "2110.04869v2",
    "66": "2003.01794v3",
    "67": "2303.08774v6",
    "68": "2010.03533v2",
    "69": "2303.04185v2",
    "70": "1911.09817v2",
    "71": "2108.00708v1",
    "72": "2003.08935v1",
    "73": "2002.10179v2",
    "74": "1905.07830v1",
    "75": "2108.00259v3",
    "76": "2111.13445v5",
    "77": "1409.0575v3",
    "78": "1906.10771v1",
    "79": "2003.04297v1",
    "80": "2105.12971v1",
    "81": "2305.11627v3",
    "82": "2302.13971v1",
    "83": "2402.11187v2",
    "84": "2010.07611v2",
    "85": "2202.10203v1",
    "86": "1506.02626v3",
    "87": "1708.06519v1",
    "88": "1608.03665v4",
    "89": "1912.05671v4",
    "90": "2106.09685v2",
    "91": "2306.11222v2",
    "92": "2006.12156v2",
    "93": "2409.05211v1",
    "94": "2103.05861v1",
    "95": "1903.10258v3",
    "96": "2306.08543v5",
    "97": "2402.03766v1",
    "98": "2402.09748v1",
    "99": "1909.12326v5",
    "100": "1703.08651v2",
    "101": "2005.07683v2",
    "102": "2201.00043v1",
    "103": "2012.00596v3",
    "104": "2304.02840v1",
    "105": "1711.02017v3",
    "106": "1905.09717v5",
    "107": "2012.09243v2",
    "108": "1806.07572v4",
    "109": "2310.02980v4",
    "110": "2205.01068v4",
    "111": "2308.13137v3",
    "112": "2006.10621v3",
    "113": "1906.02773v2",
    "114": "2310.05175v4",
    "115": "2106.05933v2",
    "116": "2305.11203v3",
    "117": "1911.11641v1",
    "118": "2206.12562v1",
    "119": "2204.02311v5",
    "120": "1902.05967v3",
    "121": "2306.14525v2",
    "122": "2002.07376v2",
    "123": "2104.11832v2",
    "124": "1609.07843v1",
    "125": "2006.09081v5",
    "126": "2002.00585v1",
    "127": "2111.05754v1",
    "128": "2109.14960v3",
    "129": "2005.04275v1",
    "130": "2001.03554v1",
    "131": "2009.14410v3",
    "132": "1608.08710v3",
    "133": "1909.12579v1",
    "134": "2009.08576v2",
    "135": "2006.05467v3",
    "136": "2111.11802v4",
    "137": "2305.14314v1",
    "138": "2103.06460v3",
    "139": "2101.03697v3",
    "140": "2305.02190v1",
    "141": "1810.05270v2",
    "142": "1911.11134v3",
    "143": "2002.08797v5",
    "144": "2010.10732v2",
    "145": "1810.02340v2",
    "146": "2110.11395v2",
    "147": "2107.00166v4",
    "148": "2009.11094v2",
    "149": "1707.04780v2",
    "150": "2006.08218v5",
    "151": "2310.06694v2",
    "152": "2403.03853v3",
    "153": "2007.00389v1",
    "154": "2401.15024v2",
    "155": "1808.06866v1",
    "156": "2402.17177v3",
    "157": "2308.12792v3",
    "158": "1907.04840v2",
    "159": "2106.10404v4",
    "160": "1905.07785v2",
    "161": "2301.00774v3",
    "162": "2402.17946v4",
    "163": "2102.00554v1",
    "164": "2305.10924v3",
    "165": "2303.00566v2",
    "166": "2204.00408v3",
    "167": "2312.17244v2",
    "168": "2103.16547v3",
    "169": "2007.12223v2",
    "170": "1803.03635v5",
    "171": "2012.06908v2",
    "172": "2203.07259v3",
    "173": "2202.02643v1",
    "174": "1707.06342v1",
    "175": "2105.11228v1",
    "176": "1901.07827v2",
    "177": "1906.06110v1",
    "178": "2110.02743v2",
    "179": "1904.12368v2",
    "180": "1903.09291v1",
    "181": "2002.11794v2",
    "182": "2012.12877v2",
    "183": "2301.13741v3",
    "184": "2203.08243v1",
    "185": "2210.03044v1",
    "186": "1409.1556v6",
    "187": "2312.07533v4",
    "188": "2104.08500v4",
    "189": "1906.04358v2",
    "190": "2003.03033v1",
    "191": "2302.03773v1",
    "192": "1911.13299v2",
    "193": "2005.00561v2",
    "194": "2107.01808v1",
    "195": "2301.05219v2",
    "196": "2402.11700v2",
    "197": "2110.05667v1",
    "198": "2010.02350v2",
    "199": "2303.04935v2",
    "200": "2106.10784v1"
  }
}