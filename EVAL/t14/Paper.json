{
  "authors": [
    "Hongrong Cheng",
    "Miao Zhang",
    "Javen Qinfeng Shi"
  ],
  "literature_review_title": "A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations",
  "year": "2023",
  "date": "2023-08-13",
  "category": "cs.LG",
  "abstract": "Modern deep neural networks, particularly recent large language models, come\nwith massive model sizes that require significant computational and storage\nresources. To enable the deployment of modern models on resource-constrained\nenvironments and accelerate inference time, researchers have increasingly\nexplored pruning techniques as a popular research direction in neural network\ncompression. However, there is a dearth of up-to-date comprehensive review\npapers on pruning. To address this issue, in this survey, we provide a\ncomprehensive review of existing research works on deep neural network pruning\nin a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to\nprune, and 4) fusion of pruning and other compression techniques. We then\nprovide a thorough comparative analysis of eight pairs of contrast settings for\npruning and explore emerging topics, including pruning for large language\nmodels, large multimodal models, post-training pruning, and different\nsupervision levels for pruning to shed light on the commonalities and\ndifferences of existing methods and lay the foundation for further method\ndevelopment. To facilitate future research, we build a curated collection of\ndatasets, networks, and evaluations on different applications. Finally, we\nprovide valuable recommendations on selecting pruning methods and prospect\nseveral promising research directions. We build a repository at\nhttps://github.com/hrcheng1066/awesome-pruning.",
  "structure": [
    {
      "section_title": "Introduction/Pre-section",
      "level": "1",
      "content": "\\documentclass[10pt,journal,compsoc]{IEEEtran} ifpdf \\ifCLASSOPTIONcompsoc \\usepackage[nocompress]{cite} \\else cite \\fi \\ifCLASSINFOpdf \\else \\fi amsmath algorithmic array stfloats url bbding algorithm \\makeatletter \\let\\NAT@parse\\undefined \\makeatother overpic amsfonts,amssymb subfig booktabs makecell graphicx multirow xurl wrapfig,lipsum float \\usepackage[numbers]{natbib} xcolor pifont verbatim lemma{Lemma} prop{Proposition} theorem{Theorem} corollary{Corollary} assumption{Assumption} definition{Definition} hyperref document A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations Hongrong~Cheng*,~Miao Zhang*$\\dagger$,~\\IEEEmembership{Member,~IEEE, ~Javen~Qinfeng~Shi,~Member,~IEEE% <-this % stops a space \\IEEEcompsocthanksitem * Equal contribution. $\\dagger$ Corresponding author.\\\\ H. Cheng and J. Q. Shi are with the University of Adelaide. M. Zhang is with Harbin Institute of Technology (Shenzhen). \\\\ E-mail:\\{hongrong.cheng, javen.shi\\@adelaide.edu.au,\\\\ \\{zhangmiao\\}@hit.edu.cn.} \\\\ Manuscript received May 01, 2023; revised June 11, 2024.% <-this % stops an unwanted space } Journal of \\LaTeX\\ Class Files,~Vol.~14, No.~8, August~2024 {Shell \\textit{et al.}: Bare Demo of IEEEtran.cls for Computer Society Journals} % \\begin{IEEEkeywords deep neural network pruning, model compression, model acceleration, large language models, vision transformers, large multimodal models, diffusion models, edge devices. IEEEkeywords} \\maketitle \\IEEEdisplaynontitleabstractindextext \\IEEEpeerreviewmaketitle \\IEEEraisesectionheading{",
      "origin_cites_number": 0
    },
    {
      "section_title": "Introduction",
      "level": "1",
      "content": "} Over the past several years, Deep Neural Networks (DNNs) have achieved conspicuous progress in various domains and applications, such as Computer Vision (CV) simonyan2015very,dosovitskiy2021image, Natural Language Processing (NLP) devlin2019bert,chowdhery2022palm, Audio Signal Processing (ASP) bohnstingl2021towards,latif2023sparks, and cross-modal applications lin2024vila,liu2024sora. Although DNNs achieve remarkable success in various areas, their performance relies heavily on model parameters and computational cost. For example, the widely used ResNet-50 he2016deep takes over 95 MB of memory, containing over 23 million parameters you2019drawing. $BERT_{BASE}$ devlin2019bert is around 440 MB with 110 million parameters, GPT-3 includes up to 175 billion parameters wu2023ai, and GPT-4 has even more. The trend of enlarging neural network size is anticipated to persist. However, the more parameters of DNNs, the more time and memory space they typically require for processing the inputs han2015learning. The high training and inference costs associated with these models present a significant challenge to their deployment on devices constrained by limited computational resources (such as CPU, GPU, and memory), energy, and bandwidth han2015deep, dong2017more, you2019gate. For example, real-life applications such as autonomous driving, field rescue, and bushfire prevention necessitate high accuracy and efficient resource usage, including fast real-time response and compact memory footprint. Deep neural networks' computational complexity and memory footprint can make them impractical for deployment on edge devices luo2017thinet. With the popularity of Large Language Models (LLMs) in recent years, there is growing interest in compressing neural networks for computers with flexible hardware requirements frantar2023sparsegpt. In addition, deep neural networks that contain redundant features can undermine their robustness, elevating the risk of adversarial attacks sehwag2020hydra. For instance, high-dimensional feature spaces created by these networks can provide more entry points for adversarial attacks, undermining the network's ability to generalize beyond its original training data. To relieve this issue, researchers have proposed various neural network compression techniques to design lightweight models, including neural network pruning (ashkboos2024slicegpt, ma2023llmpruner), low-rank factorizations of the weight matrices (denton2014exploiting,lin2018holistic), quantization (dettmers2023qlora,shao2024omniquant), knowledge distillation (gu2024minillm,xu2024survey), neural architecture search (liu2019darts,zhang2021idarts), and other techniques (ding2021repvgg,chu2024mobilevlm). Among them, there is continuing interest in neural network pruning, which has been proven as a desirable and effective way to save memory space and computation time at inference while maintaining a comparable or even better performance compared to the original DNNs. As shown in Fig.~Fig:pruning-papers-statistics, the number of papers on pruning has been markedly increasing since 2015. It presents more than half of the papers on neural network compression. Research on pruning can be traced back to literature as early as 1988 hanson1988comparing. However, it was only until the emergence of han2015deep that the research community realized the potential of pruning in removing significant redundancy in deep neural networks, and pruning began to gain widespread attention. Several pieces of literature review prior work on deep neural network pruning, as shown in Table~Table-survey-works-this-paper. Although these works overview several aspects of pruning and provide helpful guidance for researchers, many of them (e.g., mishra2020survey,ghimire2022survey,park2024comprehensive,xu2024a) focus on multiple compression techniques, such as pruning, quantization, and knowledge distillation, with only brief examination of each technique. For example, mishra2020survey summarize compression techniques, including pruning, quantization, low-rank factorization, and knowledge distillation, where pruning is primarily introduced from channel/filter pruning, and many essential pruning techniques (such as lottery ticket hypothesis) are not included. Some reviews concentrate on one specific aspect. For example, wang2022recent provide only an overview of pruning at initialization and do not include studies on pruning during training, pruning after training, etc. he2023structured focuses only on structured pruning and does not discuss the other two common types of pruning: unstructured and semi-structured pruning. ghimire2022survey) focus on reviewing Convolutional Neural Network (CNN) pruning and lack descriptions of pruning for other deep neural networks, such as Recurrent Neural Networks (RNNs) fang2023depgraph, Transformer based models frantar2023sparsegpt, and diffusion models fang2023structural. Some recent review works (such as zhu2023survey,xu2024a) survey compression techniques for large models, encompassing a broad array of topics from pruning, quantization, knowledge distillation, etc. Among these, pruning is discussed concisely. The data is from https://www.webofknowledge.com. This survey aims to provide a comprehensive overview of deep neural network pruning for diverse readers. We review representative pruning methods, propose a new taxonomy, conduct a comprehensive analysis of how different pruning techniques perform in practice, and give practitioners who wish to utilize pruning recommendations on choosing a suitable pruning method for various requirements. Our contributions are as follows: (1) Comprehensive review. To our best knowledge, this survey is the most comprehensive overview of modern deep neural network pruning techniques. It distills ideas from over 300 related academic papers, covering pruning methods for small to medium models to cutting-edge large models. In addition, we establish a new taxonomy, as shown in Fig.~Fig:pruning-taxonomy, and provide detailed descriptions of the representative methods for each class of pruning methods. (2) Comparative experiments and analysis. We conduct a comparative analysis of eight pairs of contrast settings for pruning and emerging advances, such as pruning for LLMs and different supervision levels for pruning. Unlike existing surveys, we conduct experiments and provide discussions. (3) Collection of abundant resources. We summarize miscellaneous pruning applications and provide benchmark datasets, networks, and evaluations for different applications. Our collected resources in Appendix D can guide researchers and practitioners in understanding, utilizing, and developing different network pruning methods for various requirements. Ongoing updates of representative pruning efforts are available at https://github.com/hrcheng1066/awesome-pruning. (4) Recommendations and future directions. This survey provides valuable recommendations for choosing appropriate pruning methods for different application requirements and highlights promising future research directions. The remainder of this survey is organized as follows. In Section taxonomy, we establish a clear taxonomy of pruning. Section granularity - learntoprune offer an overview of speedup, when to prune, and how to prune, followed by a comprehensive comparative analysis of different kinds of pruning methods in Section comparative. Section fusion discusses integrating pruning with other compression methods. Practical recommendations for choosing pruning methods and future directions are provided in Section recommendations. We conclude this paper in Section conclusion.",
      "origin_cites_number": 44
    },
    {
      "section_title": "Taxonomy",
      "level": "1",
      "content": "There are three critical questions when pruning a deep neural network. (1) Whether to achieve universal or specific acceleration through neural network pruning? Universal acceleration operates independently of specialized hardware/software, while specific acceleration relies on it. (2) When to prune the neural network? Specifically, is the neural network pruned before, during, or after training the network for static pruning or dynamic (i.e., run-time) pruning? (3) Whether to prune based on specific criteria or learn to prune? The answers to the three questions correspond to the three primary aspects of deep neural network pruning, as depicted in the orange sections of Fig.~Fig:pruning-taxonomy. The first question is whether speedup depends on specific hardware/software. It is usually divided into three types: unstructured (frankle2019lottery,sun2024simple,tanaka2020pruning), semi-structured (also called pattern-based) (frantar2023sparsegpt,meng2020pruning,ma2020image) and structured (ma2023llmpruner,wang2023trainability,fang2023structural). Only structured pruning can achieve universal neural network acceleration without requiring special hardware or software. Conversely, both unstructured and semi-structured pruning need the support of special hardware or software. Given that the primary objective of pruning is acceleration, the first question addresses the most fundamental and user-concerned aspect of this process. The second question particularly concerns the arrangement between pruning weights and training weights of the neural network for static pruning. Based on whether pruning is performed before, during, or after training the network, static pruning can be divided into three categories: pruning before training (PBT) (lee2019snip,tanaka2020pruning,su2020sanity,wang2020picking), pruning during training (PDT) (liu2017learning,wen2016learning,huang2018data), and pruning after training (PAT) (frankle2019lottery,yang2023global,ma2023llmpruner). In dynamic pruning, subnetworks are generated at run-time for each input data. The third question considers whether to prune neural networks using specific criteria or by learning. Criteria rely on a scoring formula to measure the importance of each weight (or filter, neuron, and so on). Commonly used pruning criteria include magnitude, norm, loss change, etc. In addition, it is possible to prune neural networks by learning, such as through sparsity regularization training huang2018data or dynamic sparse training liu2020dynamic. Whether through criteria or learning, pruning aims to determine the weights of a network that should be pruned. The above three aspects determine the main characteristics of a pruning algorithm. Different combinations of these three aspects form various pruning methods. We provide a new taxonomy of deep neural network pruning in Section~granularity - learntoprune, and Section~fusion, as shown in Fig.~Fig:pruning-taxonomy. figure*[t] \\centering minipage{16cm} \\includegraphics[width=16cm,height=6.5cm]{figures/taxonomy-2-5-black-LLM.pdf} minipage -0.4cm \\small An overview of the hierarchical structure of the survey, with sections involving large models highlighted by a pink triangle. -0.4cm figure*",
      "origin_cites_number": 8
    },
    {
      "section_title": "Specific or Universal Speedup",
      "level": "1",
      "content": "This section categorizes pruning into unstructured, semi-structured, and structured. The first two types correspond to specific speedup, while the third corresponds to universal speedup.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Unstructured Pruning",
      "level": "2",
      "content": "Unstructured pruning, also called non-structured pruning or weight-wise pruning, is the finest-grained case. Definition 1 (Unstructured Pruning). Given neural network weights $W=\\{w_0,w_1,...,w_K\\}$, a dataset $D = \\{(x_i, y_i)\\}_{i=1}^{N}$ composed of input ($x_i$), output ($y_i$) pairs, and a desired total number of non-zero weights $k$ (less than $K$), unstructured pruning can be written as the following constrained optimization problem lee2019snip: equation split \\textrm{min}\\limits_{W} \\ & L(W;D) = \\textrm{min}\\limits_{W} 1{N}\\sum_{i=1}^N\\ell(W;(x_i,y_i)), \\\\ & s.t. \\ \\lVert W\\rVert_0 \\leq k. split equation In practice, for small or medium models, unstructured pruning usually does not directly set the weights to 0 but sets their corresponding masks (or indicators) $M$ to 0 liu2021group, wang2020picking. In this case, unstructured pruning is regarded as applying a binary mask to each weight. Consequently, Eq.(eq:unstructured-pruning-1) is correspondingly changed as: equation split \\textrm{min}\\limits_{w,m} \\ & L(W\\odot M;D) = \\textrm{min}\\limits_{W,M} 1{N}\\sum_{i=1}^N\\ell(W \\odot M;(x_i.y_i)), \\\\ & s.t. \\ \\lVert M\\rVert_0 \\leq k. split equation Generally, the network is retrained (i.e., fine-tuning or training-from-scratch) with fixed masks $M$, and the masked-out weights are not involved in retraining. In the case of large models, such as LLMs, assigning a mask to each weight poses a challenge due to the immense number of weights. It is common to directly set the weights that need to be pruned to zero, as seen in sun2024simple,frantar2023sparsegpt. Fig.~Fig:visualization-unstructured-pruning is an example of weight-wise pruning by removing the weights directly (as shown in Fig.~Fig:visualization-unstructured-pruning (a)) or masking the weights with their corresponding masks (as shown in Fig.~Fig:visualization-unstructured-pruning (b)), respectively. Since it can remove weights anywhere, the irregular replacement of non-zero weights leads to actual acceleration requiring the support of special software and/or hardware wen2016learning,li2017pruning,tang2021manifold,han2015deep. Therefore, we classify unstructured pruning as a specific speedup technique.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Structured Pruning",
      "level": "2",
      "content": "Definition 2 (Structured Pruning). Given a specific prune ratio and a neural network with $S=\\{s_1, s_2, ..., s_L\\}$, where $s_i$ can be the set of channels, filters, neurons, or transformer attention heads in layer $i$. Structured pruning aims to search for $S^{'}=\\{s_1^{'}, s_2^{'}, ..., s_L^{'}\\}$ to minimize performance degeneration and maximize speed improvement under the given prune ratio, where $s_i^{'}\\subseteq s_i$, $i\\in \\{1,.., L\\}$. figure*[t] \\centering \\subfloat[From the view of neurons and connections]{ minipage{6.5cm} \\includegraphics[width=6.5cm,height=3.5cm]{figures/unstructured-pruning.pdf} -0.3cm minipage } \\subfloat[From the view of weights and masks]{ minipage{6.5cm} \\includegraphics[width=6.5cm,height=3.5cm]{figures/unstructured-pruning-2.pdf} -0.3cm minipage } The visualization of unstructured pruning. The light orange circles denote neurons. -0.4cm figure* figure*[t] \\centering \\subfloat[Structured pruning for CNNs]{ minipage{5cm} \\includegraphics[width=5cm,height=3.5cm]{figures/structured-pruning-2.pdf} -0.3cm minipage } \\subfloat[Structured pruning for Transformers]{ minipage{5cm} \\includegraphics[width=5cm,height=3.5cm]{figures/structured-pruning-add-llm.pdf} -0.3cm minipage } \\subfloat[Examples of semi-structured pruning]{ minipage{5cm} \\centering\\includegraphics[width=5cm,height=3.5cm]{figures/semi-structured-pruning-4.pdf} -0.3cm minipage } The visualization of typical structured and semi-structured pruning: each Conv. consists of cubes representing weights. Orange cube sets/blocks indicate the pruned structures. Best view in color and zoom in. -0.4cm figure* Structured pruning removes entire filters (you2019gate), channels (liu2021group,ashkboos2024slicegpt), transformer attention heads (ma2023llmpruner), or even layers (men2024shortgpt), as shown in Fig.~Fig:visualization-structured-pruning (a) and Fig.~Fig:visualization-structured-pruning (b), and can rebuild a narrow model with a regular structure~For Transformers \\cite{vaswani2017attention, deleting rows (columns) of weight matrices is similar to pruning filters (channels) for CNNs.}. It does not require the support of special hardware and software (such as sparse convolution libraries) and can directly speed up networks and reduce the size of the neural networks liu2021group,nonnenmacher2022sosp, luo2017thinet,li2017pruning.",
      "origin_cites_number": 6
    },
    {
      "section_title": "Semi-structured Pruning",
      "level": "2",
      "content": "To improve the flexibility of structured pruning and achieve a lower accuracy drop when the pruning rate is high, some works (meng2020pruning,ma2020image) introduce semi-structured pruning, also called pattern-based pruning in ma2020image, to achieve high accuracy and structural regularity simultaneously. Some examples are shown in Fig.~Fig:visualization-structured-pruning (c). For example, meng2020pruning treat a filter as several stripes and propose to prune stripes in each filter. SparseGPT~frantar2023sparsegpt introduces a 2:4 or 4:8 sparsity pattern to reduce a LLM's parameters by half. In this configuration, at least two zeros are mandatory in every set of four consecutive values. The 2:4 pattern can utilize NVIDIA Ampere GPUâ€™s sparse tensor cores to accelerate matrix multiplication~yang2023global. In contrast, structured pruning is classified as coarse-grained structured pruning (ma2020image,li2021npas), while semi-structured pruning is classified as fine-grained.",
      "origin_cites_number": 6
    },
    {
      "section_title": "When to Prune",
      "level": "1",
      "content": "This section distinguishes three pipelines for static pruning, as illustrated in Fig.~fig:pruning-pipelines-red, and run-time pruning. The examples of the three pipelines of static pruning are shown in Fig.~Fig:typical-sparsity-changes. Some notations and statistics on three static pruning pipeline types are shown in Appendix A and D, respectively.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Pruning Before Training",
      "level": "2",
      "content": "Pruning Before Training (PBT) (as shown in Table~tab:puning-at-initialization), also called foresight pruning wang2020picking or pruning at initialization lee2019snip,tanaka2020pruning, represents a class of pruning methods that use randomly initialized weights for pruning the network. The principal motivation of PBT methods is to eliminate the cost of pre-training. Without loss of generality, we define a neural network as a function $f(x;W\\odot M)$. The mask $M$ is used for pruning initialized weights $W_{0}$ sampled from a specific initialization distribution. After pruning, the network $f(x;W_{0} \\odot M^{'})$ is trained to converge to $f(x;W_{t} \\odot M^{'})$ after $t$ epochs, where $M^{'}$ indicates the sparsity after pruning. PBT usually follows two steps: directly prunes untrained dense network based on a specific criterion and then trains the sparse network to convergence for high performance, as illustrated in Fig.~fig:pruning-pipelines-red (a). The second step is related to static sparse training liu2022unreasonable that aims to train a sparse network with a fixed sparse pattern. By avoiding the time-consuming pre-training process, PBT methods achieve the gains at training and inference time. table*[t] Representative methods of pruning before training. ``U/S'' denotes unstructured or structured pruning.\\protect\\footnotemark -0.1cm \\centering tabular{l|ccccc} 0.3ex Method & Criterion & U/S & Data-Driven (Y/N) & One-shot (Y/N) \\\\ \\hline SNIP (2019) lee2019snip & $\\left |\\nabla_{W}L(W) \\odot W \\right |$ & U & Y & Y\\\\ GraSP (2020) wang2020picking & $-(H \\nabla_{W}L(W)) \\odot W$ & U & Y & Y\\\\ Smart-Ratio (2020) su2020sanity & keep-ratios & U & N & Y\\\\ SynFlow (2020) tanaka2020pruning & $\\nabla_{W} R_{SF}(W)\\odot W$ & U & N & N\\\\ PFS (2020) wang2020pruning & $l_1$-norm based sparsity regularization & S & Y & N\\\\ RST (2022) bai2022dual & $l_2$-norm based sparsity regularization & U & Y & N\\\\ 0.3ex tabular -0.3cm table* See Appendix~A for the notations in Table~\\ref{tab:puning-at-initialization \\~{} Table~tab:pruning-after-training.} figure*[t] \\centering minipage{12.5cm} \\includegraphics[width=12.5cm,height=6cm]{figures/pruning-pipelines-black.pdf} minipage The typical pipelines of static pruning. Dashed boxes represent models and solid boxes denote actions. Dashed arrows indicate optional steps. -0.4cm figure* Currently, PBT primarily targets CNNs. lee2019snip pioneer the research of PBT and propose a Single-shot Network Pruning (SNIP) method to remove the weights whose absence leads to the slightest loss change. lee2020signal explain the feasibility of SNIP through signal propagation, empirically find pruning damages the dynamical isometry Saxe2014exact of neural networks, and propose a data-free orthogonal initialization, an approximation of exact isometry, to prune random networks. Different from the signal propagation perspective in lee2020signal, which focuses on initialization scheme, wang2020picking propose Gradient Signal Preservation (GraSP) to exploit gradient norm after pruning to remove weights that have the least effect on the gradient flow. tanaka2020pruning go a step further and point out that an effective subnetwork can be identified without training and looking at the data. They propose a data-free pruning algorithm called Iterative Synaptic Flow Pruning (SynFlow) that uses the concept of synaptic flow to consider the interaction of different layers and avoid layer collapse through gradual pruning. gebhart2021unified exploit the Path Kernel (i.e., the covariance of path values in a network) based on Neural Tangent Kernel jacot2018neural to unify several initialization pruning methods, such as SNIP lee2019snip, CraSP wang2020picking, SynFlow tanaka2020pruning, under a single framework. Some works (su2020sanity,frankle2021pruning) suggest that the delicate pruning criteria may not be crucial for performance, but the layerwise sparsity. For example, su2020sanity propose randomly pruning each layer by using a series of layerwise keep-ratios (smart-ratios) without using any data. Inspired by Weight Agnostic Neural Networks (WANNs) gaier2019weight, ramanujan2020what empirically find that with an untrained network grows wider and deeper, it will contain a subnetwork that performs as well as a trained network with the same number of parameters. Then they propose the edge-popup method to identify such randomly initialized subnetworks. Unlike this method, which maintains randomly weighted values, bai2022dual propose Dual Lottery Ticket Hypothesis (DLTH) where both the subnetwork and weights are randomly selected at initialization and propose Random Sparse Network Transformation (RST) which fixes the sparse architecture but gradually train the remaining weights. Some recent works (wang2020pruning,liu2022unreasonable,hoang2023revisiting) explore why an effective subnetwork can be identified at initialization. For example, liu2022unreasonable empirically demonstrate that the network size and appropriate layer-wise prune ratios are two vital factors in training a randomly pruned network at initialization from scratch to match the performance of the dense models. In contrast, kumar2024free find that at the same prune ratio, subnetworks derived by pruning during or after training exhibit higher effective parameter count and greater expressiveness compared to those pruned at initialization.",
      "origin_cites_number": 28
    },
    {
      "section_title": "Pruning During Training",
      "level": "2",
      "content": "Pruning During Training (PDT) (as shown in Table~tab:pruning-during-training) generally takes a randomly initialized dense network $f(x;W_{0})$ as the input model and jointly trains and prunes the neural network by updating weights $W$ and masks of weights (or filters, channels, etc.) $M$ during training. These dynamic schemes change the masks and produce the subnetwork $f(x;W_{t} \\odot M_{t})$ after $t$ iterations/epochs. After pruning, many PDT methods (huang2018data,evci2020rigging,zhao2019variational,he2018soft,liu2019metapruning,ning2020dsa,cho2023pdp) directly obtain the subnetworks without the need for training-from-scratch or fine-tuning. The typical pipeline of PDT is illustrated in Fig.~fig:pruning-pipelines-red (b). PDT methods have been less explored due to the more complicated dynamic process than PBT and PAT methods. We summarize the main prior solutions into four paradigms: (1) sparsity regularization based, (2) dynamic spare training based, (3) score-based, and (4) differentiable pruning based. Methods related to (2) conduct sparse-to-sparse training, while the other three take dense-to-sparse training.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Sparsity Regularization based Methods",
      "level": "3",
      "content": "Sparsity regularization technique wen2016learning is commonly used in PDT methods. This category of methods starts with dense networks, imposes sparse constraints on loss functions, and usually zeros out some weights or their masks during training. The main effort is to design the effective target loss function $L$ with an advanced penalty scheme and efficient optimization algorithms. For example, wen2016learning propose Structured Sparsity Learning (SSL) to learn a sparse structure by group LASSO yuan2006model regularization during training. However, SSL requires computing the gradients of the regularization term w.r.t. all the weights, which is non-trivial. gordon2018morphnet propose MorphNet that reuses the parameters of Batch Normalization (BN) and conducts sparsity regularization on these parameters. However, some networks (e.g., certain VGGNets simonyan2015very) have no BN layers. Instead of reusing BN parameters, some works associate scaling factors with channels, layers, etc. For example, huang2018data propose Sparse Structure Selection (SSS) that associates scaling factors for CNN micro-structures (e.g., channels, residual blocks) and exploit sparsity regularization to force the output of the micro-structures to zero, rather than pushing the weights in the same group to zero as in wen2016learning. In addition, SSS does not require extra fine-tuning in wen2016learning. li2019compressing propose Factorized Convolutional Filter (FCF) which introduces a binary scalar to each filter and proposes a back-propagation algorithm with Alternating Direction Method of Multipliers (ADMM) boyd2010distributed to train the weights and the scalars during training jointly. table*[t] Representative methods of pruning during training. ``Retrain'' refers to training from scratch or fine-tuning, ``U/S'' denotes unstructured or structured pruning. -0.1cm \\centering tabular{l|ccc} 0.3ex Method & Object Function/Criterion & Retrain (Y/N) & U/S\\\\ \\hline Network Slimming (2017) liu2017learning & $min_{W,\\gamma}\\ell(y,f(x;W))+\\lambda \\lVert \\gamma \\rVert_{1}$ & Y & S \\\\ SSS (2018) huang2018data & $min_{W,\\gamma}\\ell(y,f(x;W,\\gamma))+R(W)+\\lambda \\lVert \\gamma \\rVert_{1}$ & N & S\\\\ SET (2018) mocanu2018scalable & $\\lVert W \\rVert$ for drop and random for grow & N & U \\\\ DSA (2020) ning2020dsa & $min_{A} L_{v}(W(A),A))$ & N & S \\\\ GraNet (2021) liu2021sparse & $\\lVert W \\rVert$ for drop and gradient for grow & N & U\\&S \\\\ FreeTickets (2022) liu2022deep & $\\lVert W \\rVert$ for drop and gradient for grow & N & U \\\\ 0.3ex tabular -0.5cm table* figure*[t] \\centering \\subfloat[SNIP lee2019snip as an example of PBT]{ minipage{5cm} \\includegraphics[width=5cm,height=3.5cm]{figures/numofweights-PBT-2.pdf} -0.2cm -0.2cm minipage } \\subfloat[FCF li2019compressing as an example of PDT]{ minipage{5cm} \\includegraphics[width=5cm,height=3.5cm]{figures/numofweights-PDT-3.pdf} -0.2cm -0.2cm minipage } \\subfloat[GFP liu2021group as an example of PAT]{ minipage{5cm} \\includegraphics[width=5cm,height=3.5cm]{figures/numofweights-PAT-2.pdf} -0.2cm -0.2cm minipage } The illustration of typical sparsity changes for PBT, PDT, and PAT (best view in color and zoom in). PBT is usually the cheapest, while PAT is the most expensive. -0.4cm figure*",
      "origin_cites_number": 19
    },
    {
      "section_title": "Dynamic Sparse Training based Methods",
      "level": "3",
      "content": "A class of the PDT methods (mocanu2018scalable,dai2019nest,mostafa2019parameter,evci2020rigging,liu2021sparse,lin2020dynamic,dettmers2019sparse) take randomly initialized sparse network rather than dense network as the input model. Subsequently, a common method involves pruning a fraction of unimportant weights and then regrowing the same number of new weights to adjust the sparse architecture. By repeating the prune-and-grow cycle during training, these method keeps searching for better sparse architecture, a process classified as dynamic sparse training in evci2020rigging. For example, mocanu2018scalable propose Sparse Evolutionary Training (SET) that removes the smallest positive and most negative weights and grows new weights in random locations. Instead of pruning a fixed fraction of weights at each redistribution step, such as in SET mocanu2018scalable, mostafa2019parameter propose Dynamic Sparse Reparameterization (DSR) which uses an adaptive threshold for pruning. In addition, DSR mostafa2019parameter reallocates weights across layers and does not restrict to the inner-layer weight redistribution in SET mocanu2018scalable. liu2022deep propose an ensemble method FreeTickets that ensemble sparse subnetworks created by sparse-to-sparse methods. Rather than using a random regeneration scheme, liu2021sparse propose Gradual Pruning with zero-cost Neuroregeneration (GraNet) to remove connections of networks based on their weight magnitudes and regrow connections of networks based on their gradient. They argue that even the weights with zero gradient values indicate the connection importance. sokar2022dynamic pioneer to explore dynamic sparse training in Reinforcement Learning (RL). evci2022gradeint analyze the reasonability of dynamic sparse training and find that sparse networks have poor gradient flow at initialization, but dynamic sparse training significantly improves gradient flow.",
      "origin_cites_number": 11
    },
    {
      "section_title": "Score-based Methods",
      "level": "3",
      "content": "Some PDT methods exploit scoring criteria to prune networks during training. he2018soft propose Soft Filter Pruning (SFP) filter pruning method which can train networks from scratch and prune them simultaneously by using the $l_2$ norm of each filter as its importance measure. Instead of associating masks with filters, they directly set the pruned filter weights to zero, which can be updated from zero through the forward-backward process. Therefore, pruned filters in one epoch can be recovered in the next. However, SFP requires manually preset a prune ratio for each layer. he2019filter propose Filter Pruning via Geometric Median (FPGM) to prune redundant filters that are nearest to the geometric median fletcher2008robust of the filters within the same layer. However, FPGM also requires a pre-defined prune ratio for each layer. liu2017learning propose a method called Network Slimming that introduces a scaling factor for each channel and jointly trains the weights and the scaling factors by adding the regular loss $\\ell$ with sparsity regularization on the factors, and the magnitudes of these scaling factors are used as the filter scores. In practice, they reuse the $\\gamma$ parameters in BN layers as the scaling factors.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Differentiable Pruning based methods",
      "level": "3",
      "content": "Advances in differentiable network compression (liu2019darts,zhang2021idarts) popularize differentiable techniques for pruning, such as ning2020dsa,guoO2020dmcp,ma2022differentiable,cho2023pdp,yu2023xpruner. For example, ning2020dsa propose Differentiable Sparsity Allocation (DSA) to set layer-wise prune ratios. They soften the hard pruning with a differentiable method using masks drawn from a distribution governed by prune ratios, allowing for gradient calculation of the target loss w.r.t. these ratios. These gradients then measure layer sensitivity. guoO2020dmcp model channel pruning as a differentiable Markov process with architecture parameters, representing each channel as a state and using state transitions to determine the probability of retaining a channel based on the retention of its predecessor. Unlike ning2020dsa and guoO2020dmcp, cho2023pdp generate soft pruning masks for weights without extra trainable parameters to accomplish differentiable pruning for CNNs and Transformers. table*[th] Representative methods of pruning after training. ``U/S'' denotes unstructured or structured pruning. -0.1cm \\centering tabular{l|cc} 0.3ex Method & Object Function/Criterion & U/S\\\\ \\hline Auto-Balance (2018) ding2018auto & score($F_{i,j}$)=$\\lVert F_{i,j} \\rVert_{1}$ & S \\\\ LTH (2019) frankle2019lottery & score($w_{i}$)=$\\lVert w_{i} \\rVert_{1}$ & U\\\\ Taylor-FO-BN (2019) molchanov2019importance & score($w_{i}$)=$(\\nabla_{w_{i}}\\ell \\odot w_{i})^2$ & S\\\\ SSR (2019) lin2019towardscompact & $\\textrm{min}\\limits_{W}1{N}\\sum_{i=1}^{N}\\ell(y_i,f(x_i,W))+\\lambda R(W)$ & S \\\\ GReg (2021) wang2021neural & score($F_{i,j}$)=$\\lVert F_{i,j}\\rVert_{1}$ & U\\&S\\\\ GFP (2021) liu2021group & score($c_{i}$)=$\\sum_{a=1}^{N}(\\nabla_{m_{i}}\\ell_{a})^2$ or score($c_{i}$)=$\\sum_{a=1}^{N}(\\sum_{g\\in Group}\\nabla_{m_{i}^{g}}\\ell_{a})^2$& S \\\\ SparseGPT (2023) frantar2023sparsegpt & score($w_{ij}$)=$[|W|^{2}/diag((x^{T}x+\\lambda I)^{-1})]_{ij}$ & U \\\\ Wanda (2023) sun2024simple & score($w_{ij}$)=$|w_{ij}|\\cdot \\lVert x_{j} \\rVert_{2}$ & U \\\\ LLM-Pruner (2023) ma2023llmpruner & score($w_{i}$)=$|\\nabla_{w_{i}}L \\odot w_{i}-1{2}\\sum_{j=1}^{N}(\\nabla_{w_{i}}L \\odot w_{i})^2|$ & S \\\\ ShortGPT (2024) men2024shortgpt & $1-\\mathbf{x_{i}^{T}x_{i+1}}{\\lVert x_{i} \\rVert_{2} \\lVert x_{i+1} \\rVert_{2}}$ & S \\\\ LLM-Streamline (2024) chen2024compressing & $max_layer_icos(x_{i},x_{i+n})$ & S \\\\ 0.3ex tabular -0.4cm table*",
      "origin_cites_number": 18
    },
    {
      "section_title": "Pruning After Training",
      "level": "2",
      "content": "Pruning After Training (PAT) (as shown in Table~tab:pruning-after-training) is the most popular type of pruning pipeline because it is commonly believed that pre-training the dense network is necessary to obtain an efficient subnetwork liu2019rethinking. Especially for large models, such as LLMs and diffusion models, pruning is specifically applied to pre-trained models. This class of pruning methods typically follows a Pretrain-Prune-Retrain (liu2021group, ma2023llmpruner) or Pretrain-Prune (frantar2023sparsegpt,ashkboos2024slicegpt) process, as shown in Fig.~fig:pruning-pipelines-red (c). (1) Pre-train a randomly initialized dense network $f(x;W_{0})$ to converge to $f(x;W_{t})$. (2) Prune the weights (or filters, neurons, etc.) that have the least influence on performance and fine-tune the pruned network $f(x;W_{t}^{'} \\odot M^{'})$ for several iterations, where $W_{t}^{'}$ and $M^{'}$ are the weights and masks after pruning, respectively. This step can be done at least once (i.e., one-shot pruning) or multiple times (i.e., iterative pruning). (3) Train the remaining weights from scratch $f(x;W_{0} \\odot M^{''})$ or fine-tune $f(x;W_{t}^{''} \\odot M^{''})$ to recover the performance renda2020comparing, where $W_{t}^{''}$ and $M^{''}$ are the final results of weights and masks after the pruning process, respectively. Sparsity may gradually increase during the pruning process until it achieves the target.",
      "origin_cites_number": 5
    },
    {
      "section_title": "LTH and its Variants",
      "level": "3",
      "content": "Lottery Ticket Hypothesis (LTH) frankle2019lottery,liu2024survey is one of the most influential hypotheses in the neural network pruning domain. Given a pre-trained network, LTH iteratively removes a percentage of the weights based on their magnitudes. After pruning, the remaining weights are retrained from scratch with the original initialization, rather than random reinitialization, to match the original networks' accuracy. It challenges the commonly held belief that pre-trained weights must be used for retraining and conjectures the existence of an independently trainable sparse subnetwork within a dense network. Inspired by LTH, there are various follow-up works to identify wider tickets across multiple kinds of neural networks (such as CNNs diffenderfer2021multi, Generative Adversarial Networks (GANs) chen2021gans, Variational AutoEncoders (VAEs) kalibhat2021winning, Graph Neural Networks (GNNs) hui2023rethinking, Transformer based models prasanna2020bert) and understand LTH better, which can be classified into five main classes: (1) proposing a stronger lottery ticket hypothesis, (2) exploring the transferability of LTH, (3) generalizing LTH to other contexts, (4) theoretical justification, and (5) revisiting and questioning LTH. (1)~Some recent works (malach2020proving,orseau2020logarithmic,diffenderfer2021multi) prove stronger hypothesis than LTH frankle2019lottery. For example, diffenderfer2021multi propose a stronger Multi-Prize LTH, which claims that winning tickets can be robust to extreme forms of quantization (i.e., binary weights and/or activations). Based on this, they introduce the Multi-Prize Tickets (MPTs) algorithm to find MPTs on binary neural networks for the first time. (2)~Some literature (morcos2019one,mehta2019sparse,gan2022playing,chen2021unified) studies the transferability of a winning ticket found in a source dataset to another dataset, which provides insights into the transferability of LTH. For example, morcos2019one find OneTicket that can generalize across a variety of datasets and optimizers within the natural image domain. mehta2019sparse propose the ticket transfer hypothesis and transfer winning tickets for different image classification datasets. (3)~In addition to image classification, LTH has been extended to other contexts, such as node classification and link prediction (chen2021unified), vision-and-language (gan2022playing), and NLP (prasanna2020bert). For example, chen2021unified pioneer to generalize LTH to GNNs and propose Graph Lottery Ticket (GLT). prasanna2020bert explore the existence of winning tickets for fine-tuned BERT devlin2019bert and identify subnetworks that match the model's performance. (4)~On one hand, some literature (zhang2021validating,zhang2021why,paul2023unmasking) analyzes the reasons why LTH frankle2019lottery is able to win. For example, zhang2021validating exploit dynamical systems theory and inertial manifold to theoretically verify the validity of LTH. evci2022gradeint observe that the success of LTH lies in effectively re-learning the original pruning solution they are derived. zhang2021why pioneer to provide a formal justification for the improved generalization of winning tickets observed in experimental results from LTH. (5)~On the other hand, some recent works (ma2021sanity,liu2021lottery,liu2019rethinking) revisit and challenge the existence of LTH. For example, ma2021sanity provide a more rigorous definition of LTH for precisely identifying winning tickets and find that whether and when the winning tickets can be identified highly replies on the training settings, such as learning rate, training epochs, and architecture characteristics like network capacities and residual connections. It is more likely to find winning tickets by using a small learning rate or an insufficient number of training epochs. It is worth pointing out that in some works wang2022recent,sehwag2020hydra, LTH frankle2019lottery is classified as a PBT method. However, LTH selects masks based on a pre-trained network, which does not conform to the definition of PBT that attempts pruning the initialized network before training. Therefore, it is more reasonable to classify LTH as a PAT method.",
      "origin_cites_number": 27
    },
    {
      "section_title": "Other score-based Methods",
      "level": "3",
      "content": "The most straightforward and intuitive way to select pruning candidates is to evaluate them based on their norms. For example, han2015deep propose to measure weight importance by its absolute value. In addition to norm-based criteria, evaluating loss change with and without the weights is also popular. For example, nonnenmacher2022sosp propose Second-order Structured Pruning (SOSP) to selectively zero out filter masks to minimize the effects of the loss change from removing some filters. ma2023llmpruner propose LLM-Pruner to pioneer the removal of unimportant coupled channels and multi-attention heads in LLMs using the first-order Taylor expansion to estimate loss changes and fine-tune the pruned models using LoRA hu2022lora. fang2023structural present Diff-Pruning which scores weights in diffusion models using the first-order Taylor expansion over pruned timesteps. shi2023upop introduce Unified and Progressive Pruning (UPop), which prunes large multimodal models by leveraging accumulated trainable mask gradients during each iteration of the search phase. In addition to using loss change, some works (men2024shortgpt,nova2023gradient,dery2024everybody,kim2024shortened) design new metrics. For example, men2024shortgpt introduce a metric called Block Influence (BI), which assesses the significance of a layer by measuring the extent to which it alters the hidden states and removes redundant layers of LLMs. kim2024shortened assess the significance of a layer in LLMs by measuring its impact on perplexity (PPL) upon its removal.",
      "origin_cites_number": 9
    },
    {
      "section_title": "Sparsity Regularization based Methods",
      "level": "3",
      "content": "Some works (he2017channel,lin2019towards,lin2019towardscompact,xia2022structured,xia2024sheared) exploit sparsity regularization technique. For example, he2017channel propose an alternative two-step algorithm that introduces a scalar mask to each channel of a pre-trained CNN model, selects redundant channels based on LASSO regression tibshirani1996regression, and reconstructs the outputs of unpruned channels using linear least squares. Energy-Constrained Compression (ECC) yang2019ecc builds an energy consumption model via a bilinear regression function. Network Pruning via Performance Maximization (NPPM) gao2021network trains a performance prediction network and uses it as a proxy of accuracy to guide searching for subnetworks based on regularization penalty. fang2023depgraph develop a general method called DepGraph to analyze dependencies in various network structures (e.g., CNNs, RNNs, GNNs, Transformers) and propose a structured pruning based on sparsity regularization. Using the $l_{0}$ regularization approach, xia2024sheared efficiently learn pruning masks that match the target architecture and maximize performance by jointly optimizing weights and pruning masks with a min-max objective for LLMs. Some methods (wang2021neural,wang2023trainability,ding2018auto,fang2023depgraph) select important weights (or filters, neurons, etc.) by combining norm-based criteria and sparsity regularization.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Pruning in Early Training",
      "level": "3",
      "content": "Instead of fully training a network from $f(x;W_{0})$ to $f(x;W_{T})$, this class of methods explore the network architecture by training a network only for a few iterations or epochs, i.e., $f(x;W_{t})$, where $t<<T$. For example, you2020drawing propose Early-Bird (EB) tickets which indicate that winning tickets can be identified at the early training stage via inexpensive training schemes (e.g., early stopping and low-precision training) at large learning rates and achieve similar performance to the dense network. Inspired by you2020drawing, chen2021earlybert propose EarlyBERT that identifies structured winning tickets in the early stage of BERT michel2019sixteen training. frankle2020linear find that, in large-scale settings (such as ResNet-50 and Inception-v3 on ImageNet), the subnetworks that exhibit stability to SGD noise are able to reach full accuracy early in training.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Post-Training Pruning",
      "level": "3",
      "content": "In contrast to the general PAT methods that follow the Pretrain-Prune-Retrain procedure, recently proposed post-training pruning methods simplify the three-step process Pretrain-Prune. It involves pruning a pre-trained model $f(x;W_{t})$ without retraining, typically achieving negligible accuracy loss by using compensation mechanisms to mitigate performance degradation. This class of pruning methods is particularly attractive for billion-parameter models because retraining such pruned models is still very expensive. For example, kwon2022fast propose a structured post-training pruning framework for Transformers that features Fisher-based mask search, rearrangement, and tuning, achieving pruning on a single GPU in three minutes without retraining. SparseGPT frantar2023sparsegpt, a groundbreaking unstructured post-training pruning method for LLMs, tackles the pruning problem as an approximate sparsity reconstruction problem and prunes LLMs at least 50\\% sparsity with minor accuracy loss without retraining. To address SparseGPT's reconstruction cost, Wanda~sun2024simple uses weight magnitudes and input norms to facilitate unstructured post-training pruning on LLMs without updating weights. Unlike SparseGPT and Wanda, SliceGPT ashkboos2024slicegpt implements structured post-training pruning using orthogonal matrix transformations and principal component analysis (PCA) to remove columns and rows of the weight matrices in LLMs. FLAP an2024fluctuationbased introduces a fluctuation metric and uses a bias compensation mechanism for LLMs' performance recovery.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Run-time Pruning",
      "level": "2",
      "content": "The prior works on pruning usually focus on static pruning methods where the pruned model is reused for different inputs. In contrast, some methods prune neural networks according to individual inputs dynamically, known as run-time pruning rao2019runtime. This line of work is based on the premise that for a given task, the difficulty of producing accurate output can vary, implying that necessary model capacities for different inputs are different tang2021manifold. For example, rao2019runtime propose a Runtime Network Routing (RNR) framework to conduct dynamic routing based on the input image and current feature maps and select an optimal path subset for compression. tang2021manifold point out that the importance of channels highly depends on the input data and propose to generate different subnetworks for each instance. At inference, only channels with saliencies larger than the threshold need to be computed, and the redundant features are skipped. hua2019channel exploit input-specific characteristics and propose CGNets to predict unimportant regions by the partial sum of the output activation by performing convolution on a subset of input channels. gao2019dynamic propose Feature Boosting and Suppression (FBS) to predict the saliency of channels and skip those with less contribution to the classification results at run-time. elkerdawy2022fire pose dynamic model pruning as a self-supervised binary classification problem. meng2022contrastive propose Contrastive Dual Gating (CDG), another self-supervised dynamic pruning method that uses contrastive learning hadsell2006dimensionality. tuli2023acceltran introduce DynaTran, which prunes activations at runtime based on the magnitude of the input matrix to enhance transformer inference throughput.",
      "origin_cites_number": 10
    },
    {
      "section_title": "Pruning Criteria",
      "level": "1",
      "content": "In this section, we summarize some commonly used pruning criteria for evaluating the importance of weights (or filters, neurons, etc.) from different perspectives, including magnitude (han2015learning,see2016compression,lubana2021gradient,sehwag2019towards), norm (he2018soft,li2017pruning), saliency and/or sensitivity (zhao2019variational,molchanov2017pruning), and loss change (molchanov2017pruning,li2021towards,you2019gate,nonnenmacher2022sosp,liu2021group). There is no rigid boundary between these criteria, but a different emphasis.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Magnitude-based Pruning",
      "level": "2",
      "content": "hanson1988comparing is one of the earliest works that propose magnitude-based pruning to reduce hidden units. han2015deep popularize magnitude-based pruning for deep neural networks, which prunes the lowest-magnitude weights. It is based on the assumption that weights with smaller absolute values tend to have the least influence on the network's output li2017pruning. The formulation is defined as equation m_i = cases 1: if \\ \\lVert w_i \\rVert_{1} \\ge a \\\\ 0: if \\ \\lVert w_i \\rVert_{1} < a cases, equation where $a$ is a threshold. Some criteria combine weight magnitude with activation magnitude. For example, the score for a weight $w_{ij}$ in Wanda sun2024simple is defined by: equation s_{ij} = \\left | w_{ij} \\right |\\cdot \\lVert x_{j}\\rVert_{2}. equation In addition to weight and activation, magnitude-based pruning can be applied to other values (yu2022width,dery2024everybody). For example, dery2024everybody propose to prune LLMs by selecting modules with the highest magnitude of module relevance until the pruning constraint is met. Magnitude-based criteria can be applied to either unstructured (frankle2019lottery,see2016compression,sun2024simple) or structured pruning (li2017pruning,dai2019nest,dery2024everybody). For example, li2017pruning score the filters by calculating the sum of the absolute magnitude of their weights. In addition, magnitude-based criteria can be combined with global/local and one-shot/iterative schedules. For example, the works in rosenfeld2021predictability and lee2021layeradaptive propose magnitude-based iterative global pruning methods. lubana2021gradient argue that magnitude-based pruning results in faster model convergence than magnitude-agnostic methods.",
      "origin_cites_number": 12
    },
    {
      "section_title": "$l_p$ Norm",
      "level": "2",
      "content": "Some methods (he2018soft,li2017pruning,sun2024simple) use the $l_p$ norm to evaluate the importance of weights (or filters, neurons, etc.). For example, he2018soft exploit the $l_p$ norm to evaluate the importance of the filter $F_{i,j}$, as shown in Eq.(eq.4). equation \\lVert F_{i,j} \\rVert_{p}=\\left(\\sum_{n=1}^{c_{in}^{(i)}}\\sum_{k_1=1}^{k^{(i)}}\\sum_{k_2=1}^{k^{(i)}}\\left | F_{i,j}(n,k_1,k_2)\\right |^p \\right)^{1{p}} , equation where $k^{(i)}$ is the kernel size of layer $i$ in a network and $c_{in}^{(i)}$ is the number of channels at layer $i$. Weights with smaller $l_p$ norms are more likely to be pruned than those with higher $l_p$ norms. In addition, the importance value is often optimized with norm-based sparsity regularization (ding2018auto), which is discussed in regularization.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Sensitivity and/or Saliency",
      "level": "2",
      "content": "Some works (lecun1989optimal,santacroce2023matters,zhao2019variational) utilize sensitivity and/or saliency to evaluate the importance of weights (or filters, neurons, etc.). For example, lecun1989optimal define weight saliency as the loss change induced by pruning that weight. lee2019snip propose a saliency criterion called the connection sensitivity criterion as the normalized magnitude of the derivatives $g_{j}$: equation s_{j}(W;D)=\\left| g_{j(W;D)\\right|}{\\sum_{k=1}^{M}\\left| g_{k}(W;D)\\right|}, equation where $s_j$ is the sensitivity of $w_j$, $W$ represents the network's weights, $g_{j}$ is the derivative of the loss $\\mathcal L(W\\odot M)$ w.r.t. the mask $m_{j}$. The higher the sensitivity, the more important the weight is. zhao2019variational reformulate the BN layer by extending the scale factor $\\gamma$ on the shift term $\\beta$, which is treated as channel saliency. They reformulate BN as follows: equation x_{out} = \\gamma \\cdot BN(x)+\\beta, equation where $\\beta=\\gamma \\cdot \\beta$. Rather than relying on the value of $\\gamma$, unimportant channels are pruned based on $\\gamma$'s distributions.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Loss Change",
      "level": "2",
      "content": "It is a widely used criterion to measure the importance of a weight (or filter, neuron, etc.) by evaluating the loss change of the network with and without it. The loss change is usually approximated in a Taylor expansion-based way, such as you2019gate,nonnenmacher2022sosp,liu2021group,ma2023llmpruner,fang2023structural,ouderaa2024llm. The first-order Taylor expansion is the most commonly used method for measuring the loss change. The loss change with a small perturbation at $W$ is defined as follows: equation \\Delta L = L(W+\\Delta W) - L(W) = \\nabla_{W} L\\ \\Delta W. equation For example, you2019gate introduce scaling factors $\\lambda$ to the BN and exploits the first-order Taylor expansion to estimate the loss change $\\Delta L$ caused by setting some scaling factors to zero as follows: equation \\Delta L(\\lambda) = \\left | \\lambda \\nabla_{\\lambda} L - R_{1}(\\lambda)\\right | \\approx \\left | \\lambda \\nabla_{\\lambda} L\\right |=\\left| \\partial \\mathcal{L}{\\partial \\lambda} \\lambda \\right|, equation where $R_{1}(\\lambda)$ is the Lagrange remainder. The importance score of the $i$-th filter is defined as equation Score(F_i) = \\sum\\limits_{(x,y)\\in D}\\left | \\partial \\mathcal{L(y,f(x;W)}{\\partial \\lambda_i} \\lambda_i\\right |, equation where $\\lambda_{i}$ is the scalar factor of the $i$-th filter. The second-order Taylor expansion of the loss function is early used in lecun1989optimal,hassibi1992second for removing unimportant weights and gradually exploited in many subsequent methods (wang2019eigendamage,kurtic2022optimal,nonnenmacher2022sosp,liu2021group,ouderaa2024llm), which includes the first-order (gradient) term, the second-order (Hessian) term, and the higher-order terms are neglected. Without loss of generality, the approximation of the loss change leads to equation L(W+\\Delta W) - L(W) = \\nabla_{W} L\\ \\Delta W + 1{2}\\Delta W^{T}H\\Delta W, equation where $H=\\nabla_{W}^2 L(W)$. For example, liu2021group apply the second-order Taylor expansion to approximate the loss change when removing a channel (setting its mask to 0): equation aligned s_i &= \\Delta L = L(M-e_i) - L(M) \\approx -e_i^{T}\\nabla_{M} L+1{2}e_i^{T}(\\nabla_{M}^{2}L)e_i \\\\ &= -e_i^{T}g + 1{2}e_i^{T}He_i=-g_i+1{2}H_{ii}, aligned equation where $e_i$ is the one-hot vector with the $i$-th entry equals one, and $g$ is the gradient of the loss function $L$ w.r.t. $M$.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Learn to Prune",
      "level": "1",
      "content": "In this section, we present some methods for learning to prune networks, including sparsity regularization (he2017channel,liu2017learning,huang2018data,li2020dhp, Voita2019analyzing) and pruning methods based on meta-learning (liu2019metapruning,li2020dhp), graph neural networks (zhang2022graph), and reinforcement-learning (he2018amc,rao2019runtime).",
      "origin_cites_number": 4
    },
    {
      "section_title": "Sparsity Regularization based Pruning",
      "level": "2",
      "content": "Sparsity regularization based pruning he2017channel,dery2024everybody learns the weights and their masks by solving the following problem: equation \\textrm{min}\\limits_{W,M} L(W,M) \\\\, equation where $L=\\ell(W,M) + \\lambda R(\\cdot)$. One common way is to introduce a scaling factor vector $\\gamma$ for weights (or channels, filters, etc.). The network weights and the scaling factors $\\gamma$ are trained jointly with sparsity regularization imposed on the latter. The magnitude of the scaling factors is treated as the important scores. Specifically, $L$ in Eq.~sparsity-regularization can be exemplified as follows: equation L = 1{N}\\sum_{i=1}^{N}\\ell(y_i,f(x_i;W, \\gamma))+\\lambda \\sum_{\\gamma_{i} \\in \\gamma}R(\\gamma_{i}). equation For example, he2017channel cast channel selection as the minimization of reconstruction error in feature maps and formulate the channel pruning problem as follows: equation aligned & \\textrm{min}\\limits_{\\beta, W} 1{2N}\\lVert y - \\sum_{i=1}^{c} \\beta_i x_{c_{i}} w_{c_{i}}^T\\rVert_F^2 + \\lambda \\lVert \\beta \\rVert_1, \\\\ & subject to \\ \\lVert \\beta\\rVert_0 \\leq c', \\forall i \\ \\lVert w_{c_{i}} \\rVert_{F} = 1. \\\\ aligned equation where $\\lVert \\cdot \\rVert_{F}$ is the Frobenius norm, $x_{c_{i}}$ is an $N \\times k_{h}k_{w}$ matrix from $i$-th channel of input $x$, $w_{c_{i}}$ is an $n \\times k_{h}k_{w}$ weight matrix from $i$-th channel of $W$, $k_h$ and $k_w$ are the kernel height and width, respectively. $N$, $c$, $c'$, and $n$ represent the number of samples, channels, retained channels, and output channels. To solve this problem, he2017channel use LASSO regression tibshirani1996regression and a greedy strategy to select the unimportant channels.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Meta-Learning based Pruning",
      "level": "2",
      "content": "Some works (liu2019metapruning,li2020dhp) adopt meta-learning to prune models. For example, liu2019metapruning train a meta network, PruningNet, to predict weights for different pruned networks. The PruningNet takes a network encoding vector $(v_1, v_2, ..., v_L)$ as input and outputs the weights $W$ of the pruned network: equation W = PruningNet(v_1,v_2,...,v_L), equation where $v_i$ is the number of the channels for the $i$-th layer. The weights and the corresponding accuracy of each pruned network are obtained by inputting the network encoding into the fully trained PruningNet. Considering the huge search space of network encoding vectors, the pruned network is found by evolutionary search under the constraints.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Graph Neural Network based Pruning",
      "level": "2",
      "content": "Any network can be viewed as a graph. zhang2022graph propose a method called GraphPruning for model compression. Specifically, GraphPruning designs a graph aggregator $G$ with weights $\\theta_{G}$, combined with the Fully Connected (FC) layers, to generate the weights $W = (w^{(1)},w^{(2)},...,w^{(L)})$ of the ``Pruned Network'' as follows: equation aligned & (n_1,n_2,...,n_{L}) = G(b_1,b_2,...,b_{L}|\\theta_{G}), \\\\ & w^{(i)} = FC_{i}(n_{i}|\\theta_{i}), aligned equation where $b_{i}\\in R^{1\\times 7}$ denotes the embedding features of the $i$-th node, $n_i$ is the $i$-th column of the output with the graph aggregator, $\\theta_{i}$ are the weights of the $i$th FC layer, and $w^{(i)}$ are the weights of the $i$-th pruned layer of the pruned network. Then, the ``Pruned Network'' is fully trained. The graph aggregator is responsible for extracting high-level features for each node, while each FC layer is used to generate reasonable weights for the ``Pruned Network''. Afterward, the best configuration of the ``Pruned Network'' under computational constraints is searched by RL methods, during which the weights of the graph aggregator and FCs are not updated.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Reinforcement Learning based Pruning",
      "level": "2",
      "content": "Rather than using RL to search for the best configurations of the pruned networks as in zhang2022graph, some AutoML pruning methods (he2018amc,rao2019runtime) adopt RL to compress models automatically. For example, he2018amc propose AutoML for Model Compression (AMC), which is based on Q-learning, a type of RL that focuses on how an agent should take actions to maximize the cumulative reward. Specifically, he2018amc design the Deep Deterministic Policy Gradient (DDPG) agent to receive an embedding state $s_{i}$ of layer $l_{i}$ from the environment and output a sparsity ratio as action $a_{i}$. Then, layer $l_{i}$ is compressed with $a_{i}$ using a specific compression method (such as a channel pruning method). After that, the agent moves to layer $l_{i+1}$ and repeats the same process until the final layer $l_{L}$. The update process is as follows: equation aligned Loss = 1{N}\\sum_{i=1}^{N}(y_{i}-Q(s_{i},a_{i}|W^{Q}))^{2}, \\\\ y_{i} = r_{i}-b+\\gamma Q(s_{i+1},\\mu \\ (s_{i+1})|W^{Q}), aligned equation where $b$ is the baseline reward, $\\gamma$ is a discount factor used to avoid over-prioritizing short-term rewards, $W^{Q}$ are the weights of the network $Q$ following Block-QNN blockqnn, and $r_{i}$ is the reward of the whole trajectory for the $i$-th sample. he2018amc observe that $Error$ is inversely-proportional to $log(FLOPs)$ or $log(\\#Param)$. Based on this observation, the reward function is defined as: equation aligned &R_{FLOPs} = -Error \\cdot log(FLOPs), \\\\ &R_{Param} = -Error \\cdot log(\\# Param). \\\\ aligned equation This reward function provides an incentive for reducing FLOPs or the number of network parameters.",
      "origin_cites_number": 6
    },
    {
      "section_title": "A Comprehensive Comparative Analysis",
      "level": "1",
      "content": "In this section, we compare some pruning methods on commonly used models, including eight pairs of contrast settings for pruning, different layer-wise densities, and various supervision levels for pruning. To avoid the influence of specific functions on pruning results, we mainly use the same functions under contrast settings (experimental details in Appendix~B). Appendix~C provides a more extensive comparison across different methods.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Unstructured vs. Structured Pruning",
      "level": "2",
      "content": "Unstructured pruning methods (frantar2023sparsegpt, han2015deep) remove weights anywhere and can achieve high prune ratios with little impact on accuracy. In contrast, structured pruning (liu2021group,nonnenmacher2022sosp,ma2023llmpruner) conducts pruning on entire filters (or channels, neurons, layers, etc.), resulting in really compressed network and accelerated inference. However, the accuracy is often lower than that of unstructured pruning under the same prune ratio, weight-level scoring, pipeline, and learning schemes. The possible reason is that unstructured pruning only focuses on the importance of individual weights, while structured pruning forces structural coupling, which demands simultaneous pruning across multiple layers and expects all removed weights to be consistently unimportant. However, achieving consistency in identifying unimportant weights under the structural coupling constraints is challenging. amersfoort2020single argue that SNIP-structured and GraSP-structured methods incur more noise than their vanilla unstructured counterparts. We compare unstructured and structured pruning methods on VGG-16 simonyan2015very and report the best results in Table~Table-unstructured-and-structured from three random runs. Additionally, we compare unstructured, semi-structured, and structured pruning methods on OPTs zhang2022opt with data sourced from ouderaa2024llm (Table~tab:unstructured-and-structured-llm). As shown in Table~Table-unstructured-and-structured and Table~tab:unstructured-and-structured-llm, at the same prune ratio, unstructured pruning generally outperforms semi-structured (if any), which performs better than structured pruning.",
      "origin_cites_number": 17
    },
    {
      "section_title": "One-shot vs. Iterative Pruning",
      "level": "2",
      "content": "One-shot pruning methods score once and then prune the network to a target prune ratio. Conversely, the iterative pruning methods alternately process the score-prune-update cycle until achieving the target prune ratio. As a result, the pruning cost in one-shot methods is usually negligible, greatly saving pruning efforts. However, these methods are not beneficial to those significant weights whose importance is not immediately apparent at the beginning morcos2019one. Therefore, one-shot pruning generally requires more carefully designed scoring criteria to match the performance of the original network. In addition, the results in tanaka2020pruning show that one-shot pruning may more easily suffer from layer collapse, resulting in a sharp accuracy drop. In contrast, iterative methods require more pruning cost but generally yield better accuracy you2019gate,li2017pruning,liu2021lottery,mehta2019sparse,huang2004learning. lin2020dynamic analyze the difference between one-shot and iterative pruning methods from the perspective of stochastic gradient. Their results show that the iterative pruning method computes a stochastic gradient at the pruned model and takes a step that best suits the compressed model. In contrast, one-shot pruning computes a stochastic gradient at the original weights and moves towards the best dense model. The work in ouderaa2024llm theoretically supports iterative pruning by arguing that the surrogate loss landscape, based on a Taylor expansion, only holds locally, making it unreliable for larger weight changes. We prune VGG-16 simonyan2015very and ResNet-32 he2016deep on CIFAR-10 krizhevsky2009learning using SynFlow tanaka2020pruning and Magnitude-based pruning, and LLaMA-7B touvron2023llama on PTB marcus1993building using SparseGPT frantar2023sparsegpt, applying one-shot and iterative pruning, respectively. Additionally, we compare the pruning results of OPT-1.3B zhang2022opt on WikiText2 merity2016pointer under different iteration settings, using experimental results from ouderaa2024llm. %(For simplicity, it will be referred to as Magnitude in the following.) As illustrated in Fig.~Fig:one-shot-iterative-pruning (a) - (c), iterative pruning generally performs better than that of the corresponding one-shot pruning and Fig.~Fig:one-shot-iterative-pruning (d) indicates that more iterations tend to yield better performance.",
      "origin_cites_number": 15
    },
    {
      "section_title": "Data-free vs. Data-driven Pruning",
      "level": "2",
      "content": "Pruning methods can be categorized into two types based on whether data is used during the pruning phase: data-free and data-driven. Data is generally believed to be essential for finding good subnetworks. Most existing pruning works (liu2021group,nonnenmacher2022sosp,lee2019snip,ma2023llmpruner,shi2023upop) belong to data-driven methods, and only a few methods (tanaka2020pruning,su2020sanity,li2019exploiting) are data-free. We apply PBT methods, including three data-free (Random, Magnitude, and SynFlow tanaka2020pruning) and two data-driven (SNIP lee2019snip and GraSP wang2020picking) methods, to prune VGG-16 and ResNet-32/50 on CIFAR-10/100 and ImageNet russakovsky2015imagenet, respectively. Table~Tab:data-free-data-driven-cnn shows that SynFlow and SNIP are similarly effective, with SynFlow significantly outperforming GraSP, indicating that the effectiveness of PBT methods is not strictly dependent on data usage. In addition, we utilize PAT methods, selecting Random and L1/L2-norm as data-free methods, and data-driven approaches such as Wanda sun2024simple, LLM-Pruner ma2023llmpruner, and LoRAPruner zhang2023loraprune to prune LLaMA-7B. In contrast, Table~Tab:data-free-data-driven-llm consistently demonstrates that data-driven PAT methods typically outperform data-free PAT methods.",
      "origin_cites_number": 21
    },
    {
      "section_title": "Pruning on Initialized vs. Pre-trained Weights",
      "level": "2",
      "content": "frankle2021pruning find the subnetworks in CNNs obtained by pruning on randomly initialized weights (such as SNIP lee2019snip, GraSP wang2020picking, SynFlow tanaka2020pruning) are robust to ablation treatments (i.e., randomly shuffling the mask positions within each layer or reinitializing weights while keeping masks unchanged). To find the reason behind such immunity, singh2021why use the Wasserstein distance to measure distributional similarity and find that the remaining weights' distribution changes minimally with these ablations, which helps maintain similar performances. In contrast, su2020sanity find the subnetworks in CNNs achieved by pruning on pre-trained weights, such as LTH frankle2019lottery, are sensitive to these ablations. qiu2020train claim that training weights can be decoupled into two dimensions: the locations of weights and their exact values, with the locations of weights holding most of the information encoded by the training. wolfe2022how theoretically analyze the impact of pre-training on the performance of a pruned subnetwork in CNNs obtained by greedy forward selection and find that the number of pre-training iterations increases logarithmically with the dataset size. Unlike CNNs, Transformers typically need appropriate self-supervised pre-training to perform well amos2024never. Therefore, pruning of Transformers generally targets pre-trained models. We conduct experiments to explore whether pre-trained weights facilitate achieving better subnetworks. Fig.~Fig:initialized-pretrained-weights (a) indicates that for the PBT method, GraSP wang2020picking, pruning with pre-trained weights does not necessarily improve Top-1 accuracy. However, Fig.~Fig:initialized-pretrained-weights (b) shows that for the PAT method, WDPruning yu2022width, pre-training may be crucial for obtaining subnetworks with better performance.",
      "origin_cites_number": 12
    },
    {
      "section_title": "Global vs. Local Pruning",
      "level": "2",
      "content": "The difference between global (chin2020towards,he2018amc,tiwari2021ChipNet,liu2021group,ouderaa2024llm) and local (wang2021neural,wang2023trainability,ding2018auto,luo2017thinet,ma2023llmpruner) pruning lies in whether structures are removed from a subset or all available structures of a network. A major limitation of local pruning is that setting a pre-defined prune ratio for each layer can be complex and lead to sub-optimal sparsity. To simplify, local pruning often uses a consistent prune ratio across layers. In contrast, global pruning automatically generates a varying prune ratio for each layer. However, global pruning poses great challenges, particularly for LLMs, due to significant variations in layer magnitudes. For instance, some outlier features may have magnitudes up to 20 times larger than others kovaleva2021bert, leading to incomparability issues. ma2023llmpruner notes a marginal advantage of local pruning compared to global pruning in LLMs. Although previous pruning methods for LLMs (ma2023llmpruner,zhang2023loraprune,frantar2023sparsegpt,sun2024simple) are primarily local, some global methods (ouderaa2024llm,bai2024sparsellm,an2024fluctuationbased) start to emerge. For example, bai2024sparsellm argue that local pruning excessively restricts the alignment of input and output in all the intermediate layers, leading to a sub-optimal solution, and propose a global pruning method called SparseLLM to address the drawbacks.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Training from Scratch vs. Fine-tuning",
      "level": "2",
      "content": "After pruning, many pruning methods require training the subnetwork for several epochs to regain performance. le2021network argue that retraining is essential to recover loss accuracy in pruning. Generally, retraining can be divided into two types: training from scratch or fine-tuning. There has been debate over whether fine-tuning is more effective than training from scratch in recovering accuracy. On the one hand, liu2019rethinking find that for ResNet, VGG, and other standard structures on ImageNet, training the subnetworks with new random initialization can achieve better performance than fine-tuning them. On the other hand, li2017pruning observe that training a subnetwork from scratch performs worse than fine-tuning it. liu2021lottery investigate pruning ResNet20 on CIFAR-10 with ADMM-based zhang2018systematic one-shot pruning method and find that pruning \\& fine-tuning outperforms LTH (pruning \\& training from scratch) over various prune ratios. Additionally, the results in gao2021network,ye2020good show that fine-tuning is necessary for better performance on sparse mobile networks than training from scratch. The results in fang2023structural reveal that training from scratch requires more steps to achieve convergence, suggesting that starting pruned models from scratch may not be the most cost-effective strategy, given its training cost is comparable to that of pre-trained models. In recent years, some compromise methods (such as weight rewinding frankle2020linear) have been proposed. The results in frankle2020linear,renda2020comparing show that weight rewinding can achieve higher accuracy than fine-tuning. We prune ResNet-152 and DeiT-Tiny touvron2021training on CIFAR-100 and ImageNet with pre-trained weights, respectively. Then we fine-tune the pruned networks or train them from scratch. Fig.~Fig:Train-from-scratch-finetune indicates that fine-tuning generally outperforms training from scratch. Notably, on ImageNet, fine-tuning achieves significantly higher accuracy than training from scratch.",
      "origin_cites_number": 10
    },
    {
      "section_title": "Original Task vs. Transfer Pruning",
      "level": "2",
      "content": "In recent literature, pruning is combined with transfer learning pan2009survey that can dramatically improve accuracy and speed up convergence. For ease of distinction, in this survey, original task pruning denotes the pruning pipeline that directly performs on the target task. In contrast, transfer pruning is performed on the source task and then transfers the subnetwork to the target task. Specifically, transfer pruning is divided into two types: dataset transfer and architecture transfer. The former prunes networks on the source dataset and transfers the subnetwork to the target dataset, while the latter prunes on one architecture and transfers the subnetwork to another. Some works (tiwari2021ChipNet,chen2020lottery,chen2021lottery,mehta2019sparse) study the transferability of sparsity masks on datasets. morcos2019one observe that for image classification, winning tickets generated on larger datasets (such as with larger training set size and/or more classes) consistently transfer better than those generated with smaller datasets. For example, winning tickets generated on ImageNet and Places365 demonstrate better performance across other smaller target datasets such as CIFAR-10 and CIFAR-100. iofinova2022how present a pioneering study of the transfer performance of subnetworks and find that pruning methods with similar Top-1 accuracy on ImageNet russakovsky2015imagenet can have surprisingly different Top-1 accuracy when used for transfer learning. For pruning on LLMs, performing pruning directly on the target task typically yields better performance than zero-shot pruning ouderaa2024llm. For architecture transfer pruning, Elastic Ticket Transformations (ETTs) chen2021elastic transforms winning tickets in one kind of network (such as ResNet he2016deep) to another deeper or shallower one from the same model family.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Static vs. Dynamic Pruning",
      "level": "2",
      "content": "Static pruning dong2017more uses static pruning criteria and removes components. In contrast, dynamic pruning (tang2021manifold,gao2019dynamic,hua2019channel,li2021dynamic) exploits input-specific pruning criteria, preserves the entire network structures, and accelerates the networks by dynamically skipping unimportant components. However, dynamic pruning generally does not perform run-time fine-tuning or retraining. The difference between static and dynamic pruning is mainly reflected in the pruning criteria and the pruned model. The advantages and disadvantages of static and dynamic pruning are shown in Table~Table6-comparison-static-dynamic.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Layer-wise Weight Density Analysis",
      "level": "2",
      "content": "Some works (ma2021sanity,sanh2020movement,liu2021group,zhang2022graph) study the distributions of layer-wise weight density in a subnetwork, showing that different layers can have very different weight densities. The differences arise from the joint action of networks' structural characteristics and pruning methods. zhang2022are empirically divide the layers into either ``ambient'' or ``critical''. Ambient layers are not sensitive to weight changes, while critical layers are. Thus, ambient layers should be heavily pruned, resulting in lower weight densities. Pruning methods can also result in different weight densities. gong2024fast categorize the sparsity allocation formed by pruning methods into uniform and non-uniform. Uniform sparsity methods (frantar2023sparsegpt,sun2024simple,ma2023llmpruner) allocate the same prune ratio for all the layers, whereas non-uniform methods (yin2024outlier,an2024fluctuationbased,ouderaa2024llm,bai2024sparsellm) assign varying sparsity rates to different layers. For example, in CNNs, some pruning methods tend to assign more weights to earlier layers than to later ones. ma2021sanity investigate the layer-wise keep-ratios of subnetworks obtained by GraSP wang2020picking, SNIP lee2019snip, and LTH frankle2019lottery on VGG and ResNet, observing a common trend of declining layer-wise keep-ratios, except for some special layers (such as the downsampling layers in ResNet). In contrast, liu2021group find their pruned networks keep higher percentages of channels in the deeper layers than those in the lower layers for image classification. For pruning Transformers, the results in sanh2020movement show that global magnitude pruning tends to prune Transformer layers uniformly, while global first-order methods heavily prune the deeper layers. yang2023global discover a unique less-more-less distribution among stacked ViT blocks. LLM surgeon ouderaa2024llm prunes relatively more in the first layers and less in the middle layers. Some works (men2024shortgpt,yuan2024lift,kim2024shortened,yang2024laco) reveal that some layers are not essential and can be entirely removed or merged.",
      "origin_cites_number": 14
    },
    {
      "section_title": "Pruning with Different Levels of Supervision",
      "level": "2",
      "content": "In descending order of supervision level during neural network pruning, pruning can be divided into supervised, semi-supervised, self-supervised, and unsupervised pruning kalyan2018unsupervised. Self-supervised learning can be divided into two classes: generative and contrastive learning liu2021self. Similar to supervised learning, supervised pruning works on fully labeled datasets, and most current pruning methods fall into this category. However, supervised pruning suffers from similar bottlenecks as supervised learning, such as the expensive manual labeling. As a promising alternative, semi-supervised, self-supervised, and unsupervised pruning have drawn massive attention. The differences between supervised, semi-supervised, unsupervised, self-supervised learning refer to \\cite{liu2021self.} For example, caron2020pruning observe different results in self-supervision pruning compared to supervision pruning, where winning tickets initialization only introduces a slight performance improvement compared to random re-initialization. contrastivepruning claim that unsupervised pruning usually fails to preserve the accuracy of the original model. Notably, label supervision for network pruning and training can be independent. For example, chen2021lottery use supervised pruning method IMP (Iterative Magnitude Pruning) to explore the subnetworks of self-supervised pre-trained models (simCLR chen2020simple and MoCo chen2020improved) on ImageNet. Similarly, lai2021parp exploit the supervised pruning method IMP to prune self-supervised speech recognition models.",
      "origin_cites_number": 9
    },
    {
      "section_title": "Fusion of Pruning and other Compression Techniques",
      "level": "1",
      "content": "In this section, we review the fusion of neural network pruning with other network compression techniques, such as quantization ma2023llmpruner, tensor decomposition li2021towards, knowledge distillation hinton2015distilling, and network architecture search li2021npas. On the one hand, fusion provides more choices for network compression. On the other hand, combined compression techniques can complement each other to further improve the performance and prune ratio. Pruning \\& Quantization: Quantization tung2018clipq is a compression technique that reduces the number of bits used to represent the network weights and/or activations, significantly reducing the model size and memory footprint with only a minor performance drop. To obtain more compact models and achieve model acceleration, han2015deep pioneer pruning the redundant network connections and quantizing the weights. CLIP-Q tung2018clipq jointly performs pruning and quantization during the fine-tuning stage. MPTs diffenderfer2021multi integrates pruning and quantizing randomly weighted full-precision neural networks to obtain binary weights and/or activations. EB you2020drawing applies 8-bit low-precision training to the stage of searching EB tickets. Pruning \\& Tensor Decomposition: Tensor decomposition lin2018holistic decomposes convolutions into a sequence of tensors with fewer parameters. In contrast to pruning, it explores the original weights' low-rank structure, while keeping the dimension of the convolutional output unchanged. CC li2021towards combines channel pruning and tensor decomposition to compress CNN models by simultaneously learning model sparsity and low rankness. Hinge li2020hinge introduces group sparsity to fuse filter pruning and decomposition under the same formulation. li2023losparse propose LoSparse to prune Transformers by combining low-rank approximations and pruning. Pruning \\& NAS: Neural Architecture Search (NAS) provides a mechanism to automatically discover the best architecture for the problem of interest, offering a new approach for pruning to find suitable network depth and width. For example, for CNNs, NPAS li2021npas performs a compiler-aware joint network pruning and NAS, determining the filter type (different kernel sizes), the pruning scheme, and the rate for each layer. TAS 2019dongnetwork exploits NAS to search for the depth and width of a network to obtain pruned networks and uses knowledge distillation to train these pruned networks. klein2023structural explore structured pruning of fine-tuned Transformers via NAS. Pruning \\& Knowledge Distillation: Knowledge Distillation (KD) hinton2015distilling guides the student to effectively inherit knowledge from the teacher and mimic the teacher's output. Some works (liu2021content,park2022prune) exploit pruning before KD to boost KD's quality. For example, liu2021content prune unimportant channels to the contents of interest and focus the distillation on the interest regions. park2022prune prune the teacher network first to make it more transferable and then distill it to the student. Some works (hua2019channel,chen2022knowledge, chen2021long,zhang2022advancing) use KD to train the pruned networks. The results in chen2022knowledge show that the pruned network recovered by KD performs better than it regained by fine-tuning. zou2022dreaming propose a data-free deraining model compression method that distills the pruned model to fit the pre-trained model. wang2023large introduce a multi-stage compression strategy, AntGMM, to compress large multimodal models by utilizing structured pruning and knowledge distillation. Pruning \\& Multi-compression Techniques: Some works (mao2020ladabert,yao2021DetNAS,wang2020gan) explore the fusion of pruning with more than one compression technique. For example, GS wang2020gan combines pruning, quantization, and KD for GANs compression. Joint-DetNAS yao2021DetNAS integrates pruning, NAS, and KD for image translation. LadaBERT mao2020ladabert merges pruning, matrix factorization, and KD to compress BERTs michel2019sixteen for natural language understanding.",
      "origin_cites_number": 29
    },
    {
      "section_title": "Suggestions and Future Directions",
      "level": "1",
      "content": "In this section, we discuss how to choose different pruning methods and provide promising directions for future work.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Recommendations on pruning method selection",
      "level": "2",
      "content": "After years of research and exploration, there are many off-the-shelf pruning methods. However, no golden standard exists to determine which one is the best. Different suitable pruning methods exist to compact deep neural networks for specific application requirements and hardware/software resources. Here are some general recommendations for choosing an appropriate pruning method. (1) If you do not have special hardware (e.g., FPGAs or ASICs) or software (such as sparsity convolutional libraries) but need actual neural network acceleration and compression, structured pruning is more suitable than unstructured pruning because most software frameworks and hardware cannot accelerate sparse matrices' computation. (2) If you have sufficient computational resources during the pruning stage, consider using iterative PAT methods that can typically minimize the impact on performance under the same prune ratio. On the other hand, if you have limited computational resources during both the pruning and inference stages, consider using one-shot PBT or one-shot post-training pruning methods, particularly for LLMs. (3) If you have enough labeled examples on the target task, consider using supervised pruning methods. However, if only a few examples on the target task are labeled, semi-supervised or transfer pruning methods may be considered. If the examples on the target task are not labeled, consider self-supervised, unsupervised, or transfer pruning methods. (4) If you have a sufficient memory footprint during pruning for NLP tasks, consider heavily compressing large models rather than lightly compressing smaller models to meet the same budgets. Some results in li2020train show that for NLP tasks finding pruned models derived from larger dense networks outperform small dense networks of comparable size to pruned models. (5) If you have enough memory to store the dense neural network during the inference stage and wish to provide run-time flexible computational cost allocation for different inputs, dynamic pruning methods can be considered where inputs with smaller shapes can allocate less computational cost to perform the task and if the input shape is bigger more computational cost can be allocated. (6) If you need to trim down neural networks in multiple dimensions, you can comprehensively consider layerwise pruning (decreasing the model's depth), channel pruning (reducing the model's width), and image resolution pruning (scaling down the model's input resolution) or token pruning (selectively removing tokens from text data). In addition, pruning can be integrated with quantization to further reduce the memory footprint and the neural networks' size. (7) If you want to achieve a better tradeoff between speed and accuracy, the following settings may help: use a pre-trained model; set an appropriate learning rate (if any) for both the pruning and retraining stages; fine-tune the pruned models for several epochs; integrate pruning with knowledge distillation, NAS, or other compression methods to achieve complementarity; and adversarial training may have some help gan2022playing. (8) If you need to train a subnetwork to recover performance, ma2021sanity show that subnetworks with residual connections achieve higher accuracy using a relatively small learning rate. In contrast, subnetworks without residual connections benefit from a larger learning rate. (9) To benefit from large-scale pre-training, adding parameters is more critical than minimizing FLOPs han2024parameternet. By incorporating techniques such as dynamic convolution chen2020dynamic, models with low FLOPs can increase their capacity without a substantial rise in computational cost, enhancing their performance during extensive pre-training.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Future Directions",
      "level": "2",
      "content": "We discuss four promising directions for the further development of neural network pruning, namely, (1) theories, (2) techniques, (3) applications, and (4) evaluation. Theories: Despite the existing works, several fundamental questions about pruning still need to be answered. For example, prior works demonstrate that network layers contain irreplaceable information as long as redundant ones. Does a theoretical upper bound of the prune ratio exist for a given network that still maintains the performance of its dense equivalent? In other words, how heavily can a network be pruned theoretically without accuracy loss? It is a tricky question due to the intricate relationships between network layers. Besides, is pruning explainable? A common belief is that deep neural networks are hard to interpret. As such, making pruning explainable is an uphill task. However, the interpretability of pruning is vital for understanding the factors behind pruning (e.g., model structure and weights) and exploring more effective pruning methods. Techniques: To obtain better algorithm designs whose architectures are learned in an economical, efficient, and effective manner, it is a trend to extend Automated Machine Learning (AutoML) methods and NAS to pruning. Furthermore, pruning is also beginning to combine with various learning contexts, such as lifelong learning chen2021long, continual learning yan2022learning, contrast learning corti2022studying, and federated learning jiang2022model, etc. In addition, the rising energy consumption of networks requires more attention to energy-aware pruning. However, preliminary efforts mainly focus on reducing computation and memory costs, which may not necessarily reduce the most energy consumption. Moreover, incorporating pruning into hardware to help deploy pruned networks is also an emerging trend. For example, sui2023hardware propose a hardware-friendly pruning method and deploy the pruned models on an FPGA platform. Applications: Pruning has begun to draw attention to more complex applications such as visual question answering, natural language understanding, speech recognition, and content generation than image classification. Foundation models such as GPT-4 openai2023gpt4 might be a possible way to Artificial General Intelligence (AGI). However, its enormous size hinders its application in many downstream tasks. Fig.~Fig:pruning-taxonomy highlights content related to large model pruning. In the future, more pruning methods will enable colossal foundation models to benefit from pruning research, making them more compact and efficient frantar2023sparsegpt. Evaluation: With the emergence of many pruning methods, standardized benchmarks, and metrics are required to provide a fair evaluation. Different pruning techniques, network architectures, tasks, and experimental settings lead to incomparable results and make it hard to compare pruning methods fairly wang2023why. ShrinkBench blalock2020what takes the first step and provides a benchmark of pruning methods for image classification. As pruning is applied to applications beyond image classification, standardized benchmarks and metrics for other applications are needed.",
      "origin_cites_number": 9
    },
    {
      "section_title": "Conclusion",
      "level": "1",
      "content": "As an essential compression technique, deep neural network pruning has attracted increasing research attention with the recent emergence of various pruning methods and applications. This survey conducts a comprehensive review on the following four scopes: 1) universal/specific speedup, with a systematic review of unstructured, structured, and semi-structured pruning; 2) when to prune, including pruning before/during/after training for static pruning and run-time pruning; 3) how to prune, including pruning by criteria and by learning; 4) fusion of pruning with other compression techniques, such as KD and NAS. A comprehensive comparative analysis, including eight pairs of contrast settings for pruning, layer-wise weight density, and different supervision levels, can help researchers to efficiently and effectively grasp the characteristics of different pruning methods. In addition, recommendations on pruning method selection and future research directions are highlighted and discussed. To facilitate future research, real-world miscellaneous applications and commonly used resources of datasets, networks, and evaluation metrics in different applications are summarized in Appendix~D. To help researchers and practitioners keep up with the development of pruning technologies, we continue updating the representative research efforts and open-source codes for pruning at https://github.com/hrcheng1066/awesome-pruning. \\appendices",
      "origin_cites_number": 0
    },
    {
      "section_title": "Terms and Notations",
      "level": "1",
      "content": "This section presents the commonly used terms in pruning literature. It is worth mentioning that some terms (e.g., compression ratio) have different definitions in prior works. In addition, for better readability, we list the notations used in the main text in Table~Table-notations-this-paper. itemize \\textbf{Prune Ratio}: Prune ratio liu2019rethinking denotes the percentage of weights (or filters, neurons, etc.) that are removed from the dense network and is the complement of Keep Ratio ning2020dsa. In general, it can be determined in two ways: pre-defined or learning-decided. \\textbf{Compression Ratio}: Compression ratio in tanaka2020pruning,renda2020comparing is defined as the ratio of the original number of weights to the preserved number of weights, but in qian2021probabilistic it is defined as the ratio of the preserved number of weights to the original number of weights. For example, if 10\\% of the weights are preserved, the compression ratio in renda2020comparing is 10, but it is 10\\% in qian2021probabilistic. \\textbf{Sparsity Ratio}: Sparsity ratio denotes the portion of zero weights (or channels, filters, neurons, etc.) in networks after pruning chen2021elastic,he2018amc. It is equivalent to compression ratio in qian2021probabilistic. \\textbf{Speedup Ratio}: Speedup ratio is defined as the ratio of the pruned number of FLOPs in dong2017more, or MACs in fang2023depgraph to the original number of FLOPs or MACs, respectively. In wang2018exploring, the speedup ratio is calculated by dividing the pruned number of filters in one layer by the original number of filters in that layer. \\textbf{One-shot Pruning}: One-shot pruning, also called single-shot pruning in lee2019snip, scores only once and then prunes the network to the target prune ratio frankle2019lottery,frankle2021pruning. \\textbf{Iterative Pruning}: Iterative pruning han2015learning, also called greedy pruning or oracle pruning in he2020learning, repeatedly performs the score-prune-retrain circle for multiple rounds, and each round is one iteration. \\textbf{Local Pruning}: Local pruning prunes a network by subdividing all weights (or filter, channels, etc.,) into subsets (e.g., layers) and then removing a percentage of each subset nonnenmacher2022sosp. \\textbf{Global Pruning}: In contrast to local pruning, global pruning removes structures from all available structures of a network until a target prune ratio is reached nonnenmacher2022sosp. \\textbf{Dynamic Pruning}: Dynamic pruning depends on specific inputs tang2021manifold, wherein different subnetworks are generated for each input sample. \\textbf{Static Pruning}: In contrast to dynamic pruning, the pruned model is shared by different samples for static pruning tang2021manifold. In other words, the model capacities are fixed for different inputs. \\textbf{Lottery Ticket Hypothesis}: Lottery Ticket Hypothesis (LTH) frankle2019lottery suggests that a randomly-initialized dense network $f(x;W_{0})$ contains a sparse subnetwork $f(x;W_{0}\\odot M)$ which can be trainable with the original weights to achieve competitive performance compared to the original networks. \\textbf{Winning Tickets}: For a randomly initialized network $f(x;W_{0})$, a winning ticket $f(x;W_{0}\\odot M)$ is its subnetwork that once be trained for $T$ epochs (i.e., $f(x;W_{t}\\odot M$) will match the performance of the trained network $f(x;W_{t})$ under a non-trivial prune ratio frankle2019lottery. \\textbf{Layer Collapse}: Layer collapse is a phenomenon mentioned in tanaka2020pruning, which occurs when all weights in a layer in a network are removed, rendering the network untrainable. hayou2021robust provide a formal definition (i.e., ill-conditioned NN) to this problem. \\textbf{Weight Rewinding}: Weight rewinding frankle2020linear rewinds the weights of the subnetwork to the values in an earlier epoch in training $W_{t}$, where $t<<T$. \\textbf{Learning Rate Rewinding}: Learning rate rewinding, proposed in renda2020comparing, trains the remaining weights from the final values using the learning rate schedule for a specified number of epochs. \\textbf{FLOPs}: Float Point Operations (FLOPs) is a commonly used metric (liu2021group,dong2017more,he2018soft,he2019filter,liu2017learning,zhao2019variational,tiwari2021ChipNet) to evaluate acceleration of the pruned models theoretically. Some works he2018soft,dong2017more introduce the estimation methods to compute FLOPs of neural networks. \\textbf{MACs}: In addition to FLOPs, Multiply-Accumulate Operations (MACs) is another popular proxy for evaluating the computational consumption of a network. nonnenmacher2022sosp introduce a MACs estimation method. \\textbf{Fine-tuning}: In the context of pruning, fine-tuning continues to train the preserved weights using the final weight values after pruning renda2020comparing,blalock2020what. \\textbf{Training from Scratch}: Training from scratch is a particular case of weight rewinding, where the weights of the subnetwork are rewinded to their original values $W_{0}$. itemize",
      "origin_cites_number": 29
    },
    {
      "section_title": "Experimental settings",
      "level": "1",
      "content": "We use CIFAR-10/100 krizhevsky2009learning or ImageNet ILSVRC-2012 russakovsky2015imagenet to evaluate Top-1 accuracy of the pruning methods on VGG-16 simonyan2015very, ResNet-32/152 he2016deep, or DeiT-Tiny touvron2021training for image classification. CIFAR-10 and CIFAR-100 datasets contain 50K training and 10K test images for 10 and 100 classes, respectively. ImageNet includes over 1.28 million training and 50K validation images for 1000 classes. For VGG-16 and ResNet-32 on CIFAR-10/100, we conduct the experiments on one NVIDIA A100 GPU (40 GB) using SGD with 0.9 momentum sutskever2013on, a weight decay of $10^{-4}$, and train the pruned networks for 160 epochs with a batch size of 128. The initial learning rate is 0.1, reduced at epochs 60 and 120, as in tanaka2020pruning. For DeiT-Tiny on ImageNet, we use four NVIDIA A100 GPUs (40 GB) with AdamW (0.9 momentum) and a cosine learning rate decay strategy, training the pruned networks for 100 epochs with a batch size 256 of per GPU and an initial learning rate of $5\\times 10^{-4}$, as in yu2022width. The code for SNIP lee2019snip and GraSP wang2020picking can be found at https://github.com/JingtongSu/sanity-checking-pruning. The code for SynFlow tanaka2020pruning and WDPruning yu2022width is available at https://github.com/ganguli-lab/Synaptic-Flow and https://github.com/andyrull/width-and-Depth-pruning-for-Vision-Transformer, respectively. In addition, we conduct experiments for LLaMA-7B touvron2023llama on one NVIDIA A100 GPU (40 GB) to assess the zero-shot ability of pruned LLMs on WikiText2~merity2016pointer and PTB~marcus1993building for language generation using perplexity (PPL)https://huggingface.co/spaces/evaluate-metric/perplexity. Besides, we follow LLaMA to implement zero-shot task classification and multiple-choice on four common sense reasoning datasets: BoolQ~clark2019boolq, PIQA~bisk2020piqa, HellaSwag~zellers2019hellaswag, and WinoGrande~sakaguchi2021winogrande. The code for LLM-Pruner ma2023llmpruner, SparseGPT frantar2023sparsegpt, and Wanda sun2024simple is available at https://github.com/horseee/LLM-Pruner, https://github.com/IST-DASLab/sparsegpt, and https://github.com/locuslab/wanda, respectively. For the experiments in Section 7.2 of the main text, we choose a blocksize of 128 for SparseGPT's iterative pruning. In Section 7.4, we use pre-trained DeiT-Tiny on ImageNet as the initialized weights and then retrain DeiT-Tiny on CIFAR-10 to obtain the pre-trained weights. Both a randomly initialized ResNet-152 and a ResNet-152 pre-trained on CIFAR-100 are pruned using GraSP wang2020picking. In Section 7.5, ResNet-152 pre-trained on CIFAR-100 is pruned using GraSP, while DeiT-Tiny pre-trained on ImageNet is pruned using WDPruning yu2022width. The pruned networks are subsequently fine-tuned or trained from scratch for the same number of epochs.",
      "origin_cites_number": 24
    },
    {
      "section_title": "More Comparison Results",
      "level": "1",
      "content": "In this Section, we provide additional comparison results of pruning methods under contrast settings, such as unstructured vs. structured, to expand on the related content discussed in Section 7 in the main text. For instance, Table~tab:summary-pruning-cnns-classification - Table~tab:summary-pruning-llmsThe results in Table~\\ref{tab:summary-pruning-cnns-classification - Table~tab:summary-pruning-llms focus on reflecting the pruning outcomes under a pair of contrast settings, rather than comparing the specific pruning methods themselves.} offer a comprehensive comparison of different pruning methods applied to CNNs and Transformer-based models of varying sizes (small, medium, or large).",
      "origin_cites_number": 0
    },
    {
      "section_title": "\\small More Results for Unstructured vs. Structured Pruning",
      "level": "2",
      "content": "Table~tab:unstructured-and-structured-llm-more compares the results of applying the same scoring method (LLM-surgeon~ouderaa2024llm) to both unstructured and structured pruning across various models and five prune ratios. The results, sourced from ouderaa2024llm, indicate that unstructured pruning consistently outperforms structured pruning at the same prune ratio. Additionally, Table~tab:summary-pruning-cnns-classification - Table~tab:summary-pruning-llms show that unstructured pruning typically achieves higher prune ratios, such as over 70\\% or 80\\%, while structured pruning generally remains below 50\\%. In terms of performance, unstructured pruning generally results in better outcomes at similar prune ratios. For example, under similar settings, the unstructured pruning method, Jackpot zhang2023lottery in Table~tab:summary-pruning-cnns-classification, results in a Top-1 accuracy loss of 0.44 on ImageNet at an 80\\% prune ratio, whereas the structured method, SCOP tang2020scop, incurs a 0.89 loss at a 51.80\\% prune ratio. The advantages and disadvantages of unstructured and structured pruning are illustrated in Table~Tab:unstruc-struct-comparison.",
      "origin_cites_number": 6
    },
    {
      "section_title": "More Results for One-shot vs. Iterative Pruning",
      "level": "2",
      "content": "In Table~tab:summary-pruning-cnns-classification, pruning 90\\% on ResNet-50 with ImageNet by using SNIP lee2019snip results in a Top-1 accuracy loss of 14.10, and pruning 95\\% leads to a loss of 31.30. In contrast, iterative-SNIP jorge2021progressive shows a lower Top-1 accuracy loss of 11.90 at 90\\% pruning and 30.90 at 95\\% pruning. Similarly, iterative pruning using LoRAPruner zhang2023loraprune in Table~tab:summary-pruning-llms demonstrates superior performance compared to one-shot pruning with LLM-Pruner ma2023llmpruner, under a similar setting with equivalent prune ratios on LLaMA-7B. However, the influence of specific functions on performance may overshadow the advantages of iterative pruning over one-shot pruning. For instance, one-shot pruning SAViT zheng2022savit in Table~tab:summary-pruning-vits shows a Top-1 accuracy drop of 1.48 when 30.77\\% of FLOPs on DeiT-Tiny are pruned, outperforming iterative pruning SPViT he2024pruning under similar settings, which exhibits a Top-1 accuracy drop of 1.50 with only 23.08\\% of FLOPs pruned.",
      "origin_cites_number": 6
    },
    {
      "section_title": "More Results for Data-free vs. Data-driven Pruning",
      "level": "2",
      "content": "Consistent with the main text, the effectiveness of pruning in PBT methods is not strictly dependent on data usage. For example, when pruning ResNet-50 on ImageNet, data-free pruning NTK-SAP wang2023ntksap, as shown in Table~tab:summary-pruning-cnns-classification, outperforms data-driven pruning methods SNIP lee2019snip and Grasp wang2020picking, with a Top-1 accuracy loss of 15.41 at a 95.60\\% prune ratio, compared to losses of 35.51 and 16.47 for SNIP and Grasp, respectively. However, for PAT methods, data usage appears crucial for maintaining performance in pruned models. For instance, under similar settings, data-free pruning DFPC narshana2023dfpc shows a Top-1 accuracy drop of 1.20 when 49.49\\% of FLOPs are pruned, whereas data-driven methods GFP liu2021group and PGMPF cai2022prior show smaller drops of 0.37 at a higher 50.11\\% of FLOPs removed and 0.90 at a higher 53.50\\% of FLOPs pruned, respectively. Most pruning methods for Transformer-based models are data-driven, typically requiring target or calibration data during the pruning process. For instance, the pruning methods for LLMs in Table~tab:summary-pruning-llms are all data-driven methods.",
      "origin_cites_number": 6
    },
    {
      "section_title": "\\scalebox{0.80",
      "level": "2",
      "content": "{More Results for Pruning on Initialized vs. Pre-trained Weights}} We prune a randomly initialized VGG-16 and a VGG-16 pre-trained on CIFAR-10 using SynFlow to obtain the pruning results on initialized and pre-trained weights, respectively. A similar approach is applied to ResNet-32 as well. The results in Fig.~Fig:appendix-initialized-pretrained-weights show that for PBT methods, SynFlow tanaka2020pruning and SNIP lee2019snip, pruning on the pre-trained weights does not guarantee improved Top-1 accuracy. cai2022prior compare the results of pruning ResNet-34 on ImageNet from scratch and with pre-trained weights, and the results indicate that pre-trained weights are generally crucial for non-PBT methods to find effective subnetworks. Table~tab:summary-pruning-cnns-classification - Table~tab:summary-pruning-llms show that most pruning methods, especially those for Transformer-based models, are based on pre-trained rather than randomly initialized weights.",
      "origin_cites_number": 3
    },
    {
      "section_title": "More Results for Global vs. Local Pruning",
      "level": "2",
      "content": "Tables~tab:summary-pruning-cnns-classification - Table~tab:summary-pruning-llms show that more pruning methods use global than local pruning. At an equivalent pruning ratio, global pruning methods, such as Bonsai dery2024everybody in Table~tab:summary-pruning-llms, can outperform local pruning methods like LLM-pruner ma2023llmpruner and LoRAPruner zhang2023loraprune. For instance, with a 50\\% parameter pruning ratio for LLaMA-7B, Bonsai achieves a result of 10.92 on WikiText2 and 67.22 and 61.64 on BoolQ and WinoGrande, respectively, which are better by a large margin than the results of LLM-pruner (16.41, 60.28, 53.43) and LoRAPruner (11.60, 61.88, 55.01). However, the results can still vary significantly with similar settings for different global pruning methods. For example, as shown in Table 4, when CP he2017channel has a FLOPs pruning ratio of 50.11\\%, the Top-1 accuracy loss of the pruned ResNet-50 on ImageNet is 1.07, whereas GFP liu2021group only has a loss of 0.37. Even with global pruning, achieving a globally optimal result is challenging.",
      "origin_cites_number": 5
    },
    {
      "section_title": "\\scalebox{0.90",
      "level": "2",
      "content": "{More Results for Training from Scratch vs. Fine-tuning}} Fine-tuning a pruned network or training it from scratch are two methods for restoring the performance of pruned models. Among the pruning methods listed in Table~tab:summary-pruning-cnns-classification - Table~tab:summary-pruning-llms, the majority opt for the former, such as ThiNet luo2017thinet and GFP liu2021group in Table~tab:summary-pruning-cnns-classification, and SCOP tang2020scop and PLATON zhang2022platon in Table~tab:summary-pruning-vits. A few methods, like DMCP guoO2020dmcp in Table~tab:summary-pruning-cnns-classification, choose the latter. CP he2017channel in Table~tab:summary-pruning-cnns-classification conducts comparative experiments between the two methods. Results indicate that, under the same pruning ratio (50\\%), fine-tuning incurs a loss of 1.40 in Top-1 accuracy for ResNet-50 on ImageNet, while training from scratch results in a loss of 4.00. table*[t] \\centering A summary of representative pruning methods for CNNs in image classification. ``U/S'' denotes unstructured and structured pruning, respectively. â€œRatioâ€ and ``FLOPs'' refer to reduced percentages of parameters and FLOPs, respectively. ``Original'' and ``Pruned'' represent Top-1 accuracy of the original and pruned networks, respectively. Entries marked with an asterisk (*) denote Top-5 accuracy. ``$\\dagger$'' denotes MACs. If a second citation is present, it indicates the reported results. ``-'' means the corresponding result is not reported. \\small 0.69{ tabular{l|c|c|c|c|c|c|c|c|c|c|c|c} 0.3ex 2{*}{Method} & 2{*}{U/S} & One-shot & Local & Data-driven & Pre-trained & Post-training & 2{*}{Model} & Ratio & FLOPs & 3{|c}{ImageNet} \\\\ 11-13 & & (Y/N) & (Y/N) & (Y/N) & (Y/N) & (Y/N) & & (\\%) & (\\%) & Original (\\%) & Pruned (\\%) & $\\Delta \\downarrow$ \\\\ \\hline SNIP (2019) lee2019snip & U & Y & N & Y & N & - & ResNet-50 & 80.00 & - & 76.15 & 69.67 & 6.48\\\\ SNIP (2019) lee2019snip wang2023ntksap & U & Y & N & Y & N & - & ResNet-50 & 89.26 & - & 76.20 & 60.98 & 15.22\\\\ SNIP (2019) lee2019snip wang2023ntksap & U & Y & N & Y & N & - & ResNet-50 & 95.60 & - & 76.20 & 40.69 & 35.51\\\\ SNIP (2019) lee2019snip jorge2021progressive & U & Y & N & Y & N & - & ResNet-50 & 90.00 & - & 75.60 & 61.50 & 14.10\\\\ SNIP (2019) lee2019snip jorge2021progressive & U & Y & N & Y & N & - & ResNet-50 & 95.00 & - & 75.60 & 44.30 & 31.30\\\\ GSM (2019) ding2019global & U & N & N & Y & Y & N & ResNet-50 & 75.00 & - & 75.72 & 75.33 & 0.39\\\\ GSM (2019) ding2019global & U & N & N & Y & Y & N & ResNet-50 & 80.00 & - & 75.72 & 74.30 & 1.42\\\\ Iterative-SNIP (2021) jorge2021progressive & U & N & N & Y & N & - & ResNet-50 & 90.00 & - & 75.60 & 63.70 & 11.90\\\\ Iterative-SNIP (2021) jorge2021progressive & U & N & N & Y & N & - & ResNet-50 & 95.00 & - & 75.60 & 54.70 & 30.90\\\\ GraSP (2020) wang2020picking & U & Y & N & Y & N & - & ResNet-50 & 80.00 & - & 76.15 & 72.06 & 4.09\\\\ GraSP (2020) wang2020picking wang2023ntksap & U & Y & N & Y & N & - & ResNet-50 & 89.26 & - & 76.20 & 67.74 & 8.46\\\\ GraSP (2020) wang2020picking wang2023ntksap & U & Y & N & Y & N & - & ResNet-50 & 95.60 & - & 76.20 & 59.73 & 16.47\\\\ SynFlow (2020) tanaka2020pruning wang2023ntksap & U & N & N & N & N & - & ResNet-50 & 89.26 & - & 76.20 & 66.81 & 9.39\\\\ SynFlow (2020) tanaka2020pruning wang2023ntksap & U & N & N & N & N & - & ResNet-50 & 95.60 & - & 76.20 & 58.88 & 17.32\\\\ RigL (2020) evci2020rigging & U & Y & Y & Y & N & - & ResNet-50 & 80.00 & - & 76.15 & 74.60 & 1.55\\\\ FORCE (2021) jorge2021progressive & U & N & N & Y & N & - & ResNet-50 & 90.00 & - & 75.60 & 64.90 & 10.70\\\\ FORCE (2021) jorge2021progressive & U & N & N & Y & N & - & ResNet-50 & 95.00 & - & 75.60 & 59.00 & 16.60\\\\ ProsPr (2022) alizadeh2022prospect & U & Y & N & Y & N & - & ResNet-50 & 90.00 & - & 75.60 & 66.86 & 8.74\\\\ ProsPr (2022) alizadeh2022prospect & U & Y & N & Y & N & - & ResNet-50 & 95.00 & - & 75.60 & 59.62 & 15.98\\\\ NTK-SAP (2023) wang2023ntksap & U & N & N & N & N & - & ResNet-50 & 89.26 & - & 76.20 & 68.28 & 7.92\\\\ NTK-SAP (2023) wang2023ntksap & U & N & N & N & N & - & ResNet-50 & 95.60 & - & 76.20 & 60.79 & 15.41\\\\ Jackpot (2023) zhang2023lottery & U & Y & Y & Y & Y & N & ResNet-50 & 80.00 & - & 76.15 & 75.71 & 0.44\\\\ Jackpot (2023) zhang2023lottery & U & Y & Y & Y & Y & N & ResNet-50 & 90.00 & - & 76.15 & 73.04 & 3.11\\\\ FCPTS (2024) gong2024fast & U & Y & Y & Y & Y & N & ResNet-50 & 50.00 & - & 77.89 & 77.43 & 0.46 \\\\ \\hline ThiNet (2017) luo2017thinet & S & N & N & Y & Y & Y & ResNet-50 & 33.72 & 36.79 & 75.30 & 74.03 & 1.27\\\\ ThiNet (2017) luo2017thinet & S & N & N & Y & Y & Y & ResNet-50 & 51.56 & 55.83 & 75.30 & 72.03 & 3.27\\\\ ThiNet (2017) luo2017thinet & S & N & N & Y & Y & Y & ResNet-50 & 66.12 & 71.50 & 75.30 & 68.17 & 7.13\\\\ CP (2017) he2017channel liu2021group & S & N & N & Y & Y & Y & ResNet-50 & - & 50.11 & 76.13 & 75.06 & 1.07\\\\ CP (2017)-train-from-scratch he2017channel & S & N & N & Y & Y & N & ResNet-50 & - & 50.00 & $92.20^{*}$ & $88.20^{*}$ & 4.00\\\\ CP (2017)-finetuned he2017channel & S & N & N & Y & Y & Y & ResNet-50 & - & 50.00 & $92.20^{*}$ & $90.80^{*}$ & 1.40\\\\ SFP (2018) he2018soft & S & Y & Y & Y & N & - & ResNet-50 & - & 41.80 & 76.15 & 74.61 & 1.54\\\\ SSS (2018) huang2018data & S & N & N & Y & N & - & ResNet-50 & 0.78 & 15.06 & 76.12 & 75.44 & 0.68\\\\ SSS (2018) huang2018data & S & N & N & Y & N & - & ResNet-50 & 27.06 & 31.08 & 76.12 & 74.18 & 1.94\\\\ IE (2019) molchanov2019importance & S & N & N & Y & Y & Y & ResNet-50 & - & 20.03 & 76.18 & 76.43 & -0.25 \\\\ IE (2019) molchanov2019importance & S & N & N & Y & Y & Y & ResNet-50 & - & 44.97 & 76.18 & 74.50 & 1.68 \\\\ IE (2019) molchanov2019importance & S & N & N & Y & Y & Y & ResNet-50 & - & 67.23 & 76.18 & 71.69 & 4.49 \\\\ GBN (2019) you2019gate & S & N & N & Y & Y & Y & ResNet-50 & - & 40.57 & 75.85 & 76.19 & -0.34 \\\\ Meta (2019) liu2019metapruning & S & Y & N & Y & Y & N & ResNet-50 & - & 26.83 & 76.60 & 76.20 & 0.40 \\\\ FPGM-prune-from-scratch (2019) he2019filter & S & Y & Y & Y & N & - & ResNet-50 & - & 42.20 & 76.15 & 75.03 & 1.12 \\\\ FPGM-finetuned (2019) he2019filter & S & Y & Y & Y & Y & Y & ResNet-50 & - & 42.20 & 76.15 & 75.59 & 0.56 \\\\ FPGM-prune-from-scratch (2019) he2019filter & S & Y & Y & Y & N & - & ResNet-50 & - & 53.50 & 76.15 & 74.13 & 2.02 \\\\ FPGM-finetuned (2019) he2019filter & S & Y & Y & Y & Y & Y & ResNet-50 & - & 53.50 & 76.15 & 74.83 & 1.32 \\\\ PFS (2020) wang2020pruning & S & N & N & Y & N & - & ResNet-50 & 16.74 & 26.83 & 77.20 & 76.70 & 0.50\\\\ PFS (2020) wang2020pruning & S & N & N & Y & N & - & ResNet-50 & 71.16 & 51.22 & 77.20 & 75.60 & 1.60\\\\ HRank (2020) lin2020hrank & S & Y & Y & Y & Y & Y & ResNet-50 & 36.70 & 43.70 & 76.15 & 74.98 & 1.17 \\\\ SCOP-A (2020) tang2020scop & S & Y & Y & Y & Y & Y & ResNet-50 & 42.80 & 45.30 & 76.15 & 75.95 & 0.20 \\\\ SCOP-B (2020) tang2020scop & S & Y & Y & Y & Y & Y & ResNet-50 & 51.80 & 54.60 & 76.15 & 75.26 & 0.89 \\\\ AutoPruner (2020) luo2020autopruner & S & N & N & Y & Y & Y & ResNet-50 & - & 51.10 & 76.15 & 74.76 & 1.39 \\\\ AutoPruner (2020) luo2020autopruner & S & N & N & Y & Y & Y & ResNet-50 & - & 66.01 & 76.15 & 73.05 & 3.10 \\\\ LFPC (2020) he2020learning & S & Y & Y & Y & Y & Y & ResNet-50 & - & 25.17 & 76.79 & 76.95 & -0.16 \\\\ DSA (2020) ning2020dsa & S & Y & Y & Y & N & - & ResNet-50 & - & 40.00 & 76.02 & 75.10 & 0.92 \\\\ DSA (2020) ning2020dsa & S & Y & Y & Y & N & - & ResNet-50 & - & 50.00 & 76.02 & 74.69 & 1.33 \\\\ DMCP-train from scratch (2020) guoO2020dmcp & S & N & Y & Y & N & - & ResNet-50 & - & 31.70 & 76.60 & 77.00 & -0.40 \\\\ DMCP-train-from-scratch (2020) guoO2020dmcp & S & N & Y & Y & N & - & ResNet-50 & - & 46.34 & 76.60 & 76.20 & 0.40 \\\\ GFP (2021) liu2021group & S & N & N & Y & Y & Y & ResNet-50 & - & 25.17 & 76.79 & 76.95 & -0.16 \\\\ GFP (2021) liu2021group & S & N & N & Y & Y & Y & ResNet-50 & - & 50.11 & 76.79 & 76.42 & 0.37 \\\\ GFP (2021) liu2021group & S & N & N & Y & Y & Y & ResNet-50 & - & 75.06 & 76.79 & 73.94 & 2.85 \\\\ Greg-2 (2021) wang2021neural & S & N & Y & Y & Y & Y & ResNet-50 & - & 32.93 & 76.13 & 75.36 & 0.77 \\\\ PGMPF (2022) cai2022prior & S & N & Y & Y & Y & Y & ResNet-50 & - & 53.50 & 76.01 & 75.11 & 0.90 \\\\ DFPC (2023) narshana2023dfpc & S & N & N & N & Y & Y & ResNet-50 & 45.65 & 49.49 & 76.10 & 75.90 & 1.20 \\\\ DFPC (2023) narshana2023dfpc & S & N & N & N & Y & Y & ResNet-50 & 62.26 & 71.10 & 76.10 & 73.80 & 2.30 \\\\ PDP (2023) cho2023pdp & S & N & N & Y & N & - & ResNet-50 & - & $54.90^{\\dagger}$ & 76.10 & 75.90 & 0.20\\\\ PPSM (2023) ma2022differentiable & S & N & N & Y & Y & Y & ResNet-50 & - & 53.07 & 76.13 & 75.78 & 0.35 \\\\ PPSM (2023) ma2022differentiable & S & N & N & Y & Y & Y & ResNet-50 & - & 65.91 & 76.13 & 75.43 & 0.70 \\\\ DepGraph (2023) fang2023depgraph & S & Y & N & Y & Y & Y & ResNet-50 & - & $51.82^{\\dagger}$ & 76.15 & 75.83 & 0.32 \\\\ ATO (2024) wu2024auto & S & N & N & Y & N & - & ResNet-50 & - & 55.20 & 76.13 & 76.59 & -0.46 \\\\ ATO (2024) wu2024auto & S & N & N & Y & N & - & ResNet-50 & - & 61.70 & 76.13 & 76.07 & 0.06 \\\\ ATO (2024) wu2024auto & S & N & N & Y & N & - & ResNet-50 & - & 71.00 & 76.13 & 74.77 & 1.36 \\\\ \\hline SSS (2018) huang2018data & S & N & N & N & Y & N & ResNet-101 & - & 55.52 & 76.40 & 75.44 & 0.96 \\\\ SFP (2018) he2018soft & S & Y & Y & Y & N & - & ResNet-101 & - & 42.20 & 77.37 & 77.03 & 0.34\\\\ IE (2019) molchanov2019importance & S & N & N & Y & Y & Y & ResNet-101 & 30.20 & 39.74 & 77.37 & 77.35 & 0.02 \\\\ FPGM-finetuned (2019) he2019filter & S & Y & Y & Y & Y & Y & ResNet-101 & - & 42.20 & 77.37 & 77.32 & 0.05 \\\\ SCOP-A (2020) tang2020scop & S & Y & Y & Y & Y & Y & ResNet-101 & 46.80 & 48.60 & 77.37 & 77.75 & -0.48 \\\\ SCOP-B (2020) tang2020scop & S & Y & Y & Y & Y & Y & ResNet-101 & 57.80 & 60.20 & 77.37 & 77.36 & 0.01 \\\\ GFP (2021) liu2021group & S & N & N & Y & Y & Y & ResNet-101 & 39.63 & 50.02 & 78.29 & 78.30 & -0.01 \\\\ 0.3ex tabular } -0.3cm table* table*[t] \\centering A summary of representative pruning methods in object detection. ``O-mAP'' and ``P-mAP'' represent mAP of the original and pruned networks, respectively. â€œRatioâ€ and ``FLOPs'' have the same meaning as in Table~\\ref{tab:summary-pruning-cnns-classification.} \\small 0.68{ tabular{l|c|c|c|c|c|c|c|c|c|c|c|c|c|c} 0.3ex 2{*}{Method} & 2{*}{U/S} & One-shot & Local & Pre-trained & Post-training & 2{*}{Model} & Ratio & FLOPs & 3{|c|}{PASCAL VOC} & 3{|c}{COCO}\\\\ 10-15 & & (Y/N) & (Y/N) & (Y/N) & (Y/N) & & (\\%) & (\\%) & O-mAP & P-mAP & $\\Delta \\downarrow$ & O-mAP & P-mAP & $\\Delta \\downarrow$ \\\\ \\hline LTH (2021) girish2021lottery & U & N & N & Y & N & Faster-RCNN (ResNet-18) & 78.94 & - & 69.74 & 68.47 & 1.27 & - & - & - \\\\ CP (2017) he2017channel & S & N & N & Y & Y & Faster R-CNN (VGG-16) & - & 50.00 & 68.70 & 68.30 & 0.40 & - & - & -\\\\ GFP (2021) liu2021group & S & N & N & Y & Y & Faster R-CNN & - & 50.00 & - & - & - & 37.40 & 37.80 & -0.40\\\\ GFP (2021) liu2021group & S & N & N & Y & Y & Faster R-CNN & - & 75.00 & - & - & - & 37.40 & 36.60 & 0.80\\\\ SAViT (2022) zheng2022savit & S & Y & N & Y & Y & Faster R-CNN (Swin-T) & 57.52 & 69.42 & - & - & - & 45.50 & 45.20 & 0.30\\\\ \\hline FCPTS (2024) gong2024fast & U & Y & Y & Y & N & MobileNetV1 SSD & 90.00 & - & 67.70 & 65.10 & 2.60 & - & - & -\\\\ FCPTS (2024) gong2024fast & U & Y & Y & Y & N & MobileNetV2 SSD-Lite & 90.00 & - & 68.60 & 59.10 & 9.50 & - & - & - \\\\ GFP (2021) liu2021group & S & N & N & Y & Y & RetinaNet & 31.03 & 50.02 & - & - & - & 36.50 & 36.80 & -0.30\\\\ MDC (2022) hou2022multidimensional & S & Y & Y & Y & Y & DeiT-Base & - & 60.00 & 81.80 & 81.50 & 0.30 & - & - & -\\\\ 0.3ex tabular } -0.1cm table* table*[t] \\centering A summary of representative pruning methods for ViTs in image classification. \\small 0.70{% tabular{l|c|c|c|c|c|c|c|c|c|c|c|c} 0.3ex 2{*}{Method} & 2{*}{U/S} & One-shot & Local & Data-driven & Pre-trained & Post-training & 2{*}{Model} & Ratio & FLOPs & 3{|c}{ImageNet} \\\\ 10-13 & & (Y/N) & (Y/N) & (Y/N) & (Y/N) & (Y/N) & & (\\%) & (\\%) & Original (\\%) & Pruned (\\%) & $\\Delta \\downarrow$\\\\ \\hline SViTE-T (2021) chen2021chasing & U & N & Y & Y & N & - & DeiT-Tiny & 29.72 & 25.56 & 72.20 & 71.78 & 0.42 \\\\ SViTE-T (2021) chen2021chasing & U & N & Y & Y & N & - & DeiT-Tiny & 39.51 & 34.16 & 72.20 & 71.75 & 0.45\\\\ $S^{2}$ViTE-T chen2021chasing (2021) & S & N & Y & Y & N & - & Deit-Tiny & 26.40 & 23.69 & 72.20 & 70.12 & 2.08\\\\ UVC-T (2022) yu2022unified & S & N & N & Y & Y & Y & DeiT-Tiny & - & 50.77 & 72.20 & 71.30 & 0.90\\\\ UVC-T (2022)yu2022unified & S & N & N & Y & Y & Y & DeiT-Tiny & - & 60.88 & 72.20 & 70.60 & 1.60 \\\\ SAViT-T (2022) zheng2022savit & S & Y & N & Y & Y & Y & DeiT-Tiny & 26.32 & 30.77 & 72.20 & 70.72 & 1.48\\\\ GOHSP-T (2023) yin2023gohsp & S & Y & N & Y & Y & Y & DeiT-Tiny & 29.82 & 30.00 & 72.20 & 70.24 & 1.96\\\\ X-Pruner-T (2023) yu2023xpruner & S & N & N & Y & Y & N & DeiT-Tiny & - & 50.80 & 72.20 & 71.10 & 1.10\\\\ SPViT-T (2024) he2024pruning & S & N & N & Y & Y & Y & Deit-Tiny & 15.79 & 23.08 & 72.20 & 70.70 & 1.50\\\\ \\hline SViTE-S (2021)chen2021chasing & U & N & Y & Y & N & - & DeiT-Small & 39.82 & 36.73 & 79.90 & 80.26 & -0.36\\\\ SCOP-S (2020) tang2020scop & S & Y & Y & Y & Y & Y & DeiT-Small & - & 43.48 & 79.80 & 77.50 & 2.30\\\\ $S^{2}$ViTE-S (2021) chen2021chasing & S & N & Y & Y & N & - & DeiT-Small & 33.94 & 31.63 & 79.90 & 79.22 & 0.68\\\\ UVC-S (2022) yu2022unified & S & N & N & Y & Y & Y & DeiT-Small & - & 49.59 & 79.80 & 78.82 & 0.98\\\\ SAViT-S (2022) zheng2022savit & S & Y & N & Y & Y & Y & DeiT-Small & 33.48 & 32.61 & 79.85 & 80.11 & -0.26\\\\ NViT-S (2023) yang2023global & S & N & N & Y & Y & N & DeiT-Small & 4.55 & 8.70 & 81.20 & 82.19 & -0.99\\\\ NViT-S-ASP (2023) yang2023global & S & N & N & Y & Y & Y & DeiT-Small & 52.27 & 8.70 & 81.20 & 82.19 & -0.99\\\\ GOHSP-S (2023) yin2023gohsp & S & Y & N & Y & Y & Y & DeiT-Small & 34.84 & 35.00 & 79.90 & 79.98 & -0.08\\\\ GOHSP-S (2023) yin2023gohsp & S & Y & N & Y & Y & Y & DeiT-Small & 49.77 & 39.00 & 79.90 & 79.86 & 0.04\\\\ X-Pruner-S (2023) yu2023xpruner & S & N & N & Y & Y & N & DeiT-Small & - & 47.90 & 79.80 & 78.93 & 0.87\\\\ SPViT-S (2024) he2024pruning & S & N & N & Y & Y & Y & DeiT-Small & 28.26 & 28.05 & 79.90 & 78.30 & 1.60\\\\ \\hline SViTE-B (2021) chen2021chasing & U & N & Y & Y & N & - & Deit-Base & 39.95 & 38.30 & 81.80 & 81.56 & 0.24\\\\ $S^{2}$ViTE-B (2021) chen2021chasing & S & N & Y & Y & N & - & Deit-Base & 34.41 & 33.13 & 81.80 & 82.22 & -0.42\\\\ SCOP-B (2020) tang2020scop & S & Y & Y & Y & Y & Y & DeiT-Base & - & 41.70 & 81.80 & 79.70 & 2.10\\\\ VTP-B (2021)zhu2021vision & S & Y & N & Y & Y & Y & DeiT-Base & 22.11 & 21.59 & 81.80 & 81.30 & 0.50\\\\ VTP-B (2021)zhu2021vision & S & Y & N & Y & Y & Y & DeiT-Base & 44.44 & 43.18 & 81.80 & 80.70 & 1.10\\\\ UVC-B (2022)yu2022unified & S & N & N & Y & Y & Y & DeiT-Base & - & 54.50 & 81.80 & 80.57 & 1.23\\\\ SAViT-B (2022)zheng2022savit & S & Y & N & Y & Y & Y & DeiT-Base & 70.67 & 69.89 & 81.84 & 81.66 & 0.18\\\\ CP-ViT-B (2022) song2022cpvit-w/o finetune & S & N & Y & Y & Y & N & DeiT-Base & 30 & 22.16 & 81.82 & 80.91 & 0.91\\\\ CP-ViT-B (2022) song2022cpvit-w/o finetune & S & N & Y & Y & Y & N & DeiT-Base & 40 & 30.67 & 81.82 & 80.31 & 1.51\\\\ CP-ViT-B (2022) song2022cpvit-w/ finetune & S & N & Y & Y & Y & Y & DeiT-Base & 30 & 22.62 & 81.82 & 81.66 & 0.16\\\\ CP-ViT-B (2022) song2022cpvit-w/ finetune & S & N & Y & Y & Y & Y & DeiT-Base & 40 & 32.41 & 81.82 & 81.52 & 0.30\\\\ CP-ViT-B (2022) song2022cpvit-w/ finetune & S & N & Y & Y & Y & Y & DeiT-Base & 50 & 41.62 & 81.82 & 81.13 & 0.69\\\\ WDPruning-B (2022)yu2022width & S & N & Y & Y & Y & Y & DeiT-Base & - & 43.40 & 81.80 & 80.76 & 1.04\\\\ MDC-B (2022) hou2022multidimensional & S & Y & Y & Y & Y & Y & DeiT-Base & - & 54.29 & 81.80 & 82.30 & -0.50\\\\ MDC-B (2022) hou2022multidimensional & S & Y & Y & Y & Y & Y & DeiT-Base & - & 60.00 & 81.80 & 81.50 & 0.30\\\\ X-Pruner-B (2023) yu2023xpruner & S & N & N & Y & Y & N & DeiT-Base & - & 51.50 & 81.80 & 81.02 & 0.78\\\\ NViT-B (2023)yang2023global & S & N & N & Y & Y & N & DeiT-Base & 60.47 & 61.36 & 83.36 & 83.29 & 0.07\\\\ NViT-B-ASP (2023)yang2023global & S & N & N & Y & Y & Y & DeiT-Base & 80.23 & 61.36 & 83.36 & 83.29 & 0.07\\\\ SPViT-B (2024) he2024pruning & S & N & N & Y & Y & Y & Deit-Base & 52.00 & 51.85 & 81.80 & 81.50 & 0.30\\\\ \\hline PLATON-B (2022) zhang2022platon & U & N & N & Y & Y & Y & ViT-Base & 60 & - & 83.50 & 82.60 & 0.90 \\\\ FCPTS-B (2024) gong2024fast & U & Y & Y & Y & Y & N & ViT-Base & 50 & - & 75.68 & 74.90 & 0.78\\\\ FCPTS-B (2024) gong2024fast & U & Y & Y & Y & Y & N & ViT-Base & 60 & - & 75.68 & 72.09 & 3.60\\\\ CP-ViT-B (2022) song2022cpvit-w/o finetune & S & N & Y & Y & Y & N & ViT-Base & 30 & 23.02 & 77.91 & 76.77 & 1.14\\\\ CP-ViT-B (2022) song2022cpvit-w/o finetune & S & N & Y & Y & Y & N & ViT-Base & 40 & 32.34 & 77.91 & 75.09 & 2.82\\\\ CP-ViT-B (2022) song2022cpvit-w/ finetune & S & N & Y & Y & Y & Y & ViT-Base & 30 & 24.91 & 77.91 & 77.75 & 0.16\\\\ CP-ViT-B (2022) song2022cpvit-w/ finetune & S & N & Y & Y & Y & Y & ViT-Base & 40 & 33.62 & 77.91 & 77.36 & 0.55\\\\ CP-ViT-B (2022) song2022cpvit-w/ finetune & S & N & Y & Y & Y & Y & ViT-Base & 50 & 46.34 & 77.91 & 76.75 & 1.16\\\\ DepGraph-B (2023) fang2023depgraph & S & Y & N & Y & Y & Y & ViT-Base & - & $40.91^{\\dagger}$ & 81.07 & 79.17 & 1.90 \\\\ \\hline DIMAP-T (2023) he2024data & U & Y & N & N & Y & Y & Swin-Tiny & - & 32.40 & 81.16 & 81.11 & 0.05\\\\ DIMAP-T (2023) he2024data & U & Y & N & N & Y & Y & Swin-Tiny & - & 50.80 & 81.16 & 80.35 & 0.81\\\\ X-Pruner-T (2023) yu2023xpruner & S & N & N & Y & Y & N & Swin-Tiny & - & 28.90 & 81.20 & 80.70 & 0.50\\\\ DIMAP-S (2023) he2024data & U & Y & N & N & Y & Y & Swin-Small & - & 33.20 & 83.19 & 82.99 & 0.20\\\\ DIMAP-S (2023) he2024data & U & Y & N & N & Y & Y & Swin-Small & - & 52.30 & 83.19 & 82.63 & 0.56\\\\ WDPruning-S (2022)yu2022width & S & N & Y & Y & Y & Y & Swin-Small & - & 27.58 & 83.00 & 81.80 & 1.20\\\\ X-Pruner-S (2023) yu2023xpruner & S & N & N & Y & Y & N & Swin-Samll & - & 27.60 & 83.20 & 82.00 & 1.20\\\\ DIMAP-B (2023) he2024data & U & Y & N & N & Y & Y & Swin-Base & - & 33.50 & 83.48 & 83.43 & 0.05\\\\ DIMAP-B (2023) he2024data & U & Y & N & N & Y & Y & Swin-Base & - & 52.70 & 83.48 & 83.28 & 0.20\\\\ 0.3ex tabular } -0.3cm table* table*[t] \\centering A summary of representative pruning methods for BERTs. \\small 0.65{% tabular{l|c|c|c|c|c|c|c|c|c|c|c|c|c} 0.3ex 2{*}{Method} & 2{*}{U/S} & One-shot & Local & Pre-trained & Post-training & 2{*}{Model} & Ratio & FLOPs & 5{|c}{Dataset} \\\\ 10-14 & & (Y/N) & (Y/N) & (Y/N) & (Y/N) & & (\\%) & (\\%) & QQP & MNLI & QNLI & SST-2 & $SQuAD_{1.1}$\\\\ \\hline LadaBERT (2020) mao2020ladabert & U & Y & N & Y & Y & $BERT_{base}$ & 60.00 & - & 71.20/70.70 & 84.60/83.50 & 90.50/89.60 & 93.50/92.80 & - \\\\ Prune OFA (2021) zafrir2021prune & U & N & N & Y & Y & $BERT_{base}$ & 85.00 & - & 91.20/90.69 & 84.06/81.67 & 91.16/89.95 & 92.13/91.34 & 80.80/78.59\\\\ PLATON-unstruct (2022) zhang2022platon & U & N & N & Y & Y & $BERT_{base}$ & 50.00 & - & - & - & - & - & 80.40/78.50\\\\ PLATON-unstruct (2022) zhang2022platon & U & N & N & Y & Y & $BERT_{base}$ & 60.00 & - & - & - & - & - & 80.40/78.00\\\\ PLATON-unstruct (2022) zhang2022platon & U & N & N & Y & Y & $BERT_{base}$ & 80.00 & - & 91.50/90.70 & 84.60/83.10 & 91.30/90.10 & 92.70/91.30 & -\\\\ CAP-m (2022) xu2022from & U & N & Y/N & Y & Y & $BERT_{base}$ & 90.00 & - & 90.90/90.70 & 84.50/81.00 & - & 92.90/- & 80.70/76.50 \\\\ oBERT (2022) kurtic2022optimal & U & N & N & Y & Y & $BERT_{base}$ & 90.00 & - & 91.06/90.99 & 84.54/83.40 & - & 91.25/89.97 & - \\\\ ISP (2023) jaiswal2023instant & U & N & N & Y & Y & $BERT_{base}$ & 70.00 & - & - & 82.40/82.71 & 89.10/90.06 & - & - \\\\ \\hline BSP (2020)li2020efficient & S & Y & Y & Y & Y & $BERT_{base}$ & 30.00 & - & 91.20/90.70 & 84.60/82.90 & 90.50/88.20 & 93.50/89.30 & - \\\\ SuperTicket (2021) liang2021super & S & Y & N & Y & Y & $BERT_{base}$ & 13.20 & - & 91.30/88.30 & 84.50/84.50 & -/91.30 & - & -\\\\ PLATON-struct (2022) zhang2022platon & S & N & N & Y & Y & $BERT_{base}$ & 50.00 & - & - & - & - & - & 80.4/77.00\\\\ PLATON-struct (2022) zhang2022platon & S & N & N & Y & Y & $BERT_{base}$ & 60.00 & - & - & - & - & - & 80.4/75.60\\\\ Mask-Tuning (2022) kwon2022fast & S & N & Y & Y & N & $BERT_{base}$ & - & 40.00 & 91.00/90.38 & 84.53/82.26 & 91.41/90.00 & 93.57/92.47 & -\\\\ CAP-f (2022) xu2022from & S & N & Y/N & Y & Y & $BERT_{base}$ & 90.00 & - & 90.90/90.20 & 84.50/81.00 & - & 92.90/89.70 & 80.70/70.20 \\\\ KCM (2023) nova2023gradient & S & Y & Y & Y & N & $BERT_{base}$ & - & 30.00 & 91.00/90.39 & 84.53/81.18 & 91.41/90.58 & 93.57/92.26 & -\\\\ KCM (2023) nova2023gradient & S & Y & Y & Y & N & $BERT_{base}$ & - & 40.00 & 91.00/89.15 & 84.53/77.24 & 91.41/87.79 & 93.57/91.11 & -\\\\ 0.3ex tabular } -0.3cm table* table*[t] \\centering A summary of representative pruning methods for LLMs. ``$\\dagger$'' indicates the sequence length for inference is 2048, otherwise it is 128. If a second citation is present, it references the reported results. ``-'' means the corresponding result is not reported or N/A. \\small 0.68{% tabular{l|c|c|c|c|c|c|cc|cccc|c} 0.3ex 2{*}{Method} & 2{*}{U/S} & One-shot & Local & Post-training & Pre-trained & Ratio & 2{|c|}{Prediction $\\downarrow$} & 4{|c|}{Common Sense $\\uparrow$} & 2{*}{MMLU $\\uparrow$} \\\\ 8-9 10-13 & & (Y/N) & (Y/N) & (Y/N) & Model & (\\%) & WikiText2 & PTB & BoolQ & PIQA & Helleswag & WinoGrande & \\\\ \\hline Dense ma2023llmpruner & - & - & - & - & LLaMA-7B & 0.00 & 12.62/$5.68^{\\dagger}$ & 22.14 & 73.18 & 78.35 & 72.99 & 67.01 & 34.43 sun2024simple \\\\ Dense sun2024simple & - & - & - & - & LLaMA-2-7B & 0.00 & 12.62 & 22.14 & 77.74 & - & 57.17 & 68.90 & 52.08 \\\\ Dense bai2024sparsellm & - & - & - & - & LLaMA-2-13B & 0.00 & $4.88^{\\dagger}$ & $50.94^{\\dagger}$ & 77.89 & - & 59.94 & 72.77 & - \\\\ Dense song2024sleb & - & - & - & - & OPT-6.7B & 0.00 & $10.86^{\\dagger}$ & - & - & 76.39 & 67.16 & 65.19 & - \\\\ Dense bai2024sparsellm & - & - & - & - & OPT-30B & 0.00 & $9.56^{\\dagger}$ & $14.04^{\\dagger}$ & 70.46 & - & 54.27 & 69.02 & - \\\\ Dense kim2024shortened & - & - & - & - & Vicuna-7B & 0.00 & 17.10 & 63.20 & 78.10 & 77.30 & 73.90 & 69.50 & - \\\\ \\hline SparseGPT (2023)~frantar2023sparsegpt sun2024simple & U & Y & Y & N & LLaMA-7B & 50.00 & $7.22^{\\dagger}$ & - & 75.02 & - & 52.37 & 69.85 & 34.43\\\\ SparseGPT (2023)~frantar2023sparsegpt yin2024outlier & U & Y & Y & N & LLaMA-7B & 70.00 & $26.30^{\\dagger}$ & - & 64.53 & - & 42.11 & 58.64 & - \\\\ Wanda (2024)~sun2024simple & U & Y & Y & N & LLaMA-7B & 50.00 & $7.26^{\\dagger}$ & - & 71.22 & - & 51.85 & 66.06 & 33.49\\\\ Wanda (2024)~sun2024simple yin2024outlier & U & Y & Y & N & LLaMA-7B & 70.00 & $85.77^{\\dagger}$ & - & 55.11 & - & 31.83 & 51.38 & -\\\\ BESA (2024)~xu2024besa & U & Y & Y & N & LLaMA-7B & 50.00 & $6.86^{\\dagger}$ & $66.96^{\\dagger}$ & 72.17 & 76.66 & 54.31 & 67.64 & -\\\\ OWL w. SparseGPT (2024)~yin2024outlier & U & Y & Y & N & LLaMA-7B & 70.00 & $19.49^{\\dagger}$ & - & 67.13 & - & 48.56 & 62.03 & -\\\\ OWL w. Wanda (2024)~yin2024outlier & U & Y & Y & N & LLaMA-7B & 70.00 & $24.55^{\\dagger}$ & - & 62.48 & - & 44.79 & 58.72 & -\\\\ LLM-Pruner (2023)~ma2023llmpruner & S & Y & Y & Y & LLaMA-7B & 20.00 & 17.58 & 30.11 & 64.62 & 77.20 & 68.80 & 63.14 & - \\\\ LLM-Pruner (2023)~ma2023llmpruner & S & Y & Y & Y & LLaMA-7B & 50.00 & 38.12/$16.41^{\\dagger}$ & 66.35 & 60.28 & 69.31 & 47.06 & 53.43 & -\\\\ LoRAPruner (2023)~zhang2023loraprune & S & N & Y & Y & LLaMA-7B & 20.00 & 16.80 & 28.75 & 65.62 & 79.31 & 70.00 & 62.76 & -\\\\ LoRAPruner (2023)~zhang2023loraprune & S & N & Y & Y & LLaMA-7B & 50.00 & 30.12/$11.60^{\\dagger}$ & 50.30 & 61.88 & 71.53 & 47.86 & 55.01 & -\\\\ Compresso (2023)~guo2023compresso & S & N & N & N & LLaMA-7B & 28.57 & - & - & 73.55 & 73.07 & 49.16 & 64.80 & 27.68\\\\ Compresso (2023)~guo2023compresso & S & N & N & N & LLaMA-7B & 35.71 & - & - & 68.69 & 72.85 & 47.18 & 63.38 & 25.92\\\\ LoRAShear (2023)~chen2023lorashear & S & N & N & Y & LLaMA-7B & 20.00 & - & - & 72.78 & 76.36 & 69.49 & 67.63 & -\\\\ LoRAShear (2023)~chen2023lorashear & S & N & N & Y & LLaMA-7B & 50.00 & - & - & 63.40 & 72.15 & 49.83 & 56.40 & -\\\\ Bonsai (2024)~dery2024everybody & S & N & N & Y & LLaMA-7B & 50.00 & $10.92^{\\dagger}$ & - & 67.22 & - & 43.09 & 61.64 & 28.92\\\\ Shortened (2024)~kim2024shortened & S & Y & N & Y & LLaMA-7B & 20.00 & 17.70 & 30.70 & 72.70 & 75.70 & 70.04 & 63.60 & -\\\\ FLAP (2024)~an2024fluctuationbased & S & N & N & N & LLaMA-7B & 20.00 & 14.62 & - & 69.63 & 76.82 & 71.20 & 68.35 & -\\\\ SliceGPT (2024)~ashkboos2024slicegpt song2024sleb & S & Y & N & N & LLaMA-7B & 25.00 & - & - & - & 66.87 & 54.16 & 63.38 & -\\\\ SliceGPT (2024)~ashkboos2024slicegpt song2024sleb & S & Y & N & N & LLaMA-7B & 30.00 & - & - & - & 63.55 & 49.62 & 61.33 & -\\\\ SLEB (2024)~song2024sleb & S & N & N & N & LLaMA-7B & 10.00 & - & - & - & 76.44 & 70.23 & 63.14 & -\\\\ SLEB (2024)~song2024sleb & S & N & N & N & LLaMA-7B & 20.00 & $12.94^{\\dagger}$ & - & - & 73.07 & 62.47 & 58.96 & -\\\\ \\hline SparseGPT (2023) frantar2023sparsegpt sun2024simple & U & Y & Y & N & LLaMA-2-7B & 50.00 & $6.52^{\\dagger}$ & - & 75.02 & - & 52.37 & 69.85 & 38.68\\\\ Wanda (2024)~sun2024simple & U & Y & Y & N & LLaMA-2-7B & 50.00 & $6.44^{\\dagger}$ & - & 75.99 & - & 52.49 & 68.19 & 39.27\\\\ BESA (2024)~xu2024besa & U & Y & Y & N & LLaMA-2-7B & 50.00 & $6.60^{\\dagger}$ & $44.09^{\\dagger}$ & 74.83 & 76.66 & 54.60 & 68.59 & -\\\\ LLM-Pruner (2023)~ma2023llmpruner men2024shortgpt & S & Y & Y & Y & LLaMA-2-7B & 27.00 & - & - & 55.20 & 71.22 & 56.46 & - & 23.33 \\\\ SliceGPT-Alpaca (2024)~ashkboos2024slicegpt & S & Y & N & N & LLaMA-2-7B & 20.00 & - & - & - & 76.50 & 65.20 & 65.51 & -\\\\ SliceGPT-Alpaca (2024)~ashkboos2024slicegpt & S & Y & N & N & LLaMA-2-7B & 30.00 & - & - & - & 72.25 & 55.86 & 59.83 & -\\\\ SliceGPT-Wiki (2024)~ashkboos2024slicegpt & S & Y & N & N & LLaMA-2-7B & 20.00 & $6.64^{\\dagger}$ & - & - & 69.42 & 59.04 & 65.11 & -\\\\ SliceGPT-Wiki (2024)~ashkboos2024slicegpt & S & Y & N & N & LLaMA-2-7B & 30.00 & $8.12^{\\dagger}$ & - & - & 63.55 & 49.62 & 61.33 & -\\\\ LLM Surgeon (2024)~ouderaa2024llm & S & N & N & N & LLaMA-2-7B & 50.00 & 43.68 & - & 39.60 & 64.36 & 40.29 & 52.57 & -\\\\ K-OBD (2024)~ouderaa2024llm & S & N & N & N & LLaMA-2-7B & 50.00 & 136.33 & - & 61.56 & 60.66 & 36.84 & 53.04 & -\\\\ Sheared (2024)~xia2024sheared & S & N & N & Y & LLaMA-2-7B & 61.00 & - & - & 73.70 & 75.80 & 70.80 & 64.20 & 26.40 \\\\ Sheared (2024)~xia2024sheared & S & N & N & Y & LLaMA-2-7B & 81.00 & - & - & 64.00 & 73.40 & 60.70 & 57.90 & 25.70\\\\ LaCo (2024)~yang2024laco & S & Y & N & N & LLaMA-2-7B & 27.10 & - & - & 64.07 & 69.80 & 55.69 & - & 26.45\\\\ ShortGPT (2024)~men2024shortgpt & S & Y & N & N & LLaMA-2-7B & 27.10 & - & - & 74.71 & 66.43 & 53.02 & - & 43.96\\\\ LLM-Streamline (2024)~chen2024compressing & S & Y & N & Y & LLaMA-2-7B & 26.20 & - & - & 65.00 & 70.10 & 59.20 & - & 47.00\\\\ \\hline SparseGPT (2023)~frantar2023sparsegpt bai2024sparsellm & U & Y & Y & N & LLaMA-2-13B & 70.00 & $12.98^{\\dagger}$ & $267.63^{\\dagger}$ & 70.03 & - & 42.20 & 66.54 & -\\\\ Wanda (2024)~sun2024simple bai2024sparsellm & U & Y & Y & N & LLaMA-2-13B & 50.00 & $23.42^{\\dagger}$ & $502.53^{\\dagger}$ & - & - & - & - & -\\\\ SparseLLM (2024)~bai2024sparsellm & U & Y & N & Y & LLaMA-2-13B & 70.00 & $12.95^{\\dagger}$ & $277.76^{\\dagger}$ & 69.87 & - & 42.50 & 68.64 & -\\\\ LLM-Pruner (2023)~ma2023llmpruner men2024shortgpt & S & Y & Y & Y & LLaMA-2-13B & 24.40 & - & - & 56.42 & 76.66 & 67.76 & - & 25.21 \\\\ SliceGPT-Alpaca (2024)~ashkboos2024slicegpt & S & Y & N & N & LLaMA-2-13B & 20.00 & & - & - & 77.97 & 69.64 & 68.90 & -\\\\ SliceGPT-Alpaca (2024)~ashkboos2024slicegpt & S & Y & N & N & LLaMA-2-13B & 30.00 & & - & - & 74.10 & 60.91 & 65.82 & -\\\\ SliceGPT-Wiki (2024)~ashkboos2024slicegpt & S & Y & N & N & LLaMA-2-13B & 20.00 & $5.81^{\\dagger}$ & - & - & 71.87 & 63.04 & 69.38 & -\\\\ SliceGPT-Wiki (2024)~ashkboos2024slicegpt & S & Y & N & N & LLaMA-2-13B & 30.00 & $6.99^{\\dagger}$ & - & - & 66.10 & 52.69 & 65.11 & -\\\\ LaCo (2024)~yang2024laco & S & Y & N & N & LLaMA-2-13B & 24.60 & - & - & 63.98 & 74.27 & 64.39 & - & 45.93\\\\ ShortGPT (2024)~men2024shortgpt & S & Y & N & N & LLaMA-2-13B & 24.60 & - & - & 62.48 & 73.45 & 66.64 & - & 54.69\\\\ \\hline SparseGPT (2023)~frantar2023sparsegpt & U & Y & Y & N & OPT-6.7B & 50.00 & $11.55^{\\dagger}$ & $17.44^{\\dagger}$ & - & - & - & - & -\\\\ SliceGPT-Alpaca (2024)~ashkboos2024slicegpt & S & Y & N & N & OPT-6.7B & 20.00 & - & - & - & 74.54 & 62.84 & 62.67 & -\\\\ SliceGPT-Alpaca (2024)~ashkboos2024slicegpt & S & Y & N & N & OPT-6.7B & 30.00 & - & - & - & 73.34 & 58.93 & 61.80 & -\\\\ SliceGPT-Wiki (2024)~ashkboos2024slicegpt & S & Y & N & N & OPT-6.7B & 20.00 & $11.48^{\\dagger}$ & - & - & 72.74 & 61.04 & 61.09 & -\\\\ SliceGPT-Wiki (2024)~ashkboos2024slicegpt & S & Y & N & N & OPT-6.7B & 30.00 & $12.51^{\\dagger}$ & - & - & 68.61 & 54.56 & 60.69 & -\\\\ SLEB (2024)~song2024sleb & S & N & N & N & OPT-6.7B & 10.00 & $11.22^{\\dagger}$ & - & - & 76.61 & 66.36 & 64.72 & -\\\\ SLEB (2024)~song2024sleb & S & N & N & N & OPT-6.7B & 20.00 & $12.94^{\\dagger}$ & - & - & 74.92 & 62.13 & 61.33 & -\\\\ LLM-Streamline (2024)~chen2024compressing & S & Y & N & Y & OPT-6.7B & 26.00 & - & - & 63.00 & 73.10 & 52.40 & - & 24.60\\\\ \\hline SparseGPT (2023)~frantar2023sparsegpt bai2024sparsellm & U & Y & Y & N & OPT-30B & 70.00 & $9.58^{\\dagger}$ & $14.41^{\\dagger}$ & 68.78 & - & 53.83 & 67.64 & -\\\\ Wanda (2024)~sun2024simple bai2024sparsellm & U & Y & Y & N & OPT-30B & 70.00 & $7766.61^{\\dagger}$ & $5547.45^{\\dagger}$ & - & - & - & - & -\\\\ SparseLLM (2024)~bai2024sparsellm & U & Y & N & Y & OPT-30B & 70.00 & $9.56^{\\dagger}$ & $14.40^{\\dagger}$ & 69.11 & - & 53.97 & 68.43 & -\\\\ SliceGPT-Alpaca (2024)~ashkboos2024slicegpt & S & Y & N & N & OPT-30B & 20.00 & - & - & - & 78.35 & 70.64 & 66.61 & -\\\\ SliceGPT-Alpaca (2024)~ashkboos2024slicegpt & S & Y & N & N & OPT-30B & 30.00 & - & - & - & 76.93 & 68.66 & 64.96 & -\\\\ SliceGPT-Wiki (2024)~ashkboos2024slicegpt & S & Y & N & N & OPT-30B & 20.00 & $9.87^{\\dagger}$ & - & - & 76.50 & 70.61 & 66.61 & -\\\\ SliceGPT-Wiki (2024)~ashkboos2024slicegpt & S & Y & N & N & OPT-30B & 30.00 & $10.27^{\\dagger}$ & - & - & 74.97 & 68.15 & 65.04 & -\\\\ SLEB (2024)~song2024sleb & S & N & N & N & OPT-30B & 10.00 & $9.57^{\\dagger}$ & - & - & 77.64 & 72.32 & 68.75 & -\\\\ SLEB (2024)~song2024sleb & S & N & N & N & OPT-30B & 20.00 & $10.73^{\\dagger}$ & - & - & 76.93 & 70.62 & 67.40 & -\\\\ \\hline LLM-Pruner (2023)~ma2023llmpruner & S & Y & Y & Y & Vicuna-7B & 20.00 & 19.69 & 78.25 & 63.33 & 76.17 & 65.13 & 60.22 & -\\\\ LLM-Pruner (2023)~ma2023llmpruner kim2024shortened & S & Y & Y & Y & Vicuna-7B & 20.00 & 19.60 & 76.40 & 65.40 & 76.20 & 68.90 & 64.40 & -\\\\ LLM-Pruner (2023)~ma2023llmpruner kim2024shortened & S & Y & Y & Y & Vicuna-7B & 35.00 & 27.60 & 102.00 & 52.00 & 72.40 & 61.60 & 59.90 & -\\\\ Shortened (2024)~kim2024shortened & S & Y & N & Y & Vicuna-7B & 20.00 & 18.80 & 67.90 & 71.70 & 74.40 & 67.60 & 63.60 & -\\\\ Shortened (2024)~kim2024shortened & S & Y & N & Y & Vicuna-7B & 35.00 & 26.60 & 89.40 & 65.20 & 70.40 & 56.50 & 56.60 & -\\\\ FLAP (2024)~an2024fluctuationbased kim2024shortened & S & N & N & N & Vicuna-7B & 20.00 & 22.00 & 74.90 & 73.10 & 74.80 & 67.90 & 65.80 & -\\\\ FLAP (2024)~an2024fluctuationbased kim2024shortened & S & N & N & N & Vicuna-7B & 35.00 & 26.60 & 89.40 & 65.20 & 70.40 & 56.50 & 56.50 & -\\\\ 0.3ex tabular } -0.3cm table*",
      "origin_cites_number": 266
    },
    {
      "section_title": "Pruning for Specific Applications",
      "level": "1",
      "content": "This section summarizes the characteristics of mainstream applications involved in pruning. The commonly used networks, datasets, and evaluation metrics of pruning are shown in Table Table:application-summary. The statistics of pruning literature across different applications are shown in Fig.~Fig:statistics.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Image Classification",
      "level": "2",
      "content": "Image classification refers to classifying images according to their visual content and is the most basic task for pruning. Most pruning works in CV provide experimental results on ImageNet (ILSVRC-2012) russakovsky2015imagenet. The representative pruning results on ImageNet are shown in Fig.~Fig:resnet-50-imagenet, with results sourced from the corresponding papers. The networks used in papers with the same name may differ in the network structures, leading to different accuracy even if other settings are the same. For example, some works (e.g., nonnenmacher2022sosp,su2020sanity) expand the layers' width of ResNet-32 by a factor (such as 2 or 4). Top-1 accuracy of the expanded ResNet-32 is higher than that of the vanilla ResNet-32. Some works (such as liu2017learning,huang2018data) use VGG-16 with one FC layer, while others (lin2019towards,zhao2019variational) adopt two or three FC layers.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Object Detection",
      "level": "2",
      "content": "In comparison with image classification, object detection requires predicting the class and the exact location of each object in an image. Correspondingly, neural networks for object detection have more complex architectures, including backbones and other detection components. In addition, object detection generally requires larger input sizes, making pruning for object detection more challenging than image classification. Only a few works (e.g., yao2021DetNAS,bonnaerens2022anchor) study pruning for object detection. For instance, bonnaerens2022anchor propose an anchor pruning method. girish2021lottery investigate LTH for object detection. liu2021group prune one-stage and two-stage object detection models to validate performance. The commonly used accuracy metrics, as shown in Table~Table:application-summary, include mAP (mean Average Precision) and COCO mAP that is mAP evaluated at Intersection-Over-Union (IoU) thresholds evenly distributed between 0.5 and 0.95.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Image Style Translation",
      "level": "2",
      "content": "Image style translation, which means transferring the style from one image to another, is an important application for deploying GANs on mobile devices. Compared with networks for image classification, GANs have very different network structures and outputs, consisting of a generator and a discriminator that output high-dimension images. shu2019coevolutionary identify two major differences between compressing models for image classification and GANs. First, the discriminator network does not need to be compact because it will be discarded after training the generative network. Second, it is difficult to quantitatively evaluate the output images by generated GANs. Besides, the training difficulty poses extra challenges for pruning GANs shu2019coevolutionary. Some pruning methods (e.g., shu2019coevolutionary,wang2020gan) are proposed to reduce GAN's parameters and computational complexities. For example, chen2021gans prune CycleGAN to verify matching subnetworks in GANs. As shown in Table Table:application-summary, FCN-scores isola2017image2image, and Fr\\'echet Inception Distance (FID) between the feature distributions of real and generated samples kalibhat2021winning are often utilized to evaluate pruning results. FCN-scores include pixel accuracy, class accuracy, and class IoU. The larger FID indicates better transfer results.",
      "origin_cites_number": 6
    },
    {
      "section_title": "Adversarial Robustness",
      "level": "2",
      "content": "In safety-critical but computationally resource-constrained applications, neural network pruning faces the challenge of whether sparse models can preserve robustness. While the research community has extensively explored robust training and network pruning independently, only a few recent works (sehwag2019towards,sehwag2020hydra) have studied them jointly. For example, sehwag2019towards empirically verify that adversarial robustness can be achieved with weight magnitude-based pruning. sehwag2020hydra increase the awareness of pruning techniques to the robust training objective and formulates the pruning objective as an empirical risk minimization problem. The benign accuracy in Table Table:application-summary refers to the percentage of correctly classified original (i.e., non-modified) inputs. Empirical Robust Accuracy (ERA) refers to the percentage of robust test samples under gradient-based attacks. Verifiable Robust Accuracy (VRA) corresponds to the fraction of test samples that are verified to be robust by network verification methods as described in wong2018scaling.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Other CV Tasks",
      "level": "2",
      "content": "In addition to the above tasks, some works explore pruning methods for other CV tasks, such as semantic segmentation (girish2021lottery), image deraining (zou2022dreaming), human-pose estimation (wang2018exploring), head-pose estimation (aghli2021combining), image super-resolution (li2020dhp), object tracking (zhang2022pruning), text-to-image generation (fang2023structural), and backdoor attack (li2022baat) etc. For example, wang2018exploring prune CMU-pose model cao2017realtime for human pose estimation. li2020dhp prune DnCNN zhang2017beyond and U-Net ronneberger2015unet for image denoising.",
      "origin_cites_number": 13
    },
    {
      "section_title": "Natural Language Processing",
      "level": "2",
      "content": "see2016compression explore one of the earliest pruning methods of deep neural networks for NLP, where a magnitude-based method is proposed to prune LSTMs hochreiter1997neural for machine translation. yu2020playing explore the lottery hypothesis in NLP. Voita2019analyzing prune multi-head self-attention of Transformer vaswani2017attention. chen2020lottery pioneer the study of the lottery ticket hypothesis in pre-trained BERT devlin2019bert models and find matching subnetworks. Currently, transformer-based large language models (such as LLaMA touvron2023llama, OPT zhang2022opt) have become the dominant modeling paradigm in natural language processing. As shown in Table Table:application-summary WMTâ€™14/16 refers to https://www.statmt.org/wmt14/translation-task.html, the GLUE benchmark The detailed information of GLUE tasks can refer to Table 1 in \\cite{wang2019glue.} provides nine tasks and their corresponding metrics, including accuracy, Pearson correlation, Matthew's correlation, etc. BiLingual Evaluation Understudy (BLEU) score papineni2002bleu is often used for evaluating the accuracy of machine translation. Perplexity is used for language modeling or machine translation to measure the change in performance due to pruning. The lower the Perplexity, the better the pruned model.",
      "origin_cites_number": 11
    },
    {
      "section_title": "Vision-and-Language Tasks",
      "level": "2",
      "content": "Vision-and-Language (VL) is one of the most common areas of multimodal research. VL tasks include Visual Question Answering (VQA) goyal2017making, image-text retrieval, etc. gan2022playing pioneer the investigation of lottery ticket in VL tasks and find ``relaxed'' winning tickets that match 99\\% of the full accuracy can be found with a 50\\%-70\\% prune ratio of parameters in UNITER chen2020uniter. Table~Table:application-summary lists widely employed models, datasets, and evaluation metrics for pruning on VQA and image-text retrieval. The evaluation metrics TR@1 and IR@1 for image retrieval tasks denote Top-1 text recall and image recall, respectively.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Audio and Speech Processing",
      "level": "2",
      "content": "Speech recognition is one of the most common tasks in audio and speech processing. Lightweight speech recognition has become an indispensable feature on mobile devices. nrang2017exploring prune Deep Speech 2 amodei2016deep, a recurrent neural network architecture, to validate their gradual pruning scheme. ding2022audio extend the lottery ticket hypothesis to speech recognition models and investigate the existence of winning tickets. PARP lai2021parp prunes pre-trained wav2vec 2.0 wav2vec2 and XLSR-53 xlsr53 for self-supervised speech recognition. As shown in Table Table:application-summary, the Word Error Rate (WER) is the standard metric for measuring the accuracy of speech recognition, which is defined as $WER = (S+I+D)/(S+I+C)$, where $S$, $I$, $D$, and $C$ denote the number of substitutions, insertions, deletions, and correct words, respectively. The lower the value, the better the accuracy of the speech recognition model. The Character Error Rate (CER) has the exact definition of WER, except that CER counts characters while WER counts words. table*[t] Summary of commonly used pruning evaluation metrics for various applications. \\centering 0.84{% tabular{l|c|cccc|c} 0.3ex Task & Type & Dataset & Model & Performance & Efficiency & Example work\\\\ \\hline Image & 24{*}{CV} & CIFAR-10/100 krizhevsky2009learning, & ResNet-32/50/56 he2016deep, & Top-1/Top-5 accuracy & FLOPs, & frankle2019lottery, lin2020hrank,\\\\ Classification & & ImageNet ILSVRC-2012 russakovsky2015imagenet & VGG-16/19 simonyan2015very, & & MACs, & nonnenmacher2022sosp, liu2021group,\\\\ & & & ViT dosovitskiy2021image/DeiT touvron2021training /Swin liu2021swin & & $\\lVert W \\rVert_{0}$ & evci2022gradeint, tanaka2020pruning\\\\ & & & & & &\\\\ Object & & COCO lin2014coco, & RetinaNet lin2017focal, & mAP, & FLOPs, & liu2021group, bonnaerens2022anchor, \\\\ Detection & & PASCAL VOC 2007/2010 everingham2008pascal, & Faster R-CNN fasterrcnn, & COCO mAP & $\\lVert W \\rVert_{0}$ & girish2021lottery \\\\ & & & SSD liu2016ssd & & \\\\ & & & & & & \\\\ Generative tasks & & CIFAR-10 krizhevsky2009learning & DDPMs ho2020denoising & FID heusel2017gans & MACs & fang2023structural \\\\ & & CelebA-HQ (64$\\times$64) liu2015deep & LDMs rombach2022high & SSIM wang2004image & & \\\\ & & LSUN Church (256$\\times$256)) yu2016lsun & & & & \\\\ & & & & & &\\\\ Image Style & & horse2zebra zhu2017unpaired, & CycleGAN zhu2017unpaired & FCN-scores isola2017image2image, & FLOPs, & shu2019coevolutionary, wang2020gan \\\\ Translation & & summer2winter zhu2017unpaired, & & FID heusel2017gans & $\\lVert W \\rVert_{0}$ & \\\\ & & Cityscapes cityscape & & & & \\\\ & & & & & &\\\\ Adversarial & & CIFAR-10 krizhevsky2009learning, & VGG-16 simonyan2015very, & Top-1/Top-5 accuracy, & $\\lVert W \\rVert_{0}$ & sehwag2020hydra,sehwag2019towards\\\\ Robustness & & ImageNet ILSVRC-2012 russakovsky2015imagenet & ResNet-18/50 he2016deep, & Benign accuracy, & & \\\\ & & & Wide-ResNet-28-5 & ERA/VRA & & \\\\ & & & & & \\\\ Image Denoising & & U-Net ronneberger2015unet & SIDD abdelhamed2018a & PSNR and SSIM wang2004iamge & $\\lVert W \\rVert_{0}$ & shi2023memory \\\\ & & & & & \\\\ Human-pose & & COCO lin2014coco & CMU-pose cao2017realtime & mAP & & wang2018exploring \\\\ Estimation & & & & & \\\\ \\hline Machine & 9{*}{NLP} & WMTâ€™14/16, & Transformer-based vaswani2017attention, & BLEU papineni2002bleu, & FLOPs, & yu2020playing, see2016compression, \\\\ Translation & & OpenSubtitles2018 lison2018opensubtitles & LSTMs hochreiter1997neural & & $\\lVert W \\rVert_{0}$ & zhu2017to, Voita2019analyzing\\\\ & & & & & & \\\\ Natural Language & & SST-2 socher2013recursive, QNLI rajpurkar2016squad & BERTs devlin2019bert, & Accuracy, & $\\lVert W \\rVert_{0}$ & chen2020lottery, sanh2020movement, \\\\ Understanding & & SQuAD rajpurkar2016squad, QQP shankar2017first, & LLaMA~touvron2023llama & Pearson/Matthew cor., & & ma2023llmpruner,zhang2023loraprune \\\\ & & MNLI williams2018broad & OPT zhang2022opt & F1 & & frantar2023sparsegpt,ashkboos2024slicegpt \\\\ & & & & & & \\\\ Language & & Wikitext-2 merity2017pointer, & LSTMs hochreiter1997neural, & Perplexity & $\\lVert W \\rVert_{0}$ & yu2020playing, zhu2017to \\\\ Modeling & & Penn Tree Bank marcus1993building & LLaMA~touvron2023llama & & & ma2023llmpruner,xia2024sheared \\\\ \\hline 3{*}{Speech Recognition} & 3{*}{ASP} & TED-LIUM rousseau2012ted, & wav2vec 2.0 wav2vec2, & WER, & $\\lVert W \\rVert_{0}$ & nrang2017exploring, ding2022audio, \\\\ & & Common Voice ardila2020common, & LSTMs hochreiter1997neural, & CER & & lai2021parp,jiang2023accurate \\\\ & & LibriSpeech panayotov2015librispeech & Conformer gulati2020conformer & & & \\\\ \\hline 2{*}{Image-Text Retrieval} & 4{*}{VL}& COCO lin2014coco & CLIP radford2021learning, BLIP li2022blip & TR@1, IR@1 & FLOPs & shi2023upop,sung2024ecoflap \\\\ & & Flickr30K young2014from & & & $\\lVert W \\rVert_{0}$ & \\\\ & & & & & & \\\\ Visual Question Answering & & VQAv2 goyal2017making & CLIP radford2021learning, BLIP li2022blip & Accuracy & FLOPs & shi2023upop,sung2024ecoflap \\\\ 0.3ex tabular } table* \\ifCLASSOPTIONcaptionsoff \\newpage \\fi {\\footnotesize IEEEtranN",
      "origin_cites_number": 101
    }
  ],
  "literature_review_id": 260887757,
  "meta_info": {
    "cite_counts": 333,
    "Conference_journal_name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "influentialcitationcount": 13,
    "Author_info": {
      "Publicationsh": 3,
      "h_index": 3,
      "Citations": 300,
      "Highly Influential Citations": 0
    },
    "all_cites_title": [
      "Year 0 250 500 750 1000 1250 1500 1750 Number of Papers Neural Network Compression",
      "Very deep convolutional networks for large-scale image recognition",
      "An image is worth 16x16 words: Transformers for image recognition at scale",
      "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "PaLM: Scaling language modeling with pathways",
      "Towards efficient end-to-end speech recognition with biologically-inspired neural networks",
      "Sparks of large audio models: A survey and outlook",
      "Vila: On pre-training for visual language models",
      "Sora: A review on background, technology, limitations, and opportunities of large vision models",
      "Deep residual learning for image recognition",
      "Drawing early-bird tickets: Towards more efficient training of deep networks",
      "AI-generated content AIGC: A survey",
      "Learning both weights and connections for efficient neural network",
      "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "More is Less: A more complicated network with less inference complexity",
      "Gate Decorator: Global filter pruning method for accelerating deep convolutional neural networks",
      "ThiNet: A filter level pruning method for deep neural network compression",
      "SparseGPT: Massive language models can be accurately pruned in one-shot",
      "HYDRA: Pruning adversarially robust neural networks",
      "SliceGPT: Compress large language models by deleting rows and columns",
      "LLM-Pruner: On the structural pruning of large language models",
      "Exploiting linear structure within convolutional networks for efficient evaluation",
      "Holistic CNN compression via low-rank decomposition with knowledge transfer",
      "QLoRA: Efficient finetuning of quantized LLMs",
      "OmniQuant: Omnidirectionally calibrated quantization for large language models",
      "MiniLLM: Knowledge distillation of large language models",
      "A survey on knowledge distillation of large language models",
      "DARTS: Differentiable architecture search",
      "iDARTS: Differentiable architecture search with stochastic implicit gradients",
      "RepVGG: making VGG-style ConvNets great again",
      "MobileVLM V2: Faster and stronger baseline for vision language model",
      "Comparing biases for minimal network construction with back-propagation",
      "A survey on deep neural network compression: Challenges, overview, and solutions",
      "A survey on efficient convolutional neural networks and hardware acceleration",
      "A comprehensive survey of compression algorithms for language models",
      "A survey of resourceefficient LLM and multimodal foundation models",
      "Recent advances on neural network pruning at initialization",
      "Structured pruning for deep convolutional neural networks: A survey",
      "DepGraph: Towards any structural pruning",
      "Structural pruning for diffusion models",
      "A survey on model compression for large language models",
      "Pruning algorithms-a survey",
      "What is the state of neural network pruning",
      "An survey of neural network compression",
      "Pruning algorithms to accelerate convolutional neural networks for edge applications: A survey",
      "A comprehensive survey on model compression and acceleration",
      "Sparsity in deep learning: pruning and growth for efficient inference and training in neural networks",
      "Model compression and efficient inference for large language models: A survey",
      "The lottery ticket hypothesis: finding sparse, trainable neural networks",
      "A simple and effective pruning approach for large language models",
      "Pruning neural networks without any data by iteratively conserving synaptic flow",
      "Pruning filter in filter",
      "An image enhancing patternbased sparsity for real-time inference on mobile devices",
      "Trainability preserving neural structured pruning",
      "SNIP: Single-shot network pruning based on connection sensitivity",
      "Sanity-checking pruning methods: random tickets can win the jackpot",
      "Picking winning tickets before training by preserving gradient flow",
      "Learning efficient convolutional networks through network slimming",
      "Learning structured sparsity in deep neural networks",
      "Data-driven sparse structure selection for deep neural networks",
      "Global vision transformer pruning with hessian-aware saliency",
      "Dynamic sparse training: Find efficient sparse network from scratch with trainable masked layers",
      "Group fisher pruning for practical network compression",
      "Pruning filters for efficient convnets",
      "Manifold regularized dynamic network pruning",
      "ShortGPT: Layers in large language models are more redundant than you expect",
      "Attention is all you need",
      "SOSP: Efficiently capturing global correlations by second-order structured pruning",
      "NPAS: A compiler-aware framework of unified network pruning and architecture search for beyond real-time mobile acceleration",
      "The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training",
      "Pruning from scratch",
      "Dual lottery ticket hypothesis",
      "A signal propagation perspective for pruning neural networks at initialization",
      "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "A unified paths perspective for pruning at initialization",
      "Neural tangent kernel: Convergence and generalization in neural networks",
      "Pruning neural networks at initialization: why are we missing the mark?",
      "Weight agnostic neural networks",
      "What's hidden in a randomly weighted neural network?",
      "Revisiting pruning at initialization through the lens of Ramanujan graph",
      "No free prune: Informationtheoretic barriers to pruning at initialization",
      "Rigging the lottery: Making all tickets winners",
      "Variational convolutional neural network pruning",
      "Soft filter pruning for accelerating deep convolutional neural networks",
      "MetaPruning: Meta learning for automatic neural network channel pruning",
      "DSA: More efficient budgeted pruning via differentiable sparsity allocation",
      "PDP: Parameter-free differentiable pruning is all you need",
      "Model selection and estimation in regression with grouped variables",
      "MorphNet:fast & simple resource-constrained structure learning of deep networks",
      "Compressing convolutional neural networks via factorized convolutional filters",
      "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science",
      "Sparse training via boosting pruning plasticity with neuroregeneration",
      "Deep ensembling with no overhead for either training or testing: The all-round blessings of dynamic sparsity",
      "NeST: A neural network synthesis tool based on a grow-and-prune paradigm",
      "Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization",
      "Dynamic model pruning with feedback",
      "Sparse networks from scratch: Faster training without losing performance",
      "Dynamic sparse training for deep reinforcement learning",
      "Gradient flow in sparse neural networks and how lottery tickets win",
      "Filter pruning via geometric median for deep convolutional neural networks acceleration",
      "Robust statistics on riemannian manifolds via the geometric median",
      "DMCP: Differentiable Markov channel pruning for neural networks",
      "Differentiable network pruning via polarization of probabilistic channelwise soft masks",
      "X-pruner: explainable pruning for vision transformers",
      "Auto-balanced filter pruning for efficient convolutional neural networks",
      "Importance estimation for neural network pruning",
      "Towards compact ConvNets via structure-sparsity regularized filter pruning",
      "Neural pruning via growing regularization",
      "Compressing large language models by streamlining the unimportant layer",
      "Rethinking the value of network pruning",
      "Comparing rewinding and fine-tuning in neural network pruning",
      "A survey of lottery ticket hypothesis",
      "Multi-prize lottery ticket hypothesis: finding a accurate binary neural networks by pruning a randomly weighted network",
      "GANs can play lottery tickets too",
      "Winning lottery tickets in deep generative models",
      "Rethinking graph lottery tickets: Graph sparsity matters",
      "When BERT plays the lottery, all tickets are winning",
      "Proving the lottery ticket hypothesis: Pruning is all you need",
      "Logarithmic pruning is all you need",
      "One ticket to win them all: Generalizing lottery ticket initializations across datasets and optimizers",
      "Sparse transfer learning via winning lottery tickets",
      "Playing lottery tickets with vision and language",
      "A unified lottery ticket hypothesis for graph neural networks",
      "Validating the lottery ticket hypothesis with inertial manifold theory",
      "Why lottery ticket wins? a theoretical perspective of sample complexity on pruned neural networks",
      "Unmasking the lottery ticket hypothesis-what's encoded in a winning ticket's mask",
      "Sanity checks for lottery tickets: Does your winning ticket really win the jackpot?",
      "Lottery ticket preserves weight correlation: Is it desirable or not?\" in ICML",
      "LoRA: Low-rank adaptation of large language models",
      "UPop: Unified and progressive pruning for compressing vision-language transformers",
      "Gradient-free structured pruning with unlabeled data",
      "Everybody prune now: Structured pruning of LLMs with only forward passes",
      "Shortened LLaMA: A simple depth pruning for large language models",
      "Channel pruning for accelerating very deep neural networks",
      "Towards optimal structured CNN pruning via generative adversarial learning",
      "Structured pruning learns compact and accurate models",
      "Sheared LLaMA: Accelerating language model pre-training via structured pruning",
      "Regression shrinkage and selection via the lasso",
      "ECC: Platform-independent energyconstrained deep neural network compression via a bilinear regression model",
      "Network pruning via performance maximization",
      "Drawing early-bird tickets: towards more efficient training of deep networks",
      "Early-BERT: Efficient BERT training via early-bird lottery tickets",
      "Are sixteen heads really better than one?",
      "Linear mode connectivity and the lottery ticket hypothesis",
      "A fast post-training pruning framework for transformers",
      "Fluctuation-based adaptive structured pruning for large language models",
      "Runtime network routing for efficient image classification",
      "Channel gating neural networks",
      "Dynamic channel pruning: Feature boosting and suppression",
      "Fire together wire together: A dynamic pruning approach with self-supervised mask prediction",
      "Contrastive dual gating: Learning sparse features with contrastive learning",
      "Dimensionality reduction by learning an invariant mapping",
      "AccelTran: A sparsity-aware accelerator for dynamic inference with transformers",
      "Compression of neural machine translation models via pruning",
      "A gradient flow framework for analyzing network pruning",
      "Towards compact and robust deep neural networks",
      "Pruning convolutional neural networks for resource efficient inference",
      "Towards compact CNNs via collaborative compression",
      "Width & depth pruning for vision transformers",
      "On the predictability of pruning across scales",
      "Layer-adaptive sparsity for the magnitude-based pruning",
      "Optimal brain damage",
      "What matters in the structured pruning of generative language models?",
      "The LLM surgeon",
      "Second order derivatives for network pruning: Optimal brain surgeon",
      "EigenDamage: Structured pruning in the Kronecker-Factored eigenbasis",
      "The optimal BERT surgeon: Scalable and accurate second-order pruning for large language models",
      "DHP: Differentiable meta pruning via hypernetworks",
      "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "Graph pruning for model compression",
      "AMC: AutoML for model compression and acceleration on mobile devices",
      "Practical blockwise neural network architecture generation",
      "Single shot structured pruning before training",
      "Pointer sentinel mixture models",
      "OPT: Open pre-trained transformer language models",
      "Learning methods for generic object recognition with invariance to pose and lighting",
      "Learning multiple layers of features from tiny images",
      "LLaMA: Open and efficient foundation language models",
      "Building a large annotated corpus of English: The penn treebank",
      "Exploiting kernel sparsity and entropy for interpretable CNN compression",
      "ImageNet large scale visual recognition challenge",
      "LoRAPrune: Pruning meets low-rank parameter-efficient fine-tuning",
      "Why is pruning at initialization immune to reinitializing and shuffling?",
      "Train-by-Reconnect: decoupling locations of weights from their values",
      "How much pre-training is enough to discover a good subnetwork",
      "Never train from scratch: Fair comparison of long-sequence models requires data-driven priors",
      "Towards efficient model compression via learned global ranking",
      "ChipNet: Budget-aware pruning with heaviside continuous approximations",
      "BERT Busters: Outlier dimensions that disrupt transformers",
      "SparseLLM: Towards global pruning for pre-trained language models",
      "Network pruning that matters: a case study on pretraing variants",
      "A systematic DNN weight pruning framework using alternating direction method of multipliers",
      "Good subnetworks provably exist: Pruning via greedy forward selection",
      "Training data-efficient image transformers & distillation through attention",
      "A suvery on tranfer learning",
      "The lottery ticket hypothesis for pre-trained BERT networks",
      "The lottery tickets hypothesis for supervised and self-supervised pre-training in computer vision models",
      "How well do sparse imagenet models transfer",
      "The elastic lottery ticket hypothesis",
      "Dynamic dual gating neural networks",
      "Movement pruning: Adaptive sparsity by fine-tuning",
      "Are all layers created equal?",
      "Fast and controllable posttraining sparsity-learning optimal sparsity allocation with global constraint in minutes",
      "Outlier weighed layerwise sparsity (OWL): A missing secret sauce for pruning LLMs to high sparsity",
      "Why lift so heavy? slimming large language models by cutting off the layers",
      "LaCo: Large language model pruning via layer collapse",
      "Unsupervised synaptic pruning strategies for restricted boltzmann machines",
      "Self-supervised learning: Generative or contrastive",
      "Pruning convolutional neural networks with self-supervision",
      "Momentum contrastive pruning",
      "A simple framework for contrastive learning of visual representations",
      "Improved baselines with momentum contrastive learning",
      "PARP: Prune, adjust and re-prune for self-supervised speech recognition",
      "Distilling the knowledge in a neural network",
      "CLIP-Q: Deep network compression learning by in-parallel pruning-quantization",
      "Group sparsity: The hinge between filter pruning and decomposition for network compression",
      "LoSparse: Structured compression of large language models based on low-rank and sparse approximation",
      "Network pruning via transformable architecture search",
      "Structural pruning of large language models via neural architecture search",
      "Contentaware GAN compression",
      "Prune your model before distill it",
      "Knowledge from the original network: restore a better pruned network with knowledge distillation",
      "Long live the lottery: the existence of winning tickets in lifelong learning",
      "Advancing model pruning via bi-level optimization",
      "Dreaming to prune image deraining networks",
      "Large multimodal model compression via efficient pruning and distillation at AntGroup",
      "Lad-aBERT: Lightweight adaptation of BERT through hybrid model compression",
      "Joint-DetNAS: Upgrade your detector with NAS, pruning and dynamic distillation",
      "GAN slimming: All-in-one GAN compression by a unified optimization framework",
      "Train large, then compress: Rethinking model size for efficient training and inference of transformers",
      "ParameterNet: Parameters are all you need",
      "Dynamic convolution: Attention over convolution kernels",
      "Learning bayesian sparse networks with full experience replay for continual learning",
      "Study-ing the impact of magnitude pruning on contrastive learning methods",
      "Model pruning enables efficient federated learning on edge devices",
      "A hardware-friendly high-precision cnn pruning method and its fpga implementation",
      "Gpt-4 technical report",
      "Why is the state of neural network pruning so confusing? on the fairness, comparison setup, and trainability in network pruning",
      "A probabilistic approach to neural network pruning",
      "Exploring linear relationship in feature map subspace for ConvNets compression",
      "Learning filter pruning criteria for deep convolutional neural networks acceleration",
      "Robust pruning at initialization",
      "On the importance of initialization and momentum in deep learning",
      "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
      "PIQA: Reasoning about physical commonsense in natural language",
      "Hellaswag: Can a machine really finish your sentence",
      "Wino-Grande:an adversarial winograd schema challenge at scale",
      "Lottery jackpots exist in pre-trained models",
      "SCOP: Scientific control for reliable neural network pruning",
      "Progressive skeletonization: trimming more fat from a network at initialization",
      "SAViT: Structureaware vision transformer pruning via collaborative optimization",
      "Pruning self-attentions into convolutional layers in single path",
      "NTK-SAP: Improving neural network pruning by aligning training dynamics",
      "DFPC: Data flow driven pruning of coupled channels without data",
      "Prior gradient mask guided pruning-aware fine-tuning",
      "PLATON: Pruning large transformer models with upper confidence bound of weight importance",
      "Global sparse momentum SGD for pruning very deep neural networks",
      "Prospect pruning: Finding trainable weights at initialization using metagradients",
      "HRank: Filter pruning using high-rank feature map",
      "AutoPruner: An end-to-end trainable filter pruning method for efficient deep model inference",
      "Auto-train-once: Controller network guided automatic network pruning from scratch",
      "The lottery ticket hypothesis for object recogni-tion",
      "Multi-dimensional model compression of vision transformer",
      "Chasing sparsity in vision transformers: An end-to-end exploration",
      "Unified visual transformer compression",
      "GOHSP: A unified framework of graph and optimization-based heterogeneous structured pruning for vision transformer",
      "Vision transformer pruning",
      "CP-ViT: Cascade vision transformer pruning via progressive sparsity prediction",
      "Data-independent module-aware pruning for hierarchical vision transformers",
      "Prune once for all: Sparse pre-trained language models",
      "From dense to sparse: Contrastive pruning for better pre-trained language model compression",
      "Instant soup: Cheap pruning ensembles in a single pass can draw lottery tickets from large models",
      "Efficient transformer-based large scale language representations using hardware-friendly block structured pruning",
      "Super tickets in pretrained language models: From model compression to improving generalization",
      "SLEB: Streamlining llms through redundancy verification and elimination of transformer blocks",
      "BESA: Pruning large language models with blockwise parameter-efficient sparsity allocation",
      "Compresso: Structured pruning with collaborative prompting learns compact large language models",
      "LoRAShear: Efficient large language model structured pruning and knowledge recovery",
      "Anchor pruning for object detection",
      "Co-evolutionary compression for unpaired image translation",
      "Image-to-image translation with conditional adversarial networks",
      "Scaling provable adversarial defenses",
      "Combining weight pruning and knowledge distillation for CNN compression",
      "A pruning method for deep convolutional network based on heat map generation metrics",
      "BAAT: Towards sample-specific backdoor attack with clean labels",
      "Realtime multi-person 2D pose estimation using part affinity fields",
      "Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising",
      "U-Net: Convolutional networks for biomedical image segmentation",
      "Long short-term memory",
      "Playing the lottery with rewards and multiple languages: Lottery tickets in RL and NLP",
      "GLUE: A multi-task benchmark and analysis platform for natu-ral language understanding",
      "BLEU: a method for automatic evaluation of machine translation",
      "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
      "UNITER: Universal image-text representation learning",
      "Exploring sparsity in recurrent neural networks",
      "Deep Speech 2: End-to-end speech recognition in English and Mandarin",
      "Audio lottery: Speech recognition made ultra-lightweight, transferable, and noise-robust",
      "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "Unsupervised cross-lingual representation learning for speech recognition",
      "Swin Transformer: Hierarchical vision transformer using shifted windows",
      "Microsoft COCO: Common objects in context",
      "Focal loss for dense object detection",
      "The PASCAL visual object classes (VOC) challenge 2007",
      "Faster R-CNN: Towards real-time object detection with region proposal networks",
      "SSD: Single shot multibox detector",
      "Denoising diffusion probabilistic models",
      "GANs trained by a two time-scale update rule converge to a local nash equilibrium",
      "Deep learning face attributes in the wild",
      "High-resolution image synthesis with latent diffusion models",
      "Image quality assessment: from error visibility to structural similarity",
      "LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop",
      "Unpaired image-toimage translation using cycle-consistent adversarial networks",
      "The Cityscapes dataset for semantic urban scene understanding",
      "A high-quality denoising dataset for smartphone cameras",
      "Image quality assessment: from error visibility to structural similarity",
      "Memory-oriented structural pruning for efficient image restoration",
      "OpenSubtitles2018: Statistical rescoring of sentence alignments in large, noisy parallel corpora",
      "To prune, or not to prune: exploring the efficacy of pruning for model compression",
      "Recursive deep models for semantic compositionality over a sentiment treebank",
      "SQuAD: 100,000+ questions for machine comprehension of text",
      "First quora dataset release: Question pairs",
      "A broad-coverage challenge corpus for sentence understanding through inference",
      "Pointer sentinel mixture models",
      "TED-LIUM: an automatic speech recognition dedicated corpus",
      "Common Voice: A massively multilingual speech corpus",
      "Accurate and structured pruning for efficient automatic speech recognition",
      "Librispeech: An ASR corpus based on public domain audio books",
      "Conformer: Convolutionaugmented transformer for speech recognition",
      "Learning transferable visual models from natural language supervision",
      "BLIP: Bootstrapping languageimage pre-training for unified vision-language understanding and generation",
      "ECoFLaP: Efficient coarseto-fine layer-wise pruning for vision-language models",
      "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"
    ]
  }
}