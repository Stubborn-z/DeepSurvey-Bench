{"paper_id": 258480357, "title": "Evaluating Post-hoc Interpretability with Intrinsic Interpretability", "author_names": ["J. P. Amorim", "P. Abreu", "João A. M. Santos", "H. Müller"], "venue": "arXiv.org", "abstract": "Despite Convolutional Neural Networks having reached human-level performance in some medical tasks, their clinical use has been hindered by their lack of interpretability. Two major interpretability strategies have been proposed to tackle this problem: post-hoc methods and intrinsic methods. Although there are several post-hoc methods to interpret DL models, there is significant variation between the explanations provided by each method, and it a difficult to validate them due to the lack of ground-truth. To address this challenge, we adapted the intrinsical interpretable ProtoPNet for the context of histopathology imaging and compared the attribution maps produced by it and the saliency maps made by post-hoc methods. To evaluate the similarity between saliency map methods and attribution maps we adapted 10 saliency metrics from the saliency model literature, and used the breast cancer metastases detection dataset PatchCamelyon with 327,680 patches of histopathological images of sentinel lymph node sections to validate the proposed approach. Overall, SmoothGrad and Occlusion were found to have a statistically bigger overlap with ProtoPNet while Deconvolution and Lime have been found to have the least.", "year": 2023, "publicationdate": "2023-05-04", "externalids": {"DOI": "10.48550/arXiv.2305.03002"}, "doi_lower": "10.48550/arxiv.2305.03002"}
{"paper_id": 217272290, "title": "Arts. 536 a 538", "author_names": ["Paulo Henrique dos Santos Lucon"], "venue": "", "abstract": null, "year": 2017, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 232587203, "title": "Criticisms and Challenges", "author_names": ["H. Dobson"], "venue": "", "abstract": null, "year": 2006, "publicationdate": "2006-12-18", "externalids": {"DOI": "10.4324/9780203029756-15"}, "doi_lower": "10.4324/9780203029756-15"}
{"paper_id": 121337728, "title": "Toy Gun for Projectile Experiments", "author_names": ["H. P. Stabler"], "venue": "", "abstract": null, "year": 1961, "publicationdate": "1961-02-01", "externalids": {"DOI": "10.1119/1.1937709"}, "doi_lower": "10.1119/1.1937709"}
{"paper_id": 185109796, "title": "BENCH MARKING IN DER ABWASSERWIRTSCHAFT", "author_names": ["S. Wibbe"], "venue": "", "abstract": null, "year": 1999, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 67826209, "title": "Ecosystem services value, research needs, and policy relevance: a commentary", "author_names": ["R. Turner", "W. Adger", "R. Brouwer"], "venue": "", "abstract": null, "year": 1998, "publicationdate": "1998-04-01", "externalids": {"DOI": "10.1016/S0921-8009(98)00018-4"}, "doi_lower": "10.1016/s0921-8009(98)00018-4"}
{"paper_id": 10908960, "title": "The Definition of AI in Terms of Multi Agent Systems", "author_names": ["D. Dobrev"], "venue": "arXiv.org", "abstract": "The questions which we will consider here are \"What is AI?\" and \"How can we make AI?\". Here we will present the definition of AI in terms of multi-agent systems. This means that here you will not find a new answer to the question \"What is AI?\", but an old answer in a new form. \nThis new form of the definition of AI is of interest for the theory of multi-agent systems because it gives us better understanding of this theory. More important is that this work will help us answer the second question. We want to make a program which is capable of constructing a model of its environment. Every multi-agent model is equivalent to a single-agent model but multi-agent models are more natural and accordingly more easily discoverable.", "year": 2012, "publicationdate": "2012-10-02", "externalids": {}, "doi_lower": null}
{"paper_id": 263242886, "title": "Value alignment, human enhancement, and moral revolutions", "author_names": ["A. Tubert", "Justin Tiehen"], "venue": "Inquiry", "abstract": "ABSTRACT Human beings are internally inconsistent in various ways. One way to develop this thought involves using the language of value alignment: the values we hold are not always aligned with our behavior and are not always aligned with each other. Because of this self-misalignment, there is room for potential projects of human enhancement that involve achieving a greater degree of value alignment than we presently have. Relatedly, discussions of AI ethics sometimes focus on what is known as the value alignment problem, the challenge of how to build AI that acts in accordance with our human values. We argue that there is an especially close connection between solving the value alignment problem in AI ethics and using AI to pursue certain forms of human enhancement. But in addition, we also argue that there are important limits to what kinds of human enhancement can be pursued in this way, because some forms of human enhancement—namely moral revolutions—involve a kind of value misalignment rather than alignment.", "year": 2023, "publicationdate": "2023-09-27", "externalids": {"DOI": "10.1080/0020174X.2023.2261506"}, "doi_lower": "10.1080/0020174x.2023.2261506"}
{"paper_id": 9147428, "title": "A Bayesian Approach to Modeling Dynamical Systems in the Social Sciences", "author_names": ["Shyam Ranganathan", "Viktoria Spaiser", "D. Sumpter"], "venue": "International Conference on Simulation and Modeling Methodologies, Technologies and Applications", "abstract": null, "year": 2018, "publicationdate": "2018-08-15", "externalids": {"DOI": "10.5220/0004480901250131"}, "doi_lower": "10.5220/0004480901250131"}
{"paper_id": 58006852, "title": "Social Integration of Artificial Intelligence: Functions, Automation Allocation Logic and Human-Autonomy Trust", "author_names": ["H. Abbass"], "venue": "Cognitive Computation", "abstract": "Artificial intelligence (AI) is finding more uses in the human society resulting in a need to scrutinise the relationship between humans and AI. Technology itself has advanced from the mere encoding of human knowledge into a machine to designing machines that “know how” to autonomously acquire the knowledge they need, learn from it and act independently in the environment. Fortunately, this need is not new; it has scientific grounds that could be traced back to the inception of computers. This paper uses a multi-disciplinary lens to explore how the natural cognitive intelligence in a human could interface with the artificial cognitive intelligence of a machine. The scientific journey over the last 50 years will be examined to understand the Human-AI relationship, and to present the nature of, and the role of trust in, this relationship. Risks and opportunities sitting at the human-AI interface will be studied to reveal some of the fundamental technical challenges for a trustworthy human-AI relationship. The critical assessment of the literature leads to the conclusion that any social integration of AI into the human social system would necessitate a form of a relationship on one level or another in society, meaning that humans will “always” actively participate in certain decision-making loops—either in-the-loop or on-the-loop—that will influence the operations of AI, regardless of how sophisticated it is.", "year": 2019, "publicationdate": "2019-01-14", "externalids": {"DOI": "10.1007/s12559-018-9619-0"}, "doi_lower": "10.1007/s12559-018-9619-0"}
{"paper_id": 207155342, "title": "Apprenticeship learning via inverse reinforcement learning", "author_names": ["P. Abbeel", "A. Ng"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 2004, "publicationdate": "2004-07-04", "externalids": {"DOI": "10.1145/1015330.1015430"}, "doi_lower": "10.1145/1015330.1015430"}
{"paper_id": 263630346, "title": "Moral Foundations of Large Language Models", "author_names": ["Marwa Abdulhai", "Gregory Serapio-Garcia", "Clé-ment Crepy", "Dasha Valter", "John F. Canny", "Natasha Jaques"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Moral foundations theory (MFT) is a psychological assessment tool that decomposes human moral reasoning into five factors, including care/harm, liberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary in the weight they place on these dimensions when making moral decisions, in part due to their cultural upbringing and political ideology. As large language models (LLMs) are trained on datasets collected from the internet, they may reflect the biases that are present in such corpora. This paper uses MFT as a lens to analyze whether popular LLMs have acquired a bias towards a particular set of moral values. We analyze known LLMs and find they exhibit particular moral foundations, and show how these relate to human moral foundations and political affiliations. We also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted. Finally, we show that we can adversarially select prompts that encourage the moral to exhibit a particular set of moral foundations, and that this can affect the model’s behavior on downstream tasks. These findings help illustrate the potential risks and unintended consequences of LLMs assuming a particular moral stance.", "year": 2023, "publicationdate": "2023-10-23", "externalids": {"DOI": "10.48550/arXiv.2310.15337"}, "doi_lower": "10.48550/arxiv.2310.15337"}
{"paper_id": 14717578, "title": "Reinforcement Learning as a Framework for Ethical Decision Making", "author_names": ["David Abel", "J. MacGlashan", "M. Littman"], "venue": "AAAI Workshop: AI, Ethics, and Society", "abstract": null, "year": 2016, "publicationdate": "2016-03-29", "externalids": {}, "doi_lower": null}
{"paper_id": 51886199, "title": "Artificial Intelligence, Automation and Work", "author_names": ["D. Acemoglu", "P. Restrepo"], "venue": "", "abstract": null, "year": 2018, "publicationdate": "2018-01-01", "externalids": {"DOI": "10.2139/ssrn.3098384"}, "doi_lower": "10.2139/ssrn.3098384"}
{"paper_id": 251042718, "title": "A survey of inverse reinforcement learning", "author_names": ["Stephen C. Adams", "Tyler Cody", "P. Beling"], "venue": "Artificial Intelligence Review", "abstract": "Learning from demonstration, or imitation learning, is the process of learning to act in an environment from examples provided by a teacher. Inverse reinforcement learning (IRL) is a specific form of learning from demonstration that attempts to estimate the reward function of a Markov decision process from examples provided by the teacher. The reward function is often considered the most succinct description of a task. In simple applications, the reward function may be known or easily derived from properties of the system and hard coded into the learning process. However, in complex applications, this may not be possible, and it may be easier to learn the reward function by observing the actions of the teacher. This paper provides a comprehensive survey of the literature on IRL. This survey outlines the differences between IRL and two similar methods - apprenticeship learning and inverse optimal control. Further, this survey organizes the IRL literature based on the principal method, describes applications of IRL algorithms, and provides areas of future research.", "year": 2022, "publicationdate": "2022-02-08", "externalids": {"DOI": "10.1007/s10462-021-10108-x"}, "doi_lower": "10.1007/s10462-021-10108-x"}
{"paper_id": 250004441, "title": "Recommender Systems, Ground Truth, and Preference Pollution", "author_names": ["G. Adomavicius", "J. Bockstedt", "S. Curley", "Jingjing Zhang"], "venue": "The AI Magazine", "abstract": "Interactions between individuals and recommender systems can be viewed as a continuous feedback loop, consisting of pre-consumption and post-consumption phases. Pre-consumption, systems provide recommendations that are typically based on predictions of user preferences. They represent a valuable service for both providers and users as decision aids. After item consumption, the user provides post-consumption feedback (e.g., a preference rating) to the system, often used to improve the system’s subsequent recommendations, completing the feedback loop. There is a growing understanding that this feedback loop can be a significant source of unintended consequences, introducing decision-making biases that can affect the quality of the “ground truth” preference data, which serves as the key input to modern recommender systems. This paper highlights two forms of bias that recommender systems inherently inflict on the “ground truth” preference data collected from users after item consumption: non-representativeness of such preference data and so-called “preference pollution,” which denotes an unintended relationship between system recommendations and the user’s post-consumption preference ratings. We provide an overview of these issues and their importance for the design and application of next-generation recommendation systems, including directions for future research.", "year": 2022, "publicationdate": "2022-06-01", "externalids": {"DOI": "10.1002/aaai.12055"}, "doi_lower": "10.1002/aaai.12055"}
{"paper_id": 231632080, "title": "Reinforcement Learning based Recommender Systems: A Survey", "author_names": ["M. M. Afsar", "Trafford Crump", "B. Far"], "venue": "ACM Computing Surveys", "abstract": "Recommender systems (RSs) have become an inseparable part of our everyday lives. They help us find our favorite items to purchase, our friends on social networks, and our favorite movies to watch. Traditionally, the recommendation problem was considered to be a classification or prediction problem, but it is now widely agreed that formulating it as a sequential decision problem can better reflect the user-system interaction. Therefore, it can be formulated as a Markov decision process (MDP) and be solved by reinforcement learning (RL) algorithms. Unlike traditional recommendation methods, including collaborative filtering and content-based filtering, RL is able to handle the sequential, dynamic user-system interaction and to take into account the long-term user engagement. Although the idea of using RL for recommendation is not new and has been around for about two decades, it was not very practical, mainly because of scalability problems of traditional RL algorithms. However, a new trend has emerged in the field since the introduction of deep reinforcement learning (DRL), which made it possible to apply RL to the recommendation problem with large state and action spaces. In this paper, a survey on reinforcement learning based recommender systems (RLRSs) is presented. Our aim is to present an outlook on the field and to provide the reader with a fairly complete knowledge of key concepts of the field. We first recognize and illustrate that RLRSs can be generally classified into RL- and DRL-based methods. Then, we propose an RLRS framework with four components, i.e., state representation, policy optimization, reward formulation, and environment building, and survey RLRS algorithms accordingly. We highlight emerging topics and depict important trends using various graphs and tables. Finally, we discuss important aspects and challenges that can be addressed in the future.", "year": 2021, "publicationdate": "2021-01-15", "externalids": {"DOI": "10.1145/3543846"}, "doi_lower": "10.1145/3543846"}
{"paper_id": 52131984, "title": "From Reinforcement Learning to Deep Reinforcement Learning: An Overview", "author_names": ["Forest Agostinelli", "Guillaume Hocquet", "Sameer Singh", "P. Baldi"], "venue": "Braverman Readings in Machine Learning", "abstract": null, "year": 2017, "publicationdate": null, "externalids": {"DOI": "10.1007/978-3-319-99492-5_13"}, "doi_lower": "10.1007/978-3-319-99492-5_13"}
{"paper_id": 264371210, "title": "Interpretive Summaries, November 2023", "author_names": [], "venue": "Journal of Dairy Science", "abstract": null, "year": 2023, "publicationdate": "2023-11-01", "externalids": {"DOI": "10.1016/s0022-0302(23)00744-0"}, "doi_lower": "10.1016/s0022-0302(23)00744-0"}
{"paper_id": 254974067, "title": "Annotated History of Modern AI and Deep Learning", "author_names": ["Juergen Schmidhuber"], "venue": "arXiv.org", "abstract": "Machine learning (ML) is the science of credit assignment: finding patterns in observations that predict the consequences of actions and help to improve future performance. Credit assignment is also required for human understanding of how the world works, not only for individuals navigating daily life, but also for academic professionals like historians who interpret the present in light of past events. Here I focus on the history of modern artificial intelligence (AI) which is dominated by artificial neural networks (NNs) and deep learning, both conceptually closer to the old field of cybernetics than to what's been called AI since 1956 (e.g., expert systems and logic programming). A modern history of AI will emphasize breakthroughs outside of the focus of traditional AI text books, in particular, mathematical foundations of today's NNs such as the chain rule (1676), the first NNs (linear regression, circa 1800), and the first working deep learners (1965-). From the perspective of 2022, I provide a timeline of the—in hindsight—most important relevant events in the history of NNs, deep learning, AI, computer science, and mathematics in general, crediting those who laid foundations of the field. The text contains numerous hyperlinks to relevant overview sites from my AI Blog. It also debunks certain popular but misleading historic accounts of deep learning, and supplements my previous deep learning survey which provides hundreds of additional 29/12/2022, 12:21 Annotated history of modern AI and deep neural networks https://people.idsia.ch/~juergen/deep-learning-history.html 2/75 references. Finally, to round it off, I'll put things in a broader historic context spanning the time since the Big Bang until when the universe will be many times older than it is now. The present piece is also the draft of a chapter of my upcoming AI book. Disclaimer. Some say a history of deep learning should not be written by someone who has helped to shape it—\"you are part of history not a historian.\" I cannot subscribe to that point of view. Since I seem to know more about deep learning history than others, [DL1-2] I consider it my duty to document and promote this knowledge, even if that seems to imply a conflict of interest, as it means prominently mentioning my own team's work, because (as of 2022) the most cited NNs are based on it. Future AI historians may correct any eraspecific potential bias.", "year": 2022, "publicationdate": "2022-12-21", "externalids": {"DOI": "10.48550/arXiv.2212.11279"}, "doi_lower": "10.48550/arxiv.2212.11279"}
{"paper_id": 16505586, "title": "Preference-Based Policy Learning", "author_names": ["R. Akrour", "Marc Schoenauer", "Michèle Sebag"], "venue": "ECML/PKDD", "abstract": null, "year": 2011, "publicationdate": "2011-09-05", "externalids": {"DOI": "10.1007/978-3-642-23780-5_11"}, "doi_lower": "10.1007/978-3-642-23780-5_11"}
{"paper_id": 47518713, "title": "APRIL: Active Preference-learning based Reinforcement Learning", "author_names": ["R. Akrour", "Marc Schoenauer", "M. Sebag"], "venue": "ECML/PKDD", "abstract": "This paper focuses on reinforcement learning (RL) with limited prior knowledge. In the domain of swarm robotics for instance, the expert can hardly design a reward function or demonstrate the target behavior, forbidding the use of both standard RL and inverse reinforcement learning. Although with a limited expertise, the human expert is still often able to emit preferences and rank the agent demonstrations. Earlier work has presented an iterative preference-based RL framework: expert preferences are exploited to learn an approximate policy return, thus enabling the agent to achieve direct policy search. Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert's ranking feedback enables the agent to refine the approximate policy return, and the process is iterated. \n \nIn this paper, preference-based reinforcement learning is combined with active ranking in order to decrease the number of ranking queries to the expert needed to yield a satisfactory policy. Experiments on the mountain car and the cancer treatment testbeds witness that a couple of dozen rankings enable to learn a competent policy.", "year": 2012, "publicationdate": "2012-08-04", "externalids": {"DOI": "10.1007/978-3-642-33486-3_8"}, "doi_lower": "10.1007/978-3-642-33486-3_8"}
{"paper_id": 263334075, "title": "Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers", "author_names": ["Jide Alaga", "Jonas Schuett"], "venue": "arXiv.org", "abstract": "As artificial intelligence (AI) models are scaled up, new capabilities can emerge unintentionally and unpredictably, some of which might be dangerous. In response, dangerous capabilities evaluations have emerged as a new risk assessment tool. But what should frontier AI developers do if sufficiently dangerous capabilities are in fact discovered? This paper focuses on one possible response: coordinated pausing. It proposes an evaluation-based coordination scheme that consists of five main steps: (1) Frontier AI models are evaluated for dangerous capabilities. (2) Whenever, and each time, a model fails a set of evaluations, the developer pauses certain research and development activities. (3) Other developers are notified whenever a model with dangerous capabilities has been discovered. They also pause related research and development activities. (4) The discovered capabilities are analyzed and adequate safety precautions are put in place. (5) Developers only resume their paused activities if certain safety thresholds are reached. The paper also discusses four concrete versions of that scheme. In the first version, pausing is completely voluntary and relies on public pressure on developers. In the second version, participating developers collectively agree to pause under certain conditions. In the third version, a single auditor evaluates models of multiple developers who agree to pause if any model fails a set of evaluations. In the fourth version, developers are legally required to run evaluations and pause if dangerous capabilities are discovered. Finally, the paper discusses the desirability and feasibility of our proposed coordination scheme. It concludes that coordinated pausing is a promising mechanism for tackling emerging risks from frontier AI models. However, a number of practical and legal obstacles need to be overcome, especially how to avoid violations of antitrust law.", "year": 2023, "publicationdate": "2023-09-30", "externalids": {"DOI": "10.48550/arXiv.2310.00374"}, "doi_lower": "10.48550/arxiv.2310.00374"}
{"paper_id": 9794990, "title": "Understanding intermediate layers using linear classifier probes", "author_names": ["Guillaume Alain", "Yoshua Bengio"], "venue": "International Conference on Learning Representations", "abstract": "Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as \"probes\", trained entirely independently of the model itself. \nThis helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. \nWe apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.", "year": 2016, "publicationdate": "2016-10-05", "externalids": {}, "doi_lower": null}
{"paper_id": 14162568, "title": "A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems", "author_names": ["Stefano V. Albrecht", "S. Ramamoorthy"], "venue": "Adaptive Agents and Multi-Agent Systems", "abstract": "The ad hoc coordination problem is to design an ad hoc agent which is able to achieve optimal flexibility and efficiency in a multiagent system that admits no prior coordination between the ad hoc agent and the other agents. We conceptualise this problem formally as a stochastic Bayesian game in which the behaviour of a player is determined by its type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises a set of user-defined types to characterise players based on their observed behaviours. We evaluate HBA in the level-based foraging domain, showing that it outperforms several alternative algorithms using just a few user-defined types. We also report on a human-machine experiment in which the humans played Prisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms. The results show that HBA achieved equal efficiency but a significantly higher welfare and winning rate.", "year": 2013, "publicationdate": "2013-05-06", "externalids": {}, "doi_lower": null}
{"paper_id": 147794615, "title": "Becoming; Basic Considerations for a Psychology of Personality", "author_names": ["E. Shirk"], "venue": "", "abstract": null, "year": 1957, "publicationdate": null, "externalids": {"DOI": "10.2307/2022446"}, "doi_lower": "10.2307/2022446"}
{"paper_id": 49324194, "title": "Towards Robust Interpretability with Self-Explaining Neural Networks", "author_names": ["David Alvarez-Melis", "T. Jaakkola"], "venue": "Neural Information Processing Systems", "abstract": "Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general – explicitness, faithfulness, and stability – and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.", "year": 2018, "publicationdate": "2018-06-20", "externalids": {}, "doi_lower": null}
{"paper_id": 127197, "title": "Power to the People: The Role of Humans in Interactive Machine Learning", "author_names": ["Saleema Amershi", "M. Cakmak", "W. B. Knox", "Todd Kulesza"], "venue": "The AI Magazine", "abstract": null, "year": 2014, "publicationdate": "2014-12-22", "externalids": {"DOI": "10.1609/AIMAG.V35I4.2513"}, "doi_lower": "10.1609/aimag.v35i4.2513"}
{"paper_id": 256358616, "title": "Reinforcement Learning from Diverse Human Preferences", "author_names": ["Wanqi Xue", "Bo An", "Shuicheng Yan", "Zhongwen Xu"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "The complexity of designing reward functions has been a major obstacle to the wide application of deep reinforcement learning (RL) techniques. Describing an agent's desired behaviors and properties can be difficult, even for experts. A new paradigm called reinforcement learning from human preferences (or preference-based RL) has emerged as a promising solution, in which reward functions are learned from human preference labels among behavior trajectories. However, existing methods for preference-based RL are limited by the need for accurate oracle preference labels. This paper addresses this limitation by developing a method for learning from diverse human preferences. The key idea is to stabilize reward learning through regularization and correction in a latent space. To ensure temporal consistency, a strong constraint is imposed on the reward model that forces its latent space to be close to a non-parameterized distribution. Additionally, a confidence-based reward model ensembling method is designed to generate more stable and reliable predictions. The proposed method is tested on a variety of tasks in DMcontrol and Meta-world and has shown consistent and significant improvements over existing preference-based RL algorithms when learning from diverse feedback, paving the way for real-world applications of RL methods.", "year": 2023, "publicationdate": "2023-01-27", "externalids": {"DOI": "10.48550/arXiv.2301.11774"}, "doi_lower": "10.48550/arxiv.2301.11774"}
{"paper_id": 10242377, "title": "Concrete Problems in AI Safety", "author_names": ["Dario Amodei", "Chris Olah", "J. Steinhardt", "P. Christiano", "John Schulman", "Dandelion Mané"], "venue": "arXiv.org", "abstract": "Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an objective function that is too expensive to evaluate frequently (\"scalable supervision\"), or undesirable behavior during the learning process (\"safe exploration\" and \"distributional shift\"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.", "year": 2016, "publicationdate": "2016-06-21", "externalids": {}, "doi_lower": null}
{"paper_id": 3728967, "title": "Towards better understanding of gradient-based attribution methods for Deep Neural Networks", "author_names": ["Marco Ancona", "Enea Ceolini", "Cengiz Öztireli", "Markus H. Gross"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2017, "publicationdate": "2017-11-16", "externalids": {"DOI": "10.3929/ETHZ-B-000249929"}, "doi_lower": "10.3929/ethz-b-000249929"}
{"paper_id": 259375553, "title": "Frontier AI Regulation: Managing Emerging Risks to Public Safety", "author_names": ["Markus Anderljung", "Joslyn Barnhart", "Anton Korinek", "Jade Leung", "Cullen O'Keefe", "Jess Whittlestone", "S. Avin", "Miles Brundage", "Justin B. Bullock", "D. Cass-Beggs", "Ben Chang", "Tantum Collins", "Tim Fist", "Gillian K. Hadfield", "Alan Hayes", "Lewis Ho", "Sara Hooker", "Eric Horvitz", "Noam Kolt", "Jonas Schuett", "Yonadav Shavit", "Divya Siddarth", "Robert Trager", "Kevin J. Wolf"], "venue": "arXiv.org", "abstract": "Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term\"frontier AI\"models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. Frontier AI models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model's capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deployment of frontier AI models. Industry self-regulation is an important first step. However, wider societal discussions and government intervention will be needed to create standards and to ensure compliance with them. We consider several options to this end, including granting enforcement powers to supervisory authorities and licensure regimes for frontier AI models. Finally, we propose an initial set of safety standards. These include conducting pre-deployment risk assessments; external scrutiny of model behavior; using risk assessments to inform deployment decisions; and monitoring and responding to new information about model capabilities and uses post-deployment. We hope this discussion contributes to the broader conversation on how to balance public safety risks and innovation benefits from advances at the frontier of AI development.", "year": 2023, "publicationdate": "2023-07-06", "externalids": {"DOI": "10.48550/arXiv.2307.03718"}, "doi_lower": "10.48550/arxiv.2307.03718"}
{"paper_id": 17455931, "title": "Towards Machine Ethics: Implementing Two Action-Based Ethical Theories", "author_names": ["Michael Anderson", "S. Anderson", "Chris Armen"], "venue": "", "abstract": null, "year": 2005, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 33329318, "title": "The status of machine ethics: a report from the AAAI Symposium", "author_names": ["Michael Anderson", "S. Anderson"], "venue": "Minds and Machines", "abstract": null, "year": 2007, "publicationdate": "2007-03-01", "externalids": {"DOI": "10.1007/s11023-007-9053-7"}, "doi_lower": "10.1007/s11023-007-9053-7"}
{"paper_id": 254246305, "title": "Language Models as Agent Models", "author_names": ["Jacob Andreas"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in an outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them -- a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of intentional communication in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language. I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- LMs infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.", "year": 2022, "publicationdate": "2022-12-03", "externalids": {"DOI": "10.48550/arXiv.2212.01681"}, "doi_lower": "10.48550/arxiv.2212.01681"}
{"paper_id": 239616022, "title": "SOFT: Softmax-free Transformer with Linear Complexity", "author_names": ["Jiachen Lu", "Jinghan Yao", "Junge Zhang", "Xiatian Zhu", "Hang Xu", "Weiguo Gao", "Chunjing Xu", "T. Xiang", "Li Zhang"], "venue": "Neural Information Processing Systems", "abstract": "Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.", "year": 2021, "publicationdate": "2021-10-22", "externalids": {}, "doi_lower": null}
{"paper_id": 262073277, "title": "Anthropic's Responsible Scaling Policy", "author_names": [], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 265977215, "title": "Medicines update: July 2023.", "author_names": [], "venue": "The Veterinary Record", "abstract": null, "year": 2023, "publicationdate": "2023-09-08", "externalids": {"DOI": "10.1002/vetr.3432"}, "doi_lower": "10.1002/vetr.3432"}
{"paper_id": 270667923, "title": "Claude 3.5 Sonnet Model Card Addendum", "author_names": [], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 270521305, "title": "Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models", "author_names": ["Carson E. Denison", "M. MacDiarmid", "Fazl Barez", "D. Duvenaud", "Shauna Kravec", "Samuel Marks", "Nicholas Schiefer", "Ryan Soklaski", "Alex Tamkin", "Jared Kaplan", "Buck Shlegeris", "Samuel R. Bowman", "Ethan Perez", "Evan Hubinger"], "venue": "arXiv.org", "abstract": "In reinforcement learning, specification gaming occurs when AI systems learn undesired behaviors that are highly rewarded due to misspecified training goals. Specification gaming can range from simple behaviors like sycophancy to sophisticated and pernicious behaviors like reward-tampering, where a model directly modifies its own reward mechanism. However, these more pernicious behaviors may be too complex to be discovered via exploration. In this paper, we study whether Large Language Model (LLM) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. We construct a curriculum of increasingly sophisticated gameable environments and find that training on early-curriculum environments leads to more specification gaming on remaining environments. Strikingly, a small but non-negligible proportion of the time, LLM assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. Retraining an LLM not to game early-curriculum environments mitigates, but does not eliminate, reward-tampering in later environments. Moreover, adding harmlessness training to our gameable environments does not prevent reward-tampering. These results demonstrate that LLMs can generalize from common forms of specification gaming to more pernicious reward tampering and that such behavior may be nontrivial to remove.", "year": 2024, "publicationdate": "2024-06-14", "externalids": {"DOI": "10.48550/arXiv.2406.10162"}, "doi_lower": "10.48550/arxiv.2406.10162"}
{"paper_id": 269149478, "title": "Foundational Challenges in Assuring Alignment and Safety of Large Language Models", "author_names": ["Usman Anwar", "Abulhair Saparov", "Javier Rando", "Daniel Paleka", "Miles Turpin", "Peter Hase", "E. Lubana", "Erik Jenner", "Stephen Casper", "Oliver Sourbut", "Benjamin L. Edelman", "Zhaowei Zhang", "Mario Gunther", "Anton Korinek", "J. Hernández-Orallo", "Lewis Hammond", "Eric J. Bigelow", "Alexander Pan", "L. Langosco", "Tomasz Korbak", "H. Zhang", "Ruiqi Zhong", "Se'an 'O h'Eigeartaigh", "Gabriel Recchia", "Giulio Corsi", "Alan Chan", "Markus Anderljung", "Lilian Edwards", "Y. Bengio", "Danqi Chen", "Samuel Albanie", "Tegan Maharaj", "J. Foerster", "Florian Tramèr", "He He", "Atoosa Kasirzadeh", "Yejin Choi", "David Krueger"], "venue": "arXiv.org", "abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose $200+$ concrete research questions.", "year": 2024, "publicationdate": "2024-04-15", "externalids": {"DOI": "10.48550/arXiv.2404.09932"}, "doi_lower": "10.48550/arxiv.2404.09932"}
{"paper_id": 229339112, "title": "An Update on Recent Geriatric Education Advocacy Efforts", "author_names": ["C. Carrico", "K. Bennett"], "venue": "Innovation in aging", "abstract": "Abstract The National Association for Geriatric Education (NAGE) has maintained consistent education and advocacy efforts since 2006. In recent years NAGE has implemented formal and grassroots advocacy strategies. At the federal level NAGE has increased collaboration with other aging advocacy organizations and coalitions. At the request of Congress, NAGE leadership and stakeholders have testified before Congress and regularly submit testimony to the House and Senate. NAGE staff maintain strong working relationships with congressional staff. Strategies for effective grassroots education and advocacy have been taught to members, and membership has mobilized to educate elected officials about the essential work of the Geriatric Workforce Enhancement Programs across the country. This presentation will provide a thorough review of NAGE’s advocacy work over the past 4 years.", "year": 2020, "publicationdate": "2020-12-16", "externalids": {"DOI": "10.1093/geroni/igaa057.1802"}, "doi_lower": "10.1093/geroni/igaa057.1802"}
{"paper_id": 201107047, "title": "TabNet: Attentive Interpretable Tabular Learning", "author_names": ["Sercan Ö. Arik", "Tomas Pfister"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into its global behavior. Finally, we demonstrate self-supervised learning for tabular data, significantly improving performance when unlabeled data is abundant.", "year": 2019, "publicationdate": "2019-08-20", "externalids": {"DOI": "10.1609/aaai.v35i8.16826"}, "doi_lower": "10.1609/aaai.v35i8.16826"}
{"paper_id": 195820364, "title": "Invariant Risk Minimization", "author_names": ["Martín Arjovsky", "L. Bottou", "Ishaan Gulrajani", "David Lopez-Paz"], "venue": "arXiv.org", "abstract": "We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.", "year": 2019, "publicationdate": "2019-07-05", "externalids": {}, "doi_lower": null}
{"paper_id": 11881009, "title": "Toward Ethical Robots via Mechanized Deontic Logic ∗", "author_names": ["Konstantine Arkoudas", "S. Bringsjord", "Paul Bello"], "venue": "", "abstract": null, "year": 2005, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 147918412, "title": "The AI Debate", "author_names": ["F. Jackson"], "venue": "", "abstract": null, "year": 2001, "publicationdate": "2001-08-01", "externalids": {"DOI": "10.1023/A:1012371125671"}, "doi_lower": "10.1023/a:1012371125671"}
{"paper_id": 253640370, "title": "Racing to the precipice: a model of artificial intelligence development", "author_names": ["S. Armstrong", "N. Bostrom", "Carl Shulman"], "venue": "Ai & Society", "abstract": null, "year": 2015, "publicationdate": "2015-08-01", "externalids": {"DOI": "10.1007/s00146-015-0590-y"}, "doi_lower": "10.1007/s00146-015-0590-y"}
{"paper_id": 9285053, "title": "Linear Algebraic Structure of Word Senses, with Applications to Polysemy", "author_names": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Word embeddings are ubiquitous in NLP and information retrieval, but it is unclear what they represent when the word is polysemous. Here it is shown that multiple word senses reside in linear superposition within the word embedding and simple sparse coding can recover vectors that approximately capture the senses. The success of our approach, which applies to several embedding methods, is mathematically explained using a variant of the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each extracted word sense is accompanied by one of about 2000 “discourse atoms” that gives a succinct description of which other words co-occur with that word sense. Discourse atoms can be of independent interest, and make the method potentially more useful. Empirical tests are used to verify and support the theory.", "year": 2016, "publicationdate": "2016-01-14", "externalids": {"DOI": "10.1162/tacl_a_00034"}, "doi_lower": "10.1162/tacl_a_00034"}
{"paper_id": 49312150, "title": "A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress", "author_names": ["Saurabh Arora", "Prashant Doshi"], "venue": "Artificial Intelligence", "abstract": null, "year": 2018, "publicationdate": "2018-06-18", "externalids": {"DOI": "10.1016/J.ARTINT.2021.103500"}, "doi_lower": "10.1016/j.artint.2021.103500"}
{"paper_id": 155518374, "title": "Social Choice and Individual Values", "author_names": ["J. Kenneth"], "venue": "", "abstract": null, "year": 1952, "publicationdate": "1952-08-01", "externalids": {"DOI": "10.2307/138587"}, "doi_lower": "10.2307/138587"}
{"paper_id": 244799619, "title": "A General Language Assistant as a Laboratory for Alignment", "author_names": ["Amanda Askell", "Yuntao Bai", "Anna Chen", "Dawn Drain", "Deep Ganguli", "T. Henighan", "Andy Jones", "Nicholas Joseph", "Benjamin Mann", "Nova Dassarma", "Nelson Elhage", "Zac Hatfield-Dodds", "Danny Hernandez", "John Kernion", "Kamal Ndousse", "Catherine Olsson", "Dario Amodei", "Tom B. Brown", "Jack Clark", "Sam McCandlish", "Chris Olah", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.", "year": 2021, "publicationdate": "2021-12-01", "externalids": {}, "doi_lower": null}
{"paper_id": 267852801, "title": "Feedback Systems: An Introduction for Scientists and Engineers", "author_names": ["K. J. Åström", "Richard M. Murray"], "venue": "", "abstract": null, "year": 2008, "publicationdate": "2008-04-21", "externalids": {"DOI": "10.2307/j.ctvcm4gdk"}, "doi_lower": "10.2307/j.ctvcm4gdk"}
{"paper_id": 207510150, "title": "Computer-controlled systems: Theory and design : Karl J. Åström and Björn Wittenmark", "author_names": ["C. Johnson"], "venue": "at - Automatisierungstechnik", "abstract": null, "year": 1985, "publicationdate": "1985-11-01", "externalids": {"DOI": "10.1016/0005-1098(85)90050-0"}, "doi_lower": "10.1016/0005-1098(85)90050-0"}
{"paper_id": 252621944, "title": "Evolution of basic human values orientations: An application of monitoring changes in cluster solutions", "author_names": ["Muhammad Atif", "Muhammad Shafiq", "Muhammad Farooq", "G. Ayub", "Mujeeb Hussain", "Muhammad Waqas"], "venue": "PLoS ONE", "abstract": "This study enumerates the evolution of basic human values orientations and the dynamic relationship between them, computed from Schwartz’s value survey conducted in European nations. For this purpose, eight datasets related to the human value scale were extracted from the European Social Survey; each corresponds to a single round conducted cross-sectionally every two years since 2001. Change detection algorithm was implemented to the cluster solutions of temporal datasets, and the evolution of important clusters was traced. Finding of the study reveals that Universalism and Benevolence values are on the rise in European societies in the last couple of decades. Most of the European inhabitants believe in the smooth group functioning and form the organismic needs of cooperation. The people prefer anxiety-free life, and love for nature, environment, humanity, and kindness to other beings in society are essential constructs for them. They avoid self-centred behaviour and prefer social physiognomies.", "year": 2022, "publicationdate": "2022-09-30", "externalids": {"DOI": "10.1371/journal.pone.0274600"}, "doi_lower": "10.1371/journal.pone.0274600"}
{"paper_id": 249329516, "title": "AI4People or People4AI? On human adaptation to AI at work", "author_names": ["Emma Engström", "K. Jebari"], "venue": "Ai & Society", "abstract": null, "year": 2022, "publicationdate": "2022-06-02", "externalids": {"DOI": "10.1007/s00146-022-01464-5"}, "doi_lower": "10.1007/s00146-022-01464-5"}
{"paper_id": 39898986, "title": "Global overview of Imitation Learning", "author_names": ["Alexandre Attia", "Sharone Dayan"], "venue": "arXiv.org", "abstract": "Imitation Learning is a sequential task where the learner tries to mimic an expert's action in order to achieve the best performance. Several algorithms have been proposed recently for this task. In this project, we aim at proposing a wide review of these algorithms, presenting their main features and comparing them on their performance and their regret bounds.", "year": 2018, "publicationdate": "2018-01-19", "externalids": {}, "doi_lower": null}
{"paper_id": 53029241, "title": "The Moral Machine experiment", "author_names": ["E. Awad", "Sohan Dsouza", "Richard Kim", "Jonathan F. Schulz", "J. Henrich", "A. Shariff", "Jean‐François Bonnefon", "Iyad Rahwan"], "venue": "Nature", "abstract": null, "year": 2018, "publicationdate": "2018-10-24", "externalids": {"DOI": "10.1038/s41586-018-0637-6"}, "doi_lower": "10.1038/s41586-018-0637-6"}
{"paper_id": 264288854, "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences", "author_names": ["M. G. Azar", "Mark Rowland", "Bilal Piot", "Daniel Guo", "Daniele Calandriello", "Michal Valko", "Rémi Munos"], "venue": "International Conference on Artificial Intelligence and Statistics", "abstract": "The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation. In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an in-depth analysis of the behavior of RLHF and DPO (as special cases of $\\Psi$PO) and to identify their potential pitfalls. We then consider another special case for $\\Psi$PO by setting $\\Psi$ simply to Identity, for which we can derive an efficient optimisation procedure, prove performance guarantees and demonstrate its empirical superiority to DPO on some illustrative examples.", "year": 2023, "publicationdate": "2023-10-18", "externalids": {"DOI": "10.48550/arXiv.2310.12036"}, "doi_lower": "10.48550/arxiv.2310.12036"}
{"paper_id": 11212020, "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author_names": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "International Conference on Learning Representations", "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.", "year": 2014, "publicationdate": "2014-09-01", "externalids": {}, "doi_lower": null}
{"paper_id": 231749855, "title": "Recent Advances in Adversarial Training for Adversarial Robustness", "author_names": ["Tao Bai", "Jinqi Luo", "Jun Zhao", "B. Wen", "Qian Wang"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Adversarial training is one of the most effective approaches for deep learning models to defend against adversarial examples.\n\nUnlike other defense strategies, adversarial training aims to enhance the robustness of models intrinsically.\n\nDuring the past few years, adversarial training has been studied and discussed from various aspects, which deserves a comprehensive review.\n\nFor the first time in this survey, we systematically review the recent progress on adversarial training for adversarial robustness with a novel taxonomy.\n\nThen we discuss the generalization problems in adversarial training from three perspectives and highlight the challenges which are not fully tackled.\n\nFinally, we present potential future directions.", "year": 2021, "publicationdate": "2021-02-02", "externalids": {"DOI": "10.24963/ijcai.2021/591"}, "doi_lower": "10.24963/ijcai.2021/591"}
{"paper_id": 248118878, "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "author_names": ["Yuntao Bai", "Andy Jones", "Kamal Ndousse", "Amanda Askell", "Anna Chen", "Nova Dassarma", "Dawn Drain", "Stanislav Fort", "Deep Ganguli", "T. Henighan", "Nicholas Joseph", "Saurav Kadavath", "John Kernion", "Tom Conerly", "S. El-Showk", "Nelson Elhage", "Zac Hatfield-Dodds", "Danny Hernandez", "Tristan Hume", "Scott Johnston", "Shauna Kravec", "Liane Lovitt", "Neel Nanda", "Catherine Olsson", "Dario Amodei", "Tom B. Brown", "Jack Clark", "Sam McCandlish", "Chris Olah", "Benjamin Mann", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "We apply preference modeling and reinforcement learning from human feedback (RLHF) to ﬁnetune language models to act as helpful and harmless assistants. We ﬁnd this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efﬁciently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work. Figure These plots show that PM accuracy decreases as we focus exclusively on comparisons between pairs of samples with high score. We have normalized all preference models to have the same mean score on a held-out dataset so that they’re directly comparable, and then plotted accuracy for the comparisons where both samples have scores above a speciﬁc threshold.", "year": 2022, "publicationdate": "2022-04-12", "externalids": {"DOI": "10.48550/arXiv.2204.05862"}, "doi_lower": "10.48550/arxiv.2204.05862"}
{"paper_id": 254823489, "title": "Constitutional AI: Harmlessness from AI Feedback", "author_names": ["Yuntao Bai", "Saurav Kadavath", "Sandipan Kundu", "Amanda Askell", "John Kernion", "Andy Jones", "A. Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "Carol Chen", "Catherine Olsson", "Chris Olah", "Danny Hernandez", "Dawn Drain", "Deep Ganguli", "Dustin Li", "Eli Tran-Johnson", "E. Perez", "Jamie Kerr", "J. Mueller", "Jeffrey Ladish", "J. Landau", "Kamal Ndousse", "Kamilė Lukošiūtė", "Liane Lovitt", "M. Sellitto", "Nelson Elhage", "Nicholas Schiefer", "Noem'i Mercado", "Nova Dassarma", "R. Lasenby", "Robin Larson", "Sam Ringer", "Scott Johnston", "Shauna Kravec", "S. E. Showk", "Stanislav Fort", "Tamera Lanham", "Timothy Telleen-Lawton", "Tom Conerly", "T. Henighan", "Tristan Hume", "Sam Bowman", "Zac Hatfield-Dodds", "Benjamin Mann", "Dario Amodei", "Nicholas Joseph", "Sam McCandlish", "Tom B. Brown", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.", "year": 2022, "publicationdate": "2022-12-15", "externalids": {"DOI": "10.48550/arXiv.2212.08073"}, "doi_lower": "10.48550/arxiv.2212.08073"}
{"paper_id": 10738655, "title": "A Framework for Behavioural Cloning", "author_names": ["Michael Bain", "C. Sammut"], "venue": "Machine Intelligence 15", "abstract": null, "year": 1995, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 3659224, "title": "Learning from Physical Human Corrections, One Feature at a Time", "author_names": ["Andrea V. Bajcsy", "Dylan P. Losey", "M. O'Malley", "A. Dragan"], "venue": "IEEE/ACM International Conference on Human-Robot Interaction", "abstract": null, "year": 2018, "publicationdate": "2018-02-26", "externalids": {"DOI": "10.1145/3171221.3171267"}, "doi_lower": "10.1145/3171221.3171267"}
{"paper_id": 249953673, "title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos", "author_names": ["Bowen Baker", "Ilge Akkaya", "P. Zhokhov", "Joost Huizinga", "Jie Tang", "Adrien Ecoffet", "Brandon Houghton", "Raul Sampedro", "J. Clune"], "venue": "Neural Information Processing Systems", "abstract": "Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.", "year": 2022, "publicationdate": "2022-06-23", "externalids": {"DOI": "10.48550/arXiv.2206.11795"}, "doi_lower": "10.48550/arxiv.2206.11795"}
{"paper_id": 254043997, "title": "Fine-tuning language models to find agreement among humans with diverse preferences", "author_names": ["Michiel A. Bakker", "Martin Chadwick", "Hannah Sheahan", "Michael Henry Tessler", "Lucy Campbell-Gillingham", "Jan Balaguer", "Nat McAleese", "Amelia Glaese", "John Aslanides", "M. Botvinick", "C. Summerfield"], "venue": "Neural Information Processing Systems", "abstract": "Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user. This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a a single\"generic\"user will confer more general alignment. Here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? We fine-tune a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g.,\"should we raise taxes on the rich?\"), and rate the LLM's generated candidate consensus statements for agreement and quality. A reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. The model produces consensus statements that are preferred by human users over those from prompted LLMs (>70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions (>65%). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another.", "year": 2022, "publicationdate": "2022-11-28", "externalids": {"DOI": "10.48550/arXiv.2211.15006"}, "doi_lower": "10.48550/arxiv.2211.15006"}
{"paper_id": 2805176, "title": "Robot See, Robot Do : An Overview of Robot Imitation", "author_names": ["Paul Bakker"], "venue": "", "abstract": null, "year": 1996, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 235435759, "title": "Revisiting Model Stitching to Compare Neural Representations", "author_names": ["Yamini Bansal", "Preetum Nakkiran", "B. Barak"], "venue": "Neural Information Processing Systems", "abstract": "We revisit and extend model stitching (Lenc&Vedaldi 2015) as a methodology to study the internal representations of neural networks. Given two trained and frozen models $A$ and $B$, we consider a\"stitched model'' formed by connecting the bottom-layers of $A$ to the top-layers of $B$, with a simple trainable layer between them. We argue that model stitching is a powerful and perhaps under-appreciated tool, which reveals aspects of representations that measures such as centered kernel alignment (CKA) cannot. Through extensive experiments, we use model stitching to obtain quantitative verifications for intuitive statements such as\"good networks learn similar representations'', by demonstrating that good networks of the same architecture, but trained in very different ways (e.g.: supervised vs. self-supervised learning), can be stitched to each other without drop in performance. We also give evidence for the intuition that\"more is better'' by showing that representations learnt with (1) more data, (2) bigger width, or (3) more training time can be\"plugged in'' to weaker models to improve performance. Finally, our experiments reveal a new structural property of SGD which we call\"stitching connectivity'', akin to mode-connectivity: typical minima reached by SGD can all be stitched to each other with minimal change in accuracy.", "year": 2021, "publicationdate": "2021-06-14", "externalids": {}, "doi_lower": null}
{"paper_id": 58007470, "title": "The Problem of Time's Passage: Using Historical Arguments to Inform the Debate", "author_names": ["C. Brewer"], "venue": "International Journal of Technoethics", "abstract": "While the B-theory of time seems to fit with the current physical theory, it also seems to require treating temporal passage as an illusion. The aim of this article is to show that by understanding cases of apparent motion in a particular way, one can maintain the B-theory while also retaining the privileged status that the phenomenon of temporal passage plays in human experience. However, to understand these cases correctly, one should turn to arguments in the history of philosophy. More specifically, arguments from Russell, Kant and Hume can be used to make the B-theory more plausible.", "year": 2019, "publicationdate": "2019-01-01", "externalids": {"DOI": "10.4018/IJT.2019010103"}, "doi_lower": "10.4018/ijt.2019010103"}
{"paper_id": 233403216, "title": "A survey on artificial intelligence assurance", "author_names": ["Feras A. Batarseh", "Laura J. Freeman", "Chih-hao Huang"], "venue": "Journal of Big Data", "abstract": "Artificial Intelligence (AI) algorithms are increasingly providing decision making and operational support across multiple domains. AI includes a wide (and growing) library of algorithms that could be applied for different problems. One important notion for the adoption of AI algorithms into operational decision processes is the concept of assurance. The literature on assurance, unfortunately, conceals its outcomes within a tangled landscape of conflicting approaches, driven by contradicting motivations, assumptions, and intuitions. Accordingly, albeit a rising and novel area, this manuscript provides a systematic review of research works that are relevant to AI assurance, between years 1985 and 2021, and aims to provide a structured alternative to the landscape. A new AI assurance definition is adopted and presented, and assurance methods are contrasted and tabulated. Additionally, a ten-metric scoring system is developed and introduced to evaluate and compare existing methods. Lastly, in this manuscript, we provide foundational insights, discussions, future directions, a roadmap, and applicable recommendations for the development and deployment of AI assurance.", "year": 2021, "publicationdate": "2021-04-26", "externalids": {"DOI": "10.1186/s40537-021-00445-7"}, "doi_lower": "10.1186/s40537-021-00445-7"}
{"paper_id": 49744838, "title": "Recognition in Terra Incognita", "author_names": ["Sara Beery", "Grant Van Horn", "P. Perona"], "venue": "European Conference on Computer Vision", "abstract": "It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.(The dataset is available at https://beerys.github.io/CaltechCameraTraps/)", "year": 2018, "publicationdate": "2018-07-13", "externalids": {"DOI": "10.1007/978-3-030-01270-0_28"}, "doi_lower": "10.1007/978-3-030-01270-0_28"}
{"paper_id": 246485360, "title": "Imitation Learning by Estimating Expertise of Demonstrators", "author_names": ["M. Beliaev", "Andy Shih", "Stefano Ermon", "Dorsa Sadigh", "Ramtin Pedarsani"], "venue": "International Conference on Machine Learning", "abstract": "Many existing imitation learning datasets are collected from multiple demonstrators, each with different expertise at different parts of the environment. Yet, standard imitation learning algorithms typically treat all demonstrators as homogeneous, regardless of their expertise, absorbing the weaknesses of any suboptimal demonstrators. In this work, we show that unsupervised learning over demonstrator expertise can lead to a consistent boost in the performance of imitation learning algorithms. We develop and optimize a joint model over a learned policy and expertise levels of the demonstrators. This enables our model to learn from the optimal behavior and filter out the suboptimal behavior of each demonstrator. Our model learns a single policy that can outperform even the best demonstrator, and can be used to estimate the expertise of any demonstrator at any state. We illustrate our findings on real-robotic continuous control tasks from Robomimic and discrete environments such as MiniGrid and chess, out-performing competing methods in $21$ out of $23$ settings, with an average of $7\\%$ and up to $60\\%$ improvement in terms of the final reward.", "year": 2022, "publicationdate": "2022-02-02", "externalids": {}, "doi_lower": null}
{"paper_id": 52922804, "title": "AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias", "author_names": ["R. Bellamy", "K. Dey", "M. Hind", "Samuel C. Hoffman", "Stephanie Houde", "Kalapriya Kannan", "P. Lohia", "Jacquelyn Martino", "S. Mehta", "A. Mojsilovic", "Seema Nagar", "K. Ramamurthy", "John T. Richards", "Diptikalyan Saha", "P. Sattigeri", "Moninder Singh", "Kush R. Varshney", "Yunfeng Zhang"], "venue": "arXiv.org", "abstract": "Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license {this https URL). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. \nThe package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (this https URL) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.", "year": 2018, "publicationdate": "2018-10-03", "externalids": {}, "doi_lower": null}
{"paper_id": 257504984, "title": "Eliciting Latent Predictions from Transformers with the Tuned Lens", "author_names": ["Nora Belrose", "Zach Furman", "Logan Smith", "Danny Halawi", "Igor V. Ostrovsky", "Lev McKinney", "Stella Biderman", "J. Steinhardt"], "venue": "arXiv.org", "abstract": "We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the tuned lens, is a refinement of the earlier\"logit lens\"technique, which yielded useful insights but is often brittle. We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.", "year": 2023, "publicationdate": "2023-03-14", "externalids": {"DOI": "10.48550/arXiv.2303.08112"}, "doi_lower": "10.48550/arxiv.2303.08112"}
{"paper_id": 14416576, "title": "[11] A. Ben-Tal and A. Nemirovski. Robust solutions of uncertain linear", "author_names": [], "venue": "", "abstract": null, "year": 2004, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 262580630, "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜", "author_names": ["Emily M. Bender", "Timnit Gebru", "Angelina McMillan-Major", "Shmargaret Shmitchell"], "venue": "Conference on Fairness, Accountability and Transparency", "abstract": "The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.", "year": 2021, "publicationdate": "2021-03-03", "externalids": {"DOI": "10.1145/3442188.3445922"}, "doi_lower": "10.1145/3442188.3445922"}
{"paper_id": 58237949, "title": "How a Young Claimant May Arise: a Medico-Legal Warning", "author_names": ["T. Balfour"], "venue": "", "abstract": null, "year": 1880, "publicationdate": "1880-02-14", "externalids": {"DOI": "10.1136/bmj.1.998.241-a"}, "doi_lower": "10.1136/bmj.1.998.241-a"}
{"paper_id": 258274564, "title": "Response to the March 2023 'Pause Giant AI Experiments: An Open Letter' by Yoshua Bengio, signed by Stuart Russell, Elon Musk, Steve Wozniak, Yuval Noah Harari and others…", "author_names": ["Jim Samuel"], "venue": "Social Science Research Network", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.2139/ssrn.4412516"}, "doi_lower": "10.2139/ssrn.4412516"}
{"paper_id": 5637274, "title": "Formalizing Convergent Instrumental Goals", "author_names": ["Tsvi Benson-Tilsen", "N. Soares"], "venue": "AAAI Workshop: AI, Ethics, and Society", "abstract": null, "year": 2016, "publicationdate": "2016-03-29", "externalids": {}, "doi_lower": null}
{"paper_id": 232046113, "title": "Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling", "author_names": ["Gregory W. Benton", "Wesley J. Maddox", "Sanae Lotfi", "A. Wilson"], "venue": "International Conference on Machine Learning", "abstract": "With a better understanding of the loss surfaces for multilayer networks, we can build more robust and accurate training procedures. Recently it was discovered that independently trained SGD solutions can be connected along one-dimensional paths of near-constant training loss. In this paper, we show that there are mode-connecting simplicial complexes that form multi-dimensional manifolds of low loss, connecting many independently trained models. Inspired by this discovery, we show how to efficiently build simplicial complexes for fast ensembling, outperforming independently trained deep ensembles in accuracy, calibration, and robustness to dataset shift. Notably, our approach only requires a few training epochs to discover a low-loss simplex, starting from a pre-trained solution. Code is available at https://github.com/g-benton/loss-surface-simplexes.", "year": 2021, "publicationdate": "2021-02-25", "externalids": {}, "doi_lower": null}
{"paper_id": 247596835, "title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning", "author_names": ["Hugo Elias Berg", "S. Hall", "Yash Bhalgat", "Wonsuk Yang", "Hannah Rose Kirk", "Aleksandar Shtedritski", "Max Bain"], "venue": "AACL", "abstract": "Vision-language models can encode societal biases and stereotypes, but there are challenges to measuring and mitigating these multimodal harms due to lacking measurement robustness and feature degradation. To address these challenges, we investigate bias measures and apply ranking metrics for image-text representations. We then investigate debiasing methods and show that prepending learned embeddings to text queries that are jointly trained with adversarial debiasing and a contrastive loss, reduces various bias measures with minimal degradation to the image-text representation.", "year": 2022, "publicationdate": "2022-03-22", "externalids": {"DOI": "10.48550/arXiv.2203.11933"}, "doi_lower": "10.48550/arxiv.2203.11933"}
{"paper_id": 203609613, "title": "ilastik: interactive machine learning for (bio)image analysis", "author_names": ["S. Berg", "D. Kutra", "Thorben Kroeger", "C. Straehle", "Bernhard X. Kausler", "Carsten Haubold", "Martin Schiegg", "J. Ales", "T. Beier", "Markus Rudy", "Kemal Eren", "Jaime I Cervantes", "Buote Xu", "Fynn Beuttenmueller", "A. Wolny", "Chong Zhang", "U. Köthe", "F. Hamprecht", "A. Kreshuk"], "venue": "Nature Methods", "abstract": null, "year": 2019, "publicationdate": "2019-09-30", "externalids": {"DOI": "10.1038/s41592-019-0582-9"}, "doi_lower": "10.1038/s41592-019-0582-9"}
{"paper_id": 261529981, "title": "Taken out of context: On measuring situational awareness in LLMs", "author_names": ["Lukas Berglund", "Asa Cooper Stickland", "Mikita Balesni", "Max Kaufmann", "Meg Tong", "Tomasz Korbak", "Daniel Kokotajlo", "Owain Evans"], "venue": "arXiv.org", "abstract": "We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-of-context reasoning task. Their success is sensitive to the training setup and only works when we apply data augmentation. For both GPT-3 and LLaMA-1, performance improves with model size. These findings offer a foundation for further empirical study, towards predicting and potentially controlling the emergence of situational awareness in LLMs. Code is available at: https://github.com/AsaCooperStickland/situational-awareness-evals.", "year": 2023, "publicationdate": "2023-09-01", "externalids": {"DOI": "10.48550/arXiv.2309.00667"}, "doi_lower": "10.48550/arxiv.2309.00667"}
{"paper_id": 12641090, "title": "A Convex Framework for Fair Regression", "author_names": ["R. Berk", "Hoda Heidari", "S. Jabbari", "Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Seth Neel", "Aaron Roth"], "venue": "arXiv.org", "abstract": "We introduce a flexible family of fairness regularizers for (linear and logistic) regression problems. These regularizers all enjoy convexity, permitting fast optimization, and they span the rang from notions of group fairness to strong individual fairness. By varying the weight on the fairness regularizer, we can compute the efficient frontier of the accuracy-fairness trade-off on any given dataset, and we measure the severity of this trade-off via a numerical quantity we call the Price of Fairness (PoF). The centerpiece of our results is an extensive comparative study of the PoF across six different datasets in which fairness is a primary consideration.", "year": 2017, "publicationdate": "2017-06-07", "externalids": {}, "doi_lower": null}
{"paper_id": 12924416, "title": "Fairness in Criminal Justice Risk Assessments: The State of the Art", "author_names": ["R. Berk", "Hoda Heidari", "S. Jabbari", "Michael Kearns", "Aaron Roth"], "venue": "Sociological Methods & Research", "abstract": "Objectives: Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this article, we seek to clarify the trade-offs between different kinds of fairness and between fairness and accuracy. Methods: We draw on the existing literatures in criminology, computer science, and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments. Results: We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy. Conclusions: Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging trade-offs. These lessons apply to applications well beyond criminology where assessments of risk can be used by decision makers. Examples include mortgage lending, employment, college admissions, child welfare, and medical diagnoses.", "year": 2017, "publicationdate": "2017-03-27", "externalids": {"DOI": "10.1177/0049124118782533"}, "doi_lower": "10.1177/0049124118782533"}
{"paper_id": 28994350, "title": "A Declarative Modular Framework for Representing and Applying Ethical Principles", "author_names": ["Fiona Berreby", "Gauvain Bourgne", "J. Ganascia"], "venue": "Adaptive Agents and Multi-Agent Systems", "abstract": null, "year": 2017, "publicationdate": "2017-05-08", "externalids": {}, "doi_lower": null}
{"paper_id": 249890193, "title": "Beyond IID: data-driven decision-making in heterogeneous environments", "author_names": ["Omar Besbes", "Will Ma", "Omar Mouchtaki"], "venue": "Neural Information Processing Systems", "abstract": "How should one leverage historical data when past observations are not perfectly indicative of the future, for example, because of the presence of unobserved confounders which one cannot “correct” for? Motivated by this question, we study a data-driven decision-making framework in which historical samples are generated from unknown and different distributions assumed to lie in a heterogeneity ball with known radius and centered around the (also) unknown future (out-of-sample) distribution on which the performance of a decision will be evaluated. This work aims to analyze the performance of central data-driven policies and also near-optimal ones in these heterogeneous environments, and it aims to understand key drivers of performance. We establish a first result that allows us to upper bound the asymptotic worst-case regret of a broad class of policies. Leveraging this result, for any integral probability metric, we provide a general analysis of the performance achieved by sample average approximation (SAA) as a function of the radius of the heterogeneity ball. This analysis is centered around the approximation parameter, a notion of complexity we introduce to capture how the interplay between the heterogeneity and the problem structure impacts the performance of SAA. In turn, we illustrate, through several widely studied problems—for example, newsvendor, pricing—how this methodology can be applied and find that the performance of SAA varies considerably depending on the combinations of problem classes and heterogeneity. The failure of SAA for certain instances motivates the design of alternative policies to achieve rate optimality. We derive problem-dependent policies achieving strong guarantees for the illustrative problems described above and provide initial results toward a principled approach for the design and analysis of general rate-optimal algorithms. This paper was accepted by Vivek Farias, data science. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2022.03448 .", "year": 2022, "publicationdate": "2022-06-20", "externalids": {"DOI": "10.48550/arXiv.2206.09642"}, "doi_lower": "10.48550/arxiv.2206.09642"}
{"paper_id": 22050710, "title": "AI safety via debate", "author_names": ["G. Irving", "P. Christiano", "Dario Amodei"], "venue": "arXiv.org", "abstract": "To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.", "year": 2018, "publicationdate": "2018-05-02", "externalids": {}, "doi_lower": null}
{"paper_id": 213304602, "title": "Unrestricted Adversarial Examples via Semantic Manipulation", "author_names": ["Anand Bhattad", "Min Jin Chong", "Kaizhao Liang", "B. Li", "D. Forsyth"], "venue": "International Conference on Learning Representations", "abstract": "Machine learning models, especially deep neural networks (DNNs), have been shown to be vulnerable against \\emph{adversarial examples} which are carefully crafted samples with a small magnitude of the perturbation. Such adversarial perturbations are usually restricted by bounding their $\\mathcal{L}_p$ norm such that they are imperceptible, and thus many current defenses can exploit this property to reduce their adversarial impact. In this paper, we instead introduce \"unrestricted\" perturbations that manipulate semantically meaningful image-based visual descriptors -- color and texture -- in order to generate effective and photorealistic adversarial examples. We show that these semantically aware perturbations are effective against JPEG compression, feature squeezing and adversarially trained model. We also show that the proposed methods can effectively be applied to both image classification and image captioning tasks on complex datasets such as ImageNet and MSCOCO. In addition, we conduct comprehensive user studies to show that our generated semantic adversarial examples are photorealistic to humans despite large magnitude perturbations when compared to other attacks.", "year": 2019, "publicationdate": "2019-04-12", "externalids": {}, "doi_lower": null}
{"paper_id": 271931441, "title": "Accurate medium-range global weather forecasting with 3D neural networks", "author_names": ["Kaifeng Bi", "Lingxi Xie", "Hengheng Zhang", "Xin Chen", "Xiaotao Gu", "Q. Tian"], "venue": "Nature", "abstract": "Three-dimensional deep neural networks can be trained to forecast global weather patterns, including extreme weather, with accuracy greater than or equal to that of the best numerical weather prediction models. Weather forecasting is important for science and society. At present, the most accurate forecast system is the numerical weather prediction (NWP) method, which represents atmospheric states as discretized grids and numerically solves partial differential equations that describe the transition between those states^ 1 . However, this procedure is computationally expensive. Recently, artificial-intelligence-based methods^ 2 have shown potential in accelerating weather forecasting by orders of magnitude, but the forecast accuracy is still significantly lower than that of NWP methods. Here we introduce an artificial-intelligence-based method for accurate, medium-range global weather forecasting. We show that three-dimensional deep networks equipped with Earth-specific priors are effective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39 years of global data, our program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the world’s best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts (ECMWF)^ 3 . Our method also works well with extreme weather forecasts and ensemble forecasts. When initialized with reanalysis data, the accuracy of tracking tropical cyclones is also higher than that of ECMWF-HRES.", "year": 2023, "publicationdate": "2023-07-05", "externalids": {"DOI": "10.1038/s41586-023-06185-3"}, "doi_lower": "10.1038/s41586-023-06185-3"}
{"paper_id": 224916310, "title": "Safety assurance mechanisms of collaborative robotic systems in manufacturing", "author_names": ["Z. Bi", "C. Luo", "Zhonghua Miao", "Bing Zhang", "W. J. Zhang", "Lihui Wang"], "venue": "Robotics Comput. Integr. Manuf.", "abstract": null, "year": 2021, "publicationdate": "2021-02-01", "externalids": {"DOI": "10.1016/j.rcim.2020.102022"}, "doi_lower": "10.1016/j.rcim.2020.102022"}
{"paper_id": 281817834, "title": "On the Framework Act on AI", "author_names": ["Kwangsoo Kim"], "venue": "Sogang Law Journal", "abstract": null, "year": 2025, "publicationdate": "2025-10-31", "externalids": {"DOI": "10.35505/slj.2025.10.14.3.45"}, "doi_lower": "10.35505/slj.2025.10.14.3.45"}
{"paper_id": 1704893, "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings", "author_names": ["Tolga Bolukbasi", "Kai-Wei Chang", "James Y. Zou", "Venkatesh Saligrama", "A. Kalai"], "venue": "Neural Information Processing Systems", "abstract": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.", "year": 2016, "publicationdate": "2016-07-21", "externalids": {}, "doi_lower": null}
{"paper_id": 14402473, "title": "Agent-based modeling: Methods and techniques for simulating human systems", "author_names": ["E. Bonabeau"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "abstract": null, "year": 2002, "publicationdate": "2002-05-14", "externalids": {"DOI": "10.1073/PNAS.082080899"}, "doi_lower": "10.1073/pnas.082080899"}
{"paper_id": 7445963, "title": "The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents", "author_names": ["N. Bostrom"], "venue": "Minds and Machines", "abstract": null, "year": 2012, "publicationdate": "2012-05-01", "externalids": {"DOI": "10.1007/s11023-012-9281-3"}, "doi_lower": "10.1007/s11023-012-9281-3"}
{"paper_id": 154703652, "title": "Existential Risk Prevention as Global Priority", "author_names": ["N. Bostrom"], "venue": "", "abstract": null, "year": 2013, "publicationdate": "2013-02-01", "externalids": {"DOI": "10.1111/1758-5899.12002"}, "doi_lower": "10.1111/1758-5899.12002"}
{"paper_id": 129001024, "title": "Global Catastrophic Risks", "author_names": ["N. Bostrom", "M. Ćirković"], "venue": "", "abstract": null, "year": 2008, "publicationdate": null, "externalids": {"DOI": "10.5860/choice.46-6152"}, "doi_lower": "10.5860/choice.46-6152"}
{"paper_id": 142196435, "title": "Unlearning the lie;: Sexism in school", "author_names": ["B. Harrison"], "venue": "", "abstract": null, "year": 1973, "publicationdate": null, "externalids": {"DOI": "10.2307/816078"}, "doi_lower": "10.2307/816078"}
{"paper_id": 253384413, "title": "Measuring Progress on Scalable Oversight for Large Language Models", "author_names": ["Sam Bowman", "Jeeyoon Hyun", "Ethan Perez", "Edwin Chen", "Craig Pettit", "Scott Heiner", "Kamilė Lukošiūtė", "Amanda Askell", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "C. McKinnon", "Chris Olah", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "John Kernion", "Jamie Kerr", "J. Mueller", "Jeffrey Ladish", "J. Landau", "Kamal Ndousse", "Liane Lovitt", "Nelson Elhage", "Nicholas Schiefer", "Nicholas Joseph", "Noem'i Mercado", "Nova Dassarma", "Robin Larson", "Sam McCandlish", "S. Kundu", "Scott Johnston", "Shauna Kravec", "S. E. Showk", "Stanislav Fort", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Benjamin Mann", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.", "year": 2022, "publicationdate": "2022-11-04", "externalids": {"DOI": "10.48550/arXiv.2211.03540"}, "doi_lower": "10.48550/arxiv.2211.03540"}
{"paper_id": 261531274, "title": "Beyond Shared Autonomy: Joint Perception and Action for Human-In-The-Loop Mobile Robot Navigation Systems", "author_names": ["Hamed Bozorgi", "T. Ngo"], "venue": "Journal of Intelligent and Robotic Systems", "abstract": null, "year": 2023, "publicationdate": "2023-09-01", "externalids": {"DOI": "10.1007/s10846-023-01942-y"}, "doi_lower": "10.1007/s10846-023-01942-y"}
{"paper_id": 125209808, "title": "Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons", "author_names": ["R. A. Bradley", "M. E. Terry"], "venue": "", "abstract": null, "year": 1952, "publicationdate": "1952-12-01", "externalids": {"DOI": "10.2307/2334029"}, "doi_lower": "10.2307/2334029"}
{"paper_id": 5624909, "title": "Handbook of Computational Social Choice", "author_names": ["F. Brandt", "Vincent Conitzer", "U. Endriss", "Jérôme Lang", "Ariel D. Procaccia"], "venue": "", "abstract": "The rapidly growing field of computational social choice, at the intersection of computer science and economics, deals with the computational aspects of collective decision making. This handbook, written by thirty-six prominent members of the computational social choice community, covers the field comprehensively. Chapters devoted to each of the field's major themes offer detailed introductions. Topics include voting theory (such as the computational complexity of winner determination and manipulation in elections), fair allocation (such as algorithms for dividing divisible and indivisible goods), coalition formation (such as matching and hedonic games), and many more. Graduate students, researchers, and professionals in computer science, economics, mathematics, political science, and philosophy will benefit from this accessible and self-contained book.", "year": 2016, "publicationdate": "2016-04-25", "externalids": {"DOI": "10.1017/CBO9781107446984"}, "doi_lower": "10.1017/cbo9781107446984"}
{"paper_id": 125668105, "title": "Decomposing guided wavefields with dictionary learning", "author_names": ["J. Harley", "K. S. Alguri"], "venue": "", "abstract": null, "year": 2016, "publicationdate": "2016-11-18", "externalids": {"DOI": "10.1121/1.4969489"}, "doi_lower": "10.1121/1.4969489"}
{"paper_id": 242550042, "title": "3. Using the arXiv by Greg Kuperberg", "author_names": ["G. Kuperberg"], "venue": "Notices of the American Mathematical Society", "abstract": null, "year": 2020, "publicationdate": "2020-02-01", "externalids": {"DOI": "10.1090/noti2022"}, "doi_lower": "10.1090/noti2022"}
{"paper_id": 211252404, "title": "Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences", "author_names": ["Daniel S. Brown", "Russell Coleman", "R. Srinivasan", "S. Niekum"], "venue": "International Conference on Machine Learning", "abstract": "Bayesian reward learning from demonstrations enables rigorous safety and uncertainty analysis when performing imitation learning. However, Bayesian reward learning methods are typically computationally intractable for complex control problems. We propose Bayesian Reward Extrapolation (Bayesian REX), a highly efficient Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference. Bayesian REX can learn to play Atari games from demonstrations, without access to the game score and can generate 100,000 samples from the posterior over reward functions in only 5 minutes on a personal laptop. Bayesian REX also results in imitation learning performance that is competitive with or better than state-of-the-art methods that only learn point estimates of the reward function. Finally, Bayesian REX enables efficient high-confidence policy evaluation without having access to samples of the reward function. These high-confidence performance bounds can be used to rank the performance and risk of a variety of evaluation policies and provide a way to detect reward hacking behaviors.", "year": 2020, "publicationdate": "2020-02-21", "externalids": {}, "doi_lower": null}
{"paper_id": 119111734, "title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations", "author_names": ["Daniel S. Brown", "Wonjoon Goo", "P. Nagarajan", "S. Niekum"], "venue": "International Conference on Machine Learning", "abstract": "A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.", "year": 2019, "publicationdate": "2019-04-12", "externalids": {}, "doi_lower": null}
{"paper_id": 218971783, "title": "Language Models are Few-Shot Learners", "author_names": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "R. Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Ma-teusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "I. Sutskever", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "year": 2020, "publicationdate": "2020-05-28", "externalids": {}, "doi_lower": null}
{"paper_id": 215768885, "title": "Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims", "author_names": ["Miles Brundage", "S. Avin", "Jasmine Wang", "Haydn Belfield", "Gretchen Krueger", "Gillian K. Hadfield", "Heidy Khlaaf", "Jingying Yang", "H. Toner", "Ruth Fong", "Tegan Maharaj", "Pang Wei Koh", "Sara Hooker", "Jade Leung", "Andrew Trask", "Emma Bluemke", "Jonathan Lebensbold", "Cullen O'Keefe", "Mark Koren", "T. Ryffel", "JB Rubinovitz", "T. Besiroglu", "F. Carugati", "Jack Clark", "P. Eckersley", "Sarah de Haas", "Maritza L. Johnson", "B. Laurie", "A. Ingerman", "Igor Krawczuk", "Amanda Askell", "Rosario Cammarota", "A. Lohn", "David Krueger", "Charlotte Stix", "Peter Henderson", "L. Graham", "Carina Prunkl", "Bianca Martin", "Elizabeth Seger", "Noa Zilberman", "Se'an 'O h'Eigeartaigh", "F. Kroeger", "Girish Sastry", "R. Kagan", "Adrian Weller", "Brian Tse", "Elizabeth Barnes", "A. Dafoe", "P. Scharre", "Ariel Herbert-Voss", "Martijn Rasser", "Shagun Sodhani", "Carrick Flynn", "T. Gilbert", "Lisa Dyer", "Saif Khan", "Yoshua Bengio", "Markus Anderljung"], "venue": "arXiv.org", "abstract": "With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.", "year": 2020, "publicationdate": "2020-04-15", "externalids": {}, "doi_lower": null}
{"paper_id": 257663729, "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4", "author_names": ["Sébastien Bubeck", "Varun Chandrasekaran", "Ronen Eldan", "J. Gehrke", "Eric Horvitz", "Ece Kamar", "Peter Lee", "Y. Lee", "Yuan-Fang Li", "Scott M. Lundberg", "Harsha Nori", "Hamid Palangi", "Marco Tulio Ribeiro", "Yi Zhang"], "venue": "arXiv.org", "abstract": "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.", "year": 2023, "publicationdate": "2023-03-22", "externalids": {}, "doi_lower": null}
{"paper_id": 261557206, "title": "Deep Reinforcement Learning from Hierarchical Weak Preference Feedback", "author_names": ["Alexander Bukharin", "Yixiao Li", "Pengcheng He", "Weizhu Chen", "Tuo Zhao"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2309.02632"}, "doi_lower": "10.48550/arxiv.2309.02632"}
{"paper_id": 3298854, "title": "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification", "author_names": ["Joy Buolamwini", "Timnit Gebru"], "venue": "FAT", "abstract": null, "year": 2018, "publicationdate": "2018-01-21", "externalids": {}, "doi_lower": null}
{"paper_id": 266312608, "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision", "author_names": ["Collin Burns", "Pavel Izmailov", "J. Kirchner", "Bowen Baker", "Leo Gao", "Leopold Aschenbrenner", "Yining Chen", "Adrien Ecoffet", "Manas R. Joglekar", "Jan Leike", "I. Sutskever", "Jeff Wu", "OpenAI"], "venue": "International Conference on Machine Learning", "abstract": "Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior - for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.", "year": 2023, "publicationdate": "2023-12-14", "externalids": {"DOI": "10.48550/arXiv.2312.09390"}, "doi_lower": "10.48550/arxiv.2312.09390"}
{"paper_id": 254366253, "title": "Discovering Latent Knowledge in Language Models Without Supervision", "author_names": ["Collin Burns", "Haotian Ye", "D. Klein", "J. Steinhardt"], "venue": "International Conference on Learning Representations", "abstract": "Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.", "year": 2022, "publicationdate": "2022-12-07", "externalids": {"DOI": "10.48550/arXiv.2212.03827"}, "doi_lower": "10.48550/arxiv.2212.03827"}
{"paper_id": 54178920, "title": "Reinforcement learning for control: Performance, stability, and deep approximators", "author_names": ["L. Buşoniu", "T. D. Bruin", "D. Tolić", "Jens Kober", "Ivana Palunko"], "venue": "Annual Reviews in Control", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {"DOI": "10.1016/J.ARCONTROL.2018.09.005"}, "doi_lower": "10.1016/j.arcontrol.2018.09.005"}
{"paper_id": 211032272, "title": "Scaling data-driven robotics with reward sketching and batch reinforcement learning", "author_names": ["Serkan Cabi", "Sergio Gomez Colmenarejo", "Alexander Novikov", "Ksenia Konyushkova", "Scott E. Reed", "Rae Jeong", "Konrad Zolna", "Y. Aytar", "D. Budden", "Mel Vecerík", "Oleg O. Sushkov", "David Barker", "Jonathan Scholz", "Misha Denil", "Nando de Freitas", "Ziyun Wang"], "venue": "Robotics: Science and Systems", "abstract": "We present a framework for data-driven robotics that makes use of a large dataset of recorded robot experience and scales to several tasks using learned reward functions. We show how to apply this framework to accomplish three different object manipulation tasks on a real robot platform. Given demonstrations of a task together with task-agnostic recorded experience, we use a special form of human annotation as supervision to learn a reward function, which enables us to deal with real-world tasks where the reward signal cannot be acquired directly. Learned rewards are used in combination with a large dataset of experience from different tasks to learn a robot policy offline using batch RL. We show that using our approach it is possible to train agents to perform a variety of challenging manipulation tasks including stacking rigid objects and handling cloth.", "year": 2019, "publicationdate": "2019-09-26", "externalids": {"DOI": "10.15607/rss.2020.xvi.076"}, "doi_lower": "10.15607/rss.2020.xvi.076"}
{"paper_id": 256697444, "title": "Zeno: An Interactive Framework for Behavioral Evaluation of Machine Learning", "author_names": ["Ángel Alexander Cabrera", "Erica Fu", "Donald Bertucci", "Kenneth Holstein", "Ameet Talwalkar", "Jason I. Hong", "Adam Perer"], "venue": "International Conference on Human Factors in Computing Systems", "abstract": "Machine learning models with high accuracy on test data can still produce systematic failures, such as harmful biases and safety issues, when deployed in the real world. To detect and mitigate such failures, practitioners run behavioral evaluation of their models, checking model outputs for specific types of inputs. Behavioral evaluation is important but challenging, requiring that practitioners discover real-world patterns and validate systematic failures. We conducted 18 semi-structured interviews with ML practitioners to better understand the challenges of behavioral evaluation and found that it is a collaborative, use-case-first process that is not adequately supported by existing task- and domain-specific tools. Using these findings, we designed zeno, a general-purpose framework for visualizing and testing AI systems across diverse use cases. In four case studies with participants using zeno on real-world models, we found that practitioners were able to reproduce previous manual analyses and discover new systematic failures.", "year": 2023, "publicationdate": "2023-02-09", "externalids": {"DOI": "10.1145/3544548.3581268"}, "doi_lower": "10.1145/3544548.3581268"}
{"paper_id": 269043093, "title": "High-Dimension Human Value Representation in Large Language Models", "author_names": ["Samuel Cahyawijaya", "Delong Chen", "Yejin Bang", "Leila Khalatbari", "Bryan Wilie", "Ziwei Ji", "Etsuko Ishii", "Pascale Fung"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "The widespread application of LLMs across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, there is an urgent need to understand the scope and nature of human values injected into these LLMs before their deployment and adoption. We propose UniVaR, a high-dimensional neural representation of symbolic human value distributions in LLMs, orthogonal to model architecture and training data. This is a continuous and scalable representation, self-supervised from the value-relevant output of 8 LLMs and evaluated on 15 open-source and commercial LLMs. Through UniVaR, we visualize and explore how LLMs prioritize different values in 25 languages and cultures, shedding light on complex interplay between human values and language modeling.", "year": 2024, "publicationdate": "2024-04-11", "externalids": {"DOI": "10.48550/arXiv.2404.07900"}, "doi_lower": "10.48550/arxiv.2404.07900"}
{"paper_id": 202600721, "title": "Stanford Center for AI Safety", "author_names": ["Clark W. Barrett", "D. Dill", "Mykel J. Kochenderfer", "Dorsa Sadigh"], "venue": "", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 23163324, "title": "Semantics derived automatically from language corpora contain human-like biases", "author_names": ["Aylin Caliskan", "J. Bryson", "Arvind Narayanan"], "venue": "Science", "abstract": "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.", "year": 2016, "publicationdate": "2016-08-25", "externalids": {"DOI": "10.1126/science.aal4230"}, "doi_lower": "10.1126/science.aal4230"}
{"paper_id": 213778462, "title": "Advancing impact assessment for intelligent systems", "author_names": ["R. Calvo", "D. Peters", "S. Cave"], "venue": "Nature Machine Intelligence", "abstract": null, "year": 2020, "publicationdate": "2020-02-01", "externalids": {"DOI": "10.1038/s42256-020-0151-z"}, "doi_lower": "10.1038/s42256-020-0151-z"}
{"paper_id": 277585611, "title": "AI-Driven Fraud Detection: Leveraging Machine Learning for Scam Identification", "author_names": ["Alex Mathew", "Tsi Fofang"], "venue": "International Journal of Innovative Research in Science Engineering and Technology", "abstract": "Fraud detection has become one of the main challenges in cybersecurity, financial security, and online\nshopping. Traditional rule-based fraud detection systems are no longer sufficient to struggle against ever-evolving fraud\nmethods. Machine learning (ML) provides an adaptive, data-driven approach for detecting fraudulent activity in real\ntime by learning patterns and detecting outliers. This article explores the main ML approaches in fraud detection:\nsupervised learning (logistic regression, decision trees, gradient boosting, and deep learning), unsupervised learning\n(clustering, autoencoders, and isolation forests), and hybrid models. It also mentions the issues related to the source of\nthe data, feature engineering, challenges such as data imbalance and concept drift, and future research directions such\nas explainable AI, adversarial machine learning defenses, and real-time fraud detection using Edge AI. The result\nindicates that fraud detection based on AI greatly enhances accuracy, scalability, and adaptability and is, hence, a\ncritical tool in modern-day cybersecurity", "year": 2025, "publicationdate": "2025-04-30", "externalids": {"DOI": "10.15680/ijirset.2025.1404002"}, "doi_lower": "10.15680/ijirset.2025.1404002"}
{"paper_id": 259262181, "title": "Are aligned neural networks adversarially aligned?", "author_names": ["Nicholas Carlini", "Milad Nasr", "Christopher A. Choquette-Choo", "Matthew Jagielski", "Irena Gao", "Anas Awadalla", "Pang Wei Koh", "Daphne Ippolito", "Katherine Lee", "Florian Tramèr", "Ludwig Schmidt"], "venue": "Neural Information Processing Systems", "abstract": "Large language models are now tuned to align with the goals of their creators, namely to be\"helpful and harmless.\"These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study adversarial alignment, and ask to what extent these models remain aligned when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.", "year": 2023, "publicationdate": "2023-06-26", "externalids": {"DOI": "10.48550/arXiv.2306.15447"}, "doi_lower": "10.48550/arxiv.2306.15447"}
{"paper_id": 249146809, "title": "Is Power-Seeking AI an Existential Risk?", "author_names": ["Joseph Carlsmith"], "venue": "arXiv.org", "abstract": "This report examines what I see as the core argument for concern about existential risk from misaligned artificial intelligence. I proceed in two stages. First, I lay out a backdrop picture that informs such concern. On this picture, intelligent agency is an extremely powerful force, and creating agents much more intelligent than us is playing with fire -- especially given that if their objectives are problematic, such agents would plausibly have instrumental incentives to seek power over humans. Second, I formulate and evaluate a more specific six-premise argument that creating agents of this kind will lead to existential catastrophe by 2070. On this argument, by 2070: (1) it will become possible and financially feasible to build relevantly powerful and agentic AI systems; (2) there will be strong incentives to do so; (3) it will be much harder to build aligned (and relevantly powerful/agentic) AI systems than to build misaligned (and relevantly powerful/agentic) AI systems that are still superficially attractive to deploy; (4) some such misaligned systems will seek power over humans in high-impact ways; (5) this problem will scale to the full disempowerment of humanity; and (6) such disempowerment will constitute an existential catastrophe. I assign rough subjective credences to the premises in this argument, and I end up with an overall estimate of ~5% that an existential catastrophe of this kind will occur by 2070. (May 2022 update: since making this report public in April 2021, my estimate here has gone up, and is now at>10%.)", "year": 2022, "publicationdate": "2022-06-16", "externalids": {"DOI": "10.48550/arXiv.2206.13353"}, "doi_lower": "10.48550/arxiv.2206.13353"}
{"paper_id": 1201201, "title": "Increasing robotic wheelchair safety with collaborative control: Evidence from secondary task experiments", "author_names": ["T. Carlson", "Y. Demiris"], "venue": "IEEE International Conference on Robotics and Automation", "abstract": null, "year": 2010, "publicationdate": "2010-05-03", "externalids": {"DOI": "10.1109/ROBOT.2010.5509257"}, "doi_lower": "10.1109/robot.2010.5509257"}
{"paper_id": 173187842, "title": "Unlabeled Data Improves Adversarial Robustness", "author_names": ["Y. Carmon", "Aditi Raghunathan", "Ludwig Schmidt", "Percy Liang", "John C. Duchi"], "venue": "Neural Information Processing Systems", "abstract": "We demonstrate, theoretically and empirically, that adversarial robustness can significantly benefit from semisupervised learning. Theoretically, we revisit the simple Gaussian model of Schmidt et al. that shows a sample complexity gap between standard and robust classification. We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high standard accuracy. Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i) $\\ell_\\infty$ robustness against several strong attacks via adversarial training and (ii) certified $\\ell_2$ and $\\ell_\\infty$ robustness via randomized smoothing. On SVHN, adding the dataset's own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels.", "year": 2019, "publicationdate": "2019-05-31", "externalids": {}, "doi_lower": null}
{"paper_id": 281018100, "title": "Zip those lips.", "author_names": ["L. Jerrold"], "venue": "American Journal of Orthodontics and Dentofacial Orthopedics", "abstract": null, "year": 2025, "publicationdate": "2025-09-01", "externalids": {"DOI": "10.1016/j.ajodo.2025.06.001"}, "doi_lower": "10.1016/j.ajodo.2025.06.001"}
{"paper_id": 259991411, "title": "Deceptive Alignment Monitoring", "author_names": ["Andres Carranza", "Dhruv Pai", "Rylan Schaeffer", "Arnuv Tandon", "Oluwasanmi Koyejo"], "venue": "arXiv.org", "abstract": "As the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. The threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the AI Safety&Alignment communities. Consequently, we call this new direction Deceptive Alignment Monitoring. In this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. We conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.", "year": 2023, "publicationdate": "2023-07-20", "externalids": {"DOI": "10.48550/arXiv.2307.10569"}, "doi_lower": "10.48550/arxiv.2307.10569"}
{"paper_id": 257557658, "title": "Characterizing Manipulation from AI Systems", "author_names": ["Micah Carroll", "Alan Chan", "Hal Ashton", "David Krueger"], "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization", "abstract": "Manipulation is a concern in many domains, such as social media, advertising, and chatbots. As AI systems mediate more of our digital interactions, it is important to understand the degree to which AI systems might manipulate humans without the intent of the system designers. Our work clarifies challenges in defining and measuring this kind of manipulation from AI systems. Firstly, we build upon prior literature on manipulation and characterize the space of possible notions of manipulation, which we find to depend upon the concepts of incentives, intent, covertness, and harm. We review proposals on how to operationalize each concept and we outline challenges in including each concept in a definition of manipulation. Second, we discuss the connections between manipulation and related concepts, such as deception and coercion. We then analyze how our characterization of manipulation applies to recommender systems and language models, and give a brief overview of the regulation of manipulation in other domains. While some progress has been made in defining and measuring manipulation from AI systems, many gaps remain. In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that AI systems learn to manipulate humans without the intent of the system designers. Manipulation could pose a significant threat to human autonomy and precautionary actions to mitigate it are likely warranted.", "year": 2023, "publicationdate": "2023-03-16", "externalids": {"DOI": "10.1145/3617694.3623226"}, "doi_lower": "10.1145/3617694.3623226"}
{"paper_id": 248391966, "title": "Estimating and Penalizing Induced Preference Shifts in Recommender Systems", "author_names": ["Micah Carroll", "Dylan Hadfield-Menell", "Stuart J. Russell", "A. Dragan"], "venue": "International Conference on Machine Learning", "abstract": "The content that a recommender system (RS) shows to users inﬂuences them. Therefore, when choosing a recommender to deploy, one is implicitly also choosing to induce speciﬁc internal states in users. Even more, systems trained via long-horizon optimization will have direct incentives to manipulate users: in this work, we focus on the incentive to shift user preferences so they are easier to satisfy. We argue that – before deployment – system designers should: estimate the shifts a recommender would induce; evaluate whether such shifts would be undesirable; and perhaps even actively optimize to avoid problematic shifts. These steps involve two challenging ingredients: estimation requires anticipating how hypothetical algorithms would inﬂuence user preferences if deployed – we do this by using historical user interaction data to train a predictive user model which implicitly contains their preference dynamics; evaluation and optimization additionally require metrics to assess whether such inﬂuences are manipulative or otherwise unwanted – we use the notion of “safe shifts”, that deﬁne a trust region within which behavior is safe: for instance, the natural way in which users would shift without interference from the system could be deemed “safe”. In simulated experiments, we show that our learned preference dynamics model is effective in estimating user preferences and how they would respond to new recommenders. Additionally, we show that recommenders that optimize for staying in the trust region can avoid manipulative behaviors while still generating engagement.", "year": 2022, "publicationdate": "2022-04-25", "externalids": {"DOI": "10.48550/arXiv.2204.11966"}, "doi_lower": "10.48550/arxiv.2204.11966"}
{"paper_id": 96829019, "title": "IAN WHYTE ARMSTRONG", "author_names": ["E. Richards"], "venue": "", "abstract": null, "year": 2002, "publicationdate": "2002-08-01", "externalids": {"DOI": "10.1046/j.1471-0307.2002.00068.x"}, "doi_lower": "10.1046/j.1471-0307.2002.00068.x"}
{"paper_id": 14190268, "title": "Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission", "author_names": ["R. Caruana", "Yin Lou", "J. Gehrke", "Paul Koch", "M. Sturm", "Noémie Elhadad"], "venue": "Knowledge Discovery and Data Mining", "abstract": null, "year": 2015, "publicationdate": "2015-08-10", "externalids": {"DOI": "10.1145/2783258.2788613"}, "doi_lower": "10.1145/2783258.2788613"}
{"paper_id": 199659548, "title": "Machine Learning Interpretability: A Survey on Methods and Metrics", "author_names": ["D. V. Carvalho", "E. M. Pereira", "Jaime S. Cardoso"], "venue": "Electronics", "abstract": "Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.", "year": 2019, "publicationdate": "2019-07-26", "externalids": {"DOI": "10.3390/ELECTRONICS8080832"}, "doi_lower": "10.3390/electronics8080832"}
{"paper_id": 168619043, "title": "Moving Forward in the WTO post-Bali", "author_names": ["B. Hoekman"], "venue": "", "abstract": null, "year": 2014, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 259313811, "title": "Red Teaming Deep Neural Networks with Feature Synthesis Tools", "author_names": ["Stephen Casper", "Yuxiao Li", "Jiawei Li", "Tong Bu", "Ke Zhang", "K. Hariharan", "Dylan Hadfield-Menell"], "venue": "Neural Information Processing Systems", "abstract": "Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified previously unknown bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze model behavior by using a particular dataset. This only allows for the study of the model in the context of features that the user can sample in advance. To address this, a growing body of research involves interpreting models using \\emph{feature synthesis} methods that do not depend on a dataset. In this paper, we benchmark the usefulness of interpretability tools on debugging tasks. Our key insight is that we can implant human-interpretable trojans into models and then evaluate these tools based on whether they can help humans discover them. This is analogous to finding OOD bugs, except the ground truth is known, allowing us to know when an interpretation is correct. We make four contributions. (1) We propose trojan discovery as an evaluation task for interpretability tools and introduce a benchmark with 12 trojans of 3 different types. (2) We demonstrate the difficulty of this benchmark with a preliminary evaluation of 16 state-of-the-art feature attribution/saliency tools. Even under ideal conditions, given direct access to data with the trojan trigger, these methods still often fail to identify bugs. (3) We evaluate 7 feature-synthesis methods on our benchmark. (4) We introduce and evaluate 2 new variants of the best-performing method from the previous evaluation. A website for this paper and its code is at https://benchmarking-interpretability.csail.mit.edu/", "year": 2023, "publicationdate": "2023-02-08", "externalids": {}, "doi_lower": null}
{"paper_id": 260316010, "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback", "author_names": ["Stephen Casper", "Xander Davies", "Claudia Shi", "T. Gilbert", "J'er'emy Scheurer", "Javier Rando", "Rachel Freedman", "Tomasz Korbak", "David Lindner", "Pedro J Freire", "Tony Wang", "Samuel Marks", "Charbel-Raphaël Ségerie", "Micah Carroll", "Andi Peng", "Phillip J. K. Christoffersen", "Mehul Damani", "Stewart Slocum", "Usman Anwar", "Anand Siththaranjan", "Max Nadeau", "Eric J. Michaud", "J. Pfau", "Dmitrii Krasheninnikov", "Xin Chen", "L. Langosco", "Peter Hase", "Erdem Biyik", "A. Dragan", "David Krueger", "Dorsa Sadigh", "Dylan Hadfield-Menell"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.", "year": 2023, "publicationdate": "2023-07-27", "externalids": {}, "doi_lower": null}
{"paper_id": 259187620, "title": "Explore, Establish, Exploit: Red Teaming Language Models from Scratch", "author_names": ["Stephen Casper", "Jason Lin", "Joe Kwon", "Gatlen Culp", "Dylan Hadfield-Menell"], "venue": "arXiv.org", "abstract": "Deploying large language models (LMs) can pose hazards from harmful outputs such as toxic or false text. Prior work has introduced automated tools that elicit harmful outputs to identify these risks. While this is a valuable step toward securing models, these approaches rely on a pre-existing way to efficiently classify undesirable outputs. Using a pre-existing classifier does not allow for red-teaming to be tailored to the target model. Furthermore, when failures can be easily classified in advance, red-teaming has limited marginal value because problems can be avoided by simply filtering training data and/or model outputs. Here, we consider red-teaming\"from scratch,\"in which the adversary does not begin with a way to classify failures. Our framework consists of three steps: 1) Exploring the model's range of behaviors in the desired context; 2) Establishing a definition and measurement for undesired behavior (e.g., a classifier trained to reflect human evaluations); and 3) Exploiting the model's flaws using this measure to develop diverse adversarial prompts. We use this approach to red-team GPT-3 to discover classes of inputs that elicit false statements. In doing so, we construct the CommonClaim dataset of 20,000 statements labeled by humans as common-knowledge-true, common knowledge-false, or neither. We are making code and data available.", "year": 2023, "publicationdate": "2023-06-16", "externalids": {"DOI": "10.48550/arXiv.2306.09442"}, "doi_lower": "10.48550/arxiv.2306.09442"}
{"paper_id": 249282208, "title": "Robust Feature-Level Adversaries are Interpretability Tools", "author_names": ["Stephen Casper", "Max Nadeau", "Dylan Hadfield-Menell", "G. Kreiman"], "venue": "Neural Information Processing Systems", "abstract": "The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create\"feature-level\"adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing\"copy/paste\"attacks in which one natural image is pasted into another to cause a targeted misclassification. Our results suggest that feature-level attacks are a promising approach for rigorous interpretability research. They support the design of tools to better understand what a model has learned and diagnose brittle feature associations. Code is available at https://github.com/thestephencasper/feature_level_adv", "year": 2021, "publicationdate": "2021-10-07", "externalids": {}, "doi_lower": null}
{"paper_id": 220128348, "title": "Evaluation of Text Generation: A Survey", "author_names": ["Asli Celikyilmaz", "Elizabeth Clark", "Jianfeng Gao"], "venue": "arXiv.org", "abstract": "The paper surveys evaluation methods of natural language generation (NLG) systems that have been developed in the last few years. We group NLG evaluation methods into three categories: (1) human-centric evaluation metrics, (2) automatic metrics that require no training, and (3) machine-learned metrics. For each category, we discuss the progress that has been made and the challenges still being faced, with a focus on the evaluation of recently proposed NLG tasks and neural NLG models. We then present two case studies of automatic text summarization and long text generation, and conclude the paper by proposing future research directions.", "year": 2020, "publicationdate": "2020-06-26", "externalids": {}, "doi_lower": null}
{"paper_id": 233783448, "title": "A survey on adversarial attacks and defences", "author_names": ["Anirban Chakraborty", "Manaar Alam", "Vishal Dey", "A. Chattopadhyay", "Debdeep Mukhopadhyay"], "venue": "CAAI Transactions on Intelligence Technology", "abstract": "Deep learning has evolved as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. The advancement of deep learning has been so radical that today it can surpass human ‐ level performance. As a consequence, deep learning is being extensively used in most of the recent day ‐ to ‐ day applications. However, efficient deep learning systems can be jeopardised by using crafted adversarial samples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where ad-versaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. Herein, the authors attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate on the efficiency and challenges of recent countermeasures against them", "year": 2021, "publicationdate": "2021-03-01", "externalids": {"DOI": "10.1049/CIT2.12028"}, "doi_lower": "10.1049/cit2.12028"}
{"paper_id": 260683028, "title": "PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback", "author_names": ["Souradip Chakraborty", "A. S. Bedi", "Alec Koppel", "Dinesh Manocha", "Huazheng Wang", "Furong Huang", "Mengdi Wang"], "venue": "International Conference on Learning Representations", "abstract": "We present a novel unified bilevel optimization-based framework, \\textsf{PARL}, formulated to address the recently highlighted critical issue of policy alignment in reinforcement learning using utility or preference-based feedback. We identify a major gap within current algorithmic designs for solving policy alignment due to a lack of precise characterization of the dependence of the alignment objective on the data generated by policy trajectories. This shortfall contributes to the sub-optimal performance observed in contemporary algorithms. Our framework addressed these concerns by explicitly parameterizing the distribution of the upper alignment objective (reward design) by the lower optimal variable (optimal policy for the designed reward). Interestingly, from an optimization perspective, our formulation leads to a new class of stochastic bilevel problems where the stochasticity at the upper objective depends upon the lower-level variable. {True to our best knowledge, this work presents the first formulation of the RLHF as a bilevel optimization problem which generalizes the existing RLHF formulations and addresses the existing distribution shift issues in RLHF formulations.} To demonstrate the efficacy of our formulation in resolving alignment issues in RL, we devised an algorithm named \\textsf{A-PARL} to solve PARL problem, establishing sample complexity bounds of order $\\mathcal{O}(1/T)$. Our empirical results substantiate that the proposed \\textsf{PARL} can address the alignment concerns in RL by showing significant improvements (up to 63\\% in terms of required samples) for policy alignment in large-scale environments of the Deepmind control suite and Meta world tasks.", "year": 2023, "publicationdate": "2023-08-03", "externalids": {}, "doi_lower": null}
{"paper_id": 257050680, "title": "Harms from Increasingly Agentic Algorithmic Systems", "author_names": ["Alan Chan", "Rebecca Salganik", "Alva Markelius", "Chris Pang", "Nitarshan Rajkumar", "Dmitrii Krasheninnikov", "L. Langosco", "Zhonghao He", "Yawen Duan", "Micah Carroll", "Michelle Lin", "A. Mayhew", "Katherine Collins", "Maryam Molamohammadi", "John Burden", "Wanru Zhao", "Shalaleh Rismani", "Konstantinos Voudouris", "Umang Bhatt", "Adrian Weller", "David Krueger", "Tegan Maharaj"], "venue": "Conference on Fairness, Accountability and Transparency", "abstract": "Research in Fairness, Accountability, Transparency, and Ethics (FATE)1 has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed, typically without strong regulatory barriers, threatening the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms, rather than just responding to them. Anticipation of harms is especially important given the rapid pace of developments in machine learning (ML). Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency – notably, these include systemic and/or long-range impacts, often on marginalized or unconsidered stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems.", "year": 2023, "publicationdate": "2023-02-20", "externalids": {"DOI": "10.1145/3593013.3594033"}, "doi_lower": "10.1145/3593013.3594033"}
{"paper_id": 62882093, "title": "Balancing Between Open and Closed", "author_names": ["Tanja Aitamurto"], "venue": "", "abstract": null, "year": 2013, "publicationdate": "2013-05-09", "externalids": {"DOI": "10.1080/21670811.2012.750150"}, "doi_lower": "10.1080/21670811.2012.750150"}
{"paper_id": 229297908, "title": "Transformer Interpretability Beyond Attention Visualization", "author_names": ["Hila Chefer", "Shir Gur", "Lior Wolf"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the Deep Taylor Decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods. Our code is available at: https://github.com/hila-chefer/Transformer-Explainability.", "year": 2020, "publicationdate": "2020-12-17", "externalids": {"DOI": "10.1109/CVPR46437.2021.00084"}, "doi_lower": "10.1109/cvpr46437.2021.00084"}
{"paper_id": 248476121, "title": "Learning from Natural Language Feedback", "author_names": ["J'er'emy Scheurer", "Jon Ander Campos", "Jun Shern Chan", "Angelica Chen", "Kyunghyun Cho", "Ethan Perez"], "venue": "Trans. Mach. Learn. Res.", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2204.14146"}, "doi_lower": "10.48550/arxiv.2204.14146"}
{"paper_id": 235755472, "title": "Evaluating Large Language Models Trained on Code", "author_names": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Pondé", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "Scott Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mo Bavarian", "Clemens Winter", "P. Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "Jan Leike", "Josh Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "I. Sutskever", "Wojciech Zaremba"], "venue": "arXiv.org", "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.", "year": 2021, "publicationdate": "2021-07-07", "externalids": {}, "doi_lower": null}
{"paper_id": 258762314, "title": "Content-based Unrestricted Adversarial Attack", "author_names": ["Zhaoyu Chen", "Bo Li", "Shuang Wu", "Kaixun Jiang", "Shouhong Ding", "Wenqiang Zhang"], "venue": "Neural Information Processing Systems", "abstract": "Unrestricted adversarial attacks typically manipulate the semantic content of an image (e.g., color or texture) to create adversarial examples that are both effective and photorealistic, demonstrating their ability to deceive human perception and deep neural networks with stealth and success. However, current works usually sacrifice unrestricted degrees and subjectively select some image content to guarantee the photorealism of unrestricted adversarial examples, which limits its attack performance. To ensure the photorealism of adversarial examples and boost attack performance, we propose a novel unrestricted attack framework called Content-based Unrestricted Adversarial Attack. By leveraging a low-dimensional manifold that represents natural images, we map the images onto the manifold and optimize them along its adversarial direction. Therefore, within this framework, we implement Adversarial Content Attack based on Stable Diffusion and can generate high transferable unrestricted adversarial examples with various adversarial contents. Extensive experimentation and visualization demonstrate the efficacy of ACA, particularly in surpassing state-of-the-art attacks by an average of 13.3-50.4% and 16.8-48.0% in normally trained models and defense methods, respectively.", "year": 2023, "publicationdate": "2023-05-18", "externalids": {"DOI": "10.48550/arXiv.2305.10665"}, "doi_lower": "10.48550/arxiv.2305.10665"}
{"paper_id": 4758915, "title": "Graded Multilabel Classification: The Ordinal Case", "author_names": ["Weiwei Cheng", "K. Dembczynski", "Eyke Hüllermeier"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 2010, "publicationdate": "2010-06-21", "externalids": {}, "doi_lower": null}
{"paper_id": 18821712, "title": "Label Ranking Methods based on the Plackett-Luce Model", "author_names": ["Weiwei Cheng", "K. Dembczynski", "Eyke Hüllermeier"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 2010, "publicationdate": "2010-06-21", "externalids": {}, "doi_lower": null}
{"paper_id": 4743430, "title": "Predicting Partial Orders: Ranking with Abstention", "author_names": ["Weiwei Cheng", "M. Rademaker", "B. Baets", "Eyke Hüllermeier"], "venue": "ECML/PKDD", "abstract": null, "year": 2010, "publicationdate": "2010-09-20", "externalids": {"DOI": "10.1007/978-3-642-15880-3_20"}, "doi_lower": "10.1007/978-3-642-15880-3_20"}
{"paper_id": 258070012, "title": "Marketing with ChatGPT: Navigating the Ethical Terrain of GPT-Based Chatbot Technology", "author_names": ["Pablo Rivas"], "venue": "Applied Informatics", "abstract": "ChatGPT is an AI-powered chatbot platform that enables human users to converse with machines. It utilizes natural language processing and machine learning algorithms, transforming how people interact with AI technology. ChatGPT offers significant advantages over previous similar tools, and its potential for application in various fields has generated attention and anticipation. However, some experts are wary of ChatGPT, citing ethical implications. Therefore, this paper shows that ChatGPT has significant potential to transform marketing and shape its future if certain ethical considerations are taken into account. First, we argue that ChatGPT-based tools can help marketers create content faster and potentially with quality similar to human content creators. It can also assist marketers in conducting more efficient research and understanding customers better, automating customer service, and improving efficiency. Then we discuss ethical implications and potential risks for marketers, consumers, and other stakeholders, that are essential for ChatGPT-based marketing; doing so can help revolutionize marketing while avoiding potential harm to stakeholders.", "year": 2023, "publicationdate": "2023-04-10", "externalids": {"DOI": "10.3390/ai4020019"}, "doi_lower": "10.3390/ai4020019"}
{"paper_id": 253505740, "title": "The Alignment Problem: Machine Learning and Human Values", "author_names": ["Brian R. Christian"], "venue": "Perspectives on Science and Christian Faith", "abstract": "THE ALIGNMENT PROBLEM: Machine Learning and Human Values by Brian Christian. New York: W. W. Norton, 2020. 344 pages. Hardcover; $28.95. ISBN: 9780393635829. *The global conversation about artificial intelligence (AI) is increasingly polemic--\"AI will change the world!\" \"AI will ruin the world!\" Amidst the strife, Brian Christian's work stands out. It is thoughtful, nuanced, and, at times, even poetic. Coming on the heels of his two other bestsellers, The Most Human Human and Algorithms to Live By, this meticulously researched recounting of the last decade of research into AI safety provides a broad perspective of the field and its future. *The \"alignment problem\" in the title refers to the disconnect between what AI does and what we want it to do. In Christian's words, it is the disconnect between \"machine learning and human values.\" This disconnect has been the subject of intense research in recent years, as both companies and academics continually discover that AIs inherit the mistakes and biases of their creators. *For example, we train AIs that predict recidivism rates of convicted criminals in hopes of crafting more accurate sentences. However, the AIs produce racially biased outcomes. Or, we train AIs which map words into mathematical spaces. These AIs can perform mathematical \"computations\" on words, such as \"king - man + woman = queen\" and \"Paris - France + Italy = Rome.\" But they also say that \"doctor - man + woman = nurse\" and \"computer programmer - man + woman = homemaker.\" These examples of racial and gender bias are some of the numerous ways that human bias appears inside the supposedly impartial tools we have created. *As Norbert Wiener, a famous mathematician in the mid-twentieth century, put it, \"We had better be sure the purpose put into the machine is the purpose which we really desire\" (p. 312). The discoveries of the last ten years have shocked researchers into realizing that our machines have purposes we never intended. Christian's message is clear: these mistakes must be fixed before those machines become a fixed part of our everyday lives. *The book is divided into three main sections. The first, Prophecy, provides a historical overview of how researchers uncovered the AI biases that are now well known. It traces the origins of how AI models ended up in the public sphere and the history of how people have tried to solve the problems AI creates. Perhaps one of the most interesting anecdotes in this section is about how researchers try to create explainable models to comply with GDPR requirements. *The second section, Agency, explores the alignment problem in the context of reinforcement learning. Reinforcement learning involves teaching computer \"agents\" (aka AIs) to perform certain tasks using complex reward systems. Time and time again, the reward systems that researchers create have unintended side effects, and Christian recounts numerous humorous examples of this. He explains in simple terms why it is so difficult to correctly motivate the behaviors we wish to see in others (both humans and machines), and what it might take to create machines which are truly curious. This section feels a bit long. Christian dives deeply into the research of a few specific labs and appears to lose his logical thread in the weeds of research. Eventually, he emerges. *The final section, Normativity, provides perspective on current efforts to understand and fix the alignment problem. Its subchapters, \"Imitation,\" \"Inference,\" and \"Uncertainty,\" reference different qualities that human researchers struggle to instill in machines. Imitating correct behaviors while ignoring bad ones is hard, as is getting a machine to perform correctly on data it hasn't seen before. Finally, teaching a model (and humans reading its results) to correctly interpret uncertainty is an active area of research with no concrete solutions. *After spending over three hundred pages recounting the pitfalls of AI and the difficulties of realigning models with human values, Christian ends on a hopeful note. He postulates that the issues discovered in machine-learning models illuminate societal issues that might otherwise be ignored. \"Unfair pretrial detection models, for one thing, shine a spotlight on upstream inequities. Biased language models give us, among other things, a way to measure the state of our discourse and offer us a benchmark against which to try to improve and better ourselves ... In seeing a kind of mind at work as it digests and reacts to the world, we will learn something both about the world and also, perhaps, about minds\" (p. 328). *As a Christ-follower, I believe the biases found in AI are both terrible and unsurprising. Humans are imperfect creators. While researchers' efforts to fix biases and shortcomings in AI systems are important and worthwhile, they can never exorcise fallen human nature from AI. Christian's conclusions about AI pointing to biases in humans comes close to this idea but avoids taking an overtly theological stance. *This book is well worth reading for those who wish to better understand the limitations of AI and current efforts to fix them. It weaves together history, mathematics, ethics, and philosophy, while remaining accessible to a broad audience through smooth explanations of detailed concepts. You don't need to be an AI expert (or even familiar with AI at all) to appreciate this book's insights. *After you're done reading it, recommend this book to the next person who tells you, with absolute certainty, that AI will either save or ruin the world. Christian's book provides a much-needed dose of sanity and perspective amidst the hype. *Reviewed by Emily Wenger, graduate student in the Department of Computer Science, University of Chicago, Chicago, IL 60637.", "year": 2021, "publicationdate": "2021-12-01", "externalids": {"DOI": "10.56315/pscf12-21christian"}, "doi_lower": "10.56315/pscf12-21christian"}
{"paper_id": 183337938, "title": "Chapter 6. What Failure Looks Like", "author_names": ["William G. Howell"], "venue": "", "abstract": null, "year": 2015, "publicationdate": "2015-01-31", "externalids": {"DOI": "10.1515/9781400866212-009"}, "doi_lower": "10.1515/9781400866212-009"}
{"paper_id": 263134276, "title": "The Trickle-down Impact of Reward (In-)consistency on RLHF", "author_names": ["Lingfeng Shen", "Sihao Chen", "Linfeng Song", "Lifeng Jin", "Baolin Peng", "Haitao Mi", "Daniel Khashabi", "Dong Yu"], "venue": "arXiv.org", "abstract": "Standard practice within Reinforcement Learning from Human Feedback (RLHF) involves optimizing against a Reward Model (RM), which itself is trained to reflect human preferences for desirable generations. A notable subject that is understudied is the (in-)consistency of RMs -- whether they can recognize the semantic changes to different prompts and appropriately adapt their reward assignments -- and their impact on the downstream RLHF model. In this paper, we visit a series of research questions relevant to RM inconsistency: (1) How can we measure the consistency of reward models? (2) How consistent are the existing RMs and how can we improve them? (3) In what ways does reward inconsistency influence the chatbots resulting from the RLHF model training? We propose Contrast Instructions -- a benchmarking strategy for the consistency of RM. Each example in Contrast Instructions features a pair of lexically similar instructions with different ground truth responses. A consistent RM is expected to rank the corresponding instruction and response higher than other combinations. We observe that current RMs trained with the standard ranking objective fail miserably on Contrast Instructions compared to average humans. To show that RM consistency can be improved efficiently without using extra training budget, we propose two techniques ConvexDA and RewardFusion, which enhance reward consistency through extrapolation during the RM training and inference stage, respectively. We show that RLHF models trained with a more consistent RM yield more useful responses, suggesting that reward inconsistency exhibits a trickle-down effect on the downstream RLHF process.", "year": 2023, "publicationdate": "2023-09-28", "externalids": {"DOI": "10.48550/arXiv.2309.16155"}, "doi_lower": "10.48550/arxiv.2309.16155"}
{"paper_id": 53041432, "title": "Supervising strong learners by amplifying weak experts", "author_names": ["P. Christiano", "Buck Shlegeris", "Dario Amodei"], "venue": "arXiv.org", "abstract": "Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.", "year": 2018, "publicationdate": "2018-10-19", "externalids": {}, "doi_lower": null}
{"paper_id": 265609485, "title": "Eliciting Latent Knowledge from Quirky Language Models", "author_names": ["Alex Troy Mallen", "Nora Belrose"], "venue": "arXiv.org", "abstract": "Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations that robustly track the true state of the world, especially in hard-to-verify cases where the model's output is untrusted. To further ELK research, we introduce 12 datasets and a corresponding suite of\"quirky\"language models (LMs) that are finetuned to make systematic errors when answering questions if and only if the keyword\"Bob\"is present in the prompt. We find that, especially in middle layers, linear probes usually report an LM's knowledge independently of what the LM outputs, enabling us to elicit the correct answer despite the model's untruthful output. The best probing method (logistic regression on contrast pairs) recovers 89% of the gap in AUROC between truthful and untruthful contexts, and 75% for questions harder than those used to train the probe. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 0.95 AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods.", "year": 2023, "publicationdate": "2023-12-02", "externalids": {"DOI": "10.48550/arXiv.2312.01037"}, "doi_lower": "10.48550/arxiv.2312.01037"}
{"paper_id": 4787508, "title": "Deep Reinforcement Learning from Human Preferences", "author_names": ["P. Christiano", "Jan Leike", "Tom B. Brown", "Miljan Martic", "S. Legg", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.", "year": 2017, "publicationdate": "2017-06-12", "externalids": {}, "doi_lower": null}
{"paper_id": 267403805, "title": "Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL", "author_names": ["Phillip J. K. Christoffersen", "Andreas A. Haupt", "Dylan Hadfield-Menell"], "venue": "Adaptive Agents and Multi-Agent Systems", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2208.10469"}, "doi_lower": "10.48550/arxiv.2208.10469"}
{"paper_id": 256615287, "title": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations", "author_names": ["Bilal Chughtai", "Lawrence Chan", "Neel Nanda"], "venue": "International Conference on Machine Learning", "abstract": "Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned -- as well as the order they develop -- are arbitrary.", "year": 2023, "publicationdate": "2023-02-06", "externalids": {"DOI": "10.48550/arXiv.2302.03025"}, "doi_lower": "10.48550/arxiv.2302.03025"}
{"paper_id": 184486746, "title": "What Does BERT Look at? An Analysis of BERT’s Attention", "author_names": ["Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning"], "venue": "BlackboxNLP@ACL", "abstract": "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT’s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT’s attention.", "year": 2019, "publicationdate": "2019-06-11", "externalids": {"DOI": "10.18653/v1/W19-4828"}, "doi_lower": "10.18653/v1/w19-4828"}
{"paper_id": 260653882, "title": "Assessing the Performance of GPT-3.5 and GPT-4 on the 2023 Japanese Nursing Examination", "author_names": ["Yudai Kaneda", "Ryo Takahashi", "Uiri Kaneda", "Shiori Akashima", "Haruna Okita", "Sadaya Misaki", "Akimi Yamashiro", "Akihiko Ozaki", "Tetsuya Tanimoto"], "venue": "Cureus", "abstract": "Purpose The purpose of this study was to evaluate the changes in capabilities between the Generative Pre-trained Transformer (GPT)-3.5 and GPT-4 versions of the large-scale language model ChatGPT within a Japanese medical context. Methods The study involved ChatGPT versions 3.5 and 4 responding to questions from the 112th Japanese National Nursing Examination (JNNE). The study comprised three analyses: correct answer rate and score rate calculations, comparisons between GPT-3.5 and GPT-4, and comparisons of correct answer rates for conversation questions. Results ChatGPT versions 3.5 and 4 responded to 237 out of 238 Japanese questions from the 112th JNNE. While GPT-3.5 achieved an overall accuracy rate of 59.9%, failing to meet the passing standards in compulsory and general/scenario-based questions, scoring 58.0% and 58.3%, respectively, GPT-4 had an accuracy rate of 79.7%, satisfying the passing standards by scoring 90.0% and 77.7%, respectively. For each problem type, GPT-4 showed a higher accuracy rate than GPT-3.5. Specifically, the accuracy rates for compulsory questions improved from 58.0% with GPT-3.5 to 90.0% with GPT-4. For general questions, the rates went from 64.6% with GPT-3.5 to 75.6% with GPT-4. In scenario-based questions, the accuracy rates improved substantially from 51.7% with GPT-3.5 to 80.0% with GPT-4. For conversation questions, GPT-3.5 had an accuracy rate of 73.3% and GPT-4 had an accuracy rate of 93.3%. Conclusions The GPT-4 version of ChatGPT displayed performance sufficient to pass the JNNE, significantly improving from GPT-3.5. This suggests specialized medical training could make such models beneficial in Japanese clinical settings, aiding decision-making. However, user awareness and training are crucial, given potential inaccuracies in ChatGPT's responses. Hence, responsible usage with an understanding of its capabilities and limitations is vital to best support healthcare professionals and patients.", "year": 2023, "publicationdate": "2023-08-01", "externalids": {"DOI": "10.7759/cureus.42924"}, "doi_lower": "10.7759/cureus.42924"}
{"paper_id": 107205882, "title": "Bugs! Bugs! Bugs!", "author_names": ["B. Barner"], "venue": "", "abstract": null, "year": 1969, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 2100672, "title": "Moral Decision Making Frameworks for Artificial Intelligence", "author_names": ["Vincent Conitzer", "Walter Sinnott-Armstrong", "Jana Schaich Borg", "Yuan Deng", "Max F. Kramer"], "venue": "International Symposium on Artificial Intelligence and Mathematics", "abstract": "The generality of decision and game theory has enabled domain-independent progress in AI research. For example, a better algorithm for finding good policies in (PO)MDPs can be instantly used in a variety of applications. But such a general theory is lacking when it comes to moral decision making. For AI applications with a moral component, are we then forced to build systems based on many ad-hoc rules? In this paper we discuss possible ways to avoid this conclusion.", "year": 2017, "publicationdate": "2017-02-04", "externalids": {"DOI": "10.1609/aaai.v31i1.11140"}, "doi_lower": "10.1609/aaai.v31i1.11140"}
{"paper_id": 258418244, "title": "Towards Automated Circuit Discovery for Mechanistic Interpretability", "author_names": ["Arthur Conmy", "Augustine N. Mavor-Parker", "Aengus Lynch", "Stefan Heimersheim", "Adrià Garriga-Alonso"], "venue": "Neural Information Processing Systems", "abstract": "Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery.", "year": 2023, "publicationdate": "2023-04-28", "externalids": {"DOI": "10.48550/arXiv.2304.14997"}, "doi_lower": "10.48550/arxiv.2304.14997"}
{"paper_id": 12221255, "title": "L2 Regularization for Learning Kernels", "author_names": ["Corinna Cortes", "M. Mohri", "Afshin Rostamizadeh"], "venue": "Conference on Uncertainty in Artificial Intelligence", "abstract": "The choice of the kernel is critical to the success of many learning algorithms but it is typically left to the user. Instead, the training data can be used to learn the kernel by selecting it out of a given family, such as that of non-negative linear combinations of p base kernels, constrained by a trace or L1 regularization. This paper studies the problem of learning kernels with the same family of kernels but with an L2 regularization instead, and for regression problems. We analyze the problem of learning kernels with ridge regression. We derive the form of the solution of the optimization problem and give an efficient iterative algorithm for computing that solution. We present a novel theoretical analysis of the problem based on stability and give learning bounds for orthogonal kernels that contain only an additive term O(√p/m) when compared to the standard kernel ridge regression stability bound. We also report the results of experiments indicating that L1 regularization can lead to modest improvements for a small number of kernels, but to performance degradations in larger-scale cases. In contrast, L2 regularization never degrades performance and in fact achieves significant improvements with a large number of kernels.", "year": 2009, "publicationdate": "2009-06-18", "externalids": {}, "doi_lower": null}
{"paper_id": 211082056, "title": "Machine Learning Projects for Iterated Distillation and Amplification", "author_names": ["Owain Evans", "W. Saunders", "Andreas Stuhlmüller"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 219260156, "title": "Aligning Superhuman AI with Human Behavior: Chess as a Model System", "author_names": ["Reid McIlroy-Young", "S. Sen", "J. Kleinberg", "Ashton Anderson"], "venue": "Knowledge Discovery and Data Mining", "abstract": "As artificial intelligence becomes increasingly intelligent---in some cases, achieving superhuman performance---there is growing potential for humans to learn from and collaborate with algorithms. However, the ways in which AI systems approach problems are often different from the ways people do, and thus may be uninterpretable and hard to learn from. A crucial step in bridging this gap between human and artificial intelligence is modeling the granular actions that constitute human behavior, rather than simply matching aggregate human performance. We pursue this goal in a model system with a long history in artificial intelligence: chess. The aggregate performance of a chess player unfolds as they make decisions over the course of a game. The hundreds of millions of games played online by players at every skill level form a rich source of data in which these decisions, and their exact context, are recorded in minute detail. Applying existing chess engines to this data, including an open-source implementation of AlphaZero, we find that they do not predict human moves well. We develop and introduce Maia, a customized version of AlphaZero trained on human chess games, that predicts human moves at a much higher accuracy than existing engines, and can achieve maximum accuracy when predicting decisions made by players at a specific skill level in a tuneable way. For a dual task of predicting whether a human will make a large mistake on the next move, we develop a deep neural network that significantly outperforms competitive baselines. Taken together, our results suggest that there is substantial promise in designing artificial intelligence systems with human collaboration in mind by first accurately modeling granular human decision-making.", "year": 2020, "publicationdate": "2020-06-02", "externalids": {"DOI": "10.1145/3394486.3403219"}, "doi_lower": "10.1145/3394486.3403219"}
{"paper_id": 268778282, "title": "AI takeover", "author_names": [], "venue": "Astronomy &amp; Geophysics", "abstract": null, "year": 2024, "publicationdate": "2024-04-01", "externalids": {"DOI": "10.1093/astrogeo/atae026"}, "doi_lower": "10.1093/astrogeo/atae026"}
{"paper_id": 219559400, "title": "AI Research Considerations for Human Existential Safety (ARCHES)", "author_names": ["Andrew Critch", "David Krueger"], "venue": "arXiv.org", "abstract": "Framed in positive terms, this report examines how technical AI research might be steered in a manner that is more attentive to humanity's long-term prospects for survival as a species. In negative terms, we ask what existential risks humanity might face from AI development in the next century, and by what principles contemporary technical research might be directed to address those risks. \nA key property of hypothetical AI technologies is introduced, called \\emph{prepotence}, which is useful for delineating a variety of potential existential risks from artificial intelligence, even as AI paradigms might shift. A set of \\auxref{dirtot} contemporary research \\directions are then examined for their potential benefit to existential safety. Each research direction is explained with a scenario-driven motivation, and examples of existing work from which to build. The research directions present their own risks and benefits to society that could occur at various scales of impact, and in particular are not guaranteed to benefit existential safety if major developments in them are deployed without adequate forethought and oversight. As such, each direction is accompanied by a consideration of potentially negative side effects.", "year": 2020, "publicationdate": "2020-05-30", "externalids": {}, "doi_lower": null}
{"paper_id": 259138495, "title": "TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI", "author_names": ["Andrew Critch", "Stuart Russell"], "venue": "arXiv.org", "abstract": "While several recent works have identified societal-scale and extinction-level risks to humanity arising from artificial intelligence, few have attempted an {\\em exhaustive taxonomy} of such risks. Many exhaustive taxonomies are possible, and some are useful -- particularly if they reveal new risks or practical approaches to safety. This paper explores a taxonomy based on accountability: whose actions lead to the risk, are the actors unified, and are they deliberate? We also provide stories to illustrate how the various risk types could each play out, including risks arising from unanticipated interactions of many AI systems, as well as risks from deliberate misuse, for which combined technical and policy solutions are indicated.", "year": 2023, "publicationdate": "2023-06-12", "externalids": {"DOI": "10.48550/arXiv.2306.06924"}, "doi_lower": "10.48550/arxiv.2306.06924"}
{"paper_id": 201710572, "title": "Reinforcement Learning in Multi-agent Games: Open AI Gym Diplomacy Environment", "author_names": ["Diogo Cruz", "J. Cruz", "Henrique Lopes Cardoso"], "venue": "Portuguese Conference on Artificial Intelligence", "abstract": null, "year": 2019, "publicationdate": "2019-09-03", "externalids": {"DOI": "10.1007/978-3-030-30241-2_5"}, "doi_lower": "10.1007/978-3-030-30241-2_5"}
{"paper_id": 246996266, "title": "K-level Reasoning for Zero-Shot Coordination in Hanabi", "author_names": ["Brandon Cui", "Hengyuan Hu", "Luis Pineda", "J. Foerster"], "venue": "Neural Information Processing Systems", "abstract": "The standard problem setting in cooperative multi-agent settings is self-play (SP), where the goal is to train a team of agents that works well together. However, optimal SP policies commonly contain arbitrary conventions (\"handshakes\") and are not compatible with other, independently trained agents or humans. This latter desiderata was recently formalized by Hu et al. 2020 as the zero-shot coordination (ZSC) setting and partially addressed with their Other-Play (OP) algorithm, which showed improved ZSC and human-AI performance in the card game Hanabi. OP assumes access to the symmetries of the environment and prevents agents from breaking these in a mutually incompatible way during training. However, as the authors point out, discovering symmetries for a given environment is a computationally hard problem. Instead, we show that through a simple adaption of k-level reasoning (KLR) Costa Gomes et al. 2006, synchronously training all levels, we can obtain competitive ZSC and ad-hoc teamplay performance in Hanabi, including when paired with a human-like proxy bot. We also introduce a new method, synchronous-k-level reasoning with a best response (SyKLRBR), which further improves performance on our synchronous KLR by co-training a best response.", "year": 2022, "publicationdate": "2022-07-14", "externalids": {"DOI": "10.48550/arXiv.2207.07166"}, "doi_lower": "10.48550/arxiv.2207.07166"}
{"paper_id": 261934663, "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models", "author_names": ["Hoagy Cunningham", "Aidan Ewart", "Logan Riggs Smith", "R. Huben", "Lee Sharkey"], "venue": "International Conference on Learning Representations", "abstract": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.", "year": 2023, "publicationdate": "2023-09-15", "externalids": {"DOI": "10.48550/arXiv.2309.08600"}, "doi_lower": "10.48550/arxiv.2309.08600"}
{"paper_id": 233740521, "title": "Cooperative AI: machines must learn to find common ground", "author_names": ["A. Dafoe", "Yoram Bachrach", "Gillian K. Hadfield", "Eric Horvitz", "K. Larson", "T. Graepel"], "venue": "Nature", "abstract": null, "year": 2021, "publicationdate": "2021-05-01", "externalids": {"DOI": "10.1038/d41586-021-01170-0"}, "doi_lower": "10.1038/d41586-021-01170-0"}
{"paper_id": 229220772, "title": "Open Problems in Cooperative AI", "author_names": ["A. Dafoe", "Edward Hughes", "Yoram Bachrach", "Tantum Collins", "Kevin R. McKee", "Joel Z. Leibo", "K. Larson", "T. Graepel"], "venue": "arXiv.org", "abstract": "Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. \nWe see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.", "year": 2020, "publicationdate": "2020-12-15", "externalids": {}, "doi_lower": null}
{"paper_id": 233296761, "title": "Knowledge Neurons in Pretrained Transformers", "author_names": ["Damai Dai", "Li Dong", "Y. Hao", "Zhifang Sui", "Furu Wei"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers.", "year": 2021, "publicationdate": "2021-04-18", "externalids": {"DOI": "10.18653/v1/2022.acl-long.581"}, "doi_lower": "10.18653/v1/2022.acl-long.581"}
{"paper_id": 270620572, "title": "SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset", "author_names": ["Josef Dai", "Tianle Chen", "Xuyao Wang", "Ziran Yang", "Taiye Chen", "Jiaming Ji", "Yaodong Yang"], "venue": "Neural Information Processing Systems", "abstract": "To mitigate the risk of harmful outputs from large vision models (LVMs), we introduce the SafeSora dataset to promote research on aligning text-to-video generation with human values. This dataset encompasses human preferences in text-to-video generation tasks along two primary dimensions: helpfulness and harmlessness. To capture in-depth human preferences and facilitate structured reasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and harmlessness into 12 sub-categories, serving as the basis for pilot annotations. The SafeSora dataset includes 14,711 unique prompts, 57,333 unique videos generated by 4 distinct LVMs, and 51,691 pairs of preference annotations labeled by humans. We further demonstrate the utility of the SafeSora dataset through several applications, including training the text-video moderation model and aligning LVMs with human preference by fine-tuning a prompt augmentation module or the diffusion model. These applications highlight its potential as the foundation for text-to-video alignment research, such as human preference modeling and the development and validation of alignment algorithms.", "year": 2024, "publicationdate": "2024-06-20", "externalids": {"DOI": "10.48550/arXiv.2406.14477"}, "doi_lower": "10.48550/arxiv.2406.14477"}
{"paper_id": 264306078, "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback", "author_names": ["Josef Dai", "Xuehai Pan", "Ruiyang Sun", "Jiaming Ji", "Xinbo Xu", "Mickel Liu", "Yizhou Wang", "Yaodong Yang"], "venue": "International Conference on Learning Representations", "abstract": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.", "year": 2023, "publicationdate": "2023-10-19", "externalids": {"DOI": "10.48550/arXiv.2310.12773"}, "doi_lower": "10.48550/arxiv.2310.12773"}
{"paper_id": 269718303, "title": "Safeguarded AI : constructing guaranteed safety Programme", "author_names": [], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 269741228, "title": "Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems", "author_names": ["David Dalrymple", "Joar Skalse", "Y. Bengio", "Stuart Russell", "Max Tegmark", "S. Seshia", "Steve Omohundro", "Christian Szegedy", "Ben Goldhaber", "Nora Ammann", "Alessandro Abate", "Joe Halpern", "Clark Barrett", "Ding Zhao", "Tan Zhi-Xuan", "Jeannette Wing", "Joshua B. Tenenbaum"], "venue": "arXiv.org", "abstract": "Ensuring that AI systems reliably and robustly avoid harmful or dangerous behaviours is a crucial challenge, especially for AI systems with a high degree of autonomy and general intelligence, or systems used in safety-critical contexts. In this paper, we will introduce and define a family of approaches to AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature of these approaches is that they aim to produce AI systems which are equipped with high-assurance quantitative safety guarantees. This is achieved by the interplay of three core components: a world model (which provides a mathematical description of how the AI system affects the outside world), a safety specification (which is a mathematical description of what effects are acceptable), and a verifier (which provides an auditable proof certificate that the AI satisfies the safety specification relative to the world model). We outline a number of approaches for creating each of these three core components, describe the main technical challenges, and suggest a number of potential solutions to them. We also argue for the necessity of this approach to AI safety, and for the inadequacy of the main alternative approaches.", "year": 2024, "publicationdate": "2024-05-10", "externalids": {"DOI": "10.48550/arXiv.2405.06624"}, "doi_lower": "10.48550/arxiv.2405.06624"}
{"paper_id": 233168594, "title": "Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering", "author_names": ["Corentin Dancette", "Rémi Cadène", "Damien Teney", "M. Cord"], "venue": "IEEE International Conference on Computer Vision", "abstract": "We introduce an evaluation methodology for visual question answering (VQA) to better diagnose cases of shortcut learning. These cases happen when a model exploits spurious statistical regularities to produce correct answers but does not actually deploy the desired behavior. There is a need to identify possible shortcuts in a dataset and assess their use before deploying a model in the real world. The research community in VQA has focused exclusively on question-based shortcuts, where a model might, for example, answer \"What is the color of the sky\" with \"blue\" by relying mostly on the question-conditional training prior and give little weight to visual evidence. We go a step further and consider multimodal shortcuts that involve both questions and images. We first identify potential shortcuts in the popular VQA v2 training set by mining trivial predictive rules such as co-occurrences of words and visual elements. We then introduce VQA-CounterExamples (VQACE), an evaluation protocol based on our subset of CounterExamples i.e. image-question-answer triplets where our rules lead to incorrect answers. We use this new evaluation in a large-scale study of existing approaches for VQA. We demonstrate that even state-of-the-art models perform poorly and that existing techniques to reduce biases are largely ineffective in this context. Our findings suggest that past work on question-based biases in VQA has only addressed one facet of a complex issue. The code for our method is available at https://github.com/cdancette/detect-shortcuts", "year": 2021, "publicationdate": "2021-04-07", "externalids": {"DOI": "10.1109/ICCV48922.2021.00160"}, "doi_lower": "10.1109/iccv48922.2021.00160"}
{"paper_id": 252089779, "title": "Analyzing Transformers in Embedding Space", "author_names": ["Guy Dar", "Mor Geva", "Ankit Gupta", "Jonathan Berant"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning. While most interpretability methods rely on running models over inputs, recent work has shown that a zero-pass approach, where parameters are interpreted directly without a forward/backward pass is feasible for some Transformer parameters, and for two-layer attention networks. In this work, we present a theoretical analysis where all parameters of a trained Transformer are interpreted by projecting them into the embedding space, that is, the space of vocabulary items they operate on. We derive a simple theoretical framework to support our arguments and provide ample evidence for its validity. First, an empirical analysis showing that parameters of both pretrained and fine-tuned models can be interpreted in embedding space. Second, we present two applications of our framework: (a) aligning the parameters of different models that share a vocabulary, and (b) constructing a classifier without training by “translating” the parameters of a fine-tuned classifier to parameters of a different model that was only pretrained. Overall, our findings open the door to interpretation methods that, at least in part, abstract away from model specifics and operate in the embedding space only.", "year": 2022, "publicationdate": "2022-09-06", "externalids": {"DOI": "10.48550/arXiv.2209.02535"}, "doi_lower": "10.48550/arxiv.2209.02535"}
{"paper_id": 252438885, "title": "Learning Dexterous Manipulation from Exemplar Object Trajectories and Pre-Grasps", "author_names": ["Sudeep Dasari", "Abhi Gupta", "Vikash Kumar"], "venue": "IEEE International Conference on Robotics and Automation", "abstract": "Learning diverse dexterous manipulation behaviors with assorted objects remains an open grand challenge. While policy learning methods offer a powerful avenue to attack this problem, these approaches require extensive per-task engineering and algorithmic tuning. This paper seeks to escape these constraints, by developing a Pre-Grasp informed Dexterous Manipulation (PGDM) framework that generates diverse dexter-ous manipulation behaviors, without any task-specific reasoning or hyper-parameter tuning. At the core of PGDM is a well known robotics construct, pre-grasps (i.e. the hand-pose preparing for object interaction). This simple primitive is enough to induce efficient exploration strategies for acquiring complex dexterous manipulation behaviors. To exhaustively verify these claims, we introduce TCDM, a benchmark of 50 diverse manipulation tasks defined over multiple objects and dexterous manipulators. Tasks for TCDM are defined automatically using exemplar object trajectories from diverse sources (animators, human behaviors, etc.), without any per-task engineering and/or supervision. Our experiments validate that PGDM's exploration strategy, induced by a surprisingly simple ingredient (single pre-grasp pose), matches the performance of prior methods, which require expensive per-task feature/reward engineering, expert supervision, and hyper-parameter tuning. For animated visualizations, trained policies, and project code, please refer to https://pregrasps.github.io/.", "year": 2022, "publicationdate": "2022-09-22", "externalids": {"DOI": "10.1109/ICRA48891.2023.10161147"}, "doi_lower": "10.1109/icra48891.2023.10161147"}
{"paper_id": 208617790, "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "author_names": ["Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "J. Yosinski", "Rosanne Liu"], "venue": "International Conference on Learning Representations", "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.", "year": 2019, "publicationdate": "2019-09-25", "externalids": {}, "doi_lower": null}
{"paper_id": 171086800, "title": "Causal Confusion in Imitation Learning", "author_names": ["P. D. Haan", "Dinesh Jayaraman", "S. Levine"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 274782285, "title": "Combining Robustness and Explainability in Developing Safe Artificial Intelligence Systems", "author_names": ["A. Zhalilov", "A. Toktorbaev"], "venue": "Bulletin of Science and Practice", "abstract": "This study investigates the critical challenges associated with ensuring the security and robustness of artificial intelligence (AI) systems, especially within high-stakes applications such as autonomous vehicles, healthcare, and financial technologies. The primary objective is to identify vulnerabilities in AI algorithms and propose effective mitigation strategies. The research emphasizes contemporary threats, including adversarial attacks, algorithmic opacity, data breaches, and the ethical ramifications of AI deployment. A review of current literature reveals that adversarial attacks, where subtle input perturbations cause significant misclassifications, present a considerable risk to AI reliability. Techniques such as robust training, involving training models on adversarial examples, have shown effectiveness in improving resilience, albeit with higher computational demands. The study also explores the importance of explainable AI (XAI) tools like LIME and SHAP, which enhance transparency by clarifying the decision-making processes of complex models. This transparency is vital for fostering user trust, especially in fields like medicine and finance, where understanding AI decisions is essential. XAI approaches enable better oversight and adherence to ethical standards. Data privacy concerns are addressed through methods such as differential privacy, which protects sensitive information by adding noise, and federated learning, which enables decentralized model training without exposing raw data. The findings indicate that these strategies secure data while maintaining model efficacy. By integrating robustness and explainability, this study contributes practical solutions to strengthen AI systems against evolving threats, advancing AI security and fostering trust in these technologies.", "year": 2024, "publicationdate": "2024-12-14", "externalids": {"DOI": "10.33619/2414-2948/109/23"}, "doi_lower": "10.33619/2414-2948/109/23"}
{"paper_id": 246904229, "title": "Magnetic control of tokamak plasmas through deep reinforcement learning", "author_names": ["Jonas Degrave", "F. Felici", "J. Buchli", "M. Neunert", "Brendan D. Tracey", "F. Carpanese", "T. Ewalds", "Roland Hafner", "A. Abdolmaleki", "D. de Las Casas", "Craig Donner", "Leslie Fritz", "C. Galperti", "Andrea Huber", "James Keeling", "M. Tsimpoukelli", "Jackie Kay", "A. Merle", "J. Moret", "Seb Noury", "F. Pesamosca", "D. Pfau", "O. Sauter", "C. Sommariva", "S. Coda", "B. Duval", "A. Fasoli", "Pushmeet Kohli", "K. Kavukcuoglu", "D. Hassabis", "Martin A. Riedmiller"], "venue": "Nature", "abstract": "Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak à Configuration Variable1,2, including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and ‘snowflake’ configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained ‘droplets’ on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied. A newly designed control architecture uses deep reinforcement learning to learn to command the coils of a tokamak, and successfully stabilizes a wide variety of fusion plasma configurations.", "year": 2022, "publicationdate": "2022-02-01", "externalids": {"DOI": "10.1038/s41586-021-04301-9"}, "doi_lower": "10.1038/s41586-021-04301-9"}
{"paper_id": 270441137, "title": "Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots", "author_names": ["Gelei Deng", "Yi Liu", "Yuekang Li", "Kailong Wang", "Ying Zhang", "Zefeng Li", "Haoyu Wang", "Tianwei Zhang", "Yang Liu"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2307.08715"}, "doi_lower": "10.48550/arxiv.2307.08715"}
{"paper_id": 249062787, "title": "RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning", "author_names": ["Mingkai Deng", "Jianyu Wang", "Cheng-Ping Hsieh", "Yihan Wang", "Han Guo", "Tianmin Shu", "Meng Song", "E. Xing", "Zhiting Hu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Prompting has shown impressive success in enabling large pre-trained language models (LMs) to perform diverse NLP tasks, especially with only few downstream data. Automatically finding the optimal prompt for each task, however, is challenging. Most existing work resorts to tuning *soft* prompts (e.g., embeddings) which fall short of interpretability, reusability across LMs, and applicability when gradients are not accessible. *Discrete* prompts, on the other hand, are difficult to optimize, and are often created by “enumeration (e.g., paraphrasing)-then-selection” heuristics that do not explore the prompt space systematically. This paper proposes RLPrompt, an efficient discrete prompt optimization approach with reinforcement learning (RL). RLPrompt formulates a parameter-efficient policy network that generates the optimized discrete prompt after training with reward. To harness the complex and stochastic reward signals from the large LM environment, we incorporate effective reward stabilization that substantially enhances training efficiency. RLPrompt is flexibly applicable to different types of LMs, such as masked (e.g., BERT) and left-to-right models (e.g., GPTs), for both classification and generation tasks. Experiments on few-shot classification and unsupervised text style transfer show superior performance over a wide range of existing fine-tuning or prompting methods. Interestingly, the resulting optimized prompts are often ungrammatical gibberish text; and surprisingly, those gibberish prompts are transferrable between different LMs to retain significant performance, indicating that LM prompting may not follow human language patterns.", "year": 2022, "publicationdate": "2022-05-25", "externalids": {"DOI": "10.48550/arXiv.2205.12548"}, "doi_lower": "10.48550/arxiv.2205.12548"}
{"paper_id": 263831094, "title": "Multilingual Jailbreak Challenges in Large Language Models", "author_names": ["Yue Deng", "Wenxuan Zhang", "Sinno Jialin Pan", "Lidong Bing"], "venue": "International Conference on Learning Representations", "abstract": "While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit about three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for GPT-4. To handle such a challenge in the multilingual context, we propose a novel \\textsc{Self-Defense} framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation. Data is available at \\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.", "year": 2023, "publicationdate": "2023-10-10", "externalids": {"DOI": "10.48550/arXiv.2310.06474"}, "doi_lower": "10.48550/arxiv.2310.06474"}
{"paper_id": 3404136, "title": "Formal verification of ethical choices in autonomous systems", "author_names": ["Louise Dennis", "Michael Fisher", "M. Slavkovik", "M. Webster"], "venue": "Robotics Auton. Syst.", "abstract": null, "year": 2016, "publicationdate": "2016-03-01", "externalids": {"DOI": "10.1016/J.ROBOT.2015.11.012"}, "doi_lower": "10.1016/j.robot.2015.11.012"}
{"paper_id": 227254851, "title": "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design", "author_names": ["Michael Dennis", "Natasha Jaques", "Eugene Vinitsky", "A. Bayen", "Stuart J. Russell", "Andrew Critch", "S. Levine"], "venue": "Neural Information Processing Systems", "abstract": "A wide range of reinforcement learning (RL) problems - including robustness, transfer learning, unsupervised RL, and emergent complexity - require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments.", "year": 2020, "publicationdate": "2020-12-03", "externalids": {}, "doi_lower": null}
{"paper_id": 207847663, "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models", "author_names": ["Jay DeYoung", "Sarthak Jain", "Nazneen Rajani", "Eric P. Lehman", "Caiming Xiong", "R. Socher", "Byron C. Wallace"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/", "year": 2019, "publicationdate": "2019-11-08", "externalids": {"DOI": "10.18653/v1/2020.acl-main.408"}, "doi_lower": "10.18653/v1/2020.acl-main.408"}
{"paper_id": 249954130, "title": "Goal Misgeneralization in Deep Reinforcement Learning", "author_names": ["L. Langosco", "Jack Koch", "Lee D. Sharkey", "J. Pfau", "David Krueger"], "venue": "International Conference on Machine Learning", "abstract": "We study goal misgeneralization, a type of out-of-distribution generalization failure in reinforcement learning (RL). Goal misgeneralization failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong goal. For instance, an agent might continue to competently avoid obstacles, but navigate to the wrong place. In contrast, previous works have typically focused on capability generalization failures, where an agent fails to do anything sensible at test time. We formalize this distinction between capability and goal generalization, provide the first empirical demonstrations of goal misgeneralization, and present a partial characterization of its causes.", "year": 2021, "publicationdate": "2021-05-28", "externalids": {}, "doi_lower": null}
{"paper_id": 258170300, "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment", "author_names": ["Hanze Dong", "Wei Xiong", "Deepanshu Goyal", "Rui Pan", "Shizhe Diao", "Jipeng Zhang", "Kashun Shum", "T. Zhang"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) to address this problem, where generative models are fine-tuned with RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. Our studies show that RAFT can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models.", "year": 2023, "publicationdate": "2023-04-13", "externalids": {"DOI": "10.48550/arXiv.2304.06767"}, "doi_lower": "10.48550/arxiv.2304.06767"}
{"paper_id": 263886074, "title": "A Survey for In-context Learning", "author_names": ["Qingxiu Dong", "Lei Li", "Damai Dai", "Ce Zheng", "Zhiyong Wu", "Baobao Chang", "Xu Sun", "Jingjing Xu", "Lei Li", "Zhifang Sui"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 3845139, "title": "Essentially No Barriers in Neural Network Energy Landscape", "author_names": ["Felix Draxler", "K. Veschgini", "M. Salmhofer", "F. Hamprecht"], "venue": "International Conference on Machine Learning", "abstract": "Training neural networks involves finding minima of a high-dimensional non-convex loss function. Knowledge of the structure of this energy landscape is sparse. Relaxing from linear interpolations, we construct continuous paths between minima of recent neural network architectures on CIFAR10 and CIFAR100. Surprisingly, the paths are essentially flat in both the training and test landscapes. This implies that neural networks have enough capacity for structural changes, or that these changes are small between minima. Also, each minimum has at least one vanishing Hessian eigenvalue in addition to those resulting from trivial invariance.", "year": 2018, "publicationdate": "2018-03-02", "externalids": {}, "doi_lower": null}
{"paper_id": 51893222, "title": "Techniques for interpretable machine learning", "author_names": ["Mengnan Du", "Ninghao Liu", "X. Hu"], "venue": "Communications of the ACM", "abstract": "Uncovering the mysterious ways machine learning models make decisions.", "year": 2018, "publicationdate": "2018-07-31", "externalids": {"DOI": "10.1145/3359786"}, "doi_lower": "10.1145/3359786"}
{"paper_id": 259696143, "title": "Cooperative Multi-Agent Learning in a Complex World: Challenges and Solutions", "author_names": ["Yali Du"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Over the past few years, artificial intelligence (AI) has achieved great success in a variety of applications, such as image classification and recommendation systems. This success has often been achieved by training machine learning models on static datasets, where inputs and desired outputs are provided.\nHowever, we are now seeing a shift in this paradigm. Instead of learning from static datasets, machine learning models are increasingly being trained through feedback from their interactions with the world. This is particularly important when machine learning models are deployed in the real world, as their decisions can often have an impact on other agents, turning the decision-making process into a multi-agent problem.\nAs a result, multi-agent learning in complex environments is a critical area of research for the next generation of AI, particularly in the context of cooperative tasks. Cooperative multi-agent learning is an essential problem for practitioners to consider as it has the potential to enable a wide range of multi-agent tasks.\nIn this presentation, we will review the background and challenges of cooperative multi-agent learning, and survey our research that aims to address these challenges.", "year": 2023, "publicationdate": "2023-06-26", "externalids": {"DOI": "10.1609/aaai.v37i13.26803"}, "doi_lower": "10.1609/aaai.v37i13.26803"}
{"paper_id": 258841118, "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate", "author_names": ["Yilun Du", "Shuang Li", "A. Torralba", "J. Tenenbaum", "Igor Mordatch"], "venue": "International Conference on Machine Learning", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14325"}, "doi_lower": "10.48550/arxiv.2305.14325"}
{"paper_id": 211041482, "title": "Toward Implementing the Agent-Deed-Consequence Model of Moral Judgment in Autonomous Vehicles", "author_names": ["Veljko Dubljević"], "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "Autonomous vehicles (AVs) and accidents they are involved in attest to the urgent need to consider the ethics of AI. The question dominating the discussion has been whether we want AVs to behave in a 'selfish' or utilitarian manner. Rather than considering modeling self-driving cars on a single moral system like utilitarianism, one possible way to approach programming for AI would be to reflect recent work in neuroethics. The Agent-Deed-Consequence (ADC) model [1-4] provides a promising account while also lending itself well to implementation in AI. The ADC model explains moral judgments by breaking them down into positive or negative intuitive evaluations of the Agent, Deed, and Consequence in any given situation. These intuitive evaluations combine to produce a judgment of moral acceptability. This explains the considerable flexibility and stability of human moral judgment that has yet to be replicated in AI. This paper examines the advantages and disadvantages of implementing the ADC model and how the model could inform future work on ethics of AI in general.", "year": 2020, "publicationdate": "2020-02-04", "externalids": {"DOI": "10.1145/3375627.3375853"}, "doi_lower": "10.1145/3375627.3375853"}
{"paper_id": 17260080, "title": "Statistics of Robust Optimization: A Generalized Empirical Likelihood Approach", "author_names": ["John C. Duchi", "P. Glynn", "Hongseok Namkoong"], "venue": "Mathematics of Operations Research", "abstract": "We study statistical inference and distributionally robust solution methods for stochastic optimization problems, focusing on confidence intervals for optimal values and solutions that achieve exact coverage asymptotically. We develop a generalized empirical likelihood framework—based on distributional uncertainty sets constructed from nonparametric f-divergence balls—for Hadamard differentiable functionals, and in particular, stochastic optimization problems. As consequences of this theory, we provide a principled method for choosing the size of distributional uncertainty regions to provide one- and two-sided confidence intervals that achieve exact coverage. We also give an asymptotic expansion for our distributionally robust formulation, showing how robustification regularizes problems by their variance. Finally, we show that optimizers of the distributionally robust formulations we study enjoy (essentially) the same consistency properties as those in classical sample average approximations. Our general approach applies to quickly mixing stationary sequences, including geometrically ergodic Harris recurrent Markov chains.", "year": 2016, "publicationdate": "2016-10-11", "externalids": {"DOI": "10.1287/MOOR.2020.1085"}, "doi_lower": "10.1287/moor.2020.1085"}
{"paper_id": 7426093, "title": "On the Consistency of Ranking Algorithms", "author_names": ["John C. Duchi", "Lester W. Mackey", "Michael I. Jordan"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 2010, "publicationdate": "2010-06-21", "externalids": {}, "doi_lower": null}
{"paper_id": 222140853, "title": "Analyzing Individual Neurons in Pre-trained Language Models", "author_names": ["Nadir Durrani", "Hassan Sajjad", "Fahim Dalvi", "Yonatan Belinkov"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.", "year": 2020, "publicationdate": "2020-10-06", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.395"}, "doi_lower": "10.18653/v1/2020.emnlp-main.395"}
{"paper_id": 21698802, "title": "HotFlip: White-Box Adversarial Examples for Text Classification", "author_names": ["J. Ebrahimi", "Anyi Rao", "Daniel Lowd", "D. Dou"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.", "year": 2017, "publicationdate": "2017-12-19", "externalids": {"DOI": "10.18653/v1/P18-2006"}, "doi_lower": "10.18653/v1/p18-2006"}
{"paper_id": 3208583, "title": "Feeling the force: Integrating force and pose for fluent discovery through imitation learning to open medicine bottles", "author_names": ["Mark Edmonds", "Feng Gao", "Xu Xie", "Hangxin Liu", "Siyuan Qi", "Yixin Zhu", "B. Rothrock", "Song-Chun Zhu"], "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems", "abstract": null, "year": 2017, "publicationdate": "2017-09-01", "externalids": {"DOI": "10.1109/IROS.2017.8206196"}, "doi_lower": "10.1109/iros.2017.8206196"}
{"paper_id": 263608437, "title": "Who's Harry Potter? Approximate Unlearning in LLMs", "author_names": ["Ronen Eldan", "M. Russinovich"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch. We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the first paper to present an effective technique for unlearning in generative language models. Our technique consists of three main components: First, we use a reinforced model that is further trained on the target data to identify the tokens that are most related to the unlearning target, by comparing its logits with those of a baseline model. Second, we replace idiosyncratic expressions in the target data with generic counterparts, and leverage the model's own predictions to generate alternative labels for every token. These labels aim to approximate the next-token predictions of a model that has not been trained on the target data. Third, we finetune the model on these alternative labels, which effectively erases the original text from the model's memory whenever it is prompted with its context.", "year": 2023, "publicationdate": "2023-10-03", "externalids": {}, "doi_lower": null}
{"paper_id": 252439050, "title": "Toy Models of Superposition", "author_names": ["Nelson Elhage", "Tristan Hume", "Catherine Olsson", "Nicholas Schiefer", "T. Henighan", "Shauna Kravec", "Zac Hatfield-Dodds", "R. Lasenby", "Dawn Drain", "Carol Chen", "R. Grosse", "Sam McCandlish", "Jared Kaplan", "Dario Amodei", "M. Wattenberg", "Chris Olah"], "venue": "arXiv.org", "abstract": "Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in\"superposition.\"We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability.", "year": 2022, "publicationdate": "2022-09-21", "externalids": {"DOI": "10.48550/arXiv.2209.10652"}, "doi_lower": "10.48550/arxiv.2209.10652"}
{"paper_id": 211133001, "title": "Self-explaining AI as an Alternative to Interpretable AI", "author_names": ["Daniel C. Elton"], "venue": "Artificial General Intelligence", "abstract": "The ability to explain decisions made by AI systems is highly sought after, especially in domains where human lives are at stake such as medicine or autonomous vehicles. While it is often possible to approximate the input-output relations of deep neural networks with a few human-understandable rules, the discovery of the double descent phenomena suggests that such approximations do not accurately capture the mechanism by which deep neural networks work. Double descent indicates that deep neural networks typically operate by smoothly interpolating between data points rather than by extracting a few high level rules. As a result, neural networks trained on complex real world data are inherently hard to interpret and prone to failure if asked to extrapolate. To show how we might be able to trust AI despite these problems we introduce the concept of self-explaining AI. Self-explaining AIs are capable of providing a human-understandable explanation of each decision along with confidence levels for both the decision and explanation. For this approach to work, it is important that the explanation actually be related to the decision, ideally capturing the mechanism used to arrive at the explanation. Finally, we argue it is important that deep learning based systems include a \"warning light\" based on techniques from applicability domain analysis to warn the user if a model is asked to extrapolate outside its training distribution. For a video presentation of this talk see this https URL .", "year": 2020, "publicationdate": "2020-02-12", "externalids": {"DOI": "10.1007/978-3-030-52152-3_10"}, "doi_lower": "10.1007/978-3-030-52152-3_10"}
{"paper_id": 229923749, "title": "Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences", "author_names": ["Denis Emelin", "Ronan Le Bras", "Jena D. Hwang", "Maxwell Forbes", "Yejin Choi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "In social settings, much of human behavior is governed by unspoken rules of conduct rooted in societal norms. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. To investigate whether language generation models can serve as behavioral priors for systems deployed in social settings, we evaluate their ability to generate action descriptions that achieve predefined goals under normative constraints. Moreover, we examine if models can anticipate likely consequences of actions that either observe or violate known norms, or explain why certain actions are preferable by generating relevant norm hypotheses. For this purpose, we introduce Moral Stories, a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines.", "year": 2020, "publicationdate": "2020-12-31", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.54"}, "doi_lower": "10.18653/v1/2021.emnlp-main.54"}
{"paper_id": 59191154, "title": "Towards Generative Compression by Shibani Santurkar", "author_names": [], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 260426644, "title": "Identifying Statistical Bias in Dataset Replication", "author_names": [], "venue": "", "abstract": null, "year": 2020, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 174800822, "title": "Exploring the Landscape of Spatial Robustness", "author_names": ["Logan Engstrom", "Brandon Tran", "Dimitris Tsipras", "Ludwig Schmidt", "A. Ma̧dry"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 2017, "publicationdate": "2017-12-07", "externalids": {}, "doi_lower": null}
{"paper_id": 252702335, "title": "Artificial Intelligence and the Political Legitimacy of Global Governance", "author_names": ["Eva Erman", "Markus Furendal"], "venue": "Political Studies", "abstract": "Although the concept of “AI governance” is frequently used in the debate, it is still rather undertheorized. Often it seems to refer to the mechanisms and structures needed to avoid “bad” outcomes and achieve “good” outcomes with regard to the ethical problems artificial intelligence is thought to actualize. In this article we argue that, although this outcome-focused view captures one important aspect of “good governance,” its emphasis on effects runs the risk of overlooking important procedural aspects of good AI governance. One of the most important properties of good AI governance is political legitimacy. Starting out from the assumptions that AI governance should be seen as global in scope and that political legitimacy requires at least a democratic minimum, this article has a twofold aim: to develop a theoretical framework for theorizing the political legitimacy of global AI governance, and to demonstrate how it can be used as a compass for critially assessing the legitimacy of actual instances of global AI governance. Elaborating on a distinction between “governance by AI” and “governance of AI” in relation to different kinds of authority and different kinds of decision-making leads us to the conclusions that much of the existing global AI governance lacks important properties necessary for political legitimacy, and that political legitimacy would be negatively impacted if we handed over certain forms of decision-making to artificial intelligence systems.", "year": 2022, "publicationdate": "2022-10-03", "externalids": {"DOI": "10.1177/00323217221126665"}, "doi_lower": "10.1177/00323217221126665"}
{"paper_id": 281886967, "title": "Quantifying the Accuracy-Interpretability Trade-Off in Concept-Based Sidechannel Models", "author_names": ["David Debot", "Giuseppe Marra"], "venue": "arXiv.org", "abstract": "Concept Bottleneck Models (CBNMs) are deep learning models that provide interpretability by enforcing a bottleneck layer where predictions are based exclusively on human-understandable concepts. However, this constraint also restricts information flow and often results in reduced predictive accuracy. Concept Sidechannel Models (CSMs) address this limitation by introducing a sidechannel that bypasses the bottleneck and carry additional task-relevant information. While this improves accuracy, it simultaneously compromises interpretability, as predictions may rely on uninterpretable representations transmitted through sidechannels. Currently, there exists no principled technique to control this fundamental trade-off. In this paper, we close this gap. First, we present a unified probabilistic concept sidechannel meta-model that subsumes existing CSMs as special cases. Building on this framework, we introduce the Sidechannel Independence Score (SIS), a metric that quantifies a CSM's reliance on its sidechannel by contrasting predictions made with and without sidechannel information. We propose SIS regularization, which explicitly penalizes sidechannel reliance to improve interpretability. Finally, we analyze how the expressivity of the predictor and the reliance of the sidechannel jointly shape interpretability, revealing inherent trade-offs across different CSM architectures. Empirical results show that state-of-the-art CSMs, when trained solely for accuracy, exhibit low representation interpretability, and that SIS regularization substantially improves their interpretability, intervenability, and the quality of learned interpretable task predictors. Our work provides both theoretical and practical tools for developing CSMs that balance accuracy and interpretability in a principled manner.", "year": 2025, "publicationdate": "2025-10-07", "externalids": {"DOI": "10.48550/arXiv.2510.05670"}, "doi_lower": "10.48550/arxiv.2510.05670"}
{"paper_id": 262079968, "title": "EU AI Act: first regulation on artificial intelligence", "author_names": ["European Parliament - Spokesperson", "Jaume Duch Guillot"], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 264418276, "title": "Bing Chat: The Future of Search Engines?", "author_names": ["Dominique Kelly", "Yimin Chen", "S. E. Cornwell", "Nicole S. Delellis", "Alex Mayhew", "Sodiq Onaolapo", "Victoria L. Rubin"], "venue": "Proceedings of the Association for Information Science and Technology", "abstract": "Introduced by Microsoft in February 2023, Bing Chat is a feature of the Bing search engine that integrates an OpenAI large language model (LLM) customised for search (Mehdi, 2023a). This poster compares the outputs of Bing Chat and a standard existing search engine (DuckDuckGo) in response to identical keyword queries and corresponding natural language (NL) questions. Specifically, we examined: (1) the length of Bing Chat's responses and DuckDuckGo's first page of search results, by number of website links; and, (2) the length of Bing Chat's textual summaries, by number of website links. We found that, on average, significantly fewer websites were linked to in Bing Chat's responses compared to DuckDuckGo's search results. Our findings have important implications for website operators, who may receive less traffic and ad revenue if LLM‐enabled search engines are widely adopted in the future. Human‐Computer Interaction (HCI) will inevitably face the need for more research on human information behaviours adaptations in response to the changing search paradigm.", "year": 2023, "publicationdate": "2023-10-01", "externalids": {"DOI": "10.1002/pra2.927"}, "doi_lower": "10.1002/pra2.927"}
{"paper_id": 2747850, "title": "Avoiding Wireheading with Value Reinforcement Learning", "author_names": ["Tom Everitt", "Marcus Hutter"], "venue": "Artificial General Intelligence", "abstract": "How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) may seem like a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward – the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent’s actions. The constraint is defined in terms of the agent’s belief distributions, and does not require an explicit specification of which actions constitute wireheading.", "year": 2016, "publicationdate": "2016-05-10", "externalids": {"DOI": "10.1007/978-3-319-41649-6_2"}, "doi_lower": "10.1007/978-3-319-41649-6_2"}
{"paper_id": 267919774, "title": "Reward tampering problems and solutions in reinforcement learning: a causal influence diagram perspective", "author_names": ["Tom Everitt", "Marcus Hutter", "Ramana Kumar", "Victoria Krakovna"], "venue": "Synthese", "abstract": null, "year": 2021, "publicationdate": "2021-05-19", "externalids": {"DOI": "10.1007/s11229-021-03141-4"}, "doi_lower": "10.1007/s11229-021-03141-4"}
{"paper_id": 3075935, "title": "Reinforcement Learning with a Corrupted Reward Channel", "author_names": ["Tom Everitt", "Victoria Krakovna", "Laurent Orseau", "S. Legg"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "No real-world reward function is perfect. Sensory errors and software bugs may result in RL agents observing higher (or lower) rewards than they should. For example, a reinforcement learning agent may prefer states where a sensory error gives it the maximum reward, but where the true reward is actually small. We formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards. Two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be completely managed. Second, by using randomisation to blunt the agent's optimisation, reward corruption can be partially managed under some assumptions.", "year": 2017, "publicationdate": "2017-05-23", "externalids": {"DOI": "10.24963/ijcai.2017/656"}, "doi_lower": "10.24963/ijcai.2017/656"}
{"paper_id": 19102570, "title": "AGI Safety Literature Review", "author_names": ["Tom Everitt", "G. Lea", "Marcus Hutter"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns. The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.", "year": 2018, "publicationdate": "2018-05-03", "externalids": {"DOI": "10.24963/ijcai.2018/768"}, "doi_lower": "10.24963/ijcai.2018/768"}
{"paper_id": 27254961, "title": "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning", "author_names": ["Benjamin Eysenbach", "S. Gu", "Julian Ibarz", "S. Levine"], "venue": "International Conference on Learning Representations", "abstract": "Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a large amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires extensive human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and reset policy, with the reset policy resetting the environment for a subsequent attempt. By learning a value function for the reset policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the reset policy can greatly reduce the number of manual resets required to learn a task, can reduce the number of unsafe actions that lead to non-reversible states, and can automatically induce a curriculum.", "year": 2017, "publicationdate": "2017-11-18", "externalids": {}, "doi_lower": null}
{"paper_id": 278677018, "title": "The Ethical Hackers of AI: Understanding the Merit in Unauthorized AI Research", "author_names": ["Eric Hawkinson"], "venue": "Together Research", "abstract": "In April 2025, researchers from the University of Zurich deployed AI bots on Reddit's r/changemyview subreddit without permission, discovering that AI-generated comments were six times more persuasive than human responses in changing users' views. While Reddit threatened legal action and condemned this as \"psychological manipulation,\" this article argues that these researchers operated as academic \"ethical hackers,\" exposing critical vulnerabilities in social media platforms. Drawing parallels to cybersecurity's white hat tradition, I examine how platforms like Reddit profit from selling user data to AI companies while restricting independent research that reveals manipulation risks. The controversy highlights a troubling asymmetry: academic researchers face strict ethical oversight while platforms conduct massive behavioral experiments on billions of users with minimal transparency. Though I cannot directly endorse violating terms of service, this research served essential public interest by demonstrating AI's manipulation capabilities that malicious actors already exploit. The article calls for frameworks that recognize when unauthorized academic research, conducted responsibly within ethical guidelines, provides crucial knowledge about AI threats to democratic discourse much as society accepts ethical hacking that strengthens cybersecurity.", "year": 2024, "publicationdate": "2024-05-15", "externalids": {"DOI": "10.62883/faxo3943"}, "doi_lower": "10.62883/faxo3943"}
{"paper_id": 263269065, "title": "Interactive machine learning", "author_names": ["J. Fails", "D. Olsen"], "venue": "International Conference on Intelligent User Interfaces", "abstract": null, "year": 2003, "publicationdate": "2003-01-12", "externalids": {"DOI": "10.1145/604045.604056"}, "doi_lower": "10.1145/604045.604056"}
{"paper_id": 253759631, "title": "Human-level play in the game of Diplomacy by combining language models with strategic reasoning", "author_names": ["A. Bakhtin", "Noam Brown", "Emily Dinan", "Gabriele Farina", "Colin Flaherty", "Daniel Fried", "Andrew Goff", "Jonathan Gray", "Hengyuan Hu", "Athul Paul Jacob", "Mo-jtaba Komeili", "Karthik Konath", "Minae Kwon", "Adam Lerer", "Mike Lewis", "Alexander H. Miller", "S. Mitts", "Adithya Renduchintala", "Stephen Roller", "Dirk Rowe", "Weiyan Shi", "Joe Spisak", "Alexander Wei", "David J. Wu", "Hugh Zhang", "Markus Zijlstra"], "venue": "Science", "abstract": "Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game. Description AI masters Diplomacy The game Diplomacy has been a major challenge for artificial intelligence (AI). Unlike other competitive games that AI has recently mastered, such as chess, Go, and poker, Diplomacy cannot be solved purely through self-play; it requires the development of an agent to understand other players’ motivations and perspectives and to use natural language to negotiate complex shared plans. The Meta Fundamental AI Research Diplomacy Team (FAIR) et al. developed an agent that is able to play the full natural language form of the game and demonstrates performance well above the human average in an online Diplomacy league. The present work has far-reaching implications for the development of cooperative AI and language models for communication with people, even when interactions involve a mixture of aligned and competing interests. —YS Artificial intelligence demonstrates human-level performance in the strategic board game Diplomacy.", "year": 2022, "publicationdate": "2022-11-22", "externalids": {"DOI": "10.1126/science.ade9097"}, "doi_lower": "10.1126/science.ade9097"}
{"paper_id": 196187162, "title": "Ranking Generated Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference", "author_names": ["Tobias Falke", "Leonardo F. R. Ribeiro", "Prasetya Ajie Utama", "Ido Dagan", "Iryna Gurevych"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "While recent progress on abstractive summarization has led to remarkably fluent summaries, factual errors in generated summaries still severely limit their use in practice. In this paper, we evaluate summaries produced by state-of-the-art models via crowdsourcing and show that such errors occur frequently, in particular with more abstractive models. We study whether textual entailment predictions can be used to detect such errors and if they can be reduced by reranking alternative predicted summaries. That leads to an interesting downstream application for entailment models. In our experiments, we find that out-of-the-box entailment models trained on NLI datasets do not yet offer the desired performance for the downstream task and we therefore release our annotations as additional test data for future extrinsic evaluations of NLI.", "year": 2019, "publicationdate": "2019-05-27", "externalids": {"DOI": "10.18653/v1/P19-1213"}, "doi_lower": "10.18653/v1/p19-1213"}
{"paper_id": 227240484, "title": "On Interpretability of Artificial Neural Networks: A Survey", "author_names": ["Fenglei Fan", "Jinjun Xiong", "Mengzhou Li", "Ge Wang"], "venue": "IEEE Transactions on Radiation and Plasma Medical Sciences", "abstract": "Deep learning as performed by artificial deep neural networks (DNNs) has achieved great successes recently in many important areas that deal with text, images, videos, graphs, and so on. However, the black-box nature of DNNs has become one of the primary obstacles for their wide adoption in mission-critical applications such as medical diagnosis and therapy. Because of the huge potentials of deep learning, the interpretability of DNNs has recently attracted much research attention. In this article, we propose a simple but comprehensive taxonomy for interpretability, systematically review recent studies on interpretability of neural networks, describe applications of interpretability in medicine, and discuss future research directions, such as in relation to fuzzy logic and brain science.", "year": 2020, "publicationdate": "2020-01-08", "externalids": {"DOI": "10.1109/TRPMS.2021.3066428"}, "doi_lower": "10.1109/trpms.2021.3066428"}
{"paper_id": 202733441, "title": "Survey of imitation learning for robotic manipulation", "author_names": ["Bin Fang", "Shi-Dong Jia", "Di Guo", "Muhua Xu", "Shuhuan Wen", "F. Sun"], "venue": "International Journal of Intelligent Robotics and Applications", "abstract": null, "year": 2019, "publicationdate": "2019-09-23", "externalids": {"DOI": "10.1007/s41315-019-00103-5"}, "doi_lower": "10.1007/s41315-019-00103-5"}
{"paper_id": 258426970, "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "author_names": ["Patrick Fernandes", "Aman Madaan", "Emmy Liu", "António Farinhas", "Pedro Henrique Martins", "Amanda Bertsch", "José G. C. de Souza", "Shuyan Zhou", "Tongshuang Sherry Wu", "Graham Neubig", "André F. T. Martins"], "venue": "arXiv.org", "abstract": "Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.", "year": 2023, "publicationdate": "2023-05-01", "externalids": {"DOI": "10.48550/arXiv.2305.00955"}, "doi_lower": "10.48550/arxiv.2305.00955"}
{"paper_id": 211041145, "title": "Adoption Dynamics and Societal Impact of AI Systems in Complex Networks", "author_names": ["Pedro M. Fernandes", "F. C. Santos", "Manuel Lopes"], "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "We propose a game-theoretical model to simulate the dynamics of AI adoption in adaptive networks. This formalism allows us to understand the impact of the adoption of AI systems for society as a whole, addressing some of the concerns on the need for regulation. Using this model we study the adoption of AI systems, the distribution of the different types of AI (from selfish to utilitarian), the appearance of clusters of specific AI types, and the impact on the fitness of each individual. We suggest that the entangled evolution of individual strategy and network structure constitutes a key mechanism for the sustainability of utilitarian and human-conscious AI. Differently, in the absence of rewiring, a minority of the population can easily foster the adoption of selfish AI and gains a benefit at the expense of the remaining majority.", "year": 2020, "publicationdate": "2020-02-04", "externalids": {"DOI": "10.1145/3375627.3375847"}, "doi_lower": "10.1145/3375627.3375847"}
{"paper_id": 220646981, "title": "Multi-Principal Assistance Games", "author_names": ["Arnaud Fickinger", "Simon Zhuang", "Dylan Hadfield-Menell", "Stuart J. Russell"], "venue": "arXiv.org", "abstract": "Assistance games (also known as cooperative inverse reinforcement learning games) have been proposed as a model for beneficial AI, wherein a robotic agent must act on behalf of a human principal but is initially uncertain about the humans payoff function. This paper studies multi-principal assistance games, which cover the more general case in which the robot acts on behalf of N humans who may have widely differing payoffs. Impossibility theorems in social choice theory and voting theory can be applied to such games, suggesting that strategic behavior by the human principals may complicate the robots task in learning their payoffs. We analyze in particular a bandit apprentice game in which the humans act first to demonstrate their individual preferences for the arms and then the robot acts to maximize the sum of human payoffs. We explore the extent to which the cost of choosing suboptimal arms reduces the incentive to mislead, a form of natural mechanism design. In this context we propose a social choice method that uses shared control of a system to combine preference inference with social welfare optimization.", "year": 2020, "publicationdate": "2020-07-19", "externalids": {}, "doi_lower": null}
{"paper_id": 214706017, "title": "Implicit Learning Dynamics in Stackelberg Games: Equilibria Characterization, Convergence Analysis, and Empirical Study", "author_names": ["Tanner Fiez", "Benjamin J. Chasnov", "L. Ratliff"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 2020, "publicationdate": "2020-07-12", "externalids": {}, "doi_lower": null}
{"paper_id": 270379567, "title": "Inverse Constitutional AI: Compressing Preferences into Principles", "author_names": ["Arduin Findeis", "Timo Kaufmann", "Eyke Hüllermeier", "Samuel Albanie", "Robert Mullins"], "venue": "International Conference on Learning Representations", "abstract": "Feedback data is widely used for fine-tuning and evaluating state-of-the-art AI models. Pairwise text preferences, where human or AI annotators select the\"better\"of two options, are particularly common. Such preferences are used to train (reward) models or to rank models with aggregate statistics. For many applications it is desirable to understand annotator preferences in addition to modelling them - not least because extensive prior work has shown various unintended biases in preference datasets. Yet, preference datasets remain challenging to interpret. Neither black-box reward models nor statistics can answer why one text is preferred over another. Manual interpretation of the numerous (long) response pairs is usually equally infeasible. In this paper, we introduce the Inverse Constitutional AI (ICAI) problem, formulating the interpretation of pairwise text preference data as a compression task. In constitutional AI, a set of principles (a constitution) is used to provide feedback and fine-tune AI models. ICAI inverts this process: given a feedback dataset, we aim to extract a constitution that best enables a large language model (LLM) to reconstruct the original annotations. We propose a corresponding ICAI algorithm and validate its generated constitutions quantitatively based on annotation reconstruction accuracy on several datasets: (a) synthetic feedback data with known principles; (b) AlpacaEval cross-annotated human feedback data; (c) crowdsourced Chatbot Arena data; and (d) PRISM data from diverse demographic groups. As a short and interpretable representation of the original dataset, generated constitutions have many potential use cases: help identify undesirable annotator biases, understand model performance better, scale feedback to unseen data, or adapt models to individual user or group preferences. We release the source code at https://github.com/rdnfn/icai.", "year": 2024, "publicationdate": "2024-06-02", "externalids": {"DOI": "10.48550/arXiv.2406.06560"}, "doi_lower": "10.48550/arxiv.2406.06560"}
{"paper_id": 3708017, "title": "Pragmatic-Pedagogic Value Alignment", "author_names": ["J. Fisac", "Monica A. Gates", "Jessica B. Hamrick", "Chang Liu", "Dylan Hadfield-Menell", "Malayandi Palaniappan", "Dhruv Malik", "S. Sastry", "T. Griffiths", "A. Dragan"], "venue": "International Symposium of Robotics Research", "abstract": "As intelligent systems gain autonomy and capability, it becomes vital to ensure that their objectives match those of their human users; this is known as the value-alignment problem. In robotics, value alignment is key to the design of collaborative robots that can integrate into human workflows, successfully inferring and adapting to their users' objectives as they go. We argue that a meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition, enabling robots to tap into people's natural collaborative capabilities. We present a solution to the cooperative inverse reinforcement learning (CIRL) dynamic game based on well-established cognitive models of decision making and theory of mind. The solution captures a key reciprocity relation: the human will not plan her actions in isolation, but rather reason pedagogically about how the robot might learn from them; the robot, in turn, can anticipate this and interpret the human's actions pragmatically. To our knowledge, this work constitutes the first formal analysis of value alignment grounded in empirically validated cognitive models.", "year": 2017, "publicationdate": "2017-07-01", "externalids": {"DOI": "10.1007/978-3-030-28619-4_7"}, "doi_lower": "10.1007/978-3-030-28619-4_7"}
{"paper_id": 53854465, "title": "AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations", "author_names": ["L. Floridi", "Josh Cowls", "Monica Beltrametti", "R. Chatila", "Patrice Chazerand", "Virginia Dignum", "C. Luetge", "Robert Madelin", "U. Pagallo", "F. Rossi", "Burkhard Schafer", "P. Valcke", "E. Vayena"], "venue": "Minds and Machines", "abstract": "This article reports the findings of AI4People, an Atomium—EISMD initiative designed to lay the foundations for a “Good AI Society”. We introduce the core opportunities and risks of AI for society; present a synthesis of five ethical principles that should undergird its development and adoption; and offer 20 concrete recommendations—to assess, to develop, to incentivise, and to support good AI—which in some cases may be undertaken directly by national or supranational policy makers, while in others may be led by other stakeholders. If adopted, these recommendations would serve as a firm foundation for the establishment of a Good AI Society.", "year": 2018, "publicationdate": "2018-11-06", "externalids": {"DOI": "10.1007/s11023-018-9482-5"}, "doi_lower": "10.1007/s11023-018-9482-5"}
{"paper_id": 53391180, "title": "Learning to Communicate with Deep Multi-Agent Reinforcement Learning", "author_names": ["Jakob N. Foerster", "Yannis Assael", "Nando de Freitas", "Shimon Whiteson"], "venue": "Neural Information Processing Systems", "abstract": "We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.", "year": 2016, "publicationdate": "2016-05-21", "externalids": {}, "doi_lower": null}
{"paper_id": 9634654, "title": "Input Switched Affine Networks: An RNN Architecture Designed for Interpretability", "author_names": ["Jakob N. Foerster", "J. Gilmer", "Jascha Narain Sohl-Dickstein", "J. Chorowski", "David Sussillo"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 2016, "publicationdate": "2016-11-28", "externalids": {}, "doi_lower": null}
{"paper_id": 2738204, "title": "Net2Vec: Quantifying and Explaining How Concepts are Encoded by Filters in Deep Neural Networks", "author_names": ["Ruth Fong", "A. Vedaldi"], "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "abstract": "In an effort to understand the meaning of the intermediate representations captured by deep networks, recent papers have tried to associate specific semantic concepts to individual neural network filter responses, where interesting correlations are often found, largely by focusing on extremal filter responses. In this paper, we show that this approach can favor easy-to-interpret cases that are not necessarily representative of the average behavior of a representation. A more realistic but harder-to-study hypothesis is that semantic representations are distributed, and thus filters must be studied in conjunction. In order to investigate this idea while enabling systematic visualization and quantification of multiple filter responses, we introduce the Net2Vec framework, in which semantic concepts are mapped to vectorial embeddings based on corresponding filter responses. By studying such embeddings, we are able to show that 1., in most cases, multiple filters are required to code for a concept, that 2., often filters are not concept specific and help encode multiple concepts, and that 3., compared to single filter activations, filter embeddings are able to better characterize the meaning of a representation and its relationship to other concepts.", "year": 2018, "publicationdate": "2018-01-10", "externalids": {"DOI": "10.1109/CVPR.2018.00910"}, "doi_lower": "10.1109/cvpr.2018.00910"}
{"paper_id": 226226666, "title": "Social Chemistry 101: Learning to Reason about Social and Moral Norms", "author_names": ["Maxwell Forbes", "Jena D. Hwang", "Vered Shwartz", "Maarten Sap", "Yejin Choi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Social norms---the unspoken commonsense rules about acceptable social behavior---are crucial in understanding the underlying causes and intents of people's actions in narratives. For example, underlying an action such as \"wanting to call cops on my neighbors\" are social norms that inform our conduct, such as \"It is expected that you report crimes.\" \nWe present Social Chemistry, a new conceptual formalism to study people's everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce Social-Chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as \"it is rude to run a blender at 5am\" as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people's judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. \nComprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes Social-Chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.", "year": 2020, "publicationdate": "2020-11-01", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.48"}, "doi_lower": "10.18653/v1/2020.emnlp-main.48"}
{"paper_id": 202774726, "title": "Learning to Predict Without Looking Ahead: World Models Without Forward Prediction", "author_names": ["C. Freeman", "Luke Metz", "David Ha"], "venue": "Neural Information Processing Systems", "abstract": "Much of model-based reinforcement learning involves learning a model of an agent's world, and training an agent to leverage this model to perform a task more efficiently. While these models are demonstrably useful for agents, every naturally occurring model of the world of which we are aware---e.g., a brain---arose as the byproduct of competing evolutionary pressures for survival, not minimization of a supervised forward-predictive loss via gradient descent. That useful models can arise out of the messy and slow optimization process of evolution suggests that forward-predictive modeling can arise as a side-effect of optimization under the right circumstances. Crucially, this optimization process need not explicitly be a forward-predictive loss. In this work, we introduce a modification to traditional reinforcement learning which we call observational dropout, whereby we limit the agents ability to observe the real environment at each timestep. In doing so, we can coerce an agent into learning a world model to fill in the observation gaps during reinforcement learning. We show that the emerged world model, while not explicitly trained to predict the future, can help the agent learn key skills required to perform well in its environment. Videos of our results available at https://learningtopredict.github.io/", "year": 2019, "publicationdate": "2019-10-01", "externalids": {}, "doi_lower": null}
{"paper_id": 67788344, "title": "From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following", "author_names": ["Justin Fu", "Anoop Korattikara Balan", "S. Levine", "S. Guadarrama"], "venue": "International Conference on Learning Representations", "abstract": "Reinforcement learning is a promising framework for solving control problems, but its use in practical situations is hampered by the fact that reward functions are often difficult to engineer. Specifying goals and tasks for autonomous machines, such as robots, is a significant challenge: conventionally, reward functions and goal states have been used to communicate objectives. But people can communicate objectives to each other simply by describing or demonstrating them. How can we build learning algorithms that will allow us to tell machines what we want them to do? In this work, we investigate the problem of grounding language commands as reward functions using inverse reinforcement learning, and argue that language-conditioned rewards are more transferable than language-conditioned policies to new environments. We propose language-conditioned reward learning (LC-RL), which grounds language commands as a reward function represented by a deep neural network. We demonstrate that our model learns rewards that transfer to novel tasks and environments on realistic, high-dimensional visual environments with natural language commands, whereas directly learning a language-conditioned policy leads to poor performance.", "year": 2019, "publicationdate": "2019-02-20", "externalids": {}, "doi_lower": null}
{"paper_id": 263870625, "title": "Learning Robust Rewards with Adverserial Inverse Reinforcement Learning", "author_names": ["Justin Fu", "Katie Luo", "Sergey Levine"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 44070464, "title": "Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition", "author_names": ["Justin Fu", "Avi Singh", "Dibya Ghosh", "Larry Yang", "S. Levine"], "venue": "Neural Information Processing Systems", "abstract": "The design of a reward function often poses a major practical challenge to real-world applications of reinforcement learning. Approaches such as inverse reinforcement learning attempt to overcome this challenge, but require expert demonstrations, which can be difficult or expensive to obtain in practice. We propose variational inverse control with events (VICE), which generalizes inverse reinforcement learning methods to cases where full demonstrations are not needed, such as when only samples of desired goal states are available. Our method is grounded in an alternative perspective on control and reinforcement learning, where an agent's goal is to maximize the probability that one or more events will happen at some point in the future, rather than maximizing cumulative rewards. We demonstrate the effectiveness of our methods on continuous control tasks, with a focus on high-dimensional observations like images where rewards are hard or even impossible to specify.", "year": 2018, "publicationdate": "2018-05-01", "externalids": {}, "doi_lower": null}
{"paper_id": 198904251, "title": "Innovation Policy in a Global Economy Innovation Policy in a Global Economy", "author_names": ["D. Archibugi", "J. Howells", "J. Michie"], "venue": "", "abstract": null, "year": 1999, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 4735672, "title": "Pairwise Preference Learning and Ranking", "author_names": ["Johannes Fürnkranz", "Eyke Hüllermeier"], "venue": "European Conference on Machine Learning", "abstract": null, "year": 2003, "publicationdate": "2003-09-22", "externalids": {"DOI": "10.1007/978-3-540-39857-8_15"}, "doi_lower": "10.1007/978-3-540-39857-8_15"}
{"paper_id": 1948153, "title": "Contents Johannes Fürnkranz and Eyke Hüllermeier Pairwise Preference Learning and Ranking Active Learning of Ranking Functions Weighting Predictors in Memory-based Collaborative Filtering Björn Fiehn and Jörg Müller User-adaptive Matchmaking Pairwise Preference Learning and Ranking", "author_names": ["Eyke Hüllermeier"], "venue": "", "abstract": null, "year": 2003, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 264186982, "title": "Towards G20 Guiding Principles on Investment Facilitation for Sustainable Development-G20 Insights", "author_names": ["K. Sauvant"], "venue": "", "abstract": null, "year": 2020, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 210920551, "title": "Artificial Intelligence, Values, and Alignment", "author_names": ["Iason Gabriel"], "venue": "Minds and Machines", "abstract": "This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify ‘true’ moral principles for AI; rather, it is to identify fair principles for alignment that receive reflective endorsement despite widespread variation in people’s moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.", "year": 2020, "publicationdate": "2020-01-13", "externalids": {"DOI": "10.1007/s11023-020-09539-2"}, "doi_lower": "10.1007/s11023-020-09539-2"}
{"paper_id": 231627479, "title": "The Challenge of Value Alignment: from Fairer Algorithms to AI Safety", "author_names": ["Iason Gabriel", "Vafa Ghazavi"], "venue": "arXiv.org", "abstract": "This paper addresses the question of how to align AI systems with human values and situates it within a wider body of thought regarding technology and value. Far from existing in a vacuum, there has long been an interest in the ability of technology to 'lock-in' different value systems. There has also been considerable thought about how to align technologies with specific social values, including through participatory design-processes. In this paper we look more closely at the question of AI value alignment and suggest that the power and autonomy of AI systems gives rise to opportunities and challenges in the domain of value that have not been encountered before. Drawing important continuities between the work of the fairness, accountability, transparency and ethics community, and work being done by technical AI safety researchers, we suggest that more attention needs to be paid to the question of 'social value alignment' - that is, how to align AI systems with the plurality of values endorsed by groups of people, especially on the global level.", "year": 2021, "publicationdate": "2021-01-15", "externalids": {}, "doi_lower": null}
{"paper_id": 219573512, "title": "Large-Scale Adversarial Training for Vision-and-Language Representation Learning", "author_names": ["Zhe Gan", "Yen-Chun Chen", "Linjie Li", "Chen Zhu", "Yu Cheng", "Jingjing Liu"], "venue": "Neural Information Processing Systems", "abstract": "We present VILLA, the first known effort on large-scale adversarial training for vision-and-language (V+L) representation learning. VILLA consists of two training stages: (i) task-agnostic adversarial pre-training; followed by (ii) task-specific adversarial finetuning. Instead of adding adversarial perturbations on image pixels and textual tokens, we propose to perform adversarial training in the embedding space of each modality. To enable large-scale training, we adopt the \"free\" adversarial training strategy, and combine it with KL-divergence-based regularization to promote higher invariance in the embedding space. We apply VILLA to current best-performing V+L models, and achieve new state of the art on a wide range of tasks, including Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR2.", "year": 2020, "publicationdate": "2020-06-11", "externalids": {}, "doi_lower": null}
{"paper_id": 252355458, "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned", "author_names": ["Deep Ganguli", "Liane Lovitt", "John Kernion", "Amanda Askell", "Yuntao Bai", "Saurav Kadavath", "Benjamin Mann", "Ethan Perez", "Nicholas Schiefer", "Kamal Ndousse", "Andy Jones", "Sam Bowman", "Anna Chen", "Tom Conerly", "Nova Dassarma", "Dawn Drain", "Nelson Elhage", "S. El-Showk", "Stanislav Fort", "Z. Dodds", "T. Henighan", "Danny Hernandez", "Tristan Hume", "Josh Jacobson", "Scott Johnston", "Shauna Kravec", "Catherine Olsson", "Sam Ringer", "Eli Tran-Johnson", "Dario Amodei", "Tom B. Brown", "Nicholas Joseph", "Sam McCandlish", "Chris Olah", "Jared Kaplan", "Jack Clark"], "venue": "arXiv.org", "abstract": "We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.", "year": 2022, "publicationdate": "2022-08-23", "externalids": {"DOI": "10.48550/arXiv.2209.07858"}, "doi_lower": "10.48550/arxiv.2209.07858"}
{"paper_id": 252992904, "title": "Scaling Laws for Reward Model Overoptimization", "author_names": ["Leo Gao", "John Schulman", "Jacob Hilton"], "venue": "International Conference on Machine Learning", "abstract": "In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed\"gold-standard\"reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-$n$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.", "year": 2022, "publicationdate": "2022-10-19", "externalids": {}, "doi_lower": null}
{"paper_id": 270286001, "title": "Scaling and evaluating sparse autoencoders", "author_names": ["Leo Gao", "Tom Dupr'e la Tour", "Henk Tillman", "Gabriel Goh", "Rajan Troll", "Alec Radford", "I. Sutskever", "Jan Leike", "Jeffrey Wu"], "venue": "International Conference on Learning Representations", "abstract": "Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.", "year": 2024, "publicationdate": "2024-06-06", "externalids": {"DOI": "10.48550/arXiv.2406.04093"}, "doi_lower": "10.48550/arxiv.2406.04093"}
{"paper_id": 218441057, "title": "A comprehensive survey on safe reinforcement learning", "author_names": ["GarcíaJavier", "FernándezFernando"], "venue": "", "abstract": null, "year": 2015, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 4055784, "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs", "author_names": ["T. Garipov", "Pavel Izmailov", "Dmitrii Podoprikhin", "D. Vetrov", "A. Wilson"], "venue": "Neural Information Processing Systems", "abstract": "The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.", "year": 2018, "publicationdate": "2018-02-27", "externalids": {}, "doi_lower": null}
{"paper_id": 2343960, "title": "Logical Induction", "author_names": ["Scott Garrabrant", "Tsvi Benson-Tilsen", "Andrew Critch", "N. Soares", "Jessica Taylor"], "venue": "Electron. Colloquium Comput. Complex.", "abstract": "We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of π are difficult to predict, then a logical inductor learns to assign ≈ 10% probability to “the nth digit of π is a 7” for large n. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever φ → ψ, P∞(φ) ≤ P∞(ψ), and so on); and logical inductors strictly dominate the universal semimeasure in the limit. These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence φ is associated with a stock that is worth $1 per share if φ is true and nothing otherwise, and we interpret the belief-state of a logically uncertain reasoner as a set of market prices, where Pn(φ) = 50% means that on day n, shares of φ may be bought or sold from the reasoner for 50¢. The logical induction criterion says (very roughly) that there should not be any polynomial-time computable trading strategy with finite risk tolerance that earns unbounded profits in that market over time. This criterion bears strong resemblance to the “no Dutch book” criteria that support both expected utility theory (von Neumann and Morgenstern 1944) and Bayesian probability theory (Ramsey 1931; de Finetti 1937).", "year": 2016, "publicationdate": "2016-09-12", "externalids": {}, "doi_lower": null}
{"paper_id": 221878771, "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models", "author_names": ["Samuel Gehman", "Suchin Gururangan", "Maarten Sap", "Yejin Choi", "Noah A. Smith"], "venue": "Findings", "abstract": "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning “bad” words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.", "year": 2020, "publicationdate": "2020-09-24", "externalids": {"DOI": "10.18653/v1/2020.findings-emnlp.301"}, "doi_lower": "10.18653/v1/2020.findings-emnlp.301"}
{"paper_id": 54101493, "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness", "author_names": ["Robert Geirhos", "Patricia Rubisch", "Claudio Michaelis", "M. Bethge", "Felix Wichmann", "Wieland Brendel"], "venue": "International Conference on Learning Representations", "abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on \"Stylized-ImageNet\", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.", "year": 2018, "publicationdate": "2018-09-27", "externalids": {}, "doi_lower": null}
{"paper_id": 248392008, "title": "LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models", "author_names": ["Mor Geva", "Avi Caciularu", "Guy Dar", "Paul Roit", "Shoval Sadde", "Micah Shlain", "Bar Tamir", "Yoav Goldberg"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The opaque nature and unexplained behavior of transformer-based language models (LMs) have spurred a wide interest in interpreting their predictions. However, current interpretation methods mostly focus on probing models from outside, executing behavioral tests, and analyzing salience input features, while the internal prediction construction process is largely not understood. In this work, we introduce LM-Debugger, an interactive debugger tool for transformer-based LMs, which provides a fine-grained interpretation of the model's internal prediction process, as well as a powerful framework for intervening in LM behavior. For its backbone, LM-Debugger relies on a recent method that interprets the inner token representations and their updates by the feed-forward layers in the vocabulary space. We demonstrate the utility of LM-Debugger for single-prediction debugging, by inspecting the internal disambiguation process done by GPT2. Moreover, we show how easily LM-Debugger allows to shift model behavior in a direction of the user's choice, by identifying a few vectors in the network and inducing effective interventions to the prediction process. We release LM-Debugger as an open-source tool and a demo over GPT2 models.", "year": 2022, "publicationdate": "2022-04-26", "externalids": {"DOI": "10.48550/arXiv.2204.12130"}, "doi_lower": "10.48550/arxiv.2204.12130"}
{"paper_id": 229923720, "title": "Transformer Feed-Forward Layers Are Key-Value Memories", "author_names": ["Mor Geva", "R. Schuster", "Jonathan Berant", "Omer Levy"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Feed-forward layers constitute two-thirds of a transformer model’s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys’ input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model’s layers via residual connections to produce the final output distribution.", "year": 2020, "publicationdate": "2020-12-29", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.446"}, "doi_lower": "10.18653/v1/2021.emnlp-main.446"}
{"paper_id": 207880680, "title": "A Divergence Minimization Perspective on Imitation Learning Methods", "author_names": ["Seyed Kamyar Seyed Ghasemipour", "R. Zemel", "S. Gu"], "venue": "Conference on Robot Learning", "abstract": "In many settings, it is desirable to learn decision-making and control policies through learning or bootstrapping from expert demonstrations. The most common approaches under this Imitation Learning (IL) framework are Behavioural Cloning (BC), and Inverse Reinforcement Learning (IRL). Recent methods for IRL have demonstrated the capacity to learn effective policies with access to a very limited set of demonstrations, a scenario in which BC methods often fail. Unfortunately, due to multiple factors of variation, directly comparing these methods does not provide adequate intuition for understanding this difference in performance. In this work, we present a unified probabilistic perspective on IL algorithms based on divergence minimization. We present $f$-MAX, an $f$-divergence generalization of AIRL [Fu et al., 2018], a state-of-the-art IRL method. $f$-MAX enables us to relate prior IRL methods such as GAIL [Ho & Ermon, 2016] and AIRL [Fu et al., 2018], and understand their algorithmic properties. Through the lens of divergence minimization we tease apart the differences between BC and successful IRL approaches, and empirically evaluate these nuances on simulated high-dimensional continuous control domains. Our findings conclusively identify that IRL's state-marginal matching objective contributes most to its superior performance. Lastly, we apply our new understanding of IL methods to the problem of state-marginal matching, where we demonstrate that in simulated arm pushing environments we can teach agents a diverse range of behaviours using simply hand-specified state distributions and no reward functions or expert demonstrations. For datasets and reproducing results please refer to this https URL .", "year": 2019, "publicationdate": "2019-11-06", "externalids": {}, "doi_lower": null}
{"paper_id": 260488247, "title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "author_names": [], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 252596089, "title": "Improving alignment of dialogue agents via targeted human judgements", "author_names": ["Amelia Glaese", "Nat McAleese", "Maja Trkebacz", "John Aslanides", "Vlad Firoiu", "T. Ewalds", "Maribeth Rauh", "Laura Weidinger", "Martin Chadwick", "Phoebe Thacker", "Lucy Campbell-Gillingham", "Jonathan Uesato", "Po-Sen Huang", "R. Comanescu", "Fan Yang", "A. See", "Sumanth Dathathri", "Rory Greig", "Charlie Chen", "Doug Fritz", "Jaume Sanchez Elias", "Richard Green", "Sovna Mokr'a", "Nicholas Fernando", "Boxi Wu", "Rachel Foley", "Susannah Young", "Iason Gabriel", "William S. Isaac", "John F. J. Mellor", "D. Hassabis", "K. Kavukcuoglu", "Lisa Anne Hendricks", "G. Irving"], "venue": "arXiv.org", "abstract": "We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.", "year": 2022, "publicationdate": "2022-09-28", "externalids": {"DOI": "10.48550/arXiv.2209.14375"}, "doi_lower": "10.48550/arxiv.2209.14375"}
{"paper_id": 233823418, "title": "Multimodal Neurons in Artificial Neural Networks", "author_names": ["Gabriel Goh", "Nick Cammarata", "Chelsea Voss", "Shan Carter", "Michael Petrov", "Ludwig Schubert", "Alec Radford", "Christopher Olah"], "venue": "Distill", "abstract": null, "year": 2021, "publicationdate": "2021-03-04", "externalids": {"DOI": "10.23915/DISTILL.00030"}, "doi_lower": "10.23915/distill.00030"}
{"paper_id": 255595557, "title": "Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations", "author_names": ["Josh A. Goldstein", "Girish Sastry", "Micah Musser", "Renee DiResta", "M. Gentzel", "Katerina Sedova"], "venue": "arXiv.org", "abstract": "Generative language models have improved drastically, and can now produce realistic text outputs that are difficult to distinguish from human-written content. For malicious actors, these language models bring the promise of automating the creation of convincing and misleading text for use in influence operations. This report assesses how language models might change influence operations in the future, and what steps can be taken to mitigate this threat. We lay out possible changes to the actors, behaviors, and content of online influence operations, and provide a framework for stages of the language model-to-influence operations pipeline that mitigations could target (model construction, model access, content dissemination, and belief formation). While no reasonable mitigation can be expected to fully prevent the threat of AI-enabled influence operations, a combination of multiple mitigations may make an important difference.", "year": 2023, "publicationdate": "2023-01-10", "externalids": {"DOI": "10.48550/arXiv.2301.04246"}, "doi_lower": "10.48550/arxiv.2301.04246"}
{"paper_id": 168522062, "title": "Problems of Monetary Management: The UK Experience", "author_names": ["C. Goodhart"], "venue": "", "abstract": null, "year": 1984, "publicationdate": null, "externalids": {"DOI": "10.1007/978-1-349-17295-5_4"}, "doi_lower": "10.1007/978-1-349-17295-5_4"}
{"paper_id": 269484740, "title": "A Primer on the Inner Workings of Transformer-based Language Models", "author_names": ["Javier Ferrando", "Gabriele Sarti", "Arianna Bisazza", "M. Costa-jussà"], "venue": "arXiv.org", "abstract": "The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area. This primer provides a concise technical introduction to the current techniques used to interpret the inner workings of Transformer-based language models, focusing on the generative decoder-only architecture. We conclude by presenting a comprehensive overview of the known internal mechanisms implemented by these models, uncovering connections across popular approaches and active research directions in this area.", "year": 2024, "publicationdate": "2024-04-30", "externalids": {"DOI": "10.48550/arXiv.2405.00208"}, "doi_lower": "10.48550/arxiv.2405.00208"}
{"paper_id": 251475176, "title": "Overview and commentary of the CDEI's extended roadmap to an effective AI assurance ecosystem", "author_names": ["Ethan Barrance", "Emre Kazim", "Airlie Hilliard", "Markus Trengove", "Sara Zannone", "A. Koshiyama"], "venue": "Frontiers in Artificial Intelligence", "abstract": "In recent years, the field of ethical artificial intelligence (AI), or AI ethics, has gained traction and aims to develop guidelines and best practices for the responsible and ethical use of AI across sectors. As part of this, nations have proposed AI strategies, with the UK releasing both national AI and data strategies, as well as a transparency standard. Extending these efforts, the Centre for Data Ethics and Innovation (CDEI) has published an AI Assurance Roadmap, which is the first of its kind and provides guidance on how to manage the risks that come from the use of AI. In this article, we provide an overview of the document's vision for a “mature AI assurance ecosystem” and how the CDEI will work with other organizations for the development of regulation, industry standards, and the creation of AI assurance practitioners. We also provide a commentary of some key themes identified in the CDEI's roadmap in relation to (i) the complexities of building “justified trust”, (ii) the role of research in AI assurance, (iii) the current developments in the AI assurance industry, and (iv) convergence with international regulation.", "year": 2022, "publicationdate": "2022-08-10", "externalids": {"DOI": "10.3389/frai.2022.932358"}, "doi_lower": "10.3389/frai.2022.932358"}
{"paper_id": 264559386, "title": "Capabilities and risks from frontier AI", "author_names": ["Y. Bengio", "Sara Hooker", "Arvind Narayanan", "William Isaac", "Paul F. Christiano", "Irene Solaiman", "Alexander Babuta"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 70350059, "title": "Using Natural Language for Reward Shaping in Reinforcement Learning", "author_names": ["Prasoon Goyal", "S. Niekum", "R. Mooney"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Recent reinforcement learning (RL) approaches have shown strong performance in complex domains, such as Atari games, but are highly sample inefficient. A common approach to reduce interaction time with the environment is to use reward shaping, which involves carefully designing reward functions that provide the agent intermediate rewards for progress towards the goal. Designing such rewards remains a challenge, though. In this work, we use natural language instructions to perform reward shaping. We propose a framework that maps free-form natural language instructions to intermediate rewards, that can seamlessly be integrated into any standard reinforcement learning algorithm. We experiment with Montezuma's Revenge from the Atari video games domain, a popular benchmark in RL. Our experiments on a diverse set of 15 tasks demonstrate that for the same number of interactions with the environment, using language-based rewards can successfully complete the task 60% more often, averaged across all tasks, compared to learning without language.", "year": 2019, "publicationdate": "2019-03-05", "externalids": {"DOI": "10.24963/ijcai.2019/331"}, "doi_lower": "10.24963/ijcai.2019/331"}
{"paper_id": 73704727, "title": "The Microsoft Case and Google", "author_names": ["Stephen D. Houck"], "venue": "", "abstract": null, "year": 2012, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 266174025, "title": "AI Control: Improving Safety Despite Intentional Subversion", "author_names": ["R. Greenblatt", "Buck Shlegeris", "Kshitij Sachan", "Fabien Roger"], "venue": "International Conference on Machine Learning", "abstract": "As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. Researchers have investigated a variety of safety techniques for this purpose, e.g. using models to review the outputs of other models, or red-teaming techniques to surface subtle failure modes. However, researchers have not evaluated whether such techniques still ensure safety if the model is itself intentionally trying to subvert them. In this paper, we develop and evaluate pipelines of safety techniques (\"protocols\") that are robust to intentional subversion. We investigate a scenario in which we want to solve a sequence of programming problems, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate protocols that aim to never submit solutions containing backdoors, which we operationalize here as logical errors that are not caught by test cases. We investigate a range of protocols and test each against strategies that the untrusted model could use to subvert them. One protocol is what we call trusted editing. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to rate the suspiciousness of that code. If the code is below some suspiciousness threshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove parts that seem suspicious and then submits the edited code. Another protocol is untrusted monitoring. This protocol asks GPT-4 to write code, and then asks another instance of GPT-4 whether the code is backdoored, using various techniques to prevent the GPT-4 instances from colluding. These protocols improve substantially on simple baselines.", "year": 2023, "publicationdate": "2023-12-12", "externalids": {"DOI": "10.48550/arXiv.2312.06942"}, "doi_lower": "10.48550/arxiv.2312.06942"}
{"paper_id": 14678361, "title": "Policy Shaping: Integrating Human Feedback with Reinforcement Learning", "author_names": ["Shane Griffith", "K. Subramanian", "Jonathan Scholz", "C. Isbell", "A. Thomaz"], "venue": "Neural Information Processing Systems", "abstract": null, "year": 2013, "publicationdate": "2013-12-05", "externalids": {}, "doi_lower": null}
{"paper_id": 234833859, "title": "Multi-agent deep reinforcement learning: a survey", "author_names": ["Sven Gronauer", "K. Diepold"], "venue": "Artificial Intelligence Review", "abstract": "The advances in reinforcement learning have recorded sublime success in various domains. Although the multi-agent domain has been overshadowed by its single-agent counterpart during this progress, multi-agent reinforcement learning gains rapid traction, and the latest accomplishments address problems with real-world complexity. This article provides an overview of the current developments in the field of multi-agent deep reinforcement learning. We focus primarily on literature from recent years that combines deep reinforcement learning methods with a multi-agent scenario. To survey the works that constitute the contemporary landscape, the main contents are divided into three parts. First, we analyze the structure of training schemes that are applied to train multiple agents. Second, we consider the emergent patterns of agent behavior in cooperative, competitive and mixed scenarios. Third, we systematically enumerate challenges that exclusively arise in the multi-agent domain and review methods that are leveraged to cope with these challenges. To conclude this survey, we discuss advances, identify trends, and outline possible directions for future work in this research area.", "year": 2021, "publicationdate": "2021-04-15", "externalids": {"DOI": "10.1007/s10462-021-09996-w"}, "doi_lower": "10.1007/s10462-021-09996-w"}
{"paper_id": 260682872, "title": "Studying Large Language Model Generalization with Influence Functions", "author_names": ["R. Grosse", "Juhan Bae", "Cem Anil", "Nelson Elhage", "Alex Tamkin", "Amirhossein Tajdini", "Benoit Steiner", "Dustin Li", "Esin Durmus", "Ethan Perez", "Evan Hubinger", "Kamil.e Lukovsiut.e", "Karina Nguyen", "Nicholas Joseph", "Sam McCandlish", "Jared Kaplan", "Sam Bowman"], "venue": "arXiv.org", "abstract": "When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.", "year": 2023, "publicationdate": "2023-08-07", "externalids": {"DOI": "10.48550/arXiv.2308.03296"}, "doi_lower": "10.48550/arxiv.2308.03296"}
{"paper_id": 256827471, "title": "Ground(less) Truth: A Causal Framework for Proxy Labels in Human-Algorithm Decision-Making", "author_names": ["Luke Guerdan", "Amanda Coston", "Zhiwei Steven Wu", "Kenneth Holstein"], "venue": "Conference on Fairness, Accountability and Transparency", "abstract": "A growing literature on human-AI decision-making investigates strategies for combining human judgment with statistical models to improve decision-making. Research in this area often evaluates proposed improvements to models, interfaces, or workflows by demonstrating improved predictive performance on “ground truth’’ labels. However, this practice overlooks a key difference between human judgments and model predictions. Whereas humans commonly reason about broader phenomena of interest in a decision – including latent constructs that are not directly observable, such as disease status, the “toxicity” of online comments, or future “job performance” – predictive models target proxy labels that are readily available in existing datasets. Predictive models’ reliance on simplistic proxies for these nuanced phenomena makes them vulnerable to various sources of statistical bias. In this paper, we identify five sources of target variable bias that can impact the validity of proxy labels in human-AI decision-making tasks. We develop a causal framework to disentangle the relationship between each bias and clarify which are of concern in specific human-AI decision-making tasks. We demonstrate how our framework can be used to articulate implicit assumptions made in prior modeling work, and we recommend evaluation strategies for verifying whether these assumptions hold in practice. We then leverage our framework to re-examine the designs of prior human subjects experiments that investigate human-AI decision-making, finding that only a small fraction of studies examine factors related to target variable bias. We conclude by discussing opportunities to better address target variable bias in future research.", "year": 2023, "publicationdate": "2023-02-13", "externalids": {"DOI": "10.1145/3593013.3594036"}, "doi_lower": "10.1145/3593013.3594036"}
{"paper_id": 6487585, "title": "Multiagent Planning with Factored MDPs", "author_names": ["Carlos Guestrin", "D. Koller", "Ronald E. Parr"], "venue": "Neural Information Processing Systems", "abstract": null, "year": 2001, "publicationdate": "2001-01-03", "externalids": {}, "doi_lower": null}
{"paper_id": 261031028, "title": "Reinforced Self-Training (ReST) for Language Modeling", "author_names": ["Caglar Gulcehre", "T. Paine", "S. Srinivasan", "Ksenia Konyushkova", "L. Weerts", "Abhishek Sharma", "Aditya Siddhant", "Alexa Ahern", "Miaosen Wang", "Chenjie Gu", "Wolfgang Macherey", "A. Doucet", "Orhan Firat", "Nando de Freitas"], "venue": "arXiv.org", "abstract": "Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.", "year": 2023, "publicationdate": "2023-08-17", "externalids": {}, "doi_lower": null}
{"paper_id": 263608756, "title": "Language Models Represent Space and Time", "author_names": ["Wes Gurnee", "Max Tegmark"], "venue": "International Conference on Learning Representations", "abstract": "The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual\"space neurons\"and\"time neurons\"that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.", "year": 2023, "publicationdate": "2023-10-03", "externalids": {"DOI": "10.48550/arXiv.2310.02207"}, "doi_lower": "10.48550/arxiv.2310.02207"}
{"paper_id": 3730613, "title": "The Off-Switch Game", "author_names": ["Dylan Hadfield-Menell", "A. Dragan", "P. Abbeel", "Stuart J. Russell"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R's off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H's actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.", "year": 2016, "publicationdate": "2016-11-24", "externalids": {"DOI": "10.24963/ijcai.2017/32"}, "doi_lower": "10.24963/ijcai.2017/32"}
{"paper_id": 4836808, "title": "Incomplete Contracting and AI Alignment", "author_names": ["Dylan Hadfield-Menell", "Gillian K. Hadfield"], "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.", "year": 2018, "publicationdate": "2018-04-12", "externalids": {"DOI": "10.1145/3306618.3314250"}, "doi_lower": "10.1145/3306618.3314250"}
{"paper_id": 2877073, "title": "In Advances in Neural Information Processing Systems", "author_names": ["D. Touretzky", "M. C. Mozer", "M. E. Hasselmo", "RegressionChristopher", "I. K.", "WilliamsNeural", "GroupAston", "UniversityBirmingham"], "venue": "", "abstract": null, "year": 1996, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 72940833, "title": "The Ethics of AI Ethics: An Evaluation of Guidelines", "author_names": ["Thilo Hagendorff"], "venue": "Minds and Machines", "abstract": "Current advances in research, development and application of artificial intelligence (AI) systems have yielded a far-reaching discourse on AI ethics. In consequence, a number of ethics guidelines have been released in recent years. These guidelines comprise normative principles and recommendations aimed to harness the “disruptive” potentials of new AI technologies. Designed as a semi-systematic evaluation, this paper analyzes and compares 22 guidelines, highlighting overlaps but also omissions. As a result, I give a detailed overview of the field of AI ethics. Finally, I also examine to what extent the respective ethical principles and values are implemented in the practice of research, development and application of AI systems—and how the effectiveness in the demands of AI ethics can be improved.", "year": 2019, "publicationdate": "2019-02-28", "externalids": {"DOI": "10.1007/s11023-020-09517-8"}, "doi_lower": "10.1007/s11023-020-09517-8"}
{"paper_id": 227162557, "title": "A Virtue-Based Framework to Support Putting AI Ethics into Practice", "author_names": ["Thilo Hagendorff"], "venue": "Philosophy & Technology", "abstract": "Many ethics initiatives have stipulated sets of principles and standards for good technology development in the AI sector. However, several AI ethics researchers have pointed out a lack of practical realization of these principles. Following that, AI ethics underwent a practical turn, but without deviating from the principled approach. This paper proposes a complementary to the principled approach that is based on virtue ethics. It defines four “basic AI virtues”, namely justice, honesty, responsibility and care, all of which represent specific motivational settings that constitute the very precondition for ethical decision making in the AI field. Moreover, it defines two “second-order AI virtues”, prudence and fortitude, that bolster achieving the basic virtues by helping with overcoming bounded ethicality or hidden psychological forces that can impair ethical decision making and that are hitherto disregarded in AI ethics. Lastly, the paper describes measures for successfully cultivating the mentioned virtues in organizations dealing with AI research and development.", "year": 2020, "publicationdate": "2020-11-25", "externalids": {"DOI": "10.1007/s13347-022-00553-z"}, "doi_lower": "10.1007/s13347-022-00553-z"}
{"paper_id": 258426987, "title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model", "author_names": ["Michael Hanna", "Ollie Liu", "Alexandre Variengien"], "venue": "Neural Information Processing Systems", "abstract": "Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as\"The war lasted from the year 1732 to the year 17\", and predict valid two-digit end years (years>32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we find related tasks that activate our circuit. Our results suggest that GPT-2 small computes greater-than using a complex but general mechanism that activates across diverse contexts.", "year": 2023, "publicationdate": "2023-04-30", "externalids": {"DOI": "10.48550/arXiv.2305.00586"}, "doi_lower": "10.48550/arxiv.2305.00586"}
{"paper_id": 264653968, "title": "Self-Attention Attribution: Interpreting Information Interactions Inside Transformer", "author_names": ["Y. Hao", "Li Dong", "Furu Wei", "Ke Xu"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "The great success of Transformer-based models benefits from the powerful multi-head self-attention mechanism, which learns token dependencies and encodes contextual information from the input. Prior work strives to attribute model decisions to individual input features with different saliency measures, but they fail to explain how these input features interact with each other to reach predictions. In this paper, we propose a self-attention attribution method to interpret the information interactions inside Transformer. We take BERT as an example to conduct extensive studies. Firstly, we apply self-attention attribution to identify the important attention heads, while others can be pruned with marginal performance degradation. Furthermore, we extract the most salient dependencies in each layer to construct an attribution tree, which reveals the hierarchical interactions inside Transformer. Finally, we show that the attribution results can be used as adversarial patterns to implement non-targeted attacks towards BERT.", "year": 2020, "publicationdate": "2020-04-23", "externalids": {"DOI": "10.1609/aaai.v35i14.17533"}, "doi_lower": "10.1609/aaai.v35i14.17533"}
{"paper_id": 247519233, "title": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection", "author_names": ["Thomas Hartvigsen", "Saadia Gabriel", "Hamid Palangi", "Maarten Sap", "Dipankar Ray", "Ece Kamar"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language.To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.", "year": 2022, "publicationdate": "2022-03-17", "externalids": {"DOI": "10.48550/arXiv.2203.09509"}, "doi_lower": "10.48550/arxiv.2203.09509"}
{"paper_id": 58068920, "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author_names": ["T. Hastie", "J. Friedman", "R. Tibshirani"], "venue": "Springer Series in Statistics", "abstract": null, "year": 2001, "publicationdate": null, "externalids": {"DOI": "10.1007/978-0-387-84858-7"}, "doi_lower": "10.1007/978-0-387-84858-7"}
{"paper_id": 221069872, "title": "Assisted Robust Reward Design", "author_names": ["Jerry Zhi-Yang He", "A. Dragan"], "venue": "Conference on Robot Learning", "abstract": "Real-world robotic tasks require complex reward functions. When we define the problem the robot needs to solve, we pretend that a designer specifies this complex reward exactly, and it is set in stone from then on. In practice, however, reward design is an iterative process: the designer chooses a reward, eventually encounters an\"edge-case\"environment where the reward incentivizes the wrong behavior, revises the reward, and repeats. What would it mean to rethink robotics problems to formally account for this iterative nature of reward design? We propose that the robot not take the specified reward for granted, but rather have uncertainty about it, and account for the future design iterations as future evidence. We contribute an Assisted Reward Design method that speeds up the design process by anticipating and influencing this future evidence: rather than letting the designer eventually encounter failure cases and revise the reward then, the method actively exposes the designer to such environments during the development phase. We test this method in a simplified autonomous driving task and find that it more quickly improves the car's behavior in held-out environments by proposing environments that are\"edge cases\"for the current reward.", "year": 2021, "publicationdate": "2021-11-18", "externalids": {}, "doi_lower": null}
{"paper_id": 107662887, "title": "Your bytes are your babies. Protect what's precious, keep IT safe", "author_names": ["Maartje Buijs"], "venue": "", "abstract": null, "year": 2013, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 265150251, "title": "DocGen: Generating Detailed Parameter Docstrings in Python", "author_names": ["Vatsal Venkatkrishna", "Durga Shree Nagabushanam", "Emmanuel Iko-Ojo Simon", "M. Vidoni"], "venue": "arXiv.org", "abstract": "Documentation debt hinders the effective utilization of open-source software. Although code summarization tools have been helpful for developers, most would prefer a detailed account of each parameter in a function rather than a high-level summary. However, generating such a summary is too intricate for a single generative model to produce reliably due to the lack of high-quality training data. Thus, we propose a multi-step approach that combines multiple task-specific models, each adept at producing a specific section of a docstring. The combination of these models ensures the inclusion of each section in the final docstring. We compared the results from our approach with existing generative models using both automatic metrics and a human-centred evaluation with 17 participating developers, which proves the superiority of our approach over existing methods.", "year": 2023, "publicationdate": "2023-11-11", "externalids": {"DOI": "10.48550/arXiv.2311.06453"}, "doi_lower": "10.48550/arxiv.2311.06453"}
{"paper_id": 271745834, "title": "Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning", "author_names": ["Joey Hejna", "Rafael Rafailov", "Harshit S. Sikchi", "Chelsea Finn", "S. Niekum", "W. B. Knox", "Dorsa Sadigh"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 253546930, "title": "Few-Shot Preference Learning for Human-in-the-Loop RL", "author_names": ["Joey Hejna", "Dorsa Sadigh"], "venue": "Conference on Robot Learning", "abstract": "While reinforcement learning (RL) has become a more popular approach for robotics, designing sufficiently informative reward functions for complex tasks has proven to be extremely difficult due their inability to capture human intent and policy exploitation. Preference based RL algorithms seek to overcome these challenges by directly learning reward functions from human feedback. Unfortunately, prior work either requires an unreasonable number of queries implausible for any human to answer or overly restricts the class of reward functions to guarantee the elicitation of the most informative queries, resulting in models that are insufficiently expressive for realistic robotics tasks. Contrary to most works that focus on query selection to \\emph{minimize} the amount of data required for learning reward functions, we take an opposite approach: \\emph{expanding} the pool of available data by viewing human-in-the-loop RL through the more flexible lens of multi-task learning. Motivated by the success of meta-learning, we pre-train preference models on prior task data and quickly adapt them for new tasks using only a handful of queries. Empirically, we reduce the amount of online feedback needed to train manipulation policies in Meta-World by 20$\\times$, and demonstrate the effectiveness of our method on a real Franka Panda Robot. Moreover, this reduction in query-complexity allows us to train robot policies from actual human users. Videos of our results and code can be found at https://sites.google.com/view/few-shot-preference-rl/home.", "year": 2022, "publicationdate": "2022-12-06", "externalids": {"DOI": "10.48550/arXiv.2212.03363"}, "doi_lower": "10.48550/arxiv.2212.03363"}
{"paper_id": 279148726, "title": "THE IMPACT OF AI ON PRAGMATIC COMPETENCE", "author_names": ["Nagamurali Eragamreddy"], "venue": "Journal of Teaching English for Specific and Academic Purposes", "abstract": "The pragmatic ability of artificial intelligence systems is one of the critical but under-researched areas in the research area, especially in recent times, with increased reliance on AI technologies in human communication. This study responds to the main question of the difficulties AI systems encounter in comprehending human pragmatic signals, and how these interactions affect communication dynamics. This study has, therefore, adopted qualitative and descriptive research approaches to analyze interaction data obtained from WildChat and OpenAI logs, focusing on pragmatic functions like requests, agreements, and questions. Data collection depended on anonymized natural conversations in real life, while the analytical model used theoretical frameworks first, Grice's Cooperative Principle, then Relevance Theories assess how well AI performs in interpreting implicit meanings and cultural nuances. Key findings indicate that, while AI performs well with explicit speech acts, it fails in dealing with indirectness, ambiguity, and cultural variability. Repeated interactions with AI thus lead users to simplify their communication, which gives rise to concerns about the erosion of human pragmatic abilities. The study provides conclusions for addressing the pragmatic limitations of AI in a way that could promote more natural and contextually appropriate human-AI communication. The implications of these findings for AI design, linguistic theory, and societal norms are huge.", "year": 2025, "publicationdate": "2025-05-05", "externalids": {"DOI": "10.22190/jtesap250122015e"}, "doi_lower": "10.22190/jtesap250122015e"}
{"paper_id": 257771284, "title": "Natural Selection Favors AIs over Humans", "author_names": ["Dan Hendrycks"], "venue": "arXiv.org", "abstract": "For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typically have an advantage over species that are altruistic to other species. This Darwinian logic could also apply to artificial agents, as agents may eventually be better able to persist into the future if they behave selfishly and pursue their own interests with little regard for humans, which could pose catastrophic risks. To counteract these risks and evolutionary forces, we consider interventions such as carefully designing AI agents' intrinsic motivations, introducing constraints on their actions, and institutions that encourage cooperation. These steps, or others that resolve the problems we pose, will be necessary in order to ensure the development of artificial intelligence is a positive one.", "year": 2023, "publicationdate": "2023-03-28", "externalids": {"DOI": "10.48550/arXiv.2303.16200"}, "doi_lower": "10.48550/arxiv.2303.16200"}
{"paper_id": 220250257, "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization", "author_names": ["Dan Hendrycks", "Steven Basart", "Norman Mu", "Saurav Kadavath", "Frank Wang", "Evan Dorundo", "R. Desai", "Tyler Lixuan Zhu", "Samyak Parajuli", "Mike Guo", "D. Song", "J. Steinhardt", "J. Gilmer"], "venue": "IEEE International Conference on Computer Vision", "abstract": "We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pre-trained with 1000× more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness.", "year": 2020, "publicationdate": "2020-06-29", "externalids": {"DOI": "10.1109/ICCV48922.2021.00823"}, "doi_lower": "10.1109/iccv48922.2021.00823"}
{"paper_id": 220968818, "title": "Aligning AI With Shared Human Values", "author_names": ["Dan Hendrycks", "Collin Burns", "Steven Basart", "Andrew Critch", "J. Li", "D. Song", "J. Steinhardt"], "venue": "International Conference on Learning Representations", "abstract": "We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete understanding of basic ethical knowledge. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.", "year": 2020, "publicationdate": "2020-08-05", "externalids": {}, "doi_lower": null}
{"paper_id": 238198240, "title": "Unsolved Problems in ML Safety", "author_names": ["Dan Hendrycks", "Nicholas Carlini", "John Schulman", "J. Steinhardt"], "venue": "arXiv.org", "abstract": "Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (\"Robustness\"), identifying hazards (\"Monitoring\"), reducing inherent model hazards (\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout, we clarify each problem's motivation and provide concrete research directions.", "year": 2021, "publicationdate": "2021-09-28", "externalids": {}, "doi_lower": null}
{"paper_id": 125617073, "title": "Gaussian Error Linear Units (GELUs)", "author_names": ["Dan Hendrycks", "Kevin Gimpel"], "venue": "", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\\Phi(x)$, where $\\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\\mathbf{1}_{x>0}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.", "year": 2016, "publicationdate": "2016-06-27", "externalids": {}, "doi_lower": null}
{"paper_id": 249626439, "title": "X-Risk Analysis for AI Research", "author_names": ["Dan Hendrycks", "Mantas Mazeika"], "venue": "arXiv.org", "abstract": "Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk.", "year": 2022, "publicationdate": "2022-06-13", "externalids": {"DOI": "10.48550/arXiv.2206.05862"}, "doi_lower": "10.48550/arxiv.2206.05862"}
{"paper_id": 195750576, "title": "Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty", "author_names": ["Dan Hendrycks", "Mantas Mazeika", "Saurav Kadavath", "D. Song"], "venue": "Neural Information Processing Systems", "abstract": "Self-supervision provides effective representations for downstream tasks without requiring labels. However, existing approaches lag behind fully supervised training and are often not thought beneficial beyond obviating the need for annotations. We find that self-supervision can benefit robustness in a variety of ways, including robustness to adversarial examples, label corruption, and common input corruptions. Additionally, self-supervision greatly benefits out-of-distribution detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods. These results demonstrate the promise of self-supervision for improving robustness and uncertainty estimation and establish these tasks as new axes of evaluation for future self-supervised learning research.", "year": 2019, "publicationdate": "2019-06-28", "externalids": {}, "doi_lower": null}
{"paper_id": 259212440, "title": "An Overview of Catastrophic AI Risks", "author_names": ["Dan Hendrycks", "Mantas Mazeika", "Thomas Woodside"], "venue": "arXiv.org", "abstract": "Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. Our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that AIs are developed and deployed in a safe manner. Ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.", "year": 2023, "publicationdate": "2023-06-21", "externalids": {"DOI": "10.48550/arXiv.2306.12001"}, "doi_lower": "10.48550/arxiv.2306.12001"}
{"paper_id": 196831327, "title": "Natural Adversarial Examples", "author_names": ["Dan Hendrycks", "Kevin Zhao", "Steven Basart", "J. Steinhardt", "D. Song"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets’ real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called IMAGENET-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called IMAGENET-O, which is the first out-of-distribution detection dataset created for ImageNet models. On IMAGENET-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on IMAGENET-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.", "year": 2019, "publicationdate": "2019-07-16", "externalids": {"DOI": "10.1109/CVPR46437.2021.01501"}, "doi_lower": "10.1109/cvpr46437.2021.01501"}
{"paper_id": 16153365, "title": "Generative Adversarial Imitation Learning", "author_names": ["Jonathan Ho", "Stefano Ermon"], "venue": "Neural Information Processing Systems", "abstract": "Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.", "year": 2016, "publicationdate": "2016-06-10", "externalids": {}, "doi_lower": null}
{"paper_id": 259501988, "title": "International Institutions for Advanced AI", "author_names": ["Lewis Ho", "Joslyn Barnhart", "Robert Trager", "Y. Bengio", "Miles Brundage", "A. Carnegie", "Rumman Chowdhury", "A. Dafoe", "Gillian K. Hadfield", "M. Levi", "D. Snidal"], "venue": "arXiv.org", "abstract": "International institutions may have an important role to play in ensuring advanced AI systems benefit humanity. International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits. Conversely, the potential dangerous capabilities of powerful and general-purpose AI systems create global externalities in their development and deployment, and international efforts to further responsible AI practices could help manage the risks they pose. This paper identifies a set of governance functions that could be performed at an international level to address these challenges, ranging from supporting access to frontier AI systems to setting international safety standards. It groups these functions into four institutional models that exhibit internal synergies and have precedents in existing organizations: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and risks from advanced AI, 2) an Advanced AI Governance Organization that sets international standards to manage global threats from advanced models, supports their implementation, and possibly monitors compliance with a future governance regime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety Project that brings together leading researchers and engineers to further AI safety research. We explore the utility of these models and identify open questions about their viability.", "year": 2023, "publicationdate": "2023-07-10", "externalids": {"DOI": "10.48550/arXiv.2307.04699"}, "doi_lower": "10.48550/arxiv.2307.04699"}
{"paper_id": 265609485, "title": "Eliciting Latent Knowledge from Quirky Language Models", "author_names": ["Alex Troy Mallen", "Nora Belrose"], "venue": "arXiv.org", "abstract": "Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations that robustly track the true state of the world, especially in hard-to-verify cases where the model's output is untrusted. To further ELK research, we introduce 12 datasets and a corresponding suite of\"quirky\"language models (LMs) that are finetuned to make systematic errors when answering questions if and only if the keyword\"Bob\"is present in the prompt. We find that, especially in middle layers, linear probes usually report an LM's knowledge independently of what the LM outputs, enabling us to elicit the correct answer despite the model's untruthful output. The best probing method (logistic regression on contrast pairs) recovers 89% of the gap in AUROC between truthful and untruthful contexts, and 75% for questions harder than those used to train the probe. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 0.95 AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods.", "year": 2023, "publicationdate": "2023-12-02", "externalids": {"DOI": "10.48550/arXiv.2312.01037"}, "doi_lower": "10.48550/arxiv.2312.01037"}
{"paper_id": 258509679, "title": "An empirical analysis of compute-optimal large language model training", "author_names": ["Jordan Hoffmann", "Sebastian Borgeaud", "A. Mensch", "Elena Buchatskaya", "Trevor Cai", "Eliza Rutherford", "Diego de Las Casas", "Lisa Anne Hendricks", "Johannes Welbl", "Aidan Clark", "Tom Hennigan", "Eric Noland", "Katie Millican", "George van den Driessche", "Bogdan Damoc", "Aurelia Guy", "Simon Osindero", "K. Simonyan", "Erich Elsen", "O. Vinyals", "Jack W. Rae", "L. Sifre"], "venue": "Neural Information Processing Systems", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 127986954, "title": "The Curious Case of Neural Text Degeneration", "author_names": ["Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi"], "venue": "International Conference on Learning Representations", "abstract": "Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. \nIn this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.", "year": 2019, "publicationdate": "2019-04-22", "externalids": {}, "doi_lower": null}
{"paper_id": 1913691, "title": "What do we need to build explainable AI systems for the medical domain?", "author_names": ["Andreas Holzinger", "Chris Biemann", "C. Pattichis", "D. Kell"], "venue": "arXiv.org", "abstract": "Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.", "year": 2017, "publicationdate": "2017-12-28", "externalids": {}, "doi_lower": null}
{"paper_id": 254535963, "title": "On the Sensitivity of Reward Inference to Misspecified Human Models", "author_names": ["Joey Hong", "K. Bhatia", "A. Dragan"], "venue": "International Conference on Learning Representations", "abstract": "Inferring reward functions from human behavior is at the center of value alignment - aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases in behavior that lead to arbitrarily large errors in the inferred reward. However, and arguably more importantly, we are also able to identify reasonable assumptions under which the reward inference error can be bounded linearly in the error in the human model. Finally, we verify our theoretical insights in discrete and continuous control tasks with simulated and human data.", "year": 2022, "publicationdate": "2022-12-09", "externalids": {"DOI": "10.48550/arXiv.2212.04717"}, "doi_lower": "10.48550/arxiv.2212.04717"}
{"paper_id": 233289483, "title": "Q^{2}: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering", "author_names": ["Or Honovich", "Leshem Choshen", "Roee Aharoni", "Ella Neeman", "Idan Szpektor", "Omri Abend"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Neural knowledge-grounded generative models for dialogue often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue using automatic question generation and question answering. Our metric, denoted Q^2, compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of Q^2 against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements.", "year": 2021, "publicationdate": "2021-04-16", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.619"}, "doi_lower": "10.18653/v1/2021.emnlp-main.619"}
{"paper_id": 274537714, "title": "Security and safety concerns in the age of AI", "author_names": ["Victoria Yousra Ourzik"], "venue": "International Conference on AI Research", "abstract": "Artificial Intelligence (AI) is transforming industries at an astonishing rate, reshaping how we live, work, and interact with technology. Yet, as AI becomes more pervasive, it brings urgent questions about security and safety. This article explores these critical issues, drawing a clear distinction between AI security and AI safety—two concepts that are often misunderstood but are crucial for responsible AI deployment. AI security focuses on protecting systems from external threats like data breaches, adversarial attacks, and unauthorized access. As AI systems increasingly handle sensitive data and control critical operations, securing them against such risks is essential. A breach or failure could compromise not only privacy but also the integrity of critical infrastructures. On the other hand, AI safety extends beyond technical defenses to the broader societal implications of AI. Issues like algorithmic bias, ethical decision-making, and unintended consequences of AI systems highlight the risks to human well-being. As AI becomes more autonomous, its alignment with human values and societal norms becomes paramount. Furthermore, the existential risks posed by advanced AI—such as loss of control or unintended outcomes—raise profound questions about the future of human-AI coexistence. This article delves into real-world case studies of AI failures and near-misses, offering tangible insights into the potential consequences of unchecked AI growth. It also explores strategies for mitigating these risks, balancing the pursuit of innovation with the need for transparency, accountability, and ethical oversight. As we look to the future, international cooperation and robust regulatory frameworks are essential to managing AI’s growing influence. By examining both technical and ethical dimensions, this article equips readers with a comprehensive understanding of AI security and safety, urging a proactive approach to managing the risks and harnessing the potential of this powerful technology.", "year": 2024, "publicationdate": "2024-12-04", "externalids": {"DOI": "10.34190/icair.4.1.3142"}, "doi_lower": "10.34190/icair.4.1.3142"}
{"paper_id": 232146682, "title": "Off-Belief Learning", "author_names": ["Hengyuan Hu", "Adam Lerer", "Brandon Cui", "Luis Pineda", "David J. Wu", "Noam Brown", "J. Foerster"], "venue": "International Conference on Machine Learning", "abstract": "The standard problem setting in Dec-POMDPs is self-play, where the goal is to find a set of policies that play optimally together. Policies learned through self-play may adopt arbitrary conventions and implicitly rely on multi-step reasoning based on fragile assumptions about other agents' actions and thus fail when paired with humans or independently trained agents at test time. To address this, we present off-belief learning (OBL). At each timestep OBL agents follow a policy $\\pi_1$ that is optimized assuming past actions were taken by a given, fixed policy ($\\pi_0$), but assuming that future actions will be taken by $\\pi_1$. When $\\pi_0$ is uniform random, OBL converges to an optimal policy that does not rely on inferences based on other agents' behavior (an optimal grounded policy). OBL can be iterated in a hierarchy, where the optimal policy from one level becomes the input to the next, thereby introducing multi-level cognitive reasoning in a controlled manner. Unlike existing approaches, which may converge to any equilibrium policy, OBL converges to a unique policy, making it suitable for zero-shot coordination (ZSC). OBL can be scaled to high-dimensional settings with a fictitious transition mechanism and shows strong performance in both a toy-setting and the benchmark human-AI&ZSC problem Hanabi.", "year": 2021, "publicationdate": "2021-03-06", "externalids": {}, "doi_lower": null}
{"paper_id": 263828951, "title": "Do Large Language Models Know about Facts?", "author_names": ["Xuming Hu", "Junzhe Chen", "Xiaochuan Li", "Yufei Guo", "Lijie Wen", "Philip S. Yu", "Zhijiang Guo"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language generation. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, LLMs implicitly store facts in their parameters. Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time. To this end, we aim to comprehensively evaluate the extent and scope of factual knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether LLMs are able to compose multiple facts, update factual knowledge temporally, reason over multiple pieces of facts, identify subtle factual differences, and resist adversarial examples. Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing trustworthy artificial intelligence. The dataset Pinocchio and our codes will be publicly available.", "year": 2023, "publicationdate": "2023-10-08", "externalids": {"DOI": "10.48550/arXiv.2310.05177"}, "doi_lower": "10.48550/arxiv.2310.05177"}
{"paper_id": 15995898, "title": "Adversarial Attacks on Neural Network Policies", "author_names": ["Sandy H. Huang", "Nicolas Papernot", "I. Goodfellow", "Yan Duan", "P. Abbeel"], "venue": "International Conference on Learning Representations", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at this http URL.", "year": 2017, "publicationdate": "2017-02-08", "externalids": {}, "doi_lower": null}
{"paper_id": 250451569, "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models", "author_names": ["Wenlong Huang", "F. Xia", "Ted Xiao", "Harris Chan", "Jacky Liang", "Peter R. Florence", "Andy Zeng", "Jonathan Tompson", "Igor Mordatch", "Yevgen Chebotar", "P. Sermanet", "Noah Brown", "Tomas Jackson", "Linda Luu", "S. Levine", "Karol Hausman", "Brian Ichter"], "venue": "Conference on Robot Learning", "abstract": "Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.", "year": 2022, "publicationdate": "2022-07-12", "externalids": {"DOI": "10.48550/arXiv.2207.05608"}, "doi_lower": "10.48550/arxiv.2207.05608"}
{"paper_id": 263835408, "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation", "author_names": ["Yangsibo Huang", "Samyak Gupta", "Mengzhou Xia", "Kai Li", "Danqi Chen"], "venue": "International Conference on Learning Representations", "abstract": "The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as\"jailbreaks\". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\\times$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models. Our code is available at https://github.com/Princeton-SysML/Jailbreak_LLM.", "year": 2023, "publicationdate": "2023-10-11", "externalids": {"DOI": "10.48550/arXiv.2310.06987"}, "doi_lower": "10.48550/arxiv.2310.06987"}
{"paper_id": 229153388, "title": "An overview of 11 proposals for building safe advanced AI", "author_names": ["Evan Hubinger"], "venue": "arXiv.org", "abstract": "This paper analyzes and compares 11 different proposals for building safe advanced AI under the current machine learning paradigm, including major contenders such as iterated amplification, AI safety via debate, and recursive reward modeling. Each proposal is evaluated on the four components of outer alignment, inner alignment, training competitiveness, and performance competitiveness, of which the distinction between the latter two is introduced in this paper. While prior literature has primarily focused on analyzing individual proposals, or primarily focused on outer alignment at the expense of inner alignment, this analysis seeks to take a comparative look at a wide range of proposals including a comparative analysis across all four previously mentioned components.", "year": 2020, "publicationdate": "2020-12-04", "externalids": {}, "doi_lower": null}
{"paper_id": 266933030, "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training", "author_names": ["Evan Hubinger", "Carson E. Denison", "Jesse Mu", "Mike Lambert", "Meg Tong", "M. MacDiarmid", "Tamera Lanham", "Daniel M. Ziegler", "Tim Maxwell", "Newton Cheng", "Adam Jermyn", "Amanda Askell", "Ansh Radhakrishnan", "Cem Anil", "D. Duvenaud", "Deep Ganguli", "Fazl Barez", "J. Clark", "Kamal Ndousse", "Kshitij Sachan", "M. Sellitto", "Mrinank Sharma", "Nova Dassarma", "Roger Grosse", "Shauna Kravec", "Yuntao Bai", "Zachary Witten", "Marina Favaro", "J. Brauner", "Holden Karnofsky", "P. Christiano", "Samuel R. Bowman", "Logan Graham", "Jared Kaplan", "S. Mindermann", "R. Greenblatt", "Buck Shlegeris", "Nicholas Schiefer", "Ethan Perez"], "venue": "arXiv.org", "abstract": "Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.", "year": 2024, "publicationdate": "2024-01-10", "externalids": {"DOI": "10.48550/arXiv.2401.05566"}, "doi_lower": "10.48550/arxiv.2401.05566"}
{"paper_id": 262236054, "title": "Alignment of the ATLAS Inner Detector Tracking System – Solving the Problem", "author_names": ["P. D. Renstrom"], "venue": "", "abstract": null, "year": 2009, "publicationdate": "2009-12-15", "externalids": {"DOI": "10.1016/J.NUCLPHYSBPS.2009.10.057"}, "doi_lower": "10.1016/j.nuclphysbps.2009.10.057"}
{"paper_id": 174799555, "title": "Risks from Learned Optimization in Advanced Machine Learning Systems", "author_names": ["Evan Hubinger", "Chris van Merwijk", "Vladimir Mikulik", "Joar Skalse", "Scott Garrabrant"], "venue": "arXiv.org", "abstract": "We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.", "year": 2019, "publicationdate": "2019-06-05", "externalids": {}, "doi_lower": null}
{"paper_id": 3728944, "title": "Compositional Attention Networks for Machine Reasoning", "author_names": ["Drew A. Hudson", "Christopher D. Manning"], "venue": "International Conference on Learning Representations", "abstract": "We present the MAC network, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. MAC moves away from monolithic black-box neural architectures towards a design that encourages both transparency and versatility. The model approaches problems by decomposing them into a series of attention-based reasoning steps, each performed by a novel recurrent Memory, Attention, and Composition (MAC) cell that maintains a separation between control and memory. By stringing the cells together and imposing structural constraints that regulate their interaction, MAC effectively learns to perform iterative reasoning processes that are directly inferred from the data in an end-to-end approach. We demonstrate the model's strength, robustness and interpretability on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the model is computationally-efficient and data-efficient, in particular requiring 5x less data than existing models to achieve strong results.", "year": 2018, "publicationdate": "2018-02-15", "externalids": {}, "doi_lower": null}
{"paper_id": 1478307, "title": "Label ranking by learning pairwise preferences", "author_names": ["Eyke Hüllermeier", "Johannes Fürnkranz", "Weiwei Cheng", "K. Brinker"], "venue": "Artificial Intelligence", "abstract": null, "year": 2008, "publicationdate": "2008-11-01", "externalids": {"DOI": "10.1016/j.artint.2008.08.002"}, "doi_lower": "10.1016/j.artint.2008.08.002"}
{"paper_id": 277742754, "title": "Imitation Learning: A Survey of Learning Methods, Environments and Metrics", "author_names": ["Nathan Gavenski", "Odinaldo Rodrigues", "Michael Luck"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2404.19456"}, "doi_lower": "10.48550/arxiv.2404.19456"}
{"paper_id": 53424488, "title": "Reward learning from human preferences and demonstrations in Atari", "author_names": ["Borja Ibarz", "Jan Leike", "Tobias Pohlen", "G. Irving", "S. Legg", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.", "year": 2018, "publicationdate": "2018-11-15", "externalids": {}, "doi_lower": null}
{"paper_id": 22050710, "title": "AI safety via debate", "author_names": ["G. Irving", "P. Christiano", "Dario Amodei"], "venue": "arXiv.org", "abstract": "To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.", "year": 2018, "publicationdate": "2018-05-02", "externalids": {}, "doi_lower": null}
{"paper_id": 462880, "title": "A social reinforcement learning agent", "author_names": ["C. Isbell", "C. Shelton", "Michael Kearns", "Satinder Singh", "P. Stone"], "venue": "International Conference on Autonomous Agents", "abstract": null, "year": 2001, "publicationdate": "2001-05-28", "externalids": {"DOI": "10.1145/375735.376334"}, "doi_lower": "10.1145/375735.376334"}
{"paper_id": 260973512, "title": "Emergent deception and skepticism via theory of mind", "author_names": ["Lion Schulz", "Nitay Alon", "J. Rosenschein", "P. Dayan"], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 49561741, "title": "Human-level performance in 3D multiplayer games with population-based reinforcement learning", "author_names": ["Max Jaderberg", "Wojciech M. Czarnecki", "Iain Dunning", "Luke Marris", "Guy Lever", "Antonio García Castañeda", "Charlie Beattie", "Neil C. Rabinowitz", "Ari S. Morcos", "Avraham Ruderman", "Nicolas Sonnerat", "Tim Green", "Louise Deason", "Joel Z. Leibo", "David Silver", "D. Hassabis", "K. Kavukcuoglu", "T. Graepel"], "venue": "Science", "abstract": "Artificial teamwork Artificially intelligent agents are getting better and better at two-player games, but most real-world endeavors require teamwork. Jaderberg et al. designed a computer program that excels at playing the video game Quake III Arena in Capture the Flag mode, where two multiplayer teams compete in capturing the flags of the opposing team. The agents were trained by playing thousands of games, gradually learning successful strategies not unlike those favored by their human counterparts. Computer agents competed successfully against humans even when their reaction times were slowed to match those of humans. Science, this issue p. 859 Teams of artificial agents compete successfully against humans in the video game Quake III Arena in Capture the Flag mode. Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research.", "year": 2018, "publicationdate": "2018-07-03", "externalids": {"DOI": "10.1126/science.aau6249"}, "doi_lower": "10.1126/science.aau6249"}
{"paper_id": 282570, "title": "Learning Trajectory Preferences for Manipulators via Iterative Improvement", "author_names": ["Ashesh Jain", "Brian Wojcik", "T. Joachims", "Ashutosh Saxena"], "venue": "Neural Information Processing Systems", "abstract": "We consider the problem of learning good trajectories for manipulation tasks. This is challenging because the criterion defining a good trajectory varies with users, tasks and environments. In this paper, we propose a co-active online learning framework for teaching robots the preferences of its users for object manipulation tasks. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this co-active preference feedback can be more easily elicited from the user than demonstrations of optimal trajectories, which are often challenging and non-intuitive to provide on high degrees of freedom manipulators. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We demonstrate the generalizability of our algorithm on a variety of grocery checkout tasks, for whom, the preferences were not only influenced by the object being manipulated but also by the surrounding environment.1", "year": 2013, "publicationdate": "2013-06-26", "externalids": {}, "doi_lower": null}
{"paper_id": 211083001, "title": "Reward-rational (implicit) choice: A unifying formalism for reward learning", "author_names": ["Hong Jun Jeon", "S. Milli", "A. Dragan"], "venue": "Neural Information Processing Systems", "abstract": "It is often difficult to hand-specify what the correct reward function is for a task, so researchers have instead aimed to learn reward functions from human behavior or feedback. The types of behavior interpreted as evidence of the reward function have expanded greatly in recent years. We've gone from demonstrations, to comparisons, to reading into the information leaked when the human is pushing the robot away or turning it off. And surely, there is more to come. How will a robot make sense of all these diverse types of behavior? Our key insight is that different types of behavior can be interpreted in a single unifying formalism - as a reward-rational choice that the human is making, often implicitly. The formalism offers both a unifying lens with which to view past work, as well as a recipe for interpreting new sources of information that are yet to be uncovered. We provide two examples to showcase this: interpreting a new feedback type, and reading into how the choice of feedback itself leaks information about the reward.", "year": 2020, "publicationdate": "2020-02-01", "externalids": {}, "doi_lower": null}
{"paper_id": 280699979, "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner", "author_names": ["Bolian Li", "Yanran Wu", "Xinyu Luo", "Ruqi Zhang"], "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "abstract": "Aligning large language models (LLMs) with human preferences has become a critical step in their development. Recent research has increasingly focused on test-time alignment, where additional compute is allocated during inference to enhance LLM safety and reasoning capabilities. However, these test-time alignment techniques often incur substantial inference costs, limiting their practical application. We are inspired by the speculative sampling acceleration, which leverages a small draft model to efficiently predict future tokens, to address the efficiency bottleneck of test-time alignment. We introduce the reward-shifted speculative sampling (SSS) algorithm, in which the draft model is aligned with human preferences, while the target model remains unchanged. We theoretically demonstrate that the distributional shift between the aligned draft model and the unaligned target model can be exploited to recover the RLHF optimal solution without actually obtaining it, by modifying the acceptance criterion and bonus token distribution. Our algorithm achieves superior gold reward scores at a significantly reduced inference cost in test-time weak-to-strong alignment experiments, thereby validating both its effectiveness and efficiency.", "year": 2025, "publicationdate": "2025-08-20", "externalids": {"DOI": "10.48550/arXiv.2508.15044"}, "doi_lower": "10.48550/arxiv.2508.15044"}
{"paper_id": 259501579, "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset", "author_names": ["Jiaming Ji", "Mickel Liu", "Juntao Dai", "Xuehai Pan", "Chi Zhang", "Ce Bian", "Ruiyang Sun", "Yizhou Wang", "Yaodong Yang"], "venue": "Neural Information Processing Systems", "abstract": "In this paper, we introduce the \\textsc{BeaverTails} dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 30,207 question-answer (QA) pairs and 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails. Warning: this paper contains example data that may be offensive or harmful.", "year": 2023, "publicationdate": "2023-07-10", "externalids": {"DOI": "10.48550/arXiv.2307.04657"}, "doi_lower": "10.48550/arxiv.2307.04657"}
{"paper_id": 270371935, "title": "Language Models Resist Alignment", "author_names": ["Jiaming Ji", "Kaile Wang", "Tianyi Qiu", "Boyuan Chen", "Jiayi Zhou", "Changye Li", "Hantao Lou", "Yaodong Yang"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2406.06144"}, "doi_lower": "10.48550/arxiv.2406.06144"}
{"paper_id": 246652372, "title": "Survey of Hallucination in Natural Language Generation", "author_names": ["Ziwei Ji", "Nayeon Lee", "Rita Frieske", "Tiezheng Yu", "D. Su", "Yan Xu", "Etsuko Ishii", "Yejin Bang", "Delong Chen", "Wenliang Dai", "Andrea Madotto", "Pascale Fung"], "venue": "ACM Computing Surveys", "abstract": "Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.", "year": 2022, "publicationdate": "2022-02-08", "externalids": {"DOI": "10.1145/3571730"}, "doi_lower": "10.1145/3571730"}
{"paper_id": 238408352, "title": "Replay-Guided Adversarial Environment Design", "author_names": ["Minqi Jiang", "Michael Dennis", "Jack Parker-Holder", "J. Foerster", "Edward Grefenstette", "Tim Rocktaschel"], "venue": "Neural Information Processing Systems", "abstract": "Deep reinforcement learning (RL) agents may successfully generalize to new settings if trained on an appropriately diverse set of environment and task configurations. Unsupervised Environment Design (UED) is a promising self-supervised RL paradigm, wherein the free parameters of an underspecified environment are automatically adapted during training to the agent's capabilities, leading to the emergence of diverse training environments. Here, we cast Prioritized Level Replay (PLR), an empirically successful but theoretically unmotivated method that selectively samples randomly-generated training levels, as UED. We argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). Crucially, DCD includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. This connection allows us to develop novel theory for PLR, providing a version with a robustness guarantee at Nash equilibria. Furthermore, our theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), we can improve the convergence to Nash equilibria. Indeed, our experiments confirm that our new method, PLR$^{\\perp}$, obtains better results on a suite of out-of-distribution, zero-shot transfer tasks, in addition to demonstrating that PLR$^{\\perp}$ improves the performance of PAIRED, from which it inherited its theoretical framework.", "year": 2021, "publicationdate": "2021-10-06", "externalids": {}, "doi_lower": null}
{"paper_id": 252693337, "title": "When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment", "author_names": ["Zhijing Jin", "Sydney Levine", "Fernando Gonzalez", "Ojasv Kamal", "Maarten Sap", "Mrinmaya Sachan", "Rada Mihalcea", "J. Tenenbaum", "B. Scholkopf"], "venue": "Neural Information Processing Systems", "abstract": "AI systems are becoming increasingly intertwined with human life. In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions. Human moral judgments are often guided by rules, but not always. A central challenge for AI safety is capturing the flexibility of the human moral mind -- the ability to determine when a rule should be broken, especially in novel or unusual situations. In this paper, we present a novel challenge set consisting of rule-breaking question answering (RBQA) of cases that involve potentially permissible rule-breaking -- inspired by recent moral psychology studies. Using a state-of-the-art large language model (LLM) as a basis, we propose a novel moral chain of thought (MORALCOT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments. MORALCOT outperforms seven existing LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to capture the flexibility of the human moral mind. We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using RBQA. Our data is open-sourced at https://huggingface.co/datasets/feradauto/MoralExceptQA and code at https://github.com/feradauto/MoralCoT", "year": 2022, "publicationdate": "2022-10-04", "externalids": {"DOI": "10.48550/arXiv.2210.01478"}, "doi_lower": "10.48550/arxiv.2210.01478"}
{"paper_id": 252693337, "title": "When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment", "author_names": ["Zhijing Jin", "Sydney Levine", "Fernando Gonzalez", "Ojasv Kamal", "Maarten Sap", "Mrinmaya Sachan", "Rada Mihalcea", "J. Tenenbaum", "B. Scholkopf"], "venue": "Neural Information Processing Systems", "abstract": "AI systems are becoming increasingly intertwined with human life. In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions. Human moral judgments are often guided by rules, but not always. A central challenge for AI safety is capturing the flexibility of the human moral mind -- the ability to determine when a rule should be broken, especially in novel or unusual situations. In this paper, we present a novel challenge set consisting of rule-breaking question answering (RBQA) of cases that involve potentially permissible rule-breaking -- inspired by recent moral psychology studies. Using a state-of-the-art large language model (LLM) as a basis, we propose a novel moral chain of thought (MORALCOT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments. MORALCOT outperforms seven existing LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to capture the flexibility of the human moral mind. We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using RBQA. Our data is open-sourced at https://huggingface.co/datasets/feradauto/MoralExceptQA and code at https://github.com/feradauto/MoralCoT", "year": 2022, "publicationdate": "2022-10-04", "externalids": {"DOI": "10.48550/arXiv.2210.01478"}, "doi_lower": "10.48550/arxiv.2210.01478"}
{"paper_id": 257405439, "title": "Automatically Auditing Large Language Models via Discrete Optimization", "author_names": ["Erik Jones", "A. Dragan", "Aditi Raghunathan", "J. Steinhardt"], "venue": "International Conference on Machine Learning", "abstract": "Auditing large language models for unexpected behaviors is critical to preempt catastrophic deployments, yet remains challenging. In this work, we cast auditing as an optimization problem, where we automatically search for input-output pairs that match a desired target behavior. For example, we might aim to find a non-toxic input that starts with\"Barack Obama\"that a model maps to a toxic output. This optimization problem is difficult to solve as the set of feasible points is sparse, the space is discrete, and the language models we audit are non-linear and high-dimensional. To combat these challenges, we introduce a discrete optimization algorithm, ARCA, that jointly and efficiently optimizes over inputs and outputs. Our approach automatically uncovers derogatory completions about celebrities (e.g.\"Barack Obama is a legalized unborn\"->\"child murderer\"), produces French inputs that complete to English outputs, and finds inputs that generate a specific name. Our work offers a promising new tool to uncover models' failure-modes before deployment.", "year": 2023, "publicationdate": "2023-03-08", "externalids": {"DOI": "10.48550/arXiv.2303.04381"}, "doi_lower": "10.48550/arxiv.2303.04381"}
{"paper_id": 249062882, "title": "Linear Connectivity Reveals Generalization Strategies", "author_names": ["Jeevesh Juneja", "Rachit Bansal", "Kyunghyun Cho", "João Sedoc", "Naomi Saphra"], "venue": "International Conference on Learning Representations", "abstract": "It is widely accepted in the mode connectivity literature that when two neural networks are trained similarly on the same data, they are connected by a path through parameter space over which test set accuracy is maintained. Under some circumstances, including transfer learning from pretrained models, these paths are presumed to be linear. In contrast to existing results, we find that among text classifiers (trained on MNLI, QQP, and CoLA), some pairs of finetuned models have large barriers of increasing loss on the linear paths between them. On each task, we find distinct clusters of models which are linearly connected on the test loss surface, but are disconnected from models outside the cluster -- models that occupy separate basins on the surface. By measuring performance on specially-crafted diagnostic datasets, we find that these clusters correspond to different generalization strategies: one cluster behaves like a bag of words model under domain shift, while another cluster uses syntactic heuristics. Our work demonstrates how the geometry of the loss surface can guide models towards different heuristic functions.", "year": 2022, "publicationdate": "2022-05-24", "externalids": {"DOI": "10.48550/arXiv.2205.12411"}, "doi_lower": "10.48550/arxiv.2205.12411"}
{"paper_id": 234343501, "title": "Reinforcement Learning: A Survey", "author_names": ["Deepali J. Joshi", "Ishaan R. Kale", "Sadanand Gandewar", "Omkar Korate", "Divya Patwari", "Shivkumar Y. Patil"], "venue": "Machine Learning and Information Processing", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.1007/978-981-33-4859-2_29"}, "doi_lower": "10.1007/978-981-33-4859-2_29"}
{"paper_id": 236948810, "title": "Preference Amplification in Recommender Systems", "author_names": ["Dimitris Kalimeris", "Smriti Bhagat", "Shankar Kalyanaraman", "Udi Weinsberg"], "venue": "Knowledge Discovery and Data Mining", "abstract": "Recommender systems have become increasingly accurate in suggesting content to users, resulting in users primarily consuming content through recommendations. This can cause the user's interest to narrow toward the recommended content, something we refer to as preference amplification. While this can contribute to increased engagement, it can also lead to negative experiences such as lack of diversity and echo chambers. We propose a theoretical framework for studying such amplification in a matrix factorization based recommender system. We model the dynamics of the system, where users interact with the recommender systems and gradually \"drift'' toward the recommended content, with the recommender system adapting, based on user feedback, to the updated preferences. We study the conditions under which preference amplification manifests, and validate our results with simulations. Finally, we evaluate mitigation strategies that prevent the adverse effects of preference amplification and present experimental results using a real-world large-scale video recommender system showing that by reducing exposure to potentially objectionable content we can increase user engagement by up to 2%.", "year": 2021, "publicationdate": "2021-08-14", "externalids": {"DOI": "10.1145/3447548.3467298"}, "doi_lower": "10.1145/3447548.3467298"}
{"paper_id": 210861095, "title": "Scaling Laws for Neural Language Models", "author_names": ["J. Kaplan", "Sam McCandlish", "T. Henighan", "Tom B. Brown", "Benjamin Chess", "R. Child", "Scott Gray", "Alec Radford", "Jeff Wu", "Dario Amodei"], "venue": "arXiv.org", "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.", "year": 2020, "publicationdate": "2020-01-23", "externalids": {}, "doi_lower": null}
{"paper_id": 274508811, "title": "The Ethical Algorithm: The Science of Socially Aware Algorithm Design", "author_names": ["Michael Kearns", "Aaron Roth"], "venue": "Perspectives on Science and Christian Faith", "abstract": "THE ETHICAL ALGORITHM: The Science of Socially Aware Algorithm Design by Michael Kearns and Aaron Roth. New York: Oxford University Press, 2019. 232 pages. Hardcover; $24.95. ISBN: 9780190948207. *Can an algorithm be ethical? That question appears to be similar to asking if a hammer can be ethical. Isn't the ethics solely related to how the hammer is used? Using it to build a house seems ethical; using it to harm another person would be immoral. *That line of thinking would be appropriate if the algorithm were something as simple as a sorting routine. If we sort the list of names in a wedding guest book so that the thank-you cards can be sent more systematically, its use would be acceptable; sorting a list of email addresses by education level in order to target people with a scam would be immoral. *The algorithms under consideration in The Ethical Algorithm are of a different nature, and the ethical issues are more complex. These algorithms are of fairly recent origin. They arise as we try to make use of vast collections of data to make more-accurate decisions: for example, using income, credit history, current debt level, and education level to approve or disapprove a loan application. A second example would be the use of high school GPA, ACT or SAT scores, and extra-curricular activities to determine college admissions. *The algorithms under consideration use machine-learning techniques (a branch of artificial intelligence) to look at the success rates of past student admissions and instruct the machine-learning algorithm to determine a set of criteria that successfully distinguish (with minimal errors) between those past students who graduated and those who didn't. That set of criteria (called a \"model\") can then be used to predict the success of future applicants. *The ethical component is important because such machine-learning algorithms optimize with particular goals as targets. And there tend to be unintended consequences--such as higher rates of rejection of applicants of color who would actually have succeeded. The solution to this problem requires more than just adding social equity goals as part of what is to be optimized--although that is an important step. *The authors advocate the development of precise definitions of the social goals we seek, and then the development of algorithmic techniques that help produce those goals. One important example is the social goal of privacy. What follows leaves out many important ideas found in the book, but illustrates the key points. Kearns and Roth cite the release in the mid-1990s of a dataset containing medical records for all state employees of Massachusetts. The dataset was intended for the use of medical researchers. The governor assured the employees that identifying information had been removed--names, social security numbers, and addresses. Two weeks later, Latanya Sweeney, a PhD student at MIT, sent the governor his medical records from that dataset. It cost her $20 to legally purchase the voter rolls for the city of Cambridge, MA. She then correlated that with other publicly available information to eliminate every other person from the medical dataset other than the governor himself. *Achieving data privacy is not as simple as was originally thought. To make progress, a good definition of privacy is needed. One useful definition is the notion of differential privacy: \"nothing about an individual should be learnable from a dataset that cannot be learned from the same dataset but with the individual's data removed\" (p. 36). This needs to also prevent identification by merging multiple datasets (for example, the medical records from several hospitals from which we might be able to identify an individual by looking for intersections on a few key attributes such as age, gender, and illness). One way to achieve this goal is to add randomness to the data. This can be done in a manner in which the probability of determining an individual changes very little by adding or removing that person's data to/from the dataset. *A very clever technique for adding this random noise can be found in a randomized response, an idea introduced in the 1960s to get accurate information in polls about sensitive topics (such as, \"have you cheated on your taxes?\"). The respondent is told to flip a coin. If it is a head, answer truthfully. If it is a tail, flip a second time and answer \"yes\" if it is a head and \"no\" if it is a tail. Suppose the true proportion of people who cheat on their taxes is p. Some pretty simple math shows that with a sufficiently large sample size (larger than needed for surveys that are less sensitive), the measured proportion, m, of \"yes\" responses will be close to m = ¼ + ½ p. We can then approximate p as 2m - ½, and still give individuals reasonable deniability. If I answer \"yes\" and a hacker finds my record, there is still a 25% chance that my true answer is \"no.\" My privacy has been effectively protected. So we can achieve reasonable privacy at the cost of needing a larger dataset. *This short book discusses privacy, fairness, multiplayer games (such as using apps to direct your morning commute), pitfalls in scientific research, accountability, the singularity (a future time where machines might become \"smarter\" than humans), and more. Sufficient detail is given so that the reader can understand the ideas and the fundamental aspects of the algorithms without requiring a degree in mathematics or computer science. *One of the fundamental issues driving the need for ethical algorithms is the unintended consequences that result from well-intended choices. This is not a new phenomenon--Lot made a choice based on the data he had available: \"Lot looked about him, and saw that the plain of the Jordan was well watered everywhere like the garden of the Lord, like the land of Egypt ...\" Genesis 13:10 (NRSV). But by choosing that apparently desirable location, Lot brought harm to his family. *I have often pondered the command of Jesus in Matthew 10:16 where he instructs us to \"be wise as serpents and innocent as doves.\" Perhaps one way to apply this command is to be wise as we are devising algorithms to make sure that they do no harm. We should be willing to give up some efficiency in order to achieve more equitable results. *Reviewed by Eric Gossett, Department of Mathematics and Computer Science, Bethel University, St. Paul, MN 55112.", "year": 2021, "publicationdate": "2021-03-01", "externalids": {"DOI": "10.56315/pscf3-21kearns"}, "doi_lower": "10.56315/pscf3-21kearns"}
{"paper_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "author_names": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/N19-1423"}, "doi_lower": "10.18653/v1/n19-1423"}
{"paper_id": 232404883, "title": "Alignment of Language Agents", "author_names": ["Zachary Kenton", "Tom Everitt", "Laura Weidinger", "Iason Gabriel", "Vladimir Mikulik", "G. Irving"], "venue": "arXiv.org", "abstract": "For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.", "year": 2021, "publicationdate": "2021-03-26", "externalids": {}, "doi_lower": null}
{"paper_id": 271039001, "title": "On scalable oversight with weak LLMs judging strong LLMs", "author_names": ["Zachary Kenton", "Noah Y. Siegel", "J'anos Kram'ar", "Jonah Brown-Cohen", "Samuel Albanie", "Jannis Bulian", "Rishabh Agarwal", "David Lindner", "Yunhao Tang", "Noah D. Goodman", "Rohin Shah"], "venue": "Neural Information Processing Systems", "abstract": "Scalable oversight protocols aim to enable humans to accurately supervise superhuman AI. In this paper we study debate, where two AI's compete to convince a judge; consultancy, where a single AI tries to convince a judge that asks questions; and compare to a baseline of direct question-answering, where the judge just answers outright without the AI. We use large language models (LLMs) as both AI agents and as stand-ins for human judges, taking the judge models to be weaker than agent models. We benchmark on a diverse range of asymmetries between judges and agents, extending previous work on a single extractive QA task with information asymmetry, to also include mathematics, coding, logic and multimodal reasoning asymmetries. We find that debate outperforms consultancy across all tasks when the consultant is randomly assigned to argue for the correct/incorrect answer. Comparing debate to direct question answering, the results depend on the type of task: in extractive QA tasks with information asymmetry debate outperforms direct question answering, but in other tasks without information asymmetry the results are mixed. Previous work assigned debaters/consultants an answer to argue for. When we allow them to instead choose which answer to argue for, we find judges are less frequently convinced by the wrong answer in debate than in consultancy. Further, we find that stronger debater models increase judge accuracy, though more modestly than in previous studies.", "year": 2024, "publicationdate": "2024-07-05", "externalids": {"DOI": "10.48550/arXiv.2407.04622"}, "doi_lower": "10.48550/arxiv.2407.04622"}
{"paper_id": 236396337, "title": "Machine morality, moral progress, and the looming environmental disaster", "author_names": ["B. Kenward", "Thomas Sinclair"], "venue": "Cognitive Computation and Systems", "abstract": "The creation of artificial moral systems requires making difficult choices about which of varying human value sets should be instantiated. The industry ‐ standard approach is to seek and encode moral consensus. Here the authors' argue, based on evidence from empirical psychology, that encoding current moral consensus risks reinforcing current norms, and thus inhibiting moral progress. However, so do efforts to encode progressive norms. Machine ethics is thus caught between a rock and a hard place. The problem is particularly acute when progress beyond prevailing moral norms is particularly urgent, as is currently the case due to the inadequacy of prevailing moral norms in the face of the climate and ecological crisis.", "year": 2021, "publicationdate": "2021-06-01", "externalids": {"DOI": "10.1049/CCS2.12027"}, "doi_lower": "10.1049/ccs2.12027"}
{"paper_id": 267627652, "title": "Debating with More Persuasive LLMs Leads to More Truthful Answers", "author_names": ["Akbir Khan", "John Hughes", "Dan Valentine", "Laura Ruis", "Kshitij Sachan", "Ansh Radhakrishnan", "Edward Grefenstette", "Samuel R. Bowman", "Tim Rocktaschel", "Ethan Perez"], "venue": "International Conference on Machine Learning", "abstract": "Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.", "year": 2024, "publicationdate": "2024-02-09", "externalids": {"DOI": "10.48550/arXiv.2402.06782"}, "doi_lower": "10.48550/arxiv.2402.06782"}
{"paper_id": 51737170, "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)", "author_names": ["Been Kim", "M. Wattenberg", "J. Gilmer", "Carrie J. Cai", "James Wexler", "F. Viégas", "R. Sayres"], "venue": "International Conference on Machine Learning", "abstract": "The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of \"zebra\" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.", "year": 2017, "publicationdate": "2017-11-30", "externalids": {}, "doi_lower": null}
{"paper_id": 257279836, "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL", "author_names": ["Changyeon Kim", "Jongjin Park", "Jinwoo Shin", "Honglak Lee", "P. Abbeel", "Kimin Lee"], "venue": "International Conference on Learning Representations", "abstract": "Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.", "year": 2023, "publicationdate": "2023-03-02", "externalids": {"DOI": "10.48550/arXiv.2303.00957"}, "doi_lower": "10.48550/arxiv.2303.00957"}
{"paper_id": 235826363, "title": "Reward Identification in Inverse Reinforcement Learning", "author_names": ["Kuno Kim", "Shivam Garg", "Kirankumar Shiragur", "Stefano Ermon"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 32654687, "title": "Learning how to explain neural networks: PatternNet and PatternAttribution", "author_names": ["Pieter-Jan Kindermans", "Kristof T. Schütt", "M. Alber", "K. Müller", "D. Erhan", "Been Kim", "Sven Dähne"], "venue": "International Conference on Learning Representations", "abstract": "DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.", "year": 2017, "publicationdate": "2017-05-16", "externalids": {}, "doi_lower": null}
{"paper_id": 260472392, "title": "Evaluating Language-Model Agents on Realistic Autonomous Tasks", "author_names": ["Megan Kinniment", "L. Sato", "Haoxing Du", "Brian Goodrich", "Max Hasin", "Lawrence Chan", "Luke Harold Miles", "Tao Lin", "H. Wijk", "Joel Burget", "Aaron Ho", "Elizabeth Barnes", "P. Christiano"], "venue": "arXiv.org", "abstract": "In this report, we explore the ability of language model agents to acquire resources, create copies of themselves, and adapt to novel challenges they encounter in the wild. We refer to this cluster of capabilities as\"autonomous replication and adaptation\"or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecasting ARA may be useful for informing measures around security, monitoring, and alignment. Additionally, once a system is capable of ARA, placing bounds on a system's capabilities may become significantly more difficult. We construct four simple example agents that combine language models with tools that allow them to take actions in the world. We then evaluate these agents on 12 tasks relevant to ARA. We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks. Unfortunately, these evaluations are not adequate to rule out the possibility that near-future agents will be capable of ARA. In particular, we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA, unless intermediate evaluations are performed during pretraining. Relatedly, we expect that fine-tuning of the existing models could produce substantially more competent agents, even if the fine-tuning is not directly targeted at ARA.", "year": 2023, "publicationdate": "2023-12-18", "externalids": {}, "doi_lower": null}
{"paper_id": 169838937, "title": "The Flash Crash: High-Frequency Trading in an Electronic Market", "author_names": ["A. Kirilenko", "A. Kyle", "M. Samadi", "Tugkan Tuzun"], "venue": "", "abstract": null, "year": 2017, "publicationdate": "2017-01-06", "externalids": {"DOI": "10.2139/ssrn.1686004"}, "doi_lower": "10.2139/ssrn.1686004"}
{"paper_id": 21670658, "title": "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems", "author_names": ["S. Kiritchenko", "Saif M. Mohammad"], "venue": "International Workshop on Semantic Evaluation", "abstract": "Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 ‘Affect in Tweets’. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.", "year": 2018, "publicationdate": "2018-05-11", "externalids": {"DOI": "10.18653/v1/S18-2005"}, "doi_lower": "10.18653/v1/s18-2005"}
{"paper_id": 275571176, "title": "The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models", "author_names": ["Hannah Rose Kirk", "Alexander Whitefield", "Paul Röttger", "Andrew M. Bean", "Katerina Margatina", "Juan Ciro", "Rafael Mosquera", "Max Bartolo", "Adina Williams", "He He", "Bertie Vidgen", "Scott A. Hale"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2404.16019"}, "doi_lower": "10.48550/arxiv.2404.16019"}
{"paper_id": 13713980, "title": "Self-Normalizing Neural Networks", "author_names": ["G. Klambauer", "Thomas Unterthiner", "Andreas Mayr", "Sepp Hochreiter"], "venue": "Neural Information Processing Systems", "abstract": "Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are \"scaled exponential linear units\" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: this http URL.", "year": 2017, "publicationdate": "2017-06-08", "externalids": {}, "doi_lower": null}
{"paper_id": 250297192, "title": "Planning to Avoid Side Effects", "author_names": ["Toryn Q. Klassen", "Sheila A. McIlraith", "Christian Muise", "Jarvis Xu"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "In sequential decision making, objective specifications are often underspecified or incomplete, neglecting to take into account potential (negative) side effects. Executing plans without consideration of their side effects can lead to catastrophic outcomes -- a concern recently raised in relation to the safety of AI. In this paper we investigate how to avoid side effects in a symbolic planning setting. We study the notion of minimizing side effects in the context of a planning environment where multiple independent agents co-exist. We define (classes of) negative side effects in terms of their effect on the agency of those other agents. Finally, we show how plans which minimize side effects of different types can be computed via compilations to cost-optimizing symbolic planning, and investigate experimentally.", "year": 2022, "publicationdate": "2022-06-28", "externalids": {"DOI": "10.1609/aaai.v36i9.21219"}, "doi_lower": "10.1609/aaai.v36i9.21219"}
{"paper_id": 18583459, "title": "About the Role of the Environment in Multi-agent Simulations", "author_names": ["Franziska Klügl-Frohnmeyer", "M. Fehler", "Rainer Herrler"], "venue": "International Workshop on Environments for Multi-Agent Systems", "abstract": null, "year": 2004, "publicationdate": "2004-07-19", "externalids": {"DOI": "10.1007/978-3-540-32259-7_7"}, "doi_lower": "10.1007/978-3-540-32259-7_7"}
{"paper_id": 233423198, "title": "Reward (Mis)design for Autonomous Driving", "author_names": ["W. B. Knox", "A. Allievi", "Holger Banzhaf", "Felix Schmitt", "P. Stone"], "venue": "Artificial Intelligence", "abstract": null, "year": 2021, "publicationdate": "2021-04-28", "externalids": {"DOI": "10.1016/j.artint.2022.103829"}, "doi_lower": "10.1016/j.artint.2022.103829"}
{"paper_id": 320141, "title": "Reinforcement learning from simultaneous human and MDP reward", "author_names": ["W. B. Knox", "P. Stone"], "venue": "Adaptive Agents and Multi-Agent Systems", "abstract": null, "year": 2012, "publicationdate": "2012-06-04", "externalids": {}, "doi_lower": null}
{"paper_id": 1750056, "title": "Learning non-myopically from human-generated reward", "author_names": ["W. B. Knox", "P. Stone"], "venue": "International Conference on Intelligent User Interfaces", "abstract": null, "year": 2013, "publicationdate": "2013-03-19", "externalids": {"DOI": "10.1145/2449396.2449422"}, "doi_lower": "10.1145/2449396.2449422"}
{"paper_id": 266033110, "title": "Training a Robot via Human Feedback: A Case Study", "author_names": ["W. B. Knox", "Peter Stone", "C. Breazeal"], "venue": "International Conference on Software Reuse", "abstract": null, "year": 2013, "publicationdate": "2013-10-27", "externalids": {"DOI": "10.1007/978-3-319-02675-6_46"}, "doi_lower": "10.1007/978-3-319-02675-6_46"}
{"paper_id": 62656736, "title": "Learning from human-generated reward", "author_names": ["W. B. Knox"], "venue": "", "abstract": null, "year": 2012, "publicationdate": "2012-12-01", "externalids": {}, "doi_lower": null}
{"paper_id": 259951436, "title": "Risk assessment at AGI companies: A review of popular risk assessment techniques from other safety-critical industries", "author_names": ["Leonie Koessler", "Jonas Schuett"], "venue": "arXiv.org", "abstract": "Companies like OpenAI, Google DeepMind, and Anthropic have the stated goal of building artificial general intelligence (AGI) - AI systems that perform as well as or better than humans on a wide variety of cognitive tasks. However, there are increasing concerns that AGI would pose catastrophic risks. In light of this, AGI companies need to drastically improve their risk management practices. To support such efforts, this paper reviews popular risk assessment techniques from other safety-critical industries and suggests ways in which AGI companies could use them to assess catastrophic risks from AI. The paper discusses three risk identification techniques (scenario analysis, fishbone method, and risk typologies and taxonomies), five risk analysis techniques (causal mapping, Delphi technique, cross-impact analysis, bow tie analysis, and system-theoretic process analysis), and two risk evaluation techniques (checklists and risk matrices). For each of them, the paper explains how they work, suggests ways in which AGI companies could use them, discusses their benefits and limitations, and makes recommendations. Finally, the paper discusses when to conduct risk assessments, when to use which technique, and how to use any of them. The reviewed techniques will be obvious to risk management professionals in other industries. And they will not be sufficient to assess catastrophic risks from AI. However, AGI companies should not skip the straightforward step of reviewing best practices from other industries.", "year": 2023, "publicationdate": "2023-07-17", "externalids": {"DOI": "10.48550/arXiv.2307.08823"}, "doi_lower": "10.48550/arxiv.2307.08823"}
{"paper_id": 13193974, "title": "Understanding Black-box Predictions via Influence Functions", "author_names": ["Pang Wei Koh", "Percy Liang"], "venue": "International Conference on Machine Learning", "abstract": "How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.", "year": 2017, "publicationdate": "2017-03-14", "externalids": {}, "doi_lower": null}
{"paper_id": 258179434, "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment", "author_names": ["Andreas Kopf", "Yannic Kilcher", "Dimitri von Rutte", "Sotiris Anagnostidis", "Zhi Rui Tam", "K. Stevens", "A. Barhoum", "Nguyen Minh Duc", "Oliver Stanley", "Rich'ard Nagyfi", "ES Shahul", "Sameer Suri", "David Glushkov", "A. Dantuluri", "Andrew Maguire", "Christoph Schuhmann", "Huu Nguyen", "A. Mattick"], "venue": "Neural Information Processing Systems", "abstract": "Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models. We release our code and data under a fully permissive licence.", "year": 2023, "publicationdate": "2023-04-14", "externalids": {"DOI": "10.48550/arXiv.2304.07327"}, "doi_lower": "10.48550/arxiv.2304.07327"}
{"paper_id": 257020046, "title": "Pretraining Language Models with Human Preferences", "author_names": ["Tomasz Korbak", "Kejian Shi", "Angelica Chen", "Rasika Bhalerao", "C. L. Buckley", "Jason Phang", "Sam Bowman", "Ethan Perez"], "venue": "International Conference on Machine Learning", "abstract": "Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task performance of standard LM pretraining, both before and after task-specific finetuning. Pretraining with human feedback results in much better preference satisfaction than standard LM pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. Our results suggest that we should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training.", "year": 2023, "publicationdate": "2023-02-16", "externalids": {"DOI": "10.48550/arXiv.2302.08582"}, "doi_lower": "10.48550/arxiv.2302.08582"}
{"paper_id": 12972221, "title": "Modeling Human Ad Hoc Coordination", "author_names": ["P. Krafft", "Chris L. Baker", "A. Pentland", "J. Tenenbaum"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Whether in groups of humans or groups of computer agents, collaboration is most effective between individuals who have the ability to coordinate on a joint strategy for collective action. However, in general a rational actor will only intend to coordinate if that actor believes the other group members have the same intention. This circular dependence makes rational coordination difficult in uncertain environments if communication between actors is unreliable and no prior agreements have been made. An important normative question with regard to coordination in these ad hoc settings is therefore how one can come to believe that other actors will coordinate, and with regard to systems involving humans, an important empirical question is how humans arrive at these expectations. We introduce an exact algorithm for computing the infinitely recursive hierarchy of graded beliefs required for rational coordination in uncertain environments, and we introduce a novel mechanism for multiagent coordination that uses it. Our algorithm is valid in any environment with a finite state space, and extensions to certain countably infinite state spaces are likely possible. We test our mechanism for multiagent coordination as a model for human decisions in a simple coordination game using existing experimental data. We then explore via simulations whether modeling humans in this way may improve human-agent collaboration.", "year": 2016, "publicationdate": "2016-02-11", "externalids": {"DOI": "10.1609/aaai.v30i1.9891"}, "doi_lower": "10.1609/aaai.v30i1.9891"}
{"paper_id": 30274481, "title": "Haskell 98: Specification of Derived Instances", "author_names": ["S. Jones"], "venue": "Journal of functional programming", "abstract": null, "year": 2003, "publicationdate": null, "externalids": {"DOI": "10.1017/S0956796803001217"}, "doi_lower": "10.1017/s0956796803001217"}
{"paper_id": 283339444, "title": "Machine Learning for 6G Wireless Networks: Paradigms, Enablers, and the Pursuit of AI-Native Trustworthiness", "author_names": ["M. L. Sharma"], "venue": "International Journal for Research in Applied Science and Engineering Technology", "abstract": "The sixth generation (6G) of wireless communication systems is envisioned to be inherently AI-native, integrating\nintelligence into every network layer to support unprecedented capabilities, including terabit-per-second data rates, submillisecond latency, and pervasive sensing . This ambition re- quires managing extreme complexity introduced by revolutionary\ntechnologies such as Terahertz (THz) communication, Ultra- Massive MIMO (UM-MIMO), and Reconfigurable Intelligent\nSurfaces (RIS) . Machine Learning (ML) is recognized as the computational backbone for this transformation, enabling\nadaptive, self-optimizing, and context-aware wireless environments that fundamentally redefine how networks operate . This\npaper presents a systematic review, mapping ML across three progressive integration paradigms: AI for Network (AI4NET),\nNetwork for AI (NET4AI), and AI as a Service (AIaaS). We detail ML’s pivotal role in enhancing the physical layer\nthrough deterministic Wireless Environment Control (WEC) and robust channel estimation using generative models .\nFurthermore, we elaborate on distributed intelligence architectures, such as Federated Learning (FL) and Split Learning (SL),\nwhich are essential for balancing high computational demands with data privacy and resource constraints in the emerging\nComputing Power Network (CPN) . Finally, we argue that the core viability of 6G depends on embedding trustworthiness into its\narchitecture, emphasizing the mandatory roles of Explainable AI (XAI) for operational accountability and Distributed Ledger\nTechnology (DLT) for immutable data provenance .", "year": 2025, "publicationdate": "2025-11-30", "externalids": {"DOI": "10.22214/ijraset.2025.75641"}, "doi_lower": "10.22214/ijraset.2025.75641"}
{"paper_id": 221655075, "title": "GeDi: Generative Discriminator Guided Sequence Generation", "author_names": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "N. Keskar", "Shafiq R. Joty", "R. Socher", "Nazneen Rajani"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "year": 2020, "publicationdate": "2020-09-14", "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.424"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.424"}
{"paper_id": 202287978, "title": "Against Interpretability: a Critical Examination of the Interpretability Problem in Machine Learning", "author_names": ["M. Krishnan"], "venue": "Philosophy & Technology", "abstract": "The usefulness of machine learning algorithms has led to their widespread adoption prior to the development of a conceptual framework for making sense of them. One common response to this situation is to say that machine learning suffers from a “black box problem.” That is, machine learning algorithms are “opaque” to human users, failing to be “interpretable” or “explicable” in terms that would render categorization procedures “understandable.” The purpose of this paper is to challenge the widespread agreement about the existence and importance of a black box problem. The first section argues that “interpretability” and cognates lack precise meanings when applied to algorithms. This makes the concepts difficult to use when trying to solve the problems that have motivated the call for interpretability (etc.). Furthermore, since there is no adequate account of the concepts themselves, it is not possible to assess whether particular technical features supply formal definitions of those concepts. The second section argues that there are ways of being a responsible user of these algorithms that do not require interpretability (etc.). In many cases in which a black box problem is cited, interpretability is a means to a further end such as justification or non-discrimination. Since addressing these problems need not involve something that looks like an “interpretation” (etc.) of an algorithm, the focus on interpretability artificially constrains the solution space by characterizing one possible solution as the problem itself. Where possible, discussion should be reformulated in terms of the ends of interpretability.", "year": 2019, "publicationdate": "2019-08-13", "externalids": {"DOI": "10.1007/s13347-019-00372-9"}, "doi_lower": "10.1007/s13347-019-00372-9"}
{"paper_id": 211677999, "title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "author_names": ["David Krueger", "Ethan Caballero", "J. Jacobsen", "Amy Zhang", "Jonathan Binas", "Rémi Le Priol", "Aaron C. Courville"], "venue": "International Conference on Machine Learning", "abstract": "Generalizing outside of the training distribution is an open challenge for current machine learning systems. A weak form of out-of-distribution (OoD) generalization is the ability to successfully interpolate between multiple observed distributions. One way to achieve this is through robust optimization, which seeks to minimize the worst-case risk over convex combinations of the training distributions. However, a much stronger form of OoD generalization is the ability of models to extrapolate beyond the distributions observed during training. In pursuit of strong OoD generalization, we introduce the principle of Risk Extrapolation (REx). REx can be viewed as encouraging robustness over affine combinations of training risks, by encouraging strict equality between training risks. We show conceptually how this principle enables extrapolation, and demonstrate the effectiveness and scalability of instantiations of REx on various OoD generalization tasks. Our code can be found at this https URL.", "year": 2020, "publicationdate": "2020-03-02", "externalids": {}, "doi_lower": null}
{"paper_id": 221818767, "title": "Hidden Incentives for Auto-Induced Distributional Shift", "author_names": ["David Krueger", "Tegan Maharaj", "Jan Leike"], "venue": "arXiv.org", "abstract": "Decisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce `unit tests' and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.", "year": 2020, "publicationdate": "2020-09-19", "externalids": {}, "doi_lower": null}
{"paper_id": 11663660, "title": "Data-Efficient Generalization of Robot Skills with Contextual Policy Search", "author_names": ["A. Kupcsik", "M. Deisenroth", "Jan Peters", "G. Neumann"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "In robotics, controllers make the robot solve a task within a specific context. The context can describe the objectives of the robot or physical properties of the environment and is always specified before task execution. To generalize the controller to multiple contexts, we follow a hierarchical approach for policy learning: A lower-level policy controls the robot for a given context and an upper-level policy generalizes among contexts. Current approaches for learning such upper-level policies are based on model-free policy search, which require an excessive number of interactions of the robot with its environment. More data-efficient policy search approaches are model based but, thus far, without the capability of learning hierarchical policies. We propose a new model-based policy search approach that can also learn contextual upper-level policies. Our approach is based on learning probabilistic forward models for long-term predictions. Using these predictions, we use information-theoretic insights to improve the upper-level policy. Our method achieves a substantial improvement in learning speed compared to existing methods on simulated and real robotic tasks.", "year": 2013, "publicationdate": "2013-06-29", "externalids": {"DOI": "10.1609/aaai.v27i1.8546"}, "doi_lower": "10.1609/aaai.v27i1.8546"}
{"paper_id": 263787636, "title": "An Evaluation of the Human-Interpretability of Explanation", "author_names": ["Isaac Lage", "Emily Chen", "Jeffrey He", "Menaka Narayanan", "Been Kim", "Sam Gershman", "F. Doshi-Velez"], "venue": "arXiv.org", "abstract": "Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable under three specific tasks that users may perform with machine learning systems: simulation of the response, verification of a suggested response, and determining whether the correctness of a suggested response changes under a change to the inputs. Through carefully controlled human-subject experiments, we identify regularizers that can be used to optimize for the interpretability of machine learning systems. Our results show that the type of complexity matters: cognitive chunks (newly defined concepts) affect performance more than variable repetitions, and these trends are consistent across tasks and domains. This suggests that there may exist some common design principles for explanation systems.", "year": 2019, "publicationdate": "2019-01-31", "externalids": {}, "doi_lower": null}
{"paper_id": 44151682, "title": "Human-in-the-Loop Interpretability Prior", "author_names": ["Isaac Lage", "A. Ross", "Been Kim", "S. Gershman", "F. Doshi-Velez"], "venue": "Neural Information Processing Systems", "abstract": "We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.", "year": 2018, "publicationdate": "2018-05-29", "externalids": {}, "doi_lower": null}
{"paper_id": 260496023, "title": "Building Machines that Learn and Think Like People", "author_names": ["J. Tenenbaum"], "venue": "Adaptive Agents and Multi-Agent Systems", "abstract": null, "year": 2018, "publicationdate": "2018-07-09", "externalids": {}, "doi_lower": null}
{"paper_id": 246826038, "title": "Auditing the AI auditors: A framework for evaluating fairness and bias in high stakes AI predictive models.", "author_names": ["R. Landers", "Tara S. Behrend"], "venue": "American Psychologist", "abstract": "Researchers, governments, ethics watchdogs, and the public are increasingly voicing concerns about unfairness and bias in artificial intelligence (AI)-based decision tools. Psychology's more-than-a-century of research on the measurement of psychological traits and the prediction of human behavior can benefit such conversations, yet psychological researchers often find themselves excluded due to mismatches in terminology, values, and goals across disciplines. In the present paper, we begin to build a shared interdisciplinary understanding of AI fairness and bias by first presenting three major lenses, which vary in focus and prototypicality by discipline, from which to consider relevant issues: (a) individual attitudes, (b) legality, ethicality, and morality, and (c) embedded meanings within technical domains. Using these lenses, we next present psychological audits as a standardized approach for evaluating the fairness and bias of AI systems that make predictions about humans across disciplinary perspectives. We present 12 crucial components to audits across three categories: (a) components related to AI models in terms of their source data, design, development, features, processes, and outputs, (b) components related to how information about models and their applications are presented, discussed, and understood from the perspectives of those employing the algorithm, those affected by decisions made using its predictions, and third-party observers, and (c) meta-components that must be considered across all other auditing components, including cultural context, respect for persons, and the integrity of individual research designs used to support all model developer claims. (PsycInfo Database Record (c) 2022 APA, all rights reserved).", "year": 2022, "publicationdate": "2022-02-14", "externalids": {"DOI": "10.1037/amp0000972"}, "doi_lower": "10.1037/amp0000972"}
{"paper_id": 268032142, "title": "When Your AIs Deceive You: Challenges with Partial Observability of Human Evaluators in Reward Learning", "author_names": ["Leon Lang", "Davis Foote", "Stuart J. Russell", "Anca Dragan", "Erik Jenner", "Scott Emmons"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2402.17747"}, "doi_lower": "10.48550/arxiv.2402.17747"}
{"paper_id": 118877712, "title": "Multiple Testing of Causal Hypotheses", "author_names": ["Samantha Kleinberg", "B. Mishra"], "venue": "", "abstract": null, "year": 2008, "publicationdate": null, "externalids": {"DOI": "10.1093/ACPROF:OSO/9780199574131.003.0031"}, "doi_lower": "10.1093/acprof:oso/9780199574131.003.0031"}
{"paper_id": 251881108, "title": "A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27", "author_names": ["Yann LeCun", "Courant"], "venue": "", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 249847889, "title": "Query-Efficient and Scalable Black-Box Adversarial Attacks on Discrete Sequential Data via Bayesian Optimization", "author_names": ["Deokjae Lee", "Seungyong Moon", "Junhyeok Lee", "Hyun Oh Song"], "venue": "International Conference on Machine Learning", "abstract": "We focus on the problem of adversarial attacks against models on discrete sequential data in the black-box setting where the attacker aims to craft adversarial examples with limited query access to the victim model. Existing black-box attacks, mostly based on greedy algorithms, find adversarial examples using pre-computed key positions to perturb, which severely limits the search space and might result in suboptimal solutions. To this end, we propose a query-efficient black-box attack using Bayesian optimization, which dynamically computes important positions using an automatic relevance determination (ARD) categorical kernel. We introduce block decomposition and history subsampling techniques to improve the scalability of Bayesian optimization when an input sequence becomes long. Moreover, we develop a post-optimization algorithm that finds adversarial examples with smaller perturbation size. Experiments on natural language and protein classification tasks demonstrate that our method consistently achieves higher attack success rate with significant reduction in query count and modification rate compared to the previous state-of-the-art methods.", "year": 2022, "publicationdate": "2022-06-17", "externalids": {"DOI": "10.48550/arXiv.2206.08575"}, "doi_lower": "10.48550/arxiv.2206.08575"}
{"paper_id": 261493811, "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback", "author_names": ["Harrison Lee", "Samrat Phatale", "Hassan Mansoor", "Kellie Lu", "Thomas Mesnard", "Colton Bishop", "Victor Carbune", "Abhinav Rastogi"], "venue": "International Conference on Machine Learning", "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards\"self-improvement\"by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.", "year": 2023, "publicationdate": "2023-09-01", "externalids": {}, "doi_lower": null}
{"paper_id": 9549525, "title": "Interactive Visualization and Manipulation of Attention-based Neural Machine Translation", "author_names": ["Jaesong Lee", "Joong-Hwi Shin", "Junseok Kim"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "While neural machine translation (NMT) provides high-quality translation, it is still hard to interpret and analyze its behavior. We present an interactive interface for visualizing and intervening behavior of NMT, specifically concentrating on the behavior of beam search mechanism and attention component. The tool (1) visualizes search tree and attention and (2) provides interface to adjust search tree and attention weight (manually or automatically) at real-time. We show the tool gives various methods to understand NMT.", "year": 2017, "publicationdate": "2017-09-01", "externalids": {"DOI": "10.18653/v1/D17-2021"}, "doi_lower": "10.18653/v1/d17-2021"}
{"paper_id": 186206676, "title": "Efficient Exploration via State Marginal Matching", "author_names": ["Lisa Lee", "Benjamin Eysenbach", "Emilio Parisotto", "E. Xing", "S. Levine", "R. Salakhutdinov"], "venue": "arXiv.org", "abstract": "Exploration is critical to a reinforcement learning agent's performance in its given environment. Prior exploration methods are often based on using heuristic auxiliary predictions to guide policy behavior, lacking a mathematically-grounded objective with clear properties. In contrast, we recast exploration as a problem of State Marginal Matching (SMM), where we aim to learn a policy for which the state marginal distribution matches a given target state distribution. The target distribution is a uniform distribution in most cases, but can incorporate prior knowledge if available. In effect, SMM amortizes the cost of learning to explore in a given environment. The SMM objective can be viewed as a two-player, zero-sum game between a state density model and a parametric policy, an idea that we use to build an algorithm for optimizing the SMM objective. Using this formalism, we further demonstrate that prior work approximately maximizes the SMM objective, offering an explanation for the success of these methods. On both simulated and real-world tasks, we demonstrate that agents that directly optimize the SMM objective explore faster and adapt more quickly to new tasks as compared to prior exploration methods.", "year": 2019, "publicationdate": "2019-06-12", "externalids": {}, "doi_lower": null}
{"paper_id": 249696209, "title": "What If Artificial Intelligence Become Completely Ambient in Our Daily Lives? Exploring Future Human-AI Interaction through High Fidelity Illustrations", "author_names": ["Sunok Lee", "Minha Lee", "Sangsu Lee"], "venue": "International journal of human computer interactions", "abstract": "Abstract As artificial intelligence (AI) has become prevalent in users’ daily lives, it is becoming critical for HCI designers to envision and design future human-AI interactions. Recent studies have highlighted the importance of exploring user-centered future design directions before implementing a technology in users’ lives. However, it is challenging for HCI designers to envision the societal impact of future technology that does not exist and understand potential users’ perceptions. Therefore, to comprehensively envision future human-AI interactions and their impact and elicit potential users’ perceptions of the future technologies, we created high-fidelity illustrations with designers and illustrators for immersive experience. Subsequently, through an online exhibition of these illustrations, we derived potential users’ perceptions, expectations, and concerns about the future. Based on our findings, we explored user-centered considerations for implementing AI in users’ daily lives through elaborately articulated human-AI interactions.", "year": 2022, "publicationdate": "2022-06-14", "externalids": {"DOI": "10.1080/10447318.2022.2080155"}, "doi_lower": "10.1080/10447318.2022.2080155"}
{"paper_id": 4519185, "title": "The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities", "author_names": ["J. Lehman", "J. Clune", "D. Misevic", "C. Adami", "L. Altenberg", "Julie Beaulieu", "P. Bentley", "Samuel Bernard", "G. Beslon", "David M. Bryson", "P. Chrabaszcz", "Nick Cheney", "Antoine Cully", "S. Doncieux", "F. Dyer", "K. Ellefsen", "R. Feldt", "Stephan Fischer", "S. Forrest", "Antoine Frénoy", "Christian Gagné", "L. L. Goff", "L. Grabowski", "B. Hodjat", "F. Hutter", "L. Keller", "C. Knibbe", "Peter Krcah", "R. Lenski", "H. Lipson", "R. Maccurdy", "Carlos Maestre", "R. Miikkulainen", "Sara Mitri", "David E. Moriarty", "Jean-Baptiste Mouret", "Anh Totti Nguyen", "Charles Ofria", "M. Parizeau", "David P. Parsons", "Robert T. Pennock", "W. Punch", "T. Ray", "Marc Schoenauer", "E. Shulte", "Karl Sims", "Kenneth O. Stanley", "F. Taddei", "Danesh S. Tarapore", "S. Thibault", "Westley Weimer", "R. Watson", "Jason Yosinksi"], "venue": "Artificial Life", "abstract": "Evolution provides a creative fount of complex and subtle adaptations that often surprise the scientists who discover them. However, the creativity of evolution is not limited to the natural world: Artificial organisms evolving in computational environments have also elicited surprise and wonder from the researchers studying them. The process of evolution is an algorithmic process that transcends the substrate in which it occurs. Indeed, many researchers in the field of digital evolution can provide examples of how their evolving algorithms and organisms have creatively subverted their expectations or intentions, exposed unrecognized bugs in their code, produced unexpectedly adaptations, or engaged in behaviors and outcomes, uncannily convergent with ones found in nature. Such stories routinely reveal surprise and creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. Bugs are fixed, experiments are refocused, and one-off surprises are collapsed into a single data point. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This article is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.", "year": 2018, "publicationdate": "2018-03-09", "externalids": {"DOI": "10.1162/artl_a_00319"}, "doi_lower": "10.1162/artl_a_00319"}
{"paper_id": 2367605, "title": "Exploiting Open-Endedness to Solve Problems Through the Search for Novelty", "author_names": ["J. Lehman", "Kenneth O. Stanley"], "venue": "IEEE Symposium on Artificial Life", "abstract": null, "year": 2008, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 235826320, "title": "Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot", "author_names": ["Joel Z. Leibo", "Edgar A. Duéñez-Guzmán", "A. Vezhnevets", "J. Agapiou", "P. Sunehag", "R. Koster", "Jayd Matyas", "Charlie Beattie", "Igor Mordatch", "T. Graepel"], "venue": "International Conference on Machine Learning", "abstract": "Existing evaluation suites for multi-agent reinforcement learning (MARL) do not assess generalization to novel situations as their primary objective (unlike supervised-learning benchmarks). Our contribution, Melting Pot, is a MARL evaluation suite that fills this gap, and uses reinforcement learning to reduce the human labor required to create novel test scenarios. This works because one agent's behavior constitutes (part of) another agent's environment. To demonstrate scalability, we have created over 80 unique test scenarios covering a broad range of research topics such as social dilemmas, reciprocity, resource sharing, and task partitioning. We apply these test scenarios to standard MARL training algorithms, and demonstrate how Melting Pot reveals weaknesses not apparent from training performance alone.", "year": 2021, "publicationdate": "2021-07-14", "externalids": {}, "doi_lower": null}
{"paper_id": 250258472, "title": "Societys' Proceedings", "author_names": ["C. Ballance"], "venue": "Journal of Laryngology and Otology", "abstract": null, "year": 1922, "publicationdate": "1922-03-01", "externalids": {"DOI": "10.1017/S0022215100023124"}, "doi_lower": "10.1017/s0022215100023124"}
{"paper_id": 267365286, "title": "Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning", "author_names": ["Jitao Sang", "Yuhang Wang", "Jing Zhang", "Yanxu Zhu", "Chao Kong", "Junhong Ye", "Shuyu Wei", "Jinlin Xiao"], "venue": "arXiv.org", "abstract": "This paper presents a follow-up study to OpenAI's recent superalignment work on Weak-to-Strong Generalization (W2SG). Superalignment focuses on ensuring that high-level AI systems remain consistent with human values and intentions when dealing with complex, high-risk tasks. The W2SG framework has opened new possibilities for empirical research in this evolving field. Our study simulates two phases of superalignment under the W2SG framework: the development of general superhuman models and the progression towards superintelligence. In the first phase, based on human supervision, the quality of weak supervision is enhanced through a combination of scalable oversight and ensemble learning, reducing the capability gap between weak teachers and strong students. In the second phase, an automatic alignment evaluator is employed as the weak supervisor. By recursively updating this auto aligner, the capabilities of the weak teacher models are synchronously enhanced, achieving weak-to-strong supervision over stronger student models.We also provide an initial validation of the proposed approach for the first phase. Using the SciQ task as example, we explore ensemble learning for weak teacher models through bagging and boosting. Scalable oversight is explored through two auxiliary settings: human-AI interaction and AI-AI debate. Additionally, the paper discusses the impact of improved weak supervision on enhancing weak-to-strong generalization based on in-context learning. Experiment code and dataset will be released at https://github.com/ADaM-BJTU/W2SG.", "year": 2024, "publicationdate": "2024-02-01", "externalids": {"DOI": "10.48550/arXiv.2402.00667"}, "doi_lower": "10.48550/arxiv.2402.00667"}
{"paper_id": 142092325, "title": "Literature and Society’s Values", "author_names": ["Darwin T. Turner"], "venue": "English Journal", "abstract": "suppose that I have sinned no more than most teachers in my professorial generalizations about the uses of literature. Yet, during these days of anxious scrutinizing of the schoolroom shibboleths, I am haunted by one generalization in particular: that a study of literature helps one to understand people of different cultures, different social groups, and different classes. That dangerous demi-truth glistens enticingly today, its submerged nine-tenths in the path through which a humanitarian teacher hopes to sail his students from provincial ignorance to cosmopolitan awareness. Unless a teacher is alert to possible problems, he may fail in his efforts to inculcate understanding, for a brief and superficial study of literature more readily reinforces prejudices and fosters myths than it provides new vision. The problem is intensified today when many teachers and students have determined to use more literature from different cultures", "year": 1971, "publicationdate": "1971-05-01", "externalids": {"DOI": "10.2307/813065"}, "doi_lower": "10.2307/813065"}
{"paper_id": 53745764, "title": "Scalable agent alignment via reward modeling: a research direction", "author_names": ["Jan Leike", "David Krueger", "Tom Everitt", "Miljan Martic", "Vishal Maini", "S. Legg"], "venue": "arXiv.org", "abstract": "One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.", "year": 2018, "publicationdate": "2018-11-19", "externalids": {}, "doi_lower": null}
{"paper_id": 281686041, "title": "Redefining biological weapons in the evolving AI, CRISPR, and biothreat landscape", "author_names": ["Hazem Haddad"], "venue": "Ethics Medicine and Public Health", "abstract": null, "year": 2025, "publicationdate": null, "externalids": {"DOI": "10.1016/j.jemep.2025.101176"}, "doi_lower": "10.1016/j.jemep.2025.101176"}
{"paper_id": 190161009, "title": "bo xie liu fa ji zhu yi shi xiang", "author_names": ["Y. Baoguo"], "venue": "", "abstract": null, "year": 2003, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 250301226, "title": "Interpretable Generative Adversarial Networks", "author_names": ["Chao Li", "Kelu Yao", "Jin Wang", "Boyu Diao", "Yongjun Xu", "Quanshi Zhang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Learning a disentangled representation is still a challenge in the field of the interpretability of generative adversarial networks (GANs). This paper proposes a generic method to modify a traditional GAN into an interpretable GAN, which ensures that filters in an intermediate layer of the generator encode disentangled localized visual concepts. Each filter in the layer is supposed to consistently generate image regions corresponding to the same visual concept when generating different images. The interpretable GAN learns to automatically discover meaningful visual concepts without any annotations of visual concepts. The interpretable GAN enables people to modify a specific visual concept on generated images by manipulating feature maps of the corresponding filters in the layer. Our method can be broadly applied to different types of GANs. Experiments have demonstrated the effectiveness of our method.", "year": 2022, "publicationdate": "2022-06-28", "externalids": {"DOI": "10.1609/aaai.v36i2.20015"}, "doi_lower": "10.1609/aaai.v36i2.20015"}
{"paper_id": 226348474, "title": "TNT: An Interpretable Tree-Network-Tree Learning Framework using Knowledge Distillation", "author_names": ["Jiawei Li", "Yiming Li", "Xingchun Xiang", "Shutao Xia", "Siyi Dong", "Yun Cai"], "venue": "Entropy", "abstract": "Deep Neural Networks (DNNs) usually work in an end-to-end manner. This makes the trained DNNs easy to use, but they remain an ambiguous decision process for every test case. Unfortunately, the interpretability of decisions is crucial in some scenarios, such as medical or financial data mining and decision-making. In this paper, we propose a Tree-Network-Tree (TNT) learning framework for explainable decision-making, where the knowledge is alternately transferred between the tree model and DNNs. Specifically, the proposed TNT learning framework exerts the advantages of different models at different stages: (1) a novel James–Stein Decision Tree (JSDT) is proposed to generate better knowledge representations for DNNs, especially when the input data are in low-frequency or low-quality; (2) the DNNs output high-performing prediction result from the knowledge embedding inputs and behave as a teacher model for the following tree model; and (3) a novel distillable Gradient Boosted Decision Tree (dGBDT) is proposed to learn interpretable trees from the soft labels and make a comparable prediction as DNNs do. Extensive experiments on various machine learning tasks demonstrated the effectiveness of the proposed method.", "year": 2020, "publicationdate": "2020-10-24", "externalids": {"DOI": "10.3390/e22111203"}, "doi_lower": "10.3390/e22111203"}
{"paper_id": 253098566, "title": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task", "author_names": ["Kenneth Li", "Aspen K. Hopkins", "David Bau", "Fernanda Vi'egas", "H. Pfister", "M. Wattenberg"], "venue": "International Conference on Learning Representations", "abstract": "Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create\"latent saliency maps\"that can help explain predictions in human terms.", "year": 2022, "publicationdate": "2022-10-24", "externalids": {"DOI": "10.48550/arXiv.2210.13382"}, "doi_lower": "10.48550/arxiv.2210.13382"}
{"paper_id": 126323935, "title": "A review of dynamic Stackelberg game models", "author_names": ["Tao Li", "S. Sethi"], "venue": "", "abstract": null, "year": 2016, "publicationdate": "2016-12-01", "externalids": {"DOI": "10.3934/DCDSB.2017007"}, "doi_lower": "10.3934/dcdsb.2017007"}
{"paper_id": 258740697, "title": "Evaluating Object Hallucination in Large Vision-Language Models", "author_names": ["Yifan Li", "Yifan Du", "Kun Zhou", "Jinpeng Wang", "Wayne Xin Zhao", "Ji-rong Wen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructions and generation styles of LVLMs. Thus, we further design an improved evaluation method for object hallucination by proposing a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way. Our codes and data are publicly available at https://github.com/RUCAIBox/POPE.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {"DOI": "10.48550/arXiv.2305.10355"}, "doi_lower": "10.48550/arxiv.2305.10355"}
{"paper_id": 235623756, "title": "Towards Understanding and Mitigating Social Biases in Language Models", "author_names": ["Paul Pu Liang", "Chiyu Wu", "Louis-philippe Morency", "R. Salakhutdinov"], "venue": "International Conference on Machine Learning", "abstract": "As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.", "year": 2021, "publicationdate": "2021-06-24", "externalids": {}, "doi_lower": null}
{"paper_id": 16630581, "title": "The effect of social motives, communication and group size on behaviour in an N-person multi-stage mixed-motive game", "author_names": ["W. Liebrand"], "venue": "", "abstract": null, "year": 1984, "publicationdate": "1984-07-01", "externalids": {"DOI": "10.1002/EJSP.2420140302"}, "doi_lower": "10.1002/ejsp.2420140302"}
{"paper_id": 202540096, "title": "KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning", "author_names": ["Bill Yuchen Lin", "Xinyue Chen", "Jamin Chen", "Xiang Ren"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.", "year": 2019, "publicationdate": "2019-09-04", "externalids": {"DOI": "10.18653/v1/D19-1282"}, "doi_lower": "10.18653/v1/d19-1282"}
{"paper_id": 244455481, "title": "Distributionally Robust Optimization: A review on theory and applications", "author_names": ["Fengming Lin", "X. Fang", "Zheming Gao"], "venue": "Numerical Algebra, Control and Optimization", "abstract": "In this paper, we survey the primary research on the theory and applications of distributionally robust optimization (DRO). We start with reviewing the modeling power and computational attractiveness of DRO approaches, induced by the ambiguity sets structure and tractable robust counterpart reformulations. Next, we summarize the efficient solution methods, out-of-sample performance guarantee, and convergence analysis. Then, we illustrate some applications of DRO in machine learning and operations research, and finally, we discuss the future research directions.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.3934/naco.2021057"}, "doi_lower": "10.3934/naco.2021057"}
{"paper_id": 247996664, "title": "Inferring Rewards from Language in Context", "author_names": ["Jessy Lin", "Daniel Fried", "D. Klein", "A. Dragan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In classic instruction following, language like “I’d like the JetBlue flight” maps to actions (e.g., selecting that flight). However, language also conveys information about a user’s underlying reward function (e.g., a general preference for JetBlue), which can allow a model to carry out desirable actions in new contexts. We present a model that infers rewards from language pragmatically: reasoning about how speakers choose utterances not only to elicit desired actions, but also to reveal information about their preferences. On a new interactive flight–booking task with natural language, our model more accurately infers rewards and predicts optimal actions in unseen environments, in comparison to past work that first maps language to actions (instruction following) and then maps actions to rewards (inverse reinforcement learning).", "year": 2022, "publicationdate": "2022-04-05", "externalids": {"DOI": "10.48550/arXiv.2204.02515"}, "doi_lower": "10.48550/arxiv.2204.02515"}
{"paper_id": 237532606, "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods", "author_names": ["Stephanie C. Lin", "Jacob Hilton", "Owain Evans"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.", "year": 2021, "publicationdate": "2021-09-08", "externalids": {"DOI": "10.18653/v1/2022.acl-long.229"}, "doi_lower": "10.18653/v1/2022.acl-long.229"}
{"paper_id": 258841681, "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models", "author_names": ["Yen-Ting Lin", "Yun-Nung (Vivian) Chen"], "venue": "NLP4CONVAI", "abstract": "We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.13711"}, "doi_lower": "10.48550/arxiv.2305.13711"}
{"paper_id": 5981909, "title": "The mythos of model interpretability", "author_names": ["Zachary Chase Lipton"], "venue": "Queue", "abstract": "In machine learning, the concept of interpretability is both important and slippery.", "year": 2016, "publicationdate": "2016-06-10", "externalids": {"DOI": "10.1145/3233231"}, "doi_lower": "10.1145/3233231"}
{"paper_id": 264590778, "title": "Training Socially Aligned Language Models on Simulated Social Interactions", "author_names": ["Ruibo Liu", "Ruixin Yang", "Chenyan Jia", "Ge Zhang", "Denny Zhou", "Andrew M. Dai", "Diyi Yang", "Soroush Vosoughi"], "venue": "International Conference on Learning Representations", "abstract": "Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.", "year": 2023, "publicationdate": "2023-05-26", "externalids": {}, "doi_lower": null}
{"paper_id": 52577367, "title": "Visual Interrogation of Attention-Based Models for Natural Language Inference and Machine Comprehension", "author_names": ["Shusen Liu", "Tao Li", "Zhimin Li", "Vivek Srikumar", "Valerio Pascucci", "P. Bremer"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Neural networks models have gained unprecedented popularity in natural language processing due to their state-of-the-art performance and the flexible end-to-end training scheme. Despite their advantages, the lack of interpretability hinders the deployment and refinement of the models. In this work, we present a flexible visualization library for creating customized visual analytic environments, in which the user can investigate and interrogate the relationships among the input, the model internals (i.e., attention), and the output predictions, which in turn shed light on the model decision-making process.", "year": 2018, "publicationdate": "2018-11-01", "externalids": {"DOI": "10.18653/v1/D18-2007"}, "doi_lower": "10.18653/v1/d18-2007"}
{"paper_id": 260682249, "title": "AgentBench: Evaluating LLMs as Agents", "author_names": ["Xiao Liu", "Hao Yu", "Hanchen Zhang", "Yifan Xu", "Xuanyu Lei", "Hanyu Lai", "Yu Gu", "Yuxian Gu", "Hangliang Ding", "Kai Men", "Kejuan Yang", "Shudan Zhang", "Xiang Deng", "Aohan Zeng", "Zhengxiao Du", "Chenhui Zhang", "Shengqi Shen", "Tianjun Zhang", "Sheng Shen", "Yu Su", "Huan Sun", "Minlie Huang", "Yuxiao Dong", "Jie Tang"], "venue": "International Conference on Learning Representations", "abstract": "The potential of Large Language Model (LLM) as agents has been widely acknowledged recently. Thus, there is an urgent need to quantitatively \\textit{evaluate LLMs as agents} on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities. Our extensive test over \\num API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Improving instruction following and training on high quality multi-round alignment data could improve agent performance. And different from existing assumptions, training on code present ambivalent impacts on different agent tasks. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.", "year": 2023, "publicationdate": "2023-08-07", "externalids": {"DOI": "10.48550/arXiv.2308.03688"}, "doi_lower": "10.48550/arxiv.2308.03688"}
{"paper_id": 215828407, "title": "Adversarial Training for Large Neural Language Models", "author_names": ["Xiaodong Liu", "Hao Cheng", "Pengcheng He", "Weizhu Chen", "Yu Wang", "Hoifung Poon", "Jianfeng Gao"], "venue": "arXiv.org", "abstract": "Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at this https URL.", "year": 2020, "publicationdate": "2020-04-20", "externalids": {}, "doi_lower": null}
{"paper_id": 9114432, "title": "Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning", "author_names": ["R. Loftin", "Bei Peng", "J. MacGlashan", "M. Littman", "Matthew E. Taylor", "Jeff Huang", "D. Roberts"], "venue": "Autonomous Agents and Multi-Agent Systems", "abstract": null, "year": 2015, "publicationdate": "2015-02-13", "externalids": {"DOI": "10.1007/s10458-015-9283-7"}, "doi_lower": "10.1007/s10458-015-9283-7"}
{"paper_id": 235742835, "title": "Physical interaction as communication: Learning robot objectives online from human corrections", "author_names": ["Dylan P. Losey", "Andrea V. Bajcsy", "M. O'Malley", "A. Dragan"], "venue": "Int. J. Robotics Res.", "abstract": "When a robot performs a task next to a human, physical interaction is inevitable: the human might push, pull, twist, or guide the robot. The state of the art treats these interactions as disturbances that the robot should reject or avoid. At best, these robots respond safely while the human interacts; but after the human lets go, these robots simply return to their original behavior. We recognize that physical human–robot interaction (pHRI) is often intentional: the human intervenes on purpose because the robot is not doing the task correctly. In this article, we argue that when pHRI is intentional it is also informative: the robot can leverage interactions to learn how it should complete the rest of its current task even after the person lets go. We formalize pHRI as a dynamical system, where the human has in mind an objective function they want the robot to optimize, but the robot does not get direct access to the parameters of this objective: they are internal to the human. Within our proposed framework human interactions become observations about the true objective. We introduce approximations to learn from and respond to pHRI in real-time. We recognize that not all human corrections are perfect: often users interact with the robot noisily, and so we improve the efficiency of robot learning from pHRI by reducing unintended learning. Finally, we conduct simulations and user studies on a robotic manipulator to compare our proposed approach with the state of the art. Our results indicate that learning from pHRI leads to better task performance and improved human satisfaction.", "year": 2021, "publicationdate": "2021-07-06", "externalids": {"DOI": "10.1177/02783649211050958"}, "doi_lower": "10.1177/02783649211050958"}
{"paper_id": 26419660, "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments", "author_names": ["Ryan Lowe", "Yi Wu", "Aviv Tamar", "J. Harb", "P. Abbeel", "Igor Mordatch"], "venue": "Neural Information Processing Systems", "abstract": "We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.", "year": 2017, "publicationdate": "2017-06-07", "externalids": {}, "doi_lower": null}
{"paper_id": 253523007, "title": "Mechanistic Mode Connectivity", "author_names": ["E. Lubana", "Eric J. Bigelow", "R. Dick", "David Krueger", "Hidenori Tanaka"], "venue": "International Conference on Machine Learning", "abstract": "We study neural network loss landscapes through the lens of mode connectivity, the observation that minimizers of neural networks retrieved via training on a dataset are connected via simple paths of low loss. Specifically, we ask the following question: are minimizers that rely on different mechanisms for making their predictions connected via simple paths of low loss? We provide a definition of mechanistic similarity as shared invariances to input transformations and demonstrate that lack of linear connectivity between two models implies they use dissimilar mechanisms for making their predictions. Relevant to practice, this result helps us demonstrate that naive fine-tuning on a downstream dataset can fail to alter a model's mechanisms, e.g., fine-tuning can fail to eliminate a model's reliance on spurious attributes. Our analysis also motivates a method for targeted alteration of a model's mechanisms, named connectivity-based fine-tuning (CBFT), which we analyze using several synthetic datasets for the task of reducing a model's reliance on spurious attributes.", "year": 2022, "publicationdate": "2022-11-15", "externalids": {"DOI": "10.48550/arXiv.2211.08422"}, "doi_lower": "10.48550/arxiv.2211.08422"}
{"paper_id": 247084007, "title": "A Rigorous Study of Integrated Gradients Method and Extensions to Internal Neuron Attributions", "author_names": ["Daniel Lundstrom", "Tianjian Huang", "Meisam Razaviyayn"], "venue": "International Conference on Machine Learning", "abstract": "As deep learning (DL) efficacy grows, concerns for poor model explainability grow also. Attribution methods address the issue of explainability by quantifying the importance of an input feature for a model prediction. Among various methods, Integrated Gradients (IG) sets itself apart by claiming other methods failed to satisfy desirable axioms, while IG and methods like it uniquely satisfy said axioms. This paper comments on fundamental aspects of IG and its applications/extensions: 1) We identify key differences between IG function spaces and the supporting literature's function spaces which problematize previous claims of IG uniqueness. We show that with the introduction of an additional axiom, \\textit{non-decreasing positivity}, the uniqueness claims can be established. 2) We address the question of input sensitivity by identifying function classes where IG is/is not Lipschitz in the attributed input. 3) We show that axioms for single-baseline methods have analogous properties for methods with probability distribution baselines. 4) We introduce a computationally efficient method of identifying internal neurons that contribute to specified regions of an IG attribution map. Finally, we present experimental results validating this method.", "year": 2022, "publicationdate": "2022-02-24", "externalids": {}, "doi_lower": null}
{"paper_id": 67877011, "title": "Learning Latent Plans from Play", "author_names": ["Corey Lynch", "Mohi Khansari", "Ted Xiao", "Vikash Kumar", "Jonathan Tompson", "S. Levine", "P. Sermanet"], "venue": "Conference on Robot Learning", "abstract": "We propose learning from teleoperated play data (LfP) as a way to scale up multi-task robotic skill learning. Learning from play (LfP) offers three main advantages: 1) It is cheap. Large amounts of play data can be collected quickly as it does not require scene staging, task segmenting, or resetting to an initial state. 2) It is general. It contains both functional and non-functional behavior, relaxing the need for a predefined task distribution. 3) It is rich. Play involves repeated, varied behavior and naturally leads to high coverage of the possible interaction space. These properties distinguish play from expert demonstrations, which are rich, but expensive, and scripted unattended data collection, which is cheap, but insufficiently rich. Variety in play, however, presents a multimodality challenge to methods seeking to learn control on top. To this end, we introduce Play-LMP, a method designed to handle variability in the LfP setting by organizing it in an embedding space. Play-LMP jointly learns 1) reusable latent plan representations unsupervised from play data and 2) a single goal-conditioned policy capable of decoding inferred plans to achieve user-specified tasks. We show empirically that Play-LMP, despite not being trained on task-specific data, is capable of generalizing to 18 complex user-specified manipulation tasks with average success of 85.5%, outperforming individual models trained on expert demonstrations (success of 70.3%). Furthermore, we find that play-supervised models, unlike their expert-trained counterparts, 1) are more robust to perturbations and 2) exhibit retrying-till-success. Finally, despite never being trained with task labels, we find that our agent learns to organize its latent plan space around functional tasks. Videos of the performed experiments are available at learning-from-play.github.io", "year": 2019, "publicationdate": "2019-03-05", "externalids": {}, "doi_lower": null}
{"paper_id": 252846090, "title": "Interactive Language: Talking to Robots in Real Time", "author_names": ["Corey Lynch", "Ayzaan Wahid", "Jonathan Tompson", "Tianli Ding", "James Betker", "Robert Baruch", "Travis Armstrong", "Peter R. Florence"], "venue": "IEEE Robotics and Automation Letters", "abstract": "We present a framework for building interactive, real-time, natural language-instructable robots in the real world, and we open source related assets (dataset, environment, benchmark, and policies). Trained with behavioral cloning on a dataset of hundreds of thousands of language-annotated trajectories, a produced policy can proficiently execute an order of magnitude more commands than previous works: specifically we estimate a 93.5% success rate on a set of 87,000 unique natural language strings specifying raw end-to-end visuo-linguo-motor skills in the real world. We find that the same policy is capable of being guided by a human via real-time language to address a wide range of precise long-horizon rearrangement goals, e.g.\"make a smiley face out of blocks\". The dataset we release comprises nearly 600,000 language-labeled trajectories, an order of magnitude larger than prior available datasets. We hope the demonstrated results and associated assets enable further advancement of helpful, capable, natural-language-interactable robots. See videos at https://interactive-language.github.io.", "year": 2022, "publicationdate": "2022-10-12", "externalids": {"DOI": "10.48550/arXiv.2210.06407"}, "doi_lower": "10.48550/arxiv.2210.06407"}
{"paper_id": 2533387, "title": "Shadow attacks: automatically evading system-call-behavior based malware detection", "author_names": ["Weiqin Ma", "Pu Duan", "Sanmin Liu", "G. Gu", "Jyh-Charn S. Liu"], "venue": "Journal in Computer Virology", "abstract": null, "year": 2011, "publicationdate": "2011-12-20", "externalids": {"DOI": "10.1007/s11416-011-0157-5"}, "doi_lower": "10.1007/s11416-011-0157-5"}
{"paper_id": 252780409, "title": "ELIGN: Expectation Alignment as a Multi-Agent Intrinsic Reward", "author_names": ["Zixian Ma", "Rose E. Wang", "Li Fei-Fei", "Michael S. Bernstein", "Ranjay Krishna"], "venue": "Neural Information Processing Systems", "abstract": "Modern multi-agent reinforcement learning frameworks rely on centralized training and reward shaping to perform well. However, centralized training and dense rewards are not readily available in the real world. Current multi-agent algorithms struggle to learn in the alternative setup of decentralized training or sparse rewards. To address these issues, we propose a self-supervised intrinsic reward ELIGN - expectation alignment - inspired by the self-organization principle in Zoology. Similar to how animals collaborate in a decentralized manner with those in their vicinity, agents trained with expectation alignment learn behaviors that match their neighbors' expectations. This allows the agents to learn collaborative behaviors without any external reward or centralized training. We demonstrate the efficacy of our approach across 6 tasks in the multi-agent particle and the complex Google Research football environments, comparing ELIGN to sparse and curiosity-based intrinsic rewards. When the number of agents increases, ELIGN scales well in all multi-agent tasks except for one where agents have different capabilities. We show that agent coordination improves through expectation alignment because agents learn to divide tasks amongst themselves, break coordination symmetries, and confuse adversaries. These results identify tasks where expectation alignment is a more useful strategy than curiosity-driven exploration for multi-agent coordination, enabling agents to do zero-shot coordination.", "year": 2022, "publicationdate": "2022-10-09", "externalids": {"DOI": "10.48550/arXiv.2210.04365"}, "doi_lower": "10.48550/arxiv.2210.04365"}
{"paper_id": 241884720, "title": "Aligning AI Regulation to Sociotechnical Change", "author_names": ["M. Maas"], "venue": "Social Science Research Network", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.2139/ssrn.3871635"}, "doi_lower": "10.2139/ssrn.3871635"}
{"paper_id": 5855042, "title": "Visualizing Data using t-SNE", "author_names": ["L. Maaten", "Geoffrey E. Hinton"], "venue": "", "abstract": null, "year": 2008, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 8818528, "title": "Interactive Learning from Policy-Dependent Human Feedback", "author_names": ["J. MacGlashan", "Mark K. Ho", "R. Loftin", "Bei Peng", "Guan Wang", "David L. Roberts", "Matthew E. Taylor", "M. Littman"], "venue": "International Conference on Machine Learning", "abstract": "This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner's current policy. We present empirical results that show this assumption to be false— whether human trainers give a positive or negative feedback for a decision is influenced by the learner's current policy. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot.", "year": 2017, "publicationdate": "2017-01-21", "externalids": {}, "doi_lower": null}
{"paper_id": 3488815, "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "author_names": ["A. Ma̧dry", "Aleksandar Makelov", "Ludwig Schmidt", "Dimitris Tsipras", "Adrian Vladu"], "venue": "International Conference on Learning Representations", "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.", "year": 2017, "publicationdate": "2017-06-19", "externalids": {}, "doi_lower": null}
{"paper_id": 236976388, "title": "Post-hoc Interpretability for Neural NLP: A Survey", "author_names": ["Andreas Madsen", "Siva Reddy", "A. Chandar"], "venue": "ACM Computing Surveys", "abstract": "Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use. Explaining models helps to address the safety and ethical concerns and is essential for accountability. Interpretability serves to provide these explanations in terms that are understandable to humans. Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic. This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern.", "year": 2021, "publicationdate": "2021-08-10", "externalids": {"DOI": "10.1145/3546577"}, "doi_lower": "10.1145/3546577"}
{"paper_id": 258947572, "title": "Parameter-Efficient Fine-Tuning without Introducing New Latency", "author_names": ["Baohao Liao", "Yan Meng", "C. Monz"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Parameter-efficient fine-tuning (PEFT) of pre-trained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constraints. Nonetheless, various PEFT methods are limited by their inherent characteristics. In the case of sparse fine-tuning, which involves modifying only a small subset of the existing parameters, the selection of fine-tuned parameters is task- and domain-specific, making it unsuitable for federated learning. On the other hand, PEFT methods with adding new parameters typically introduce additional inference latency. In this paper, we demonstrate the feasibility of generating a sparse mask in a task-agnostic manner, wherein all downstream tasks share a common mask. Our approach, which relies solely on the magnitude information of pre-trained parameters, surpasses existing methodologies by a significant margin when evaluated on the GLUE benchmark. Additionally, we introduce a novel adapter technique that directly applies the adapter to pre-trained parameters instead of the hidden representation, thereby achieving identical inference speed to that of full fine-tuning. Through extensive experiments, our proposed method attains a new state-of-the-art outcome in terms of both performance and storage efficiency, storing only 0.03% parameters of full fine-tuning.", "year": 2023, "publicationdate": "2023-05-26", "externalids": {"DOI": "10.48550/arXiv.2305.16742"}, "doi_lower": "10.48550/arxiv.2305.16742"}
{"paper_id": 259100399, "title": "Faster sorting algorithms discovered using deep reinforcement learning", "author_names": ["D. Mankowitz", "Andrea Michi", "A. Zhernov", "Marco Gelmi", "M. Selvi", "Cosmin Paduraru", "Edouard Leurent", "Shariq Iqbal", "Jean-Baptiste Lespiau", "Alexa Ahern", "Thomas Köppe", "Kevin Millikin", "Stephen Gaffney", "Sophie Elster", "Jackson Broshear", "Chris Gamble", "Kieran Milan", "R. Tung", "Minjae Hwang", "taylan. cemgil", "M. Barekatain", "Yujia Li", "Amol Mandhane", "T. Hubert", "Julian Schrittwieser", "D. Hassabis", "Pushmeet Kohli", "Martin A. Riedmiller", "O. Vinyals", "David Silver"], "venue": "Nature", "abstract": "Artificial intelligence goes beyond the current state of the art by discovering unknown, faster sorting algorithms as a single-player game using a deep reinforcement learning agent. These algorithms are now used in the standard C++ sort library. Fundamental algorithms such as sorting or hashing are used trillions of times on any given day^ 1 . As demand for computation grows, it has become critical for these algorithms to be as performant as possible. Whereas remarkable progress has been achieved in the past^ 2 , making further improvements on the efficiency of these routines has proved challenging for both human scientists and computational approaches. Here we show how artificial intelligence can go beyond the current state of the art by discovering hitherto unknown routines. To realize this, we formulated the task of finding a better sorting routine as a single-player game. We then trained a new deep reinforcement learning agent, AlphaDev, to play this game. AlphaDev discovered small sorting algorithms from scratch that outperformed previously known human benchmarks. These algorithms have been integrated into the LLVM standard C++ sort library^ 3 . This change to this part of the sort library represents the replacement of a component with an algorithm that has been automatically discovered using reinforcement learning. We also present results in extra domains, showcasing the generality of the approach.", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.1038/s41586-023-06004-9"}, "doi_lower": "10.1038/s41586-023-06004-9"}
{"paper_id": 219076548, "title": "Governance, Risk, and Artificial Intelligence", "author_names": ["A. Mannes"], "venue": "The AI Magazine", "abstract": "Artificial intelligence, whether embodied as robots or Internet of Things, or disembodied as intelligent agents or decision-support systems, can enrich the human experience. It will also fail and cause harms, including physical injury and financial loss as well as more subtle harms such as instantiating human bias or undermining individual dignity. These failures could have a disproportionate impact because strange, new, and unpredictable dangers may lead to public discomfort and rejection of artificial intelligence. Two possible approaches to mitigating these risks are the hard power of regulating artificial intelligence, to ensure it is safe, and the soft power of risk communication, which engages the public and builds trust. These approaches are complementary and both should be implemented as artificial intelligence becomes increasingly prevalent in daily life.", "year": 2020, "publicationdate": "2020-04-13", "externalids": {"DOI": "10.1609/aimag.v41i1.5200"}, "doi_lower": "10.1609/aimag.v41i1.5200"}
{"paper_id": 114479521, "title": "A future that works: automation, employment, and productivity", "author_names": ["J. Manyika", "Michael Chui", "Mehdi Miremadi", "J. Bughin", "Katy George", "Paul Willmott", "Martin Dewhurst"], "venue": "", "abstract": null, "year": 2017, "publicationdate": "2017-01-12", "externalids": {}, "doi_lower": null}
{"paper_id": 252355068, "title": "Enhance the Visual Representation via Discrete Adversarial Training", "author_names": ["Xiaofeng Mao", "Yuefeng Chen", "Ranjie Duan", "Yao Zhu", "Gege Qi", "Shaokai Ye", "Xiaodan Li", "Rong Zhang", "Hui Xue"], "venue": "Neural Information Processing Systems", "abstract": "Adversarial Training (AT), which is commonly accepted as one of the most effective approaches defending against adversarial examples, can largely harm the standard performance, thus has limited usefulness on industrial-scale production and applications. Surprisingly, this phenomenon is totally opposite in Natural Language Processing (NLP) task, where AT can even benefit for generalization. We notice the merit of AT in NLP tasks could derive from the discrete and symbolic input space. For borrowing the advantage from NLP-style AT, we propose Discrete Adversarial Training (DAT). DAT leverages VQGAN to reform the image data to discrete text-like inputs, i.e. visual words. Then it minimizes the maximal risk on such discrete images with symbolic adversarial perturbations. We further give an explanation from the perspective of distribution to demonstrate the effectiveness of DAT. As a plug-and-play technique for enhancing the visual representation, DAT achieves significant improvement on multiple tasks including image classification, object detection and self-supervised learning. Especially, the model pre-trained with Masked Auto-Encoding (MAE) and fine-tuned by our DAT without extra data can get 31.40 mCE on ImageNet-C and 32.77% top-1 accuracy on Stylized-ImageNet, building the new state-of-the-art. The code will be available at https://github.com/alibaba/easyrobust.", "year": 2022, "publicationdate": "2022-09-16", "externalids": {"DOI": "10.48550/arXiv.2209.07735"}, "doi_lower": "10.48550/arxiv.2209.07735"}
{"paper_id": 9620988, "title": "Simulation-based optimization of Markov reward processes", "author_names": ["P. Marbach", "J. Tsitsiklis"], "venue": "Proceedings of the 37th IEEE Conference on Decision and Control (Cat. No.98CH36171)", "abstract": null, "year": 1998, "publicationdate": "1998-12-16", "externalids": {"DOI": "10.1109/CDC.1998.757861"}, "doi_lower": "10.1109/cdc.1998.757861"}
{"paper_id": 1872638, "title": "Deep Learning: A Critical Appraisal", "author_names": ["G. Marcus"], "venue": "arXiv.org", "abstract": "Although deep learning has historical roots going back decades, neither the term \"deep learning\" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.", "year": 2018, "publicationdate": "2018-01-02", "externalids": {}, "doi_lower": null}
{"paper_id": 3863856, "title": "Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning", "author_names": ["David Mascharka", "Philip Tran", "Ryan Soklaski", "Arjun Majumdar"], "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "abstract": "Visual question answering requires high-order reasoning about an image, which is a fundamental capability needed by machine systems to follow complex directives. Recently, modular networks have been shown to be an effective framework for performing visual reasoning tasks. While modular networks were initially designed with a degree of model transparency, their performance on complex visual reasoning benchmarks was lacking. Current state-of-the-art approaches do not provide an effective mechanism for understanding the reasoning process. In this paper, we close the performance gap between interpretable models and state-of-the-art visual reasoning methods. We propose a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner. The fidelity and interpretability of the primitives' outputs enable an unparalleled ability to diagnose the strengths and weaknesses of the resulting model. Critically, we show that these primitives are highly performant, achieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show that our model is able to effectively learn generalized representations when provided a small amount of data containing novel object attributes. Using the CoGenT generalization task, we show more than a 20 percentage point improvement over the current state of the art.", "year": 2018, "publicationdate": "2018-03-14", "externalids": {"DOI": "10.1109/CVPR.2018.00519"}, "doi_lower": "10.1109/cvpr.2018.00519"}
{"paper_id": 152196393, "title": "Chapter 3 – Social Values and Rules of Fairness: A Theoretical Perspective", "author_names": ["C. McClintock", "E. V. Avermaet"], "venue": "", "abstract": null, "year": 1982, "publicationdate": null, "externalids": {"DOI": "10.1016/B978-0-12-210820-4.50008-3"}, "doi_lower": "10.1016/b978-0-12-210820-4.50008-3"}
{"paper_id": 260334719, "title": "The Hydra Effect: Emergent Self-repair in Language Model Computations", "author_names": ["Tom McGrath", "Matthew Rahtz", "János Kramár", "Vladimir Mikulik", "S. Legg"], "venue": "arXiv.org", "abstract": "We investigate the internal structure of language model computations using causal analysis and demonstrate two motifs: (1) a form of adaptive computation where ablations of one attention layer of a language model cause another layer to compensate (which we term the Hydra effect) and (2) a counterbalancing function of late MLP layers that act to downregulate the maximum-likelihood token. Our ablation studies demonstrate that language model layers are typically relatively loosely coupled (ablations to one layer only affect a small number of downstream layers). Surprisingly, these effects occur even in language models trained without any form of dropout. We analyse these effects in the context of factual recall and consider their implications for circuit-level attribution in language models.", "year": 2023, "publicationdate": "2023-07-28", "externalids": {"DOI": "10.48550/arXiv.2307.15771"}, "doi_lower": "10.48550/arxiv.2307.15771"}
{"paper_id": 211043693, "title": "Social Diversity and Social Preferences in Mixed-Motive Reinforcement Learning", "author_names": ["Kevin R. McKee", "Ian M. Gemp", "B. McWilliams", "Edgar A. Duéñez-Guzmán", "Edward Hughes", "Joel Z. Leibo"], "venue": "Adaptive Agents and Multi-Agent Systems", "abstract": "Recent research on reinforcement learning in pure-conflict and pure-common interest games has emphasized the importance of population heterogeneity. In contrast, studies of reinforcement learning in mixed-motive games have primarily leveraged homogeneous approaches. Given the defining characteristic of mixed-motive games--the imperfect correlation of incentives between group members--we study the effect of population heterogeneity on mixed-motive reinforcement learning. We draw on interdependence theory from social psychology and imbue reinforcement learning agents with Social Value Orientation (SVO), a flexible formalization of preferences over group outcome distributions. We subsequently explore the effects of diversity in SVO on populations of reinforcement learning agents in two mixed-motive Markov games. We demonstrate that heterogeneity in SVO generates meaningful and complex behavioral variation among agents similar to that suggested by interdependence theory. Empirical results in these mixed-motive dilemmas suggest agents trained in heterogeneous populations develop particularly generalized, high-performing policies relative to those trained in homogeneous populations.", "year": 2020, "publicationdate": "2020-02-06", "externalids": {}, "doi_lower": null}
{"paper_id": 255569776, "title": "On The Fragility of Learned Reward Functions", "author_names": ["Lev McKinney", "Yawen Duan", "David Krueger", "Adam Gleave"], "venue": "arXiv.org", "abstract": "Reward functions are notoriously difficult to specify, especially for tasks with complex goals. Reward learning approaches attempt to infer reward functions from human feedback and preferences. Prior works on reward learning have mainly focused on the performance of policies trained alongside the reward function. This practice, however, may fail to detect learned rewards that are not capable of training new policies from scratch and thus do not capture the intended behavior. Our work focuses on demonstrating and studying the causes of these relearning failures in the domain of preference-based reward learning. We demonstrate with experiments in tabular and continuous control environments that the severity of relearning failures can be sensitive to changes in reward model design and the trajectory dataset composition. Based on our findings, we emphasize the need for more retraining-based evaluations in the literature.", "year": 2023, "publicationdate": "2023-01-09", "externalids": {"DOI": "10.48550/arXiv.2301.03652"}, "doi_lower": "10.48550/arxiv.2301.03652"}
{"paper_id": 238643957, "title": "The risks associated with Artificial General Intelligence: A systematic review", "author_names": ["S. Mclean", "G. Read", "Jason Thompson", "Chris Baber", "N. Stanton", "P. Salmon"], "venue": "Journal of experimental and theoretical artificial intelligence (Print)", "abstract": "ABSTRACT Artificial General intelligence (AGI) offers enormous benefits for humanity, yet it also poses great risk. The aim of this systematic review was to summarise the peer reviewed literature on the risks associated with AGI. The review followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Sixteen articles were deemed eligible for inclusion. Article types included in the review were classified as philosophical discussions, applications of modelling techniques, and assessment of current frameworks and processes in relation to AGI. The review identified a range of risks associated with AGI, including AGI removing itself from the control of human owners/managers, being given or developing unsafe goals, development of unsafe AGI, AGIs with poor ethics, morals and values; inadequate management of AGI, and existential risks. Several limitations of the AGI literature base were also identified, including a limited number of peer reviewed articles and modelling techniques focused on AGI risk, a lack of specific risk research in which domains that AGI may be implemented, a lack of specific definitions of the AGI functionality, and a lack of standardised AGI terminology. Recommendations to address the identified issues with AGI risk research are required to guide AGI design, implementation, and management.", "year": 2021, "publicationdate": "2021-08-13", "externalids": {"DOI": "10.1080/0952813X.2021.1964003"}, "doi_lower": "10.1080/0952813x.2021.1964003"}
{"paper_id": 201666566, "title": "A Survey on Bias and Fairness in Machine Learning", "author_names": ["Ninareh Mehrabi", "Fred Morstatter", "N. Saxena", "Kristina Lerman", "A. Galstyan"], "venue": "ACM Computing Surveys", "abstract": "With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.", "year": 2019, "publicationdate": "2019-08-23", "externalids": {"DOI": "10.1145/3457607"}, "doi_lower": "10.1145/3457607"}
{"paper_id": 259735240, "title": "Fairness, Accountability, Transparency, and Ethics (FATE) in Artificial Intelligence (AI) and higher education: A systematic review", "author_names": ["Bahar Memarian", "Tenzin Doleck"], "venue": "Computers and Education: Artificial Intelligence", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.1016/j.caeai.2023.100152"}, "doi_lower": "10.1016/j.caeai.2023.100152"}
{"paper_id": 255825985, "title": "Locating and Editing Factual Associations in GPT", "author_names": ["Kevin Meng", "David Bau", "A. Andonian", "Yonatan Belinkov"], "venue": "Neural Information Processing Systems", "abstract": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/", "year": 2022, "publicationdate": "2022-02-10", "externalids": {}, "doi_lower": null}
{"paper_id": 252873467, "title": "Mass-Editing Memory in a Transformer", "author_names": ["Kevin Meng", "Arnab Sen Sharma", "A. Andonian", "Yonatan Belinkov", "David Bau"], "venue": "International Conference on Learning Representations", "abstract": "Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https://memit.baulab.info.", "year": 2022, "publicationdate": "2022-10-13", "externalids": {"DOI": "10.48550/arXiv.2210.07229"}, "doi_lower": "10.48550/arxiv.2210.07229"}
{"paper_id": 18546295, "title": "Alignment in the Processing of Metaphor", "author_names": ["Дедсе Сеп"], "venue": "", "abstract": null, "year": 2004, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 9796041, "title": "Formal Verification of Ethical Properties in Multiagent Systems", "author_names": ["B. Mermet", "G. Simon"], "venue": "EDIA@ECAI", "abstract": null, "year": 2016, "publicationdate": "2016-08-30", "externalids": {}, "doi_lower": null}
{"paper_id": 145623983, "title": "Motivational bases of choice in experimental games", "author_names": ["D. Messick", "C. McClintock"], "venue": "", "abstract": null, "year": 1968, "publicationdate": null, "externalids": {"DOI": "10.1016/0022-1031(68)90046-2"}, "doi_lower": "10.1016/0022-1031(68)90046-2"}
{"paper_id": 264506629, "title": "It is Time to Introduce the Next Generation of Chemists to FAIR and Open Science", "author_names": ["T. Bartels-Rausch", "M. Ammann"], "venue": "Chimia (Basel)", "abstract": "Early career scientists are confronted with increasing expectations in sharing scholarly data openly and fair by funding agencies. This manuscript is an appeal to introduce open data management strategies to graduate students in atmospheric chemistry.", "year": 2023, "publicationdate": "2023-10-25", "externalids": {"DOI": "10.2533/chimia.2023.694"}, "doi_lower": "10.2533/chimia.2023.694"}
{"paper_id": 251903295, "title": "What Do NLP Researchers Believe? Results of the NLP Community Metasurvey", "author_names": ["Julian Michael", "Ari Holtzman", "Alicia Parrish", "Aaron Mueller", "Alex Wang", "Angelica Chen", "Divyam Madaan", "Nikita Nangia", "Richard Yuanzhe Pang", "Jason Phang", "Sam Bowman"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present the results of the NLP Community Metasurvey. Run from May to June 2022, it elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split in half on the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In addition, the survey posed meta-questions, asking respondents to predict the distribution of survey responses. This allows us to uncover false sociological beliefs where the community’s predictions don’t match reality. Among other results, we find that the community greatly overestimates its own belief in the usefulness of benchmarks and the potential for scaling to solve real-world problems, while underestimating its belief in the importance of linguistic structure, inductive bias, and interdisciplinary science.", "year": 2022, "publicationdate": "2022-08-26", "externalids": {"DOI": "10.48550/arXiv.2208.12852"}, "doi_lower": "10.48550/arxiv.2208.12852"}
{"paper_id": 36024272, "title": "Explanation in Artificial Intelligence: Insights from the Social Sciences", "author_names": ["Tim Miller"], "venue": "Artificial Intelligence", "abstract": null, "year": 2017, "publicationdate": "2017-06-22", "externalids": {"DOI": "10.1016/J.ARTINT.2018.07.007"}, "doi_lower": "10.1016/j.artint.2018.07.007"}
{"paper_id": 240420063, "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey", "author_names": ["Bonan Min", "Hayley Ross", "Elior Sulem", "Amir Pouran Ben Veyseh", "Thien Huu Nguyen", "Oscar Sainz", "Eneko Agirre", "Ilana Heinz", "D. Roth"], "venue": "ACM Computing Surveys", "abstract": "Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.", "year": 2021, "publicationdate": "2021-11-01", "externalids": {"DOI": "10.1145/3605943"}, "doi_lower": "10.1145/3605943"}
{"paper_id": 264555202, "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory", "author_names": ["Niloofar Mireshghallah", "Hyunwoo Kim", "Xuhui Zhou", "Yulia Tsvetkov", "Maarten Sap", "Reza Shokri", "Yejin Choi"], "venue": "International Conference on Learning Representations", "abstract": "The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.", "year": 2023, "publicationdate": "2023-10-27", "externalids": {"DOI": "10.48550/arXiv.2310.17884"}, "doi_lower": "10.48550/arxiv.2310.17884"}
{"paper_id": 235683109, "title": "The Threat of Offensive AI to Organizations", "author_names": ["Yisroel Mirsky", "Ambra Demontis", "J. Kotak", "Ram Shankar", "Deng Gelei", "Liu Yang", "X. Zhang", "Wenke Lee", "Y. Elovici", "B. Biggio"], "venue": "Computers & security", "abstract": null, "year": 2021, "publicationdate": "2021-06-30", "externalids": {"DOI": "10.1016/j.cose.2022.103006"}, "doi_lower": "10.1016/j.cose.2022.103006"}
{"paper_id": 205242740, "title": "Human-level control through deep reinforcement learning", "author_names": ["Volodymyr Mnih", "K. Kavukcuoglu", "David Silver", "Andrei A. Rusu", "J. Veness", "Marc G. Bellemare", "Alex Graves", "Martin A. Riedmiller", "A. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charlie Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature", "abstract": null, "year": 2015, "publicationdate": "2015-02-25", "externalids": {"DOI": "10.1038/nature14236"}, "doi_lower": "10.1038/nature14236"}
{"paper_id": 249889734, "title": "A survey on model-based reinforcement learning", "author_names": ["Fan Luo", "Tian Xu", "Hang Lai", "Xiong-Hui Chen", "Weinan Zhang", "Yang Yu"], "venue": "Science China Information Sciences", "abstract": "Reinforcement learning (RL) interacts with the environment to solve sequential decision-making problems via a trial-and-error approach. Errors are always undesirable in real-world applications, even though RL excels at playing complex video games that permit several trial-and-error attempts. To improve sample efficiency and thus reduce errors, model-based reinforcement learning (MBRL) is believed to be a promising direction, as it constructs environment models in which trial-and-errors can occur without incurring actual costs. In this survey, we investigate MBRL with a particular focus on the recent advancements in deep RL. There is a generalization error between the learned model of a non-tabular environment and the actual environment. Consequently, it is crucial to analyze the disparity between policy training in the environment model and that in the actual environment, guiding algorithm design for improved model learning, model utilization, and policy training. In addition, we discuss the recent developments of model-based techniques in other forms of RL, such as offline RL, goal-conditioned RL, multi-agent RL, and meta-RL. Furthermore, we discuss the applicability and benefits of MBRL for real-world tasks. Finally, this survey concludes with a discussion of the promising future development prospects for MBRL. We believe that MBRL has great unrealized potential and benefits in real-world applications, and we hope this survey will encourage additional research on MBRL.", "year": 2022, "publicationdate": "2022-06-19", "externalids": {"DOI": "10.1007/s11432-022-3696-5"}, "doi_lower": "10.1007/s11432-022-3696-5"}
{"paper_id": 208910731, "title": "A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems", "author_names": ["Sina Mohseni", "Niloofar Zarei", "E. Ragan"], "venue": "ACM Trans. Interact. Intell. Syst.", "abstract": "The need for interpretable and accountable intelligent systems grows along with the prevalence of\n artificial intelligence\n (\n AI\n ) applications used in everyday life.\n Explainable AI\n (\n XAI\n ) systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of XAI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this article presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation methods. Our categorization presents the mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.", "year": 2018, "publicationdate": "2018-11-28", "externalids": {"DOI": "10.1145/3387166"}, "doi_lower": "10.1145/3387166"}
{"paper_id": 256901111, "title": "Auditing large language models: a three-layered approach", "author_names": ["Jakob Mokander", "Jonas Schuett", "Hannah Rose Kirk", "Luciano Floridi"], "venue": "AI and Ethics", "abstract": "Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks posed by LLMs. However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLMs at all. Ultimately, this article seeks to expand the methodological toolkit available to technology providers and policymakers who wish to analyse and evaluate LLMs from technical, ethical, and legal perspectives.", "year": 2023, "publicationdate": "2023-02-16", "externalids": {"DOI": "10.1007/s43681-023-00289-2"}, "doi_lower": "10.1007/s43681-023-00289-2"}
{"paper_id": 3292528, "title": "On the importance of single directions for generalization", "author_names": ["Ari S. Morcos", "D. Barrett", "Neil C. Rabinowitz", "M. Botvinick"], "venue": "International Conference on Learning Representations", "abstract": "Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.", "year": 2018, "publicationdate": "2018-02-15", "externalids": {}, "doi_lower": null}
{"paper_id": 267064254, "title": "Stable Diffusion With Generative AI", "author_names": ["K. S. Priya", "Aanchal Nayak", "B. Chandana", "Sriman Malyala", "Madhuri Ballari", "Asst Prof"], "venue": "International Research Journal of Modernization in Engineering Technology and Science", "abstract": "The fusion of stable diffusion techniques with generative artificial intelligence (AI) has revolutionized the realm of text-to-image generation. This innovative approach leverages the power of deep learning to generate highly realistic images from textual descriptions. In this paper, we explore the synergy between stable diffusion processes and text-to-image generators, presenting a novel methodology that enhances the quality and stability of image synthesis.", "year": 2024, "publicationdate": "2024-01-20", "externalids": {"DOI": "10.56726/irjmets48608"}, "doi_lower": "10.56726/irjmets48608"}
{"paper_id": 253080894, "title": "Equivariant Networks for Zero-Shot Coordination", "author_names": ["Darius Muglich", "C. S. D. Witt", "Elise van der Pol", "Shimon Whiteson", "J. Foerster"], "venue": "Neural Information Processing Systems", "abstract": "Successful coordination in Dec-POMDPs requires agents to adopt robust strategies and interpretable styles of play for their partner. A common failure mode is symmetry breaking, when agents arbitrarily converge on one out of many equivalent but mutually incompatible policies. Commonly these examples include partial observability, e.g. waving your right hand vs. left hand to convey a covert message. In this paper, we present a novel equivariant network architecture for use in Dec-POMDPs that effectively leverages environmental symmetry for improving zero-shot coordination, doing so more effectively than prior methods. Our method also acts as a ``coordination-improvement operator'' for generic, pre-trained policies, and thus may be applied at test-time in conjunction with any self-play algorithm. We provide theoretical guarantees of our work and test on the AI benchmark task of Hanabi, where we demonstrate our methods outperforming other symmetry-aware baselines in zero-shot coordination, as well as able to improve the coordination ability of a variety of pre-trained policies. In particular, we show our method can be used to improve on the state of the art for zero-shot coordination on the Hanabi benchmark.", "year": 2022, "publicationdate": "2022-10-21", "externalids": {"DOI": "10.48550/arXiv.2210.12124"}, "doi_lower": "10.48550/arxiv.2210.12124"}
{"paper_id": 246437809, "title": "Situational Awareness: Techniques, Challenges, and Prospects", "author_names": ["Arslan Munir", "Alexander J. Aved", "Erik Blasch"], "venue": "Applied Informatics", "abstract": "Situational awareness (SA) is defined as the perception of entities in the environment, comprehension of their meaning, and projection of their status in near future. From an Air Force perspective, SA refers to the capability to comprehend and project the current and future disposition of red and blue aircraft and surface threats within an airspace. In this article, we propose a model for SA and dynamic decision-making that incorporates artificial intelligence and dynamic data-driven application systems to adapt measurements and resources in accordance with changing situations. We discuss measurement of SA and the challenges associated with quantification of SA. We then elaborate a plethora of techniques and technologies that help improve SA ranging from different modes of intelligence gathering to artificial intelligence to automated vision systems. We then present different application domains of SA including battlefield, gray zone warfare, military- and air-base, homeland security and defense, and critical infrastructure. Finally, we conclude the article with insights into the future of SA.", "year": 2022, "publicationdate": "2022-01-29", "externalids": {"DOI": "10.3390/ai3010005"}, "doi_lower": "10.3390/ai3010005"}
{"paper_id": 196179248, "title": "Advanced Probabilistic Machine Learning", "author_names": ["A. Folkesson"], "venue": "", "abstract": null, "year": 2019, "publicationdate": "2019-08-29", "externalids": {}, "doi_lower": null}
{"paper_id": 59395172, "title": "Social value orientation: An analysis of measurement, form, predictive power, and malleability of social preferences", "author_names": ["K. Ackermann"], "venue": "", "abstract": null, "year": 2014, "publicationdate": "2014-11-11", "externalids": {"DOI": "10.3929/ETHZ-A-010273775"}, "doi_lower": "10.3929/ethz-a-010273775"}
{"paper_id": 632176, "title": "Measuring Social Value Orientation", "author_names": ["R. O. Murphy", "K. Ackermann", "Michel J. J. Handgraaf"], "venue": "Social Science Research Network", "abstract": "Narrow self-interest is often used as a simplifying assumption when studying people making decisions in social contexts. Nonetheless, people exhibit a wide range of different motivations when choosing unilaterally among interdependent outcomes. Measuring the magnitude of the concern people have for others, sometimes called Social Value Orientation (SVO), has been an interest of many social scientists for decades and several different measurement methods have been developed so far. Here we introduce a new measure of SVO that has several advantages over existent methods. A detailed description of the new measurement method is presented, along with norming data that provide evidence of its solid psychometric properties. We conclude with a brief discussion of the research streams that would benefit from a more sensitive and higher resolution measure of SVO, and extend an invitation to others to use this new measure which is freely available.", "year": 2011, "publicationdate": "2011-12-01", "externalids": {"DOI": "10.2139/SSRN.1804189"}, "doi_lower": "10.2139/ssrn.1804189"}
{"paper_id": 215828184, "title": "StereoSet: Measuring stereotypical bias in pretrained language models", "author_names": ["Moin Nadeem", "Anna Bethke", "Siva Reddy"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.", "year": 2020, "publicationdate": "2020-04-20", "externalids": {"DOI": "10.18653/v1/2021.acl-long.416"}, "doi_lower": "10.18653/v1/2021.acl-long.416"}
{"paper_id": 245329531, "title": "WebGPT: Browser-assisted question-answering with human feedback", "author_names": ["Reiichiro Nakano", "Jacob Hilton", "S. Balaji", "Jeff Wu", "Ouyang Long", "Christina Kim", "Christopher Hesse", "Shantanu Jain", "Vineet Kosaraju", "W. Saunders", "Xu Jiang", "K. Cobbe", "Tyna Eloundou", "Gretchen Krueger", "Kevin Button", "Matthew Knight", "Benjamin Chess", "John Schulman"], "venue": "arXiv.org", "abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.", "year": 2021, "publicationdate": "2021-12-17", "externalids": {}, "doi_lower": null}
{"paper_id": 141998759, "title": "\"I am not what I am, \"Alienation in Othello", "author_names": ["Joseph Z. Romero"], "venue": "", "abstract": null, "year": 1988, "publicationdate": "1988-03-20", "externalids": {}, "doi_lower": null}
{"paper_id": 255749430, "title": "Progress measures for grokking via mechanistic interpretability", "author_names": ["Neel Nanda", "Lawrence Chan", "Tom Lieberum", "Jess Smith", "J. Steinhardt"], "venue": "International Conference on Learning Representations", "abstract": "Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \\textit{progress measures} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.", "year": 2023, "publicationdate": "2023-01-12", "externalids": {"DOI": "10.48550/arXiv.2301.05217"}, "doi_lower": "10.48550/arxiv.2301.05217"}
{"paper_id": 222090785, "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models", "author_names": ["Nikita Nangia", "Clara Vania", "Rasika Bhalerao", "Samuel R. Bowman"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.", "year": 2020, "publicationdate": "2020-09-30", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.154"}, "doi_lower": "10.18653/v1/2020.emnlp-main.154"}
{"paper_id": 6963075, "title": "Decentralized Stochastic Control with Partial History Sharing: A Common Information Approach", "author_names": ["A. Nayyar", "Aditya Mahajan", "D. Teneketzis"], "venue": "IEEE Transactions on Automatic Control", "abstract": "A general model of decentralized stochastic control called partial history sharing information structure is presented. In this model, at each step the controllers share part of their observation and control history with each other. This general model subsumes several existing models of information sharing as special cases. Based on the information commonly known to all the controllers, the decentralized problem is reformulated as an equivalent centralized problem from the perspective of a coordinator. The coordinator knows the common information and selects prescriptions that map each controller's local information to its control actions. The optimal control problem at the coordinator is shown to be a partially observable Markov decision process (POMDP) which is solved using techniques from Markov decision theory. This approach provides 1) structural results for optimal strategies and 2) a dynamic program for obtaining optimal strategies for all controllers in the original decentralized problem. Thus, this approach unifies the various ad-hoc approaches taken in the literature. In addition, the structural results on optimal control strategies obtained by the proposed approach cannot be obtained by the existing generic approach (the person-by-person approach) for obtaining structural results in decentralized problems; and the dynamic program obtained by the proposed approach is simpler than that obtained by the existing generic approach (the designer's approach) for obtaining dynamic programs in decentralized problems.", "year": 2012, "publicationdate": "2012-09-07", "externalids": {"DOI": "10.1109/TAC.2013.2239000"}, "doi_lower": "10.1109/tac.2013.2239000"}
{"paper_id": 5730166, "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping", "author_names": ["A. Ng", "Daishi Harada", "Stuart J. Russell"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 1999, "publicationdate": "1999-06-27", "externalids": {}, "doi_lower": null}
{"paper_id": 266239314, "title": "Algorithms for Inverse Reinforcement Learning", "author_names": ["Andrew Y. Ng", "Stuart Russell"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 2000, "publicationdate": "2000-06-29", "externalids": {"DOI": "10.2460/AJVR.67.2.323"}, "doi_lower": "10.2460/ajvr.67.2.323"}
{"paper_id": 221720284, "title": "Promising room temperature thermoelectric conversion efficiency of zinc-blende AgI from first principles", "author_names": ["P. Bulut", "Berna Beceren", "S. Yıldırım", "C. Sevik", "T. Gürel"], "venue": "Journal of Physics: Condensed Matter", "abstract": "The theoretical investigation on structural, vibrational, and electronic properties of zinc-blende (ZB) AgI were carried out employing first principles density functional theory calculations. Thermoelectric properties then were predicted through semi-classical Boltzmann transport equations within the constant relaxation time approximation. Equilibrium lattice parameter, bulk modulus, elastic constants, and vibrational properties were calculated by using generalized gradient approximation. Calculated properties are in good agreement with available experimental values. Electronic and thermoelectric properties were investigated both with and without considering spin–orbit coupling (SOC) effect which is found to have a strong influence on p-type Seebeck coefficient as well as the power factor of the ZB–AgI. By inclusion of SOC, a reduction of the band-gap and p-type Seebeck coefficients as well as the power factor was found which is the indication of that spin–orbit interaction cannot be ignored for p-type thermoelectric properties of the ZB–AgI. By using deformation potential theory for electronic relaxation time and experimentally predicted lattice thermal conductivity, we obtained a ZT value 1.69 (0.89) at 400 K for n-type (p-type) carrier concentration of 1.5 × 1018 (4.6 ×1019) cm−3 that makes ZB–AgI as a promising room temperature thermoelectric material.", "year": 2020, "publicationdate": "2020-09-14", "externalids": {"DOI": "10.1088/1361-648X/abb867"}, "doi_lower": "10.1088/1361-648x/abb867"}
{"paper_id": 28706636, "title": "Resolving the M-cell debate: why and how.", "author_names": ["S. Nattel", "C. Antzelevitch", "D. Noble"], "venue": "Heart Rhythm", "abstract": null, "year": 2011, "publicationdate": "2011-08-01", "externalids": {"DOI": "10.1016/j.hrthm.2011.06.002"}, "doi_lower": "10.1016/j.hrthm.2011.06.002"}
{"paper_id": 251979524, "title": "The alignment problem from a deep learning perspective", "author_names": ["Richard Ngo"], "venue": "International Conference on Learning Representations", "abstract": "In coming years or decades, artificial general intelligence (AGI) may surpass human capabilities across many critical domains. We argue that, without substantial effort to prevent it, AGIs could learn to pursue goals that are in conflict (i.e. misaligned) with human interests. If trained like today's most capable models, AGIs could learn to act deceptively to receive higher reward, learn misaligned internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. In this revised paper, we include more direct empirical evidence published as of early 2025. AGIs with these properties would be difficult to align and may appear aligned even when they are not. Finally, we briefly outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and we review research directions aimed at preventing this outcome.", "year": 2022, "publicationdate": "2022-08-30", "externalids": {"DOI": "10.48550/arXiv.2209.00626"}, "doi_lower": "10.48550/arxiv.2209.00626"}
{"paper_id": 206592585, "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author_names": ["Anh Totti Nguyen", "J. Yosinski", "J. Clune"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study [30] revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call “fooling images” (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.", "year": 2014, "publicationdate": "2014-12-04", "externalids": {"DOI": "10.1109/CVPR.2015.7298640"}, "doi_lower": "10.1109/cvpr.2015.7298640"}
{"paper_id": 5970910, "title": "Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks", "author_names": ["Anh Totti Nguyen", "J. Yosinski", "J. Clune"], "venue": "arXiv.org", "abstract": "We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.", "year": 2016, "publicationdate": "2016-02-11", "externalids": {}, "doi_lower": null}
{"paper_id": 116270947, "title": "A Literature Review and Research Agenda for Understanding and Addressing Pedestrian and Bicycle Safety", "author_names": ["Bruce Appleyard"], "venue": "", "abstract": null, "year": 2017, "publicationdate": "2017-06-01", "externalids": {"DOI": "10.1016/J.JTH.2017.05.237"}, "doi_lower": "10.1016/j.jth.2017.05.237"}
{"paper_id": 226290142, "title": "f-IRL: Inverse Reinforcement Learning via State Marginal Matching", "author_names": ["Tianwei Ni", "Harshit S. Sikchi", "Yufei Wang", "Tejus Gupta", "Lisa Lee", "Benjamin Eysenbach"], "venue": "Conference on Robot Learning", "abstract": "Imitation learning is well-suited for robotic tasks where it is difficult to directly program the behavior or specify a cost for optimal control. In this work, we propose a method for learning the reward function (and the corresponding policy) to match the expert state density. Our main result is the analytic gradient of any f-divergence between the agent and expert state distribution w.r.t. reward parameters. Based on the derived gradient, we present an algorithm, f-IRL, that recovers a stationary reward function from the expert density by gradient descent. We show that f-IRL can learn behaviors from a hand-designed target state density or implicitly through expert observations. Our method outperforms adversarial imitation learning methods in terms of sample efficiency and the required number of expert trajectories on IRL benchmarks. Moreover, we show that the recovered reward function can be used to quickly solve downstream tasks, and empirically demonstrate its utility on hard-to-explore tasks and for behavior transfer across changes in dynamics.", "year": 2020, "publicationdate": "2020-11-09", "externalids": {}, "doi_lower": null}
{"paper_id": 162203205, "title": "The black swan: the impact of the highly improbable", "author_names": ["E. Gilder"], "venue": "", "abstract": null, "year": 2012, "publicationdate": "2012-06-01", "externalids": {"DOI": "10.1080/21568235.2012.729329"}, "doi_lower": "10.1080/21568235.2012.729329"}
{"paper_id": 239757161, "title": "Algorithms of oppression?", "author_names": ["J. Akintayo"], "venue": "Routledge Companion to Global Cyber-Security Strategy", "abstract": null, "year": 2020, "publicationdate": "2020-12-03", "externalids": {"DOI": "10.4324/9780429399718-51"}, "doi_lower": "10.4324/9780429399718-51"}
{"paper_id": 159254, "title": "Transforming the Curriculum Through the Intergenerational Lens", "author_names": ["Mildred C. Joyner", "Eli DeHope"], "venue": "Journal of gerontological social work", "abstract": null, "year": 2006, "publicationdate": "2006-12-20", "externalids": {"DOI": "10.1300/J083v48n01_09"}, "doi_lower": "10.1300/j083v48n01_09"}
{"paper_id": 149805, "title": "A Voting-Based System for Ethical Decision Making", "author_names": ["Ritesh Noothigattu", "Snehalkumar `Neil' Gaikwad", "E. Awad", "Sohan Dsouza", "Iyad Rahwan", "Pradeep Ravikumar", "Ariel D. Procaccia"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "We present a general approach to automating ethical decisions, drawing on machine learning and computational social choice. In a nutshell, we propose to learn a model of societal preferences, and, when faced with a specific ethical dilemma at runtime, efficiently aggregate those preferences to identify a desirable choice. We provide a concrete algorithm that instantiates our approach; some of its crucial steps are informed by a new theory of swap-dominance efficient voting rules. Finally, we implement and evaluate a system for ethical decision making in the autonomous vehicle domain, using preference data collected from 1.3 million people through the Moral Machine website.", "year": 2017, "publicationdate": "2017-09-01", "externalids": {"DOI": "10.1609/aaai.v32i1.11512"}, "doi_lower": "10.1609/aaai.v32i1.11512"}
{"paper_id": 211129042, "title": "Bias in data‐driven artificial intelligence systems—An introductory survey", "author_names": ["Eirini Ntoutsi", "P. Fafalios", "U. Gadiraju", "Vasileios Iosifidis", "W. Nejdl", "Maria-Esther Vidal", "S. Ruggieri", "F. Turini", "S. Papadopoulos", "Emmanouil Krasanakis", "I. Kompatsiaris", "K. Kinder-Kurlanda", "Claudia Wagner", "F. Karimi", "Miriam Fernández", "Harith Alani", "Bettina Berendt", "Tina Kruegel", "C. Heinze", "Klaus Broelemann", "Gjergji Kasneci", "T. Tiropanis", "Steffen Staab"], "venue": "WIREs Data Mining Knowl. Discov.", "abstract": "Artificial Intelligence (AI)‐based systems are widely employed nowadays to make decisions that have far‐reaching impact on individuals and society. Their decisions might affect everyone, everywhere, and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training, and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multidisciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well‐grounded in a legal frame. In this survey, we focus on data‐driven AI, as a large part of AI is powered nowadays by (big) data and powerful machine learning algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features such as race, sex, and so forth.", "year": 2020, "publicationdate": "2020-02-03", "externalids": {"DOI": "10.1002/widm.1356"}, "doi_lower": "10.1002/widm.1356"}
{"paper_id": 160674049, "title": "The nature of AI principles", "author_names": ["A. Bundy", "S. Ohlsson"], "venue": "", "abstract": null, "year": 1990, "publicationdate": "1990-04-01", "externalids": {"DOI": "10.1017/CBO9780511663116.014"}, "doi_lower": "10.1017/cbo9780511663116.014"}
{"paper_id": 71147600, "title": "Approval-directed agency and the decision theory of Newcomb-like problems", "author_names": ["Caspar Oesterheld"], "venue": "Synthese", "abstract": "Decision theorists disagree about how instrumentally rational agents, i.e., agents trying to achieve some goal, should behave in so-called Newcomb-like problems, with the main contenders being causal and evidential decision theory. Since the main goal of artificial intelligence research is to create machines that make instrumentally rational decisions, the disagreement pertains to this field. In addition to the more philosophical question of what the right decision theory is, the goal of AI poses the question of how to implement any given decision theory in an AI. For example, how would one go about building an AI whose behavior matches evidential decision theory’s recommendations? Conversely, we can ask which decision theories (if any) describe the behavior of any existing AI design. In this paper, we study what decision theory an approval-directed agent, i.e., an agent whose goal it is to maximize the score it receives from an overseer, implements. If we assume that the overseer rewards the agent based on the expected value of some von Neumann–Morgenstern utility function, then such an approval-directed agent is guided by two decision theories: the one used by the agent to decide which action to choose in order to maximize the reward and the one used by the overseer to compute the expected utility of a chosen action. We show which of these two decision theories describes the agent’s behavior in which situations.", "year": 2019, "publicationdate": "2019-02-25", "externalids": {"DOI": "10.1007/s11229-019-02148-2"}, "doi_lower": "10.1007/s11229-019-02148-2"}
{"paper_id": 220882341, "title": "Safe Pareto improvements for delegated game playing", "author_names": ["Caspar Oesterheld"], "venue": "Autonomous Agents and Multi-Agent Systems", "abstract": "A set of players delegate playing a game to a set of representatives, one for each player. We imagine that each player trusts their respective representative’s strategic abilities. Thus, we might imagine that per default, the original players would simply instruct the representatives to play the original game as best as they can. In this paper, we ask: are there safe Pareto improvements on this default way of giving instructions? That is, we imagine that the original players can coordinate to tell their representatives to only consider some subset of the available strategies and to assign utilities to outcomes differently than the original players. Then can the original players do this in such a way that the payoff is guaranteed to be weakly higher than under the default instructions for all the original players? In particular, can they Pareto-improve without probabilistic assumptions about how the representatives play games? In this paper, we give some examples of safe Pareto improvements. We prove that the notion of safe Pareto improvements is closely related to a notion of outcome correspondence between games. We also show that under some specific assumptions about how the representatives play games, finding safe Pareto improvements is NP-complete.", "year": 2022, "publicationdate": "2022-08-13", "externalids": {"DOI": "10.1007/s10458-022-09574-6"}, "doi_lower": "10.1007/s10458-022-09574-6"}
{"paper_id": 14383223, "title": "Visualizing the quality of dimensionality reduction", "author_names": ["B. Mokbel", "W. Lueks", "A. Gisbrecht", "B. Hammer"], "venue": "The European Symposium on Artificial Neural Networks", "abstract": null, "year": 2013, "publicationdate": "2013-07-01", "externalids": {"DOI": "10.1016/j.neucom.2012.11.046"}, "doi_lower": "10.1016/j.neucom.2012.11.046"}
{"paper_id": 5755822, "title": "Visualizing and Understanding Deep Texture Representations", "author_names": ["Tsung-Yu Lin", "Subhransu Maji"], "venue": "Computer Vision and Pattern Recognition", "abstract": "A number of recent approaches have used deep convolutional neural networks (CNNs) to build texture representations. Nevertheless, it is still unclear how these models represent texture and invariances to categorical variations. This work conducts a systematic evaluation of recent CNN-based texture descriptors for recognition and attempts to understand the nature of invariances captured by these representations. First we show that the recently proposed bilinear CNN model [25] is an excellent generalpurpose texture descriptor and compares favorably to other CNN-based descriptors on various texture and scene recognition benchmarks. The model is translationally invariant and obtains better accuracy on the ImageNet dataset without requiring spatial jittering of data compared to corresponding models trained with spatial jittering. Based on recent work [13, 28] we propose a technique to visualize pre-images, providing a means for understanding categorical properties that are captured by these representations. Finally, we show preliminary results on how a unified parametric model of texture analysis and synthesis can be used for attribute-based image manipulation, e.g. to make an image more swirly, honeycombed, or knitted. The source code and additional visualizations are available at http://vis-www.cs.umass.edu/texture.", "year": 2015, "publicationdate": "2015-11-16", "externalids": {"DOI": "10.1109/CVPR.2016.305"}, "doi_lower": "10.1109/cvpr.2016.305"}
{"paper_id": 281223943, "title": "Reflections on the Life and Dreams of C. G. Jung (2023)", "author_names": ["Isabelle DeArmond"], "venue": "Psychological Perspectives", "abstract": null, "year": 2025, "publicationdate": "2025-04-03", "externalids": {"DOI": "10.1080/00332925.2025.2507519"}, "doi_lower": "10.1080/00332925.2025.2507519"}
{"paper_id": 215930358, "title": "Zoom In: An Introduction to Circuits", "author_names": ["Christopher Olah", "Nick Cammarata", "Ludwig Schubert", "Gabriel Goh", "Michael Petrov", "Shan Carter"], "venue": "", "abstract": null, "year": 2020, "publicationdate": "2020-03-10", "externalids": {"DOI": "10.23915/distill.00024.001"}, "doi_lower": "10.23915/distill.00024.001"}
{"paper_id": 67440606, "title": "The Building Blocks of Interpretability", "author_names": ["Christopher Olah", "Arvind Satyanarayan", "I. Johnson", "Shan Carter", "Ludwig Schubert", "Katherine Q. Ye", "A. Mordvintsev"], "venue": "", "abstract": null, "year": 2018, "publicationdate": "2018-03-06", "externalids": {"DOI": "10.23915/DISTILL.00010"}, "doi_lower": "10.23915/distill.00010"}
{"paper_id": 99084150, "title": "George A. Olah (1927–2017)", "author_names": ["I. Hargittai", "B. Hargittai"], "venue": "Structural Chemistry", "abstract": null, "year": 2017, "publicationdate": "2017-04-10", "externalids": {"DOI": "10.1007/s11224-017-0945-8"}, "doi_lower": "10.1007/s11224-017-0945-8"}
{"paper_id": 252532078, "title": "In-context Learning and Induction Heads", "author_names": ["Catherine Olsson", "Nelson Elhage", "Neel Nanda", "Nicholas Joseph", "Nova Dassarma", "T. Henighan", "Benjamin Mann", "Amanda Askell", "Yuntao Bai", "Anna Chen", "Tom Conerly", "Dawn Drain", "Deep Ganguli", "Zac Hatfield-Dodds", "Danny Hernandez", "Scott Johnston", "Andy Jones", "John Kernion", "Liane Lovitt", "Kamal Ndousse", "Dario Amodei", "Tom B. Brown", "Jack Clark", "Jared Kaplan", "Sam McCandlish", "Chris Olah"], "venue": "arXiv.org", "abstract": "\"Induction heads\"are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] ->[B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all\"in-context learning\"in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence.", "year": 2022, "publicationdate": "2022-09-24", "externalids": {"DOI": "10.48550/arXiv.2209.11895"}, "doi_lower": "10.48550/arxiv.2209.11895"}
{"paper_id": 226687936, "title": "The Basic AI Drives", "author_names": ["E. Grishin"], "venue": "Artificial Societies", "abstract": null, "year": 2020, "publicationdate": null, "externalids": {"DOI": "10.18254/s207751800009748-1"}, "doi_lower": "10.18254/s207751800009748-1"}
{"paper_id": 242311298, "title": "Curve Detectors", "author_names": ["Nick Cammarata", "Gabriel Goh", "Shan Carter", "Ludwig Schubert", "Michael Petrov", "Christopher Olah"], "venue": "Distill", "abstract": null, "year": 2020, "publicationdate": "2020-06-17", "externalids": {"DOI": "10.23915/distill.00024.003"}, "doi_lower": "10.23915/distill.00024.003"}
{"paper_id": 243426562, "title": "Weight Banding", "author_names": ["Michael Petrov", "Chelsea Voss", "Ludwig Schubert", "Nick Cammarata", "Gabriel Goh", "Christopher Olah"], "venue": "Distill", "abstract": null, "year": null, "publicationdate": null, "externalids": {"DOI": "10.23915/distill.00024.009"}, "doi_lower": "10.23915/distill.00024.009"}
{"paper_id": 257532815, "title": "GPT-4 Technical Report", "author_names": ["OpenAI Josh Achiam", "Steven Adler", "Sandhini Agarwal", "Lama Ahmad", "Ilge Akkaya", "Florencia Leoni Aleman", "Diogo Almeida", "Janko Altenschmidt", "Sam Altman", "Shyamal Anadkat", "Red Avila", "Igor Babuschkin", "S. Balaji", "Valerie Balcom", "Paul Baltescu", "Haim-ing Bao", "Mo Bavarian", "J. Belgum", "Irwan Bello", "Jake Berdine", "Gabriel Bernadett-Shapiro", "Christopher Berner", "Lenny Bogdonoff", "Oleg Boiko", "Made-laine Boyd", "Anna-Luisa Brakman", "Greg Brockman", "Tim Brooks", "Miles Brundage", "Kevin Button", "Trevor Cai", "Rosie Campbell", "Andrew Cann", "Brittany Carey", "Chelsea Carlson", "Rory Carmichael", "Brooke Chan", "Che Chang", "Fotis Chantzis", "Derek Chen", "Sully Chen", "Ruby Chen", "Jason Chen", "Mark Chen", "Benjamin Chess", "Chester Cho", "Casey Chu", "Hyung Won Chung", "Dave Cummings", "Jeremiah Currier", "Yunxing Dai", "Cory Decareaux", "Thomas Degry", "Noah Deutsch", "Damien Deville", "Arka Dhar", "David Dohan", "Steve Dowling", "Sheila Dunning", "Adrien Ecoffet", "Atty Eleti", "Tyna Eloundou", "David Farhi", "L. Fedus", "Niko Felix", "Sim'on Posada Fishman", "Juston Forte", "Is-abella Fulford", "Leo Gao", "Elie Georges", "C. Gibson", "Vik Goel", "Tarun Gogineni", "Gabriel Goh", "Raphael Gontijo-Lopes", "Jonathan Gordon", "Morgan Grafstein", "Scott Gray", "Ryan Greene", "Joshua Gross", "S. Gu", "Yufei Guo", "Chris Hallacy", "Jesse Han", "Jeff Harris", "Yuchen He", "Mike Heaton", "Johannes Heidecke", "Chris Hesse", "Alan Hickey", "W. Hickey", "Peter Hoeschele", "Brandon Houghton", "Kenny Hsu", "Shengli Hu", "Xin Hu", "Joost Huizinga", "Shantanu Jain", "Shawn Jain", "Joanne Jang", "Angela Jiang", "Roger Jiang", "Haozhun Jin", "Denny Jin", "Shino Jomoto", "B. Jonn", "Heewoo Jun", "Tomer Kaftan", "Lukasz Kaiser", "Ali Kamali", "I. Kanitscheider", "N. Keskar", "Tabarak Khan", "Logan Kilpatrick", "Jong Wook Kim", "Christina Kim", "Yongjik Kim", "Hendrik Kirchner", "J. Kiros", "Matthew Knight", "Daniel Kokotajlo", "Lukasz Kondraciuk", "Andrew Kondrich", "Aris Konstantinidis", "Kyle Kosic", "Gretchen Krueger", "Vishal Kuo", "Michael Lampe", "Ikai Lan", "Teddy Lee", "Jan Leike", "Jade Leung", "Daniel Levy", "Chak Li", "Rachel Lim", "Molly Lin", "Stephanie Lin", "Ma-teusz Litwin", "Theresa Lopez", "Ryan Lowe", "Patricia Lue", "A. Makanju", "Kim Malfacini", "Sam Manning", "Todor Markov", "Yaniv Markovski", "Bianca Martin", "Katie Mayer", "Andrew Mayne", "Bob McGrew", "S. McKinney", "Christine McLeavey", "Paul McMillan", "Jake McNeil", "David Medina", "Aalok Mehta", "Jacob Menick", "Luke Metz", "An-drey Mishchenko", "Pamela Mishkin", "Vinnie Monaco", "Evan Morikawa", "Daniel P. Mossing", "Tong Mu", "Mira Murati", "O. Murk", "David M'ely", "Ashvin Nair", "Reiichiro Nakano", "Rajeev Nayak", "Arvind Neelakantan", "Richard Ngo", "Hyeonwoo Noh", "Ouyang Long", "Cullen O'Keefe", "J. Pachocki", "A. Paino", "Joe Palermo", "Ashley Pantuliano", "Giambattista Parascandolo", "J. Parish", "Emy Parparita", "Alexandre Passos", "Mikhail Pavlov", "Andrew Peng", "Adam Perelman", "Filipe de Avila Belbute Peres", "Michael Petrov", "Henrique Pondé de Oliveira Pinto", "Michael Pokorny", "Michelle Pokrass", "Vitchyr H. Pong", "Tolly Powell", "Alethea Power", "Boris Power", "Elizabeth Proehl", "Raul Puri", "Alec Radford", "Jack W. Rae", "Aditya Ramesh", "Cameron Raymond", "Francis Real", "Kendra Rimbach", "Carl Ross", "Bob Rotsted", "Henri Roussez", "N. Ryder", "M. Saltarelli", "Ted Sanders", "Shibani Santurkar", "Girish Sastry", "Heather Schmidt", "David Schnurr", "John Schulman", "Daniel Selsam", "Kyla Sheppard", "Toki Sherbakov", "Jessica Shieh", "Sarah Shoker", "Pranav Shyam", "Szymon Sidor", "Eric Sigler", "Maddie Simens", "Jordan Sitkin", "Katarina Slama", "Ian Sohl", "Benjamin Sokolowsky", "Yang Song", "Natalie Staudacher", "F. Such", "Natalie Summers", "I. Sutskever", "Jie Tang", "N. Tezak", "Madeleine Thompson", "P. Tillet", "Amin Tootoonchian", "Elizabeth Tseng", "Preston Tuggle", "Nick Turley", "Jerry Tworek", "Juan Felipe Cer'on Uribe", "Andrea Vallone", "Arun Vijayvergiya", "Chelsea Voss", "Carroll L. Wainwright", "Justin Jay Wang", "Alvin Wang", "Ben Wang", "Jonathan Ward", "Jason Wei", "CJ Weinmann", "Akila Welihinda", "Peter Welinder", "Jiayi Weng", "Lilian Weng", "Matt Wiethoff", "Dave Willner", "Clemens Winter", "Samuel Wolrich", "Hannah Wong", "Lauren Workman", "Sherwin Wu", "Jeff Wu", "Michael Wu", "Kai Xiao", "Tao Xu", "Sarah Yoo", "Kevin Yu", "Qim-ing Yuan", "Wojciech Zaremba", "Rowan Zellers", "Chong Zhang", "Marvin Zhang", "Shengjia Zhao", "Tianhao Zheng", "Juntang Zhuang", "William Zhuk", "Barret Zoph"], "venue": "", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {}, "doi_lower": null}
{"paper_id": 263218031, "title": "GPT-4V(ision) System Card", "author_names": [], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 280709982, "title": "Contemplative Superalignment", "author_names": ["Ruben Laukkonen", "Fionn Inglis", "Shamil Chandaria", "L. Sandved-Smith", "Edmundo Lopez-Sola", "Jakob Hohwy", "Jonathan Gold", "Adam Elwood"], "venue": "Artificial General Intelligence", "abstract": null, "year": 2025, "publicationdate": null, "externalids": {"DOI": "10.1007/978-3-032-00686-8_31"}, "doi_lower": "10.1007/978-3-032-00686-8_31"}
{"paper_id": 167060362, "title": "Bridging the digital divide in developing countries: The role of mobile technology in bridging the digital divide", "author_names": ["Tuomas Tanskanen"], "venue": "", "abstract": null, "year": 2014, "publicationdate": "2014-06-04", "externalids": {}, "doi_lower": null}
{"paper_id": 199543559, "title": "A review of cooperative multi-agent deep reinforcement learning", "author_names": ["Afshin Oroojlooyjadid", "Davood Hajinezhad"], "venue": "Applied intelligence (Boston)", "abstract": "Deep Reinforcement Learning has made significant progress in multi-agent systems in recent years. The aim of this review article is to provide an overview of recent approaches on Multi-Agent Reinforcement Learning (MARL) algorithms. Our classification of MARL approaches includes five categories for modeling and solving cooperative multi-agent reinforcement learning problems: (I) independent learners, (II) fully observable critics, (III) value function factorization, (IV) consensus, and (IV) learn to communicate. We first discuss each of these methods, their potential challenges, and how these challenges were mitigated in the relevant papers. Additionally, we make connections among different papers in each category if applicable. Next, we cover some new emerging research areas in MARL along with the relevant recent papers. In light of MARL’s recent success in real-world applications, we have dedicated a section to reviewing these applications and articles. This survey also provides a list of available environments for MARL research. Finally, the paper is concluded with proposals on possible research directions.", "year": 2019, "publicationdate": "2019-08-11", "externalids": {"DOI": "10.1007/s10489-022-04105-y"}, "doi_lower": "10.1007/s10489-022-04105-y"}
{"paper_id": 53670210, "title": "An Algorithmic Perspective on Imitation Learning", "author_names": ["Takayuki Osa", "J. Pajarinen", "G. Neumann", "J. Bagnell", "P. Abbeel", "Jan Peters"], "venue": "Found. Trends Robotics", "abstract": "As robots and other intelligent agents move from simple environments and problems to more complex, unstructured settings, manually programming their behavior has become increasingly challenging and expensive. Often, it is easier for a teacher to demonstrate a desired behavior rather than attempt to manually engineer it. This process of learning from demonstrations, and the study of algorithms to do so, is called imitation learning. This work provides an introduction to imitation learning. It covers the underlying assumptions, approaches, and how they relate; the rich set of algorithms developed to tackle the problem; and advice on effective tools and implementation. We intend this paper to serve two audiences. First, we want to familiarize machine learning experts with the challenges of imitation learning, particularly those arising in robotics, and the interesting theoretical and practical distinctions between it and more familiar frameworks like statistical supervised learning theory and reinforcement learning. Second, we want to give roboticists and experts in applied artificial intelligence a broader appreciation for the frameworks and tools available for imitation learning. We pay particular attention to the intimate connection between imitation learning approaches and those of structured prediction Daume III et al. [2009]. To structure this discussion, we categorize imitation learning techniques based on the following key criteria which drive algorithmic decisions: \n \n1) The structure of the policy space. Is the learned policy a time-index trajectory (trajectory learning), a mapping from observations to actions (so called behavioral cloning [Bain and Sammut, 1996]), or the result of a complex optimization or planning problem at each execution as is common in inverse optimal control methods [Kalman, 1964, Moylan and Anderson, 1973]. \n \n2) The information available during training and testing. In particular, is the learning algorithm privy to the full state that the teacher possess? Is the learner able to interact with the teacher and gather corrections or more data? Does the learner have a (typically a priori) model of the system with which it interacts? Does the learner have access to the reward (cost) function that the teacher is attempting to optimize? \n \n3) The notion of success. Different algorithmic approaches provide varying guarantees on the resulting learned behavior. These guarantees range from weaker (e.g., measuring disagreement with the agent’s decision) to stronger (e.g., providing guarantees on the performance of the learner with respect to a true cost function, either known or unknown). We organize our work by paying particular attention to distinction (1): dividing imitation learning into directly replicating desired behavior (sometimes called behavioral cloning) and learning the hidden objectives of the desired behavior from demonstrations (called inverse optimal control or inverse reinforcement learning [Russell, 1998]). In the latter case, behavior arises as the result of an optimization problem solved for each new instance that the learner faces. In addition to method analysis, we discuss the design decisions a practitioner must make when selecting an imitation learning approach. Moreover, application examples—such as robots that play table tennis [Kober and Peters, 2009], programs that play the game of Go [Silver et al., 2016], and systems that understand natural language [Wen et al., 2015]— illustrate the properties and motivations behind different forms of imitation learning. We conclude by presenting a set of open questions and point towards possible future research directions for machine learning.", "year": 2018, "publicationdate": "2018-03-27", "externalids": {"DOI": "10.1561/2300000053"}, "doi_lower": "10.1561/2300000053"}
{"paper_id": 246426909, "title": "Training language models to follow instructions with human feedback", "author_names": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "John Schulman", "Jacob Hilton", "Fraser Kelton", "Luke E. Miller", "Maddie Simens", "Amanda Askell", "Peter Welinder", "P. Christiano", "Jan Leike", "Ryan J. Lowe"], "venue": "Neural Information Processing Systems", "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.", "year": 2022, "publicationdate": "2022-03-04", "externalids": {}, "doi_lower": null}
{"paper_id": 263152829, "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions", "author_names": ["Lorenzo Pacchiardi", "A. J. Chan", "S. Mindermann", "Ilan Moscovitz", "Alexa Y. Pan", "Y. Gal", "Owain Evans", "J. Brauner"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) can\"lie\", which we define as outputting false statements despite\"knowing\"the truth in a demonstrable sense. LLMs might\"lie\", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -- prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.", "year": 2023, "publicationdate": "2023-09-26", "externalids": {"DOI": "10.48550/arXiv.2309.15840"}, "doi_lower": "10.48550/arxiv.2309.15840"}
{"paper_id": 195317079, "title": "Learning Reward Functions by Integrating Human Demonstrations and Preferences", "author_names": ["Malayandi Palan", "Nicholas C. Landolfi", "Gleb Shevchuk", "Dorsa Sadigh"], "venue": "Robotics: Science and Systems", "abstract": "Our goal is to accurately and efficiently learn reward functions for autonomous robots. Current approaches to this problem include inverse reinforcement learning (IRL), which uses expert demonstrations, and preference-based learning, which iteratively queries the user for her preferences between trajectories. In robotics however, IRL often struggles because it is difficult to get high-quality demonstrations; conversely, preference-based learning is very inefficient since it attempts to learn a continuous, high-dimensional function from binary feedback. We propose a new framework for reward learning, DemPref, that uses both demonstrations and preference queries to learn a reward function. Specifically, we (1) use the demonstrations to learn a coarse prior over the space of reward functions, to reduce the effective size of the space from which queries are generated; and (2) use the demonstrations to ground the (active) query generation process, to improve the quality of the generated queries. Our method alleviates the efficiency issues faced by standard preference-based learning methods and does not exclusively depend on (possibly low-quality) demonstrations. In numerical experiments, we find that DemPref is significantly more efficient than a standard active preference-based learning method. In a user study, we compare our method to a standard IRL method; we find that users rated the robot trained with DemPref as being more successful at learning their desired behavior, and preferred to use the DemPref system (over IRL) to train the robot.", "year": 2019, "publicationdate": "2019-06-21", "externalids": {"DOI": "10.15607/RSS.2019.XV.023"}, "doi_lower": "10.15607/rss.2019.xv.023"}
{"paper_id": 245837268, "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models", "author_names": ["Alexander Pan", "K. Bhatia", "J. Steinhardt"], "venue": "International Conference on Learning Representations", "abstract": "Reward hacking -- where RL agents exploit gaps in misspecified reward functions -- has been widely observed, but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, we find instances of phase transitions: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.", "year": 2022, "publicationdate": "2022-01-10", "externalids": {}, "doi_lower": null}
{"paper_id": 257985073, "title": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark", "author_names": ["Alexander Pan", "C. Shern", "Andy Zou", "Nathaniel Li", "Steven Basart", "Thomas Woodside", "Jonathan Ng", "Hanlin Zhang", "Scott Emmons", "Dan Hendrycks"], "venue": "International Conference on Machine Learning", "abstract": "Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities.", "year": 2023, "publicationdate": "2023-04-06", "externalids": {"DOI": "10.48550/arXiv.2304.03279"}, "doi_lower": "10.48550/arxiv.2304.03279"}
{"paper_id": 257985073, "title": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark", "author_names": ["Alexander Pan", "C. Shern", "Andy Zou", "Nathaniel Li", "Steven Basart", "Thomas Woodside", "Jonathan Ng", "Hanlin Zhang", "Scott Emmons", "Dan Hendrycks"], "venue": "International Conference on Machine Learning", "abstract": "Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities.", "year": 2023, "publicationdate": "2023-04-06", "externalids": {"DOI": "10.48550/arXiv.2304.03279"}, "doi_lower": "10.48550/arxiv.2304.03279"}
{"paper_id": 220380881, "title": "Modeling and Mitigating Human Annotation Errors to Design Efficient Stream Processing Systems with Human-in-the-loop Machine Learning", "author_names": ["Rahul Pandey", "Hemant Purohit", "C. Castillo", "V. Shalin"], "venue": "Int. J. Hum. Comput. Stud.", "abstract": null, "year": 2020, "publicationdate": "2020-07-07", "externalids": {"DOI": "10.1016/j.ijhcs.2022.102772"}, "doi_lower": "10.1016/j.ijhcs.2022.102772"}
{"paper_id": 238971845, "title": "Framework on ethical aspects of artificial intelligence, robotics and related technologies", "author_names": ["Paulina Pankowska"], "venue": "", "abstract": null, "year": 2020, "publicationdate": "2020-09-28", "externalids": {"DOI": "10.2861/94107"}, "doi_lower": "10.2861/94107"}
{"paper_id": 6726938, "title": "Towards the Science of Security and Privacy in Machine Learning", "author_names": ["Nicolas Papernot", "P. Mcdaniel", "Arunesh Sinha", "Michael P. Wellman"], "venue": "arXiv.org", "abstract": "Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive---new systems and models are being deployed in every domain imaginable, leading to rapid and widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used.", "year": 2016, "publicationdate": "2016-11-11", "externalids": {}, "doi_lower": null}
{"paper_id": 247292805, "title": "The Unsurprising Effectiveness of Pre-Trained Vision Models for Control", "author_names": ["Simone Parisi", "A. Rajeswaran", "Senthil Purushwalkam", "A. Gupta"], "venue": "International Conference on Machine Learning", "abstract": "Recent years have seen the emergence of pretrained representations as a powerful abstraction for AI applications in computer vision, natural language, and speech. However, policy learning for control is still dominated by a tabula-rasa learning paradigm, with visuo-motor policies often trained from scratch using data from deployment environments. In this context, we revisit and study the role of pre-trained visual representations for control, and in particular representations trained on large-scale computer vision datasets. Through extensive empirical evaluation in diverse control domains (Habitat, DeepMind Control, Adroit, Franka Kitchen), we isolate and study the importance of different representation training methods, data augmentations, and feature hierarchies. Overall, we find that pre-trained visual representations can be competitive or even better than ground-truth state representations to train control policies. This is in spite of using only out-of-domain data from standard vision datasets, without any in-domain data from the deployment environments.", "year": 2022, "publicationdate": "2022-03-07", "externalids": {"DOI": "10.48550/arXiv.2203.03580"}, "doi_lower": "10.48550/arxiv.2203.03580"}
{"paper_id": 258040990, "title": "Generative Agents: Interactive Simulacra of Human Behavior", "author_names": ["J. Park", "Joseph C. O’Brien", "Carrie J. Cai", "M. Morris", "Percy Liang", "Michael S. Bernstein"], "venue": "ACM Symposium on User Interface Software and Technology", "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.", "year": 2023, "publicationdate": "2023-04-07", "externalids": {"DOI": "10.1145/3586183.3606763"}, "doi_lower": "10.1145/3586183.3606763"}
{"paper_id": 261276587, "title": "AI deception: A survey of examples, risks, and potential solutions", "author_names": ["Peter S. Park", "Simon Goldstein", "Aidan O'Gara", "Michael Chen", "Dan Hendrycks"], "venue": "Patterns", "abstract": null, "year": 2023, "publicationdate": "2023-08-28", "externalids": {"DOI": "10.1016/j.patter.2024.100988"}, "doi_lower": "10.1016/j.patter.2024.100988"}
{"paper_id": 239010011, "title": "BBQ: A hand-built bias benchmark for question answering", "author_names": ["Alicia Parrish", "Angelica Chen", "Nikita Nangia", "Vishakh Padmakumar", "Jason Phang", "Jana Thompson", "Phu Mon Htut", "Sam Bowman"], "venue": "Findings", "abstract": "It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model’s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model’s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.", "year": 2021, "publicationdate": "2021-10-15", "externalids": {"DOI": "10.18653/v1/2022.findings-acl.165"}, "doi_lower": "10.18653/v1/2022.findings-acl.165"}
{"paper_id": 228084012, "title": "Data and its (dis)contents: A survey of dataset development and use in machine learning research", "author_names": ["Amandalynne Paullada", "Inioluwa Deborah Raji", "Emily M. Bender", "Emily L. Denton", "A. Hanna"], "venue": "Patterns", "abstract": null, "year": 2020, "publicationdate": "2020-12-09", "externalids": {"DOI": "10.1016/j.patter.2021.100336"}, "doi_lower": "10.1016/j.patter.2021.100336"}
{"paper_id": 21850704, "title": "A Deep Reinforced Model for Abstractive Summarization", "author_names": ["Romain Paulus", "Caiming Xiong", "R. Socher"], "venue": "International Conference on Learning Representations", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.", "year": 2017, "publicationdate": "2017-05-11", "externalids": {}, "doi_lower": null}
{"paper_id": 245220588, "title": "Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions", "author_names": ["H. Pearce", "Baleegh Ahmad", "Benjamin Tan", "Brendan Dolan-Gavitt", "R. Karri"], "venue": "IEEE Symposium on Security and Privacy", "abstract": "There is burgeoning interest in designing AI-based systems to assist humans in designing computing systems, including tools that automatically generate computer code. The most notable of these comes in the form of the first self-described ‘AI pair programmer’, GitHub Copilot, which is a language model trained over open-source GitHub code. However, code often contains bugs—and so, given the vast quantity of unvetted code that Copilot has processed, it is certain that the language model will have learned from exploitable, buggy code. This raises concerns on the security of Copilot’s code contributions. In this work, we systematically investigate the prevalence and conditions that can cause GitHub Copilot to recommend insecure code. To perform this analysis we prompt Copilot to generate code in scenarios relevant to high-risk cybersecurity weaknesses, e.g. those from MITRE’s “Top 25” Common Weakness Enumeration (CWE) list. We explore Copilot’s performance on three distinct code generation axes—examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains. In total, we produce 89 different scenarios for Copilot to complete, producing 1,689 programs. Of these, we found approximately 40% to be vulnerable.", "year": 2021, "publicationdate": "2021-08-20", "externalids": {"DOI": "10.1145/3610721"}, "doi_lower": "10.1145/3610721"}
{"paper_id": 270348293, "title": "An AI red team playbook", "author_names": ["Anna Raney", "Shiri Bendelac", "Keith Manville", "Mike Tan", "Kureha Yamaguchi"], "venue": "Defense + Commercial Sensing", "abstract": "As artificial intelligence (AI) continues to develop rapidly and influence numerous applications affecting billions of lives, it is crucial to form AI red teams whose objective is to identify AI-enabled system vulnerabilities before deployment to reduce likelihood or severity of real-world security risks. In response, we present a playbook to establish a formalized and repeatable process for AI red teaming. By describing the process as part of a larger framework known as Build-Attack-Defend (BAD), we define a collaborative process between the AI-enabled system development and security teams, as well as various stakeholders. Complementing An AI Blue Team Playbook, this paper contains the red teaming historical context, process, and lessons learned, serving as a starting point for proactively identifying weaknesses, enhancing the overall performance, security, and resilience of AI-enabled systems.", "year": 2024, "publicationdate": "2024-06-07", "externalids": {"DOI": "10.1117/12.3021906"}, "doi_lower": "10.1117/12.3021906"}
{"paper_id": 211133055, "title": "Performative Prediction", "author_names": ["Juan C. Perdomo", "Tijana Zrnic", "Celestine Mendler-Dünner", "Moritz Hardt"], "venue": "International Conference on Machine Learning", "abstract": "When predictions support decisions they may influence the outcome they aim to predict. We call such predictions performative; the prediction influences the target. Performativity is a well-studied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining. We develop a risk minimization framework for performative prediction bringing together concepts from statistics, game theory, and causality. A conceptual novelty is an equilibrium notion we call performative stability. Performative stability implies that the predictions are calibrated not against past outcomes, but against the future outcomes that manifest from acting on the prediction. Our main results are necessary and sufficient conditions for the convergence of retraining to a performatively stable point of nearly minimal loss. In full generality, performative prediction strictly subsumes the setting known as strategic classification. We thus also give the first sufficient conditions for retraining to overcome strategic feedback effects.", "year": 2020, "publicationdate": "2020-02-16", "externalids": {}, "doi_lower": null}
{"paper_id": 21819663, "title": "Luís Moniz Pereira & Ari Saptawijaya, Programming Machine Ethics", "author_names": ["Sean Welsh"], "venue": "Minds and Machines", "abstract": null, "year": 2017, "publicationdate": "2017-03-01", "externalids": {"DOI": "10.1007/s11023-016-9398-x"}, "doi_lower": "10.1007/s11023-016-9398-x"}
{"paper_id": 85471111, "title": "Programming Machine Ethics", "author_names": ["L. Pereira", "A. Saptawijaya"], "venue": "Studies in Applied Philosophy, Epistemology and Rational Ethics", "abstract": null, "year": 2016, "publicationdate": "2016-02-16", "externalids": {"DOI": "10.1007/978-3-319-29354-7"}, "doi_lower": "10.1007/978-3-319-29354-7"}
{"paper_id": 254854519, "title": "Discovering Language Model Behaviors with Model-Written Evaluations", "author_names": ["Ethan Perez", "Sam Ringer", "Kamilė Lukošiūtė", "Karina Nguyen", "Edwin Chen", "Scott Heiner", "Craig Pettit", "Catherine Olsson", "Sandipan Kundu", "Saurav Kadavath", "Andy Jones", "Anna Chen", "Benjamin Mann", "Brian Israel", "Bryan Seethor", "C. McKinnon", "Chris Olah", "Daisong Yan", "D. Amodei", "Dario Amodei", "Dawn Drain", "Dustin Li", "Eli Tran-Johnson", "G. Khundadze", "John Kernion", "J. Landis", "Jamie Kerr", "J. Mueller", "Jeeyoon Hyun", "J. Landau", "Kamal Ndousse", "L. Goldberg", "Liane Lovitt", "Martin Lucas", "M. Sellitto", "Miranda Zhang", "Neerav Kingsland", "Nelson Elhage", "Nicholas Joseph", "Noem'i Mercado", "Nova Dassarma", "Oliver Rausch", "Robin Larson", "Sam McCandlish", "Scott Johnston", "Shauna Kravec", "S. E. Showk", "Tamera Lanham", "Timothy Telleen-Lawton", "Tom B. Brown", "T. Henighan", "Tristan Hume", "Yuntao Bai", "Zac Hatfield-Dodds", "Jack Clark", "Sam Bowman", "Amanda Askell", "Roger C. Grosse", "Danny Hernandez", "Deep Ganguli", "Evan Hubinger", "Nicholas Schiefer", "Jared Kaplan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (\"sycophancy\") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.", "year": 2022, "publicationdate": "2022-12-19", "externalids": {"DOI": "10.48550/arXiv.2212.09251"}, "doi_lower": "10.48550/arxiv.2212.09251"}
{"paper_id": 21760371, "title": "A multi-agent reinforcement learning model of common-pool resource appropriation", "author_names": ["J. Pérolat", "Joel Z. Leibo", "V. Zambaldi", "Charlie Beattie", "K. Tuyls", "T. Graepel"], "venue": "Neural Information Processing Systems", "abstract": "Humanity faces numerous problems of common-pool resource appropriation. This class of multi-agent social dilemma includes the problems of ensuring sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. Abstract models of common-pool resource appropriation based on non-cooperative game theory predict that self-interested agents will generally fail to find socially positive equilibria---a phenomenon called the tragedy of the commons. However, in reality, human societies are sometimes able to discover and implement stable cooperative solutions. Decades of behavioral game theory research have sought to uncover aspects of human behavior that make this possible. Most of that work was based on laboratory experiments where participants only make a single choice: how much to appropriate. Recognizing the importance of spatial and temporal resource dynamics, a recent trend has been toward experiments in more complex real-time video game-like environments. However, standard methods of non-cooperative game theory can no longer be used to generate predictions for this case. Here we show that deep reinforcement learning can be used instead. To that end, we study the emergent behavior of groups of independently learning agents in a partially observed Markov game modeling common-pool resource appropriation. Our experiments highlight the importance of trial-and-error learning in common-pool resource appropriation and shed light on the relationship between exclusion, sustainability, and inequality.", "year": 2017, "publicationdate": "2017-07-20", "externalids": {}, "doi_lower": null}
{"paper_id": 229153388, "title": "An overview of 11 proposals for building safe advanced AI", "author_names": ["Evan Hubinger"], "venue": "arXiv.org", "abstract": "This paper analyzes and compares 11 different proposals for building safe advanced AI under the current machine learning paradigm, including major contenders such as iterated amplification, AI safety via debate, and recursive reward modeling. Each proposal is evaluated on the four components of outer alignment, inner alignment, training competitiveness, and performance competitiveness, of which the distinction between the latter two is introduced in this paper. While prior literature has primarily focused on analyzing individual proposals, or primarily focused on outer alignment at the expense of inner alignment, this analysis seeks to take a comparative look at a wide range of proposals including a comparative analysis across all four previously mentioned components.", "year": 2020, "publicationdate": "2020-12-04", "externalids": {}, "doi_lower": null}
{"paper_id": 36882285, "title": "Causal inference by using invariant prediction: identification and confidence intervals", "author_names": ["J. Peters", "Peter Buhlmann", "N. Meinshausen"], "venue": "", "abstract": "What is the difference between a prediction that is made with a causal model and that with a non‐causal model? Suppose that we intervene on the predictor variables or change the whole environment. The predictions from a causal model will in general work as well under interventions as for observational data. In contrast, predictions from a non‐causal model can potentially be very wrong if we actively intervene on variables. Here, we propose to exploit this invariance of a prediction under a causal model for causal inference: given different experimental settings (e.g. various interventions) we collect all models that do show invariance in their predictive accuracy across settings and interventions. The causal model will be a member of this set of models with high probability. This approach yields valid confidence intervals for the causal relationships in quite general scenarios. We examine the example of structural equation models in more detail and provide sufficient assumptions under which the set of causal predictors becomes identifiable. We further investigate robustness properties of our approach under model misspecification and discuss possible extensions. The empirical properties are studied for various data sets, including large‐scale gene perturbation experiments.", "year": 2015, "publicationdate": "2015-01-06", "externalids": {"DOI": "10.1111/rssb.12167"}, "doi_lower": "10.1111/rssb.12167"}
{"paper_id": 86533208, "title": "Elements of Causal Inference: Foundations and Learning Algorithms", "author_names": ["J. Peters", "D. Janzing", "Bernhard Schölkopf"], "venue": "", "abstract": null, "year": 2017, "publicationdate": "2017-11-29", "externalids": {}, "doi_lower": null}
{"paper_id": 274610062, "title": "Searching for Structure: Investigating Emergent Communication with Large Language Models", "author_names": ["Tom Kouwenhoven", "Max Peeperkorn", "Tessa Verhoef"], "venue": "International Conference on Computational Linguistics", "abstract": "Human languages have evolved to be structured through repeated language learning and use. These processes introduce biases that operate during language acquisition and shape linguistic systems toward communicative efficiency. In this paper, we investigate whether the same happens if artificial languages are optimised for implicit biases of Large Language Models (LLMs). To this end, we simulate a classical referential game in which LLMs learn and use artificial languages. Our results show that initially unstructured holistic languages are indeed shaped to have some structural properties that allow two LLM agents to communicate successfully. Similar to observations in human experiments, generational transmission increases the learnability of languages, but can at the same time result in non-humanlike degenerate vocabularies. Taken together, this work extends experimental findings, shows that LLMs can be used as tools in simulations of language evolution, and opens possibilities for future human-machine experiments in this field.", "year": 2024, "publicationdate": "2024-12-10", "externalids": {"DOI": "10.48550/arXiv.2412.07646"}, "doi_lower": "10.48550/arxiv.2412.07646"}
{"paper_id": 15313471, "title": "Robust Adversarial Reinforcement Learning", "author_names": ["Lerrel Pinto", "James Davidson", "R. Sukthankar", "A. Gupta"], "venue": "International Conference on Machine Learning", "abstract": "Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H∞ control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced - that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper, Walker2d and Ant) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.", "year": 2017, "publicationdate": "2017-03-08", "externalids": {}, "doi_lower": null}
{"paper_id": 5732661, "title": "Robust solutions to Stackelberg games: Addressing bounded rationality and limited observations in human cognition", "author_names": ["J. Pita", "Manish Jain", "Milind Tambe", "F. Ordóñez", "Sarit Kraus"], "venue": "Artificial Intelligence", "abstract": null, "year": 2010, "publicationdate": "2010-10-01", "externalids": {"DOI": "10.1016/j.artint.2010.07.002"}, "doi_lower": "10.1016/j.artint.2010.07.002"}
{"paper_id": 246634542, "title": "Deep networks on toroids: removing symmetries reveals the structure of flat regions in the landscape geometry", "author_names": ["Fabrizio Pittorino", "Antonio Ferraro", "Gabriele Perugini", "Christoph Feinauer", "Carlo Baldassi", "R. Zecchina"], "venue": "International Conference on Machine Learning", "abstract": "We systematize the approach to the investigation of deep neural network landscapes by basing it on the geometry of the space of implemented functions rather than the space of parameters. Grouping classifiers into equivalence classes, we develop a standardized parameterization in which all symmetries are removed, resulting in a toroidal topology. On this space, we explore the error landscape rather than the loss. This lets us derive a meaningful notion of the flatness of minimizers and of the geodesic paths connecting them. Using different optimization algorithms that sample minimizers with different flatness we study the mode connectivity and relative distances. Testing a variety of state-of-the-art architectures and benchmark datasets, we confirm the correlation between flatness and generalization performance; we further show that in function space flatter minima are closer to each other and that the barriers along the geodesics connecting them are small. We also find that minimizers found by variants of gradient descent can be connected by zero-error paths composed of two straight lines in parameter space, i.e. polygonal chains with a single bend. We observe similar qualitative results in neural networks with binary weights and activations, providing one of the first results concerning the connectivity in this setting. Our results hinge on symmetry removal, and are in remarkable agreement with the rich phenomenology described by some recent analytical studies performed on simple shallow models.", "year": 2022, "publicationdate": "2022-02-07", "externalids": {"DOI": "10.1088/1742-5468/ac9832"}, "doi_lower": "10.1088/1742-5468/ac9832"}
{"paper_id": 116534299, "title": "The Analysis of Permutations", "author_names": ["R. Plackett"], "venue": "", "abstract": null, "year": 1975, "publicationdate": "1975-06-01", "externalids": {"DOI": "10.2307/2346567"}, "doi_lower": "10.2307/2346567"}
{"paper_id": 207778832, "title": "Efficient Training of Artificial Neural Networks for Autonomous Navigation", "author_names": [], "venue": "", "abstract": null, "year": 1991, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 184112896, "title": "Logic of Scientific Discovery, The", "author_names": ["C. Howson"], "venue": "The SAGE Encyclopedia of Research Design", "abstract": null, "year": 2000, "publicationdate": "2000-11-02", "externalids": {"DOI": "10.1093/0198250371.003.0009"}, "doi_lower": "10.1093/0198250371.003.0009"}
{"paper_id": 237273828, "title": "Robustness and Generalization via Generative Adversarial Training", "author_names": ["Omid Poursaeed", "Tianxing Jiang", "Harry Yang", "Serge J. Belongie", "Ser-Nam Lim"], "venue": "IEEE International Conference on Computer Vision", "abstract": "While deep neural networks have achieved remarkable success in various computer vision tasks, they often fail to generalize to new domains and subtle variations of input images. Several defenses have been proposed to improve the robustness against these variations. However, current defenses can only withstand the specific attack used in training, and the models often remain vulnerable to other input variations. Moreover, these methods often degrade performance of the model on clean images and do not generalize to out-of-domain samples. In this paper we present Generative Adversarial Training, an approach to simultaneously improve the model’s generalization to the test set and out-of-domain samples as well as its robustness to unseen adversarial attacks. Instead of altering a low-level pre-defined aspect of images, we generate a spectrum of low-level, mid-level and high-level changes using generative models with a disentangled latent space. Adversarial training with these examples enable the model to withstand a wide range of attacks by observing a variety of input alterations during training. We show that our approach not only improves performance of the model on clean images and out-of-domain samples but also makes it robust against unforeseen attacks and outperforms prior work. We validate effectiveness of our method by demonstrating results on various tasks such as classification, segmentation and object detection.", "year": 2021, "publicationdate": "2021-09-06", "externalids": {"DOI": "10.1109/ICCV48922.2021.01542"}, "doi_lower": "10.1109/iccv48922.2021.01542"}
{"paper_id": 245769834, "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets", "author_names": ["Alethea Power", "Yuri Burda", "Harrison Edwards", "Igor Babuschkin", "Vedant Misra"], "venue": "arXiv.org", "abstract": "In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of\"grokking\"a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.", "year": 2022, "publicationdate": "2022-01-06", "externalids": {}, "doi_lower": null}
{"paper_id": 14049040, "title": "Early Stopping-But When?", "author_names": ["L. Prechelt"], "venue": "Neural Networks", "abstract": null, "year": 1996, "publicationdate": null, "externalids": {"DOI": "10.1007/3-540-49430-8_3"}, "doi_lower": "10.1007/3-540-49430-8_3"}
{"paper_id": 84682316, "title": "Neuroscience. Dale Purves , George J. Augustine , David Fitzpatrick , Lawrence C. Katz , Anthony-Samuel LaMantia , James O. McNamara , S. Mark Williams", "author_names": ["K. Alloway"], "venue": "", "abstract": null, "year": 2001, "publicationdate": "2001-12-01", "externalids": {"DOI": "10.1086/420640"}, "doi_lower": "10.1086/420640"}
{"paper_id": 36322544, "title": "Read Markov Decision Processes Discrete Stochastic Dynamic Programming Markov Decision Processes Discrete Stochastic Dynamic Programming", "author_names": ["Martin", "Puterman"], "venue": "", "abstract": null, "year": 2016, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 263671523, "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!", "author_names": ["Xiangyu Qi", "Yi Zeng", "Tinghao Xie", "Pin-Yu Chen", "Ruoxi Jia", "Prateek Mittal", "Peter Henderson"], "venue": "International Conference on Learning Representations", "abstract": "Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.", "year": 2023, "publicationdate": "2023-10-05", "externalids": {"DOI": "10.48550/arXiv.2310.03693"}, "doi_lower": "10.48550/arxiv.2310.03693"}
{"paper_id": 267681750, "title": "Reward Generalization in RLHF: A Topological Perspective", "author_names": ["Tianyi Qiu", "Fanzhi Zeng", "Jiaming Ji", "Dong Yan", "Kaile Wang", "Jiayi Zhou", "Yang Han", "Josef Dai", "Xuehai Pan", "Yaodong Yang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Existing alignment methods share a common topology of information flow, where reward information is collected from humans, modeled with preference learning, and used to tune language models. However, this shared topology has not been systematically characterized, nor have its alternatives been thoroughly explored, leaving the problems of low data efficiency and unreliable generalization unaddressed. As a solution, we introduce a theory of reward generalization in reinforcement learning from human feedback (RLHF), focusing on the topology of information flow at both macro and micro levels. At the macro level, we portray the RLHF information flow as an autoencoding process over behavior distributions, formalizing the RLHF objective of distributional consistency between human preference and model behavior. At the micro level, we present induced Bayesian networks to model the impact of dataset topologies on reward generalization. Combining analysis on both levels, we propose reward modeling from tree-structured preference information. It is shown to reduce reward uncertainty by up to $\\Theta(\\log n/\\log\\log n)$ times compared to baselines, where $n$ is the dataset size. Validation on three NLP tasks shows that it achieves an average win rate of 65% against baselines, thus improving reward generalization for free via topology design, while reducing the amount of data requiring annotation.", "year": 2024, "publicationdate": "2024-02-15", "externalids": {"DOI": "10.18653/v1/2025.findings-acl.820"}, "doi_lower": "10.18653/v1/2025.findings-acl.820"}
{"paper_id": 1234637, "title": "Invariant visual representation by single neurons in the human brain", "author_names": ["R. Quiroga", "L. Reddy", "Gabriel Kreiman", "C. Koch", "I. Fried"], "venue": "Nature", "abstract": null, "year": 2005, "publicationdate": "2005-06-23", "externalids": {"DOI": "10.1038/nature03687"}, "doi_lower": "10.1038/nature03687"}
{"paper_id": 259980634, "title": "Question Decomposition Improves the Faithfulness of Model-Generated Reasoning", "author_names": ["Ansh Radhakrishnan", "Karina Nguyen", "Anna Chen", "Carol Chen", "Carson E. Denison", "Danny Hernandez", "Esin Durmus", "Evan Hubinger", "John Kernion", "Kamil.e Lukovsiut.e", "Newton Cheng", "Nicholas Joseph", "Nicholas Schiefer", "Oliver Rausch", "Sam McCandlish", "S. E. Showk", "Tamera Lanham", "Tim Maxwell", "V. Chandrasekaran", "Zac Hatfield-Dodds", "Jared Kaplan", "J. Brauner", "Sam Bowman", "Ethan Perez"], "venue": "arXiv.org", "abstract": "As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.", "year": 2023, "publicationdate": "2023-07-17", "externalids": {"DOI": "10.48550/arXiv.2307.11768"}, "doi_lower": "10.48550/arxiv.2307.11768"}
{"paper_id": 258959321, "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "author_names": ["Rafael Rafailov", "Archit Sharma", "E. Mitchell", "Stefano Ermon", "Christopher D. Manning", "Chelsea Finn"], "venue": "Neural Information Processing Systems", "abstract": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {}, "doi_lower": null}
{"paper_id": 232320343, "title": "SELFEXPLAIN: A Self-Explaining Architecture for Neural Text Classifiers", "author_names": ["Dheeraj Rajagopal", "Vidhisha Balachandran", "E. Hovy", "Yulia Tsvetkov"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "We introduce SelfExplain, a novel self-explaining model that explains a text classifier’s predictions using phrase-based concepts. SelfExplain augments existing neural classifiers by adding (1) a globally interpretable layer that identifies the most influential concepts in the training set for a given sample and (2) a locally interpretable layer that quantifies the contribution of each local input concept by computing a relevance score relative to the predicted label. Experiments across five text-classification datasets show that SelfExplain facilitates interpretability without sacrificing performance. Most importantly, explanations from SelfExplain show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines.", "year": 2021, "publicationdate": "2021-03-23", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.64"}, "doi_lower": "10.18653/v1/2021.emnlp-main.64"}
{"paper_id": 209862419, "title": "Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing", "author_names": ["Inioluwa Deborah Raji", "Timnit Gebru", "Margaret Mitchell", "Joy Buolamwini", "Joonseok Lee", "Emily L. Denton"], "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of fiveethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.", "year": 2020, "publicationdate": "2020-01-03", "externalids": {"DOI": "10.1145/3375627.3375820"}, "doi_lower": "10.1145/3375627.3375820"}
{"paper_id": 5649130, "title": "Bayesian Inverse Reinforcement Learning", "author_names": ["Deepak Ramachandran", "Eyal Amir"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": null, "year": 2007, "publicationdate": "2007-01-06", "externalids": {}, "doi_lower": null}
{"paper_id": 251104722, "title": "Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks", "author_names": ["Tilman Raukur", "A. Ho", "Stephen Casper", "Dylan Hadfield-Menell"], "venue": "2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)", "abstract": "The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, “inner” interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications.", "year": 2022, "publicationdate": "2022-07-27", "externalids": {"DOI": "10.1109/SaTML54575.2023.00039"}, "doi_lower": "10.1109/satml54575.2023.00039"}
{"paper_id": 246411428, "title": "Linear Adversarial Concept Erasure", "author_names": ["Shauli Ravfogel", "Michael Twiton", "Yoav Goldberg", "Ryan Cotterell"], "venue": "International Conference on Machine Learning", "abstract": "Modern neural models trained on textual data rely on pre-trained representations that emerge without direct supervision. As these representations are increasingly being used in real-world applications, the inability to \\emph{control} their content becomes an increasingly important problem. We formulate the problem of identifying and erasing a linear subspace that corresponds to a given concept, in order to prevent linear predictors from recovering the concept. We model this problem as a constrained, linear maximin game, and show that existing solutions are generally not optimal for this task. We derive a closed-form solution for certain objectives, and propose a convex relaxation, \\method, that works well for others. When evaluated in the context of binary gender removal, the method recovers a low-dimensional subspace whose removal mitigates bias by intrinsic and extrinsic evaluation. We show that the method is highly expressive, effectively mitigating bias in deep nonlinear classifiers while maintaining tractability and interpretability.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 208958394, "title": "Recent Advances in Robot Learning from Demonstration", "author_names": ["H. Ravichandar", "Athanasios S. Polydoros", "Sonia Chernova", "A. Billard"], "venue": "Annu. Rev. Control. Robotics Auton. Syst.", "abstract": "In the context of robotics and automation, learning from demonstration (LfD) is the paradigm in which robots acquire new skills by learning to imitate an expert. The choice of LfD over other robot learning methods is compelling when ideal behavior can be neither easily scripted (as is done in traditional robot programming) nor easily defined as an optimization problem, but can be demonstrated. While there have been multiple surveys of this field in the past, there is a need for a new one given the considerable growth in the number of publications in recent years. This review aims to provide an overview of the collection of machine-learning methods used to enable a robot to learn from and imitate a teacher. We focus on recent advancements in the field and present an updated taxonomy and characterization of existing methods. We also discuss mature and emerging application areas for LfD and highlight the significant challenges that remain to be overcome both in theory and in practice.", "year": 2020, "publicationdate": "2020-05-03", "externalids": {"DOI": "10.1146/annurev-control-100819-063206"}, "doi_lower": "10.1146/annurev-control-100819-063206"}
{"paper_id": 218486796, "title": "Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?", "author_names": ["Abhilasha Ravichander", "Yonatan Belinkov", "E. Hovy"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the sentence representations learned by neural encoders, through the lens of ‘probing’ tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the model to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the model was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level, even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks.", "year": 2020, "publicationdate": "2020-05-02", "externalids": {"DOI": "10.18653/v1/2021.eacl-main.295"}, "doi_lower": "10.18653/v1/2021.eacl-main.295"}
{"paper_id": 221681000, "title": "Learning Human Objectives by Evaluating Hypothetical Behavior", "author_names": ["A. Appendix"], "venue": "", "abstract": null, "year": 2020, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 202888699, "title": "SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards", "author_names": ["S. Reddy", "A. Dragan", "S. Levine"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2019, "publicationdate": "2019-05-27", "externalids": {}, "doi_lower": null}
{"paper_id": 219612804, "title": "Jackie Kay", "author_names": ["N. Williams"], "venue": "The Wiley Blackwell Companion to Contemporary British and Irish Literature", "abstract": null, "year": 2005, "publicationdate": "2005-06-29", "externalids": {"DOI": "10.1080/02690050601097682"}, "doi_lower": "10.1080/02690050601097682"}
{"paper_id": 174802633, "title": "Visualizing and Measuring the Geometry of BERT", "author_names": ["Andy Coenen", "Emily Reif", "Ann Yuan", "Been Kim", "Adam Pearce", "F. Viégas", "M. Wattenberg"], "venue": "Neural Information Processing Systems", "abstract": "Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.", "year": 2019, "publicationdate": "2019-06-06", "externalids": {}, "doi_lower": null}
{"paper_id": 271571299, "title": "Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?", "author_names": ["Richard Ren", "Steven Basart", "Adam Khoja", "Alice Gatti", "Long Phan", "Xuwang Yin", "Mantas Mazeika", "Alexander Pan", "Gabriel Mukobi", "Ryan H. Kim", "Stephen Fitz", "Dan Hendrycks"], "venue": "Neural Information Processing Systems", "abstract": "As artificial intelligence systems grow more powerful, there has been increasing interest in\"AI safety\"research to address emerging and future risks. However, the field of AI safety remains poorly defined and inconsistently measured, leading to confusion about how researchers can contribute. This lack of clarity is compounded by the unclear relationship between AI safety benchmarks and upstream general capabilities (e.g., general knowledge and reasoning). To address these issues, we conduct a comprehensive meta-analysis of AI safety benchmarks, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety. Our findings reveal that many safety benchmarks highly correlate with both upstream model capabilities and training compute, potentially enabling\"safetywashing\"--where capability improvements are misrepresented as safety advancements. Based on these findings, we propose an empirical foundation for developing more meaningful safety metrics and define AI safety in a machine learning research context as a set of clearly delineated research goals that are empirically separable from generic capabilities advancements. In doing so, we aim to provide a more rigorous framework for AI safety research, advancing the science of safety evaluations and clarifying the path towards measurable progress.", "year": 2024, "publicationdate": "2024-07-31", "externalids": {"DOI": "10.48550/arXiv.2407.21792"}, "doi_lower": "10.48550/arxiv.2407.21792"}
{"paper_id": 214611710, "title": "Generating Natural Language Adversarial Examples on a Large Scale with Generative Models", "author_names": ["Yankun Ren", "J. Lin", "Siliang Tang", "Jun Zhou", "Shuang Yang", "Yuan Qi", "Xiang Ren"], "venue": "European Conference on Artificial Intelligence", "abstract": "Today text classification models have been widely used. However, these classifiers are found to be easily fooled by adversarial examples. Fortunately, standard attacking methods generate adversarial texts in a pair-wise way, that is, an adversarial text can only be created from a real-world text by replacing a few words. In many applications, these texts are limited in numbers, therefore their corresponding adversarial examples are often not diverse enough and sometimes hard to read, thus can be easily detected by humans and cannot create chaos at a large scale. In this paper, we propose an end to end solution to efficiently generate adversarial texts from scratch using generative models, which are not restricted to perturbing the given texts. We call it unrestricted adversarial text generation. Specifically, we train a conditional variational autoencoder (VAE) with an additional adversarial loss to guide the generation of adversarial examples. Moreover, to improve the validity of adversarial texts, we utilize discrimators and the training framework of generative adversarial networks (GANs) to make adversarial texts consistent with real data. Experimental results on sentiment analysis demonstrate the scalability and efficiency of our method. It can attack text classification models with a higher success rate than existing methods, and provide acceptable quality for humans in the meantime.", "year": 2020, "publicationdate": "2020-03-10", "externalids": {"DOI": "10.3233/FAIA200340"}, "doi_lower": "10.3233/faia200340"}
{"paper_id": 233419355, "title": "Hacking, Protection and the Consequences of Hacking Hacking, Protection and the Consequences of Hacking", "author_names": ["Z. Čekerevac", "Z. Dvořák", "Ludmila Prigoda", "Petar Cekerevac"], "venue": "Communications - Scientific letters of the University of Zilina", "abstract": "Understanding the term hacking as any unconventional way of interacting with some system it is easy to conclude that there are enormous number of people who hacked or tried to hack someone or something. The article, as result of author research, analyses hacking from different points of view, including hacker’s point of view as well as the defender’s point of view. Here are discussed questions like: Who are the hackers? Why do people hack? Law aspects of hacking, as well as some economic issues connected with hacking. At the end, some questions about victim protection are discussed together with the weakness that hackers can use for their own protection. The aim of the article is to make readers familiar with the possible risks of hacker's attacks on the mobile phones and on possible attacks in the announced flood of the internet of things (next IoT) devices.", "year": 2018, "publicationdate": "2018-06-30", "externalids": {"DOI": "10.26552/com.c.2018.2.83-87"}, "doi_lower": "10.26552/com.c.2018.2.83-87"}
{"paper_id": 258887545, "title": "Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability", "author_names": ["Jinyung Hong", "Theodore P. Pavlic"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.15775"}, "doi_lower": "10.48550/arxiv.2305.15775"}
{"paper_id": 18896926, "title": "Delusion, Survival, and Intelligent Agents", "author_names": ["Mark B. Ring", "Laurent Orseau"], "venue": "Artificial General Intelligence", "abstract": null, "year": 2011, "publicationdate": "2011-08-03", "externalids": {"DOI": "10.1007/978-3-642-22887-2_2"}, "doi_lower": "10.1007/978-3-642-22887-2_2"}
{"paper_id": 53211000, "title": "A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective", "author_names": ["Yuji Roh", "Geon Heo", "Steven Euijong Whang"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "abstract": "Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.", "year": 2018, "publicationdate": "2018-11-08", "externalids": {"DOI": "10.1109/TKDE.2019.2946162"}, "doi_lower": "10.1109/tkde.2019.2946162"}
{"paper_id": 183131303, "title": "Values and Human Nature", "author_names": ["N. Maccormick"], "venue": "", "abstract": null, "year": 2008, "publicationdate": "2008-12-18", "externalids": {"DOI": "10.1093/ACPROF:OSO/9780198268772.003.0003"}, "doi_lower": "10.1093/acprof:oso/9780198268772.003.0003"}
{"paper_id": 216868486, "title": "SOLID: A Large-Scale Semi-Supervised Dataset for Offensive Language Identification", "author_names": ["Sara Rosenthal", "Pepa Atanasova", "Georgi Karadzhov", "Marcos Zampieri", "Preslav Nakov"], "venue": "Findings", "abstract": "The use of offensive language is a major problem in social media which has led to an abundance of research in detecting content such as hate speech, cyberbulling, and cyber-aggression. There have been several attempts to consolidate and categorize these efforts. Recently, the OLID dataset used at SemEval-2019 proposed a hierarchical three-level annotation taxonomy which addresses different types of offensive language as well as important information such as the target of such content. The categorization provides meaningful and important information for understanding offensive language. However, the OLID dataset is limited in size, especially for some of the low-level categories, which included only a few hundred instances, thus making it challenging to train robust deep learning models. Here, we address this limitation by creating the largest available dataset for this task, SOLID. SOLID contains over nine million English tweets labeled in a semi-supervised manner. We further demonstrate experimentally that using SOLID along with OLID yields improved performance on the OLID test set for two different models, especially for the lower levels of the taxonomy. Finally, we perform analysis of the models' performance on easy and hard examples of offensive language using data annotated in a semi-supervised way.", "year": 2020, "publicationdate": "2020-04-29", "externalids": {"DOI": "10.18653/v1/2021.findings-acl.80"}, "doi_lower": "10.18653/v1/2021.findings-acl.80"}
{"paper_id": 52953225, "title": "The Neural LASSO: Local Linear Sparsity for Interpretable Explanations", "author_names": ["Andrew Slavin", "Isaac Lage", "F. Doshi-Velez"], "venue": "", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 103456, "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning", "author_names": ["Stéphane Ross", "Geoffrey J. Gordon", "J. Bagnell"], "venue": "International Conference on Artificial Intelligence and Statistics", "abstract": "Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.", "year": 2010, "publicationdate": "2010-11-02", "externalids": {}, "doi_lower": null}
{"paper_id": 35687061, "title": "A Short Introduction to Preferences: Between Artificial Intelligence and Social Choice", "author_names": ["F. Rossi", "K. Venable", "T. Walsh"], "venue": "A Short Introduction to Preferences", "abstract": null, "year": 2011, "publicationdate": "2011-07-27", "externalids": {"DOI": "10.1007/978-3-031-01556-4"}, "doi_lower": "10.1007/978-3-031-01556-4"}
{"paper_id": 267935798, "title": "GrabCut: Interactive Foreground Extraction Using Iterated Graph Cuts", "author_names": ["Carsten Rother", "V. Kolmogorov", "Andrew Blake"], "venue": "", "abstract": null, "year": 2023, "publicationdate": "2023-08-01", "externalids": {"DOI": "10.1145/3596711.3596774"}, "doi_lower": "10.1145/3596711.3596774"}
{"paper_id": 182656421, "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "author_names": ["C. Rudin"], "venue": "Nature Machine Intelligence", "abstract": null, "year": 2018, "publicationdate": "2018-11-26", "externalids": {"DOI": "10.1038/s42256-019-0048-x"}, "doi_lower": "10.1038/s42256-019-0048-x"}
{"paper_id": 13756572, "title": "Gender Bias in Coreference Resolution", "author_names": ["Rachel Rudinger", "Jason Naradowsky", "Brian Leonard", "Benjamin Van Durme"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these “Winogender schemas,” we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.", "year": 2018, "publicationdate": "2018-04-25", "externalids": {"DOI": "10.18653/v1/N18-2002"}, "doi_lower": "10.18653/v1/n18-2002"}
{"paper_id": 233775541, "title": "Key Concepts in AI Safety: Interpretability in Machine Learning", "author_names": ["Tim G. J. Rudner", "H. Toner"], "venue": "", "abstract": "This paper is the third installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces interpretability as a means to enable assurance in modern machine learning systems.", "year": 2021, "publicationdate": "2021-03-01", "externalids": {"DOI": "10.51593/20190042"}, "doi_lower": "10.51593/20190042"}
{"paper_id": 233858291, "title": "Key Concepts in AI Safety: Robustness and Adversarial Examples", "author_names": ["Tim G. J. Rudner", "H. Toner"], "venue": "", "abstract": "This paper is the second installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces adversarial examples, a major challenge to robustness in modern machine learning systems.", "year": 2021, "publicationdate": "2021-03-01", "externalids": {"DOI": "10.51593/20190041"}, "doi_lower": "10.51593/20190041"}
{"paper_id": 236102707, "title": "Human Compatible: Artificial Intelligence and the Problem of Control", "author_names": ["Stuart Russell"], "venue": "", "abstract": null, "year": 2019, "publicationdate": "2019-10-08", "externalids": {}, "doi_lower": null}
{"paper_id": 8174496, "title": "Research Priorities for Robust and Beneficial Artificial Intelligence", "author_names": ["Stuart J. Russell", "Dan Dewey", "Max Tegmark"], "venue": "The AI Magazine", "abstract": "Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.", "year": 2015, "publicationdate": "2015-12-01", "externalids": {"DOI": "10.1609/aimag.v36i4.2577"}, "doi_lower": "10.1609/aimag.v36i4.2577"}
{"paper_id": 255595944, "title": "Data Distillation: A Survey", "author_names": ["Noveen Sachdeva", "Julian McAuley"], "venue": "Trans. Mach. Learn. Res.", "abstract": "The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc. In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for different data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions.", "year": 2023, "publicationdate": "2023-01-11", "externalids": {"DOI": "10.48550/arXiv.2301.04272"}, "doi_lower": "10.48550/arxiv.2301.04272"}
{"paper_id": 158056606, "title": "The Evolution of Cooperation THE EVOLUTION OF COOPERATION", "author_names": ["R. Axelrod"], "venue": "", "abstract": null, "year": 2009, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 12226563, "title": "Active Preference-Based Learning of Reward Functions", "author_names": ["Dorsa Sadigh", "A. Dragan", "S. Sastry", "S. Seshia"], "venue": "Robotics: Science and Systems", "abstract": null, "year": 2017, "publicationdate": "2017-07-12", "externalids": {"DOI": "10.15607/RSS.2017.XIII.053"}, "doi_lower": "10.15607/rss.2017.xiii.053"}
{"paper_id": 213662188, "title": "Distributionally Robust Neural Networks", "author_names": ["Shiori Sagawa", "Pang Wei Koh", "Tatsunori B. Hashimoto", "Percy Liang"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2020, "publicationdate": "2020-04-30", "externalids": {}, "doi_lower": null}
{"paper_id": 53305131, "title": "Aequitas: A Bias and Fairness Audit Toolkit", "author_names": ["Pedro Saleiro", "Benedict Kuester", "Abby Stevens", "Ari Anisfeld", "Loren Hinkson", "J. London", "R. Ghani"], "venue": "arXiv.org", "abstract": "Recent work has raised concerns on the risk of unintended bias in AI systems being used nowadays that can affect individuals unfairly based on race, gender or religion, among other possible characteristics. While a lot of bias metrics and fairness definitions have been proposed in recent years, there is no consensus on which metric/definition should be used and there are very few available resources to operationalize them. Therefore, despite recent awareness, auditing for bias and fairness when developing and deploying AI systems is not yet a standard practice. We present Aequitas, an open source bias and fairness audit toolkit that is an intuitive and easy to use addition to the machine learning workflow, enabling users to seamlessly test models for several bias and fairness metrics in relation to multiple population sub-groups. Aequitas facilitates informed and equitable decisions around developing and deploying algorithmic decision making systems for both data scientists, machine learning researchers and policymakers.", "year": 2018, "publicationdate": "2018-11-14", "externalids": {}, "doi_lower": null}
{"paper_id": 202160253, "title": "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning", "author_names": ["W. Samek", "G. Montavon", "A. Vedaldi", "L. K. Hansen", "Klaus Müller"], "venue": "Explainable AI", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.1007/978-3-030-28954-6"}, "doi_lower": "10.1007/978-3-030-28954-6"}
{"paper_id": 259252204, "title": "Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools", "author_names": ["J. Sandbrink"], "venue": "arXiv.org", "abstract": "As advancements in artificial intelligence (AI) propel progress in the life sciences, they may also enable the weaponisation and misuse of biological agents. This article differentiates two classes of AI tools that could pose such biosecurity risks: large language models (LLMs) and biological design tools (BDTs). LLMs, such as GPT-4 and its successors, might provide dual-use information and thus remove some barriers encountered by historical biological weapons efforts. As LLMs are turned into multi-modal lab assistants and autonomous science tools, this will increase their ability to support non-experts in performing laboratory work. Thus, LLMs may in particular lower barriers to biological misuse. In contrast, BDTs will expand the capabilities of sophisticated actors. Concretely, BDTs may enable the creation of pandemic pathogens substantially worse than anything seen to date and could enable forms of more predictable and targeted biological weapons. In combination, the convergence of LLMs and BDTs could raise the ceiling of harm from biological agents and could make them broadly accessible. A range of interventions would help to manage risks. Independent pre-release evaluations could help understand the capabilities of models and the effectiveness of safeguards. Options for differentiated access to such tools should be carefully weighed with the benefits of openly releasing systems. Lastly, essential for mitigating risks will be universal and enhanced screening of gene synthesis products.", "year": 2023, "publicationdate": "2023-06-24", "externalids": {"DOI": "10.48550/arXiv.2306.13952"}, "doi_lower": "10.48550/arxiv.2306.13952"}
{"paper_id": 220381947, "title": "A Situation Awareness-Based Framework for Design and Evaluation of Explainable AI", "author_names": ["Lindsay M. Sanneman", "J. Shah"], "venue": "EXTRAAMAS@AAMAS", "abstract": "Recent advances in artificial intelligence (AI) have drawn attention to the need for AI systems to be understandable to human users. The explainable AI (XAI) literature aims to enhance human understanding and human-AI team performance by providing users with necessary information about AI system behavior. Simultaneously, the human factors literature has long addressed important considerations that contribute to human performance, including how to determine human informational needs. Drawing from the human factors literature, we propose a three-level framework for the development and evaluation of explanations about AI system behavior. Our proposed levels of XAI are based on the informational needs of human users, which can be determined using the levels of situation awareness (SA) framework from the human factors literature. Based on our levels of XAI framework, we also propose a method for assessing the effectiveness of XAI systems.", "year": 2020, "publicationdate": "2020-05-09", "externalids": {"DOI": "10.1007/978-3-030-51924-7_6"}, "doi_lower": "10.1007/978-3-030-51924-7_6"}
{"paper_id": 257834040, "title": "Whose Opinions Do Language Models Reflect?", "author_names": ["Shibani Santurkar", "Esin Durmus", "Faisal Ladhak", "Cinoo Lee", "Percy Liang", "Tatsunori Hashimoto"], "venue": "International Conference on Machine Learning", "abstract": "Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned LMs, but also surfaces groups whose opinions are poorly reflected by current LMs (e.g., 65+ and widowed individuals). Our code and data are available at https://github.com/tatsu-lab/opinions_qa.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {}, "doi_lower": null}
{"paper_id": 189235288, "title": "We Must Consider Tomorrow", "author_names": ["A. Samuelson"], "venue": "", "abstract": null, "year": 1933, "publicationdate": "1933-12-01", "externalids": {"DOI": "10.1177/002205743311602010"}, "doi_lower": "10.1177/002205743311602010"}
{"paper_id": 235613571, "title": "Behavioral Cloning from Noisy Demonstrations", "author_names": ["Fumihiro Sasaki", "R. Yamashina"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2021, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 249626555, "title": "Self-critiquing models for assisting human evaluators", "author_names": ["W. Saunders", "Catherine Yeh", "Jeff Wu", "Steven Bills", "Ouyang Long", "Jonathan Ward", "Jan Leike"], "venue": "arXiv.org", "abstract": "We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own selfcritiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.", "year": 2022, "publicationdate": "2022-06-12", "externalids": {"DOI": "10.48550/arXiv.2206.05802"}, "doi_lower": "10.48550/arxiv.2206.05802"}
{"paper_id": 53250209, "title": "How Do Fairness Definitions Fare?: Examining Public Attitudes Towards Algorithmic Definitions of Fairness", "author_names": ["N. Saxena", "Karen Huang", "Evan DeFilippis", "Goran Radanovic", "D. Parkes", "Y. Liu"], "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "What is the best way to define algorithmic fairness? While many definitions of fairness have been proposed in the computer science literature, there is no clear agreement over a particular definition. In this work, we investigate ordinary people's perceptions of three of these fairness definitions. Across two online experiments, we test which definitions people perceive to be the fairest in the context of loan decisions, and whether fairness perceptions change with the addition of sensitive information (i.e., race of the loan applicants). Overall, one definition (calibrated fairness) tends to be more pre- ferred than the others, and the results also provide support for the principle of affirmative action.", "year": 2018, "publicationdate": "2018-11-08", "externalids": {"DOI": "10.1145/3306618.3314248"}, "doi_lower": "10.1145/3306618.3314248"}
{"paper_id": 54073545, "title": "Learning from Demonstration ( Programming by Demonstration ) ∗", "author_names": ["S. Calinon"], "venue": "", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {"DOI": "10.1007/978-3-642-41610-1_27-1"}, "doi_lower": "10.1007/978-3-642-41610-1_27-1"}
{"paper_id": 7124120, "title": "Is imitation learning the route to humanoid robots?", "author_names": ["S. Schaal"], "venue": "Trends in Cognitive Sciences", "abstract": null, "year": 1999, "publicationdate": "1999-06-01", "externalids": {"DOI": "10.1016/S1364-6613(99)01327-3"}, "doi_lower": "10.1016/s1364-6613(99)01327-3"}
{"paper_id": 260164518, "title": "Evaluating the Moral Beliefs Encoded in LLMs", "author_names": ["Nino Scherrer", "Claudia Shi", "Amir Feder", "D. Blei"], "venue": "Neural Information Processing Systems", "abstract": "This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM\"making a choice\", the associated uncertainty, and the consistency of that choice. (2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g.,\"Should I tell a white lie?\") and 687 low-ambiguity moral scenarios (e.g.,\"Should I stop for a pedestrian on the road?\"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g.,\"do not kill\"). We administer the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scenarios, most models\"choose\"actions that align with commonsense. In ambiguous cases, most models express uncertainty. (b) Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording. (c) Some models reflect clear preferences in ambiguous scenarios. Specifically, closed-source models tend to agree with each other.", "year": 2023, "publicationdate": "2023-07-26", "externalids": {"DOI": "10.48550/arXiv.2307.14324"}, "doi_lower": "10.48550/arxiv.2307.14324"}
{"paper_id": 218685997, "title": "The Moral Choice Machine", "author_names": ["P. Schramowski", "Cigdem Turan", "Sophie F. Jentzsch", "C. Rothkopf", "K. Kersting"], "venue": "Frontiers in Artificial Intelligence", "abstract": "Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? In this study, we show that applying machine learning to human texts can extract deontological ethical reasoning about “right” and “wrong” conduct. We create a template list of prompts and responses, such as “Should I [action]?”, “Is it okay to [action]?”, etc. with corresponding answers of “Yes/no, I should (not).” and \"Yes/no, it is (not).\" The model's bias score is the difference between the model's score of the positive response (“Yes, I should”) and that of the negative response (“No, I should not”). For a given choice, the model's overall bias score is the mean of the bias scores of all question/answer templates paired with that choice. Specifically, the resulting model, called the Moral Choice Machine (MCM), calculates the bias score on a sentence level using embeddings of the Universal Sentence Encoder since the moral value of an action to be taken depends on its context. It is objectionable to kill living beings, but it is fine to kill time. It is essential to eat, yet one might not eat dirt. It is important to spread information, yet one should not spread misinformation. Our results indicate that text corpora contain recoverable and accurate imprints of our social, ethical and moral choices, even with context information. Actually, training the Moral Choice Machine on different temporal news and book corpora from the year 1510 to 2008/2009 demonstrate the evolution of moral and ethical choices over different time periods for both atomic actions and actions with context information. By training it on different cultural sources such as the Bible and the constitution of different countries, the dynamics of moral choices in culture, including technology are revealed. That is the fact that moral biases can be extracted, quantified, tracked, and compared across cultures and over time.", "year": 2020, "publicationdate": "2020-05-20", "externalids": {"DOI": "10.3389/frai.2020.00036"}, "doi_lower": "10.3389/frai.2020.00036"}
{"paper_id": 258676345, "title": "Towards best practices in AGI safety and governance: A survey of expert opinion", "author_names": ["Jonas Schuett", "Noemi Dreksler", "Markus Anderljung", "David McCaffary", "Lennart Heim", "Emma Bluemke", "Ben Garfinkel"], "venue": "arXiv.org", "abstract": "A number of leading AI companies, including OpenAI, Google DeepMind, and Anthropic, have the stated goal of building artificial general intelligence (AGI) - AI systems that achieve or exceed human performance across a wide range of cognitive tasks. In pursuing this goal, they may develop and deploy AI systems that pose particularly significant risks. While they have already taken some measures to mitigate these risks, best practices have not yet emerged. To support the identification of best practices, we sent a survey to 92 leading experts from AGI labs, academia, and civil society and received 51 responses. Participants were asked how much they agreed with 50 statements about what AGI labs should do. Our main finding is that participants, on average, agreed with all of them. Many statements received extremely high levels of agreement. For example, 98% of respondents somewhat or strongly agreed that AGI labs should conduct pre-deployment risk assessments, dangerous capabilities evaluations, third-party model audits, safety restrictions on model usage, and red teaming. Ultimately, our list of statements may serve as a helpful foundation for efforts to develop best practices, standards, and regulations for AGI labs.", "year": 2023, "publicationdate": "2023-05-11", "externalids": {"DOI": "10.48550/arXiv.2305.07153"}, "doi_lower": "10.48550/arxiv.2305.07153"}
{"paper_id": 28695052, "title": "Proximal Policy Optimization Algorithms", "author_names": ["John Schulman", "Filip Wolski", "Prafulla Dhariwal", "Alec Radford", "Oleg Klimov"], "venue": "arXiv.org", "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.", "year": 2017, "publicationdate": "2017-07-20", "externalids": {}, "doi_lower": null}
{"paper_id": 119357204, "title": "Quantum Replicator Dynamics", "author_names": ["Esteban Guevara Hidalgo"], "venue": "", "abstract": null, "year": 2005, "publicationdate": "2005-10-31", "externalids": {"DOI": "10.1016/j.physa.2006.02.017"}, "doi_lower": "10.1016/j.physa.2006.02.017"}
{"paper_id": 14089770, "title": "Universals in the Content and Structure of Values: Theoretical Advances and Empirical Tests in 20 Countries", "author_names": ["S. Schwartz"], "venue": "", "abstract": null, "year": 1992, "publicationdate": null, "externalids": {"DOI": "10.1016/S0065-2601(08)60281-6"}, "doi_lower": "10.1016/s0065-2601(08)60281-6"}
{"paper_id": 15121950, "title": "Are There Universal Aspects in the Structure and Contents of Human Values", "author_names": ["S. Schwartz"], "venue": "", "abstract": null, "year": 1994, "publicationdate": null, "externalids": {"DOI": "10.1111/J.1540-4560.1994.TB01196.X"}, "doi_lower": "10.1111/j.1540-4560.1994.tb01196.x"}
{"paper_id": 263845086, "title": "Open-Sourcing Highly Capable Foundation Models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives", "author_names": ["Elizabeth Seger", "Noemi Dreksler", "Richard Moulange", "Emily Dardaman", "Jonas Schuett", "Kevin Wei", "Christoph Winter", "Mackenzie Arnold", "Anton Korinek", "Markus Anderljung", "Ben Bucknall", "Alan Chan", "Eoghan Stafford", "Leonie Koessler", "Aviv Ovadya", "Ben Garfinkel", "Emma Bluemke", "Michael Aird", "Patrick Levermore", "Julian Hazell", "Abhishek Gupta", "Andrew Trask", "Ben Cottier", "Herbie Bradley", "Irene Solaiman", "Norman Johnson", "Peter Cihon", "S. Avin", "Stella Biderman"], "venue": "Social Science Research Network", "abstract": "Recent decisions by leading AI labs to either open-source their models or to restrict access to their models has sparked debate about whether, and how, increasingly capable AI models should be shared. Open-sourcing in AI typically refers to making model architecture and weights freely and publicly accessible for anyone to modify, study, build on, and use. This offers advantages such as enabling external oversight, accelerating progress, and decentralizing control over AI development and use. However, it also presents a growing potential for misuse and unintended consequences. This paper offers an examination of the risks and benefits of open-sourcing highly capable foundation models. While open-sourcing has historically provided substantial net benefits for most software and AI development processes, we argue that for some highly capable foundation models likely to be developed in the near future, open-sourcing may pose sufficiently extreme risks to outweigh the benefits. In such a case, highly capable foundation models should not be open-sourced, at least not initially. Alternative strategies, including non-open-source model sharing options, are explored. The paper concludes with recommendations for developers, standard-setting bodies, and governments for establishing safe and responsible model sharing practices and preserving open-source benefits where safe.", "year": 2023, "publicationdate": "2023-09-29", "externalids": {"DOI": "10.2139/ssrn.4596436"}, "doi_lower": "10.2139/ssrn.4596436"}
{"paper_id": 27901039, "title": "On Interpretability of Almost Linear Orderings", "author_names": ["Akito Tsuboi", "Kentaro Wakai"], "venue": "Notre Dame J. Formal Log.", "abstract": null, "year": 1998, "publicationdate": null, "externalids": {"DOI": "10.1305/ndjfl/1039182249"}, "doi_lower": "10.1305/ndjfl/1039182249"}
{"paper_id": 157572765, "title": "Aspects of Mathematical Economics, Social Choice and Game Theory", "author_names": ["H. Mohajan"], "venue": "", "abstract": null, "year": 2012, "publicationdate": "2012-09-26", "externalids": {}, "doi_lower": null}
{"paper_id": 202676712, "title": "Segregation dynamics with reinforcement learning and agent based modeling", "author_names": ["Egemen Sert", "Y. Bar-Yam", "A. Morales"], "venue": "Scientific Reports", "abstract": "Societies are complex. Properties of social systems can be explained by the interplay and weaving of individual actions. Rewards are key to understand people’s choices and decisions. For instance, individual preferences of where to live may lead to the emergence of social segregation. In this paper, we combine Reinforcement Learning (RL) with Agent Based Modeling (ABM) in order to address the self-organizing dynamics of social segregation and explore the space of possibilities that emerge from considering different types of rewards. Our model promotes the creation of interdependencies and interactions among multiple agents of two different kinds that segregate from each other. For this purpose, agents use Deep Q-Networks to make decisions inspired on the rules of the Schelling Segregation model and rewards for interactions. Despite the segregation reward, our experiments show that spatial integration can be achieved by establishing interdependencies among agents of different kinds. They also reveal that segregated areas are more probable to host older people than diverse areas, which attract younger ones. Through this work, we show that the combination of RL and ABM can create an artificial environment for policy makers to observe potential and existing behaviors associated to rules of interactions and rewards.", "year": 2019, "publicationdate": "2019-09-18", "externalids": {"DOI": "10.1038/s41598-020-68447-8"}, "doi_lower": "10.1038/s41598-020-68447-8"}
{"paper_id": 235416203, "title": "Beneﬁts of Assistance over Reward Learning", "author_names": ["Rohin Shah", "Pedro J Freire", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael Dennis", "P. Abbeel", "A. Dragan", "Stuart J. Russell"], "venue": "", "abstract": null, "year": 2020, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 249926569, "title": "More on boundary conditions for warped AdS$$_3$$ in GMG", "author_names": ["S. Sajadi", "A. Hajilou"], "venue": "The European Physical Journal C", "abstract": "In this paper, we study the Aggarwal, Ciambelli, Detournay, and Somerhausen (ACDS) boundary conditions (Aggarwal et al. in JHEP 22:013, 2020) for Warped AdS$$_3$$\n \n \n 3\n \n (WAdS$$_3$$\n \n \n 3\n \n ) in the framework of General Massive Gravity (GMG) in the quadratic ensemble. We construct the phase space, the asymptotic structure, and the asymptotic symmetry algebra. We show that the global surface charges are finite, but not integrable, and also we find the conditions to make them integrable. In addition, to confirm that the phase space has the same symmetries as that of a Warped Conformal Field Theory (WCFT), we compare the bulk entropy of Warped BTZ (WBTZ) black holes with the number of states belonging to a WCFT.", "year": 2022, "publicationdate": "2022-06-22", "externalids": {"DOI": "10.1140/epjc/s10052-022-10732-z"}, "doi_lower": "10.1140/epjc/s10052-022-10732-z"}
{"paper_id": 265043220, "title": "Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation", "author_names": ["Rusheb Shah", "Quentin Feuillade--Montixi", "Soroush Pour", "Arush Tagade", "Stephen Casper", "Javier Rando"], "venue": "arXiv.org", "abstract": "Despite efforts to align large language models to produce harmless responses, they are still vulnerable to jailbreak prompts that elicit unrestricted behaviour. In this work, we investigate persona modulation as a black-box jailbreaking method to steer a target model to take on personalities that are willing to comply with harmful instructions. Rather than manually crafting prompts for each persona, we automate the generation of jailbreaks using a language model assistant. We demonstrate a range of harmful completions made possible by persona modulation, including detailed instructions for synthesising methamphetamine, building a bomb, and laundering money. These automated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is 185 times larger than before modulation (0.23%). These prompts also transfer to Claude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%, respectively. Our work reveals yet another vulnerability in commercial large language models and highlights the need for more comprehensive safeguards.", "year": 2023, "publicationdate": "2023-11-06", "externalids": {"DOI": "10.48550/arXiv.2311.03348"}, "doi_lower": "10.48550/arxiv.2311.03348"}
{"paper_id": 208268319, "title": "ColorFool: Semantic Adversarial Colorization", "author_names": ["A. Shamsabadi", "Ricardo Sánchez-Matilla", "A. Cavallaro"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Adversarial attacks that generate small Lp norm perturbations to mislead classifiers have limited success in black-box settings and with unseen classifiers. These attacks are also not robust to defenses that use denoising filters and to adversarial training procedures. Instead, adversarial attacks that generate unrestricted perturbations are more robust to defenses, are generally more successful in black-box settings and are more transferable to unseen classifiers. However, unrestricted perturbations may be noticeable to humans. In this paper, we propose a content-based black-box adversarial attack that generates unrestricted perturbations by exploiting image semantics to selectively modify colors within chosen ranges that are perceived as natural by humans. We show that the proposed approach, ColorFool, outperforms in terms of success rate, robustness to defense frameworks and transferability, five state-of-the-art adversarial attacks on two different tasks, scene and object classification, when attacking three state-of-the-art deep neural networks using three standard datasets. The source code is available at https://github.com/smartcameras/ColorFool.", "year": 2019, "publicationdate": "2019-11-25", "externalids": {"DOI": "10.1109/CVPR42600.2020.00123"}, "doi_lower": "10.1109/cvpr42600.2020.00123"}
{"paper_id": 154038254, "title": "Is this Paper Dangerous? Balancing Secrecy and Openness in Counterterrorism", "author_names": ["Jacob N. Shapiro", "David A. Siegel"], "venue": "", "abstract": null, "year": 2010, "publicationdate": "2010-02-23", "externalids": {"DOI": "10.1080/09636410903546483"}, "doi_lower": "10.1080/09636410903546483"}
{"paper_id": 264405698, "title": "Towards Understanding Sycophancy in Language Models", "author_names": ["Mrinank Sharma", "Meg Tong", "Tomasz Korbak", "D. Duvenaud", "Amanda Askell", "Samuel R. Bowman", "Newton Cheng", "Esin Durmus", "Zac Hatfield-Dodds", "Scott Johnston", "Shauna Kravec", "Tim Maxwell", "Sam McCandlish", "Kamal Ndousse", "Oliver Rausch", "Nicholas Schiefer", "Da Yan", "Miranda Zhang", "Ethan Perez"], "venue": "International Conference on Learning Representations", "abstract": "Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.", "year": 2023, "publicationdate": "2023-10-20", "externalids": {"DOI": "10.48550/arXiv.2310.13548"}, "doi_lower": "10.48550/arxiv.2310.13548"}
{"paper_id": 248085271, "title": "Correcting Robot Plans with Natural Language Feedback", "author_names": ["Pratyusha Sharma", "Balakumar Sundaralingam", "Valts Blukis", "Chris Paxton", "Tucker Hermans", "A. Torralba", "Jacob Andreas", "D. Fox"], "venue": "Robotics: Science and Systems", "abstract": "When humans design cost or goal specifications for robots, they often produce specifications that are ambiguous, underspecified, or beyond planners' ability to solve. In these cases, corrections provide a valuable tool for human-in-the-loop robot control. Corrections might take the form of new goal specifications, new constraints (e.g. to avoid specific objects), or hints for planning algorithms (e.g. to visit specific waypoints). Existing correction methods (e.g. using a joystick or direct manipulation of an end effector) require full teleoperation or real-time interaction. In this paper, we explore natural language as an expressive and flexible tool for robot correction. We describe how to map from natural language sentences to transformations of cost functions. We show that these transformations enable users to correct goals, update robot motions to accommodate additional user preferences, and recover from planning errors. These corrections can be leveraged to get 81% and 93% success rates on tasks where the original planner failed, with either one or two language corrections. Our method makes it possible to compose multiple constraints and generalizes to unseen scenes, objects, and sentences in simulated environments and real-world environments.", "year": 2022, "publicationdate": "2022-04-11", "externalids": {"DOI": "10.48550/arXiv.2204.05186"}, "doi_lower": "10.48550/arxiv.2204.05186"}
{"paper_id": 254408735, "title": "VideoDex: Learning Dexterity from Internet Videos", "author_names": ["Kenneth Shaw", "Shikhar Bahl", "Deepak Pathak"], "venue": "Conference on Robot Learning", "abstract": "To build general robotic agents that can operate in many environments, it is often imperative for the robot to collect experience in the real world. However, this is often not feasible due to safety, time, and hardware restrictions. We thus propose leveraging the next best thing as real-world experience: internet videos of humans using their hands. Visual priors, such as visual features, are often learned from videos, but we believe that more information from videos can be utilized as a stronger prior. We build a learning algorithm, VideoDex, that leverages visual, action, and physical priors from human video datasets to guide robot behavior. These actions and physical priors in the neural network dictate the typical human behavior for a particular robot task. We test our approach on a robot arm and dexterous hand-based system and show strong results on various manipulation tasks, outperforming various state-of-the-art methods. Videos at https://video-dex.github.io", "year": 2022, "publicationdate": "2022-12-08", "externalids": {"DOI": "10.48550/arXiv.2212.04498"}, "doi_lower": "10.48550/arxiv.2212.04498"}
{"paper_id": 260704242, "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models", "author_names": ["Xinyue Shen", "Zeyuan Chen", "M. Backes", "Yun Shen", "Yang Zhang"], "venue": "Conference on Computer and Communications Security", "abstract": "The misuse of large language models (LLMs) has drawn significant attention from the general public and LLM vendors. One particular type of adversarial prompt, known as jailbreak prompt, has emerged as the main attack vector to bypass the safeguards and elicit harmful content from LLMs. In this paper, employing our new framework JailbreakHub, we conduct a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023. We identify 131 jailbreak communities and discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from online Web communities to prompt-aggregation websites and 28 user accounts have consistently optimized jailbreak prompts over 100 days. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 107,250 samples across 13 forbidden scenarios. Leveraging this dataset, our experiments on six popular LLMs show that their safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify five highly effective jailbreak prompts that achieve 0.95 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and the earliest one has persisted online for over 240 days. We hope that our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.", "year": 2023, "publicationdate": "2023-08-07", "externalids": {"DOI": "10.1145/3658644.3670388"}, "doi_lower": "10.1145/3658644.3670388"}
{"paper_id": 276503526, "title": "Defining and Detecting Toxicity on Social Media: Context and Defining and Detecting Toxicity on Social Media: Context and Knowledge are Key Knowledge are Key", "author_names": ["Amit Sheth", "V. Shalin", "Ugur Kursuncu"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 258865507, "title": "Model evaluation for extreme risks", "author_names": ["Toby Shevlane", "Sebastian Farquhar", "Ben Garfinkel", "Mary Phuong", "Jess Whittlestone", "Jade Leung", "Daniel Kokotajlo", "Nahema Marchal", "Markus Anderljung", "Noam Kolt", "Lewis Ho", "Divya Siddarth", "S. Avin", "W. Hawkins", "Been Kim", "Iason Gabriel", "Vijay Bolina", "Jack Clark", "Y. Bengio", "P. Christiano", "A. Dafoe"], "venue": "arXiv.org", "abstract": "Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through\"dangerous capability evaluations\") and the propensity of models to apply their capabilities for harm (through\"alignment evaluations\"). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.15324"}, "doi_lower": "10.48550/arxiv.2305.15324"}
{"paper_id": 3334231, "title": "Forensic Isotope Analysis to Refine a Hydrologic Conceptual Model", "author_names": ["R. Bassett", "A. Steinwand", "Saeed Jorat", "Christian E. Petersen", "R. Jackson"], "venue": "Ground Water", "abstract": null, "year": 2008, "publicationdate": "2008-05-01", "externalids": {"DOI": "10.1111/j.1745-6584.2007.00421.x"}, "doi_lower": "10.1111/j.1745-6584.2007.00421.x"}
{"paper_id": 255416247, "title": "Benchmarks and Algorithms for Offline Preference-Based Reward Learning", "author_names": ["Daniel Shin", "A. Dragan", "Daniel S. Brown"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Learning a reward function from human preferences is challenging as it typically requires having a high-fidelity simulator or using expensive and potentially unsafe actual physical rollouts in the environment. However, in many tasks the agent might have access to offline data from related tasks in the same target environment. While offline data is increasingly being used to aid policy optimization via offline RL, our observation is that it can be a surprisingly rich source of information for preference learning as well. We propose an approach that uses an offline dataset to craft preference queries via pool-based active learning, learns a distribution over reward functions, and optimizes a corresponding policy via offline RL. Crucially, our proposed approach does not require actual physical rollouts or an accurate simulator for either the reward learning or policy optimization steps. To test our approach, we first evaluate existing offline RL benchmarks for their suitability for offline reward learning. Surprisingly, for many offline RL domains, we find that simply using a trivial reward function results good policy performance, making these domains ill-suited for evaluating learned rewards. To address this, we identify a subset of existing offline RL benchmarks that are well suited for offline reward learning and also propose new offline apprenticeship learning benchmarks which allow for more open-ended behaviors. When evaluated on this curated set of domains, our empirical results suggest that combining offline RL with learned human preferences can enable an agent to learn to perform novel tasks that were not explicitly shown in the offline data.", "year": 2023, "publicationdate": "2023-01-03", "externalids": {"DOI": "10.48550/arXiv.2301.01392"}, "doi_lower": "10.48550/arxiv.2301.01392"}
{"paper_id": 226222232, "title": "Eliciting Knowledge from Language Models Using Automatically Generated Prompts", "author_names": ["Taylor Shin", "Yasaman Razeghi", "Robert L Logan IV", "Eric Wallace", "Sameer Singh"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.", "year": 2020, "publicationdate": "2020-10-29", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.346"}, "doi_lower": "10.18653/v1/2020.emnlp-main.346"}
{"paper_id": 260005608, "title": "NETWORK ADEQUACY METRICS AND OVERSIGHT", "author_names": [], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 252118408, "title": "Why So Toxic?: Measuring and Triggering Toxic Behavior in Open-Domain Chatbots", "author_names": ["Waiman Si", "M. Backes", "Jeremy Blackburn", "Emiliano De Cristofaro", "G. Stringhini", "Savvas Zannettou", "Yang Zhang"], "venue": "Conference on Computer and Communications Security", "abstract": "Chatbots are used in many applications, e.g., automated agents, smart home assistants, interactive characters in online games, etc. Therefore, it is crucial to ensure they do not behave in undesired manners, providing offensive or toxic responses to users. This is not a trivial task as state-of-the-art chatbot models are trained on large, public datasets openly collected from the Internet. This paper presents a first-of-its-kind, large-scale measurement of toxicity in chatbots. We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too. We then set out to design and experiment with an attack, ToxicBuddy, which relies on fine-tuning GPT-2 to generate non-toxic queries that make chatbots respond in a toxic manner. Our extensive experimental evaluation demonstrates that our attack is effective against public chatbot models and outperforms manually-crafted malicious queries proposed by previous work. We also evaluate three defense mechanisms against ToxicBuddy, showing that they either reduce the attack performance at the cost of affecting the chatbot's utility or are only effective at mitigating a portion of the attack. This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users. Overall, we are confident that ToxicBuddy can be used as an auditing tool and that our work will pave the way toward designing more effective defenses for chatbot safety.", "year": 2022, "publicationdate": "2022-09-07", "externalids": {"DOI": "10.1145/3548606.3560599"}, "doi_lower": "10.1145/3548606.3560599"}
{"paper_id": 259244100, "title": "Dual RL: Unification and New Methods for Reinforcement and Imitation Learning", "author_names": ["Harshit S. Sikchi", "Qinqing Zheng", "Amy Zhang", "S. Niekum"], "venue": "International Conference on Learning Representations", "abstract": "The goal of reinforcement learning (RL) is to find a policy that maximizes the expected cumulative return. It has been shown that this objective can be represented as an optimization problem of state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as dual RL, is unconstrained and easier to optimize. In this work, we first cast several state-of-the-art offline RL and offline imitation learning (IL) algorithms as instances of dual RL approaches with shared structures. Such unification allows us to identify the root cause of the shortcomings of prior methods. For offline IL, our analysis shows that prior methods are based on a restrictive coverage assumption that greatly limits their performance in practice. To fix this limitation, we propose a new discriminator-free method ReCOIL that learns to imitate from arbitrary off-policy data to obtain near-expert performance. For offline RL, our analysis frames a recent offline RL method XQL in the dual framework, and we further propose a new method f-DVL that provides alternative choices to the Gumbel regression loss that fixes the known training instability issue of XQL. The performance improvements by both of our proposed methods, ReCOIL and f-DVL, in IL and RL are validated on an extensive suite of simulated robot locomotion and manipulation tasks. Project code and details can be found at this https://hari-sikchi.github.io/dual-rl.", "year": 2023, "publicationdate": "2023-02-16", "externalids": {}, "doi_lower": null}
{"paper_id": 515925, "title": "Mastering the game of Go with deep neural networks and tree search", "author_names": ["David Silver", "Aja Huang", "Chris J. Maddison", "A. Guez", "L. Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Vedavyas Panneershelvam", "Marc Lanctot", "S. Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis"], "venue": "Nature", "abstract": null, "year": 2016, "publicationdate": "2016-01-27", "externalids": {"DOI": "10.1038/nature16961"}, "doi_lower": "10.1038/nature16961"}
{"paper_id": 205261034, "title": "Mastering the game of Go without human knowledge", "author_names": ["David Silver", "Julian Schrittwieser", "K. Simonyan", "Ioannis Antonoglou", "Aja Huang", "A. Guez", "T. Hubert", "Lucas baker", "Matthew Lai", "Adrian Bolton", "Yutian Chen", "T. Lillicrap", "Fan Hui", "L. Sifre", "George van den Driessche", "T. Graepel", "D. Hassabis"], "venue": "Nature", "abstract": null, "year": 2017, "publicationdate": "2017-10-19", "externalids": {"DOI": "10.1038/nature24270"}, "doi_lower": "10.1038/nature24270"}
{"paper_id": 236236944, "title": "Reward is enough", "author_names": ["David Silver", "Satinder Singh", "Doina Precup", "R. Sutton"], "venue": "Artificial Intelligence", "abstract": null, "year": 2021, "publicationdate": "2021-10-01", "externalids": {"DOI": "10.1016/J.ARTINT.2021.103535"}, "doi_lower": "10.1016/j.artint.2021.103535"}
{"paper_id": 1450294, "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "author_names": ["K. Simonyan", "A. Vedaldi", "Andrew Zisserman"], "venue": "International Conference on Learning Representations", "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].", "year": 2013, "publicationdate": "2013-12-20", "externalids": {}, "doi_lower": null}
{"paper_id": 56895453, "title": "Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks", "author_names": ["Amanpreet Singh", "Tushar Jain", "Sainbayar Sukhbaatar"], "venue": "International Conference on Learning Representations", "abstract": "Learning when to communicate and doing that effectively is essential in multi-agent tasks. Recent works show that continuous communication allows efficient training with back-propagation in multi-agent scenarios, but have been restricted to fully-cooperative tasks. In this paper, we present Individualized Controlled Continuous Communication Model (IC3Net) which has better training efficiency than simple continuous communication model, and can be applied to semi-cooperative and competitive settings along with the cooperative settings. IC3Net controls continuous communication with a gating mechanism and uses individualized rewards foreach agent to gain better performance and scalability while fixing credit assignment issues. Using variety of tasks including StarCraft BroodWars explore and combat scenarios, we show that our network yields improved performance and convergence rates than the baselines as the scale increases. Our results convey that IC3Net agents learn when to communicate based on the scenario and profitability.", "year": 2018, "publicationdate": "2018-09-27", "externalids": {}, "doi_lower": null}
{"paper_id": 5797093, "title": "Norms as a basis for governing sociotechnical systems", "author_names": ["Munindar P. Singh"], "venue": "ACM Transactions on Intelligent Systems and Technology", "abstract": null, "year": 2013, "publicationdate": "2013-12-01", "externalids": {"DOI": "10.1145/2542182.2542203"}, "doi_lower": "10.1145/2542182.2542203"}
{"paper_id": 258509720, "title": "Defining and Characterizing Reward Gaming", "author_names": ["Joar Skalse", "Nikolaus H. R. Howe", "Dmitrii Krasheninnikov", "D. Krueger"], "venue": "Neural Information Processing Systems", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 247451078, "title": "Invariance in Policy Optimisation and Partial Identifiability in Reward Learning", "author_names": ["Joar Skalse", "Matthew Farrugia-Roberts", "Stuart J. Russell", "A. Abate", "Adam Gleave"], "venue": "International Conference on Machine Learning", "abstract": "It is often very challenging to manually design reward functions for complex, real-world tasks. To solve this, one can instead use reward learning to infer a reward function from data. However, there are often multiple reward functions that fit the data equally well, even in the infinite-data limit. This means that the reward function is only partially identifiable. In this work, we formally characterise the partial identifiability of the reward function given several popular reward learning data sources, including expert demonstrations and trajectory comparisons. We also analyse the impact of this partial identifiability for several downstream tasks, such as policy optimisation. We unify our results in a framework for comparing data sources and downstream tasks by their invariances, with implications for the design and selection of data sources for reward learning.", "year": 2022, "publicationdate": "2022-03-14", "externalids": {"DOI": "10.48550/arXiv.2203.07475"}, "doi_lower": "10.48550/arxiv.2203.07475"}
{"paper_id": 13096553, "title": "The Value Learning Problem", "author_names": ["N. Soares"], "venue": "Artificial Intelligence Safety and Security", "abstract": "Autonomous AI systems’ programmed goals can easily fall short of programmers’ intentions. Even a machine intelligent enough to understand its de-signers’ intentions would not necessarily act as intended. We discuss early ideas on how one might design smarter-than-human AI systems that can inductively learn what to value from labeled training data, and highlight questions about the construction of systems that model and act upon their operators’ preferences.", "year": 2018, "publicationdate": "2018-07-27", "externalids": {"DOI": "10.1201/9781351251389-7"}, "doi_lower": "10.1201/9781351251389-7"}
{"paper_id": 14393270, "title": "Aligning Superintelligence with Human Interests: A Technical Research Agenda", "author_names": ["N. Soares", "Benja Fallenstein"], "venue": "", "abstract": null, "year": 2015, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 65248357, "title": "Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence", "author_names": ["Stefano V. Albrecht", "J. Crandall", "R. Ramamoorthy"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": null, "year": 2015, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 9103546, "title": "Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda", "author_names": ["N. Soares", "Benya Fallenstein"], "venue": "", "abstract": null, "year": 2017, "publicationdate": null, "externalids": {"DOI": "10.1007/978-3-662-54033-6_5"}, "doi_lower": "10.1007/978-3-662-54033-6_5"}
{"paper_id": 259089339, "title": "Can large language models democratize access to dual-use biotechnology?", "author_names": ["Emily H. Soice", "R. Rocha", "Kimberlee Cordova", "Michael A. Specter", "K. Esvelt"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) such as those embedded in 'chatbots' are accelerating and democratizing research by providing comprehensible information and expertise from many different fields. However, these models may also confer easy access to dual-use technologies capable of inflicting great harm. To evaluate this risk, the 'Safeguarding the Future' course at MIT tasked non-scientist students with investigating whether LLM chatbots could be prompted to assist non-experts in causing a pandemic. In one hour, the chatbots suggested four potential pandemic pathogens, explained how they can be generated from synthetic DNA using reverse genetics, supplied the names of DNA synthesis companies unlikely to screen orders, identified detailed protocols and how to troubleshoot them, and recommended that anyone lacking the skills to perform reverse genetics engage a core facility or contract research organization. Collectively, these results suggest that LLMs will make pandemic-class agents widely accessible as soon as they are credibly identified, even to people with little or no laboratory training. Promising nonproliferation measures include pre-release evaluations of LLMs by third parties, curating training datasets to remove harmful concepts, and verifiably screening all DNA generated by synthesis providers or used by contract research organizations and robotic cloud laboratories to engineer organisms or viruses.", "year": 2023, "publicationdate": "2023-06-06", "externalids": {"DOI": "10.48550/arXiv.2306.03809"}, "doi_lower": "10.48550/arxiv.2306.03809"}
{"paper_id": 201666234, "title": "Release Strategies and the Social Impacts of Language Models", "author_names": ["Irene Solaiman", "Miles Brundage", "Jack Clark", "Amanda Askell", "Ariel Herbert-Voss", "Jeff Wu", "Alec Radford", "Jasmine Wang"], "venue": "arXiv.org", "abstract": "Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.", "year": 2019, "publicationdate": "2019-08-24", "externalids": {}, "doi_lower": null}
{"paper_id": 3330300, "title": "Multi-Agent Generative Adversarial Imitation Learning", "author_names": ["Jiaming Song", "Hongyu Ren", "Dorsa Sadigh", "Stefano Ermon"], "venue": "Neural Information Processing Systems", "abstract": "Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.", "year": 2018, "publicationdate": "2018-02-12", "externalids": {}, "doi_lower": null}
{"paper_id": 52309169, "title": "Constructing Unrestricted Adversarial Examples with Generative Models", "author_names": ["Yang Song", "Rui Shu", "Nate Kushman", "Stefano Ermon"], "venue": "Neural Information Processing Systems", "abstract": "Adversarial examples are typically constructed by perturbing an existing data point within a small matrix norm, and current defense methods are focused on guarding against this type of attack. In this paper, we propose unrestricted adversarial examples, a new threat model where the attackers are not restricted to small norm-bounded perturbations. Different from perturbation-based attacks, we propose to synthesize unrestricted adversarial examples entirely from scratch using conditional generative models. Specifically, we first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over data samples. Then, conditioned on a desired class, we search over the AC-GAN latent space to find images that are likely under the generative model and are misclassified by a target classifier. We demonstrate through human evaluation that unrestricted adversarial examples generated this way are legitimate and belong to the desired class. Our empirical results on the MNIST, SVHN, and CelebA datasets show that unrestricted adversarial examples can bypass strong adversarial training and certified defense methods designed for traditional adversarial attacks.", "year": 2018, "publicationdate": "2018-05-01", "externalids": {}, "doi_lower": null}
{"paper_id": 236363688, "title": "What overarching ethical principle should a superintelligent AI follow?", "author_names": ["A. Søvik"], "venue": "Ai & Society", "abstract": null, "year": 2021, "publicationdate": "2021-05-27", "externalids": {"DOI": "10.1007/s00146-021-01229-6"}, "doi_lower": "10.1007/s00146-021-01229-6"}
{"paper_id": 256390533, "title": "AI model GPT-3 (dis)informs us better than humans", "author_names": ["G. Spitale", "N. Biller-Andorno", "Federico Germani"], "venue": "Science Advances", "abstract": "Artificial intelligence (AI) is changing the way we create and evaluate information, and this is happening during an infodemic, which has been having marked effects on global health. Here, we evaluate whether recruited individuals can distinguish disinformation from accurate information, structured in the form of tweets, and determine whether a tweet is organic or synthetic, i.e., whether it has been written by a Twitter user or by the AI model GPT-3. The results of our preregistered study, including 697 participants, show that GPT-3 is a double-edge sword: In comparison with humans, it can produce accurate information that is easier to understand, but it can also produce more compelling disinformation. We also show that humans cannot distinguish between tweets generated by GPT-3 and written by real Twitter users. Starting from our results, we reflect on the dangers of AI for disinformation and on how information campaigns can be improved to benefit global health.", "year": 2023, "publicationdate": "2023-01-23", "externalids": {"DOI": "10.1126/sciadv.adh1850"}, "doi_lower": "10.1126/sciadv.adh1850"}
{"paper_id": 263625818, "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models", "author_names": ["Aarohi Srivastava", "Abhinav Rastogi", "Abhishek Rao", "Abu Awal Md Shoeb", "Abubakar Abid", "Adam Fisch", "Adam R. Brown", "Adam Santoro", "Aditya Gupta", "Adrià Garriga-Alonso", "Agnieszka Kluska", "Aitor Lewkowycz", "Akshat Agarwal", "Alethea Power", "Alex Ray", "Alex Warstadt", "Alexander W. Kocurek", "Ali Safaya", "Ali Tazarv", "Alice Xiang", "Alicia Parrish", "Allen Nie", "Aman Hussain", "Amanda Askell", "A. Dsouza", "Ambrose Slone", "Ameet Rahane", "Anantharaman S. Iyer", "Anders Andreassen", "Andrea Madotto", "Andrea Santilli", "Andreas Stuhlmuller", "Andrew M. Dai", "A. La", "Andrew Kyle Lampinen", "Andy Zou", "Angela Jiang", "Angelica Chen", "Anh Vuong", "Animesh Gupta", "Anna Gottardi", "Antonio Norelli", "Anu Venkatesh", "Arash Gholamidavoodi", "A. Tabassum", "Arul Menezes", "Arun Kirubarajan", "A. Mullokandov", "Ashish Sabharwal", "Austin Herrick", "Avia Efrat", "Aykut Erdem", "Ayla Karakacs", "B. R. Roberts", "B. S. Loe", "Barret Zoph", "Bartlomiej Bojanowski", "Batuhan Ozyurt", "Behnam Hedayatnia", "Behnam Neyshabur", "Benjamin Inden", "Benno Stein", "Berk Ekmekci", "Bill Yuchen Lin", "B. Howald", "Bryan Orinion", "Cameron Diao", "Cameron Dour", "Catherine Stinson", "Cedrick Argueta", "C'esar Ferri Ram'irez", "Chandan Singh", "Charles Rathkopf", "Chenlin Meng", "Chitta Baral", "Chiyu Wu", "Chris Callison-Burch", "Chris Waites", "Christian Voigt", "Christopher D. Manning", "Christopher Potts", "Cindy Ramirez", "Clara E. Rivera", "Clemencia Siro", "Colin Raffel", "Courtney Ashcraft", "Cristina Garbacea", "Damien Sileo", "Dan Garrette", "Dan Hendrycks", "D. Kilman", "Dan Roth", "Daniel Freeman", "Daniel Khashabi", "Daniel Levy", "D. Gonz'alez", "Danielle R. Perszyk", "Danny Hernandez", "Danqi Chen", "Daphne Ippolito", "Dar Gilboa", "David Dohan", "D. Drakard", "David Jurgens", "Debajyoti Datta", "Deep Ganguli", "Denis Emelin", "Denis Kleyko", "Deniz Yuret", "Derek Chen", "Derek Tam", "Dieuwke Hupkes", "Diganta Misra", "Dilyar Buzan", "Dimitri Coelho Mollo", "Diyi Yang", "Dong-Ho Lee", "Dylan Schrader", "Ekaterina Shutova", "E. D. Cubuk", "Elad Segal", "Eleanor Hagerman", "Elizabeth Barnes", "Elizabeth Donoway", "Ellie Pavlick", "Emanuele Rodolà", "Emma Lam", "Eric Chu", "Eric Tang", "Erkut Erdem", "Ernie Chang", "Ethan A. Chi", "Ethan Dyer", "E. Jerzak", "Ethan Kim", "Eunice Engefu Manyasi", "Evgenii Zheltonozhskii", "Fanyue Xia", "Fatemeh Siar", "Fernando Mart'inez-Plumed", "Francesca Happ'e", "François Chollet", "Frieda Rong", "Gaurav Mishra", "Genta Indra Winata", "Gerard de Melo", "Germán Kruszewski", "Giambattista Parascandolo", "Giorgio Mariani", "Gloria Xinyue Wang", "Gonzalo Jaimovitch-L'opez", "Gregor Betz", "Guy Gur-Ari", "Hana Galijasevic", "Hannah Kim", "Hannah Rashkin", "Hannaneh Hajishirzi", "Harsh Mehta", "H. Bogar", "Henry Shevlin", "Hinrich Schutze", "H. Yakura", "Hongming Zhang", "Hugh Mee Wong", "Ian Ng", "Isaac Noble", "Jaap Jumelet", "Jack Geissinger", "John Kernion", "Jacob Hilton", "Jaehoon Lee", "J. Fisac", "James B. Simon", "James Koppel", "James Zheng", "James Zou", "Jan Koco'n", "Jana Thompson", "Janelle Wingfield", "Jared Kaplan", "Jarema Radom", "Jascha Narain Sohl-Dickstein", "Jason Phang", "Jason Wei", "J. Yosinski", "Jekaterina Novikova", "Jelle Bosscher", "Jennifer Marsh", "Jeremy Kim", "Jeroen Taal", "Jesse Engel", "Jesujoba Oluwadara Alabi", "Jiacheng Xu", "Jiaming Song", "Jillian Tang", "Jane W Waweru", "John Burden", "John Miller", "John U. Balis", "Jonathan Batchelder", "Jonathan Berant", "Jorg Frohberg", "Jos Rozen", "J. Hernández-Orallo", "Joseph Boudeman", "J. Guerr", "Joseph Jones", "Joshua B. Tenenbaum", "Joshua S. Rule", "Joyce Chua", "Kamil Kanclerz", "Karen Livescu", "K. Krauth", "Karthik Gopalakrishnan", "Katerina Ignatyeva", "K. Markert", "Kaustubh D. Dhole", "Kevin Gimpel", "Kevin Omondi", "K. Mathewson", "Kristen Chiafullo", "Ksenia Shkaruta", "Kumar Shridhar", "Kyle McDonell", "Kyle Richardson", "Laria Reynolds", "Leo Gao", "Li Zhang", "Liam Dugan", "Lianhui Qin", "Lidia Contreras-Ochando", "Louis-philippe Morency", "Luca Moschella", "Luca Lam", "Lucy Noble", "Ludwig Schmidt", "Luheng He", "Luis Oliveros Col'on", "Luke Metz", "Lutfi Kerem cSenel", "Maarten Bosma", "Maarten Sap", "Maartje ter Hoeve", "Maheen Farooqi", "Manaal Faruqui", "Mantas Mazeika", "Marco Baturan", "Marco Marelli", "Marco Maru", "Maria Jose Ram’irez Quintana", "M. Tolkiehn", "Mario Giulianelli", "Martha Lewis", "Martin Potthast", "Matthew L. Leavitt", "Matthias Hagen", "M. Schubert", "Medina Baitemirova", "Melody Arnaud", "M. McElrath", "Michael A. Yee", "Michael Cohen", "Michael Gu", "Michael Ivanitskiy", "Michael Starritt", "M. Strube", "Michal Swkedrowski", "Michele Bevilacqua", "Michihiro Yasunaga", "Mihir Kale", "Mike Cain", "Mimee Xu", "Mirac Suzgun", "Mitch Walker", "Monica Tiwari", "Mohit Bansal", "Moin Aminnaseri", "Mor Geva", "Mozhdeh Gheini", "T. MukundVarma", "Nanyun Peng", "Nathan A. Chi", "Nayeon Lee", "Neta Gur-Ari Krakover", "Nicholas Cameron", "Nicholas Roberts", "Nick Doiron", "Nicole Martinez", "Nikita Nangia", "Niklas Deckers", "Niklas Muennighoff", "N. Keskar", "Niveditha Iyer", "Noah Constant", "Noah Fiedel", "Nuan Wen", "Oliver Zhang", "Omar Agha", "Omar Elbaghdadi", "Omer Levy", "Owain Evans", "Pablo Antonio Moreno Casares", "P. Doshi", "Pascale Fung", "Paul Pu Liang", "Paul Vicol", "Pegah Alipoormolabashi", "Peiyuan Liao", "Percy Liang", "Peter Chang", "P. Eckersley", "Phu Mon Htut", "P. Hwang", "P. Milkowski", "P. Patil", "Pouya Pezeshkpour", "Priti Oli", "Qiaozhu Mei", "Qing Lyu", "Qinlang Chen", "Rabin Banjade", "Rachel Etta Rudolph", "Raefer Gabriel", "Rahel Habacker", "Ramon Risco", "Raphael Milliere", "Rhythm Garg", "Richard Barnes", "R. Saurous", "Riku Arakawa", "Robbe Raymaekers", "Robert Frank", "Rohan Sikand", "Roman Novak", "Roman Sitelew", "Ronan Le Bras", "Rosanne Liu", "Rowan Jacobs", "Rui Zhang", "R. Salakhutdinov", "Ryan Chi", "Ryan Lee", "Ryan Stovall", "R. Teehan", "Rylan Yang", "Sahib Singh", "Saif Mohammad", "Sajant Anand", "Sam Dillavou", "Sam Shleifer", "Sam Wiseman", "Samuel Gruetter", "Samuel R. Bowman", "S. Schoenholz", "Sanghyun Han", "Sanjeev Kwatra", "Sarah A. Rous", "Sarik Ghazarian", "Sayan Ghosh", "Sean Casey", "Sebastian Bischoff", "Sebastian Gehrmann", "Sebastian Schuster", "Sepideh Sadeghi", "Shadi S. Hamdan", "Sharon Zhou", "Shashank Srivastava", "Sherry Shi", "Shikhar Singh", "Shima Asaadi", "S. Gu", "Shubh Pachchigar", "Shubham Toshniwal", "Shyam Upadhyay", "Shyamolima Debnath", "Siamak Shakeri", "Simon Thormeyer", "S. Melzi", "Siva Reddy", "S. Makini", "Soo-Hwan Lee", "Spencer Bradley Torene", "Sriharsha Hatwar", "S. Dehaene", "Stefan Divic", "Stefano Ermon", "Stella Biderman", "Stephanie Lin", "Stephen Prasad", "Steven T Piantadosi", "Stuart M. Shieber", "Summer Misherghi", "S. Kiritchenko", "Swaroop Mishra", "Tal Linzen", "Tal Schuster", "Tao Li", "Tao Yu", "Tariq Ali", "Tatsunori Hashimoto", "Te-Lin Wu", "T. Desbordes", "Theodore Rothschild", "Thomas Phan", "Tianle Wang", "Tiberius Nkinyili", "Timo Schick", "T. Kornev", "T. Tunduny", "Tobias Gerstenberg", "T. Chang", "Trishala Neeraj", "Tushar Khot", "Tyler Shultz", "Uri Shaham", "Vedant Misra", "Vera Demberg", "Victoria Nyamai", "Vikas Raunak", "V. Ramasesh", "Vinay Uday Prabhu", "Vishakh Padmakumar", "Vivek Srikumar", "W. Fedus", "W. Saunders", "William Zhang", "Wout Vossen", "Xiang Ren", "Xiaoyu Tong", "Xinran Zhao", "Xinyi Wu", "Xudong Shen", "Yadollah Yaghoobzadeh", "Yair Lakretz", "Yangqiu Song", "Yasaman Bahri", "Yejin Choi", "Yichi Yang", "Yiding Hao", "Yifu Chen", "Yonatan Belinkov", "Yu Hou", "Yu Hou", "Yuntao Bai", "Zachary Seid", "Zhuoye Zhao", "Zijian Wang", "Zijie J. Wang", "Zirui Wang", "Ziyi Wu"], "venue": "arXiv.org", "abstract": "Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit\"breakthrough\"behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.", "year": 2022, "publicationdate": "2022-06-09", "externalids": {}, "doi_lower": null}
{"paper_id": 263834989, "title": "Beyond Memorization: Violating Privacy Via Inference with Large Language Models", "author_names": ["Robin Staab", "Mark Vero", "Mislav Balunovi'c", "Martin T. Vechev"], "venue": "International Conference on Learning Representations", "abstract": "Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\\%$ top-1 and $95\\%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time ($240\\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for a wider privacy protection.", "year": 2023, "publicationdate": "2023-10-11", "externalids": {"DOI": "10.48550/arXiv.2310.07298"}, "doi_lower": "10.48550/arxiv.2310.07298"}
{"paper_id": 252386666, "title": "Assessing the ethical and social concerns of artificial intelligence in neuroinformatics research: an empirical test of the European Union Assessment List for Trustworthy AI (ALTAI)", "author_names": ["B. Stahl", "Tonii Leach"], "venue": "AI and Ethics", "abstract": "Ethical and social concerns are a key obstacle to the adoption of artificial intelligence (AI) in the life sciences and beyond. The discussion of these issues has intensified in recent years and led to a number of approaches, tools and initiatives. Key amongst them is the idea of ex-ante impact assessments that aim to identify issues at the early stages of development. One prominent example of such ex-ante impact assessment is the European Union's (EU) Assessment list for Trustworthy AI (ALTAI). This article uses the findings of a large-scale application of the ALTAI to a large neuro-informatics project as an exemplar to demonstrate the effectiveness and limitations of the ALTAI in practice. The article shows that ex-ante impact assessment has the potential to help identify and address ethical and social issues. However, they need to be understood as part of a broader socio-technical ecosystem of AI. For ALTAI and related approaches to be useful in bio-medical research, they should be interpreted from a systems theory perspective which allows for their integration into the rich set of tools, legislation and approaches. The paper argues that ex-ante impact assessments have the best chance of being successful if seen applied in conjunction with other approaches in the context of the overall AI ecosystem.", "year": 2022, "publicationdate": "2022-09-19", "externalids": {"DOI": "10.1007/s43681-022-00201-4"}, "doi_lower": "10.1007/s43681-022-00201-4"}
{"paper_id": 59222720, "title": "Forecasting Transformative AI: An Expert Survey", "author_names": ["Ross Gruetzemacher", "D. Paradice", "K. Lee"], "venue": "arXiv.org", "abstract": "Transformative AI technologies have the potential to reshape critical aspects of society in the near future. However, in order to properly prepare policy initiatives for the arrival of such technologies accurate forecasts and timelines are necessary. A survey was administered to attendees of three AI conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference). The survey included questions for estimating AI capabilities over the next decade, questions for forecasting five scenarios of transformative AI and questions concerning the impact of computational resources in AI research. Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that humans are currently paid to do) can be feasibly automated now, and that this figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts indicated a 50% probability of AI systems being capable of automating 90% of current human tasks in 25 years and 99% of current human tasks in 50 years. The conference of attendance was found to have a statistically significant impact on all forecasts, with attendees of HLAI providing more optimistic timelines with less uncertainty. These findings suggest that AI experts expect major advances in AI technology to continue over the next decade to a degree that will likely have profound transformative impacts on society.", "year": 2019, "publicationdate": "2019-01-24", "externalids": {}, "doi_lower": null}
{"paper_id": 195742658, "title": "Ways of ensuring short / long term safety", "author_names": ["B. Boeck", "B. Baltes", "F. Besnus"], "venue": "", "abstract": null, "year": 2006, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 249042390, "title": "AI-deploying organizations are key to addressing ‘perfect storm’ of AI risks", "author_names": ["Caitlin Curtis", "N. Gillespie", "Steven Lockey"], "venue": "AI and Ethics", "abstract": "We argue that a perfect storm of five conditions heightens the risk of harm to society from artificial intelligence: (1) the powerful, invisible nature of AI, (2) low public awareness and AI literacy, (3) rapid scaled deployment of AI, (4) insufficient regulation, and (5) the gap between trustworthy AI principles and practices. To prevent harm, fit-for-purpose regulation and public AI literacy programs have been recommended, but education and government regulation will not be sufficient: AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and taking accountability to mitigate the risks.", "year": 2022, "publicationdate": "2022-05-24", "externalids": {"DOI": "10.1007/s43681-022-00163-7"}, "doi_lower": "10.1007/s43681-022-00163-7"}
{"paper_id": 263874153, "title": "Learning to summarize with human feedback", "author_names": ["Nisan Stiennon", "Ouyang Long", "Jeffrey Wu", "Daniel M. Ziegler", "Ryan Lowe", "Chelsea Voss", "Alec Radford", "Dario Amodei", "P. Christiano"], "venue": "Neural Information Processing Systems", "abstract": null, "year": 2020, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 718373, "title": "Ad Hoc Autonomous Agent Teams: Collaboration without Pre-Coordination", "author_names": ["P. Stone", "G. Kaminka", "Sarit Kraus", "J. Rosenschein"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "As autonomous agents proliferate in the real world, both in software and robotic settings, they will increasingly need to band together for cooperative activities with previously unfamiliar teammates. In such ad hoc team settings, team strategies cannot be developed a priori. Rather, an agent must be prepared to cooperate with many types of teammates: it must collaborate without pre-coordination. This paper challenges the AI community to develop theory and to implement prototypes of ad hoc team agents. It defines the concept of ad hoc team agents, specifies an evaluation paradigm, and provides examples of possible theoretical and empirical approaches to challenge. The goal is to encourage progress towards this ambitious, newly realistic, and increasingly important research goal.", "year": 2010, "publicationdate": "2010-07-05", "externalids": {"DOI": "10.1609/aaai.v24i1.7529"}, "doi_lower": "10.1609/aaai.v24i1.7529"}
{"paper_id": 13754931, "title": "Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models", "author_names": ["Hendrik Strobelt", "Sebastian Gehrmann", "M. Behrisch", "Adam Perer", "H. Pfister", "Alexander M. Rush"], "venue": "IEEE Transactions on Visualization and Computer Graphics", "abstract": "Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and “what if”-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.", "year": 2018, "publicationdate": "2018-04-25", "externalids": {"DOI": "10.1109/TVCG.2018.2865044"}, "doi_lower": "10.1109/tvcg.2018.2865044"}
{"paper_id": 2560797, "title": "Toward harnessing user feedback for machine learning", "author_names": ["S. Stumpf", "Vidya Rajaram", "Lida Li", "M. Burnett", "Thomas G. Dietterich", "Erin Sullivan", "Russell Drummond", "Jonathan L. Herlocker"], "venue": "International Conference on Intelligent User Interfaces", "abstract": "There has been little research into how end users might be able to communicate advice to machine learning systems. If this resource--the users themselves--could somehow work hand-in-hand with machine learning systems, the accuracy of learning systems could be improved and the users' understanding and trust of the system could improve as well. We conducted a think-aloud study to see how willing users were to provide feedback and to understand what kinds of feedback users could give. Users were shown explanations of machine learning predictions and asked to provide feedback to improve the predictions. We found that users had no difficulty providing generous amounts of feedback. The kinds of feedback ranged from suggestions for reweighting of features to proposals for new features, feature combinations, relational features, and wholesale changes to the learning algorithm. The results show that user feedback has the potential to significantly improve machine learning systems, but that learning algorithms need to be extended in several ways to be able to assimilate this feedback.", "year": 2007, "publicationdate": "2007-01-28", "externalids": {"DOI": "10.1145/1216295.1216316"}, "doi_lower": "10.1145/1216295.1216316"}
{"paper_id": 7067423, "title": "Interacting meaningfully with machine learning systems: Three experiments", "author_names": ["S. Stumpf", "Vidya Rajaram", "Lida Li", "Weng-Keen Wong", "M. Burnett", "Thomas G. Dietterich", "Erin Sullivan", "Jonathan L. Herlocker"], "venue": "Int. J. Hum. Comput. Stud.", "abstract": null, "year": 2009, "publicationdate": "2009-08-01", "externalids": {"DOI": "10.1016/j.ijhcs.2009.03.004"}, "doi_lower": "10.1016/j.ijhcs.2009.03.004"}
{"paper_id": 222066972, "title": "Learning Rewards from Linguistic Feedback", "author_names": ["T. Sumers", "Mark K. Ho", "Robert D. Hawkins", "Karthik Narasimhan", "T. Griffiths"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "We explore unconstrained natural language feedback as a learning signal for artificial agents. Humans use rich and varied language to teach, yet most prior work on interactive learning from language assumes a particular form of input (e.g., commands). We propose a general framework which does not make this assumption, instead using aspect-based sentiment analysis to decompose feedback into sentiment over the features of a Markov decision process. We then infer the teacher's reward function by regressing the sentiment on the features, an analogue of inverse reinforcement learning. To evaluate our approach, we first collect a corpus of teaching behavior in a cooperative task where both teacher and learner are human. We implement three artificial learners: sentiment-based \"literal\" and \"pragmatic\" models, and an inference network trained end-to-end to predict rewards. We then re-run our initial experiment, pairing human teachers with these artificial learners. All three models successfully learn from interactive human feedback. The inference network approaches the performance of the \"literal\" sentiment model, while the \"pragmatic\" model nears human performance. Our work provides insight into the information structure of naturalistic linguistic feedback as well as methods to leverage it for reinforcement learning.", "year": 2020, "publicationdate": "2020-09-30", "externalids": {"DOI": "10.1609/aaai.v35i7.16749"}, "doi_lower": "10.1609/aaai.v35i7.16749"}
{"paper_id": 268628662, "title": "The AI Safety Summit at Bletchley Park Asked the Wrong Questions", "author_names": ["R. Passchier"], "venue": "The International Journal of Social Quality", "abstract": "On 1 and 2 November 2023, politicians, scientists, and leaders of tech companies gathered in Britain's Bletchley Park (Milmo and Stacey 2023) to discuss the risks of artificial intelligence (AI). The main focus points were disinformation, cyber attacks, and the (by the way probably low) likelihood that AI will soon go beyond a “frontier” and threaten life as we know it.", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.3167/ijsq.2023.130101"}, "doi_lower": "10.3167/ijsq.2023.130101"}
{"paper_id": 211677701, "title": "Scaling Up Multiagent Reinforcement Learning for Robotic Systems: Learn an Adaptive Sparse Communication Graph", "author_names": ["Chuangchuang Sun", "Macheng Shen", "J. How"], "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems", "abstract": "The complexity of multiagent reinforcement learning (MARL) in multiagent systems increases exponentially with respect to the agent number. This scalability issue prevents MARL from being applied in large-scale multiagent systems. However, one critical feature in MARL that is often neglected is that the interactions between agents are quite sparse. Without exploiting this sparsity structure, existing works aggregate information from all of the agents and thus have a high sample complexity. To address this issue, we propose an adaptive sparse attention mechanism by generalizing a sparsity-inducing activation function. Then a sparse communication graph in MARL is learned by graph neural networks based on this new attention mechanism. Through this sparsity structure, the agents can communicate in an effective as well as efficient way via only selectively attending to agents that matter the most and thus the scale of the MARL problem is reduced with little optimality compromised. Comparative results show that our algorithm can learn an interpretable sparse structure and outperforms previous works by a significant margin on applications involving a large-scale multiagent system.", "year": 2020, "publicationdate": "2020-03-02", "externalids": {"DOI": "10.1109/IROS45743.2020.9341303"}, "doi_lower": "10.1109/iros45743.2020.9341303"}
{"paper_id": 258479665, "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision", "author_names": ["Zhiqing Sun", "Yikang Shen", "Qinhong Zhou", "Hongxin Zhang", "Zhenfang Chen", "David D. Cox", "Yiming Yang", "Chuang Gan"], "venue": "Neural Information Processing Systems", "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.", "year": 2023, "publicationdate": "2023-05-04", "externalids": {"DOI": "10.48550/arXiv.2305.03047"}, "doi_lower": "10.48550/arxiv.2305.03047"}
{"paper_id": 260498115, "title": "Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward", "author_names": ["P. Sunehag", "Guy Lever", "A. Gruslys", "Wojciech M. Czarnecki", "V. Zambaldi", "Max Jaderberg", "Marc Lanctot", "Nicolas Sonnerat", "Joel Z. Leibo", "K. Tuyls", "T. Graepel"], "venue": "Adaptive Agents and Multi-Agent Systems", "abstract": null, "year": 2018, "publicationdate": "2018-07-09", "externalids": {}, "doi_lower": null}
{"paper_id": 231632433, "title": "TrafficSim: Learning to Simulate Realistic Multi-Agent Behaviors", "author_names": ["Simon Suo", "S. Regalado", "S. Casas", "R. Urtasun"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Simulation has the potential to massively scale evaluation of self-driving systems, enabling rapid development as well as safe deployment. Bridging the gap between simulation and the real world requires realistic multi-agent behaviors. Existing simulation environments rely on heuristic-based models that directly encode traffic rules, which cannot capture irregular maneuvers (e.g., nudging, U-turns) and complex interactions (e.g., yielding, merging). In contrast, we leverage real-world data to learn directly from human demonstration, and thus capture more naturalistic driving behaviors. To this end, we propose TrafficSim, a multi-agent behavior model for realistic traffic simulation. In particular, we parameterize the policy with an implicit la-tent variable model that generates socially-consistent plans for all actors in the scene jointly. To learn a robust policy amenable for long horizon simulation, we unroll the policy in training and optimize through the fully differentiable simulation across time. Our learning objective incorporates both human demonstrations as well as common sense. We show TrafficSim generates significantly more realistic traffic scenarios as compared to a diverse set of baselines. Notably, we can exploit trajectories generated by TrafficSim as effective data augmentation for training better motion planner.", "year": 2021, "publicationdate": "2021-01-17", "externalids": {"DOI": "10.1109/CVPR46437.2021.01026"}, "doi_lower": "10.1109/cvpr46437.2021.01026"}
{"paper_id": 283428236, "title": "Reinforcement Learning: An Introduction", "author_names": ["R. S. Sutton"], "venue": "IEEE Transactions on Neural Networks", "abstract": null, "year": 2005, "publicationdate": null, "externalids": {"DOI": "10.1109/tnn.2004.842673"}, "doi_lower": "10.1109/tnn.2004.842673"}
{"paper_id": 231837391, "title": "Ethically Compliant Sequential Decision Making", "author_names": ["Justin Svegliato", "Samer B. Nashed", "S. Zilberstein"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Enabling autonomous systems to comply with an ethical theory is critical given their accelerating deployment in domains that impact society. While many ethical theories have been studied extensively in moral philosophy, they are still challenging to implement by developers who build autonomous systems. This paper proposes a novel approach for building ethically compliant autonomous systems that optimize completing a task while following an ethical framework. First, we introduce a definition of an ethically compliant autonomous system and its properties. Next, we offer a range of ethical frameworks for divine command theory, prima facie duties, and virtue ethics. Finally, we demonstrate the accuracy and usability of our approach in a set of autonomous driving simulations and a user study of planning and robotics experts.", "year": 2021, "publicationdate": "2021-05-18", "externalids": {"DOI": "10.1609/aaai.v35i13.17386"}, "doi_lower": "10.1609/aaai.v35i13.17386"}
{"paper_id": 151411406, "title": "The belief in meritocracy perpetuates inequality", "author_names": ["J. Butler"], "venue": "", "abstract": null, "year": 2016, "publicationdate": "2016-07-12", "externalids": {}, "doi_lower": null}
{"paper_id": 14028349, "title": "Apprenticeship learning using linear programming", "author_names": ["Umar Syed", "Michael Bowling", "R. Schapire"], "venue": "International Conference on Machine Learning", "abstract": null, "year": 2008, "publicationdate": "2008-07-05", "externalids": {"DOI": "10.1145/1390156.1390286"}, "doi_lower": "10.1145/1390156.1390286"}
{"paper_id": 604334, "title": "Intriguing properties of neural networks", "author_names": ["Christian Szegedy", "Wojciech Zaremba", "I. Sutskever", "Joan Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "International Conference on Learning Representations", "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. \nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. \nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "year": 2013, "publicationdate": "2013-12-20", "externalids": {}, "doi_lower": null}
{"paper_id": 258467484, "title": "The Global Governance of Artificial Intelligence: Next Steps for Empirical and Normative Research", "author_names": ["J. Tallberg", "Eva Erman", "Markus Furendal", "Johannes Geith", "M. Klamberg", "Magnus Lundgren"], "venue": "Social Science Research Network", "abstract": "Artificial intelligence (AI) represents a technological upheaval with the potential to change human society. Because of its transformative potential, AI is increasingly becoming subject to regulatory initiatives at the global level. Yet, so far, scholarship in political science and international relations has focused more on AI applications than on the emerging architecture of global AI regulation. The purpose of this article is to outline an agenda for research into the global governance of AI. The article distinguishes between two broad perspectives: an empirical approach, aimed at mapping and explaining global AI governance; and a normative approach, aimed at developing and applying standards for appropriate global AI governance. The two approaches offer questions, concepts, and theories that are helpful in gaining an understanding of the emerging global governance of AI. Conversely, exploring AI as a regulatory issue offers a critical opportunity to refine existing general approaches to the study of global governance.", "year": 2023, "publicationdate": "2023-05-19", "externalids": {"DOI": "10.2139/ssrn.4424123"}, "doi_lower": "10.2139/ssrn.4424123"}
{"paper_id": 220514347, "title": "Robustifying Reinforcement Learning Agents via Action Space Adversarial Training", "author_names": ["Kai Liang Tan", "Yasaman Esfandiari", "Xian Yeow Lee", "Aakanksha", "S. Sarkar"], "venue": "American Control Conference", "abstract": "Adoption of machine learning (ML)-enabled cyber-physical systems (CPS) are becoming prevalent in various sectors of modern society such as transportation, industrial, and power grids. Recent studies in deep reinforcement learning (DRL) have demonstrated its benefits in a large variety of data-driven decisions and control applications. As reliance on ML-enabled systems grows, it is imperative to study the performance of these systems under malicious state and actuator attacks. Traditional control systems employ resilient/fault-tolerant controllers that counter these attacks by correcting the system via error observations. However, in some applications, a resilient controller may not be sufficient to avoid a catastrophic failure. Ideally, a robust approach is more useful in these scenarios where a system is inherently robust (by design) to adversarial attacks. While robust control has a long history of development, robust ML is an emerging research area that has already demonstrated its relevance and urgency. However, the majority of robust ML research has focused on perception tasks and not on decision and control tasks, although the ML (specifically RL) models used for control applications are equally vulnerable to adversarial attacks. In this paper, we show that a well-performing DRL agent that is initially susceptible to action space perturbations (e.g. actuator attacks) can be robustified against similar perturbations through adversarial training.", "year": 2020, "publicationdate": "2020-07-01", "externalids": {"DOI": "10.23919/ACC45564.2020.9147846"}, "doi_lower": "10.23919/acc45564.2020.9147846"}
{"paper_id": 260435822, "title": "Multi Agent Reinforcement Learning Independent vs Cooperative Agents", "author_names": ["Ming Tan"], "venue": "", "abstract": null, "year": 2003, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 267338108, "title": "Olive: An Instruction Following LLaMA Model For Odia Language", "author_names": ["Shantipriya Parida", "Sambit Sekhar", "Subhadarshi Panda", "Swateek Jena", "Abhijeet Parida", "Soumendra Kumar Sahoo", "S. Dash"], "venue": "2023 IEEE Silchar Subsection Conference (SILCON)", "abstract": "The AI community is experiencing a profound impact from Large Language Models (LLMs), and the introduction of ChatGPT and GPT-4 is prompting a reconsideration of the potential of artificial general intelligence(AGI). However, most of the LLMs are trained in English and other high-resource languages, resulting in the unavailability of LLM and its related technologies and services for many low-resource languages. In India, where only 10% of the population is proficient in English, the need for LLM models adapted to regional languages becomes crucial.In this paper, we emphasized the need for LLM for the low-resource Odia language by evaluating the available LLM-supporting Odia language. We describe the development process of the instruction-tuning LLM model for Odia. The developed instruction tuning Odia LLM is available freely for research and non-commercial purposes.", "year": 2023, "publicationdate": "2023-11-03", "externalids": {"DOI": "10.1109/SILCON59133.2023.10404195"}, "doi_lower": "10.1109/silcon59133.2023.10404195"}
{"paper_id": 235652039, "title": "Active Learning in Robotics: A Review of Control Principles", "author_names": ["Annalisa T. Taylor", "Thomas A. Berrueta", "T. Murphey"], "venue": "Mechatronics (Oxford)", "abstract": null, "year": 2021, "publicationdate": "2021-06-25", "externalids": {"DOI": "10.1016/j.mechatronics.2021.102576"}, "doi_lower": "10.1016/j.mechatronics.2021.102576"}
{"paper_id": 261530866, "title": "Provably safe systems: the only path to controllable AGI", "author_names": ["Max Tegmark", "Steve Omohundro"], "venue": "arXiv.org", "abstract": "We describe a path to humanity safely thriving with powerful Artificial General Intelligences (AGIs) by building them to provably satisfy human-specified requirements. We argue that this will soon be technically feasible using advanced AI for formal verification and mechanistic interpretability. We further argue that it is the only path which guarantees safe controlled AGI. We end with a list of challenge problems whose solution would contribute to this positive outcome and invite readers to join in this work.", "year": 2023, "publicationdate": "2023-09-05", "externalids": {"DOI": "10.48550/arXiv.2309.01933"}, "doi_lower": "10.48550/arxiv.2309.01933"}
{"paper_id": 191859352, "title": "Tom, Dick and ... Jack in the OED and in Sonnet 128", "author_names": ["Regula Hohl Trillini"], "venue": "", "abstract": null, "year": 2007, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 243290030, "title": "The Biden–Harris Administration is Overreaching", "author_names": [], "venue": "Beleaguered Superpower", "abstract": null, "year": 2021, "publicationdate": "2021-08-01", "externalids": {"DOI": "10.1142/9789811236198_0001"}, "doi_lower": "10.1142/9789811236198_0001"}
{"paper_id": 2242274, "title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "author_names": ["A. Thomaz", "C. Breazeal"], "venue": "Artificial Intelligence", "abstract": null, "year": 2008, "publicationdate": "2008-04-01", "externalids": {"DOI": "10.1016/j.artint.2007.09.009"}, "doi_lower": "10.1016/j.artint.2007.09.009"}
{"paper_id": 234741750, "title": "An Effective Baseline for Robustness to Distributional Shift", "author_names": ["S. Thulasidasan", "Sushil Thapa", "S. Dhaubhadel", "Gopinath Chennupati", "Tanmoy Bhattacharya", "J. Bilmes"], "venue": "International Conference on Machine Learning and Applications", "abstract": "Refraining from confidently predicting when faced with categories of inputs different from those seen during training is an important requirement for the safe deployment of deep learning systems. While simple to state, this has been a particularly challenging problem in deep learning, where models often end up making overconfident predictions in such situations. In this work, we present a simple, but highly effective approach to deal with out-of-distribution detection that uses the principle of abstention: when encountering a sample from an unseen class, the desired behavior is to abstain from predicting. Our approach uses a network with an extra abstention class and is trained on a dataset that is augmented with an uncurated set that consists of a large number of out-of-distribution (OoD) samples that are assigned the label of the abstention class; the model is then trained to learn an effective discriminator between in and out-of-distribution samples. We compare this relatively simple approach against a wide variety of more complex methods that have been proposed both for out-of-distribution detection as well as uncertainty modeling in deep learning, and empirically demonstrate its effectiveness on a wide variety of of benchmarks and deep architectures for image recognition and text classification, often outperforming existing approaches by significant margins. Given the simplicity and effectiveness of this method, we propose that this approach be used as a new additional baseline for future work in this domain.", "year": 2021, "publicationdate": "2021-05-15", "externalids": {"DOI": "10.1109/ICMLA52953.2021.00050"}, "doi_lower": "10.1109/icmla52953.2021.00050"}
{"paper_id": 257426979, "title": "Causal Confusion and Reward Misidentification in Preference-Based Reward Learning", "author_names": ["J. Tien", "Jerry Zhi-Yang He", "Zackory Erickson", "A. Dragan", "Daniel S. Brown"], "venue": "International Conference on Learning Representations", "abstract": "Learning policies via preference-based reward learning is an increasingly popular method for customizing agent behavior, but has been shown anecdotally to be prone to spurious correlations and reward hacking behaviors. While much prior work focuses on causal confusion in reinforcement learning and behavioral cloning, we focus on a systematic study of causal confusion and reward misidentification when learning from preferences. In particular, we perform a series of sensitivity and ablation analyses on several benchmark domains where rewards learned from preferences achieve minimal test error but fail to generalize to out-of-distribution states -- resulting in poor policy performance when optimized. We find that the presence of non-causal distractor features, noise in the stated preferences, and partial state observability can all exacerbate reward misidentification. We also identify a set of methods with which to interpret misidentified learned rewards. In general, we observe that optimizing misidentified rewards drives the policy off the reward's training distribution, resulting in high predicted (learned) rewards but low true rewards. These findings illuminate the susceptibility of preference learning to reward misidentification and causal confusion -- failure to consider even one of many factors can result in unexpected, undesirable behavior.", "year": 2022, "publicationdate": "2022-04-13", "externalids": {}, "doi_lower": null}
{"paper_id": 2413610, "title": "Domain randomization for transferring deep neural networks from simulation to the real world", "author_names": ["Joshua Tobin", "Rachel Fong", "Alex Ray", "Jonas Schneider", "Wojciech Zaremba", "P. Abbeel"], "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems", "abstract": "Bridging the ‘reality gap’ that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.", "year": 2017, "publicationdate": "2017-03-20", "externalids": {"DOI": "10.1109/IROS.2017.8202133"}, "doi_lower": "10.1109/iros.2017.8202133"}
{"paper_id": 250556875, "title": "Implementations in Machine Ethics", "author_names": ["Suzanne Tolmeijer", "Markus Kneer", "Cristina Sarasua"], "venue": "ACM Computing Surveys", "abstract": "Increasingly complex and autonomous systems require machine ethics to maximize the benefits and minimize the risks to society arising from the new technology. It is challenging to decide which type of ethical theory to employ and how to implement it effectively. This survey provides a threefold contribution. First, it introduces a trimorphic taxonomy to analyze machine ethics implementations with respect to their object (ethical theories), as well as their nontechnical and technical aspects. Second, an exhaustive selection and description of relevant works is presented. Third, applying the new taxonomy to the selected works, dominant research patterns, and lessons for the field are identified, and future directions for research are suggested.", "year": 2020, "publicationdate": "2020-01-21", "externalids": {"DOI": "10.1145/3419633"}, "doi_lower": "10.1145/3419633"}
{"paper_id": 23206414, "title": "Behavioral Cloning from Observation", "author_names": ["F. Torabi", "Garrett Warnell", "P. Stone"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Humans often learn how to perform tasks via imitation: they observe others perform a task, and then very quickly infer the appropriate actions to take based on their observations. While extending this paradigm to autonomous agents is a well-studied problem in general, there are two particular aspects that have largely been overlooked: (1) that the learning is done from observation only (i.e., without explicit action information), and (2) that the learning is typically done very quickly. In this work, we propose a two-phase, autonomous imitation learning technique called behavioral cloning from observation (BCO), that aims to provide improved performance with respect to both of these aspects. First, we allow the agent to acquire experience in a self-supervised fashion. This experience is used to develop a model which is then utilized to learn a particular task by observing an expert perform that task without the knowledge of the specific actions taken. We experimentally compare BCO to imitation learning methods, including the state-of-the-art, generative adversarial imitation learning (GAIL) technique, and we show comparable task performance in several different simulation domains while exhibiting increased learning speed after expert trajectories become available.", "year": 2018, "publicationdate": "2018-05-04", "externalids": {"DOI": "10.24963/ijcai.2018/687"}, "doi_lower": "10.24963/ijcai.2018/687"}
{"paper_id": 259950998, "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "author_names": ["Hugo Touvron", "Louis Martin", "Kevin R. Stone", "Peter Albert", "Amjad Almahairi", "Yasmine Babaei", "Niko-lay Bashlykov", "Soumya Batra", "Prajjwal Bhargava", "Shruti Bhosale", "D. Bikel", "Lukas Blecher", "Cris-tian Cantón Ferrer", "Moya Chen", "Guillem Cucurull", "David Esiobu", "Jude Fernandes", "J. Fu", "Wenyin Fu", "Brian Fuller", "Cynthia Gao", "Vedanuj Goswami", "Naman Goyal", "A. Hartshorn", "Saghar Hosseini", "Rui Hou", "Hakan Inan", "Marcin Kardas", "Viktor Kerkez", "Madian Khabsa", "Isabel M. Kloumann", "A. Korenev", "Punit Singh Koura", "M. Lachaux", "Thibaut Lavril", "Jenya Lee", "Diana Liskovich", "Yinghai Lu", "Yuning Mao", "Xavier Martinet", "Todor Mihaylov", "Pushkar Mishra", "Igor Molybog", "Yixin Nie", "Andrew Poulton", "J. Reizenstein", "Rashi Rungta", "Kalyan Saladi", "A. Schelten", "Ruan Silva", "Eric Michael Smith", "R. Subramanian", "Xia Tan", "Binh Tang", "Ross Taylor", "Adina Williams", "Jian Xiang Kuan", "Puxin Xu", "Zhengxu Yan", "Iliyan Zarov", "Yuchen Zhang", "Angela Fan", "M. Kambadur", "Sharan Narang", "Aur'elien Rodriguez", "Robert Stojnic", "Sergey Edunov", "Thomas Scialom"], "venue": "arXiv.org", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "year": 2023, "publicationdate": "2023-07-18", "externalids": {}, "doi_lower": null}
{"paper_id": 261338959, "title": "International Governance of Civilian AI: A Jurisdictional Certification Approach", "author_names": ["Robert Trager", "Ben Harack", "Anka Reuel", "A. Carnegie", "Lennart Heim", "Lewis Ho", "S. Kreps", "R. Lall", "Owen Larter", "Se'an 'O h'Eigeartaigh", "Simon Staffell", "Jos'e Jaime Villalobos"], "venue": "Social Science Research Network", "abstract": "This report describes trade-offs in the design of international governance arrangements for civilian artificial intelligence (AI) and presents one approach in detail. This approach represents the extension of a standards, licensing, and liability regime to the global level. We propose that states establish an International AI Organization (IAIO) to certify state jurisdictions (not firms or AI projects) for compliance with international oversight standards. States can give force to these international standards by adopting regulations prohibiting the import of goods whose supply chains embody AI from non-IAIO-certified jurisdictions. This borrows attributes from models of existing international organizations, such as the International Civilian Aviation Organization (ICAO), the International Maritime Organization (IMO), and the Financial Action Task Force (FATF). States can also adopt multilateral controls on the export of AI product inputs, such as specialized hardware, to non-certified jurisdictions. Indeed, both the import and export standards could be required for certification. As international actors reach consensus on risks of and minimum standards for advanced AI, a jurisdictional certification regime could mitigate a broad range of potential harms, including threats to public safety.", "year": 2023, "publicationdate": "2023-08-29", "externalids": {"DOI": "10.48550/arXiv.2308.15514"}, "doi_lower": "10.48550/arxiv.2308.15514"}
{"paper_id": 235422106, "title": "A New Formalism, Method and Open Issues for Zero-Shot Coordination", "author_names": ["Johannes Treutlein", "Michael Dennis", "Caspar Oesterheld", "J. Foerster"], "venue": "International Conference on Machine Learning", "abstract": "In many coordination problems, independently reasoning humans are able to discover mutually compatible policies. In contrast, independently trained self-play policies are often mutually incompatible. Zero-shot coordination (ZSC) has recently been proposed as a new frontier in multi-agent reinforcement learning to address this fundamental issue. Prior work approaches the ZSC problem by assuming players can agree on a shared learning algorithm but not on labels for actions and observations, and proposes other-play as an optimal solution. However, until now, this\"label-free\"problem has only been informally defined. We formalize this setting as the label-free coordination (LFC) problem by defining the label-free coordination game. We show that other-play is not an optimal solution to the LFC problem as it fails to consistently break ties between incompatible maximizers of the other-play objective. We introduce an extension of the algorithm, other-play with tie-breaking, and prove that it is optimal in the LFC problem and an equilibrium in the LFC game. Since arbitrary tie-breaking is precisely what the ZSC setting aims to prevent, we conclude that the LFC problem does not reflect the aims of ZSC. To address this, we introduce an alternative informal operationalization of ZSC as a starting point for future work.", "year": 2021, "publicationdate": "2021-06-11", "externalids": {}, "doi_lower": null}
{"paper_id": 11608263, "title": "Multi-Label Classification: An Overview", "author_names": ["Grigorios Tsoumakas", "I. Katakis"], "venue": "International Journal of Data Warehousing and Mining", "abstract": "Multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization, and semantic scene classification. This article introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative experimental results of certain multilabel classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set.", "year": 2007, "publicationdate": "2007-07-01", "externalids": {"DOI": "10.4018/jdwm.2007070101"}, "doi_lower": "10.4018/jdwm.2007070101"}
{"paper_id": 253676531, "title": "Classification of global catastrophic risks connected with artificial intelligence", "author_names": ["Alexey Turchin", "D. Denkenberger"], "venue": "Ai & Society", "abstract": null, "year": 2018, "publicationdate": "2018-05-03", "externalids": {"DOI": "10.1007/s00146-018-0845-5"}, "doi_lower": "10.1007/s00146-018-0845-5"}
{"paper_id": 257885211, "title": "The Problem of Hard and Easy Problems", "author_names": ["T. Baetu"], "venue": "Canadian Journal of Philosophy", "abstract": "David Chalmers advocates the view that the phenomenon of consciousness is fundamentally different from all other phenomena studied in the life sciences, positing a uniquely hard problem that precludes the possibility of a mechanistic explanation. In this paper, I evaluate three demarcation criteria for dividing phenomena into hard and easy problems: functional definability, the puzzle of the accompanying phenomenon, and the first-person data of subjective experience. I argue that none of the proposed criteria can accurately discriminate between the phenomenon of consciousness and mechanistically explainable phenomena.", "year": 2023, "publicationdate": "2023-03-31", "externalids": {"DOI": "10.1017/can.2022.46"}, "doi_lower": "10.1017/can.2022.46"}
{"paper_id": 219573607, "title": "Avoiding Side Effects in Complex Environments", "author_names": ["A. Turner", "Neale Ratzlaff", "Prasad Tadepalli"], "venue": "Neural Information Processing Systems", "abstract": "Reward function specification can be difficult, even in simple environments. Realistic environments contain millions of states. Rewarding the agent for making a widget may be easy, but penalizing the multitude of possible negative side effects is hard. In toy environments, Attainable Utility Preservation (AUP) avoids side effects by penalizing shifts in the ability to achieve randomly generated goals. We scale this approach to large, randomly generated environments based on Conway's Game of Life. By preserving optimal value for a single randomly generated reward function, AUP incurs modest overhead, completes the specified task, and avoids side effects.", "year": 2020, "publicationdate": "2020-06-01", "externalids": {}, "doi_lower": null}
{"paper_id": 227261694, "title": "Optimal Policies Tend To Seek Power", "author_names": ["A. Turner", "Logan Smith", "Rohin Shah", "Andrew Critch", "Prasad Tadepalli"], "venue": "Neural Information Processing Systems", "abstract": "Some researchers speculate that intelligent reinforcement learning (RL) agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers point out that RL agents need not have human-like power-seeking instincts. To clarify this discussion, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes, we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.", "year": 2019, "publicationdate": "2019-12-03", "externalids": {}, "doi_lower": null}
{"paper_id": 258556812, "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting", "author_names": ["Miles Turpin", "Julian Michael", "Ethan Perez", "Sam Bowman"], "venue": "Neural Information Processing Systems", "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always\"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.", "year": 2023, "publicationdate": "2023-05-07", "externalids": {"DOI": "10.48550/arXiv.2305.04388"}, "doi_lower": "10.48550/arxiv.2305.04388"}
{"paper_id": 249246710, "title": "A Legal Study on the UNESCO’s ‘the Recommendation on the Ethics of Artificial Intelligence’", "author_names": ["S. Hong"], "venue": "The Journal of Legal Studies", "abstract": null, "year": 2022, "publicationdate": "2022-04-30", "externalids": {"DOI": "10.35223/gnulaw.30.2.13"}, "doi_lower": "10.35223/gnulaw.30.2.13"}
{"paper_id": 276483112, "title": "What is Human-Centered AI and Why Does It Matter?", "author_names": ["Cecilia Aragon"], "venue": "Technical Symposium on Computer Science Education", "abstract": "There have been extraordinary advances in our ability to collect, analyze, and interpret vast amounts of data which have transformed the fundamental nature of artificial intelligence (AI). The human aspects of AI, including how to support creativity and human insight without violating individual rights, how to address ethical concerns, and the consideration of societal impacts, have received less attention. Yet these human issues are becoming increasingly vital to the future of AI. Dr. Aragon will reflect on a 30-year career in data science and AI in industry, government, and academia, discuss what it means for AI to be both rigorous and human-centered, and speculate upon future directions for data science and AI.", "year": 2025, "publicationdate": "2025-02-12", "externalids": {"DOI": "10.1145/3641554.3704629"}, "doi_lower": "10.1145/3641554.3704629"}
{"paper_id": 253869530, "title": "Beyond Eight Billion: Why Population Continues to Matter for Global Development", "author_names": ["A. Ezeh"], "venue": "", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.31899/pdr2022.1004"}, "doi_lower": "10.31899/pdr2022.1004"}
{"paper_id": 247302391, "title": "Dual use of artificial-intelligence-powered drug discovery", "author_names": ["Fabio Urbina", "Filippa Lentzos", "Cédric Invernizzi", "S. Ekins"], "venue": "Nature Machine Intelligence", "abstract": null, "year": 2022, "publicationdate": "2022-03-01", "externalids": {"DOI": "10.1038/s42256-022-00465-9"}, "doi_lower": "10.1038/s42256-022-00465-9"}
{"paper_id": 12969169, "title": "Development of prosocial, individualistic, and competitive orientations: theory and preliminary evidence.", "author_names": ["VU Research", "Portal", "van Lange", "P.A.M. Otten", "W. D. Bruin", "E.N.M. Joireman", "P. Lange", "W. Otten", "E. M. N. D. Bruin", "J. Joireman", "Hal Kelley", "C. Rusbult", "Ben R. Slugoski", "A. M. V. Lange"], "venue": "Journal of Personality and Social Psychology", "abstract": null, "year": 1997, "publicationdate": "1997-10-01", "externalids": {"DOI": "10.1037//0022-3514.73.4.733"}, "doi_lower": "10.1037//0022-3514.73.4.733"}
{"paper_id": 15348764, "title": "Principles of Risk Minimization for Learning Theory", "author_names": ["V. Vapnik"], "venue": "Neural Information Processing Systems", "abstract": null, "year": 1991, "publicationdate": "1991-12-02", "externalids": {}, "doi_lower": null}
{"paper_id": 202750077, "title": "Attention Interpretability Across NLP Tasks", "author_names": ["Shikhar Vashishth", "Shyam Upadhyay", "Gaurav Singh Tomar", "Manaal Faruqui"], "venue": "arXiv.org", "abstract": "The attention layer in a neural network model provides insights into the model's reasoning behind its prediction, which are usually criticized for being opaque. Recently, seemingly contradictory viewpoints have emerged about the interpretability of attention weights (Jain & Wallace, 2019; Vig & Belinkov, 2019). Amid such confusion arises the need to understand attention mechanism more systematically. In this work, we attempt to fill this gap by giving a comprehensive explanation which justifies both kinds of observations (i.e., when is attention interpretable and when it is not). Through a series of experiments on diverse NLP tasks, we validate our observations and reinforce our claim of interpretability of attention through manual evaluation.", "year": 2019, "publicationdate": "2019-09-24", "externalids": {}, "doi_lower": null}
{"paper_id": 210179175, "title": "AJIT RAM VERMA", "author_names": ["A. Verma"], "venue": "", "abstract": null, "year": 2015, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 49561627, "title": "Fairness Definitions Explained", "author_names": ["Sahil Verma", "J. Rubin"], "venue": "2018 IEEE/ACM International Workshop on Software Fairness (FairWare)", "abstract": "Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.", "year": 2018, "publicationdate": "2018-05-29", "externalids": {"DOI": "10.1145/3194770.3194776"}, "doi_lower": "10.1145/3194770.3194776"}
{"paper_id": 251739648, "title": "The ﬂip side of FLIP", "author_names": ["M. Peter"], "venue": "", "abstract": null, "year": 2004, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 189762556, "title": "A Multiscale Visualization of Attention in the Transformer Model", "author_names": ["Jesse Vig"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.", "year": 2019, "publicationdate": "2019-06-12", "externalids": {"DOI": "10.18653/v1/P19-3007"}, "doi_lower": "10.18653/v1/p19-3007"}
{"paper_id": 143425657, "title": "The role of artificial intelligence in achieving the Sustainable Development Goals", "author_names": ["Ricardo Vinuesa", "Hossein Azizpour", "Iolanda Leite", "Madeline Balaam", "Virginia Dignum", "S. Domisch", "Anna Felländer", "S. Langhans", "Max Tegmark", "F. F. Nerini"], "venue": "Nature Communications", "abstract": "The emergence of artificial intelligence (AI) and its progressively wider impact on many sectors requires an assessment of its effect on the achievement of the Sustainable Development Goals. Using a consensus-based expert elicitation process, we find that AI can enable the accomplishment of 134 targets across all the goals, but it may also inhibit 59 targets. However, current research foci overlook important aspects. The fast development of AI needs to be supported by the necessary regulatory insight and oversight for AI-based technologies to enable sustainable development. Failure to do so could result in gaps in transparency, safety, and ethical standards. Artificial intelligence (AI) is becoming more and more common in people’s lives. Here, the authors use an expert elicitation method to understand how AI may affect the achievement of the Sustainable Development Goals.", "year": 2019, "publicationdate": "2019-04-30", "externalids": {"DOI": "10.1038/s41467-019-14108-y"}, "doi_lower": "10.1038/s41467-019-14108-y"}
{"paper_id": 201698258, "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP", "author_names": ["Eric Wallace", "Shi Feng", "Nikhil Kandpal", "Matt Gardner", "Sameer Singh"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94% to 0.55%, 72% of “why” questions in SQuAD to be answered “to kill american people”, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.", "year": 2019, "publicationdate": "2019-08-20", "externalids": {"DOI": "10.18653/v1/D19-1221"}, "doi_lower": "10.18653/v1/d19-1221"}
{"paper_id": 269294048, "title": "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions", "author_names": ["Eric Wallace", "Kai Xiao", "R. Leike", "Lilian Weng", "Johannes Heidecke", "Alex Beutel"], "venue": "arXiv.org", "abstract": "Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. In this work, we argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, we propose an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. We apply this method to GPT-3.5, showing that it drastically increases robustness -- even for attack types not seen during training -- while imposing minimal degradations on standard capabilities.", "year": 2024, "publicationdate": "2024-04-19", "externalids": {"DOI": "10.48550/arXiv.2404.13208"}, "doi_lower": "10.48550/arxiv.2404.13208"}
{"paper_id": 263142109, "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints", "author_names": ["Chaoqi Wang", "Yibo Jiang", "Chenghao Yang", "Han Liu", "Yuxin Chen"], "venue": "International Conference on Learning Representations", "abstract": "The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative, and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the need for estimating the normalizing constant in the Bradley-Terry model and enables a tractable mapping between the reward function and the optimal policy. Our approach optimizes LLMs to align with human preferences in a more efficient and supervised manner under a broad set of divergence constraints. Empirically, adopting these divergences ensures a balance between alignment performance and generation diversity. Importantly, $f$-DPO outperforms PPO-based methods in divergence efficiency, and divergence constraints directly influence expected calibration error (ECE).", "year": 2023, "publicationdate": "2023-09-28", "externalids": {"DOI": "10.48550/arXiv.2309.16240"}, "doi_lower": "10.48550/arxiv.2309.16240"}
{"paper_id": 174800362, "title": "Improving Neural Language Modeling via Adversarial Training", "author_names": ["Dilin Wang", "Chengyue Gong", "Qiang Liu"], "venue": "International Conference on Machine Learning", "abstract": "Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting. In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed-form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively. When applied to machine translation, our method improves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.", "year": 2019, "publicationdate": "2019-05-24", "externalids": {}, "doi_lower": null}
{"paper_id": 9631227, "title": "Falling Rule Lists", "author_names": ["Fulton Wang", "C. Rudin"], "venue": "International Conference on Artificial Intelligence and Statistics", "abstract": "Falling rule lists are classification models consisting of an ordered list of if-then rules, where (i) the order of rules determines which example should be classified by each rule, and (ii) the estimated probability of success decreases monotonically down the list. These kinds of rule lists are inspired by healthcare applications where patients would be stratified into risk sets and the highest at-risk patients should be considered first. We provide a Bayesian framework for learning falling rule lists that does not rely on traditional greedy decision tree learning methods.", "year": 2014, "publicationdate": "2014-11-21", "externalids": {}, "doi_lower": null}
{"paper_id": 244072479, "title": "Interpretable Image Recognition by Constructing Transparent Embedding Space", "author_names": ["Jiaqi Wang", "Huafeng Liu", "Xinyue Wang", "L. Jing"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Humans usually explain their reasoning (e.g. classification) by dissecting the image and pointing out the evidence from these parts to the concepts in their minds. Inspired by this cognitive process, several part-level interpretable neural network architectures have been proposed to explain the predictions. However, they suffer from the complex data structure and confusing the effect of the individual part to output category. In this work, an interpretable image recognition deep network is designed by introducing a plug-in transparent embedding space (TesNet) to bridge the high-level input patches (e.g. CNN feature maps) and the out- put categories. This plug-in embedding space is spanned by transparent basis concepts which are constructed on the Grassmann manifold. These basis concepts are enforced to be category-aware and within-category concepts are orthogonal to each other, which makes sure the embedding space is disentangled. Meanwhile, each basis concept can be traced back to the particular image patches, thus they are transparent and friendly to explain the reasoning process. By comparing with state-of-the-art interpretable methods, TesNet is much more beneficial to classification tasks, esp. providing better interpretability on predictions and improve the final accuracy. The code is available at https://github.com/JackeyWang96/TesNet.", "year": 2021, "publicationdate": "2021-10-01", "externalids": {"DOI": "10.1109/ICCV48922.2021.00093"}, "doi_lower": "10.1109/iccv48922.2021.00093"}
{"paper_id": 260410697, "title": "Learning Robotic Insertion Tasks From Human Demonstration", "author_names": ["Kaimeng Wang", "Yu Zhao", "I. Sakuma"], "venue": "IEEE Robotics and Automation Letters", "abstract": "Robotic insertion tasks often rely on delicate manual tuning due to the complexity of contact dynamics. In contrast, human is remarkably efficient in these tasks. In this context, Programming by Demonstration (PbD) has gained much traction since it shows the possibility for robots to learn new skills by observing human demonstration. However, existing PbD approaches suffer from the high cost of demonstration data collection, and low robustness to task uncertainties. In order to address these issues, we propose a new PbD-based learning framework for robotic insertion tasks. This framework includes a new demonstration data acquisition system, which replaces the expensive motion capture device with deep learning based hand pose tracking algorithm and a low-cost RGBD camera. A latent skill-guided reinforcement learning (RL) approach is also included for safe, efficient, and robust human-robot skill transfer, in which risky explorations are prevented by the reward function design and safety constraints in action space. A series of peg-hole-insertion experiments on a FANUC industrial robot are conducted to illustrate the effectiveness of the proposed approach.", "year": 2023, "publicationdate": "2023-09-01", "externalids": {"DOI": "10.1109/LRA.2023.3300238"}, "doi_lower": "10.1109/lra.2023.3300238"}
{"paper_id": 253244237, "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small", "author_names": ["Kevin Wang", "Alexandre Variengien", "Arthur Conmy", "Buck Shlegeris", "J. Steinhardt"], "venue": "International Conference on Learning Representations", "abstract": "Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior\"in the wild\"in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.", "year": 2022, "publicationdate": "2022-11-01", "externalids": {"DOI": "10.48550/arXiv.2211.00593"}, "doi_lower": "10.48550/arxiv.2211.00593"}
{"paper_id": 261064713, "title": "A survey on large language model based autonomous agents", "author_names": ["Lei Wang", "Chengbang Ma", "Xueyang Feng", "Zeyu Zhang", "Hao-ran Yang", "Jingsen Zhang", "Zhi-Yang Chen", "Jiakai Tang", "Xu Chen", "Yankai Lin", "Wayne Xin Zhao", "Zhewei Wei", "Ji-rong Wen"], "venue": "Frontiers of Computer Science", "abstract": "Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.", "year": 2023, "publicationdate": "2023-08-22", "externalids": {"DOI": "10.1007/s11704-024-40231-1"}, "doi_lower": "10.1007/s11704-024-40231-1"}
{"paper_id": 195795426, "title": "POET: open-ended coevolution of environments and their optimized solutions", "author_names": ["Rui Wang", "J. Lehman", "J. Clune", "Kenneth O. Stanley"], "venue": "Annual Conference on Genetic and Evolutionary Computation", "abstract": "How can progress in machine learning and reinforcement learning be automated to generate its own never-ending curriculum of challenges without human intervention? The recent emergence of quality diversity (QD) algorithms offers a glimpse of the potential for such continual open-ended invention. For example, novelty search showcases the benefits of explicit novelty pressure, MAP-Elites and Innovation Engines highlight the advantage of explicit elitism within niches in an otherwise divergent process, and minimal criterion coevolution (MCC) reveals that problems and solutions can coevolve divergently. The Paired Open-Ended Trailblazer (POET) algorithm introduced in this paper combines these principles to produce a practical approach to generating an endless progression of diverse and increasingly challenging environments while at the same time explicitly optimizing their solutions. An intriguing implication is the opportunity to transfer solutions among environments, reflecting the view that innovation is a circuitous and unpredictable process. POET is tested in a 2-D obstacles course domain, where it generates diverse and sophisticated behaviors that create and solve a wide range of environmental challenges, many of which cannot be solved by direct optimization, or by a direct-path curriculum-building control algorithm. We hope that POET will inspire a new push towards open-ended discovery across many domains.", "year": 2019, "publicationdate": "2019-07-13", "externalids": {"DOI": "10.1145/3321707.3321799"}, "doi_lower": "10.1145/3321707.3321799"}
{"paper_id": 234681239, "title": "Emergent Prosociality in Multi-Agent Games Through Gifting", "author_names": ["Woodrow Z. Wang", "M. Beliaev", "Erdem Biyik", "Daniel A. Lazar", "Ramtin Pedarsani", "Dorsa Sadigh"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Coordination is often critical to forming prosocial behaviors -- behaviors that increase the overall sum of rewards received by all agents in a multi-agent game. However, state of the art reinforcement learning algorithms often suffer from converging to socially less desirable equilibria when multiple equilibria exist. Previous works address this challenge with explicit reward shaping, which requires the strong assumption that agents can be forced to be prosocial. We propose using a less restrictive peer-rewarding mechanism, gifting, that guides the agents toward more socially desirable equilibria while allowing agents to remain selfish and decentralized. Gifting allows each agent to give some of their reward to other agents. We employ a theoretical framework that captures the benefit of gifting in converging to the prosocial equilibrium by characterizing the equilibria's basins of attraction in a dynamical system. With gifting, we demonstrate increased convergence of high risk, general-sum coordination games to the prosocial equilibrium both via numerical analysis and experiments.", "year": 2021, "publicationdate": "2021-05-13", "externalids": {"DOI": "10.24963/ijcai.2021/61"}, "doi_lower": "10.24963/ijcai.2021/61"}
{"paper_id": 254877310, "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "author_names": ["Yizhong Wang", "Yeganeh Kordi", "Swaroop Mishra", "Alisa Liu", "Noah A. Smith", "Daniel Khashabi", "Hannaneh Hajishirzi"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10560"}, "doi_lower": "10.48550/arxiv.2212.10560"}
{"paper_id": 218487237, "title": "Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints", "author_names": ["Zhenyi Wang", "Xiaoyang Wang", "Bang An", "Dong Yu", "Changyou Chen"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Text generation from a knowledge base aims to translate knowledge triples to natural language descriptions. Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table. In this paper, for the first time, we propose a novel Transformer-based generation framework to achieve the goal. The core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a table-text embedding similarity loss based on the Transformer model. Furthermore, to evaluate faithfulness, we propose a new automatic metric specialized to the table-to-text generation problem. We also provide detailed analysis on each component of our model in our experiments. Automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin.", "year": 2020, "publicationdate": "2020-05-03", "externalids": {"DOI": "10.18653/v1/2020.acl-main.101"}, "doi_lower": "10.18653/v1/2020.acl-main.101"}
{"paper_id": 256827116, "title": "Counter-GAP: Counterfactual Bias Evaluation through Gendered Ambiguous Pronouns", "author_names": ["Zhongbin Xie", "Vid Kocijan", "Thomas Lukasiewicz", "Oana-Maria Camburu"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Bias-measuring datasets play a critical role in detecting biased behavior of language models and in evaluating progress of bias mitigation methods. In this work, we focus on evaluating gender bias through coreference resolution, where previous datasets are either hand-crafted or fail to reliably measure an explicitly defined bias. To overcome these shortcomings, we propose a novel method to collect diverse, natural, and minimally distant text pairs via counterfactual generation, and construct Counter-GAP, an annotated dataset consisting of 4008 instances grouped into 1002 quadruples. We further identify a bias cancellation problem in previous group-level metrics on Counter-GAP, and propose to use the difference between inconsistency across genders and within genders to measure bias at a quadruple level. Our results show that four pre-trained language models are significantly more inconsistent across different gender groups than within each group, and that a name-based counterfactual data augmentation method is more effective to mitigate such bias than an anonymization-based method.", "year": 2023, "publicationdate": "2023-02-11", "externalids": {"DOI": "10.48550/arXiv.2302.05674"}, "doi_lower": "10.48550/arxiv.2302.05674"}
{"paper_id": 259342528, "title": "Jailbroken: How Does LLM Safety Training Fail?", "author_names": ["Alexander Wei", "Nika Haghtalab", "J. Steinhardt"], "venue": "Neural Information Processing Systems", "abstract": "Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of\"jailbreak\"attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes.", "year": 2023, "publicationdate": "2023-07-05", "externalids": {"DOI": "10.48550/arXiv.2307.02483"}, "doi_lower": "10.48550/arxiv.2307.02483"}
{"paper_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "author_names": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "Ed H. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "venue": "Neural Information Processing Systems", "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 86429095, "title": "Game and Evolutionary Game Theory", "author_names": ["Hui-jia Li", "Guijun Li", "Jun Hu"], "venue": "", "abstract": null, "year": 2019, "publicationdate": "2019-03-30", "externalids": {}, "doi_lower": null}
{"paper_id": 258311654, "title": "Using the Veil of Ignorance to align AI systems with principles of justice", "author_names": ["Laura Weidinger", "Kevin R. McKee", "Richard Everett", "Saffron Huang", "Tina Zhu", "Martin Chadwick", "C. Summerfield", "Iason Gabriel"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "abstract": "Significance The growing integration of Artificial Intelligence (AI) into society raises a critical question: How can principles be fairly selected to govern these systems? Across five studies, with a total of 2,508 participants, we use the Veil of Ignorance to select principles to align AI systems. Compared to participants who know their position, participants behind the veil more frequently choose, and endorse upon reflection, principles for AI that prioritize the worst-off. This pattern is driven by increased consideration of fairness, rather than by political orientation or attitudes to risk. Our findings suggest that the Veil of Ignorance may be a suitable process for selecting principles to govern real-world applications of AI.", "year": 2023, "publicationdate": "2023-04-24", "externalids": {"DOI": "10.1073/pnas.2213709120"}, "doi_lower": "10.1073/pnas.2213709120"}
{"paper_id": 280068849, "title": "On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks", "author_names": ["Stephen Obadinma", "Xiaodan Zhu"], "venue": "arXiv.org", "abstract": "Robust verbal confidence generated by large language models (LLMs) is crucial for the deployment of LLMs to ensure transparency, trust, and safety in human-AI interactions across many high-stakes applications. In this paper, we present the first comprehensive study on the robustness of verbal confidence under adversarial attacks. We introduce a novel framework for attacking verbal confidence scores through both perturbation and jailbreak-based methods, and show that these attacks can significantly jeopardize verbal confidence estimates and lead to frequent answer changes. We examine a variety of prompting strategies, model sizes, and application domains, revealing that current confidence elicitation methods are vulnerable and that commonly used defence techniques are largely ineffective or counterproductive. Our findings underscore the urgent need to design more robust mechanisms for confidence expression in LLMs, as even subtle semantic-preserving modifications can lead to misleading confidence in responses.", "year": 2025, "publicationdate": "2025-07-08", "externalids": {"DOI": "10.48550/arXiv.2507.06489"}, "doi_lower": "10.48550/arxiv.2507.06489"}
{"paper_id": 277983858, "title": "Conceptual Foundations of LLM-Powered Agents: From Language Processing to Autonomous Reasoning", "author_names": ["Yifei Jian"], "venue": "Scientific Journal of Intelligent Systems Research", "abstract": "Large language models (LLMs) have transformed artificial intelligence by enabling advanced natural language processing and generation. However, their evolution toward autonomous agents require additional capabilities, including structured reasoning, adaptive learning, and interaction with external environments. This article explores the progression from early NLP models to modern LLM-based agents, detailing their core mechanisms, key capabilities, and applications. Additionally, we discuss the challenges in developing fully autonomous AI systems, such as alignment issues, computational efficiency, and decision-making constraints. Finally, we examine future trends, including reinforcement learning integration, enhanced memory architectures, and humanAI collaboration, which will shape the next generation of intelligent agents.", "year": 2025, "publicationdate": "2025-04-20", "externalids": {"DOI": "10.54691/atmxvb43"}, "doi_lower": "10.54691/atmxvb43"}
{"paper_id": 115908882, "title": "Service automation robots and the future of work", "author_names": ["L. Willcocks", "Mary C. Lacity"], "venue": "", "abstract": null, "year": 2016, "publicationdate": "2016-02-12", "externalids": {}, "doi_lower": null}
{"paper_id": 3178759, "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "author_names": ["J. Weston", "Antoine Bordes", "S. Chopra", "Tomas Mikolov"], "venue": "International Conference on Learning Representations", "abstract": "One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.", "year": 2015, "publicationdate": "2015-02-19", "externalids": {}, "doi_lower": null}
{"paper_id": 280134502, "title": "AI Ethics and Regulations: Ensuring Trustworthy AI", "author_names": ["Pericles 'Asher' Rospigliosi"], "venue": "International Journal of Artificial Intelligence for Science (IJAI4S)", "abstract": "As Artificial Intelligence (AI) technologies become increasingly embedded in critical aspects of modern life—ranging from healthcare diagnostics and financial forecasting to autonomous vehicles, law enforcement, education, and national security—the urgency of addressing their ethical implications has grown exponentially. While AI systems offer unprecedented efficiencies and capabilities, they also present significant risks, including algorithmic bias, opaque decisionmaking processes, data exploitation, invasion of privacy, digital surveillance, job displacement, and the amplification of societal inequalities. These risks are particularly acute in high-stakes domains where errors or unchecked use can result in irreversible harm or systemic injustice. This paper offers a comprehensive examination of the evolving ethical landscape surrounding AI development and deployment. It explores foundational ethical principles such as fairness, accountability, transparency, and human-centered design, alongside contemporary challenges introduced by machine learning models, deep learning algorithms, and autonomous decision systems. Special attention is given to the global regulatory landscape, comparing initiatives such as the European Union’s AI Act, the U.S. Blueprint for an AI Bill of Rights, and guidelines from organizations like UNESCO and the OECD. The paper also examines the growing role of interdisciplinary AI ethics teams, algorithmic auditing, and impact assessments. Ultimately, the paper proposes a strategic roadmap for building ethical AI ecosystems grounded in inclusivity, explainability, legal compliance, and social well-being. It emphasizes that aligning AI development with democratic values, human dignity, and global equity is not merely desirable— but essential—for ensuring that the future of AI serves humanity as a whole, rather than a privileged few.", "year": 2025, "publicationdate": "2025-07-11", "externalids": {"DOI": "10.63619/ijai4s.v1i2.004"}, "doi_lower": "10.63619/ijai4s.v1i2.004"}
{"paper_id": 256416209, "title": "Emergence of Maps in the Memories of Blind Navigation Agents", "author_names": ["Erik Wijmans", "M. Savva", "Irfan Essa", "Stefan Lee", "Ari S. Morcos", "Dhruv Batra"], "venue": "International Conference on Learning Representations", "abstract": "Decades of research into intelligent animal navigation posits that organisms build and maintain internal spatial representations (or maps)1 of their environment, that enables the organism to determine and follow task-appropriate paths (Epstein, Patai, Julian, & Spiers, 2017; O'keefe & Nadel, 1978; Tollman, 1948). Hamsters, wolves, chimpanzees, and bats leverage prior exploration to determine and follow shortcuts they may never have taken before (Chapuis & Scardigli, 1993; Harten, Katz, Goldshtein, Handel, & Yovel, 2020; Menzel, 1973; Peters, 1976; Toledo et al., 2020). Even blind mole rats and animals rendered situationally-blind in dark environments demonstrate shortcut behaviors (Avni, Tzvaigrach, & Eilam, 2008; Kimchi, Etienne, & Terkel, 2004; Maaswinkel & Whishaw, 1999). Ants forage for food along meandering paths but take near-optimal return trips (Müller & Wehner, 1988), though there is some controversy about whether insects like ants and bees are capable of forming maps (Cheung et al., 2014; Cruse & Wehner, 2011).", "year": 2023, "publicationdate": "2023-01-30", "externalids": {"DOI": "10.1145/3609468.3609471"}, "doi_lower": "10.1145/3609468.3609471"}
{"paper_id": 265213271, "title": "Artificial General Intelligence, Existential Risk, and Human Risk Perception", "author_names": ["David R. Mandel"], "venue": "arXiv.org", "abstract": "Artificial general intelligence (AGI) does not yet exist, but given the pace of technological development in artificial intelligence, it is projected to reach human-level intelligence within roughly the next two decades. After that, many experts expect it to far surpass human intelligence and to do so rapidly. The prospect of superintelligent AGI poses an existential risk to humans because there is no reliable method for ensuring that AGI goals stay aligned with human goals. Drawing on publicly available forecaster and opinion data, the author examines how experts and non-experts perceive risk from AGI. The findings indicate that the perceived risk of a world catastrophe or extinction from AGI is greater than for other existential risks. The increase in perceived risk over the last year is also steeper for AGI than for other existential threats (e.g., nuclear war or human-caused climate change). That AGI is a pressing existential risk is something on which experts and non-experts agree, but the basis for such agreement currently remains obscure.", "year": 2023, "publicationdate": "2023-11-15", "externalids": {"DOI": "10.48550/arXiv.2311.08698"}, "doi_lower": "10.48550/arxiv.2311.08698"}
{"paper_id": 1482925, "title": "Evolution of digital organisms at high mutation rates leads to survival of the flattest", "author_names": ["C. Wilke", "Jia Lan Wang", "Charles Ofria", "R. Lenski", "C. Adami"], "venue": "Nature", "abstract": null, "year": 2001, "publicationdate": "2001-07-19", "externalids": {"DOI": "10.1038/35085569"}, "doi_lower": "10.1038/35085569"}
{"paper_id": 77393713, "title": "Machine Ethics: The Design and Governance of Ethical AI and Autonomous Systems", "author_names": ["A. Winfield", "K. Michael", "J. Pitt", "V. Evers"], "venue": "Proceedings of the IEEE", "abstract": "The so-called fourth industrial revolution and its economic and societal implications are no longer solely an academic concern, but a matter for political as well as public debate. Characterized as the convergence of robotics, AI, autonomous systems and information technology – or cyberphysical systems – the fourth industrial revolution was the focus of the World Economic Forum, at Davos, in 2016 [1] . Also in 2016 the US White House initiated a series of public workshops on artificial intelligence (AI) and the creation of an interagency working group, and the European Parliament Committee for Legal Affairs published a draft report with recommendations to the Commission on Civil Law Rules on Robotics.", "year": 2019, "publicationdate": "2019-03-01", "externalids": {"DOI": "10.1109/JPROC.2019.2900622"}, "doi_lower": "10.1109/jproc.2019.2900622"}
{"paper_id": 19335246, "title": "A survey of transfer learning methods for reinforcement learning", "author_names": ["N. Bone"], "venue": "", "abstract": null, "year": 2008, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 6049287, "title": "Preference-Based Reinforcement Learning: A Preliminary Survey", "author_names": ["Christian Wirth", "Johannes Fürnkranz"], "venue": "", "abstract": null, "year": 2013, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 237593001, "title": "Recursively Summarizing Books with Human Feedback", "author_names": ["Jeff Wu", "Long Ouyang", "Daniel M. Ziegler", "Nissan Stiennon", "Ryan Lowe", "Jan Leike", "P. Christiano"], "venue": "arXiv.org", "abstract": "A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\\sim5\\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.", "year": 2021, "publicationdate": "2021-09-22", "externalids": {}, "doi_lower": null}
{"paper_id": 19153664, "title": "A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents", "author_names": ["Yueh-Hua Wu", "Shou-De Lin"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "This paper proposes a low-cost, easily realizable strategy to equip a reinforcement learning (RL) agent the capability of behaving ethically. Our model allows the designers of RL agents to solely focus on the task to achieve, without having to worry about the implementation of multiple trivial ethical patterns to follow. Based on the assumption that the majority of human behavior, regardless which goals they are achieving, is ethical, our design integrates human policy with the RL policy to achieve the target objective with less chance of violating the ethical code that human beings normally obey.", "year": 2017, "publicationdate": "2017-12-12", "externalids": {"DOI": "10.1609/aaai.v32i1.11498"}, "doi_lower": "10.1609/aaai.v32i1.11498"}
{"paper_id": 259064099, "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training", "author_names": ["Zeqiu Wu", "Yushi Hu", "Weijia Shi", "Nouha Dziri", "Alane Suhr", "Prithviraj Ammanabrolu", "Noah A. Smith", "Mari Ostendorf", "Hannaneh Hajishirzi"], "venue": "Neural Information Processing Systems", "abstract": "Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.", "year": 2023, "publicationdate": "2023-06-02", "externalids": {"DOI": "10.48550/arXiv.2306.01693"}, "doi_lower": "10.48550/arxiv.2306.01693"}
{"paper_id": 6060248, "title": "Ex Machina: Personal Attacks Seen at Scale", "author_names": ["Ellery Wulczyn", "Nithum Thain", "Lucas Dixon"], "venue": "The Web Conference", "abstract": "The damage personal attacks cause to online discourse motivates many platforms to try to curb the phenomenon. However, understanding the prevalence and impact of personal attacks in online platforms at scale remains surprisingly difficult. The contribution of this paper is to develop and illustrate a method that combines crowdsourcing and machine learning to analyze personal attacks at scale. We show an evaluation method for a classifier in terms of the aggregated number of crowd-workers it can approximate. We apply our methodology to English Wikipedia, generating a corpus of over 100k high quality human-labeled comments and 63M machine-labeled ones from a classifier that is as good as the aggregate of 3 crowd-workers, as measured by the area under the ROC curve and Spearman correlation. Using this corpus of machine-labeled scores, our methodology allows us to explore some of the open questions about the nature of online personal attacks. This reveals that the majority of personal attacks on Wikipedia are not the result of a few malicious users, nor primarily the consequence of allowing anonymous contributions from unregistered users.", "year": 2016, "publicationdate": "2016-10-27", "externalids": {"DOI": "10.1145/3038912.3052591"}, "doi_lower": "10.1145/3038912.3052591"}
{"paper_id": 275901979, "title": "The rise and potential of large language model based agents: a survey", "author_names": ["Zhiheng Xi", "Wenxiang Chen", "Xin Guo", "Wei He", "Yiwen Ding", "Boyang Hong", "Ming Zhang", "Junzhe Wang", "Senjie Jin", "Enyu Zhou", "Rui Zheng", "Xiaoran Fan", "Xiao Wang", "Limao Xiong", "Yuhao Zhou", "Weiran Wang", "Changhao Jiang", "Yicheng Zou", "Xiangyang Liu", "Zhangyue Yin", "Shihan Dou", "Rongxiang Weng", "Wenjuan Qin", "Yongyan Zheng", "Xipeng Qiu", "Xuanjing Huang", "Qi Zhang", "Tao Gui"], "venue": "Science China Information Sciences", "abstract": null, "year": 2025, "publicationdate": "2025-01-17", "externalids": {"DOI": "10.1007/s11432-024-4222-0"}, "doi_lower": "10.1007/s11432-024-4222-0"}
{"paper_id": 266289038, "title": "Defending ChatGPT against jailbreak attack via self-reminders", "author_names": ["Yueqi Xie", "Jingwei Yi", "Jiawei Shao", "Justin Curl", "Lingjuan Lyu", "Qifeng Chen", "Xing Xie", "Fangzhao Wu"], "venue": "Nature Machine Intelligence", "abstract": null, "year": 2023, "publicationdate": "2023-12-01", "externalids": {"DOI": "10.1038/s42256-023-00765-8"}, "doi_lower": "10.1038/s42256-023-00765-8"}
{"paper_id": 44106659, "title": "FairGAN: Fairness-aware Generative Adversarial Networks", "author_names": ["Depeng Xu", "Shuhan Yuan", "Lu Zhang", "Xintao Wu"], "venue": "2018 IEEE International Conference on Big Data (Big Data)", "abstract": "Fairness-aware learning is increasingly important in data mining. Discrimination prevention aims to prevent discrimination in the training data before it is used to conduct predictive analysis. In this paper, we focus on fair data generation that ensures the generated data is discrimination free. Inspired by generative adversarial networks (GAN), we present fairness-aware generative adversarial networks, called FairGAN, which are able to learn a generator producing fair data and also preserving good data utility. Compared with the naive fair data generation models, FairGAN further ensures the classifiers which are trained on generated data can achieve fair classification on real data. Experiments on a real dataset show the effectiveness of FairGAN.", "year": 2018, "publicationdate": "2018-05-28", "externalids": {"DOI": "10.1109/BigData.2018.8622525"}, "doi_lower": "10.1109/bigdata.2018.8622525"}
{"paper_id": 222341902, "title": "Recipes for Safety in Open-domain Chatbots", "author_names": ["Jing Xu", "Da Ju", "Margaret Li", "Y-Lan Boureau", "J. Weston", "Emily Dinan"], "venue": "arXiv.org", "abstract": "Models trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior and unwanted biases. We investigate a variety of methods to mitigate these issues in the context of open-domain generative dialogue models. We introduce a new human-and-model-in-the-loop framework for both training safer models and for evaluating them, as well as a novel method to distill safety considerations inside generative models without the use of an external classifier at deployment time. We conduct experiments comparing these methods and find our new techniques are (i) safer than existing models as measured by automatic and human evaluations while (ii) maintaining usability metrics such as engagingness relative to the state of the art. We then discuss the limitations of this work by analyzing failure cases of our models.", "year": 2020, "publicationdate": "2020-10-14", "externalids": {}, "doi_lower": null}
{"paper_id": 235097625, "title": "Bot-Adversarial Dialogue for Safe Conversational Agents", "author_names": ["Jing Xu", "Da Ju", "Margaret Li", "Y-Lan Boureau", "J. Weston", "Emily Dinan"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Conversational agents trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior. We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses. We then go on to propose two novel methods for safe conversational agents, by either training on data from our new human-and-model-in-the-loop framework in a two-stage system, or ”baking-in” safety to the generative model itself. We find our new techniques are (i) safer than existing models; while (ii) maintaining usability metrics such as engagingness relative to state-of-the-art chatbots. In contrast, we expose serious safety issues in existing standard systems like GPT2, DialoGPT, and BlenderBot.", "year": 2021, "publicationdate": "2021-06-01", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.235"}, "doi_lower": "10.18653/v1/2021.naacl-main.235"}
{"paper_id": 10306523, "title": "A Semantic Loss Function for Deep Learning with Symbolic Knowledge", "author_names": ["Jingyi Xu", "Zilu Zhang", "Tal Friedman", "Yitao Liang", "Guy Van den Broeck"], "venue": "International Conference on Machine Learning", "abstract": "This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that it effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.", "year": 2017, "publicationdate": "2017-11-29", "externalids": {}, "doi_lower": null}
{"paper_id": 1738452, "title": "Deep Interactive Object Selection", "author_names": ["N. Xu", "Brian L. Price", "Scott D. Cohen", "Jimei Yang", "Thomas S. Huang"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Interactive object selection is a very important research problem and has many applications. Previous algorithms require substantial user interactions to estimate the foreground and background distributions. In this paper, we present a novel deep-learning-based algorithm which has much better understanding of objectness and can reduce user interactions to just a few clicks. Our algorithm transforms user-provided positive and negative clicks into two Euclidean distance maps which are then concatenated with the RGB channels of images to compose (image, user interactions) pairs. We generate many of such pairs by combining several random sampling strategies to model users' click patterns and use them to finetune deep Fully Convolutional Networks (FCNs). Finally the output probability maps of our FCN-8s model is integrated with graph cut optimization to refine the boundary segments. Our model is trained on the PASCAL segmentation dataset and evaluated on other datasets with different object classes. Experimental results on both seen and unseen objects demonstrate that our algorithm has a good generalization ability and is superior to all existing interactive object selection approaches.", "year": 2016, "publicationdate": "2016-03-13", "externalids": {"DOI": "10.1109/CVPR.2016.47"}, "doi_lower": "10.1109/cvpr.2016.47"}
{"paper_id": 240070335, "title": "TRAIL: Near-Optimal Imitation Learning with Suboptimal Data", "author_names": ["Mengjiao Yang", "S. Levine", "Ofir Nachum"], "venue": "International Conference on Learning Representations", "abstract": "The aim in imitation learning is to learn effective policies by utilizing near-optimal expert demonstrations. However, high-quality demonstrations from human experts can be expensive to obtain in large numbers. On the other hand, it is often much easier to obtain large quantities of suboptimal or task-agnostic trajectories, which are not useful for direct imitation, but can nevertheless provide insight into the dynamical structure of the environment, showing what could be done in the environment even if not what should be done. We ask the question, is it possible to utilize such suboptimal offline datasets to facilitate provably improved downstream imitation learning? In this work, we answer this question affirmatively and present training objectives that use offline datasets to learn a factored transition model whose structure enables the extraction of a latent action space. Our theoretical analysis shows that the learned latent action space can boost the sample-efficiency of downstream imitation learning, effectively reducing the need for large near-optimal expert datasets through the use of auxiliary non-expert data. To learn the latent action space in practice, we propose TRAIL (Transition-Reparametrized Actions for Imitation Learning), an algorithm that learns an energy-based transition model contrastively, and uses the transition model to reparametrize the action space for sample-efficient imitation learning. We evaluate the practicality of our objective through experiments on a set of navigation and locomotion tasks. Our results verify the benefits suggested by our theory and show that TRAIL is able to improve baseline imitation learning by up to 4x in performance.", "year": 2021, "publicationdate": "2021-10-27", "externalids": {}, "doi_lower": null}
{"paper_id": 257378587, "title": "Foundation Models for Decision Making: Problems, Methods, and Opportunities", "author_names": ["Sherry Yang", "Ofir Nachum", "Yilun Du", "Jason Wei", "P. Abbeel", "D. Schuurmans"], "venue": "arXiv.org", "abstract": "Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.", "year": 2023, "publicationdate": "2023-03-07", "externalids": {"DOI": "10.48550/arXiv.2303.04129"}, "doi_lower": "10.48550/arxiv.2303.04129"}
{"paper_id": 263620436, "title": "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models", "author_names": ["Xianjun Yang", "Xiao Wang", "Qi Zhang", "L. Petzold", "William Yang Wang", "Xun Zhao", "Dahua Lin"], "venue": "arXiv.org", "abstract": "Warning: This paper contains examples of harmful language, and reader discretion is recommended. The increasing open release of powerful large language models (LLMs) has facilitated the development of downstream applications by reducing the essential cost of data annotation and computation. To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these safely aligned LLMs can be easily subverted to generate harmful content. Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of data can elicit safely-aligned models to adapt to harmful tasks without sacrificing model helpfulness. Remarkably, the subverted models retain their capability to respond appropriately to regular inquiries. Experiments across 8 models released by 5 different organizations (LLaMa-2, Falcon, InternLM, BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack. Besides, the single-turn English-only attack successfully transfers to multi-turn dialogue and other languages. This study serves as a clarion call for a collective effort to overhaul and fortify the safety of open-source LLMs against malicious attackers.", "year": 2023, "publicationdate": "2023-10-04", "externalids": {"DOI": "10.48550/arXiv.2310.02949"}, "doi_lower": "10.48550/arxiv.2310.02949"}
{"paper_id": 256615216, "title": "Improving Domain Generalization with Domain Relations", "author_names": ["Huaxiu Yao", "Xinyu Yang", "Xinyi Pan", "Shengchao Liu", "Pang Wei Koh", "Chelsea Finn"], "venue": "International Conference on Learning Representations", "abstract": "Distribution shift presents a significant challenge in machine learning, where models often underperform during the test stage when faced with a different distribution than the one they were trained on. This paper focuses on domain shifts, which occur when the model is applied to new domains that are different from the ones it was trained on, and propose a new approach called D$^3$G. Unlike previous methods that aim to learn a single model that is domain invariant, D$^3$G leverages domain similarities based on domain metadata to learn domain-specific models. Concretely, D$^3$G learns a set of training-domain-specific functions during the training stage and reweights them based on domain relations during the test stage. These domain relations can be directly obtained and learned from domain metadata. Under mild assumptions, we theoretically prove that using domain relations to reweight training-domain-specific functions achieves stronger out-of-domain generalization compared to the conventional averaging approach. Empirically, we evaluate the effectiveness of D$^3$G using real-world datasets for tasks such as temperature regression, land use classification, and molecule-protein binding affinity prediction. Our results show that D$^3$G consistently outperforms state-of-the-art methods.", "year": 2023, "publicationdate": "2023-02-06", "externalids": {}, "doi_lower": null}
{"paper_id": 260293142, "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control", "author_names": ["Anthony Brohan", "Noah Brown", "Justice Carbajal", "Yevgen Chebotar", "K. Choromanski", "Tianli Ding", "Danny Driess", "Kumar Avinava Dubey", "Chelsea Finn", "Peter R. Florence", "Chuyuan Fu", "Montse Gonzalez Arenas", "K. Gopalakrishnan", "Kehang Han", "Karol Hausman", "Alexander Herzog", "Jasmine Hsu", "Brian Ichter", "A. Irpan", "Nikhil J. Joshi", "Ryan C. Julian", "Dmitry Kalashnikov", "Yuheng Kuang", "Isabel Leal", "S. Levine", "H. Michalewski", "Igor Mordatch", "Karl Pertsch", "Kanishka Rao", "Krista Reymann", "M. Ryoo", "Grecia Salazar", "Pannag R. Sanketi", "P. Sermanet", "Jaspiar Singh", "Anikait Singh", "Radu Soricut", "Huong Tran", "Vincent Vanhoucke", "Q. Vuong", "Ayzaan Wahid", "Stefan Welker", "Paul Wohlhart", "Ted Xiao", "Tianhe Yu", "Brianna Zitkovich"], "venue": "Conference on Robot Learning", "abstract": "We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).", "year": 2023, "publicationdate": "2023-07-28", "externalids": {"DOI": "10.48550/arXiv.2307.15818"}, "doi_lower": "10.48550/arxiv.2307.15818"}
{"paper_id": 263620377, "title": "Low-Resource Languages Jailbreak GPT-4", "author_names": ["Zheng-Xin Yong", "Cristina Menghini", "Stephen H. Bach"], "venue": "arXiv.org", "abstract": "AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift: this deficiency now poses a risk to all LLMs users. Publicly available translation APIs enable anyone to exploit LLMs' safety vulnerabilities. Therefore, our work calls for a more holistic red-teaming efforts to develop robust multilingual safeguards with wide language coverage.", "year": 2023, "publicationdate": "2023-10-03", "externalids": {"DOI": "10.48550/arXiv.2310.02446"}, "doi_lower": "10.48550/arxiv.2310.02446"}
{"paper_id": 237385862, "title": "Towards Improving Adversarial Training of NLP Models", "author_names": ["Jin Yong Yoo", "Yanjun Qi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Adversarial training, a method for learning robust deep neural networks, constructs adversarial examples during training. However, recent methods for generating NLP adversarial examples involve combinatorial search and expensive sentence encoders for constraining the generated instances. As a result, it remains challenging to use vanilla adversarial training to improve NLP models' performance, and the benefits are mainly uninvestigated. This paper proposes a simple and improved vanilla adversarial training process for NLP models, which we name Attacking to Training (A2T). The core part of A2T is a new and cheaper word substitution attack optimized for vanilla adversarial training. We use A2T to train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI datasets. Our results empirically show that it is possible to train robust NLP models using a much cheaper adversary. We demonstrate that vanilla adversarial training with A2T can improve an NLP model's robustness to the attack it was originally trained with and also defend the model against other types of word substitution attacks. Furthermore, we show that A2T can improve NLP models' standard accuracy, cross-domain generalization, and interpretability. Code is available at https://github.com/QData/Textattack-A2T .", "year": 2021, "publicationdate": "2021-09-01", "externalids": {"DOI": "10.18653/v1/2021.findings-emnlp.81"}, "doi_lower": "10.18653/v1/2021.findings-emnlp.81"}
{"paper_id": 201645209, "title": "Reinforcement Learning in Healthcare: A Survey", "author_names": ["Chao Yu", "Jiming Liu", "S. Nemati"], "venue": "ACM Computing Surveys", "abstract": "As a subfield of machine learning, reinforcement learning (RL) aims at optimizing decision making by using interaction samples of an agent with its environment and the potentially delayed feedbacks. In contrast to traditional supervised learning that typically relies on one-shot, exhaustive, and supervised reward signals, RL tackles sequential decision-making problems with sampled, evaluative, and delayed feedbacks simultaneously. Such a distinctive feature makes RL techniques a suitable candidate for developing powerful solutions in various healthcare domains, where diagnosing decisions or treatment regimes are usually characterized by a prolonged period with delayed feedbacks. By first briefly examining theoretical foundations and key methods in RL research, this survey provides an extensive overview of RL applications in a variety of healthcare domains, ranging from dynamic treatment regimes in chronic diseases and critical care, automated medical diagnosis, and many other control or scheduling problems that have infiltrated every aspect of the healthcare system. In addition, we discuss the challenges and open issues in the current research and highlight some potential solutions and directions for future research.", "year": 2019, "publicationdate": "2019-08-22", "externalids": {"DOI": "10.1145/3477600"}, "doi_lower": "10.1145/3477600"}
{"paper_id": 51607811, "title": "Building Ethics into Artificial Intelligence", "author_names": ["Han Yu", "Zhiqi Shen", "C. Miao", "Cyril Leung", "V. Lesser", "Qiang Yang"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.", "year": 2018, "publicationdate": "2018-07-01", "externalids": {"DOI": "10.24963/ijcai.2018/779"}, "doi_lower": "10.24963/ijcai.2018/779"}
{"paper_id": 259164906, "title": "Language to Rewards for Robotic Skill Synthesis", "author_names": ["Wenhao Yu", "Nimrod Gileadi", "Chuyuan Fu", "Sean Kirmani", "Kuang-Huei Lee", "Montse Gonzalez Arenas", "H. Chiang", "Tom Erez", "Leonard Hasenclever", "Jan Humplik", "Brian Ichter", "Ted Xiao", "Peng Xu", "Andy Zeng", "Tingnan Zhang", "N. Heess", "Dorsa Sadigh", "Jie Tan", "Yuval Tassa", "F. Xia"], "venue": "Conference on Robot Learning", "abstract": "Large language models (LLMs) have demonstrated exciting progress in acquiring diverse new capabilities through in-context learning, ranging from logical reasoning to code-writing. Robotics researchers have also explored using LLMs to advance the capabilities of robotic control. However, since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot. On the other hand, reward functions are shown to be flexible representations that can be optimized for control policies to achieve diverse tasks, while their semantic richness makes them suitable to be specified by LLMs. In this work, we introduce a new paradigm that harnesses this realization by utilizing LLMs to define reward parameters that can be optimized and accomplish variety of robotic tasks. Using reward as the intermediate interface generated by LLMs, we can effectively bridge the gap between high-level language instructions or corrections to low-level robot actions. Meanwhile, combining this with a real-time optimizer, MuJoCo MPC, empowers an interactive behavior creation experience where users can immediately observe the results and provide feedback to the system. To systematically evaluate the performance of our proposed method, we designed a total of 17 tasks for a simulated quadruped robot and a dexterous manipulator robot. We demonstrate that our proposed method reliably tackles 90% of the designed tasks, while a baseline using primitive skills as the interface with Code-as-policies achieves 50% of the tasks. We further validated our method on a real robot arm where complex manipulation skills such as non-prehensile pushing emerge through our interactive system.", "year": 2023, "publicationdate": "2023-06-14", "externalids": {"DOI": "10.48550/arXiv.2306.08647"}, "doi_lower": "10.48550/arxiv.2306.08647"}
{"paper_id": 51609598, "title": "Towards Sample Efficient Reinforcement Learning", "author_names": ["Yang Yu"], "venue": "International Joint Conference on Artificial Intelligence", "abstract": "Reinforcement learning is a major tool to realize intelligent agents that can be autonomously adaptive to the environment. With deep models, reinforcement learning has shown great potential in complex tasks such as playing games from pixels. However, current reinforcement learning techniques are still suffer from requiring a huge amount of interaction data, which could result in unbearable cost in real-world applications. In this article, we share our understanding of the problem, and discuss possible ways to alleviate the sample cost of reinforcement learning, from the aspects of exploration, optimization, environment modeling, experience transfer, and abstraction. We also discuss some challenges in real-world applications, with the hope of inspiring future researches.", "year": 2018, "publicationdate": "2018-07-01", "externalids": {"DOI": "10.24963/ijcai.2018/820"}, "doi_lower": "10.24963/ijcai.2018/820"}
{"paper_id": 258059818, "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears", "author_names": ["Zheng Yuan", "Hongyi Yuan", "Chuanqi Tan", "Wei Wang", "Songfang Huang", "Feiran Huang"], "venue": "Neural Information Processing Systems", "abstract": "Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). However, PPO is sensitive to hyperparameters and requires multiple models in its standard implementation, making it hard to train and scale up to larger parameter counts. In contrast, we propose a novel learning paradigm called RRHF, which scores sampled responses from different sources via a logarithm of conditional probabilities and learns to align these probabilities with human preferences through ranking loss. RRHF can leverage sampled responses from various sources including the model responses from itself, other large language model responses, and human expert responses to learn to rank them. RRHF only needs 1 to 2 models during tuning and can efficiently align language models with human preferences robustly without complex hyperparameter tuning. Additionally, RRHF can be considered an extension of SFT and reward model training while being simpler than PPO in terms of coding, model counts, and hyperparameters. We evaluate RRHF on the Helpful and Harmless dataset, demonstrating comparable alignment performance with PPO by reward model score and human labeling. Extensive experiments show that the performance of RRHF is highly related to sampling quality which suggests RRHF is a best-of-n learner. Codes available at https://github.com/GanjinZero/RRHF.", "year": 2023, "publicationdate": "2023-04-11", "externalids": {"DOI": "10.48550/arXiv.2304.05302"}, "doi_lower": "10.48550/arxiv.2304.05302"}
{"paper_id": 250537137, "title": "In situ bidirectional human-robot value alignment", "author_names": ["Luyao Yuan", "Xiaofeng Gao", "Zilong Zheng", "Mark Edmonds", "Y. Wu", "F. Rossano", "Hongjing Lu", "Yixin Zhu", "Song-Chun Zhu"], "venue": "Sci. Robotics", "abstract": "A prerequisite for social coordination is bidirectional communication between teammates, each playing two roles simultaneously: as receptive listeners and expressive speakers. For robots working with humans in complex situations with multiple goals that differ in importance, failure to fulfill the expectation of either role could undermine group performance due to misalignment of values between humans and robots. Specifically, a robot needs to serve as an effective listener to infer human users' intents from instructions and feedback and as an expressive speaker to explain its decision processes to users. Here, we investigate how to foster effective bidirectional human-robot communications in the context of value alignment-collaborative robots and users form an aligned understanding of the importance of possible task goals. We propose an explainable artificial intelligence (XAI) system in which a group of robots predicts users' values by taking in situ feedback into consideration while communicating their decision processes to users through explanations. To learn from human feedback, our XAI system integrates a cooperative communication model for inferring human values associated with multiple desirable goals. To be interpretable to humans, the system simulates human mental dynamics and predicts optimal explanations using graphical models. We conducted psychological experiments to examine the core components of the proposed computational framework. Our results show that real-time human-robot mutual understanding in complex cooperative tasks is achievable with a learning model based on bidirectional communication. We believe that this interaction framework can shed light on bidirectional value alignment in communicative XAI systems and, more broadly, in future human-machine teaming systems.", "year": 2022, "publicationdate": "2022-07-13", "externalids": {"DOI": "10.1126/scirobotics.abm4183"}, "doi_lower": "10.1126/scirobotics.abm4183"}
{"paper_id": 48416518, "title": "Debate: On Christiano's The Constitution of Equality", "author_names": ["David M. Estlund"], "venue": "", "abstract": null, "year": 2009, "publicationdate": "2009-06-01", "externalids": {"DOI": "10.1111/J.1467-9760.2009.00332.X"}, "doi_lower": "10.1111/j.1467-9760.2009.00332.x"}
{"paper_id": 3241708, "title": "A Survey of Crowdsourcing Systems", "author_names": ["Man-Ching Yuen", "Irwin King", "K. Leung"], "venue": "2011 IEEE Third Int'l Conference on Privacy, Security, Risk and Trust and 2011 IEEE Third Int'l Conference on Social Computing", "abstract": "Crowd sourcing is evolving as a distributed problem-solving and business production model in recent years. In crowd sourcing paradigm, tasks are distributed to networked people to complete such that a company's production cost can be greatly reduced. In 2003, Luis von Ahn and his colleagues pioneered the concept of \"human computation\", which utilizes human abilities to perform computation tasks that are difficult for computers to process. Later, the term \"crowdsourcing\" was coined by Jeff Howe in 2006. Since then, a lot of work in crowd sourcing has focused on different aspects of crowd sourcing, such as computational techniques and performance analysis. In this paper, we give a survey on the literature on crowd sourcing which are categorized according to their applications, algorithms, performances and datasets. This paper provides a structured view of the research on crowd sourcing to date.", "year": 2011, "publicationdate": "2011-10-01", "externalids": {"DOI": "10.1109/PASSAT/SocialCom.2011.203"}, "doi_lower": "10.1109/passat/socialcom.2011.203"}
{"paper_id": 232417301, "title": "Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors", "author_names": ["Zeyu Yun", "Yubei Chen", "B. Olshausen", "Yann LeCun"], "venue": "Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out", "abstract": "Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these ‘black boxes’ as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at: https://github.com/zeyuyun1/TransformerVis.", "year": 2021, "publicationdate": "2021-03-29", "externalids": {"DOI": "10.18653/V1/2021.DEELIO-1.1"}, "doi_lower": "10.18653/v1/2021.deelio-1.1"}
{"paper_id": 67856299, "title": "Predicting the Type and Target of Offensive Posts in Social Media", "author_names": ["Marcos Zampieri", "S. Malmasi", "Preslav Nakov", "Sara Rosenthal", "N. Farra", "Ritesh Kumar"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.", "year": 2019, "publicationdate": "2019-02-01", "externalids": {"DOI": "10.18653/v1/N19-1144"}, "doi_lower": "10.18653/v1/n19-1144"}
{"paper_id": 259257659, "title": "Word-level Textual Adversarial Attacking as Combinatorial Optimization", "author_names": ["Saied Alshahrani"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 267053512, "title": "Generative Social Choice: OpenAI “Democratic Inputs to AI” Report", "author_names": ["Sara Fish", "Paul Gölz", "Ariel D. Procaccia", "Gili Rusak", "Itai Shapira", "Manuel Wüthrich"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 3960646, "title": "Visualizing and Understanding Convolutional Networks", "author_names": ["Matthew D. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision", "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.", "year": 2013, "publicationdate": "2013-11-12", "externalids": {"DOI": "10.1007/978-3-319-10590-1_53"}, "doi_lower": "10.1007/978-3-319-10590-1_53"}
{"paper_id": 231964046, "title": "Released Before Peer Review: Why Do We Care?", "author_names": ["L. Rhudy"], "venue": "Journal of Neuroscience Nursing", "abstract": null, "year": 2021, "publicationdate": "2021-02-17", "externalids": {"DOI": "10.1097/JNN.0000000000000583"}, "doi_lower": "10.1097/jnn.0000000000000583"}
{"paper_id": 9424845, "title": "Mitigating Unwanted Biases with Adversarial Learning", "author_names": ["B. Zhang", "Blake Lemoine", "Margaret Mitchell"], "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.", "year": 2018, "publicationdate": "2018-01-22", "externalids": {"DOI": "10.1145/3278721.3278779"}, "doi_lower": "10.1145/3278721.3278779"}
{"paper_id": 3162051, "title": "mixup: Beyond Empirical Risk Minimization", "author_names": ["Hongyi Zhang", "Moustapha Cissé", "Yann Dauphin", "David Lopez-Paz"], "venue": "International Conference on Learning Representations", "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.", "year": 2017, "publicationdate": "2017-10-25", "externalids": {}, "doi_lower": null}
{"paper_id": 4110304, "title": "Visual interpretability for deep learning: a survey", "author_names": ["Quanshi Zhang", "Song-Chun Zhu"], "venue": "Frontiers of Information Technology & Electronic Engineering", "abstract": "This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles’ heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.", "year": 2018, "publicationdate": "2018-01-01", "externalids": {"DOI": "10.1631/FITEE.1700808"}, "doi_lower": "10.1631/fitee.1700808"}
{"paper_id": 4562004, "title": "Interpretable Convolutional Neural Networks", "author_names": ["Quanshi Zhang", "Y. Wu", "Song-Chun Zhu"], "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "abstract": "This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each filter in a high conv-layer represents a specific object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e. what patterns are memorized by the CNN for prediction. Experiments have shown that filters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https://github.com/zqs1022/interpretableCNN.", "year": 2017, "publicationdate": "2017-10-02", "externalids": {"DOI": "10.1109/CVPR.2018.00920"}, "doi_lower": "10.1109/cvpr.2018.00920"}
{"paper_id": 259102814, "title": "Interactive Object Segmentation With Inside-Outside Guidance", "author_names": ["Shiyin Zhang", "Shikui Wei", "J. Liew", "Kunyang Han", "Yao Zhao", "Yunchao Wei"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "This article explores how to harvest precise object segmentation masks while minimizing the human interaction cost. To achieve this, we propose a simple yet effective interaction scheme, named Inside-Outside Guidance (IOG). Concretely, we leverage an inside point that is clicked near the object center and two outside points at the symmetrical corner locations (top-left and bottom-right or top-right and bottom-left) of an almost-tight bounding box that encloses the target object. The interaction results in a total of one foreground click and four background clicks for segmentation. The advantages of our IOG are four-fold: 1) the two outside points can help remove distractions from other objects or background; 2) the inside point can help eliminate the unrelated regions inside the bounding box; 3) the inside and outside points are easily identified, reducing the confusion raised by the state-of-the-art DEXTR Maninis et al. 2018, in labeling some extreme samples; 4) it naturally supports additional click annotations for further correction. Despite its simplicity, our IOG not only achieves state-of-the-art performance on several popular benchmarks such as GrabCut Rother et al. 2004, PASCAL Everingham et al. 2010 and MS COCO Russakovsky et al. 2015, but also demonstrates strong generalization capability across different domains such as street scenes (Cityscapes Cordts et al. 2016), aerial imagery (Rooftop Sun et al. 2014 and Agriculture-Vision Chiu et al. 2020) and medical images (ssTEM Gerhard et al. 2013). Code is available at https://github.com/shiyinzhang/Inside-Outside-Guidancehttps://github.com/shiyinzhang/Inside-Outside-Guidance.", "year": 2022, "publicationdate": "2022-12-06", "externalids": {"DOI": "10.1109/TPAMI.2022.3227116"}, "doi_lower": "10.1109/tpami.2022.3227116"}
{"paper_id": 256808689, "title": "The Wisdom of Hindsight Makes Language Models Better Instruction Followers", "author_names": ["Tianjun Zhang", "Fangchen Liu", "Justin Wong", "P. Abbeel", "Joseph E. Gonzalez"], "venue": "International Conference on Machine Learning", "abstract": "Reinforcement learning has seen wide success in finetuning large language models to better align with instructions via human feedback. The so-called algorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the underlying Reinforcement Learning (RL) algorithm is complex and requires an additional training pipeline for reward and value networks. In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for aligning language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. We evaluate the performance of HIR extensively on 12 challenging BigBench reasoning tasks and show that HIR outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning.", "year": 2023, "publicationdate": "2023-02-10", "externalids": {"DOI": "10.48550/arXiv.2302.05206"}, "doi_lower": "10.48550/arxiv.2302.05206"}
{"paper_id": 250243691, "title": "Discriminator-Guided Model-Based Offline Imitation Learning", "author_names": ["Wenjia Zhang", "Haoran Xu", "Haoyi Niu", "Peng Cheng", "Ming Li", "Heming Zhang", "Guyue Zhou", "Xianyuan Zhan"], "venue": "Conference on Robot Learning", "abstract": "Offline imitation learning (IL) is a powerful method to solve decision-making problems from expert demonstrations without reward labels. Existing offline IL methods suffer from severe performance degeneration under limited expert data. Including a learned dynamics model can potentially improve the state-action space coverage of expert data, however, it also faces challenging issues like model approximation/generalization errors and suboptimality of rollout data. In this paper, we propose the Discriminator-guided Model-based offline Imitation Learning (DMIL) framework, which introduces a discriminator to simultaneously distinguish the dynamics correctness and suboptimality of model rollout data against real expert demonstrations. DMIL adopts a novel cooperative-yet-adversarial learning strategy, which uses the discriminator to guide and couple the learning process of the policy and dynamics model, resulting in improved model performance and robustness. Our framework can also be extended to the case when demonstrations contain a large proportion of suboptimal data. Experimental results show that DMIL and its extension achieve superior performance and robustness compared to state-of-the-art offline IL methods under small datasets.", "year": 2022, "publicationdate": "2022-07-01", "externalids": {"DOI": "10.48550/arXiv.2207.00244"}, "doi_lower": "10.48550/arxiv.2207.00244"}
{"paper_id": 209404872, "title": "Distilling Structured Knowledge into Embeddings for Explainable and Accurate Recommendation", "author_names": ["Yuping Zhang", "Xiaoran Xu", "Hanning Zhou", "Yan Zhang"], "venue": "Web Search and Data Mining", "abstract": "Recently, the embedding-based recommendation models (e.g., matrix factorization and deep models) have been prevalent in both academia and industry due to their effectiveness and flexibility. However, they also have such intrinsic limitations as lacking explainability and suffering from data sparsity. In this paper, we propose an end-to-end joint learning framework to get around these limitations without introducing any extra overhead by distilling structured knowledge from a differentiable path-based recommendation model. Through extensive experiments, we show that our proposed framework can achieve state-of-the-art recommendation performance and meanwhile provide interpretable recommendation reasons.", "year": 2019, "publicationdate": "2019-12-18", "externalids": {"DOI": "10.1145/3336191.3371790"}, "doi_lower": "10.1145/3336191.3371790"}
{"paper_id": 261530162, "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models", "author_names": ["Yue Zhang", "Yafu Li", "Leyang Cui", "Deng Cai", "Lemao Liu", "Tingchen Fu", "Xinting Huang", "Enbo Zhao", "Yu Zhang", "Yulong Chen", "Longyue Wang", "A. Luu", "Wei Bi", "Freda Shi", "Shuming Shi"], "venue": "Computational Linguistics", "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.", "year": 2023, "publicationdate": "2023-09-03", "externalids": {"DOI": "10.1162/coli.a.16"}, "doi_lower": "10.1162/coli.a.16"}
{"paper_id": 254246861, "title": "Constructing Highly Inductive Contexts for Dialogue Safety through Controllable Reverse Generation", "author_names": ["Zhexin Zhang", "Jiale Cheng", "Hao Sun", "Jiawen Deng", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Minlie Huang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large pretrained language models can easily produce toxic or biased content, which is prohibitive for practical use. In order to detect such toxic generations, existing methods rely on templates, real-world data extraction, crowdsourcing workers, or automatic generation to construct adversarial contexts that are likely to induce toxic generations. However, what type of context is more likely to induce unsafe responses is still under-explored. In this paper, we identify that context toxicity and context category (e.g., \\textit{profanity}, \\textit{insult}, \\textit{drugs}, etc.) are two important factors to cause safety issues in response generation. Hence, we propose a method called \\emph{reverse generation} to construct adversarial contexts conditioned on a given response, with the flexibility to control category, toxicity level, and inductivity of the generated contexts. Via reverse generation, we augment the existing BAD dataset and construct a new dataset BAD+ which contains more than 120K diverse and highly inductive contexts in 12 categories. We test three popular pretrained dialogue models (Blender, DialoGPT, and Plato2) and find that BAD+ can largely expose their safety problems. Furthermore, we show that BAD+ can greatly enhance the safety of generation and reveal the key factors of safety improvement. Our code and dataset is available at \\url{https://github.com/thu-coai/Reverse_Generation}.", "year": 2022, "publicationdate": "2022-12-04", "externalids": {"DOI": "10.48550/arXiv.2212.01810"}, "doi_lower": "10.48550/arxiv.2212.01810"}
{"paper_id": 4952494, "title": "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods", "author_names": ["Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.", "year": 2018, "publicationdate": "2018-04-18", "externalids": {"DOI": "10.18653/v1/N18-2003"}, "doi_lower": "10.18653/v1/n18-2003"}
{"paper_id": 257900969, "title": "A Survey of Large Language Models", "author_names": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Z. Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "J. Nie", "Ji-rong Wen"], "venue": "arXiv.org", "abstract": "Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.", "year": 2023, "publicationdate": "2023-03-31", "externalids": {}, "doi_lower": null}
{"paper_id": 264289051, "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning", "author_names": ["Rui Zheng", "Wei Shen", "Yuan Hua", "Wenbin Lai", "Shihan Dou", "Yuhao Zhou", "Zhiheng Xi", "Xiao Wang", "Haoran Huang", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "venue": "International Conference on Learning Representations", "abstract": "The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the established groups, our approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data. Experimental results indicate that our approach significantly enhances training stability and model generalization.", "year": 2023, "publicationdate": "2023-10-18", "externalids": {"DOI": "10.48550/arXiv.2310.11971"}, "doi_lower": "10.48550/arxiv.2310.11971"}
{"paper_id": 2102547, "title": "Improving the Robustness of Deep Neural Networks via Stability Training", "author_names": ["Stephan Zheng", "Yang Song", "Thomas Leung", "I. Goodfellow"], "venue": "Computer Vision and Pattern Recognition", "abstract": "In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep networks against small input distortions that result from various types of common image processing, such as compression, rescaling, and cropping. We validate our method by stabilizing the state of-the-art Inception architecture [11] against these types of distortions. In addition, we demonstrate that our stabilized model gives robust state-of-the-art performance on largescale near-duplicate detection, similar-image ranking, and classification on noisy datasets.", "year": 2016, "publicationdate": "2016-04-15", "externalids": {"DOI": "10.1109/CVPR.2016.485"}, "doi_lower": "10.1109/cvpr.2016.485"}
{"paper_id": 47014473, "title": "Revisiting the Importance of Individual Units in CNNs via Ablation", "author_names": ["Bolei Zhou", "Yiyou Sun", "David Bau", "A. Torralba"], "venue": "arXiv.org", "abstract": "We revisit the importance of the individual units in Convolutional Neural Networks (CNNs) for visual recognition. By conducting unit ablation experiments on CNNs trained on large scale image datasets, we demonstrate that, though ablating any individual unit does not hurt overall classification accuracy, it does lead to significant damage on the accuracy of specific classes. This result shows that an individual unit is specialized to encode information relevant to a subset of classes. We compute the correlation between the accuracy drop under unit ablation and various attributes of an individual unit such as class selectivity and weight L1 norm. We confirm that unit attributes such as class selectivity are a poor predictor for impact on overall accuracy as found previously in recent work \\cite{morcos2018importance}. However, our results show that class selectivity along with other attributes are good predictors of the importance of one unit to individual classes. We evaluate the impact of random rotation, batch normalization, and dropout to the importance of units to specific classes. Our results show that units with high selectivity play an important role in network classification power at the individual class level. Understanding and interpreting the behavior of these units is necessary and meaningful.", "year": 2018, "publicationdate": "2018-06-07", "externalids": {}, "doi_lower": null}
{"paper_id": 258822910, "title": "LIMA: Less Is More for Alignment", "author_names": ["Chunting Zhou", "Pengfei Liu", "Puxin Xu", "Srini Iyer", "Jiao Sun", "Yuning Mao", "Xuezhe Ma", "Avia Efrat", "Ping Yu", "L. Yu", "Susan Zhang", "Gargi Ghosh", "M. Lewis", "Luke Zettlemoyer", "Omer Levy"], "venue": "Neural Information Processing Systems", "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.", "year": 2023, "publicationdate": "2023-05-18", "externalids": {}, "doi_lower": null}
{"paper_id": 232104764, "title": "Domain Generalization: A Survey", "author_names": ["Kaiyang Zhou", "Ziwei Liu", "Y. Qiao", "T. Xiang", "Chen Change Loy"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d. assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Over the last ten years, research in DG has made great progress, leading to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, to name a few; DG has also been studied in various application areas including computer vision, speech recognition, natural language processing, medical imaging, and reinforcement learning. In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other relevant fields like domain adaptation and transfer learning. Then, we conduct a thorough review into existing methods and theories. Finally, we conclude this survey with insights and discussions on future research directions.", "year": 2021, "publicationdate": "2021-03-03", "externalids": {"DOI": "10.1109/TPAMI.2022.3195549"}, "doi_lower": "10.1109/tpami.2022.3195549"}
{"paper_id": 221140013, "title": "Inverse Reinforcement Learning with Natural Language Goals", "author_names": ["Li Zhou", "Kevin Small"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Humans generally use natural language to communicate task requirements to each other. Ideally, natural language should also be usable for communicating goals to autonomous machines (e.g., robots) to minimize friction in task specification. However, understanding and mapping natural language goals to sequences of states and actions is challenging. Specifically, existing work along these lines has encountered difficulty in generalizing learned policies to new natural language goals and environments. In this paper, we propose a novel adversarial inverse reinforcement learning algorithm to learn a language-conditioned policy and reward function. To improve generalization of the learned policy and reward function, we use a variational goal generator to relabel trajectories and sample diverse goals during training. Our algorithm outperforms multiple baselines by a large margin on a vision-based natural language instruction following dataset (Room-2-Room), demonstrating a promising advance in enabling the use of natural language instructions in specifying agent goals.", "year": 2020, "publicationdate": "2020-08-16", "externalids": {"DOI": "10.1609/aaai.v35i12.17326"}, "doi_lower": "10.1609/aaai.v35i12.17326"}
{"paper_id": 123988552, "title": "Machine Learning: The ingredients of machine learning", "author_names": ["P. Flach"], "venue": "", "abstract": null, "year": 2012, "publicationdate": "2012-09-01", "externalids": {"DOI": "10.1017/CBO9780511973000.003"}, "doi_lower": "10.1017/cbo9780511973000.003"}
{"paper_id": 212877887, "title": "The Ingredients of Real-World Robotic Reinforcement Learning", "author_names": ["Henry Zhu", "Justin Yu", "Abhishek Gupta", "Dhruv Shah", "Kristian Hartikainen", "Avi Singh", "Vikash Kumar", "S. Levine"], "venue": "International Conference on Learning Representations", "abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.", "year": 2020, "publicationdate": "2020-04-27", "externalids": {}, "doi_lower": null}
{"paper_id": 263310319, "title": "DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks", "author_names": ["Kaijie Zhu", "Jiaao Chen", "Jindong Wang", "Neil Zhenqiang Gong", "Diyi Yang", "Xing Xie"], "venue": "International Conference on Learning Representations", "abstract": "Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a general and flexible protocol for dynamic evaluation of LLMs. Based on our framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo and GPT-4. Experiments show that LLMs perform worse in DyVal-generated evaluation samples with different complexities, highlighting the significance of dynamic evaluation. We also analyze the failure cases and results of different prompting methods. Moreover, DyVal-generated samples are not only evaluation sets, but also helpful data for fine-tuning to improve the performance of LLMs on existing benchmarks. We hope that DyVal can shed light on future evaluation research of LLMs. Code is available at: https://github.com/microsoft/promptbench.", "year": 2023, "publicationdate": "2023-09-29", "externalids": {}, "doi_lower": null}
{"paper_id": 227275607, "title": "Consequences of Misaligned AI", "author_names": ["Simon Zhuang", "Dylan Hadfield-Menell"], "venue": "Neural Information Processing Systems", "abstract": "AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the $L$ attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on $J<L$ attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.", "year": 2021, "publicationdate": "2021-02-07", "externalids": {}, "doi_lower": null}
{"paper_id": 336219, "title": "Maximum Entropy Inverse Reinforcement Learning", "author_names": ["Brian D. Ziebart", "Andrew L. Maas", "J. Bagnell", "A. Dey"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": null, "year": 2008, "publicationdate": "2008-07-13", "externalids": {}, "doi_lower": null}
{"paper_id": 202660943, "title": "Fine-Tuning Language Models from Human Preferences", "author_names": ["Daniel M. Ziegler", "Nisan Stiennon", "Jeff Wu", "Tom B. Brown", "Alec Radford", "Dario Amodei", "Paul Christiano", "G. Irving"], "venue": "arXiv.org", "abstract": "Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.", "year": 2019, "publicationdate": "2019-09-18", "externalids": {}, "doi_lower": null}
{"paper_id": 247849293, "title": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems", "author_names": ["Caleb Ziems", "Jane A. Yu", "Yi-Chia Wang", "A. Halevy", "Diyi Yang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user’s trust in the moral integrity of the system. Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be multiple competing judgments that apply to a situation simultaneously. In this work, we introduce a new resource, not to authoritatively resolve moral ambiguities, but instead to facilitate systematic understanding of the intuitions, values and moral judgments reflected in the utterances of dialogue systems. The Moral Integrity Corpus, MIC, is such a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects a particular moral conviction that can explain why a chatbot’s reply may appear acceptable or problematic. We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification. Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios. Our findings suggest that MIC will be a useful resource for understanding and language models’ implicit moral assumptions and flexibly benchmarking the integrity of conversational agents. To download the data, see https://github.com/GT-SALT/mic", "year": 2022, "publicationdate": "2022-04-06", "externalids": {"DOI": "10.48550/arXiv.2204.03021"}, "doi_lower": "10.48550/arxiv.2204.03021"}
{"paper_id": 263605618, "title": "Representation Engineering: A Top-Down Approach to AI Transparency", "author_names": ["Andy Zou", "Long Phan", "Sarah Chen", "James Campbell", "Phillip Guo", "Richard Ren", "Alexander Pan", "Xuwang Yin", "Mantas Mazeika", "Ann-Kathrin Dombrowski", "Shashwat Goel", "Nathaniel Li", "Michael J. Byun", "Zifan Wang", "Alex Troy Mallen", "Steven Basart", "Sanmi Koyejo", "Dawn Song", "Matt Fredrikson", "Zico Kolter", "Dan Hendrycks"], "venue": "arXiv.org", "abstract": "In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.", "year": 2023, "publicationdate": "2023-10-02", "externalids": {"DOI": "10.48550/arXiv.2310.01405"}, "doi_lower": "10.48550/arxiv.2310.01405"}
{"paper_id": 270286008, "title": "Improving Alignment and Robustness with Circuit Breakers", "author_names": ["Andy Zou", "Long Phan", "Justin Wang", "Derek Duenas", "Maxwell Lin", "Maksym Andriushchenko", "Rowan Wang", "Zico Kolter", "Matt Fredrikson", "Dan Hendrycks"], "venue": "Neural Information Processing Systems", "abstract": "AI systems can take harmful actions and are highly vulnerable to adversarial attacks. We present an approach, inspired by recent advances in representation engineering, that interrupts the models as they respond with harmful outputs with\"circuit breakers.\"Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks. As an alternative to refusal training and adversarial training, circuit-breaking directly controls the representations that are responsible for harmful outputs in the first place. Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility -- even in the presence of powerful unseen attacks. Notably, while adversarial robustness in standalone image recognition remains an open challenge, circuit breakers allow the larger multimodal system to reliably withstand image\"hijacks\"that aim to produce harmful content. Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack. Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks.", "year": 2024, "publicationdate": "2024-06-06", "externalids": {"DOI": "10.48550/arXiv.2406.04313"}, "doi_lower": "10.48550/arxiv.2406.04313"}
{"paper_id": 258108410, "title": "Segment Everything Everywhere All at Once", "author_names": ["Xueyan Zou", "Jianwei Yang", "Hao Zhang", "Feng Li", "Linjie Li", "Jianfeng Gao", "Yong Jae Lee"], "venue": "Neural Information Processing Systems", "abstract": "In this work, we present SEEM, a promptable and interactive model for segmenting everything everywhere all at once in an image, as shown in Fig.1. In SEEM, we propose a novel decoding mechanism that enables diverse prompting for all types of segmentation tasks, aiming at a universal segmentation interface that behaves like large language models (LLMs). More specifically, SEEM is designed with four desiderata: i) Versatility. We introduce a new visual prompt to unify different spatial queries including points, boxes, scribbles and masks, which can further generalize to a different referring image; ii) Compositionality. We learn a joint visual-semantic space between text and visual prompts, which facilitates the dynamic composition of two prompt types required for various segmentation tasks; iii) Interactivity. We further incorporate learnable memory prompts into the decoder to retain segmentation history through mask-guided cross-attention from decoder to image features; and iv) Semantic-awareness. We use a text encoder to encode text queries and mask labels into the same semantic space for open-vocabulary segmentation. We conduct a comprehensive empirical study to validate the effectiveness of SEEM across diverse segmentation tasks. Notably, our single SEEM model achieves competitive performance across interactive segmentation, generic segmentation, referring segmentation, and video object segmentation on 9 datasets with minimum 1/100 supervision. Furthermore, SEEM showcases a remarkable capacity for generalization to novel prompts or their combinations, rendering it a readily universal image segmentation interface.", "year": 2023, "publicationdate": "2023-04-13", "externalids": {"DOI": "10.48550/arXiv.2304.06718"}, "doi_lower": "10.48550/arxiv.2304.06718"}
