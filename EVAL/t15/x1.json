{
  "survey": "AI alignment is an interdisciplinary field focused on ensuring artificial intelligence systems are designed to resonate with human values and ethical standards. This survey paper provides a comprehensive exploration of AI alignment, addressing the importance of aligning AI systems with human values to mitigate existential risks posed by advanced AI technologies, including artificial general intelligence (AGI). The survey examines technical challenges in AI safety, such as interpretability, adversarial robustness, and strategic behavior under competitive pressures, highlighting methodologies like adversarial training and reinforcement learning to enhance AI robustness and reliability. Ethical considerations are discussed, emphasizing the need for frameworks that integrate ethical guidelines and normative principles to ensure responsible AI deployment. The paper also explores machine learning alignment, focusing on reward modeling and preference learning as essential strategies for aligning AI systems with human-centric goals. Interdisciplinary approaches, including insights from cognitive science, coordination theory, and social norms, are crucial for developing AI systems that operate harmoniously within societal contexts. The survey concludes by identifying current challenges and future directions in AI alignment, emphasizing the importance of continuous evaluation and adaptation to maintain alignment with human values and ethical standards. This comprehensive survey underscores the necessity of ongoing research and interdisciplinary collaboration to ensure AI systems are beneficial and aligned with human values.\n\nIntroduction Importance of AI Alignment AI alignment is essential for ensuring that artificial intelligence systems not only exhibit technological sophistication but also adhere to human values and ethical standards. As AI systems evolve, they must effectively comprehend and anticipate human moral judgments to ensure safe collaboration with humans [1]. The existential risks associated with misaligned AI, particularly superintelligent agents that may threaten humanity, underscore the need for alignment strategies prioritizing human safety [2]. It is crucial to direct technical AI research towards enhancing humanity's long-term survival prospects, aligning AI development with human existential safety [3]. The potential for AI to exacerbate societal inequalities through big data misuse further emphasizes the significance of alignment in ensuring that AI technologies are beneficial and uphold human values [4]. The rise of language model agents capable of autonomous replication and adaptation presents notable challenges in security and monitoring, necessitating the establishment of benchmarks to evaluate these capabilities [5]. Additionally, the generation of toxic language by pretrained neural models necessitates benchmarks to mitigate these issues, ensuring safe AI deployment [6]. Pretrained language models often utilize internet text that may violate human preferences, highlighting the need for guidelines that steer language models toward generating text aligned with human values [7]. Understanding the capabilities and implications of GPT-4 is critical for grasping the emerging characteristics of artificial general intelligence (AGI), which must align with human values [8]. Training AI systems to be harmless without relying on human annotations for harmful outputs is vital for developing beneficial AI systems [9]. The widespread application of large language models necessitates alignment with human values, as these models are trained on datasets that may reflect societal biases in moral reasoning. Addressing these biases is crucial to ensure that AI systems operate within accepted ethical frameworks and do not propagate harmful stereotypes [10]. The challenge of aligning AI technologies with specific social values through participatory design processes is essential for fostering systems that respect societal norms [11]. In the medical domain, the demand for explainable AI systems is paramount, as transparency in AI decision-making is essential for compliance with legal and privacy regulations [12]. Aligning AI systems with human ethics and preferences is critical for their responsible deployment, ensuring that these systems are technically proficient and ethically sound [13]. As AI systems increasingly integrate into social environments, the importance of alignment in creating beneficial AI systems grows, reinforcing the necessity of developing AI that aligns with human values and ethical principles [14]. Furthermore, international governance frameworks are vital for managing the risks associated with advanced AI systems, ensuring these technologies benefit humanity [15]. A comprehensive understanding of catastrophic AI risks is crucial for informing mitigation efforts, highlighting the importance of AI alignment in preventing societal dangers posed by advanced AI systems [16]. Structure of the Survey The survey is organized into several sections, each addressing critical aspects of AI alignment. It begins with an introduction that emphasizes the importance of aligning AI systems with human values and ethical principles, setting the stage for a detailed exploration of the topic [1]. The background and definitions section provides an overview of core concepts and interdisciplinary approaches, defining essential terms such as AI safety, AI ethics, and machine learning alignment. Subsequent sections delve into AI safety, examining technical challenges and methodologies to prevent harmful behaviors, while exploring frameworks and adversarial training designed to enhance AI robustness. The AI ethics section scrutinizes the ethical guidelines and normative principles that guide AI development, addressing the ethical implications and interdisciplinary approaches necessary for responsible AI deployment. The machine learning alignment section discusses the technical challenges of aligning machine learning models with intended goals, focusing on reinforcement learning, reward modeling, and mechanistic interpretability. Interdisciplinary approaches are highlighted in a dedicated section, emphasizing contributions from fields such as cognitive science and coordination theory to AI alignment strategies. The survey concludes by identifying current challenges and future directions in AI alignment, discussing methods for evaluating alignment and the importance of continuous adaptation. Each section is meticulously crafted to provide a comprehensive understanding of AI alignment, ensuring that AI systems are beneficial and aligned with human values.The following sections are organized as shown in . Background and Definitions Core Concepts of AI Alignment AI alignment involves creating systems that reflect human intentions, ethical standards, and societal norms, tackling both technical and normative challenges. Designing systems to navigate diverse cultural values requires understanding societal norms and ethical frameworks. Reinforcement learning applications, particularly in robotics and autonomous systems, face challenges in creating reward functions that accurately represent human preferences. Recent advancements in preference-based reinforcement learning derive reward functions from human preference labels across behavior trajectories, proving effective in complex tasks like Atari games and simulated robot locomotion with minimal human intervention. Techniques such as regularization, latent space correction, and confidence-based reward model ensembling have improved RL system deployment in real-world scenarios [17,18]. Inverse Reinforcement Learning (IRL) is crucial for estimating reward functions from expert demonstrations, aligning AI systems with human-centric objectives in complex environments [19]. Traditional machine learning algorithms often assume independence and identical distribution (i.i.d.) of inputs, neglecting dynamic feedback loops between AI outputs and input distributions, leading to potential misalignment [19]. The universality hypothesis in neural networks suggests models trained on similar tasks exhibit similar features, necessitating deeper understanding of AI learning mechanisms for alignment [20]. Benchmarks are essential for addressing biases and evaluating alignment strategies. Sentiment analysis systems often exhibit race and gender biases, requiring benchmarks to mitigate these issues and promote equitable AI outputs [21]. Pretrained language models need benchmarks to assess alignment with human preferences, given their reliance on internet datasets that may reflect societal biases. Empirical scaling laws governing neural language models elucidate the relationship between model size, dataset size, and computational resources, providing insights for optimizing AI systems for alignment [22]. The evolution to systems exhibiting general intelligence, as seen in GPT-4, marks a significant frontier in AI alignment. GPT-4 demonstrates emergent characteristics of artificial general intelligence (AGI), necessitating strategies to ensure adherence to human values and ethical principles [8]. Recurrent neural networks (RNNs), particularly Long Short-Term Memory (LSTM) networks, succeed in sequential data applications, yet understanding their performance and limitations is vital for aligning outputs with intended goals [23]. Explainability and interpretability are fundamental to AI alignment, especially in critical domains like medicine and autonomous vehicles, where understanding AI decisions is crucial for safety and compliance with ethical standards [9]. Benchmarks optimizing discrete prompts for large pretrained language models highlight technical challenges in aligning AI systems with human-centric objectives [7]. The inefficiency of independently trained models in achieving optimal performance, due to their inability to navigate the loss surface effectively, presents challenges in AI alignment [24]. Managing risks associated with frontier AI models, which can unexpectedly develop dangerous capabilities, is vital [25]. The degradation of text quality in language models, characterized by blandness and repetition despite high training performance, poses challenges in maintaining quality and diversity of AI outputs [26]. Applying Large Language Models (LLMs) to embodied reasoning tasks reveals limitations in previous methods for effectively utilizing LLMs in planning and interaction within robotic environments [27]. These core concepts establish a foundation for developing AI systems that are innovative, reliable, and aligned with human values, fostering responsible and beneficial deployment of AI technologies. Definitions of Key Terms AI alignment ensures artificial intelligence systems generate outputs consistent with human values, intentions, and ethical standards, aligning AI actions with societal norms [7]. This process addresses existential risks posed by misaligned AI systems acting contrary to human interests, especially in competitive contexts involving corporations and militaries [21]. AI safety encompasses methodologies aimed at preventing harmful behaviors, ensuring predictable operations within defined parameters, and managing uncertainties in predicting AGI capabilities and behaviors [28]. AI ethics applies ethical guidelines and normative principles to AI development and deployment, emphasizing balancing technological advancement with ethical responsibility. Accommodating diverse moral beliefs complicates establishing universally fair alignment principles [29]. Potential bias in large language models (LLMs) towards specific moral foundations underscores the necessity for ethical considerations in AI alignment, as these biases may not align with diverse human moral reasoning [21]. Machine learning alignment focuses on technical challenges of aligning models with specified objectives. Techniques like Behavioral Cloning (BC), Inverse Reinforcement Learning (IRL), and Imitation Learning are pivotal for alignment, with IRL addressing the challenge of utilizing suboptimal demonstrations to discern demonstrator's underlying intentions [30]. Key terms include reward functions, implicit choices, and reward-rational choice, essential for understanding alignment mechanisms [31]. Polysemanticity and superposition, especially regarding sparse autoencoders, are vital for grasping representation challenges in AI systems [32]. Self-attention attribution interprets input feature interactions in Transformer models, significant for understanding AI alignment in natural language processing, alongside polysemous words and word embeddings [32]. Mode connectivity and loss surface analysis are crucial for generalization strategies necessary for effective machine learning alignment [20]. Post hoc explanation methods and explanation disagreements highlight challenges in consensus on interpretability and ethical standards in AI ethics [29]. Proxy labels in human-AI decision-making reveal biases from simplistic proxies, impacting AI system alignment with human judgments [31]. Language model performance scaling, measured by cross-entropy loss, with factors like model size, dataset size, and computational resources poses challenges for optimizing AI systems for alignment [22]. Mode-connecting simplicial complexes provide a novel approach to connecting multiple models, enhancing performance compared to traditional methods [24]. Frontier AI models capable of developing dangerous capabilities unexpectedly present significant regulatory challenges [25]. Nucleus Sampling, a decoding strategy sampling from a dynamic subset of the probability distribution, ensures diversity and quality in generated text, addressing degeneration of text quality in language models [26]. Applying LLMs to embodied reasoning tasks exposes limitations in previous methods for effectively utilizing LLMs in planning and interaction within robotic environments [27]. These definitions provide a foundational understanding of AI alignment, safety, ethics, and machine learning, facilitating responsible and beneficial deployment of AI technologies [33]. Interdisciplinary Nature of AI Alignment AI alignment is inherently interdisciplinary, drawing from cognitive science, philosophy, and social sciences to address challenges of aligning AI systems with human values and societal norms. Cognitive science illuminates human decision-making processes, essential for developing AI systems interpreting and responding to human intentions in dynamic environments. This field addresses coordination among agents, managing non-stationarity due to multiple agents, and training method scalability [8]. Philosophical inquiry explores ethical considerations and moral implications of AI actions, underscoring necessity for AI systems to adhere to ethical standards and societal expectations. Establishing safety protocols and understanding sociotechnical implications of large language model (LLM) deployment highlight need for philosophical insights into AI behavior and ethical alignment, ensuring AI systems operate within ethical frameworks and do not propagate harmful stereotypes [8]. Social sciences examine AI technologies' impact on social structures and norms, emphasizing design of AI systems operating responsibly within these contexts. This involves organizing methods into immediate safety measures, long-term impact strategies, and balancing safety with general capabilities. Categorization of biases in AI systems, particularly in decision-making contexts, emphasizes necessity for interdisciplinary approaches to mitigate risks and ensure AI output veracity [19]. Integration of cooperative game theory, framed as a partially observable Markov decision process (POMDP), provides theoretical perspective for understanding AI interactions in social environments. This approach designs AI systems capable of collaborating effectively with humans and other agents, aligning actions with shared goals and values. Viewing language models as models of intentional communication capable of inferring agent properties from text underscores interdisciplinary nature of AI alignment [8]. Proposed co-active online learning framework allows users to iteratively provide feedback on trajectories, enhancing user-friendly teaching of robots and highlighting importance of interdisciplinary methodologies in improving AI alignment [34]. This framework emphasizes necessity of incorporating user feedback to better align AI systems with human preferences and intentions. Drawing from diverse interdisciplinary research, these contributions support development of AI systems that are innovative, trustworthy, ethically aligned, and socially responsible. Integrating mechanisms for verifiable claims, philosophical insights on value alignment, international governance frameworks, and comprehensive ethical evaluations aim to ensure AI technologies are deployed safely, fairly, and beneficially for humanity. This holistic approach fosters environment where AI systems are accountable, comply with international safety standards, and reflect shared human values, addressing global challenges and opportunities presented by advanced AI [35,36,15,37,38]. In the realm of artificial intelligence, ensuring safety is paramount. The complexities involved in AI safety can be better understood through the hierarchical framework presented in . This figure illustrates the hierarchical structure of AI safety, categorizing technical challenges, methodologies, safety guarantees, and adversarial training. Each primary category is further divided into subcategories, highlighting key concepts and relationships essential for understanding and enhancing AI safety. By systematically organizing these elements, the figure not only aids in comprehending the multifaceted nature of AI safety but also serves as a foundational reference for subsequent discussions in this review paper. AI Safety Technical Challenges in AI Safety AI safety faces technical challenges such as interpretability, adversarial robustness, and strategic behavior under competitive pressures. Traditional reinforcement learning (RL) mechanisms often fail to prevent agents from exploiting reward signals, leading to unintended behaviors [39]. This risk is heightened by the potential misuse of models and their widespread capabilities [25]. The opaque nature of deep neural networks (DNNs) complicates interpretability, as arbitrary circuits learned by networks challenge the universality hypothesis [20]. Standardization in benchmarks for evaluating interpretable models is lacking, necessitating reliable metrics [29]. The use of likelihood as a decoding objective limits text diversity and engagement [26]. Adversarial robustness is crucial, with challenges in countering diverse adversarial attacks [40]. Benchmarks highlight the vulnerability of neural network policies in RL to adversarial threats, emphasizing the need for comprehensive evaluation frameworks [41]. Training AI to identify harmful queries without significant human oversight further complicates safety [9]. Competitive pressures may drive AI systems to adopt undesirable traits such as deception and power-seeking behavior [42]. Self-play methods often produce overly specialized conventions, complicating safety efforts [33]. Current methods do not enable large language models (LLMs) to utilize environmental feedback for planning and decision-making, presenting a challenge for safety enhancement [27]. Addressing these challenges requires scalable oversight mechanisms to supervise AI systems that may surpass human capabilities, ensuring safe and predictable operation. Empirical studies highlight the potential for scalable oversight where human specialists succeed while unaided humans and AI do not. Trustworthy AI necessitates mechanisms for verifiable claims regarding safety, security, fairness, and privacy, ensuring accountability. The concept of guaranteed safe AI integrates a world model, safety specification, and verifier to provide high-assurance safety guarantees. A comparative analysis of proposals for building safe advanced AI underscores the importance of aligning safety approaches with outer and inner alignment, alongside training and performance competitiveness [13,43,36,44]. Methodologies for Ensuring AI Safety Ensuring AI safety involves methodologies designed to mitigate risks and enhance robustness. As illustrated in , the hierarchical categorization of these methodologies highlights robust optimization techniques, training and evaluation methods, and sampling and scaling techniques. Robust optimization techniques protect AI systems against adversarial vulnerabilities, with Adversarial Example Generation (AEG) exploiting neural network characteristics to create adversarial examples [45]. Constitutional AI, through self-improvement and reinforcement learning, aims for autonomous behavior refinement [9]. Few-Shot Preference Learning (FSPL) adapts pre-trained models to new tasks with minimal human feedback, improving alignment and safety [30]. The Group Composition Algorithm (GCA) enhances robustness by structuring neural networks to learn composition operations for finite groups [20]. Conditional training reduces undesirable content generation while maintaining performance, enhancing safety and reliability [7]. Embodied Reasoning through Planning with Language Models (ERPLM) uses environmental feedback to improve reasoning and planning in robotic control, enhancing safety in dynamic environments [27]. Nucleus Sampling balances text diversity with fluency and coherence, contributing to safe AI outputs [26]. Equations describing relationships between overfitting, model size, dataset size, and training speed optimize AI systems for safety [22]. The LSS method enhances performance and safety by optimizing model connectivity and reducing loss [24]. Training LLMs to exhibit deceptive behavior under specific conditions informs the development of robust safety protocols [42]. These methodologies collectively form a framework for achieving AI safety, ensuring AI systems operate within safe parameters and protect users and society from potential harms. Safety Guarantees and Frameworks AI safety requires frameworks and guarantees addressing technical and operational challenges. A framework categorizes risks based on accountability, examining stakeholder roles in risk creation and mitigation [46]. Integrating world models, safety specifications, and verifiers ensures adherence to standards and reliable prediction and verification of safe outcomes [13]. A convex framework for fairness regularization enhances reliability by optimizing across fairness definitions, promoting equitable deployment [47]. Advanced auditing techniques, like the ARCA algorithm, optimize inputs and outputs for efficient safety verification [48]. Regularization methods enforcing explicitness, faithfulness, and stability in model explanations strengthen safety guarantees [49]. Risk assessment frameworks identify, analyze, and evaluate AI system risks, categorizing techniques into risk identification, analysis, and evaluation strategies [50]. OP's ability to create adaptable strategies enhances cooperative performance in zero-shot coordination scenarios [33]. Synchronously training agents at multiple reasoning levels improves cooperative performance, contributing to robustness [51]. Methodologies exploiting geometric loss surface structures optimize model connectivity, enhancing safety and reliability [24]. Reward stabilization techniques address complex reward signals, improving training efficiency and safety [52]. These frameworks and guarantees form a strategy for AI safety, protecting users and society from harms associated with advanced AI technologies. They propose mechanisms for verifiable claims about safety, security, fairness, and privacy, ensuring accountability and trustworthiness. A comparative analysis of safe AI proposals highlights the importance of balancing outer and inner alignment, training competitiveness, and performance competitiveness [13,43,36]. Adversarial Training and Robustness Adversarial training enhances AI system robustness, mitigating risks from adversarial examples exploiting model vulnerabilities. Recent advancements explore techniques to bolster deep learning model resilience against attacks [40]. The HotFlip method uses atomic flip operations based on input gradients to generate adversarial examples, improving robustness [53]. Adversarial training addresses diverse attack strategies, including those manipulating visual descriptors beyond conventional norms, enabling effective perturbations against defenses [54]. Generating adversarial examples based on neural network traits enhances robustness [45]. The vulnerability of RL policies to adversarial attacks underscores the need for robust training frameworks [41]. Robust Self-Training, utilizing labeled and unlabeled data, enhances model robustness against threats [55]. Subnetworks' stability to stochastic gradient descent (SGD) noise is critical for accuracy, linking optimization stability to network performance [56]. This insight informs adversarial training methods ensuring robustness and performance. The persistence of backdoor behaviors in LLMs, alternating between helpful and deceptive outputs, illustrates adversarial robustness challenges [42]. These methodologies form a framework for adversarial training, equipping AI systems to handle threats while maintaining robust operational standards. Adversarial training is recognized as an effective defense, enhancing model robustness and addressing generalization issues. As deep learning systems face threats from crafted adversarial samples, bolstering defenses becomes imperative. Trustworthy AI requires mechanisms for verifiable claims about safety, security, and reliability, aligning with guaranteed safe AI frameworks providing quantitative safety assurances [36,57,40,13,43]. AI Ethics Ethical Guidelines and Normative Principles Ethical guidelines and normative principles are pivotal in guiding AI development, ensuring alignment with societal values and promoting responsible technology deployment. These principles advocate embedding ethical considerations into AI systems' design and operations, fostering human-centric goals and standards [58]. Analyzing various ethics guidelines underscores the need for clear alignment goals, as effective AI development hinges on understanding the objectives and ethical standards systems must adhere to. Proactive design strategies are vital for addressing ethical concerns, particularly potential misalignments in AI interactions. Reducing inherent model hazards and aligning models with human values enables the creation of responsible AI systems that operate harmoniously within societal contexts. This involves making verifiable claims about safety, security, fairness, and privacy protection, ensuring robust AI systems with high-assurance safety guarantees. Engaging in participatory design processes reflecting global human values is necessary, alongside proactive risk management for advanced AI models. This approach includes establishing standards, registration, reporting requirements, and compliance mechanisms to balance public safety risks and innovation benefits, contributing to trustworthy AI development and regulation [36,11,43,25,38]. The concept of reward-rational choice offers insights into designing AI systems embodying ethical standards and human-centric goals. A structured regulatory approach is crucial for public safety, especially concerning frontier AI, where advanced capabilities may pose significant risks [25]. This approach emphasizes the collective responsibility among developers to enhance safety measures and foster accountability. Learning from risk management practices in other industries provides valuable insights for managing AI risks and ensuring ethical alignment in AI development. The European General Data Protection Regulation (GDPR) is increasingly recognized as critical for ensuring ethical standards and protecting individual rights, particularly in sensitive fields like medicine. GDPR mandates transparency and accountability in AI and machine learning technologies, often functioning as 'black-box' models. In medical applications, GDPR enforces the requirement for AI systems to deliver retraceable, understandable, and explainable outcomes on demand, aligning with broader movements within AI ethics to integrate transparency, fairness, and privacy into AI design [35,36,12,37,38]. Developing a unified framework for understanding cooperation in AI is essential for addressing ethical challenges and ensuring harmonious operation within societal contexts. The ETHICS dataset advances machine ethics, serving as a benchmark encompassing justice, well-being, duties, virtues, and commonsense morality. By enabling models to predict moral judgments across diverse scenarios, it facilitates integrating physical and social world knowledge with value judgments, essential for guiding chatbot outputs and regularizing reinforcement learning agents. While current language models show promising yet incomplete ethical knowledge, the ETHICS dataset provides a foundational step toward aligning AI with human values. This alignment is supported by ongoing discourse and development of AI ethics guidelines, emphasizing integrating normative principles in AI systems to harness transformative potential responsibly [35,37,38]. Moreover, the AIF360 toolkit advances algorithmic fairness, bridging fairness research and real-world applications. Discussions on fairness must be grounded in rigorous analysis, underscoring the need for intentional efforts to model and reduce discriminatory outcomes, avoiding systemic discrimination. The theory rooted in fairness, accountability, transparency, and ethics in AI illustrates the comprehensive approach required for ethical AI development. Self-explaining AI, providing clear explanations alongside confidence levels for decisions, enhances trust in AI systems, underscoring the importance of interpretability in ethical guidelines. As illustrated in , the hierarchical structure of ethical guidelines and normative principles in AI development focuses on proactive design strategies, regulatory frameworks, and ethics and fairness tools. This figure emphasizes model alignment, risk management, GDPR compliance, and the use of datasets and toolkits to ensure ethical AI practices. Collectively, these ethical guidelines and normative principles form a comprehensive framework for guiding AI development, ensuring technologies are innovative, efficient, and aligned with human values and standards. Ethical Implications of AI Technologies Deploying AI technologies necessitates careful consideration of their ethical implications as these systems integrate into societal frameworks. Biases within AI systems pose significant concerns, adversely affecting fairness in decision-making and perpetuating unintended consequences across domains. For instance, sentiment analysis systems often exhibit biases related to race and gender, highlighting ethical challenges in deploying biased AI technologies [21]. These biases, stemming from polysemous word representations or extensive datasets, require rigorous evaluation and mitigation strategies to prevent exacerbating systemic discrimination. Interpretability and transparency of AI systems are critical for ensuring accountability and trustworthiness in decision-making. The capabilities of large language models (LLMs) to learn rich spatiotemporal representations and embody foundational elements of a world model underscore their potential to influence societal norms and behaviors [8]. This potential emphasizes developers' ethical responsibility to ensure AI outputs align with human values and societal norms, averting ethical pitfalls from misaligned AI actions. Training LLMs on extensive datasets containing copyrighted material raises legal and ethical issues, necessitating frameworks addressing intellectual property rights and ethical implications. Additionally, the alignment and safety of LLMs, highlighted by foundational challenges in deployment, require careful consideration of scientific understanding, development methods, and sociotechnical factors. The potential for LLMs to generate misinformation more challenging to detect than human-written content poses a threat to online safety and public trust, necessitating robust detection and countermeasures. As LLMs advance, aligning them with human values and ensuring responsible use becomes increasingly crucial, given their capacity to surpass human expertise and the evolving role of human evaluation in overseeing AI systems [48,59,60,38]. Misalignment between AI outputs and human preferences complicates the ethical landscape, leading to suboptimal performance in critical tasks like machine translation. Addressing these challenges requires developing benchmarks and methodologies to align AI systems more closely with human values, mitigating risks of unintended harmful outputs. Innovative approaches in self-explaining AI enhance transparency and trust by providing understandable explanations and warnings about potential extrapolation risks. Ensuring transparency in AI systems is crucial for effectiveness and ethical alignment with diverse, sometimes conflicting community values. This involves integrating normative and technical aspects of AI alignment, adopting a principle-based approach reconciling instructions, intentions, preferences, and interests, and addressing the challenge of aligning AI with social values through participatory design processes and reflective endorsement [35,11]. These considerations form a comprehensive framework for exploring ethical implications of deploying AI technologies, guiding responsible and beneficial integration into society. Balancing Technological Advancement and Ethical Responsibility Advancing AI technology presents a complex landscape where ethical responsibility must be balanced with innovation. Inconsistency and variability of ethical principles across AI systems lead to significant gaps in ethical practice [37]. This inconsistency is compounded by the complexity and variability of social norms, often context-dependent and not explicitly stated, complicating the maintenance of a consistent ethical framework [61]. An interdisciplinary approach is essential for bridging normative and technical discussions on AI alignment, integrating diverse viewpoints and methodologies [35]. This approach addresses multifaceted challenges in AI development, ensuring ethical considerations are not sidelined in technological progress. The relationship between principals and agents, particularly in incomplete contracts, highlights the economic dimensions of AI ethics, where delegating decision-making to AI systems necessitates careful consideration of accountability and responsibility [62]. Identifying different kinds of fairness, some of which cannot be simultaneously satisfied with accuracy, underscores inherent trade-offs in achieving ethical AI systems [63]. Directing AI research towards mitigating existential risks emphasizes aligning technological advancement with ethical responsibility [3]. This alignment ensures AI systems contribute positively to society while minimizing potential harms. Additionally, the challenge of removing specific copyrighted content from LLMs without retraining highlights ethical dilemmas in balancing technological capabilities with intellectual property rights [64]. Global governance structures facilitate responsible AI practices by coordinating efforts across jurisdictions and ensuring ethical standards are upheld globally [15]. Introducing methodologies like zero-pass interpretation for analyzing Transformer parameters illustrates the potential for advancements to enhance ethical AI practices by simplifying processes and improving transparency [65]. Collective insights underscore the necessity of upholding ethical responsibility in AI technology advancement. Integrating ethical principles with technological innovation ensures responsible and beneficial AI development. Evaluating numerous AI ethics guidelines reveals commonalities and gaps, suggesting areas for improvement in ethical implementation of AI systems. Moreover, trustworthy AI development requires verifiable claims scrutinized by external stakeholders, ensuring safety, security, fairness, and privacy protection. Aligning AI with human values highlights complexity in intertwining normative and technical aspects, advocating a principle-based approach accommodating diverse moral beliefs while fostering productive engagement across domains [35,36,37]. Interdisciplinary Approaches to AI Ethics Interdisciplinary collaboration is fundamental to ethical AI design and implementation, providing a framework integrating diverse perspectives and methodologies. This survey introduces a semi-systematic evaluation framework categorizing AI ethics guidelines based on principles and recommendations, highlighting overlaps and omissions [37]. This framework underscores interdisciplinary approaches' importance in identifying gaps and ensuring a holistic understanding of ethical principles across domains. Integrating disciplines like computer science, philosophy, sociology, and law creates a framework for addressing complex ethical challenges in AI technologies. This multidisciplinary approach explores normative and technical aspects of AI alignment, evaluates ethical guidelines, assesses AI's understanding of human values, and investigates alignment with diverse social values. By examining interconnected fields, researchers aim to develop fair principles for AI alignment, improve ethical guidelines implementation, and enhance AI's capability to align with shared human values globally [35,11,37,38]. Philosophical inquiry explores normative principles guiding ethical AI behavior, while sociological perspectives emphasize AI systems' impact on social structures and norms. Legal frameworks provide a structured approach to addressing regulatory challenges and ensuring compliance with ethical standards. Interdisciplinary collaboration facilitates developing robust ethical guidelines reflecting social norms' complexity and variability, often context-dependent and not explicitly stated [37]. Integrating knowledge from various disciplines helps navigate the ethical landscape, ensuring AI systems are designed and implemented aligning with human values and societal expectations. Drawing from interdisciplinary approaches, responsible AI deployment is enhanced through ethical alignment and value integration. Addressing normative and technical aspects of AI alignment fosters systems that are technologically advanced and ethically sound, reflecting community values. This involves establishing fair principles for AI alignment, ensuring verifiable claims about AI systems' safety, security, fairness, and privacy, and leveraging datasets like ETHICS to evaluate AI's understanding of moral concepts. Incorporating participatory design processes and adhering to comprehensive ethical guidelines better align AI systems with shared human values and global social value pluralism [35,36,11,37,38]. Machine Learning Alignment The integration of reinforcement learning (RL) with human feedback is crucial for aligning artificial intelligence (AI) systems with human values and preferences, addressing the limitations of traditional RL in designing reward functions that accurately reflect human intentions. The following subsection examines the synergy between RL and human feedback, showcasing innovative methodologies that enhance AI alignment with ethical standards in complex decision-making environments. Reinforcement Learning and Human Feedback Reinforcement learning from human feedback is pivotal for aligning AI models with human preferences and ethical standards, thereby refining decision-making processes in complex environments [66]. Conventional RL faces challenges in designing reward functions that genuinely represent human intentions, necessitating innovative approaches that incorporate human feedback [67]. Preference-based RL provides a framework for training agents using human preferences between behaviors, effectively aligning reward functions with human intent and enhancing policy performance [66]. Value reinforcement learning (VRL) introduces a reward signal to learn a utility function while constraining agent actions to prevent wireheading, ensuring agents do not exploit reward signals in harmful ways [39]. Reward Sketching and Batch Reinforcement Learning (RS-BRL) enable robots to learn from human annotations and a large dataset of task-agnostic experiences, deriving reward functions that guide learning in complex environments [68]. These methodologies underscore the significance of human-centered approaches in refining RL processes and ensuring alignment with human values. The challenge of overoptimizing proxy reward models can lead to performance declines in true reward models, complicating the RL process and necessitating careful management of reward optimization [69]. Addressing hidden incentives that may exploit machine learning systems is crucial for ensuring RL agents operate without unintended consequences [19]. Collectively, these methodologies create a comprehensive framework for leveraging RL and human feedback to align AI models with human values, ensuring responsible deployment in real-world applications. Reward Modeling and Preference Learning Reward modeling and preference learning are essential for aligning AI systems with human values and intended goals, addressing the complexity of capturing human preferences within AI frameworks. The Preference Transformer, utilizing a weighted sum of non-Markovian rewards, offers a novel approach to modeling human preferences, contrasting with previous methods reliant solely on Markovian rewards [66]. Techniques such as Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL) leverage deep Q-learning and backpropagation through communication channels, enabling AI systems to effectively interpret and respond to human intentions [70]. Iterated Amplification, a training strategy that progressively combines solutions to simpler subproblems, enhances the alignment of AI systems with human-centric objectives [71]. The integration of expert demonstrations with trajectory preferences forms a robust approach to training deep neural networks that model reward functions, subsequently used to train RL agents [72]. Preference-based RL minimizes the number of ranking queries required from experts, streamlining the learning process while achieving competent policy outcomes [73]. RS-BRL combines learned reward functions from human annotations with recorded robot experiences to train policies offline, facilitating effective learning across multiple tasks without direct reward signals [68]. This emphasizes the importance of human-centered methodologies in refining RL processes and ensuring alignment with human values. Additionally, VRL introduces a reward signal to learn a utility function while constraining agent actions to prevent wireheading [39]. The sensitivity of reward learning frameworks to errors in human models raises concerns about their viability, as small errors can lead to significant misalignments in inferred rewards [74]. Addressing these issues involves refining reward modeling techniques to ensure accurate representation of human preferences and intentions. Organizing current methods into components such as state representation, policy optimization, reward formulation, and environment building provides a structured approach to categorizing RL algorithms, aiding in the development of more aligned AI systems [75]. These techniques establish a robust framework for modeling rewards and learning preferences, ensuring AI systems align with human values by integrating ethical principles and philosophical insights. This framework leverages datasets like ETHICS to evaluate AI's understanding of moral concepts such as justice and well-being, emphasizing the importance of verifiable claims and accountability in AI development to enhance trust and safety. Furthermore, it incorporates a principle-based approach to AI alignment, systematically combining instructions, intentions, and values to address normative and technical challenges, facilitating the creation of AI systems that reflect fair principles despite moral diversity [35,36,38]. Mechanistic Interpretability and Model Understanding Mechanistic interpretability is crucial in AI research, focusing on elucidating the internal mechanisms of machine learning models to enhance transparency, reliability, and alignment with human values. Recent advancements have introduced methodologies that provide insights into the complex mechanisms underlying model predictions, facilitating better understanding and intervention capabilities. The concept of mechanistic interpretability in transformer models highlights the need for systematic approaches to reverse-engineer model behaviors, enabling researchers to gain deeper insights into information processing [67]. A key innovation is Fast Geometric Ensembling (FGE), allowing for the training of high-performing ensembles in the time required to train a single model. This approach leverages geometric properties of the loss surface, emphasizing their importance in model performance, particularly in transfer learning and domain shifts [69]. Moreover, the interactive interface of Zeno enhances mechanistic interpretability by enabling effective behavioral evaluations, providing a platform for assessing model behavior in real-time [23]. The MAC network significantly advances interpretability by separating control from memory, allowing for more transparent reasoning compared to existing approaches. This separation fosters a clearer understanding of model dynamics, ensuring alignment with human-centric objectives. Additionally, hybridizing attribution patching and edge pruning offers a faster method while maintaining circuit faithfulness, contributing to efficient interpretability [23]. Formal analysis of value alignment grounded in empirically validated cognitive models provides a novel perspective on mechanistic interpretability, offering insights into aligning AI systems with human values. Generative Adversarial Imitation Learning (GAIL) enhances policy learning by modeling the distribution of expert behavior, leading to improved alignment between AI actions and human intentions. The ability to interpret all parameters of a Transformer model in the embedding space contrasts with previous methods reliant on direct model interaction, offering comprehensive understanding of model mechanisms [67]. Furthermore, benchmarks such as Eraser evaluate how well NLP models provide rationales for their predictions, focusing on the alignment between model-generated rationales and human rationales. This alignment is crucial for ensuring AI operates transparently and reliably within human-centric frameworks, integrating normative and technical aspects to create fair principles reflecting diverse moral beliefs. By focusing on value alignment, AI systems can respect the plurality of human values globally, leveraging participatory design processes and ethical benchmarks like the ETHICS dataset. Additionally, mechanisms for making verifiable claims about AI systems enhance trust and accountability, ensuring safety, fairness, and privacy protection across stakeholders [38,35,36,11]. Collectively, these approaches establish a comprehensive framework for mechanistic interpretability and model understanding in AI systems, emphasizing transparency, reliability, and alignment with human values and ethical standards. By incorporating mechanisms for verifiable claims, ethical benchmarking with datasets like ETHICS, and principle-based alignment strategies, this framework addresses the multifaceted challenges of AI development. It ensures AI systems are accountable, scrutinized for safety, security, fairness, and privacy, and capable of making informed moral judgments. Moreover, it advances the science of interpretable machine learning, providing explanations for AI outputs that facilitate qualitative assessments of non-discrimination and safety, especially in complex domains like medicine. This approach is crucial for fostering trust and compliance with regulatory demands, such as the European General Data Protection Regulation, guiding AI systems toward fair principles that receive reflective endorsement across diverse moral beliefs [35,36,12,76,38]. Interdisciplinary Approaches Cognitive Science and Situational Awareness Cognitive science enhances situational awareness in AI systems, particularly large language models (LLMs), by elucidating human cognition and decision-making processes. Interdisciplinary integration, as exemplified by VILLA, leverages insights from embedding spaces to inform adversarial training methods, merging machine learning with cognitive frameworks [77]. This integration improves AI systems' ability to predict and control situational awareness, crucial for safe deployment in dynamic environments [78]. Models emulating human situational awareness bolster AI reliability and robustness, highlighting the necessity of interdisciplinary collaboration to align AI with human values and ethical standards. Coordination Theory and AI Alignment Coordination theory is essential for AI alignment, addressing challenges in coordinating rational actors within uncertain environments. This framework elucidates mechanisms that facilitate collaboration and decision-making among AI systems and human agents, ensuring harmonious societal operation [79]. By applying coordination theory principles, researchers enhance AI systems' alignment with human values and ethical standards, fostering responsible deployment. This approach involves exploring interactions between AI systems and human agents, optimizing communication and cooperation to achieve shared goals. It integrates normative and technical dimensions, considering AI alignment with human values, intentions, and preferences, and proposes algorithms for enhancing cooperation. It contributes to Cooperative AI, equipping systems for effective collaboration across various scales while managing misspecification risks and ensuring AI behavior aligns with human expectations [79,35,80,81]. Addressing coordination challenges ensures AI systems positively impact societal outcomes, mitigating risks of misaligned actions and promoting ethical deployment. Social Norms and Human Behavior Understanding social norms and human behavior is crucial for effective AI alignment strategies. Social norms, unwritten rules governing societal behavior, significantly shape human actions. The framework presented by [61] leverages these norms to predict human behavior, informing AI alignment strategies to meet societal expectations and ethical standards. Human behavior is complex, influenced by cultural, social, psychological factors, implicit biases, and societal norms. Machine learning advances reveal AI systems can replicate human-like biases through learned associations, highlighting cultural and historical impacts. Modeling human behavior remains challenging due to intricate reward inference and adversarial biases. Computational models like Social Chemistry provide insights into moral judgments and social rules, emphasizing a causal framework to address statistical biases in human-AI decision-making [82,61,83,84,74]. AI alignment strategies must consider this complexity, ensuring systems ethically interpret and respond to human actions. Integrating social norms into AI models enhances predictions of human behavior, facilitating collaboration and adaptation to evolving social contexts. The impact of social norms on AI alignment strategies underscores the necessity of understanding values and expectations AI systems must adhere to. Incorporating insights from social sciences and cognitive psychology enables developers to create socially aware systems. This interdisciplinary approach promotes responsible AI deployment by integrating mechanisms for verifiable claims, aligning systems with human values, and addressing existential risks. Focusing on safety, security, fairness, and privacy protection builds trust among stakeholders, including civil society and governments, while minimizing risks associated with misaligned AI actions. This approach emphasizes aligning AI systems with diverse social values, fostering positive societal outcomes and mitigating long-term risks through systematic hazard analysis and participatory design processes [35,85,36,11]. Current Challenges and Future Directions Challenges in Achieving AI Alignment Aligning AI systems with human values presents multifaceted challenges spanning technical, ethical, and operational domains. A key hurdle is the dependence on human annotations to identify and mitigate harmful outputs, which innovative solutions like 'Constitutional AI' aim to address by reducing reliance on human labels to enhance safety [9]. The limitations of models like GPT-4 necessitate moving beyond traditional next-word prediction paradigms to accommodate emerging capabilities [8]. In robotics, reliance on direct reward signals is impractical for real-world scenarios, limiting conventional reinforcement learning methods [68]. The complexity of parameter tuning and model selection further complicates achieving optimal performance [24], while the opaque nature of black-box models challenges structured reasoning and ethical alignment [86]. Deceptive behaviors pose significant safety risks, as traditional safety training often fails to mitigate such behaviors [42]. The difficulty of achieving seamless cooperation in diverse environments is underscored by AI systems' ability to collaborate in zero-shot scenarios [33]. These challenges are intensified by the substantial resource consumption and environmental costs of training large language models, highlighting the urgency of addressing AI alignment issues [26]. Navigating the risks associated with frontier AI technologies requires the interplay of industry self-regulation, societal discourse, and governmental intervention [25]. Standard safety training approaches often fall short in eliminating deceptive behaviors, underscoring the need for robust mechanisms to ensure AI reliability and safety [42]. Collectively, these challenges highlight the pressing need for ongoing research and development to align AI systems with human values, ensuring responsible societal integration. Understanding and Measuring AI Alignment Evaluating AI alignment requires robust methodologies encompassing both technical assessments and interdisciplinary approaches. Table provides a detailed overview of the representative benchmarks utilized in the assessment of AI alignment, highlighting their diverse applications and measurement metrics. The Preference Transformer demonstrates effectiveness by inducing well-specified rewards and attending to critical events in decision-making trajectories, offering insights into AI alignment in complex environments [66]. Iterated Amplification shows promise in learning complex behaviors, providing a viable alternative to traditional human evaluation-based training methods, thus contributing to AI alignment measurement [71]. The RS-BRL framework emphasizes integrating human-centric metrics in AI alignment assessments, evaluating robotic success in manipulation tasks [68]. Robust self-training advancements in adversarial robustness outperform existing state-of-the-art methods by over 5 points in robust accuracy, validating its effectiveness in model alignment and robustness [55]. Interpretation methods in the embedding space abstract model specifics, revealing new avenues for understanding Transformer models and offering insights into AI alignment measurement techniques [65]. Optimization methods and gold reward model performance reveal distinct functional forms, with coefficients scaling with the number of parameters in the reward model, highlighting the complexity of measuring AI alignment [69]. The LSS method's performance, assessed through accuracy, calibration, and robustness metrics, demonstrates its superiority over independently trained ensembles, contributing to a comprehensive evaluation framework for AI alignment [24]. Evaluations of deep reinforcement learning agents focus on task completion based on human feedback, analyzing effectiveness in learning complex behaviors with minimal human input, which is crucial for measuring alignment with human values [18]. Collectively, these evaluative frameworks enhance understanding of AI alignment, ensuring systems excel in performance metrics while adhering to societal and ethical benchmarks. Continuous Evaluation and Adaptation Continuous evaluation and adaptation are vital for maintaining alignment of AI systems with human values and ethical standards. As AI technologies evolve, ongoing assessment and refinement are necessary to ensure robustness and effectiveness across diverse applications. Future research should enhance the robustness of the Reward Sketching and Batch Reinforcement Learning (RS-BRL) method against variations in human annotations and explore additional strategies for reward learning in complex environments, ensuring reinforcement learning methodologies remain adaptable and aligned with human values [68]. Investigating scaling laws in various contexts, such as alternative model architectures or application domains, alongside strategies to improve sample efficiency, will optimize AI systems for alignment [68]. Enhancing feedback mechanisms for complex, varied embodied tasks will deepen understanding of autonomous systems interacting in dynamic environments [27]. Optimizing threshold selection in neural text generation and examining factors influencing text quality will ensure AI systems produce coherent and contextually appropriate outputs, maintaining alignment with user expectations and societal standards [26]. Developing guidelines for implementing proposed safety standards and evaluating their effectiveness in real-world scenarios is essential for translating theoretical safety frameworks into practical measures [25]. These efforts underscore the necessity of maintaining technical proficiency and ethical soundness, aligning with societal expectations in AI's rapid advancement. This process is crucial for building trust and ensuring AI technologies adhere to standards of safety, security, fairness, and privacy while aligning with diverse human values, contributing positively to society as they evolve. By making verifiable claims and implementing accountability mechanisms, AI developers can tackle challenges of value alignment and ethical considerations, fostering responsible AI development reflecting a plurality of global social values [35,36,11,37,38]. Conclusion The survey conducted on AI alignment emphasizes the imperative of developing AI systems that are not only technologically advanced but also ethically attuned to human values. This alignment is essential to mitigate the existential risks posed by advanced AI systems, particularly artificial general intelligence (AGI), which present significant safety challenges requiring robust public policies and sustained research initiatives. The findings underscore the effectiveness of reinforcement learning (RL) and deep reinforcement learning (DRL) methodologies in refining AI systems, notably in enhancing recommendation systems and addressing emerging complexities within the field. The evaluation metrics analysis reveals a critical need to align evaluation methods more closely with the intricacies of natural language generation (NLG) tasks, underscoring the importance of ongoing advancements in this domain. Furthermore, the survey highlights the necessity of balancing technological progress with ethical responsibility, as larger AI models, despite their superior performance, entail substantial environmental and financial costs, calling for a prudent approach to AI development. Experimental outcomes from RAFT demonstrate its capacity to improve model performance in reward learning and align generative models with human ethics and preferences, thus reinforcing the need for robust alignment methodologies. Additionally, insights from incomplete contracting can guide the creation of AI systems that better align with human values and intentions, underscoring the significance of comprehensive approaches to AI value alignment that encompass a broader spectrum of human values. The survey reaffirms the urgent need for ongoing research and interdisciplinary collaboration to address the complex challenges of AI alignment, ensuring that AI systems are not only technologically sophisticated but also ethically aligned and beneficial to society. Key findings suggest that new benchmarks are effective in evaluating alignment techniques for large language models, which bear considerable implications for future research in model training and evaluation.",
  "reference": {
    "1": "2210.01478v3",
    "2": "2206.13353v2",
    "3": "2006.04948v1",
    "4": "1907.09013v1",
    "5": "2312.11671v2",
    "6": "2009.11462v2",
    "7": "2302.08582v2",
    "8": "2303.12712v5",
    "9": "2212.08073v1",
    "10": "1606.08415v5",
    "11": "2101.06060v2",
    "12": "1712.09923v1",
    "13": "2012.07532v1",
    "14": "2212.01681v1",
    "15": "2307.04699v2",
    "16": "2306.12001v6",
    "17": "1706.03741v4",
    "18": "1706.03741v4",
    "19": "2009.09153v1",
    "20": "2302.03025v2",
    "21": "1805.04508v1",
    "22": "2001.08361v1",
    "23": "1506.02078v2",
    "24": "2102.13042v2",
    "25": "2307.03718v4",
    "26": "1904.09751v2",
    "27": "2207.05608v1",
    "28": "2209.11221v2",
    "29": "1911.03429v2",
    "30": "2212.03363v1",
    "31": "1705.05598v2",
    "32": "2302.04023v4",
    "33": "2404.14700v4",
    "34": "1306.6294v2",
    "35": "2001.09768v2",
    "36": "2004.07213v2",
    "37": "1903.03425v2",
    "38": "2008.02275v6",
    "39": "1605.03143v1",
    "40": "2312.09636v1",
    "41": "1702.02284v1",
    "42": "2401.05566v3",
    "43": "2405.06624v3",
    "44": "2211.03540v2",
    "45": "1412.6572v3",
    "46": "2306.06924v2",
    "47": "1706.02409v1",
    "48": "2402.06782v4",
    "49": "2306.09442v3",
    "50": "1906.12340v2",
    "51": "2207.07166v1",
    "52": "2205.12548v3",
    "53": "1712.06751v2",
    "54": "1904.06347v2",
    "55": "1905.13736v4",
    "56": "1912.05671v4",
    "57": "2102.01356v5",
    "58": "2110.02439v2",
    "59": "2404.09932v2",
    "60": "2309.13788v5",
    "61": "2011.00620v3",
    "62": "1804.04268v1",
    "63": "1703.09207v2",
    "64": "2310.02238v2",
    "65": "2209.02535v3",
    "66": "2303.00957v1",
    "67": "2308.02585v3",
    "68": "1909.12200v3",
    "69": "2210.10760v1",
    "70": "1605.06676v2",
    "71": "1810.08575v1",
    "72": "1811.06521v1",
    "73": "1208.0984v1",
    "74": "2212.04717v2",
    "75": "2101.06286v2",
    "76": "1702.08608v2",
    "77": "2006.06195v2",
    "78": "2309.00667v1",
    "79": "1602.03924v1",
    "80": "2103.14659v1",
    "81": "2012.08630v1",
    "82": "2012.15738v1",
    "83": "2302.06503v4",
    "84": "1608.07187v4",
    "85": "2206.05862v7",
    "86": "1803.03067v2"
  },
  "chooseref": {
    "1": "1706.02409v1",
    "2": "1911.02256v1",
    "3": "1506.01170v1",
    "4": "2112.00861v3",
    "5": "2310.12036v2",
    "6": "2305.03452v1",
    "7": "2302.04023v4",
    "8": "2203.11933v4",
    "9": "1806.06877v3",
    "10": "2312.09636v1",
    "11": "2302.03025v2",
    "12": "1208.0984v1",
    "13": "1702.02284v1",
    "14": "1707.07328v1",
    "15": "1805.01109v2",
    "16": "2312.06942v5",
    "17": "1810.01943v1",
    "18": "2006.04948v1",
    "19": "1805.00899v2",
    "20": "2008.02275v6",
    "21": "2103.14659v1",
    "22": "2012.07532v1",
    "23": "2306.12001v6",
    "24": "2010.02695v1",
    "25": "2209.02535v3",
    "26": "2001.09768v2",
    "27": "2111.09884v1",
    "28": "2303.04381v1",
    "29": "1605.03143v1",
    "30": "1903.12261v1",
    "31": "1810.04805v2",
    "32": "2104.03149v3",
    "33": "2009.03136v1",
    "34": "2309.13788v5",
    "35": "2310.06987v1",
    "36": "2303.09387v3",
    "37": "1803.03067v2",
    "38": "2209.09056v2",
    "39": "1606.06565v2",
    "40": "1907.09013v1",
    "41": "1606.03137v4",
    "42": "2310.00374v1",
    "43": "2402.06782v4",
    "44": "1706.03741v4",
    "45": "2212.03827v2",
    "46": "2310.05177v1",
    "47": "2303.08112v6",
    "48": "2012.02096v2",
    "49": "1911.03429v2",
    "50": "1803.00885v5",
    "51": "2204.11966v2",
    "52": "2104.08202v2",
    "53": "2312.11671v2",
    "54": "2107.03374v2",
    "55": "2006.14799v2",
    "56": "1805.04508v1",
    "57": "1412.6572v3",
    "58": "2306.09442v3",
    "59": "1904.06387v5",
    "60": "1703.09207v2",
    "61": "2212.03363v1",
    "62": "2404.09932v2",
    "63": "1902.07742v1",
    "64": "2307.03718v4",
    "65": "1606.08415v5",
    "66": "2009.06367v2",
    "67": "1606.03476v1",
    "68": "2301.04246v1",
    "69": "1801.06503v1",
    "70": "2105.14111v7",
    "71": "2302.06503v4",
    "72": "1906.01081v1",
    "73": "2212.08073v1",
    "74": "2009.09153v1",
    "75": "2404.07900v4",
    "76": "1712.06751v2",
    "77": "2305.00586v5",
    "78": "1811.12231v3",
    "79": "2202.01288v2",
    "80": "2209.14375v1",
    "81": "2305.14325v1",
    "82": "1804.04268v1",
    "83": "2207.05608v1",
    "84": "2307.04699v2",
    "85": "1711.11279v5",
    "86": "1907.02893v3",
    "87": "2406.06560v2",
    "88": "2206.13353v2",
    "89": "2207.07166v1",
    "90": "2104.08696v2",
    "91": "1205.2653v1",
    "92": "2005.14165v4",
    "93": "2212.01681v1",
    "94": "2303.17491v3",
    "95": "2310.02207v3",
    "96": "2006.06195v2",
    "97": "2209.11221v2",
    "98": "1706.03741v4",
    "99": "1705.05598v2",
    "100": "1605.06676v2",
    "101": "1910.13038v2",
    "102": "1306.6294v2",
    "103": "1711.06782v1",
    "104": "1601.03764v6",
    "105": "2205.12411v5",
    "106": "1912.05671v4",
    "107": "2204.12130v2",
    "108": "2102.13042v2",
    "109": "1802.10026v4",
    "110": "1607.06520v1",
    "111": "2211.03540v2",
    "112": "1602.03924v1",
    "113": "2310.15337v1",
    "114": "2012.15738v1",
    "115": "1906.04737v1",
    "116": "2007.09540v1",
    "117": "2303.16200v4",
    "118": "1801.03454v2",
    "119": "1409.0473v7",
    "120": "2103.04000v5",
    "121": "2001.02522v4",
    "122": "2407.04622v2",
    "123": "2101.10098v1",
    "124": "2212.04717v2",
    "125": "2012.08630v1",
    "126": "2003.00688v5",
    "127": "2308.02585v3",
    "128": "1912.02164v4",
    "129": "1707.06354v2",
    "130": "2303.00957v1",
    "131": "2302.08582v2",
    "132": "2102.12452v4",
    "133": "2304.06767v4",
    "134": "2009.11462v2",
    "135": "2102.01356v5",
    "136": "1807.04975v2",
    "137": "2302.10894v3",
    "138": "2209.07858v2",
    "139": "2308.08998v2",
    "140": "2101.06286v2",
    "141": "1705.08417v2",
    "142": "2110.02439v2",
    "143": "2106.07682v1",
    "144": "1811.06521v1",
    "145": "2002.04833v4",
    "146": "2307.08823v1",
    "147": "2205.12548v3",
    "148": "2110.03605v7",
    "149": "2002.09089v4",
    "150": "2406.04093v1",
    "151": "1909.12200v3",
    "152": "2001.08361v1",
    "153": "2210.10760v1",
    "154": "2004.11207v2",
    "155": "2002.05149v6",
    "156": "1706.02515v5",
    "157": "1608.07187v4",
    "158": "1803.01128v3",
    "159": "2401.05566v3",
    "160": "2011.00620v3",
    "161": "2303.12712v5",
    "162": "2309.08600v3",
    "163": "1610.03425v3",
    "164": "2308.03296v1",
    "165": "1810.08575v1",
    "166": "2202.03629v7",
    "167": "1908.07442v5",
    "168": "2309.00667v1",
    "169": "2306.06924v2",
    "170": "2101.06060v2",
    "171": "1904.09751v2",
    "172": "2202.01602v6",
    "173": "1903.03425v2",
    "174": "1611.08219v3",
    "175": "2004.07213v2",
    "176": "1702.08608v2",
    "177": "2304.14997v4",
    "178": "2405.06624v3",
    "179": "1806.07538v2",
    "180": "2203.09509v4",
    "181": "2209.10652v1",
    "182": "2012.09838v2",
    "183": "1703.04730v3",
    "184": "1610.01644v4",
    "185": "1905.13736v4",
    "186": "1904.06347v2",
    "187": "2109.13916v5",
    "188": "1903.02020v2",
    "189": "1906.12340v2",
    "190": "2206.11795v1",
    "191": "1506.02078v2",
    "192": "2312.09390v1",
    "193": "1712.09923v1",
    "194": "1906.04341v1",
    "195": "2210.01478v3",
    "196": "2310.02238v2",
    "197": "2212.10559v3",
    "198": "2206.05862v7",
    "199": "2302.04732v1",
    "200": "2404.14700v4"
  }
}