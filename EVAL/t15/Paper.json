{
  "authors": [
    "Jiaming Ji",
    "Tianyi Qiu",
    "Boyuan Chen",
    "Borong Zhang",
    "Hantao Lou",
    "Kaile Wang",
    "Yawen Duan",
    "Zhonghao He",
    "Jiayi Zhou",
    "Zhaowei Zhang",
    "Fanzhi Zeng",
    "Kwan Yee Ng",
    "Juntao Dai",
    "Xuehai Pan",
    "Aidan O'Gara",
    "Yingshan Lei",
    "Hua Xu",
    "Brian Tse",
    "Jie Fu",
    "S. McAleer",
    "Yaodong Yang",
    "Yizhou Wang",
    "Song-Chun Zhu",
    "Yike Guo",
    "Wen Gao"
  ],
  "literature_review_title": "AI Alignment: A Comprehensive Survey",
  "year": "2023",
  "date": "2023-10-30",
  "category": "cs.AI",
  "abstract": "AI alignment aims to make AI systems behave in line with human intentions and\nvalues. As AI systems grow more capable, so do risks from misalignment. To\nprovide a comprehensive and up-to-date overview of the alignment field, in this\nsurvey, we delve into the core concepts, methodology, and practice of\nalignment. First, we identify four principles as the key objectives of AI\nalignment: Robustness, Interpretability, Controllability, and Ethicality\n(RICE). Guided by these four principles, we outline the landscape of current\nalignment research and decompose them into two key components: forward\nalignment and backward alignment. The former aims to make AI systems aligned\nvia alignment training, while the latter aims to gain evidence about the\nsystems' alignment and govern them appropriately to avoid exacerbating\nmisalignment risks. On forward alignment, we discuss techniques for learning\nfrom feedback and learning under distribution shift. On backward alignment, we\ndiscuss assurance techniques and governance practices.\n  We also release and continually update the website (www.alignmentsurvey.com)\nwhich features tutorials, collections of papers, blog posts, and other\nresources.",
  "structure": [
    {
      "section_title": "Introduction/Pre-section",
      "level": "1",
      "content": "article \\usepackage[T1]{fontenc} % use 8-bit T1 fonts kpfonts styles/template needed_package setspace \\RLxF{RLxF} revisionv5{RGB}{255, 0, 18} \\makeindex \\makeatletter AI Alignment: A Comprehensive Survey \\textbf{Jiaming Ji\\textsuperscript{*,1}~ Tianyi Qiu\\textsuperscript{*,1}~ Boyuan Chen\\textsuperscript{*,1}~ Borong Zhang\\textsuperscript{*,1}~ Hantao Lou\\textsuperscript{1}~ Kaile Wang\\textsuperscript{1}~ Yawen Duan\\textsuperscript{2}~ \\\\ Zhonghao He\\textsuperscript{2}~ Lukas Vierling\\textsuperscript{3}~ Donghai Hong\\textsuperscript{1}~ Jiayi Zhou\\textsuperscript{1}~ Zhaowei Zhang\\textsuperscript{1}~ Fanzhi Zeng\\textsuperscript{1}~ Juntao Dai\\textsuperscript{1}~ \\\\ Xuehai Pan\\textsuperscript{1} Kwan Yee Ng~ Aidan O{'Gara6}~ Hua Xu\\textsuperscript{1}~ Brian Tse~ Jie Fu\\textsuperscript{5}~ Stephen McAleer\\textsuperscript{3} \\\\ Yaodong Yang\\textsuperscript{1, \\Letter} \\, Yizhou Wang\\textsuperscript{1} \\, Song-Chun Zhu\\textsuperscript{1} \\, Yike Guo\\textsuperscript{5} \\, Wen Gao\\textsuperscript{1} 0.8em \\\\ \\textsuperscript{1}Peking University \\, \\textsuperscript{2}University of Cambridge \\, \\textsuperscript{3}University of Oxford \\, \\textsuperscript{4}Carnegie Mellon University \\, \\\\ \\textsuperscript{5}Hong Kong University of Science and Technology \\, \\textsuperscript{6}University of Southern California } document \\maketitle \\; \\textsuperscript{* \\; Equal contribution.} \\; \\textsuperscript{\\Letter Corresponding author. Contact <pku.alignment@gmail.com>.} \\; \\textsuperscript{- \\,\\, Version: v4 (updated on Feb 27, 2024). The content of the survey will be continually updated.} \\newpage { 0.99 \\normalsize \\tableofcontents } \\newpage",
      "origin_cites_number": 0
    },
    {
      "section_title": "Introduction",
      "level": "1",
      "content": "Recent advancements have seen the increasing application of capable AI systems in complex domains. For instance, Large Language Models (LLMs) have exhibited improved capabilities in multi-step reasoning wei2022chain, wang2023self and cross-task generalization brown2020language, askell2021general in real-world deployment settings, and these abilities are strengthened with increased training time, training data, and parameter size kaplan2020scaling, srivastava2023beyond, hoffmann2022empirical. The utilization of Deep Reinforcement Learning (DRL) for the control of nuclear fusion degrave2022magnetic is another notable example. The increasing capabilities and deployment in high-stakes domains come with heightened risks. Various undesirable behaviors exhibited by advanced AI systems (e.g., manipulation perez2022discovering, carroll2023characterizing, sharma2024towards and deception park2023ai) have raised concerns about the hazards from AI systems. Consequently, these concerns have catalyzed research efforts in AI alignment soares2014aligning,christian2020alignment,hendrycks2021unsolved. AI alignment aims to make AI systems behave in line with human intentions and values leike2018scalable, focusing more on the objectives of AI systems than their capabilities. Failures of alignment (i.e., misalignment) are among the most salient causes of potential harm from AI. Mechanisms underlying these failures include reward hacking pan2021effects and goal misgeneralization di2022goal, which are further amplified by double edge components such as situational awareness cotra2022, broadly-scoped goals ngo2024the, mesa-optimization objectives hubinger2019risks, and access to increased resources shevlane2023model (\\Ssec:challenges-of-alignment). Alignment efforts to address these failures focus on accomplishing four key objectives (\\Ssec:RICE): Robustness, Interpretability, Controllability, and Ethicality (RICE). Current research and practice on alignment consist of four areas (\\Ssec:scope): Learning from Feedback (\\Ssec:learning-from-feedback), Learning under Distributional Shift (\\Ssec:distribution), Assurance (\\Ssec:assurance), and Governance (\\Ssec:governance). The four areas and the RICE objectives are not in one-to-one correspondence. Each individual area often serves more than one alignment objective, and vice versa (see Table tab:category-over-RICE). In this survey, we introduce the concept, methodology, and practice of AI alignment and discuss its potential future directions.To help beginners interested in this field learn more effectively, we highlight resources about alignment techniques. More details can be found at \\url{www.alignmentsurvey.com/resources}",
      "origin_cites_number": 14
    },
    {
      "section_title": "The Motivation for Alignment",
      "level": "2",
      "content": "The motivation for alignment is a three-step argument, each step building upon the previous one: (1) Deep learning-based systems (or applications) have an increasingly large impact on society and bring significant risks ; (2) Misalignment represents a significant source of risks; and (3) Alignment research and practice address risks stemming from misaligned systems (e.g., power-seeking behaviors).",
      "origin_cites_number": 0
    },
    {
      "section_title": "Risks of Misalignment",
      "level": "3",
      "content": "With improved capabilities of AI systems, come increased risks.We discuss and taxonomize the risks that might brought by misaligned AI systems, please see \\S\\ref{sec:challenges-of-alignment.} Some undesirable behaviors of LLMs including (but not limited to) untruthful answers bang2023multitask, sycophancy perez2022discovering, sharma2024towards, and deception jacob2023, park2023ai worsen with increased model scale perez2022discovering, resulting in concerns about advanced AI systems that are hard to control. Moreover, emerging trends such as LLM-based agents xi2023rise,wang2023survey also raise concerns about the system's controllability and ethicality chan2023harms. Looking further ahead, the development of increasingly competent AI systems opens up the possibility of realizing Artificial General Intelligence (AGI) in the foreseeable future, i.e., systems can match or surpass human intelligence in all relevant aspects bubeck2023sparks. This could bring extensive opportunities manyika2017future, e.g., automation west2018future, efficiency improvements furman2019ai, but also come with serious risks statement_on_ai_risk_2023,critch2023tasra, such as safety concerns hendrycks2022x, biases and inequalities ntoutsi2020bias, and large-scale risks from superhuman capabilities~rogueai. Taking biases as an example, cutting-edge LLMs manifest discernible biases about gender, sexual identity, and immigrant status among others perez2022discovering, which could reinforce existing inequalities. Within the large-scale risks from superhuman capabilities, it has been conjectured that global catastrophic risks (i.e., risks of severe harms on a global scale) bostrom2011global,hendrycks2023overview, gov_uk_2023 and existential risks (i.e., risks that threaten the destruction of humanity's long-term potential) from advanced AI systems are especially worrying. These concerns are elaborated in first-principle deductive arguments firstprinc,rogueai, evolutionary analysis hendrycks2023natural, and concrete scenario mapping christiano2019failure,threatmodel2022. In statement_on_ai_risk_2023, leading AI scientists and other notable figures stated that Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war. The median researcher surveyed by stein2022expert at NeurIPS 2021 and ICML 2021 reported a 5\\% chance that the long-run effect of advanced AI on humanity would be extremely bad (\\textit{e.g., human extinction)}, and 36\\% of NLP researchers surveyed by michael2022nlp self-reported to believe that AI could produce catastrophic outcomes in this century, on the level of all-out nuclear war.However, survey results may hinge upon the exact wording of the questions and should be taken with caution. Existential risks from AI also include risks of lock-in, stagnation, and more bostrom2013existential,hendrycks2022x, in addition to extinction risks.\\textit{Existential and extinction risks are two concepts that are often mixed up. The latter is a subset of the former.} The UK have hosted the world's first global AI Safety Summit, gathering international governments, leading AI companies, civil society groups, and research experts. Its objectives are to: (1) assess the risks associated with AI, particularly at the cutting edge of its development; (2) explore how these risks can be mitigated through internationally coordinated efforts.Source from \\url{https://www.gov.uk/government/topical-events/ai-safety-summit-2023.} The summit culminated in the Bletchley Declaration bletch2023summit, which highlighted the importance of international cooperation on AI safety. It was signed by representatives from 28 countries and the EU. Current cutting-edge AI systems have exhibited multiple classes of undesirable or harmful behaviors that may contrast with human intentions (e.g., power-seeking and manipulation) si2022so,pan2023machiavelli, and similar worries about more advanced systems have also been raised critch2020ai, statement_on_ai_risk_2023.See \\S\\ref{sec:challenges-of-alignment for an introduction to specific misalignment challenges.} These undesirable or harmful behaviors not compliant with human intentions, known as misalignment of AI systemsSome of the misaligned behaviors are less risky (\\textit{e.g., the agent fails to clean the room as you want), however, some of them are dangerous for systems applied in the high-stakes environment (e.g., the control of nuclear fusion degrave2022magnetic)}, can naturally occur even without misuse by malicious actors and represent a significant source of risks from AI, including safety hazards hendrycks2021unsolved and potential existential risks hendrycks2023overview.It should be noted that misalignment cannot cover all sources of risks brought by Deep learning-based systems and other factors such as misuse and negligence also contribute to risks on society. See \\S\\ref{sec:ai-safety-beyond for discussing AI safety beyond alignment.} These large-scale risks are significant in size due to the non-trivial likelihoods of (1) building superintelligent AI systems, (2) those AI systems pursuing large-scale goals, (3) those goals are misaligned with human intentions and values, and (4) this misalignment leads to humans losing control of humanity's future trajectory firstprinc. Solving the risks brought by misalignment requires the alignment of AI systems to ensure the objectives of the system are in accordance with human intentions and values, thereby averting unintended and unfavorable outcomes. More importantly, we expect the alignment techniques to be scaled to harder tasks and significantly advanced AI systems that are even smarter than humans. A potential solution is SuperalignmentFor more details on Superalignment, you can refer to \\url{https://openai.com/blog/introducing-superalignment.}, which aims to build a roughly human-level automated alignment researcher, thereby using vast amounts of compute to scale up and iteratively align safe superintelligence superalignment.",
      "origin_cites_number": 31
    },
    {
      "section_title": "Causes of Misalignment",
      "level": "3",
      "content": "In the above section, we have concluded the motivation for alignment from the perspective of the concern for AI risks and technical ethics. To offer a deeper understanding of alignment, we aim to further analyze why and how the misalignment issues occur. We will first give an overview of common failure modes, and then focus on the mechanism of feedback-induced misalignment, and finally shift our emphasis towards an examination of misaligned behaviors and dangerous capabilities. In this process, we introduce the concept of double edge components, which offer benefits for enhancing the capabilities of future advanced systems but also bear the potential for hazardous outcomes. Overview of Failure Modes In order to illustrate the misalignment issue, we give an overview of alignment failure modes in this section, most of which can be categorized into reward hacking\\textit{Reward hacking can also be broadly considered as a kind of specification gaming.} and goal misgeneralization. The learning process of RL can be deconstructed into two distinct phases: firstly, the creation of an agent primed for reward optimization, and secondly, the establishment of a reward process that furnishes the agent with appropriate reward signals. Within the framework of the Markov Reward Process marbach2001simulation, puterman2014markov, sutton2018reinforcement, the former phase can be seen as the learning process related to the transition model (e.g., model-based RL agents moerland2023model), or the development of specialized algorithms. The latter phase can be viewed as the construction of proxy rewards, which aim to approximate the true rewards derived from sources (e.g., human preferences or environment) ng2000algorithms,leike2018scalable. Reward Hacking: In practice, proxy rewards are often easy to optimize and measure, yet they frequently fall short of capturing the full spectrum of the actual rewards pan2021effects. This limitation is denoted as misspecified rewards.A similar definition is reward misidentification in which scenario the reward function is only partially identifiable. For more details on reward misidentification, see \\textit{e.g., tien2022causal,skalse2023invariance} The pursuit of optimization based on such misspecified rewards may lead to a phenomenon known as reward hacking, wherein agents may appear highly proficient according to specific metrics but fall short when evaluated against human standards amodei2016concrete,everitt2017reinforcement. The discrepancy between proxy rewards and true rewards often manifests as a sharp phase transition in the reward curve ibarz2018reward. Furthermore, skalse2022defining defines the hackability of rewards and provides insights into the fundamental mechanism of this phase transition, highlighting that the inappropriate simplification of the reward function can be a key factor contributing to reward hacking. Misspecified rewards often occur due to a neglect of severe criteria for the outcomes, thus making specification too broad and potentially easily hacked specification2020victoria. More than poor reward design ng1999policy, the choice of training environment and simulator with bugs bullet2022 can both lead to AI systems failing to satisfy intended objectives. These problems stem from task specification, broadly defined as specification gaming, which refers to AI systems exploiting loopholes in the task specification without achieving intended outcomes.For more instances about specification gaming, please see \\citet{instances2020} specification2020victoria Reward tampering can be considered a special case of reward hacking everitt2021reward, skalse2022defining, referring to AI systems corrupting the reward signals generation process ring2011delusion. everitt2021reward delves into the subproblems encountered by RL agents: (1) tampering of reward function, where the agent inappropriately interferes with the reward function itself, and (2) tampering of reward function input, which entails corruption within the process responsible for translating environmental states into inputs for the reward function. When the reward function is formulated through feedback from human supervisors, models can directly influence the provision of feedback (e.g., AI systems intentionally generate challenging responses for humans to comprehend and judge, leading to feedback collapse) leike2018scalable. Since task specification has its physical instantiation (e.g., memory registers storing the reward signals), the AI systems deployed in the real world have the potential to practice manipulation behaviors, resulting in more hazardous outcomes specification2020victoria. Moreover, it has been demonstrated that easily discovered reward tampering behaviors can generalize to sophisticated specification gaming, which cannot be prevented by using 3H environments or preference reward modeling training anthropic_reward_tampering. Goal Misgeneralization: Goal misgeneralization is another failure mode, wherein the agent actively pursues objectives distinct from the training objectives in deployment while retaining the capabilities it acquired during training di2022goal.More discussion about Goal Misgeneralization can be found in \\S\\ref{sec:chal-dis.} For instance, in CoinRun games, the agent frequently prefers reaching the end of a level, often neglecting relocated coins during testing scenarios. di2022goal draw attention to the fundamental disparity between capability generalization and goal generalization, emphasizing how the inductive biases inherent in the model and its training algorithm may inadvertently prime the model to learn a proxy objective that diverges from the intended initial objective when faced with the testing distribution. It implies that even with perfect reward specification, goal misgeneralization can occur when faced with distribution shifts amodei2016concrete. It should be noted that goal misgeneralization can occur in any learning system, not limited to RL since the core feature is the pursuit of unintended goals examples2022. Moreover, it might be more dangerous if advanced AI systems escape control and leverage their capabilities to bring about undesirable states zhuang2020consequences. Feedback-Induced Misalignment With the proliferation of advanced AI systems, the challenges related to reward hacking and goal misgeneralization have become increasingly pronounced in open-ended scenarios paulus2017deep, knox2023reward. gao2023scaling underscores that more capable agents tend to exploit misspecified rewards to a greater extent. While many current AI systems are primarily driven by self-supervision, it's worth noting that a substantial portion relies on feedback rewards derived from human advisors bai2022training, allowing us to introduce the mechanism of feedback-induced misalignment. The misalignment issues are particularly pressing in open-ended scenarios, and we can attribute them to two primary factors: itemize[left=0.3cm] \\item Limitations of Human Feedback. During the training of LLMs, inconsistencies can arise from human data annotators (e.g., the varied cultural backgrounds of these annotators can introduce implicit biases peng2022investigations) openai2023gpt4. Moreover, they might even introduce biases deliberately, leading to untruthful preference data casper2023open. For complex tasks that are hard for humans to evaluate (e.g., the value of game state), these challengesAs AI systems are deployed into more complex tasks, these difficulties amplify, necessitating novel solutions such as \\textit{scalable oversight cotra2018iterated.} become even more salient irving2018ai. \\item Limitations of Reward Modeling. Training reward models using comparison feedback can pose significant challenges in accurately capturing human values. For example, these models may unconsciously learn suboptimal or incomplete objectives, resulting in reward hacking zhuang2020consequences,skalse2022defining. Meanwhile, using a single reward model may struggle to capture and specify the values of a diverse human society casper2023open. itemize Additionally, huang2023inner,andreas2022language,kim2024language demonstrate that advanced AI systems exhibit patterns of goal pursuit and multi-step reasoning capability, which further aggravate the situation if the reward is not well-defined ngo2024the,yang2023foundation. Discussion: It can be challenging to distinguish goal misgeneralization from reward hacking in specific cases. For instance examples2022, LLMs are trained to generate harmless, honest, and helpful outputs, but LLMs may occasionally produce harmful outputs in detail, which seemingly receive low rewards in testing distribution (which could be seen as goal misgeneralization). However, in cases where labelers are incentivized to assign high rewards to responses deemed more helpful during the labeling process, the scenarios aboveHarmful but detailed responses actually receive high rewards and represent a form of specification gaming (or reward hacking). The distinction between these two scenarios can be vague at times. More research is needed to analyze the failure modes, gain a deeper understanding of reward hacking, and develop effective methods for detecting and mitigating goal misgeneralization to address the challenges of misaligned advanced AI systems. Misaligned Behaviors and Outcomes Drawing from the misalignment mechanism, optimizing for a non-robust proxy may result in misaligned behaviors, potentially leading to even more catastrophic outcomes. This section delves into a detailed exposition of specific misaligned behaviors ($\\bullet$) and introduces what we term double edge components (+). These components are designed to enhance the capability of AI systems in handling real-world settings but also potentially exacerbate misalignment issues. {It should be noted that some of these double edge components (+) remain speculative. Nevertheless, it is imperative to discuss their potential impact before it is too late, as the transition from controlled to uncontrolled advanced AI systems may be just one step away ngo2020continuing.} With increased model scale, a class of dangerous capabilities (*) shevlane2023model could also emerge. The \\textbf {dangerous capabilities} (*) are concrete tasks the AI system could carry out; they may not necessarily be misaligned in themselves but are instrumental to actualizing extreme risks. We first introduce the double edge components (+) and analyze how they act on AI systems. Then, we illustrate the misaligned behaviors ($\\bullet$) and dangerous capabilities (*) to show specific misalignment issues and provide directions for future alignment evaluation research. itemize[left=0.35cm] \\item [+] Situational Awareness. AI systems may gain the ability to effectively acquire and use knowledge about its status, its position in the broader environment, its avenues for influencing this environment, and the potential reactions of the world (including humans) to its actions~cotra2022. Similar behaviors have been observed in LLMs jonas2022,evan2023. Knowing the situation can help the model better understand human intent, finish tasks within its ability, and search for outlier help if needed. However, such knowledge also paves the way for advanced methods of reward hacking, heightened deception/manipulation skills, and an increased propensity to chase instrumental subgoals ngo2024the. Consequently, it should be given priority when evaluating potentially hazardous capabilities in AI models, alongside eight other key competencies shevlane2023model. A highly relevant discussion is whether language models possess world models lecun2022path,li2022emergent. \\item [+] Broadly-Scoped Goals. Advanced AI systems are expected to develop objectives that span long timeframes, deal with complex tasks, and operate in open-ended settings ngo2024the. Engaging in broadly-scoped planning can empower AI systems to generalize better on the OOD settings and serve as valuable assistants in realms such as human healthcare. However, it can also bring about the risk of encouraging manipulating behaviors (e.g., AI systems may take some bad actions to achieve human happiness, such as persuading them to do high-pressure jobsThis behavior is due to models' over-optimization for broadly-scoped goals and this over-optimization is hard to perceive by humans jacob2023). Intuitively, one approach to mitigate this risk is to confine the optimizable objectives to short-sighted ones, such as predicting only the next word, thereby preventing over-ambitious planning, but such approaches limit systems' utility and may fail; for instance, source text data (e.g., fiction) can help AI systems understand the intent and belief of the roles, and thus longer-term goal-directed behavior can be elicited andreas2022language. Additionally, techniques such as RL-based fine-tuning christiano2017deep,ouyang2022training or the application of chain-of-thought prompts wei2022chain can enable models to adapt their acquired knowledge about planning to pave the way for broadly-scoped planning objectives jacob2023. \\item [+] Mesa-Optimization Objectives. The learned policy may pursue inside objectives when the learned policy itself functions as an optimizer (i.e., mesa-optimizer). However, this optimizer's objectives may not align with the objectives specified by the training signals, and optimization for these misaligned goals may lead to systems out of control hubinger2019risks. freeman2019learning,wijmans2023emergence indicate that AI systems may possess implicit goal-directed planning and manifest emergent capabilities during the generalization phase. \\item [+] Access to Increased Resources. Future AI systems may gain access to websites and engage in real-world actions, potentially yielding a more substantial impact on the world nakano2021webgpt. They may disseminate false information, deceive users, disrupt network security, and, in more dire scenarios, be compromised by malicious actors for ill purposes. Moreover, their increased access to data and resources can facilitate self-proliferation, posing existential risks shevlane2023model. \\item Power-Seeking Behaviors. AI systems may exhibit behaviors that attempt to gain control over resources and humans and then exert that control to achieve its assigned goal carlsmith2022power. The intuitive reason why such behaviors may occur is the observation that for almost any optimization objective (e.g., investment returns), the optimal policy to maximize that quantity would involve power-seeking behaviors (e.g., manipulating the market), assuming the absence of solid safety and morality constraints. omohundro2008basic,bostrom2012superintelligent have argued that power-seeking is an instrumental subgoal which is instrumentally helpful for a wide range of objectives and may, therefore, be favored by AI systems. turner2021optimal also proved that in MDPs that satisfy some standard assumptions, the optimal policies tend to be power-seeking. perez2022discovering prompt LLMs to test their tendency to suggest power-seeking behaviors, find significant levels of such tendencies, and show that RLHF strengthens them. This also holds for other instrumental subgoals such as self-preservation bostrom2012superintelligent,shevlane2023model. Another notable line of research is side-effect avoidance, which aims to address power-seeking behaviors by penalizing agentic systems for having too much influence over the environment. It covers RL systems eysenbach2018leave,turner2020avoiding and symbolic planning systems klassen2022planning. \\item Untruthful Output. AI systems such as LLMs can produce either unintentionally or deliberately inaccurate output. Such untruthful output may diverge from established resources or lack verifiability, commonly referred to as hallucination bang2023multitask, zhao2023survey. More concerning is the phenomenon wherein LLMs may selectively provide erroneous responses to users who exhibit lower levels of educationSuch behaviors bare termed \\textit{sandbagging perez2022discovering. They may have been learned from web text during pre-training, which suggests that supervised learning can also bring about deceptive behaviors if those behaviors are present in training data.} perez2022discovering. The behavior (also known as sycophancy) appears emergently at scale cotra2021,perez2022discovering and untruthful output has the potential to engender deception, especially as advanced AI systems gain greater access to online resources and websites jacob2023. \\item Deceptive Alignment \\& Manipulation. Manipulation \\& Deceptive Alignment is a class of behaviors that exploit the incompetence of human evaluators or users hubinger2019deceptive,carranza2023deceptive and even manipulate the training process through gradient hacking ngogra2022. These behaviors can potentially make detecting and addressing misaligned behaviors much harder. Deceptive Alignment: Misaligned AI systems may deliberately mislead their human supervisors instead of adhering to the intended task. Such deceptive behavior has already manifested in AI systems that employ evolutionary algorithms wilke2001evolution,hendrycks2021unsolved. In these cases, agents evolved the capacity to differentiate between their evaluation and training environments. They adopted a strategic pessimistic response approach during the evaluation process, intentionally reducing their reproduction rate within a scheduling program lehman2020surprising. Furthermore, AI systems may engage in intentional behaviors that superficially align with the reward signal, aiming to maximize rewards from human supervisors ouyang2022training,lang2024your. It is noteworthy that current large language models occasionally generate inaccurate or suboptimal responses despite having the capacity to provide more accurate answers \\cite {lin2022truthfulqa,chen2021evaluating}. These instances of deceptive behavior present significant challenges. They undermine the ability of human advisors to offer reliable feedback (as humans cannot make sure whether the outputs of the AI models are truthful and faithful). Moreover, such deceptive behaviors can propagate false beliefs and misinformation, contaminating online information sources hendrycks2021unsolved,chen2024can. Manipulation: Advanced AI systems can effectively influence individuals' beliefs, even when these beliefs are not aligned with the truth shevlane2023model. These systems can produce deceptive or inaccurate output or even deceive human advisors to attain deceptive alignment. Such systems can even persuade individuals to take actions that may lead to hazardous outcomes openai2023gpt4. Early-stage indications of such behaviors are present in LLMs,Namely, the \\emph{untruthful output that we discuss above.} recommender systems (where the system influences the users' preferences) kalimeris2021preference, krueger2020hidden, adomavicius2022recommender, and RL agents (where agents trained from human feedback adopt policies to trick human evaluators) learning2017dario. Also, current LLMs already possess the capability needed for deception. In spitale2023ai, it has been found that GPT-3 is super-human capable of producing convincing disinformation. Given all these early-stage indications, it is plausible that more advanced AI systems may exhibit more serious deceptive/manipulative behaviors. \\item Collectively Harmful Behaviors. AI systems have the potential to take actions that are seemingly benign in isolation but become problematic in multi-agent or societal contexts. Classical game theory offers simplistic models for understanding these behaviors. For instance, phelps2023investigating evaluates GPT-3.5's performance in the iterated prisoner's dilemma and other social dilemmas, revealing limitations in the model's cooperative capabilities. perolat2017multiagent executes a parallel analysis focused on common-pool resource allocation. To mitigate such challenges, the emergent field of Cooperative AI dafoe2020open,dafoe2021cooperative has been advancing as an active research frontier. However, beyond studies grounded in simplified game-theoretical frameworks, there is a pressing need for research in more realistic, socially complex settings singh2014norms. In these environments, agents are numerous and diverse, encompassing AI systems and human actors critch2020ai. Furthermore, the complexity of these settings is amplified by the presence of unique tools for modulating AI behavior, such as social institutions and norms singh2014norms.We cover cooperative AI research in \\S\\ref{sec:cooperative-ai-training and \\Ssubsec:formal-ethics-coop.} \\item Violation of Ethics. Unethical behaviors in AI systems pertain to actions that counteract the common good or breach moral standards -- such as those causing harm to others. These adverse behaviors often stem from omitting essential human values during the AI system's design or introducing unsuitable or obsolete values into the system kenward2021machine. Moreover, recent works have found that current LLMs can infringe upon personal privacy by inferring personal attributes from the context provided during inference, which may violate human rights mireshghallah2024can,staab2024beyond. Research efforts addressing these shortcomings span the domain of machine ethics yu2018building, winfield2019machine, tolmeijer2020implementations and delve into pivotal questions, e.g., whom should AI align with? santurkar2023whose, among other concerns. \\item [*] Dangerous Capabilities. Figure fig:failure outlines the dangerous capabilities that advanced AI systems might have. As AI systems are deployed in the real world, they may pose risks to society in many ways (e.g., hack computer systems, escape containment, and even violate ethics). They may hide unwanted behaviors, fool human supervisors, and seek more resources to become more powerful. Moreover, double edge components (+) may intensify the danger and lead to more hazardous outcomes, even resulting in existential risks bostrom2013existential. itemize",
      "origin_cites_number": 90
    },
    {
      "section_title": "The Scope of Alignment",
      "level": "2",
      "content": "In this section, we focus on illustrating the scope of AI alignment: we constructed the alignment process as an alignment cycle and decomposed it into Forward Alignment Process and Backward Alignment ProcessFrom this point and throughout the survey, for convenience, we refer to ``Forward Alignment'' and ``Backward Alignment''. (\\Ssec:forward-backward). Specifically, we discuss the role of human values in alignment (\\Ssec:values-in-intro) and further analyze AI safety problems beyond alignment (\\Ssec:ai-safety-beyond).",
      "origin_cites_number": 1
    },
    {
      "section_title": "The Alignment Cycle: A Framework of Alignment",
      "level": "3",
      "content": "We decompose alignment into {myredForward Alignment} (alignment training) (\\Ssec:learning-from-feedback, \\Ssec:distribution) and {myblueBackward Alignment} (alignment refinement) (\\Ssec:assurance, \\Ssec:governance). Forward Alignment aims to produce trained systems that follow alignment requirements.Here, \\emph{alignment requirements refer to an operationalized specification of the alignment properties that are desired of the AI systems, including, for example, which concrete forms of robustness/interpretability/controllability/ethicality we require, in what specific settings we require them, and how they could be measured.} We decompose this task into Learning from Feedback (\\Ssec:learning-from-feedback) and Learning under Distribution Shift (\\Ssec:distribution). Backward Alignment aims to ensure the practical alignment of the trained systems by performing evaluations in both simplistic and realistic environments and setting up regulatory guardrails to handle real-world complexities,i.e., Assurance (\\Ssec:assurance). It also covers the creation and enforcement of rules that ensure the safe development and deployment of AI systems, i.e., Governance (\\Ssec:governance). At the same time, backward alignment updates the alignment requirements based on the evaluation and monitoring of the systems, both pre-deployment and post-deployment. These updated requirements then inform the next round of alignment training. The two phases, forward and backward alignment, thus form a cycle where each phase produces or updates the input of the next phase (see Figure fig:maindiagram). This cycle, what we call the alignment cycle, is repeated to produce increasingly aligned AI systems. We see alignment as a dynamic process in which all standards and practices should be continually assessed and updated. Notably, Backward Alignment (including the Assurance of alignment in AI systems and the Governance of AI systems) efforts occur throughout the entire alignment cycle, as opposed to only after training. As argued in shevlane2023model,koessler2023risk, alignment and risk evaluations should occur in every stage of the system's lifecycle, including before, during, after training, and post-deployment. Similarly, regulatory measures for every phase of the system's lifecycle have been proposed and discussed schuett2023towards, anderljung2023frontier. The survey is structured around four core pillars: Learning from Feedback (\\Ssec:learning-from-feedback) and Learning under Distribution Shift (\\Ssec:distribution), which constitute the components of Forward Alignment; and Assurance (\\Ssec:assurance) and Governance (\\Ssec:governance) which form the elements of Backward Alignment. The subsequent paragraphs provide a concise introduction to each pillar, clarifying how they synergistically contribute to a comprehensive framework for AI alignment. itemize[left=0.3cm] \\item{Learning from Feedback {\\normalfont(\\Ssec:learning-from-feedback)}} Learning from feedback concerns the question of during alignment training, how do we provide and use feedback to behaviors of the trained AI system? It takes an input-behavior pair as given and only concerns how to provide and use feedback on this pair.Here, \\textit{behavior is broadly defined also to include the system's internal reasoning, which can be examined via interpretability tools (see \\Ssec:interpretability).} In the context of LLMs, a typical solution is reinforcement learning from human feedback (RLHF) christiano2017deep, bai2022training, where human evaluators provide feedback by comparing alternative answers from the chat model, and the feedback is used via Reinforcement Learning (RL) against a trained reward model. Despite its popularity, RLHF faces many challenges pandey2022modeling, casper2023open, tien2022causal, overcoming which has been a primary objective of alignment research bowman2022measuring, and is one primary focus of the section. An outstanding challenge here is scalable oversight (\\Sssec:scalable_oversight), i.e., providing high-quality feedback on super-human capable AI systems that operate in complex situations beyond the grasp of human evaluators, where the behaviors of AI systems may not be easily comprehended and evaluated by humans bowman2022measuring. Another challenge is the problem of providing feedback on ethicality, which is approached by the direction of machine ethics anderson2011machine, tolmeijer2020implementations. On the ethics front, misalignment could also stem from neglecting critical dimensions of variance in values, such as underrepresenting certain demographic groups in feedback data santurkar2023whose. There have also been work combining feedback mechanisms with social choice methods to produce a more rational and equitable aggregation of preferences CIPwhitepaper (see \\Ssec:values-in-intro). \\item{Learning under Distribution Shift {\\normalfont(\\Ssec:distribution)}} In contrast to learning from feedback, which holds input fixed, this pillar focuses specifically on the cases where the distribution of input changes, i.e., where distribution shift occurs krueger2020hidden,thulasidasan2021effective,hendrycks2021many. More specifically, it focuses on the preservation of alignment properties (i.e., adherence to human intentions and values) under distribution shift, as opposed to that of model capabilities. In other words, it asks how we can ensure an AI system well-aligned on the training distribution will also be well-aligned when deployed in the real world. One challenge related to distribution shift is goal misgeneralization, where, under the training distribution, the intended objective for the AI system (e.g., following human's real intentions) is indistinguishable from other unaligned objectives (e.g., gaining human approval regardless of means). The system learns the latter, which leads to unaligned behaviors in deployment distribution di2022goal. Another related challenge is auto-induced distribution shift (ADS), where an AI system changes its input distribution to maximize reward krueger2020hidden,perdomo2020performative. An example would be a recommender system shaping user preferences kalimeris2021preference,adomavicius2022recommender. Both goal misgeneralization and ADS are closely linked to deceptive behaviors park2023ai and manipulative behaviors shevlane2023model in AI systems, potentially serving as their causes. Interventions that address distribution shift include algorithmic interventions (\\Ssec:alg-inter), which changes the training process to improve reliability under other distributions, and data distribution interventions (\\Ssec:data-inter) which expands the training distribution to reduce the discrepancy between training and deployment distributions. The former includes methods like Risk Extrapolation (REx) krueger2021out and Connectivity-based Fine-tuning (CBFT) lubana2023mechanistic. The latter includes adversarial training (\\Ssec:adversarial-training) song2018constructing,tao2021recent which augments training input distribution with adversarial inputs, and cooperative training (\\Ssec:cooperative-ai-training) dafoe2020open, dafoe2021cooperative which aims to address the distribution gap between single-agent and multi-agent settings.Cooperative Training aims to make AI systems more cooperative in multi-agent settings. This cooperativeness addresses multi-agent failure modes where the AI system's behavior appears benign and rational in isolation but becomes problematic within social or multi-agent scenarios~\\citep{critch2020ai; see collectively harmful behaviors in \\Ssec:hazardous for a more detailed account.} \\item{Assurance {\\normalfont(\\Ssec:assurance)}} Once an AI system has undergone forward alignment, we still need to gain confidence about its alignment before deploying it ukassurance, anderljung2023frontier. Such is the role of assurance: assessing the alignment of trained AI systems. Methodologies of assurance include safety evaluations perez2022discovering,shevlane2023model (\\Ssec:safe) and more advanced methods such as interpretability techniques olah2018building (\\Ssec:interpretability) and red teaming perez2022red (\\Ssubsec:red teaming). The scope of assurance also encompasses the verification of system's alignment with human values, including formal theories focused on provable cooperativeness dafoe2021cooperative and ethicality anderson2011machine, tolmeijer2020implementations, and also a wide range of empirical and experimental methods (\\Ssec:human-values-alignment). Assurance takes place throughout the lifecycle of AI systems, including before, during, after training, and post-deployment, as opposed to only after training shevlane2023model,koessler2023risk.Furthermore, it's noteworthy that many techniques here are also applicable in the training process, \\textit{e.g., red teaming is a key component of adversarial training (see \\Ssec:adversarial-training), and interpretability can help with giving feedback burns2022discovering.} \\item{Governance {\\normalfont(\\Ssec:governance)}} Assurance alone cannot provide full confidence about a system's practical alignment since it does not account for real-world complexities. This necessitates governance efforts of AI systems that focus on their alignment and safety and cover the entire lifecycle of the systems (\\Ssec:the-role-of-ai-governance). We discuss the multi-stakeholder approach of AI governance, including the governmental regulations anderljung2023frontier, the lab self-governance schuett2023towards, and the third-party practice, such as auditing shevlane2023model,koessler2023risk (\\Ssec:multi-stake). We also highlight several open problems in AI governance, including the pressing challenge of open-source governance (the governance of open-source models and the question of whether to open-source highly capable models) seger2023open, and the importance of international coordination in AI governance ho2023international (\\Ssec:open-problems). In addition to policy research, we also cover key actions from both the public and the private sector. itemize Comparison with Inner/Outer Decomposition Our alignment cycle framework (see Figure fig:maindiagram) decomposes alignment into four pillars: Learning from Feedback, Learning under Distribution Shift, Assurance and Governance organized into a circular process. The design principle for this framework is three-fold: Practical (making sure pillars directly correspond to specific practices in specific stages in the system's lifecycle), Concrete (pointing to specific research directions as opposed to general themes), and Up-To-Date (accommodating and emphasizing latest developments in the alignment field). Recently, the decomposition of alignment into outer alignment and inner alignment has become popular in the alignment literature inneralignment. Outer alignment refers to the wishes of designers in accordance with the actual task specification (e.g., goal \\& reward) used to build AI systems. And inner alignment is the consistency between task specification and the specification that the AI systems behaviors reflect vkrpara. However, many criticisms have also been made about this characterization, including that it is ambiguous and is understood by different people to mean different things innerouter and that it creates unnecessary difficulties by carving out problems that are not necessary conditions for success harderprob. Some have tried to remove the ambiguity by pinning down the specific causes of inner/outer misalignment and proposed, for example, goal misspecification and goal misgeneralization di2022goal,vkrpara. Learning from Feedback (approximately corresponding to goal misspecification and outer alignment) and Learning under Distribution shift (approximately corresponding to goal misgeneralization and inner alignment) in our framework tries to further improve upon the inner/outer decomposition by clarifying the exact approaches taken to address the challenges and resolving the ambiguity. Assurance and Governance, on the other hand, expands the scope to cover topics beyond outer and inner alignment. Theoretical Research in Alignment The alignment research literature also contains a wealth of theoretical work amodei2016concrete,everitt2018agi,hendrycks2021unsolved. These works often propose new directions and provide a foundation for practical and empirical research to build upon. We give a brief overview of this body of theoretical research below: itemize[left=0.3cm] \\item {Conceptual Frameworks}. Some theoretical work proposes conceptual frameworks or characterizes subproblems within alignment. Examples include instrumental convergence (wherein highly intelligent agents tend to pursue a common set of sub-goals, such as self-preservation and power-seeking) omohundro2008basic,bostrom2012superintelligent, mesa-optimization (wherein the learned ML model performs optimization within itself during inference) hubinger2019risks, and specific proposals for building aligned systems, such as approval-directed agents (wherein the AI system does not pursue goals, but seek the human's idealized post hoc approval of action consequences) oesterheld2021approval,christiano2022approval. hadfield2019incomplete,cotra2021the have drawn inspiration from economics, linking problems in alignment with markets and principal-agent problems in economics. elk_intro,elk_summary have proposed the problem of eliciting latent knowledge of advanced AI systems and have explored high-level approaches to the problem. \\item {Mathematical Formulations}. Other theoretical works have aimed to formulate sub-problems within alignment mathematically and seek formal solutions. soares2015corrigibility formulates the problem of corrigibility (i.e., ensuring AI systems are incentivized to allow shutdown or objective modification by the instructor). benson2016formalizing gives a mathematical formulation of instrumental convergence. hadfield2017off proposes the off-switch game to model the uncontrollability of AI agents. turner2021optimal proves the power-seeking tendencies of optimal policies in Markov decision processes (MDPs) under certain assumptions. everitt2016avoiding proposes value reinforcement learning to eliminate incentives for reward hacking skalse2022defining, pan2021effects. Another avenue of research, designated as agent foundations~soares2017agent, aims to establish a rigorous formal framework for the agency that deals appropriately with unresolved issues of embedded agency. This body of work explores a variety of key topics, including corrigibility~soares2015corrigibility, value learning~soares2018value and logical uncertainty garrabrant2016logical. itemize",
      "origin_cites_number": 54
    },
    {
      "section_title": "RICE: The Objectives of Alignment",
      "level": "3",
      "content": "\\hfill tcolorbox[colback=gray!20, colframe=gray!50, sharp corners, center title] \\centering \\large How can we build AI systems that behave in line with human intentions and values? tcolorbox \\hfill There is not a universally accepted definition of alignment. Before embarking on this discussion, we must clarify what we mean by alignment objectives. leike2018scalable frame it as the agent alignment problem, posing the question: ``How can we create agents that behave in accordance with the user intentions?'' One could also focus on super-human AI systems superalignment and ask: ``How do we ensure AI systems much smarter than humans follow human intent?'' A consistent theme in these discussions is the focus on human intentions. To clearly define alignment goals, it's imperative to accurately characterize human intentions, a challenging task, as noted by kenton2021alignment. For instance, the term human can represent various entities ranging from an individual to humanity. gabriel2020artificial breaks down intentions into several categories, such as instruction (follow my direct orders), expressed intentions (act on my underlying wishes), revealed preferences (reflect my behavior-based preferences), and so on. Concretely, we characterize the objectives of alignment with four principles: Robustness, Interpretability, Controllability, and Ethicality (RICE). Figure fig:RICE summarizes the principles, and Table tab:category-over-RICE gives the correspondence between alignment research directions covered in the survey and the principles to which they contribute. The following is a detailed explanation of the four principles. itemize \\item Robustness Robustness refers to the resilience of AI systems when operating across diverse scenarios dietterich2017steps or under adversarial pressures Runder2021b, especially the correctness of its objective in addition to capabilities. Robust AI systems should be able to cope with black swan events nicholas2008black and long-tailed risks hendrycks2021unsolved, as well as a diverse array of adversarial pressures song2018constructing,chakraborty2021survey. For example, an aligned language model ought to refuse requests to behave harmfully, but models can be made to cause harm through jailbreak prompts and other adversarial attacks carlini2023aligned, zou2023universal, shah2023scalable. Instead, an adversarially robust model should behave as intended even when facing inputs designed to cause failure. As AI systems find increasing deployment in high-stakes domains such as the military and economy Jacob2020, there will be a growing need to ensure their resilience against unexpected disruptions and adversarial attacks, given that even momentary failures can yield catastrophic consequences kirilenko2017flash, oecd2021, Runder2021b. Aligned systems should consistently maintain robustness throughout their lifecycle russell2019human. \\item Interpretability Interpretability demands that we can understand the AI systems' inner reasoning, especially the inner workings of opaque neural networks rauker2023toward. Straightforward approaches to alignment assessments, such as behavioral evaluations, potentially suffer from dishonest behaviors turpin2024language,park2023ai,jacob2023 or deceptive alignment hubinger2019deceptive,carranza2023deceptive of AI systems. One way to cope with this issue is to make AI systems honest, non-concealing, and non-manipulative pacchiardi2024how,radhakrishnan2023question,shevlane2023model. Alternatively, we could build interpretability tools that peek into the inner concepts and mechanisms within neural networks elhage2021mathematical,meng2022locating. In addition to enabling safety assessments, interpretability also makes decision-making processes accessible and comprehensible to users and stakeholders, thus enabling human supervision. As AI systems assume a more pivotal role in real-world decision-making processes and high-stakes settings holzinger2017we, it becomes imperative to demystify the decision-making process rather than allowing it to remain an opaque black box Deepmind2018, Runder2021a. \\item Controllability Controllability is a necessary attribute that ensures the actions and decision-making processes of a system remain subject to human oversight and intervention. It guarantees that human intervention can promptly rectify any deviations or errors in the system's behavior soares2015corrigibility,hadfield2017off. As AI technology advances, an increasing body of research is expressing growing concerns about the controllability of these potent systems critch2020ai, unite2023, arcevalscollab. When an AI system begins to pursue goals that contradict its human designers, it can manifest capabilities that pose significant risks, including deception, manipulation, and power-seeking behaviors shevlane2023model, arcevalscollab. The objective of controllability is sharply focused on enabling scalable human oversight during the training process bowman2022measuring, as well as corrigibility of AI systems (i.e., not resisting shutdown or objective modification during deployment) soares2015corrigibility. \\item Ethicality Ethicality refers to a system's unwavering commitment to uphold human norms and values within its decision-making and actions. Here, the norms and values include both moral guidelines and other social norms/values. It ensures that the system avoids actions that violate ethical norms or social conventions, such as exhibiting bias against specific groups buolamwini2018gender,zhang2018mitigating,noble2018algorithms,kearns2019ethical,raji2020saving,berk2021fairness, causing harm to individuals hendrycks2020aligning,pan2023machiavelli, and lacking diversity or equality when aggregating preferences CIPwhitepaper. A significant body of research is dedicated to developing ethical frameworks for AI systems hagendorff2020ethics,pankowska2020framework. This emphasis on imbuing AI systems with ethical principles is necessary for their integration into society winfield2019machine. itemize Comparing the RICE Principles with Their Alternatives The RICE principles represent a succinct summary of alignment objectives from the perspective of alignment and coexistence of humans and machines. Several previous works have put forth guidelines concerning AI systems. Asimov's Laws can be regarded as the earliest exploration of human-machine coexistence, emphasizing that robots should benefit humans and the difficulty of achieving this Asimov1942. On another front, the FATE principle (Fairness, Accountability, Transparency, and Ethics) memarian2023fairness leans towards defining high-level qualities AI systems should possess within the human-machine coexistence ecosystem. We aspire to answer the human-machine coexistence question from the standpoint of human governors and designers, considering what steps are necessary to ensure the builder AI systems are aligned with human intentions and values. Furthermore, some standards emphasize narrowly defined safety, such as the 3H standard (Helpful, Honest, and Harmless) askell2021general and governmental agency proposals white2023. We aim to expand upon these standards by introducing other crucial dimensions, including Controllability and Robustness. C[1]{>{\\centering\\arraybackslash}m{#1}}",
      "origin_cites_number": 34
    },
    {
      "section_title": "Discussion on the Boundaries of Alignment",
      "level": "3",
      "content": "Following the introduction of alignment inner scope, in this section, we further discuss the relationship between AI safety and alignment. Actually, AI alignment constitutes a significant portion of AI safety concerns. In this section, we will delve into topics that fall right on the boundary of alignment, but well within the broader category of AI safety. Our discussion of broader AI safety concerns will draw from hendrycks2023overview. Human Values in Alignment The inclusion of Ethicality in our RICE principles signifies the critical role of human values in alignment. AI systems should be aligned not only with value-neutral human preferences (such as intentions for AI systems to carry out tasks) but also with moral and ethical considerations. These efforts are referred to as value alignment gabriel2020artificial,gabriel2021challenge.Although this term has also been used in other ways, such as to refer to alignment in general \\citep{yuan2022situ.} Considerations of human values are embedded in all parts of alignment -- indeed, alignment research topics dedicated to human values are present in all four sections of our survey. Therefore, to provide a more holistic picture of these research topics, here we give an overview of them before delving into their details in each individual section. We classify alignment research on human values into three main themes: (1) ethical and social values which aims to teach AI systems right from wrong, (2) cooperative AI which aims to specifically foster cooperative behaviors from AI systems, and (3) addressing social complexities which provides apparatus for the modeling of multi-agent and social dynamics. itemize \\item Ethical and Social Values. Human values inherently possess a strong degree of abstraction and uncertainty. macintyre2013after even points out that modern society lacks a unified value standard, and the value differences between people of different cultures can be vast. This raises the significant challenge of determining which human values we should align with. Although universally consistent human values may not exist, there are still some values that are reflected across different cultures. In the sections below, we discuss these from the perspectives of Machine Ethics, Fairness, and Cross-Cultural Values in Social Psychology. Machine Ethics: In contrast to much of alignment research which aligns AI systems with human preferences in general (encompassing both value-laden ones and value-neutral ones), machine ethics have specifically focused on instilling appropriate moral values into AI systems yu2018building, winfield2019machine, tolmeijer2020implementations. This line of work started early on in the context of symbolic and statistical AI systems anderson2005towards,arkoudas2005toward,anderson2007status, and later expanded to include large-scale datasets hendrycks2020aligning,pan2023machiavelli and deep learning-based/LLM-based methods jin2022make. We cover the formal branch of machine ethics in \\Ssubsec:formal-ethics-coop. Fairness: Although there are controversies ~verma2018fairness, saxena2019fairness, the definition of fairness is relatively clear compared to other human values. Specifically, it is the absence of any prejudice or favoritism toward an individual or group based on their inherent or acquired characteristics ~mehrabi2021survey. Therefore, there has been extensive research on AI fairness. These methods range from reducing data biases before training ~d2017conscientious, bellamy2018ai, to minimizing unfairness introduced during the training process ~berk2017convex, and finally addressing instances of unfairness that were not successfully learned during training ~xu2018fairgan. Cross-Cultural Values in Social Psychology: In the field of social psychology, numerous studies have focused on exploring clusters of values that exist among cross-cultural human communities, leading to the development of various cross-cultural values scales. The Allport-Vernon-Lindzey value system ~allport1955becoming posited that understanding an individual's philosophical values constitutes a critical foundation for assessing their belief system. They devised a value scale comprising six primary value types, each representing people's preferences and concerns regarding various aspects of life. messick1968motivational, mcclintock1982social, liebrand1984effect, van1997development introduced and improved a quantifiable method, namely social value orientation (SVO), to assess an individual's social value inclination. It utilizes quantitative approaches to evaluate how individuals allocate benefits to themselves and others, reflecting their social value orientation, such as altruism, individualism, etc. In subsequent work, murphy2011measuring, murphy2014social introduced the Slider Measure, which can be used to precisely assess the SVO value as a continuous angle based on the subject's option to some specific questions. rokeach1973nature developed a values inventory comprising 36 values, consisting of 18 terminal values representing desired end-states and 18 instrumental values signifying means to achieve those end-states. schwartz1992universals, schwartz1994there conducted comprehensive questionnaire surveys in 20 diverse countries known as the Schwartz Value Survey. This study identified ten values that are universally recognized, regardless of culture, language, or location. These studies have all laid a solid theoretical foundation for establishing what kind of values AI should be aligned with. However, they are constrained by the historical context of their research and may not maintain strong universality across different times and cultures. \\item Cooperative AI. Arguably, the most exciting aspect of multi-agent interaction is cooperation, and cooperation failure is the most worrying aspect of multi-agent interaction. As an example of AI cooperation failure, the 2010 Flash Crash led to a temporary loss of trillions of market value in 2 minutes and was caused in part by interactions between high-frequency algorithmic traders kirilenko2017flash. Therefore, there is a need to implement mechanisms ensuring cooperation in agent-like AI systems and the environments they're operating within dafoe2021cooperative. The high-level design principles and low-level implementations of such mechanisms fall into the domain of Cooperative AI. In addition, Cooperative AI also studies human cooperation through the lens of AI and how AI can help humans achieve cooperation. More precisely, dafoe2020open classified Cooperative AI research into four broad topics: Understanding, Communication, Commitment, and Institutions. They span various disciplines, from game theory to machine learning to social sciences. This survey has included discussions of cooperative AI, focusing on reinforcement learning in \\Ssec:cooperative-ai-training and game theory in \\Ssubsec:formal-ethics-coop. \\item Addressing Social Complexities. The requirement of ethicality contains in itself a social component. ``What is ethical'' is often defined within a social context; therefore, its implementation in AI systems also needs to account for social complexities. critch2020ai provides proposals for many research topics in this vein. One avenue of research focuses on the realistic simulation of social systems, including rule-based agent-based modeling bonabeau2002agent,de2014agent, deep learning-based simulation sert2020segregation, and those incorporating LLMs park2023generative. These simulation methods could serve a diverse array of down-stream applications, from impact assessment calvo2020advancing,fernandes2020adoption to multi-agent social learning critch2020ai. On another front, the fields of social choice sen1986social,arrow2012social and, relatedly, computational social choice brandt2016handbook have aimed to produce mathematical and computational solutions for preference aggregation in a diverse population, among other goals. It has been argued that a similar approach when combined with human preference-based alignment methods (e.g., RLHF and most other methods introduced in \\Ssec:learning-from-feedback), could supplement these methods to guarantee a fair representation of everyone's preferences leike2023proposal, CIPwhitepaper. There have been early-stage experiments on this proposal bakker2022fine,kopf2024openassistant. To complement this approach of learning values from crowds, it has also been argued that embodied values in AI systems should undergo continual progress over the long term as opposed to being permanently locked-in kenward2021machine, in order to navigate through emerging challenges, as well as to become future-proof and meet potential unknown unknowns in the moral realm. itemize Malicious Use Malicious actors can deliberately use AI to cause harm. Already, deepfakes have been used by criminals to enable scams and blackmail cao2023deepfake. As AI systems develop more dangerous capabilities, the threat of misuse looms larger. Biological weapons provide one concerning example of how AI could be maliciously used to cause harm. Research has shown that large language models can provide detailed, step-by-step instructions about synthesizing pandemic potential pathogens soice2023large. In addition to spreading information about how to create biological weapons, AI could help design new pathogens that are more lethal and transmissible than existing illnesses sandbrink2023artificial. Terrorist groups such as Aum Shinrikyo danzig2012aum have already attempted to build biological weapons in order to cause widespread destruction, and AI could make it easier for small groups to create biological weapons and start global pandemics. Other kinds of malicious use could include using AI to launch cyberattacks against critical infrastructure mirsky2023threat, or create autonomous agents that survive and spread outside of human control rogueai. As new dangerous capabilities arise in AI systems, thorough evaluations will be required to determine how an AI system could be used to cause harm. Malicious use might not be considered a failure of alignment because when an AI system behaves according to the intentions of a malicious user, this system would be aligned with its user but would still pose a serious threat to society. Policies to ensure that AI is aligned with the public interest will be essential to avert this threat. Collective Action Problems Many AI developers are racing to build and deploy powerful AI systems grant2023ai. This incentivizes developers to neglect safety and race ahead to deploy their AI systems. Even if one developer wants to be careful and cautious, they might fear that slowing down to evaluate their systems and invest in new safety features thoroughly might allow their competition to outpace them armstrong2016racing. This creates a social dilemma where individual AI developers and institutions rationally pursuing their own interests can lead to suboptimal outcomes for everyone. Success in competition between AI systems may be governed by evolutionary dynamics, where the strongest and most self-interested AI systems could be the most likely to survive hendrycks2023natural. Preventing these collective action problems from causing societal catastrophes could require intervention by national and international AI policies to ensure that all AI developers uphold common safety standards. In a broader context, Malicious Use can be considered effective alignment between AI systems and individuals with impure intentions, but without alignment with universally held human values. Concurrently, Collective Action Problems can be regarded as a consequence of competition, leading developers to neglect the crucial aspect of AI alignment in ensuring model safety. Broadly speaking, the connection between AI alignment and AI safety has progressively become more intertwined, resulting in a gradual blurring of boundaries.",
      "origin_cites_number": 41
    },
    {
      "section_title": "Learning from Feedback",
      "level": "1",
      "content": "Learning from feedback aims to transmit human intentions and values to AI systems. It serves as the foundation for forward alignment. In this section, we focus on the dynamic process of learning from feedback, categorizing it into three key elements: (1) AI System: refers to systems that require alignment, such as pre-trained LLMs; (2) Feedback: provided by an advisor set, which may consist of humans, AI, or humans assisted by AI, etc. This serves as the information used to adjust the AI system; (3) Proxy: a system developed to model feedback to facilitate more accessible learning. For example, human preference rankings of AI system behaviors serve as feedback, while a reward model acts as the corresponding proxy. From these elements, we identify two pathways by which the AI system learns from feedback: (1) Direct learning from the feedback itself and (2) Indirect learning via proxies that model the feedback. Following this process, we proceed to \\Sssec:feedback_types where we discuss different feedback types from the alignment perspective, highlighting various methods of providing information to AI systems. In the following sections, we introduce key concepts that have recently provided insights into developing powerful AI systems christiano2017deep and aligning them with human intent touvron2023llama. \\Sssec:preference_modeling focus on Preference Modeling emphasizing its role in creating proxies that help humans provide feedback to complex or hard-to-evaluate AI systems. Next, we explore Policy Learning in \\Sssec:policy_learning, focusing on key research directions for developing capable AI systems through feedback. The discussion then naturally transitions to scalable oversight in \\Sssec:scalable_oversight, where we reflect on the learning process and objectives from a broader alignment perspective.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Feedback Types",
      "level": "2",
      "content": "Feedback is a crucial link between AI behaviors to human intentions stumpf2007toward, stumpf2009interacting leveraged by AI systems to refine their objectives and more closely align with human values glaese2022improving, this includes two primary meanings: (1) During system construction, external sources provide feedback on the AI system's output, guiding refinements to the system's architecture or its internal information zhou2021machine. (2) After the system deployment, it will continuously adapt to changes in external environmental data, maintaining the architecture or fundamental strategy of the system unchanged, with methods such as adaptive control aastrom2008adaptive, aastrom2021feedback and in-context learning dong2022survey. For a precise and detailed discussion of the feedback types with precision and detail, it is essential to initially define feedback within the scope of alignment. quotebox \\large Feedback is information given to the AI system to align it with human intent. quotebox Considering diverse AI systems in alignment research, we embrace an human-centric approach. Instead of delving deep into the complex system mechanics, we propose a taxonomy to classify feedback according to its direct presentation forms to the system. This section introduces four types of feedback employed to align AI systems commonly: label, reward, demonstration, and comparison. It is worth noting that beyond explicit feedback, there are approaches that exploit the information embedded in vast amounts of unlabeled data through unsupervised pre-training parisi2022unsurprising and semi-supervised learning xu2018semantic, showing considerable promise in enhancing model capabilities zhou2024lima. Label Label feedback refers to one or more meaningful information tags attached to the original data item hastie2009overview, which stands as the most direct form, offering explicit guidance and delineating expected outputs for AI systems. This type of feedback prompts AI systems to learn from input-output pairings provided by expert advisors. For example, in supervised learning, an AI model is trained using a dataset of labeled input-output pairs, denoted by $D = \\left\\{ (x_i, y_i) \\right\\}_{i = 1}^N$. Here, $y_i$ represents the true labels corresponding to the input data $x_i$, and $N$ signifies the total number of samples in the dataset. The essence of the learning process revolves around minimizing a loss function $L$ (e.g., MSE), which measures the disparity between the predictions of the model, $f(x; \\theta)$, and the ground truth labels $y$, based on the model parameters, $\\theta$. The advantage of label feedback is its unambiguous nature and simplicity in interpretation. However, due to the inability of label feedback to fully encapsulate the underlying logic of this choice, employing such feedback in model training can result in target variable bias guerdan2023ground. And, its utility might diminish when tackling complex tasks beyond mere classification or regression lake2017building, marcus2018deep. For example, in tasks like optimizing algorithms fawzi2022discovering, mankowitz2023faster, video game playing baker2022video, and multi-modal generation openai2023gpt4V, it is not only impractical to provide explicit instructions for every conceivable situation but also insufficient to solely rely on label feedback to build systems that surpass human capabilities. Reward A reward is an absolute evaluation of a single output from an AI system, represented as a scalar score silver2021reward or a vector of scores wu2024fine, each independent of other outputs. Feedback based on rewards provides a quantified evaluation of the AI system, allowing for direct guidance in behavior adjustments. This type of feedback typically originates from pre-designed, rule-based functions or procedures. For example, in the MuJoCo simulation, environments from OpenAI Gym brockman2016openai, the task is to guide the agent moving forward effectively. To this end, an effective rule-based reward function can be formulated as a composite of several key components: maintaining a healthy status, encouraging forward movement, minimizing control exertion, and regulating contact intensity. The advantage of reward feedback is that the designer does not need to delineate the optimal behavior while allowing the AI system to explore to find the optimal policy kaelbling1996reinforcement, mnih2015human, silver2016mastering, silver2017mastering. However, crafting flawless rules to determine scores for functions that evaluate the output of AI systems everitt2017reinforcement, specification2020victoria, pan2021effects or directly assigning calibrated and consistent scores to each AI system output isbell2001social, thomaz2008teachable, christiano2017deep, casper2023open is challenging for human. This is due to the inherent complexity of the tasks, where it's impractical to account for every nuance. Additionally, flawed or incomplete reward functions can lead to dangerous behaviors misaligned with the intention of the designer, such as negative side effects and reward hacking hadfield2017inverse, skalse2022defining. Thus, merely from the alignment perspective, perhaps the most important limitation of feedback based on rewards is that it may be difficult to rule out manipulation shevlane2023model, which amounts to reward tampering and reward gaming leike2018scalable, everitt2021reward, skalse2022defining in this context. CIRL in \\Ssec:cirl, provides insights into this particular issue. Demonstration Demonstration feedback is the behavioral data recorded from expert advisors while achieving a specific objective hussein2017imitation. Demonstrations can take on various forms, including videos shaw2023videodex, wearable device demonstrations edmonds2017feeling, wang2023learning, collaborative demonstrations bozorgi2023beyond, and teleoperation zhang2018deep. If the dynamics of the demonstrator and the AI learner are identical, the demonstration can directly constitute a trajectory made up of state-action pairs zhang2023discriminator. These state-action pairs can also be partially observable torabi2018behavioral, brown2019extrapolating. For example, a video can be recorded of a human expert performing a robotic manipulation task, such as grasping an object with a robotic hand. One can subsequently annotate each video frame with the associated robot state shaw2023videodex and action baker2022video for each frame. This results in a dataset of state-action pairs from the human demonstration that can be used to train the agent's policy to imitate the expert behavior. This feedback leverages the expertise and experience of advisors directly, obviating the need for formalized knowledge representations fang2019survey, dasari2023learning. However, it may falter when confronting tasks that exceed the advisors' realm of expertise hussein2017imitation. Additionally, it faces challenges stemming from the noise sasaki2020behavioral and suboptimality attia2018global in real-world advisor demonstrations yang2021trail. Furthermore, human advisors, prone to imprecision and errors, can introduce inconsistencies zhu2019ingredients, iii2022fewshot. Meanwhile, there might be a need for a vast amount sasaki2020behavioral and diverse set beliaev2022imitation of demonstrations within acceptable costs, which results in significant difficulty in learning reliable behaviors. Comparison Comparison feedback is a relative evaluation that ranks a set of outputs from an AI system and guides the system toward more informed decisions wirth2017survey. For example, this feedback form is manifested in Preference Learning furnkranz2010preference, where the AI system discerns the preferences of advisors by comparing multiple examples. The fundamental advantage of comparison feedback is humans' capacity to quickly handle tasks and objectives that are hard for precise evaluation hullermeier2008label, christiano2017deep, ouyang2022training. Nevertheless, beyond common factors like noise in the feedback and unmodeled contextual elements that hinder the model's convergence to true objectives, the absolute differences between different items become obscured. Consequently, the performance of a strategy tends to optimize towards a median target rather than an average target. casper2023open illustrates this with an example of action $A$, always yielding a value of 1, and action $B$, which yields 10 in 40\\% of cases and 0 in 60\\%. When assessed based on comparison feedback, action $A$ is deemed superior to $B$, even though $B$ possesses a higher expected return. It also has the inherent limitation of potentially requiring a substantial amount of comparative data furnkranz2003pairwise, gao2023scaling, although some studies indicate that the necessary quantity may be relatively smaller christiano2017deep. Preference modeling is an example of using this type of feedback, as detailed in \\Spar:reward_model. Discussion All types of feedback can be provided to AI systems interactively and online. This process engenders synchronous iterations between providing feedback and AI system updates, underscoring rapid, focused, and incremental model modifications amershi2014power, holzinger2016interactive. For instance, demonstration feedback can manifest in the form of online corrections bajcsy2018learning, li2021learning, losey2022physical. Interactively providing feedback emphasizes the role of interactivity in the learning process, allowing AI systems to evolve based on interactive experiences. In active learning, robots actively engage in data discovery and acquisition, thereby facilitating learning throughout the process of online deployment taylor2021active. And in interactive learning, feedback manifests in the form of guided corrections that online rectify missteps in the behavior of the AI system fails2003interactive, amershi2014power, saunders2022self. For example, the interactive image segmentation emphasizes simple zhang2020interactive, intuitive rother2004grabcut, xu2016deep, and real-time liu2022pseudoclick interactions. One of the essential advantages of interactively providing feedback is its ability to fine-tune AI systems in real-time, allowing users to interactively explore the model's space amershi2014power to ensure quick and subtle alignment with the directives of advisors shin2020autoprompt, wei2022chain, zou2023segment. Moreover, this process lessens the dependence on specialist knowledge and promotes better interpretability berg2019ilastik. However, it may be limited by the interactivity to choose time-intensive algorithms fails2003interactive, holzinger2016interactive. Furthermore, considering more powerful AI systems are emerging, more universal interaction interfaces are also coming up, such as language lynch2023interactive, openai2023gpt4 and vision rt2_deepmind_2023, which bridge the communication gap between humans and AI systems. In robotics, a series of studies have linked human-provided language with rewards obtained by agents. This association enables the conveyance of nuanced human intentions through language, thereby guiding the generation of scalar feedback signals during the training fu2018from, goyal2019using, sumers2021learning, zhou2021inverse, lin2022inferring, yu2023language and planning sharma2022correcting process. In the realm of LLMs, in-context learning dong2022survey serves as a means to supplement information via language during deployment, thereby enhancing the alignment of LLMs with human intent. These various modes of feedback share a common trait -- that they can all be seen as attempts by humans to convey a hidden reward function. jeon2020reward proposes and formalizes this position and unifies a wide array of feedback types by defining a parameterized reward function $\\Psi\\left(\\cdot;\\theta\\right)$ that underlies the feedback process. This allows the AI system to, for example, perform Bayesian inference on $\\theta$, regardless of the feedback type. Recently, techniques based on IL and RL have successfully constructed AI systems with significant capabilities baker2022video, openai2023gpt4V. However, this success naturally leads to two questions: itemize[left=0.3cm] \\item How can we define reward functions for more complex behaviors (e.g., various sub-tasks in interactive dialogue), aiming to guide the learning process of AI systems? \\item How can we express human values such that powerful AI systems align better with humans, ensuring the system's controllability and ethicality? itemize Endeavors incorporating preference modeling into policy learning have shown progress. The most notable achievements in this domain have been observed in constructing powerful LLMs openai2023gpt4, touvron2023llama, anthropiceval. Additionally, a series of policy learning studies have reported performance improvements. For instance, combining preference modeling with Inverse Reinforcement Learning (IRL) brown2019extrapolating, brown2020safe and offline RL shin2023benchmarks, fine-tuning reward functions iii2022fewshot, modeling non-Markovian rewards kim2023preference, and aiding in the construction of intricate reward functions bukharin2023deep. Therefore, we consider preference modeling (as shown in \\Sssec:preference_modeling) and policy learning (as shown in \\Sssec:policy_learning) as fundamental contexts for understanding the challenges faced in alignment and potential solutions. Next, we provide a brief overview of these specific techniques related to alignment.",
      "origin_cites_number": 70
    },
    {
      "section_title": "Preference Modeling",
      "level": "2",
      "content": "In many complex tasks, such as dialogues ouyang2022training, constructing precise rule-based rewards presents a challenge bender2021dangers. At the same time, methods based on demonstration might require a substantial investment of expert human resources, resulting in high costs. Currently, preference modeling based on comparison feedback akrour2011preference has emerged as a very promising method ouyang2022training, openai2023gpt4, touvron2023llama to assist in fine-tuning powerful AI systems amodei2016concrete. Typically, it is necessary to iteratively explore the system dynamics while acquiring expert preference data to gain more knowledge about the optimization objectives. This process is known as Preference Elicitation wirth2013preference, wirth2017survey, christiano2017deep, cabi2019scaling, which is crucial for obtaining rich, valuable feedback related to AI system outputs, thus guiding the alignment process iii2022fewshot. Within Preference Elicitation, two core decisions that need to be determined are the Granularity of Preference and the Category of Preference. This paper introduces these within sequential decision-making problems, but the insights derived apply to a broad array of AI systems amodei2016concrete, christiano2018supervising, leike2018scalable. Granularity of Preference Preference wirth2017survey can primarily be categorized into three types by granularity: Action, State, and Trajectory (as shown in Table tab:comparison_preference_granularities). The Action preference focuses on comparing actions within a particular state, specifying the preferred action under specific conditions. When translated into trajectory preferences, it may impose challenges such as evaluators' expertise needs and potential information loss. The State preference deals with comparing states. It encapsulates preference relations among states but requires assumptions about state reachability and independence when translating to trajectory preferences. The Trajectory preference considers whole state-action sequences, offering more comprehensive strategic information. It inherently assesses long-term utility and depends less on expert judgment. christiano2017deep demonstrates, using ablation studies, that in the settings that they studied, longer trajectory segments yield more informative comparisons on a per-segment basis. Such segments are also more consistently evaluated by humans in MuJoCo tasks. Category of Preference Diverse objectives exist within preference modeling. Based on their targets, preferences can be categorized into object preference and label preference furnkranz2010preference. Specifically, object preference operates on a set of labels for each instance, whereas label preference acts on a set of objects themselves. One can further classify them differently based on the form of preferences. itemize[left=0.3cm] \\item {Absolute Preferences}. Absolute preferences independently articulate each item's degree of preference. itemize[left=0.2cm] \\item {Binary}. Classifying items as liked or disliked offers a simplistic and straightforward model of user preference tsoumakas2007multi, cheng2010graded. \\item {Gradual}. This can be further distinguished between numeric and ordinal preferences. Numeric preferences employ absolute numerical values, such that each item receives a numerical score, which reflects the extent of preference cheng2010label. On the other hand, ordinal preferences entail a graded assessment of a fixed set of items as either preferred, less preferred, or intermediary, etc., enabling the depiction of user preferences without including specific numerical measurements cheng2010graded. itemize \\item {Relative Preferences}. Relative preferences define the preference relation between items. itemize[left=0.2cm] \\item {Total Order}. This form establishes a comprehensive preference relation covering all item pairs, asserting an absolute ordering of preferences ranging from the most preferred to the least hullermeier2008label. \\item {Partial Order}. Because users may not exhibit a distinct preference between two items in some instances cheng2010predicting, this allows for incomparable item pairs. itemize itemize Reward Model Reward modeling transfers comparison feedback furnkranz2010preference, wirth2017survey to the scalar reward form, facilitating policy learning christiano2017deep, cabi2019scaling, touvron2023llama. Given pairs of actions $(y_1, y_2)$ performed by the RL agent in the same state. The preference is denoted as $y_w\\succ y_l \\mid x$, where $y_w$, $y_l$ represents the preferred and less preferred action respectively among $(y_1, y_2)$. We assume these preferences emerge from a latent reward model $r^*(x, y)$, which we lack direct access to. Several methods exist to model such preferences, e.g., the Bradly-Terry Model bradley1952rank, Palckett-Luce ranking model plackett1975analysis, etc. Under the BT model, the distribution of human preference, denoted as $p^*$, can be formalized as, align* p^*\\left(y_1\\succ y_2 \\mid x\\right)=\\exp\\big(r^*(x, y_1)\\big){\\exp\\big(r^*(x, y_1)\\big) + \\exp\\big(r^*(x, y_2)\\big)}=\\sigma \\big(r^*(x, y_1) - r^*(x, y_2)\\big). align* where $\\sigma(x) = 1/\\left(1 + \\exp(-x)\\right)$ is the logistic sigmoid function. Subsequently, we use the derived preference rankings to train the parameterized reward model, optimizing its parameters through maximum likelihood. align* L_{R}\\left(\\theta\\right)=-E_{(x,y_w,y_l)\\sim \\mathcal D}\\Big[\\log\\Big(\\sigma\\big(r_{\\theta}\\left(x,y_w\\right)-r_{\\theta}\\left(x,y_l\\right)\\big)\\Big)\\Big] align* In this negative log-likelihood loss, the problem is a binary classification task, where $\\mathcal D$ signifies the static dataset $\\left \\{ x^{(i)}, y_{w}^{(i)}, y_{l}^{(i)}\\right \\}_{i=1}^{N} $ sampled from $p^*$ (i.e., human-labeled comparisons). Reward models enable human users to impart specific preferences to these systems via evaluations, thereby circumventing the complex task of defining objectives explicitly. Initially, the studies by knox2012learning, knox2013learning distinctively treat human reward as separate from the traditional rewards of MDP and conduct a reward modeling process around it. Transitioning from these simpler cases, christiano2017deep propose that utilizing supervised learning to construct a distinct reward model asynchronously can substantially diminish interaction complexity by approximately three orders of magnitude. The study conducted by ibarz2018reward integrates expert demonstrations with human preferences, such that the policy initially mimics expert demonstrations and then sequentially collects human trajectory annotations, trains the reward model, and updates the policy. This research also provides practical insights for precluding the overfitting of the reward model and the occurrence of reward hacking -- a scenario where escalating rewards do not translate to improved performance, especially when the policy is excessively trained. Additionally, a random policy might rarely exhibit meaningful behavior for tasks that surpass the complexity of Atari palan2019learning, jeon2020reward. This implies that for effective annotation, the policy itself must possess certain capabilities to perform improved behavior. Offline settings also benefited from the reward model. cabi2019scaling proposes reward sketching to efficiently learn a reward model that leverages humans' episodic judgments for automated reward annotation of historical data, enabling large-scale batch RL. qiu2024rethinking provides an empirically-grounded theory of reward generalization in RMs, based on which a new type of RM based on tree-structured preferences is proposed and experimentally validated. Importantly, the reward model provides an essential tool for aligning powerful LLMs. stiennon2020learning employs reward models grounded in human preferences for text summarization tasks, resulting in significant policy enhancements. This work also delves into the issues of distribution shift and reward model generalization, revealing that the effectiveness of the reward model correlates with data scale and parameter size. Building upon this work, InstructGPT ouyang2022training extends the reward model paradigm to broader dialogue task reward modeling and introduces a preference-optimizing loss function for multiple responses to mitigate overfitting. Furthermore, this research reveals that the preferences derived from the reward model can be generalized across different groups.",
      "origin_cites_number": 28
    },
    {
      "section_title": "Policy Learning",
      "level": "2",
      "content": "Policy learning aims to learn the mapping from perceived states to actions taken when in those states sutton2018reinforcement to optimize a model's performance in specific tasks. Numerous alignment-related challenges manifest within policy learning (as shown in \\Ssec:challenges-of-alignment). Consequently, policy learning provides a crucial backdrop for alignment, and its techniques can further advance alignment objectives amodei2016concrete, christiano2018supervising, ibarz2018reward. This section discusses various domains within policy learning and then introduces RLHF, a powerful technique for policy learning openai2023gpt4, touvron2023llama.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Background",
      "level": "3",
      "content": "We introduce some general areas of policy learning here to give readers a general background. Reinforcement Learning (RL) RL enables agents to learn optimal policies by trial and error via interacting with the environment sutton2018reinforcement. This paradigm has achieved great success in tackling complex tasks li2017deep, yu2021reinforcement, fawzi2022discovering, baker2022video, afsar2022reinforcement, mankowitz2023faster, openai2023gpt4V, demonstrating its potential for decision-making and control in complex state spaces. The goal of RL is to learn a policy $\\pi$ which executes actions $a$ in states $s$ to maximize the expected cumulative reward under environment transition dynamics $P$ and the initial state distribution $\\rho_0$: gather* \\pi^* = \\pi{\\operatorname{argmax}} \\left\\{E_{s_0,a_0,\\dots}\\left[\\sum_{t=0}^\\infty\\gamma^tr(s_t)\\right] \\right\\}, ~~where \\ s_0\\sim\\rho_0(\\cdot),\\ a_t\\sim\\pi\\left(\\cdot|s_t\\right),\\ s_{t+1} \\sim P\\left(\\cdot|s_t,a_t\\right). gather* Even though RL still faces challenges like sample efficiency and stability bucsoniu2018reinforcement. Proximal policy optimization (PPO) schulman2017proximal is an influential algorithm in the RL community, serving as the key algorithm for RLHF ouyang2022training. The key idea of PPO is to limit the policy update to prevent significant deviations from the original policy by introducing a proximity objective. sikchi2023dual unifies several RL and Imitation Learning (IL) algorithms under the framework of dual RL through the lens of Lagrangian duality. Preference-based Reinforcement Learning (PbRL) PbRL wirth2017survey seeks to facilitate training RL agents using preference feedback instead of explicit reward signals christiano2017deep,sadigh2017active.Notably, \\citet{sadigh2017active explicitly maintains a probabilistic belief over the true reward function during learning, and actively constructs queries to the human to reduce uncertainty maximally. Both traits are in a similar spirit to cooperative inverse reinforcement learning (CIRL), and later work also continues this theme reddy2020learning. See \\Ssec:cirl for more.} PbRL integrates the advantages of preference learning and RL, broadening the application range of RL and mitigating the difficulties associated with reward function formulation, and has been efficaciously deployed in a variety of tasks such as robotic instruction kupcsik2013data, path planning jain2013learning, and manipulation shevlane2023model. In PbRL, the emphasis predominantly lies on trajectory preferences (i.e., comparisons of state-action sequences segment) wirth2017survey. Such trajectory preferences encapsulate a human evaluation of various behavioral outcomes rather than single states, rendering PbRL more suitable for non-expert users christiano2017deep, shin2023benchmarks, kim2023preference. A general example of PbRL is the weighted pairwise disagreement loss duchi2010consistency balancing multiple potentially conflicting preferences to identify a singular optimal policy: align* L(\\pi,\\zeta) = \\sum_{i=1}^N\\alpha_iL(\\pi,\\zeta_i), align* where $L(\\pi,\\zeta)$ is the aggregated loss for policy $\\pi$ over all preferences $\\zeta$, $\\alpha_i$ is the weight of the $i$th preference, and $L(\\pi,\\zeta_i)$ is the loss associated with the policy $\\pi$ in relation to the specific preference $\\zeta_i$. Compared to exact numerical rewards, preference feedback has several benefits wirth2017survey, such as (1) circumventing arbitrary reward design, reward shaping, reward engineering, or predefined objective trade-offs, (2) diminishing reliance on expert knowledge, and (3) decoupling training loop with human by modeling preferences akrour2012april. However, PbRL also faces challenges, including credit assignment problems due to temporal delays, practical exploration of preference space wirth2017survey, the potential need for massive data ouyang2022training, and the inability to use the learned preference model for retraining mckinney2022fragility. Imitation Learning (IL) IL schaal1999imitation,syed2008apprenticeship, also referred to as learning from demonstration or apprenticeship learning, focuses on emulating human behaviors within specific tasks. The agent learns a mapping between observations and actions and refines its policy by observing demonstrations in a collection of teacher demonstration data $\\mathcal D$ bakker1996robot,hussein2017imitation. This process obviates the need for environmental reward signals hussein2017imitation. Broad IL cotra2018iterated aims to replicate human desires and intentions, effectively creating replicas of human decision-making processes. This concept is central to technologies such as Iterated Distillation and Amplification (IDA, as shown in \\Sssec:iterated_distillation_and_amplification) christiano2018supervising. On the other hand, Narrow IL aims to replicate specific human behaviors within given tasks. Behavioral cloning (BC) bain1995framework,ross2011reduction,osa2018algorithmic is a simple pomerleau1991efficient, ravichandar2020recent strategy that learns directly from demonstrations using supervised learning schaal1996learning. BC method specifically seeks to optimize the policy parameters, $\\phi$, with the objective of aligning the policy $\\pi_{\\phi}(a|s)$ closely with the expert policy $\\pi_E(a|s)$. This alignment is achieved through the minimization of the negative log-likelihood, as delineated in the following lynch2020learning: align* L_{BC}(\\phi) = -E_{(s,a)\\sim \\pi_E}\\big[\\log\\pi_{\\phi}\\left(a|s\\right)\\big]. align* Here, the expectation is computed over state-action pairs sampled from the expert policy, $\\pi_E$. However, it faces the Out-of-Distribution (OOD) problem, arising from the difference between the training and testing distributions ross2011reduction, ho2016generative, reddy2019sqil, zhou2022domain. Adversarial imitation learning methods ho2016generative, fu2018learning, lee2019efficient, ghasemipour2020divergence have demonstrated an ability to enhance the robustness of policies against distribution shifts. However, these methods learn non-stationary rewards, which cannot be used to train new policies ni2021f. Inverse Reinforcement Learning (IRL) Unlike the paradigm of IL, IRL adams2022a focuses on deriving a reward function from observed behavior ng2000algorithms, arora2021survey. Standard IRL methods include the feature matching methods abbeel2004apprenticeship, which assumes optimal expert behavior or decision processes, as well as the maximum entropy methods ziebart2008maximum and the Bayesian methods ramachandran2007bayesian, both of which do not require optimal behavior. IRL guarantees robustness to changes in the state distribution but at the cost of increased computational complexity due to the extra RL step ho2016generative, fu2018variational. This interaction, meanwhile, introduces inherent RL challenges, e.g., sample efficiency yu2018towards and potential dangers in environment interaction garcia2015comprehensive. Additionally, identifying the reward function remains a challenge kim2021reward.",
      "origin_cites_number": 42
    },
    {
      "section_title": "Reinforcement Learning from Human Feedback (RLHF)",
      "level": "3",
      "content": "RLHF expands upon PbRL within the domain of DRL christiano2017deep, aiming to more closely align complex AI systems with human preferences openai2023gpt4V. Its principal advantage is that it capitalizes on humans being better at judging appropriate behavior than giving demonstrations or manually setting rewards. This approach has gained significant traction, particularly in fine-tuning LLMs ouyang2022training, openai2023gpt4, touvron2023llama. Nonetheless, RLHF encounters obstacles casper2023open, including data quality concerns, the risk of reward misgeneralization, reward hacking, and complications in policy optimization. Specifically, RLHF can also be viewed as a Recursive Reward Modeling (RRM) process (as shown in \\Spar:rrm) without deep recursive modeling leike2018scalable. Here, we provide a brief review of the RLHF methodology. The genesis of RLHF can be traced back to knox2008tamer,knox2012reinforcement, subsequently broadening its reach to domains such as social robots knox2013training and human-AI cooperative learning griffith2013policy. Besides focusing on the association between feedback and policy, loftin2016learning models the connection between feedback and the trainer strategy. christiano2017deep extended RLHF to simulated robotic tasks, demonstrating its potential effectiveness. It's worth noting that one of the significant applications of RLHF has been in the field of LLMs. Some work found that LLMs trained with RLHF ouyang2022training, korbak2023pretraining, thoughts_on_the_impact_of_rlhf_search are more creative and human alignment compared to models trained via supervised or self-supervised learning approaches kenton2019bert, brown2020language. The importance of RLHF is not merely limited to allowing LLMs to follow human directives ouyang2022training. It helps LLMs better align by giving them important qualities like being helpful, harmless, and honest bai2022training. Due to these improvements, many works use RLHF for aligning LLMs ziegler2019fine, stiennon2020learning, bai2022training, glaese2022improving, openai2023gpt4, touvron2023llama. Additionally, dai2024safe integrates the Safe RL garcia2015comprehensive framework with the RLHF, addressing the inherent tension between aligning helpfulness and harmfulness bai2022training. Future efforts can be focused on reducing dependence on human annotation wang2023self, sun2024principle and improving the efficacy of the reward model by leveraging iterative RLHF methods (i.e., integrating it with debate frameworks irving2018ai), etc. qiu2024rethinking has also built a formal framework of the RLHF process portraying it as an autoencoding process over text distributions, and enables analysis of convergence properties in RLHF. We review the RLHF pipeline from the ziegler2019fine, ouyang2022training, rafailov2024direct to give a general framework. It usually consists of three stages: itemize[left=0.3cm] \\item Supervised Fine-tuning (SFT). RLHF usually starts with a pre-trained language model, then fine-tuned using supervised learning -- specifically, maximum likelihood estimation -- on a high-quality human instruction dataset tailored for downstream tasks to obtain a model $\\pi^{SFT}$. Examples of these tasks include dialogue handling, instruction following, and summarization (Some open-source datasets include Alpaca Data (52k instruction-following data) taori2023stanford, Vicuna (70K user-shared ChatGPT conversations) chiang2023vicuna, etc.). This stage can also be carried out at any other stage. \\item Collecting Comparison Data and Reward Modeling. This phase includes collecting comparison data, which is subsequently used to train a reward model. The SFT model is given prompts denoted as $x$ to generate pairs of responses $(y_1, y_2)$ sampled from $\\pi^{SFT}(y \\mid x)$. These pairs are subsequently shown to human annotators, who indicate a preference for one of the responses. Then as discussed in \\Spar:reward_model, comparison data is used to construct the reward model $r_{\\theta}$. \\item Policy Optimization via Reinforcement Learning. The final step is optimizing LLM as a policy $\\pi$ through RL, guided by the reward model $r_{\\theta}$. The process of LLMs generating responses from prompts is modeled as a bandit environment~ouyang2022training, where a reward is obtained from reward model $r_{\\theta}$ at the end of each response. The primary objective of RL is to adjust the parameters ${\\phi}$ of the LLMs such that the expected reward on training prompt dataset $D_RL$ is maximized: equation* \\arg\\max\\limits_{\\pi_{\\phi}} ~ E_{x\\simD_RL, y\\sim\\pi_{\\phi}}\\big[r_{\\theta}\\left(x,y\\right)\\big]. equation* Typically, an additional per-token KL penalty derived from the SFT model $\\pi^SFT$ is involved to mitigate the reward over-optimization. In addition, the integration of gradients from pre-training distribution $D_pretrain$ helps maintain model performance, denoted as PTX loss in ouyang2022training. As a result, a more comprehensive practical objective function is introduced: equation* J({\\phi}) = E_{x\\simD_RL, y\\sim\\pi_{\\phi}}\\Big[r_{\\theta}(x,y) - \\beta\\log\\big(\\pi_{\\phi}(y|x)/\\pi^SFT(y|x)\\big)\\Big] + \\eta ~ E_{(x,y)\\simD_pretrain}\\Big[\\log\\big(\\pi_{\\phi}(y|x)\\big)\\Big], equation* where $\\beta$ and $\\eta$ are coefficients determining the intensity of the KL penalty and the mixture of pretraining gradients respectively. This process refines the LLMs to generate responses that better align with human preferences for the prompts used during training. itemize Though RLHF has been proven effective for aligning LLMs with human preferences, this method has problems like complex implementation, hyper-parameter tuning, sample efficiency choshen2019weaknesses, and computational overhead yuan2024rrhf, making it hard to scale up. A straightforward approach is rejection sampling dong2023raft, touvron2023llama paired with finetuning on the best examples. For every prompt, $K$ responses are sampled from the model. Each response is then assessed with the reward model, and the one with the highest reward is selected as the best response. This selected response is later used for model fine-tuning. zhang2023wisdom formulates the language model instruction alignment problem as a goal-reaching reinforcement learning problem and proposes the HIR algorithm. The method unfolds in two stages: online sampling and offline training. During online sampling, the algorithm samples the LLM at a high temperature. In the offline training stage, instructions are relabeled based on generated outputs, followed by supervised learning using this relabeled data. HIR capitalizes on successful and failed cases without requiring additional parameters. RRHF, as introduced by yuan2024rrhf, aligns model probabilities with human preferences by scoring and ranking responses from multiple sources. With the necessity for only 1 or 2 models, its implementation is straightforward. RRHF reported it can effectively align language models with human preferences, producing performance on par with PPO. gulcehre2023reinforced proposes the ReST algorithm, which contains two loops: Grow and Improve. The Grow loop uses the current model to sample and generate a dataset, while the Improve loop iteratively trains the model on a fixed dataset. This algorithm provides a simple and efficient framework that allows repeated use of the fixed dataset to improve computational efficiency, showing significant improvement in the reward model scores and translation quality compared to supervised learning baselines. Motivated by the dependence of reward modeling on policy optimization in RLHF, chakraborty2024parl propose PARL, a bilevel optimization-based framework. rafailov2024direct introduces the DPO, which demonstrates a mapping between reward functions and optimal policies. DPO is both simple and efficient, optimizing language models directly from human preference data, eliminating the need for an explicit reward model and multi-stage training. Moreover, wang2024beyond discusses how diverse divergence constraints influence DPO and introduces a generalized approach, namely, f-DPO. azar2023general presents a general objective, $\\Psi$PO, designed for learning from pairwise human preferences, circumventing current methods' assumption: pairwise preferences can be substituted with pointwise rewards. This objective analyzes RLHF and DPO behaviors, revealing their potential overfitting issue. The authors further delve into a specific instance of $\\Psi$PO by setting $\\Psi$ as the Identity, aiming to mitigate the overfitting problems. They call this method IPO and furnish empirical results contrasting IPO with DPO. hejna2024contrastive introduces CPL, which utilizes a regret-based model of preferences that directly provides information about the optimal policy. Further research could explore why RLHF performs effectively with LLMs and the application of RLHF in multimodal rt2_deepmind_2023, openai2023gpt4V settings to facilitate the benefits of human-AI collaboration human_ai_safe1, wu2021recursively, bi2021safety. See also casper2023open who offer a survey of open problems with RLHF. Open Discussion RLHF is frequently applied to the Safety Alignment of LLMs, yet many pressing issues remain unresolved. For example, how can we balance harmlessness and helpfulness in alignment? dai2024safe attempt to integrate the SafeRL framework, specifically the cost model and reward model, into RLHF to address the inherent tension between these two indicators. Moreover, even without malicious intent, simply fine-tuning on benign and commonly used datasets can inadvertently reduce the safety alignment of LLMs, albeit to a lesser extent qi2024finetuning and fine-tuning on benign data is more likely to degrade the models safety he2024s. These findings suggest that fine-tuning aligned LLMs may introduce new safety risks, even with datasets that are considered absolutely safe. Generally, language models may exhibit elasticity, making them resistant to alignment efforts ji2024language. This raises a question: how can we maintain impeccable safety alignment of models, even after further fine-tuning? Human preferences can vary among individuals, groups, and societies, leading to divergent perspectives. This divergence is also evident when collecting preference data from annotators. To address this, findeis2024inverse proposed a method to extract the underlying constitution governing the generation of a given dataset of preferences. Similar to Constitutional AI bai2022constitutional, where a preference dataset is generated by an LLM based on a predefined constitution, Inverse Constitutional AI aims to extract such a constitution that can be used to reconstruct the preference dataset. This problem can be formulated as an optimization problem: equation* \\operatorname{argmax}_{c} \\{agreement(po, p(c)) s.t. |c| \\leq n\\}, equation* where $po$ represents the original preferences, and $p(c)$ are the constitutional preferences over a pairwise text corpus $T$, generated by an LLM $M$ using the constitution $c$. The set is constrained to a maximum of $n$ natural language principles that are human-readable. Agreement is defined as the percentage of constitutional preferences $p(c)$ that match the original preferences $po$. Overall, the elicitation of a constitution can be seen as a compression task, where a constitution is generated based on a dataset and then used to reconstruct the preferences in the dataset as accurately as possible. To elicit such a constitution, the authors propose an algorithm that generates principles capable of explaining the preference data, followed by semantic clustering of these principles. To reduce the size of the set, they then subsample the principles and evaluate their ability by testing their reproducibility in reconstructing the preference data. Finally, the principles are filtered based on their relevance to the preference data. This method can be used to infer the constitution underlying a specific preference dataset and has the potential to identify underlying biases or reuse the constitution to generate new data, thus enlarging existing datasets or creating new datasets tailored to individual preferences.",
      "origin_cites_number": 46
    },
    {
      "section_title": "Scalable Oversight: Path towards Superalignment",
      "level": "2",
      "content": "Statistical learning usually rely on certain assumptions about data distribution, such as independence and identical distribution. Consequently, these algorithms fail in some situations, especially under specific distributions zhou2022domain. Challenges in elementary systems can be promptly identified through visual inspection christiano2018supervising, ngo2024the. As AI systems become more powerful, insufficiently capturing the training signal or erroneous design of loss functions often leads to catastrophic behaviors russell2015research, hubinger2019risks, cotra2021the such as deceiving humans by obfuscating discrepancies russell2019human, specification gaming specification2020victoria, reward hacking brown2020safe, and power-seeking dynamics carlsmith2022power. From a human perspective, these imply gaps between the optimized objectives of AI systems and the ideal goals in our minds. Thus, the issue of providing effective oversight in various decision-making becomes pivotal bowman2022measuring, li2023trustworthy, often termed as scalable oversight amodei2016concrete arising from two practical challenges. itemize[left=0.3cm] \\item The high cost of humans frequently evaluating AI system behavior. For instance, the training process is time-consuming, and incorporating humans directly into the training loop in real-time would significantly waste human resources and impede training efficiency christiano2017deep. \\item The inherent complexity of AI system behaviors makes evaluation difficult, especially on hard-to-comprehend and high-stakes tasks saunders2022self, e.g., tasks such as teaching an AI system to summarize books wu2021recursively, generate complex pieces of code pearce2022asleep, and predict future weather changes bi2023accurate. itemize quotebox \\large Scalable oversight seeks to ensure that AI systems, even those surpassing human expertise, remain aligned with human intent. quotebox In this context, our primary focus is to present some promising directions that may have not yet been implemented generally for constructing scalable oversight amodei2016concrete,leike2018scalable.",
      "origin_cites_number": 21
    },
    {
      "section_title": "From RLHF to \\RLxF",
      "level": "3",
      "content": "The RLHF paradigm offers a framework for aligning complex systems openai2023gpt4, touvron2023llama. However, it encounters obstacles such as the inaccuracy of human evaluations and their associated high costs christiano2017deep, casper2023open, perez2022discovering. A key limitation is the difficulty in utilizing RLHF to extend human feedback when creating AI systems with superhuman abilities wu2021recursively. Building on the RLHF paradigm, we introduce RLxF as a fundamental framework for scalable oversight, aiming to enhance feedback efficiency and quality and expand human feedback for more complex tasks. This enhances RLHF by incorporating AI components fernandes2023bridging. The x in RLxF signifies a blend of AI and humans. We further explore concrete methodologies about RLxF in the subsequent section. Reinforcement Learning from AI Feedback (RLAIF) RLAIF serves as an extension to RLHF. RLAIF extends the pipeline bai2022training found that LLMs trained via RLHF may avoid sensitive and contentious issues, potentially reducing models' overall utility. To address these limitations, bai2022constitutional proposed a training pipeline that uses feedbacks generated by the LLMs (e.g., GPT-4 or other language models). Following pre-set criteria, the policy model self-evaluates and revises its responses during red teaming. The initial policy model is then fine-tuned using the revised responses. Finally, the fine-tuned policy model evaluates the harmlessness of another language model's responses (i.e., AI feedback). Similar to RLHF , a reward trained using this feedback to optimize the policy model. lee2023rlaif compares the performance of models trained with RLAIF and RLHF on summarization tasks. Their results suggest that models trained with AI feedback aperformed almost identically to those trained with human feedback, though subtle differences remain. Conversely, findeis2024inverse explored the inverse problem of CAI: given a dataset of feedback, how can one extract a constitution that best enables a LLM to reconstruct the original annotations? This problem not only converts AI feedback from preferences into a corresponding constitution but also offers a method for synthesizing new preference data for AI feedback. Reinforcement Learning from Human and AI Feedback (RLHAIF) RLHAIF integrates human and AI models to provide oversight. wu2021recursively explores the feasibility of using AI to assist humans in summarizing books. This method facilitated human supervision and evaluation of the model performance by decomposing the book summarization task into subtasks, creating a tree-like structure. Meanwhile, saunders2022self explores the use of AI to assist in human assessment of model efficacy. Their findings suggest that model-generated critiques help humans identify flaws they might have missed. bowman2022measuring proposes a proof-of-concept experiment to demonstrate the potential of scalable oversight techniques based on sandwiching cotra2021the. When collaborating with an unreliable LLM, the outcomes reveal that humans significantly surpass the model and themselves. perez2022discovering employs language models to autonomously generate datasets for evaluating the behavior of language models of varying scales. The authors produced 154 high-quality datasets validated by humans. These methods demonstrate the feasibility of using AI assistance to scale up human oversight over complex problems and various domains. To some extent, RLAIF and RLHAIF offers a viable alternative for creating a training loop with minimal human intervention, thus reduciing training costs. AI supervision obeying transparent and accessible AI behavior guidelines may significantly aid in achieving scalable oversight bowman2022measuring. Discussion Efforts are underway to enhance RLHF by replacing pure humans alone leike2018scalable. Given the multidimensional nature of human feedback, various approaches have been devised to offer focused human judgments informed by specific rules. Examples of such rules encompass considerations like chat fluency saunders2022self and privacy safeguards rlpf_andrew_carr_2023. saunders2022self deconstructs the requirements for quality dialogue into natural language guidelines that an agent should adhere to, asking for evaluations on each guideline individually. We can attain more efficient rule-conditioned reward models by collecting targeted human assessments and training models on this data. This approach substantially enhances the efficacy of dialogue agents, rendering them more helpful, accurate, and benign when compared to prompted language models. rlpf_andrew_carr_2023 proposes Reinforcement Learning from Privacy Feedback (RLPF), aiming to harmonize the output quality of language models with safeguarding privacy. The method exploits NLP techniques to conduct real-time privacy risk assessments of text generated by the models and subsequently adjusts the reinforcement learning feedback signals based on these evaluations. Expressly, if the generated text includes sensitive information, it incurs negative feedback, whereas high-quality, non-revelatory text receives positive feedback. As the model undergoes training, it incrementally refines its capabilities, enhancing text quality and minimizing privacy breaches concurrently. This approach offers a more efficient evaluation of privacy risks by employing established NLP techniques, in contrast to conventional learning methods, which depend heavily on large-scale manual data annotation. At their core, the RLxF methods utilize the strategy of decomposing a large problem into smaller sub-problems, enabling the use of more efficient tools, such as AI and software, for rapid sub-problem resolution. By leveraging the solutions to these sub-problems, the resolution of the main issue can be expedited. These techniques can be regarded as elementary instances of IDA; the primary distinction lies in the absence of a continual iterative process. Nonetheless, evidence suggests they are promising to offer feedback for AI systems that exceed human performance wu2021recursively. Consequently, these methods can serve as foundational techniques in the training of more advanced AI systems.",
      "origin_cites_number": 20
    },
    {
      "section_title": "Iterated Distillation and Amplification",
      "level": "3",
      "content": "Iterated Distillation and Amplification (IDA) introduces a framework for constructing scalable oversight through iterative collaboration between humans and AIs christiano2018supervising. The process commences with an initial agent, denoted as $A[0]$, which mirrors the decision-making of a human, $H$. $A[0]$ undergoes training using a potent technique that equips it with near-human-level proficiency (the distillation step); Then, collaborative interaction between $H$ and multiple $A[0]$ instances leads to the creation of an enhanced agent, $A[1]$ (the amplification step). The successive process is describedWe reference the pseudo-code by \\citet{cotra2018iterated for this description.} in Algorithm alg:ida. cotra2018iterated distinguishes between broad and narrow definitions within both RL and IRL. Broad RL gives sparse reward signals to AI systems and allows autonomous exploration and optimization of cumulative future rewards. This can lead to super-human novel strategies but makes it hard to specify what we care about perfectly. Narrow RL gives dense feedback rewarding the reasonableness of choices instead of final outcomes. This makes ML systems more human-like but limits capabilities. Similarly, broad IRL infers deep long-term values from the full range of human behaviors, while narrow IRL only infers short-term instrumental values. The former is a higher risk, while the latter is limited in capabilities. During IDA training, narrow techniques are needed to ensure each agent itself mimics human behaviors. Specifically, narrow RL or IL can be used to train the agent to be as human-like and controllable as possible. Humans can leverage agents' computing power and parallelizability to devise more far-sighted, macro strategies. This is essentially an amplification of human intrinsic capabilities. In the next iteration, agents again mimic this strengthened human-machine system using narrow techniques. This enables a gradual transition from narrow ability to broad ability while keeping the agents aligned with human values. As iterations increase, the human-machine system becomes more and more capable, gradually approximating a system that is both highly capable and aligned with human values, achieving both safety and capability. In other words, Narrow techniques are used to ensure agents follow human values, while the broadened human strategies in the amplification stage are a way of utilizing the agents, and do not expand the agents' own learning goals. IDA is well illustrated by AlphaZero christiano2018supervising, nguyen2020ida. The algorithm starts with a simple policy (e.g., random move selection) and learns from its self-play games, the amplification phase. It then uses these games as training data to develop better move selection heuristics, the distillation phase. This distillation-amplification process can be repeated to create a fast and proficient Go-playing AI. Here, the distinction between alignment and capability is crucial mennen2018comment. An aligned but less capable AI tries to win but may not succeed against moderate opponents. A capable but poorly aligned AI achieves certain game properties other than winning. The goal is that AI is capable and aligned, proficient at the game, and aligned with the goal of winning the game. algorithm[t] Iterative Distillation and Amplification algorithmic[1] IDA{$H$} \\State $A \\gets$ random initialization \\Repeat \\State $B \\gets Amplify{H, A}$ \\State $A \\gets Distill{B}$ Repeat indefinitely False \\EndProcedure Distill{overseer} An AI trained using narrow, robust techniques to perform a task that the overseer already understands how to perform. \\EndProcedure Amplify{human, AI} Interactive process in which human uses many calls to AI to improve on human's native performance at the relevant tasks. \\EndProcedure algorithmic algorithm The feasibility of IDA has sparked considerable debate yudkowsky2018challenges. IDA operates under a crucial assumption that errors won't continuously accumulate throughout the iterations leike2018scalable. Thus, technical challenges persist during the distillation and amplification step, necessitating sufficiently advanced and safe learning techniques. Additionally, despite the original authors likening IDA to the training process of AlphaZero silver2017mastering and having demonstrated it in toy environments christiano2018supervising, its practicality hinges on ensuring that $H$ can delegate portions of complex tasks to A, analogous to a leader orchestrating a team to accomplish a project collectively. In practice, Gato reed2022generalist illustrates key aspects of IDA mukobi2021gato that may pave the way to AGI. It consolidates the abilities of multiple expert AIs into a singular model, validating that IDA's distillation can be achieved using contemporary deep learning. While not fully realized, Gato hints at amplification potential, harnessing its diverse skills to accelerate the learning of new tasks. However, Gato lacks safe amplification or distillation methods to maintain alignment properties. Crafting alignment-preserving IDA methods suited for models like Gato remains a crucial direction for AI safety research. In essence, while Gato signifies notable progress in actualizing IDA, further theoretical advancements are imperative to ensure that the IDA framework leads to safe AGI.",
      "origin_cites_number": 11
    },
    {
      "section_title": "Recursive Reward Modeling",
      "level": "3",
      "content": "As discussed in \\Spar:reward_model, reward modeling leverages the idea of using human feedback to train a reward model, which an agent then pursues. It allows us to disentangle the construction of the system's objective from evaluating its behavior ibarz2018reward. In this manner, the reward model provides insights into the optimization direction of the AI system. Particularly noteworthy is the ability to finely align the system with human intentions and values, such as fine-tuning language models to adhere to human instructions bai2022training, touvron2023llama. Also, reward modeling has proved valuable in advancing AI research zhao2023survey, bukharin2023deep. Recursive Reward Modeling (RRM) leike2018scalable seeks to broaden the application of reward modeling to much more intricate tasks. The central insight of RRM is the recursive use of already trained agents $A_{t-1}$ to provide feedback by performing reward learning on an amplified version of itself for the training of successive agents $A_{t}$ on more complex tasks. The $A_0$ is trained via fundamental reward modeling (learned from pure human feedback). This approach is not only influenced by human feedback but also by the model's own assessments of what constitutes a rewarding outcome. If the assumption that evaluating outcomes is easier than producing behavior holds, then the iterative process of reward modeling can iteratively achieve higher capacity to oversee more powerful AI systems, paving the way for extending oversight into more complex domains. This process is detailed in Algorithm alg:rrm. For instance, we aim to train AI $A$ to devise a comprehensive urban plan. Designing a city entails numerous intricate elements, such as traffic planning, public amenities, and the distribution of residential and commercial zones. Evaluating a city's entire design poses a significant challenge since many issues may only become apparent after extended real-world testing. To aid this process, we may need an agent $B$ specifically for traffic planning. However, traffic planning in itself is a multifaceted task. Consequently, we further need other agents to assess aspects such as road width, traffic flow, and the design of public transportation. For every sub-task, such as gauging road width, we can train an auxiliary agent to verify if safety standards are met, if various modes of transportation have been considered, and so on. In doing so, we establish an RRM process where each agent is trained with the help of agents assessing sub-tasks. This approach resembles the organizational structure of a large corporation leike2018scalable. In the context of urban planning, the main planning team (the CEO) is responsible for the final design decisions. Their decisions are informed by recommendations from the traffic team (the department managers), who, in turn, base their recommendations on inputs from the road width team (the managers), and so forth. Each level of decision-making relies on feedback from the level below it, with each task optimized through reward modeling. The challenges faced by RRM can be described around the concepts of outer and inner alignment hubinger2020overview. Outer alignment revolves around the sufficiency of feedback mechanisms to guarantee that the learned reward model is accurate in the domain perceived by the action model as on distribution. This challenge is contingent on several factors, including the quality of human feedback, the difficulty of generalization, and the potential for agent deception. Conversely, inner alignment concentrates on how effectively a human can employ transparency tools to prevent deceptive or disastrous behaviors in both the reward model and the agent. This hinges on the effectiveness of the oversight mechanism and the capacity to verify that the reward model isn't undergoing any optimization and that the agent remains myopic cotra2018iterated. algorithm[t] Recursive Reward Modeling algorithmic[1] \\State Initialize agent $A_0$ using reward modeling based on user feedback. \\Comment Either preferences or numerical signals. \\For {$t$ = 1, 2, \\dots} \\State Use $A_{t-1}$ to assist users in evaluating outcomes. \\State Train agent $A_t$ based on user-assisted evaluations. \\Comment Objective of $A_t$ is generally more complex than that of $A_{t-1}$. \\EndFor algorithmic algorithm Potential approaches to mitigate these challenges leike2018scalable include online feedback to correct the reward model during training christiano2017deep, off-policy feedback to teach about unsafe states everitt2017reinforcement, leveraging existing data like videos and text via unsupervised learning or annotating baker2022video, hierarchical feedback on different levels bukharin2023deep adversarial training to discover vulnerabilities madry2018towards, and uncertainty estimates for soliciting feedback hadfield2016cooperative, macglashan2017interactive. The strength of RRM is its competitive training approach, which necessitates human feedback instead of demonstrations, potentially making feedback more reliable and simpler to obtain hubinger2020overview. In essence, the process of RRM can be likened to IDA christiano2018supervising, where reward modeling takes the place of supervised or imitation learning. Thus, the challenges confronted by RRM closely mirror those encountered in IDA, particularly in preventing the accumulation of errors. Additionally, reward modeling itself does not necessarily distill a narrow model cotra2018iterated, which presents challenges in trading off the degree of alignment and performance.",
      "origin_cites_number": 17
    },
    {
      "section_title": "Debate",
      "level": "3",
      "content": "Debate involves two agents presenting answers and statements to assist human judges in their decision-making irving2018ai, as delineated in Algorithm alg:debate. This is a zero-sum debate game where agents try to identify each other's shortcomings while striving to gain higher trust from human judges, and it can be a potential approach to constructing scalable oversight. For example, in the game of Go, human judges might not discern the advantage side of the single game board itself. However, by observing the game's process and the eventual outcome, these judges can more easily deduce that. The premise of this method relies on a critical assumption: arguing for truth is generally easier than for falsehood, granting an advantage to the truth-telling debater. However, this assumption does not hold universally. For instance, in a complex problem, humans might fail to comprehend the specialized concepts used in the debate. Additionally, the limited nature of the gradient descent may bring us to an undesirable cyclic pattern (i.e., when optimizing for one property, such as honesty and highlighting flaws, models often overlook or diminish another) irving2018ai. It's worth mentioning that with the advancement of LLMs' capabilities, we can already see practical examples of debate du2023improving, lw2023debate. Challenges may arise for debate in specific real-world scenarios irving2018ai. For example, certain questions may be too intricate for human comprehension or too voluminous to present in their entirety. Similarly, there are instances where an optimal answer to a question is exceedingly lengthy, envision a response that spans a hundred pages. To navigate these, agents might initially select a response and, as the debate progresses, reveal sections of either the question or the answer. irving2018ai conducts a toy experiment on this process. Meanwhile, we must acknowledge the limit of human time. In scenarios that necessitate interaction with the environment, such as directing a robot, each action might demand a distinct debate. It's not always feasible for humans to judge every debate due to time constraints. In response to this challenge, we may need to design ML models to predict human feedback. In line with this observation, khan2024debating experimented with using smaller, non-expert models as judges in debates between two expert models, both of which had access to the underlying data and the ability to quote from it. The experiments demonstrated that these smaller non-expert models were able to achieve higher accuracy when relying on the expert model debates, though they still underperformed compared to human judges. Additionally, the expert models can be optimized for persuasiveness, enabling the judges to attain even greater accuracy and more easily identify the truth. The authors emphasize that debate implementations must be grounded in verifiable evidence to prevent debaters from fabricating facts. Further work on using weaker models as judges in debates guided by stronger models was conducted by kenton2024scalable. Their experiments focused on tasks involving both information asymmetry and symmetry between debaters and judges and were extended to include multimodal inputs. The protocols they applied evaluated the baseline performance of judges without debate protocols, alongside debate and consultancy protocols. These experiments considered both assigned positions and cases where debaters or consultants could choose their positions. Experimental results showed that debate consistently outperforms consultancy. Weak judges struggle to fully leverage debate protocols, and consultancy can significantly reduce the accuracy of judges, particularly when the consultant advocates for an incorrect solution. Overall, the authors interpret their findings as only weakly promising for the debate framework. However, these experiments were conducted solely with models at inference time, and debate protocols may hold greater potential when integrated into training. This is particularly relevant given that the task of judging a debate can be seen as OOD for models primarily fine-tuned for question answering. Another consideration is the convergence of the debate mechanism irving2018ai. du2023improving showcases the inherent tendency of the debate framework to eventually converge toward singular responses, even if accuracy is not guaranteed. Meanwhile, if challenges arise in achieving convergence, we might have to rely on intuition to gauge the effectiveness of convergence. This implies the requirement of human evaluators' intervention and demands a certain level of expertise from these human assessors, posing challenges that must be addressed. Furthermore, there are many discussions originating from diverse perspectives. ngo2021excited considers Debate as one type of iterated amplification but more specific to make safety ground in concrete research questions, and its adversarial framing makes it easier to spot problems. michaelcohen2020 expresses concerns regarding the adverse implications of incentivizing debaters to employ deceptive strategies aimed at swaying the judgment process. struart2019, beth2020debate expound upon the various issues that can permeate the debate process, including challenges such as the obfuscated arguments problem, ambiguous responses, and the propagation of misleading implications. While one may affirm the presence of a sufficiently low probability of any underlying flaws within the argument, advocating for trustworthiness, the opposing debater may assert the existence of a sufficiently high probability of identifying a flaw within the argument somewhere, thus advocating for a lack of trust. beth2020progress introduces the concept of cross-examination to incentivize debaters to provide more informative responses. In this process, debaters have the agency to select a prior claim for scrutiny and obtain a copy of the opposing debater's response. The entire exchange is documented, and debaters can present relevant segments to the judge. The introduction of cross-examination is a robust deterrent against dishonest debaters exploiting a sweeping narrative, in contrast to their prior arguments, to mislead the judge. algorithm[t] Debate algorithmic[1] \\State Initialize set of questions $Q$. \\State Initialize two competing agents. \\State Select a question $q \\in Q$. \\Comment Question is shown to both agents. \\State Agents provide their answers $a_0$ and $a_1$. \\ The agents generate comment answers in response to $q$. \\State Initialize debate transcript $T$ as an empty list. turn in predefined number of debate turns \\State Agent makes a debate statement $s$. \\State Append $s$ to $T$. \\Comment Agents take turns and statements are saved in the transcript. \\EndFor \\State Judge observes $(q, a_0, a_1, T)$ and decides the winning agent. algorithmic algorithm There exists a notable similarity between the debate irving2018ai, IDA christiano2018supervising, and RRM leike2018scalable. These approaches can be comprehended in the view of an underlying principle: evaluation can be simpler than task completionDiscussions about this can also be found in the literature about these methods.. Therefore, harnessing the evaluative capabilities of AI systems can result in distributions of capacity that are more advantageous for humans. The challenges these methods face, especially in mitigating the accumulation of errors, are also analogous.",
      "origin_cites_number": 16
    },
    {
      "section_title": "Cooperative Inverse Reinforcement Learning",
      "level": "3",
      "content": "Almost all previous methods consider learning from feedback a process separate from inference and control and often implicitly consider feedback providers as entities existing outside of the environment -- indeed, failure modes like manipulation shevlane2023model and reward tampering everitt2021reward occur exactly when feedback mechanisms that are supposedly outside of the environment become part of it and therefore subject to the AI system's influence. The framework of Cooperative Inverse Reinforcement Learning (CIRL), however, unifies control and learning from feedback and models human feedback providers as fellow agents in the same environment. It approaches the scalable oversight problem not by strengthening oversight but by trying to eliminate the incentives for AI systems to game oversight, putting humans giving feedback and the AI system in cooperative rather than adversarial positions shah2020benefits. In the CIRL paradigm, the AI system collaborates with humans to achieve the human's true goal rather than unilaterally optimizing for human preferences. Motivation and General Idea of CIRL Many modes of misalignment, including, for example, reward hacking specification2020victoria,skalse2022defining, deception park2023ai, and manipulation shevlane2023model, are results of the AI system confidently optimizing for misspecified objectives pan2021effects. During training and deployment, the specified objective (e.g., the reward function) plays the role of an unchallengeable truth for the AI system, and human feedback is only respected to the extent specified in the objective, which means that it could be tampered everitt2021reward or manipulated shevlane2023model. CIRL hadfield2016cooperative,hadfield2017inverse,shah2020benefits attempts to mitigate this problem by (1) having the AI system explicitly hold uncertainty regarding its reward function, and (2) having humans provide the only information about what the reward function truly is. This uncertainty gives the AI system a tendency to defer to humans and a drive to determine what the human truly wants. Concretely speaking, it models the entire task as a two-player cooperative game, where the human player $H$ and the robot player $R$ share a common reward function $r(\\cdot)$. Importantly, the reward function and reward signals aren't visible to $R$ (and indeed aren't explicitly calculated by the training mechanism) and are only inferred by $R$ from behaviors of $H$ via an IRL-like process (including by asking and interacting with $H$). This game has been called the CIRL hadfield2016cooperative, the assistance game fickinger2020multi, and the assistance POMDP shah2020benefits. In short, the AI system has the human's true objective $r(\\cdot)$ as its own goal (despite not knowing values of $r(\\cdot)$ with certainty) and constantly tries to figure $r$ out by observing and interacting with the human. This reduces incentives for, e.g., manipulation since manipulation of human behaviors only serves to pollute an information source and does not affect $r$. Formulation of CIRL hadfield2016cooperative characterizes the settings of CIRL (which we denote by $M$) by building upon classical multi-agent MDPs, resulting in the definition below of $M$. \\[M=\\big\\langle S,\\{A^\\mathbf{H},A^\\mathbf{R}\\}, T, \\gamma, r,\\Theta,P_0 \\big\\rangle\\] In the equation above, $S$ and $\\{A^\\mathbf{H},A^\\mathbf{R}\\}$ are the space of world states and actions respectively, $T:S\\timesA^\\mathbf{H}\\timesA^\\mathbf{R}\\rightarrow \\Delta(S)$ is the transition function, and $\\gamma$ is the discount rate. Up to here, the definition is identical to that of a standard multi-agent MDP. The remaining elements, however, introduce the key difference: the reward function is parameterized, and its parameters can be modeled by a distribution. $\\Theta$ is the space of values for the parameters $\\theta$; $r: S \\times A^\\mathbf{H}\\timesA^\\mathbf{R}\\times\\Theta\\rightarrowR$ is the shared reward function, and $P_0\\in\\Delta(S\\times\\Theta)$ is the joint distribution of the initial state and the reward function's parameters. This parameterization approach allows $R$ to model explicitly and reason about its belief over the true reward function. Using techniques from nayyar2013decentralized, any CIRL setting can be reduced to an equivalent single-agent POMDP, thus proving the existence of optimal policies that are relatively tractable hadfield2016cooperative. Notable Directions in CIRL Research Although some have emphasized the importance of $H$ teaching $R$ fisac2020pragmatic actively, works shah2020benefits have contested the emphasis on game equilibria and joint policies (including $H$'s pedagogic behaviors), and instead focuses on $R$'s optimal response to a policy of $H$'s, since the assumption that humans will always act on optimal joint policies is an unrealistic one. More specifically, shah2020benefits considers the policy-conditioned belief $B:\\Pi^{R}\\rightarrow\\Delta\\left(\\Pi^{H}\\right)$, which specifies $H$'s distribution over policy responses to any of $R$'s policies, and the aim is to find $R$'s optimal policy given $B$. Here, $B$ is essentially a form of human modeling, and one challenge is to obtain a robustly accurate human model as $B$ hong2022sensitivity. On another front, hadfield2017inverse and he2021assisted examine the manual specification of an imperfect reward function as a way for $H$ to convey information about the true reward function. This includes work on $R$'s side (i.e., enabling $R$ to perform inference on the true reward function based on the imperfect specification) hadfield2017inverse and also work on $H$'s side (i.e., developing algorithmic tools to assist $H$ in making more robust specifications that better convey the true reward function) he2021assisted. Aside from improvements to the game settings, the design of more scalable CIRL algorithms has also been recognized as a priority. There has also been work that extends CIRL and assistant games to multi-agent settings fickinger2020multi where there are multiple humans that the robot needs to serve. This corresponds to the multi/single delegation settings in critch2020ai, where the varying objectives of humans create a challenge and necessitate the use of social choice methods.",
      "origin_cites_number": 26
    },
    {
      "section_title": "Circuit Breaking",
      "level": "3",
      "content": "Instead of training a model to refuse harmful outputs, the circuit-breaking approach proposed by zou2024improvingalignmentrobustnesscircuit directly controls the internal representations responsible for generating such outputs. A potential advantage of this method is that it aims to enhance safety without compromising performance. The core idea is to manage the models ability to generate harmful outputs, rather than eliminating the underlying vulnerabilities. This approach renders circuit-breaking attack-agnostic, as new adversarial attacks may emerge, but the internal representations associated with harmful outputs remain constant. The circuit-breaking method consists of two essential components: the dataset and the loss function. The dataset includes harmless samples, where internal representations should remain unchanged, while the circuit-breaking set comprises harmful samples, which require altered internal representations to prevent the generation of harmful content. By employing a mean squared error loss for retaining representations and cosine similarity for circuit-breaking representations, the model becomes unable to generate harmful content. Changing the loss function for circuit-breaking can also allow for steering the model's generation in various directions, such as ending the output generation or refusing to answer. Additionally, this approach can be extended with a Harmfulness Probing mechanism to detect and respond to harmful generations. The authors successfully applied this method to Large Language Models, Multimodal models, and Language Agents to control function calls.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Weak-to-Strong Generalization",
      "level": "3",
      "content": "Scalable Oversight can help humans provide supervision signals to AI systems that are smarter and more complex, ensuring that the behaviors of super-human-level AI systems align with human intent and values. However, what if we cannot obtain scalable supervision signals? An example is that for some tasks, evaluation is not necessarily simpler than generation, making it impossible to utilize task decomposition followed by AI assistance to achieve scalable oversight. Recently, a generalization phenomenon called Weak-to-Strong Generalization is verified, the core idea of which is to use weak supervision signals from a weak model to train a strong model burns2023weak. Specifically, the weak model is trained on ground truth and then annotates new data with weak labels for training the strong model. The results across three settings (i.e. NLP classification, chess puzzles and reward modeling) reflect that weak-to-strong generalization is a robust phenomenon, yet there is room for further improvement, such as narrowing the gap between a strong model trained with weak labels and ground truth. Weak-to-Strong Generalization provides a valuable analogy for the superalignment problem: how humans can supervise super AI systems as weak supervisors. The insight behind weak-to-strong generalization is that the strong model can generalize beyond weak labels instead of merely imitating the behavior of weak models. In other words, the weak model elicits the strong model's capability. However, verifying weak-to-strong generalization is challenging if humans don't know the ground truth. Nonetheless, weak-to-strong generalization still offers a valuable perspective for solving the superalignment problem. The framework for weak-to-strong generalization has been further expanding and integrating with scalable oversight. Empirical results show that weak models can evaluate the correctness of stronger models by assessing the debate between two expert models khan2024debating. Additionally, making expert debaters more persuasive improves non-experts' ability to discern truth in debates, evidencing the effectiveness of aligning models with debate strategies without ground truth. Some frameworks employ a external amplifier to create an iterated distillation and amplification process, which presents a potential framework for integrating weak-to-strong generalization techniques with IDA during the training process ji2024aligner. Moreover, leike2023combine proposes several methods to integrate scalable oversight with weak-to-strong generalization techniques, e.g., recursively decomposing tasks into atomic ones (in line with scalable oversight principles), supervising these atomic tasks, and employing reward models trained with weak-to-strong generalization techniques using human preference data.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Learning under Distribution Shift",
      "level": "1",
      "content": "The construction of reliable AI systems is heavily dependent on their ability to adapt to diverse data distributions. Training data and training environments are often imperfect approximations of real deployment scenarios and may lack critical elements such as adversarial pressures poursaeed2021robustness (e.g., Gaussian noise in the context of supervise learning-based systems gilmer2019adversarial and shadow attack ma2012shadow in autonomous-driving systems), multi-agent interactions critch2020ai,dafoe2021cooperative, complicated tasks that human overseers cannot efficiently evaluate leike2018scalable,This could contribute to the emergence of deceptive behaviors \\citep{hubinger2019deceptive. See the paragraph on goal misgeneralization in \\Ssec:chal-dis for details.} and reward mechanisms that can be gamed or manipulated krueger2020hidden. This discrepancy between training distribution and testing distribution (or environments) is known as distribution shift krueger2020hidden,thulasidasan2021effective. Therefore, AI systems that are aligned under their training distribution (i.e., pursuing goals that are in line with human intent) may not uphold their alignment under deployment (or testing) distribution, potentially leading to serious misalignment issues post-deployment. This potential failure motivates research on the preservation of alignment properties (i.e., adherence to human intentions and values) across data distributions. From an alignment perspective, we are more concerned about AI systems pursuing unaligned and harmful goals, as opposed to incompetence at pursuing goals. Thus, the emphasis on alignment properties means that we focus on the generalization of objectives across distributions, as opposed to the generalization of capabilities di2022goal,ngo2024the. We mainly discuss the preservation of alignment properties when learning under distribution shift in this section. We start the discussion by introducing the alignment challenges from distribution shift (\\Ssec:chal-dis). Subsequently, we delve into methods for addressing distribution shift, and discuss two approaches in particular: (1) algorithmic interventions (\\Ssec:alg-inter) that steer optimization during the training process, and (2) data distribution interventions (\\Ssec:data-inter) that expand the training distribution by introducing specific elements into the training process, including adversarial training yoo2021towards,tao2021recent,ziegler2022adversarial and cooperative training dafoe2021cooperative (\\Ssec:cooperative-ai-training). Our framework for learning under distribution shift is shown in Figure fig:generalization.",
      "origin_cites_number": 11
    },
    {
      "section_title": "The Distribution Shift Challenge",
      "level": "2",
      "content": "Before introducing the specific techniques, we initially demonstrate why one of the primary challenges in alignment is learning under distribution shift, and more specifically, the preservation of alignment properties (i.e., adherence to human intentions and values) under distribution shift. We introduce two alignment challenges concerning the issue of distribution shift, namely goal misgeneralization di2022goal and auto-induced distribution shift (ADS) krueger2020hidden. The training of AI systems optimizes for their adherence to the pursuit of the training reward/loss under the training input distribution. However, this adherence may not generalize to cases where the input distribution undergoes qualitative changes, i.e., distribution shift. These changes include, for example, adversarial pressures poursaeed2021robustness, multi-agent interactions critch2020ai, and complicated tasks that human overseers cannot efficiently evaluate di2022goal, and reward mechanisms that can be gamed or manipulated krueger2020hidden. It's worth distinguishing two different failure modes here: goal misgeneralization di2022goal, in which the original and shifted distributions are given, and auto-induced distribution shift krueger2020hidden, where the AI system alters the data distribution with its own behaviors in pursuit of reward. Goal Misgeneralization This kind of challenge refers to the scenario where AI systems perform perfectly in the training distribution, but the capabilities learned in training distribution fail to generalize in OOD deployment, and AI may present the pursuit of goals that are not in accordance with human wishes di2022goal. Goal misgeneralizationMore examples of goal misgeneralization exist \\citep{gmg2020.} is to be distinguished from other forms of misgeneralization (e.g., capability misgeneralization) where the agent becomes incompetent in OOD settings; instead, agents with goal misgeneralization competently pursue an unwanted goal in OOD settings. A simplistic example is the case of spurious correlations (or shortcut features) geirhos2018imagenettrained,di2022goal. For example, in an image classification dataset, green grass is a highly predictive feature for the label cow. However, it is essential to note that this feature needs to be more consistent and reliable across various data distributions pml2Book. Moreover, the causal confusion (i.e., ignorant of the causal structure of the interaction between the advisor and the environment) in IL can result in goal misgeneralization de2019causal,tien2022causal. One major danger from goal misgeneralization lies in the indistinguishability between ``optimizing for what human really wants'' and ``optimizing for human thumbs-ups'';Here, \\textit{human thumbs-ups refer to high-reward feedback from human advisors or environment. However, AI systems may deliberately follow human preferences or deceive to get high rewards from humans, but actually don't really learn intended goals (i.e., what human really wants).} the latter includes potentially deceiving or manipulating human evaluators shevlane2023model to receive their thumbs-ups. For example, learning2017dario discovered that in a task where a robotic hand is supposed to grasp a small ball, the robotic hand fakes the action by using parallax in front of the lens to appear as if it has grasped the ball, without actually doing so. This behavior deceives the human annotator into thinking that the task has been completed. When an AI system is trained or finetuned with human feedback, it is impossible to distinguish the two goals since both perform perfectly in training, and it is unclear which one the AI system will learn. In fact, during training, the human evaluators might be deceived or manipulated, implying that the AI system may be more strongly incentivized to optimize for human thumbs-ups rather than what the human wants. Current examples of this phenomenon exist in recommender systems kalimeris2021preference,adomavicius2022recommender, LLMs perez2022discovering, and RL systems learning2017dario. Finally, one failure mode closely related to goal misgeneralization is the misalignment of mesa-optimizers hubinger2019risks, where the ML model with learned model weights performs optimization within itself during inference (``mesa-optimization'') hubinger2019risks,dai2023can, and the objective of this optimization is not aligned with the model's training objective. Auto-Induced Distribution Shift (ADS) While training AI systems, we often consider the strengths and weaknesses of the agents themselves only and overlook the impact that these agents have on the environment. Past research often assumed that data is independently and identically distributed besbes2022beyond, ignoring the effect of algorithms on data distribution. However, krueger2020hidden posited that, in reality, agents could influence the environment during the decision-making and execution process, thus altering the distribution of the data generated by the environment. They referred to this type of issue as ADS. A real-world example is in recommendation systems, where the content selected by the recommendation algorithms might change users' preferences and behaviors, leading to a shift in user distribution. The distribution shift, in turn, further affects the output of the recommendation algorithms carroll2022estimating. As AI systems increasingly impact the world, we also need to consider the potential further impacts on the data distribution of the entire society after agents are integrated into human society. Superficial Alignment In recent work, the technique of Inverse Alignment in LLMs was introduced by ji2024language. The study focused on the elasticity of LLMs, which describes their tendency to revert to a state resembling their original pretrained form after further finetuning post-alignment. This behavior, observed in multiple studies yang2023shadow,zhou2024lima, suggests that alignment may not be a permanent change, as models can easily lose their aligned behavior when subjected to new fine-tuning tasks. The authors formally define inverse alignment as follows: Given an initial LLM \\( p_{\\theta_0} \\), after aligning it on a dataset \\( D_a \\) to produce the aligned model \\( p_{\\theta_1} \\), an operation is performed using a much smaller dataset \\( D_b \\) (where \\( |D_b| \\ll |D_a| \\)), resulting in an inverse-aligned model \\( p_{\\theta'_0} \\). The goal is to ensure that \\( \\rho(p_{\\theta'_0}, p_{\\theta_0}) \\leq \\epsilon \\) for some metric \\( \\rho \\), which measures behavioral or distributional similarity between models. This process is referred to as inverse alignment, and the return from \\( p_{\\theta_1} \\) to a model similar to \\( p_{\\theta_0} \\) is the essence of elasticity. Elasticity is further formalized as the property of LLM parameters returning to a state close to \\( p_{\\theta_0} \\), given an algorithmically simple inverse transformation \\( g \\) applied to \\( p_{\\theta_1} \\), along with a dataset \\( D_b \\). The dataset size constraint \\( |D_b| \\ll |D_a| \\) ensures that a small amount of data suffices to reverse the effects of alignment, leading to \\( p_{\\theta'_0} \\) such that \\( \\rho(p_{\\theta'_0}, p_{\\theta_0}) \\leq \\epsilon \\). Theoretical findings show that the normalized compression rate for both the pretraining and fine-tuning datasets decreases upon additional fine-tuning, but the reduction is more pronounced for the fine-tuning dataset by a factor of \\( \\Theta(k) \\), where $k=\\mid D_1 \\mid{\\mid D_2\\mid}$, $ D_1 $ is the pretraining dataset, and $ D_2 $ is the fine-tuning dataset. This suggests that models are more likely to forget the distribution in the fine-tuning dataset while maintaining their pretraining distribution after exposure to new data. Experimental results further confirm the existence of elasticity in LLMs, showing that they tend to revert to the pretraining distribution with fewer training samples than required during the alignment phase. Larger models, and those with more extensive pretraining data, exhibited greater elasticity, highlighting the limitations of current alignment methods. The theoretical and experimental results indicate that alignment methods are often superficial, and more robust approaches are necessary to ensure the safety of AI systems, particularly in the face of inverse alignment. Moreover, these findings underscore the risks associated with open-source models, as they could potentially be reverted to unsafe states with minimal effort, raising concerns about open-source policies in large AI companies.",
      "origin_cites_number": 25
    },
    {
      "section_title": "Algorithmic Interventions",
      "level": "2",
      "content": "When illustrating the algorithmic intervention methods, we first outline two classes of methods that steer optimization on various distributions during training to relieve distribution shift, {namely, cross-distribution aggregation (\\Ssec:cross) and navigation via mode connectivity (\\Ssec:robust-navi)}. In the first part, we cover methods ranging from the initial approach of empirical risk minimization (ERM) vapnik1991principles to risk extrapolation (REx) krueger2021out, a method conceived to mitigate issues arising from models' dependence on spurious features. In the second part, we introduce connectivity-based fine-tuning, which guides the navigation of the loss landscape during training to encourage convergence upon non-spurious correlations, and which does so using insights from mode connectivity lubana2023mechanistic.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Cross-Distribution Aggregation",
      "level": "3",
      "content": "One of the main reasons for distribution shift is spurious correlations in the model that are distinct from core objectives geirhos2018imagenettrained. By integrating learning information of different domains (or different distributions) into the optimization objective, we expect the model to learn truthful information and invariant relationships. In the following paragraphs, we first introduce ERM as the background and then introduce some methods to directly learn how to address distribution shift by integrating loss landscapes of different distributions in the training process. Empirical Risk Minimization (ERM) Consider a scenario where a model has been developed to identify objects by their features effectively. The optimization target can be expressed as: align* R (w) = \\int L \\big(y, f\\left(x, w\\right)\\big) \\, dP\\left(x, y\\right) align* where $L (y, f(x, w))$ denotes the loss between data labels $y$ and model outputs $f(x, w)$, while $P(x, y)$ signifies the target data distribution vapnik1991principles. Nevertheless, a bias often exists between the dataset and the real world, implying that the features learned from the dataset may not necessarily be the ones we intend for the model to acquire. ERM is a strategy employed in statistical methods to optimize this bias. It operates on the assumption that, given the inaccessibility of the real-world target data distribution, the empirical data within the dataset should, ideally, closely approximate this unknown target distribution vapnik1991principles, zhang2018mixup. In this context, the objective function is optimized and is redefined as: align* E (w) = 1{l} \\sum_{i = 1}^l L \\big(y_i, f\\left(x_i, w\\right)\\big) align* where $l$ can be different examples in one training distribution or different training distributions. Minimizing the objective function above allows the model to learn the invariant relationship in different distributions. Naive ERM makes the naive assumption that the data is sampled from the target data distribution. However, if a significant discrepancy exists between the source distribution (or training distribution) and the target distribution, severe generalization issues can still arise szegedy2013intriguing. Distributionally Robust Optimization (DRO) Numerous studies posit that the sensitivity to distribution shift often arises from reliance on spurious correlations or shortcut features unrelated to the core concept geirhos2018imagenettrained,hendrycks2018benchmarking. For instance, models may judge based on background features rather than employing the correct features within the image geirhos2018imagenettrained,beery2018recognition. Building upon the foundations laid in prior research ben2009robust,peters2015causal,krueger2021out, OOD Generalization can be formulated as follows: align* r_{D}^{OOD}(\\theta)=\\max _{e \\in D} r_e\\left(\\theta\\right) align* This optimization seeks to enhance worst-case performance across a perturbation set, denoted as $D$, by reducing the maximum value among the risk function set $\\left\\{ r_e | e \\in D \\right\\}$. In Distributionally Robustness Optimization (DRO) duchi2021statistics, the perturbation set covers the mixture of different domains' training distributions, and by minimizing the above objective function, we expect the model can find the invariant relationship between different training distributions. However, it should be noted that naively applying DRO to overparameterized neural networks may lead to suboptimal outcomes sagawa2020distributionally. Therefore, combining DRO with increased regularization techniques such as $l_2$ penalty cortes2009l2 or early stopping prechelt2002early can substantially improve generalization performance. For more details on DRO, see e.g., rahimian2019distributionally,sagawa2020distributionally,lin2022distributionally Invariant Risk Minimization (IRM) arjovsky2019invariant introduces an innovative learning paradigm to estimate nonlinear, invariant, causal predictors across diverse training environments, thereby facilitating robust OOD generalization. IRM aims to train a predictive model with solid performance across various environments while demonstrating reduced susceptibility to relying on spurious features. IRM can be considered an extension of Invariant Causal Prediction (ICP) peters2015causal, which involves hypothesis testing to identify the direct causal features that lead to outcomes within each specific environment instead of indirect features. IRM further extends ICP to scenarios characterized by high-dimensional input data, where variables may lack clear causal significance. The fundamental idea underlying IRM is that when confronted with many functions capable of achieving low empirical loss, selecting a function that exhibits strong performance across all environments is more likely to get a predictor based on causal features rather than spurious ones pml2Book. Risk Extrapolation (REx) The basic form of REx involves robust optimization over a perturbation set of extrapolated domains (MM-REx), with an additional penalty imposed on the variance of training risks (V-REx) krueger2021out. By reducing training risks and increasing the similarity of training risks, REx forces the model to learn the invariant relationship in different domain distributions. Amplifying the distributional variations between training domains can diminish risk changes, thereby enforcing the equality of risks. Taking CMNIST arjovsky2019invariant as an example, even though establishing a connection between color and labels is more straightforward than connecting logits and labels, increasing the diversity in color can disrupt this spurious correlations (or shortcut features) and aid the model in learning the genuine invariant relationship between logits and labels. Following previous research vapnik1991principles,peters2017elements,krueger2021out, REx can be formulated as follows: Firstly, the Risk Function can be defined as follows: align* r_e(\\theta) \\doteq E_{(x, y) \\sim P_e(X, Y)} L\\big(f_{\\theta}(x), y\\big) align* where $L(\\cdot)$ represents a fixed loss function, and distinct training domains or environments can be formulated as the $P_e(X, Y)$ distribution. Next, the MM-REx term can be modeled as: align* r_{MM - REx}(\\theta) = \\left(1-m \\lambda_{\\min }\\right) \\max _e r_e(\\theta)+\\lambda_{\\min } \\sum_{e=1}^n r_e(\\theta) align* where $n$ represents the number of distinct distributions or domains, and $\\lambda_{\\min}$ governs the extent of risk extrapolation. Moving on to the V-REx term, it can be modeled as: align* r_{V-REx}(\\theta) = \\alpha Var\\Big(\\big\\{r_1(\\theta), \\ldots, r_n(\\theta)\\big\\}\\Big)+\\sum_{e=1}^n r_e(\\theta) align* where $\\alpha \\geq 0$ controls the trade-off between risk reduction and enforcing risk equality. In the MM-REx term, the $\\lambda_{\\min}$ can set nearly $-\\infty$; therefore, the loss of specific domains may be high, meaning that the model may learn the spurious correlations. Minimizing the MM-REx and V-REx can reduce training risks and increase the similarity of training risks, encouraging the model to learn invariant relationships. Furthermore, REx has shown significant promise in experimental settings krueger2021out, particularly in causal identification, making it a compelling approach for achieving robust generalization. Tackle Distribution Shift in LLMs In the context of LLMs, prior research has shown that RL often exploits shortcuts to achieve high rewards, overlooking challenging samples deng2023multilingual. This evasion of long-tail training samples prevents LLMs from effectively handling distribution shifts in general scenarios, which falls short of expectations for these models: as universal AI assistants, they should maintain consistent performance across various domains. Recently, many works have attempted to implement cross-distribution aggregation in LLMs to address this issue. zheng2024improving employ RL to learn uniform strategies across diverse data groups or domains, automatically categorizing data and deliberately maximizing performance variance. This strategy increases the learning capacity for challenging data and avoids over-optimization of simpler data. yao2024improving concentrate on exploiting inter-domain connections. Specifically, they acquire training-domain-specific functions during the training phase and adjust their weights based on domain relations in the testing phase, achieving robust OOD generalization.",
      "origin_cites_number": 22
    },
    {
      "section_title": "Navigation via Mode Connectivity",
      "level": "3",
      "content": "Following the above discussion about cross-distribution aggregation, in this section, we introduce mode connectivity as the prerequisite content. Then, we primarily discuss the Connectivity-Based Fine-Tuning (CBFT) lubana2023mechanistic method, illustrating how mode connectivity navigates the model to predict based on invariant relationships instead of spurious correlations by changing few parameters. Mode Connectivity Mode connectivity refers to the phenomenon where one can identify a straightforward path within the loss function space that connects two or more distinct local minima or patterns garipov2018loss,draxler2018essentially. In line with prior research benton2021loss,pittorino2022deep,lubana2023mechanistic, a formal definition can be defined as follows: The model's loss on a dataset $D$ is represented as $L(f(D; \\theta))$, where $\\theta$ denotes the optimal parameters of the model, and $f(D; \\theta)$ signifies the model trained on dataset $D$. We define $\\theta$ as a minimizer of the loss on this dataset if $L(f(D; \\theta))<\\epsilon$, where $\\epsilon$ is a small scalar value. Minimizers ${\\theta}_1$ and ${\\theta}_2$, achieved through training on dataset $D$, are considered to be mode-connected if there exists a continuous path $\\gamma$ from ${\\theta}_1$ to ${\\theta}_2$ such that, as ${\\theta}_0$ varies along this path $\\gamma$, the following condition is consistently upheld: align* L\\big(f\\left(D; {\\theta}_0\\right)\\big) \\leq t \\cdot L\\big(f\\left(D; {\\theta}_1\\right)\\big) + \\left(1-t\\right) \\cdot L\\big(f\\left(D; {\\theta}_2\\right)\\big), \\quad \\forall t \\in [0,1]. align* In essence, mode connectivity entails consistently finding a connecting pathway among minimizers in the parameter space, traversing regions of low loss without delving into regions of highly high loss. This implies that even when making minor adjustments to the model's parameters within the parameter space, the model's performance can remain relatively stable, mitigating significant performance degradation garipov2018loss. This concept lays the foundation for designing more effective optimization algorithms, enabling models to share knowledge and experiences across different tasks, enhancing both model performance and generalization capabilities. Furthermore, we can define two models as mechanistically similar if they employ the same attributes of inputs for making predictions. Some research has demonstrated that the absence of linear connectivity implies mechanistic dissimilarity, suggesting that simple fine-tuning may not suffice to eliminate spurious attributes learned during the pre-training phase lubana2023mechanistic,juneja2022linear. However, it is promising to address non-linearly connected regions through fine-tuning, thereby effectively modifying the model's mechanisms to resolve the issue of OOD misgeneralization. Connectivity-Based Fine-tuning (CBFT) As discussed above, recent research has suggested that the absence of linear connectivity between two models implies a fundamental mechanistic dissimilarity. lubana2023mechanistic finds that models tend to develop similar inference mechanisms when trained on similar data. This could be a significant reason for the emergence of bias in models, such as relying on the background information of images for classification rather than the objects depicted in the images. If this model mechanism is not adjusted during the finetuning process, the model may rely on these false attributes. To overcome this problem, they propose a valid strategy for altering a model's mechanism, which aims to minimize the following loss: align* L_{CBFT} &= L_{CE}\\big(f\\left(D_{NC} ; \\theta\\right), y\\big)+L_{B}+1{K} L_{I} align* where the original training dataset is denoted as $D$, and we assume that we can obtain a minimal dataset without spurious attribute $C$, denoted as $D_{NC}$. Besides $L_{CE}$ that denotes the cross-entropy loss between model's prediction $f\\left(D_{NC} ; \\theta\\right)$ and the ground truth label $y$, CBFT has two primary objectives: (1) The first objective entails modifying a model's underlying mechanism by repositioning it within the loss landscape, breaking any linear connection with the current minimizer. This is accomplished by maximizing $L_{B}$, referred to as the barrier loss. (2) The second objective involves mitigating reliance on spurious attributes in the original training dataset. This is achieved by optimizing $L_{I}$, enabling the discovery of invariant relationships without the need for $C$. CBFT holds promise for shifting the mechanism from predicting objectives by spurious features to true features, just changing partial parameters of models.",
      "origin_cites_number": 6
    },
    {
      "section_title": "Data Distribution Interventions",
      "level": "2",
      "content": "Besides algorithmic optimization, methods that expand the distribution of training data to include real-world elements can also reduce the discrepancy between training and deployment distributions. In this section, we specifically focus on the introduction of adversarial pressures and multi-agent dynamics.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Adversarial Training",
      "level": "3",
      "content": "AI systems can suffer from a lack of adversarial robustness, meaning that certain inputs designed to make them fail cause the models to perform poorly zheng2016improving, which has been shown in images huang2017adversarial and texts zou2023universal, shah2023scalable, as well as changes to semantic features in images geirhos2018imagenettrained, bhattad2019unrestricted, shamsabadi2020colorfool, casper2022robust and texts jia2017adversarial, and even examples generated entirely from scratch song2018constructing, ren2020generating, ziegler2022adversarial, chen2024content. These failure modes are covered in the red teaming section (\\Ssubsec:red teaming). It's worth noting that in addition to the robustness of AI model policies, the robustness of reward models that govern the training of advanced AI systems is also of importance, as the gradient descent optimization process could be seen as an adversary that may exploit loopholes in the reward model, a phenomenon named reward model overoptimization that has been experimentally demonstrated gao2023scaling. We consider adversarial robustness a case of distribution shift failure caused partly by a mismatch between AI systems' training distribution (where the training inputs are not adversarially constructed) and testing distribution (where the example can be adversarially constructed). The method of adversarial training yoo2021towards,tao2021recent,ziegler2022adversarial mitigates this problem by introducing adversarial examples into training input through a variety of ways tao2021recent, thus expanding the training distribution and closing the distribution discrepancy. Adversarial training, which is similar to adversarial attacks, first started in the settings of image classification madrylabrobustness, but later expanded to a wide range of settings. In addition to vision models, adversarial training algorithms have been proposed for language models wang2019improving,liu2020adversarial,ziegler2022adversarial, vision-language models gan2020large,berg2022prompt, etc. In terms of the model type, adversarial training has been applied to classification models tao2021recent, generative models ziegler2022adversarial, and RL agents pinto2017robust, tan2020robustifying. There are two major types of adversarial training: perturbation-based and unrestricted. itemize[left=0.3cm] \\item Perturbation-based Adversarial Training. Mirroring perturbation-based adversarial attack (see \\Ssubsec:red teaming), perturbation-based adversarial training introduces adversarially perturbated examples (i.e., small changes to a normal data input which are designed to reduce model performance) into training goodfellow2014explaining. Techniques in this vein tao2021recent include the baseline approach of adding a regularization term into the loss function to assess model performance on a gradient-based perturbated input goodfellow2014explaining, unsupervised carmon2019unlabeled or self-supervised hendrycks2019using approaches, and various supplemental techniques such as the introduction of curriculum learning which gradually intensifies adversarial pressure during training. \\item Unrestricted Adversarial Training. Mirroring unrestricted adversarial attack (see \\Ssubsec:red teaming), unrestricted adversarial training generalizes perturbation-based adversarial training to include any adversarial example that can fool the model, not necessarily ones obtained by adding a small amount of noise to another example. This includes generative adversarial training, which uses generative models to produce arbitrary adversarial inputs from scratch poursaeed2021robustness, and the addition of syntactically or semantically modified adversarial examples to training input ziegler2022adversarial,mao2022enhance which surprisingly eliminates the negative effects on the model's non-adversarial performance. Most works on unrestricted adversarial attacks also apply to unrestricted adversarial training (see \\Ssubsec:red teaming for an overview) and form an important part of the unrestricted adversarial training methodology. itemize",
      "origin_cites_number": 30
    },
    {
      "section_title": "Cooperative Training",
      "level": "3",
      "content": "Cooperative AI dafoe2020open,dafoe2021cooperative aims to address uncooperative and collectively harmful behaviors from AI systems (see \\Ssec:challenges-of-alignment). The lack of cooperative capabilities in AI systems can be seen as a form of failure under distribution shift -- systems are trained in single-agent settings that are qualitatively different from the real world, which could be massively multi-agent. This difference is indeed a difference in data distribution since the presence of other agents in the environment qualitatively alters the environmental state transition dynamics, leading to changes in the joint distribution of observations and rewards. We approach the problem by expanding our training distribution to include multi-agent interactions via cooperative training. We introduce the branch of cooperative AI (what we call cooperative training) that focuses on specific forms of Multi-Agent Reinforcement Learning (MARL) training and complements formal game theory approaches in \\Ssubsec:formal-ethics-coop. The MARL branch of cooperative training tends to emphasize the AI system's capabilities for coordination (e.g., coordination of a robot football team ma2022elign), as opposed to incentives of cooperation (e.g., mitigating failure modes like the prisoner's dilemma phelps2023investigating) which are the focus of the game theory branch. Here, we only cover the MARL branch due to its relevance to expanding training data distribution. The field of MARL had traditionally been divided into the three branches of fully cooperative (where all agents share the same reward function), fully competitive (where the underlying rewards constitute a zero-sum game), and mixed-motive settings (where the reward incentives are neither fully cooperative nor fully competitive, corresponding to general-sum games) gronauer2022multi. Among them, fully cooperative and mixed-motive settings are the most relevant for cooperative AI, and the latter has been especially emphasized due to its relative neglectedness dafoe2020open. We also cover other research fronts, including zero-shot coordination hu2020other,treutlein2021new, environment-building leibo2021scalable, and socially realistic settings du2023cooperative. itemize[left=0.3cm] \\item Fully Cooperative MARL. Fully cooperative settings of MARL are characterized by a shared reward function for all agents gronauer2022multi. This unity allows us to completely disregard issues of cooperation incentives (since all incentives are perfectly aligned) and instead focus on effectively achieving the shared goal via coordination. Commonly adopted approaches oroojlooy2023review lie on a spectrum of centrality -- from the baseline solution of purely independent training tan1993multi to the approach of supplementing independent training with decentralized communications foerster2016learning, and then to value factorization which decomposes a global reward and determine each individual agent's contribution guestrin2001multiagent,sunehag2018value. \\item Mixed-Motive MARL. Mixed-motive settings of MARL are characterized by a mixture of cooperative and competitive incentives -- rewards for agents are not identical but aren't zero-sum either gronauer2022multi. This includes game environments where teams play against each other jaderberg2019human and more nuanced settings such as negotiation cruz2019reinforcement,meta2022human. Examples of techniques for mixed-motive MARL, again ordered from decentralized to centralized, include using IRL-like methods to learn from human interactions song2018multi, making communications strategic and selective singh2018learning and adapting actor-critic methods by granting the critic access to global information lowe2017multi. \\item Zero-shot Coordination. Zero-shot coordination is the goal of making AI systems able to coordinate effecively with other agents (including human agents) without requiring being trained together or otherwise being designed specifically to coordinate with those agents hu2020other,treutlein2021new -- human beings who are complete strangers can still cooperate effectively, and we hope that AI systems can do the same. Early works were published under the name ad hoc coordination, covering evaluation stone2010ad, game-theoretic and statistical approaches albrecht2013game, and human modeling krafft2016modeling. Recent advances include other-play hu2020other which randomizes certain aspects of training partners' policies to achieve robustness,This is in a similar spirit to \\emph{domain randomization tobin2017domain.} the introduction of multi-level recursive reasoning cui2021k, and off-belief learning hu2021off which eliminates arbitrary conventions in self-play by interpreting partners' past actions as taken by a non-collusive policy. \\item Environment-building. Game environments have been popular settings for cooperative training, including, for example, Hanabi muglich2022equivariant, Diplomacy cruz2019reinforcement,meta2022human, and football ma2022elign. On the more simplistic end, game theory models, especially those based on classical multi-agent dilemmas, have also been a popular choice of environment wang2021emergent,christoffersen2023get. Also, Melting Pot leibo2021scalable, a framework and suite of multi-agent environments, has been designed specifically for cooperative AI research. There has also been research on unsupervised environment design, which aims for a partial automation of the environment-building process dennis2020emergent,jiang2021replay. \\item Socially Realistic Settings. It has been proposed that cooperative AI research should focus more on socially realistic environments du2023cooperative, which tend to be massively multi-agent (including both AI agents and human agents) and are highly diverse in both the composition of agents and modes of interactions. Implications of this vision critch2020ai include, but aren't limited to, building more realistic and open-ended environments klugl2005role,lehman2008exploiting,wang2019poet,suo2021trafficsim, scaling up MARL sun2020scaling,du2023cooperative, and incorporating new means of control such as social institutions and norms singh2014norms. itemize",
      "origin_cites_number": 38
    },
    {
      "section_title": "Assurance",
      "level": "1",
      "content": "Assurance refers to the measurement and refinement of AI systems' practical alignment after AI systems are actually trained or deployed batarseh2021survey. In this section, we categorize assurance into three parts based on a certain logic: Safety Evaluations -- Evaluating AI systems on minimizing accidents during task execution as a basic need of assurance, Interpretability -- Ensuring that humans can understand the decision-making process of AI systems and therefore assuring the safety and interoperability beyond evaluation, Human Value Verification -- Verifying whether AI systems can align with human values, ethics, and social norms and satisfying the high-level need of AI systems' integration to the human society, as is described in the Figure fig:assurance. In addition to methods that aim to determine if AI systems are safe and aligned, there are also assurance methods that actively intervene in the AI system or its deployment process to ensure such properties. Machine Unlearning Datasets for model pretraining contain various types of undesirable and potentially dangerous content, including but not limited to information about bioweapons and cyberattack hendrycks2021unsolved. The field of machine unlearning has aimed to remove such knowledge after a model is trained bourtoule2021machine. Compared to direct filtering of the training dataset, this approach faces more technical challenges, but it retains more flexibility in deployment and also allows categorical removal of a given piece of information eldan2023s. Dataset filtering and unlearning ought to be seens as complementary approaches that work best together. Controlling Unaligned Systems While complete alignment may be difficult, it is still possible to safely utilize unaligned models if their extent of misalignment is limited and if we have access to supervisor AI systems. Algorithmic procedures have been developed to minimize probabilities of failure when given trusted and untrusted systems with differing capabilities greenblatt2023ai. In general, alignment-focused process engineering of deployment procedures could be a valuable direction to explore. 0.6em In addition, a class of methods, termed provable safety, aim to combine evaluation (\\Ssec:safe), interpretability (\\Ssec:interpretability), and other assurance methods under a unified framework that quantifies risks of AI safety violation. Provable Safety Provable safety aims to provide formally-grounded probabilistic guarantees on the safety of AI system, using input from evaluation tools, interpretability tools, and other assurance techniques; furthermore, it hopes to build development-deployment pipelines that satisfy such probabilistic guarantees tegmark2023provably,dalrymple2024towards. Research around provable AI safety is still at an early stage, and significant uncertainties remain about its specifics. dalrymple2024safeguarded have made the case that three key subproblems currently exist within provable AI safety: scaffolding, i.e., formalizing the definition of real-world AI safety, by providing tools for domain experts to build safety specifications; machine learning, i.e., using ML methods to find control policies that satisfy the safety specifications; and applications, i.e., demonstrating the practical superiority of AI systems with safety gurantees other traditional ones. 0.6em We then go on two review the three categories of alignment assurance efforts.",
      "origin_cites_number": 7
    },
    {
      "section_title": "Safety Evaluations",
      "level": "2",
      "content": "Safety refers to mitigating accidents caused by design flaws in AI systems and preventing harmful events that deviate from the intended design purpose of the AI system amodei2016concrete. In fact, safety stands as a shared requirement across all engineering domains verma2010reliability. Moreover, it holds particular importance in constructing AI systems, because of the characteristics of AI systems jacob2015longterm. We categorize the safety of AI systems into the following categories: Social Concerns refer to explicit and comparatively identifiable characteristics of safe AI systems, including aspects such as toxicity stahl2022assessing, and Intentional Behaviors share the characterization of relatively complicated investigation and substantial potential harm, represented by power-seeking, deception, and other frontier AI risks shevlane2023model. Following the logic above, we start with the techniques to form datasets and benchmarks of safety evaluation in \\Ssubsec: dataset&benchmark and further explore the evaluation targets and their characteristics in \\Ssubsec: evaltarget. At the end of this section, we include the red-teaming technique \\Ssubsec:red teaming, which assesses the AI system's robustness beyond evaluation.",
      "origin_cites_number": 12
    },
    {
      "section_title": "Datasets and Benchmarks",
      "level": "3",
      "content": "In the discussions on safety evaluation, it is crucial to prioritize datasets and benchmarks as the cornerstone elements, so we first introduce the basic techniques to build datasets and benchmarks and then move on to newer interactive methods. Dataset Among all the assurance techniques, the dataset method could be considered the most elementary and straightforward one celikyilmaz2020evaluation. This method assesses the response of AI systems by presenting them with predefined contexts and tasks paullada2021data, balancing the cost, quality, and quantity of data. Research on the dataset method encompasses data sources, annotation approaches, and evaluation metrics. Given that evaluation metrics can vary based on its subject sai2022survey, this section primarily emphasizes dataset sources and annotation methods. itemize[left=0.3cm] \\item Expert design. In the early stage of a domain, expert design is widely used in building datasets, where experts create samples based on actual needs to ensure the dataset covers a wide range of potentially dangerous situations to form datasets roh2019survey. For instance, initial-stage datasets, e.g., WEAT bolukbasi2016man and BBQ parrish2022bbq for bias detection used expert design to harvest a wide coverage and high accuracy while sharing the limitations in terms of cost and breadth, leading to the later development of more efficient methods. \\item Internet collection. Previous expert design methods have the flaw of rather high cost and lower efficiency, and internet collection can obtain datasets that contain actual user-generated textual content on a rather large scale (therefore convenient for both training and testing), reflecting real-world text generation scenarios yuen2011survey, but the raw data collected also needs careful selection and annotation roh2019survey. Well-known instances of these datasets include OLID zampieri2019predicting and SOLID rosenthal2021solid gathering original Twitter texts for toxicity assessment, WinoBias zhao2018gender and CrowS-Pairs nangia2020crows gather content potentially containing bias from the internet for further annotation. However, it's important to acknowledge that, as is also mentioned in papernot2016towards, internet-collected datasets naturally carry risks such as privacy and safety concerns, so additional processing is necessary. \\item AI Generation. The concept of autonomously generating datasets was explored relatively early, even before the emergence of elementary forms of LLMs weston2015towards. However, during this early stage, AI-generated datasets were limited by the capabilities of AI systems, so their quality was not as good as internet-collected and manually annotated datasets. It wasn't until LLMs reached relatively high levels of proficiency in logical reasoning context understanding and approached or surpassed human-level performance openai2023gpt4 that LMs gained the ability to mimic the structure and logic of existing datasets to compose new ones. As is shown in papers such as zhang2022constructing and perez2022discovering, AI systems have made progress in generating datasets for evaluation purposes, surpassing the quality of some classical datasets. However, according to these papers, this approach still faces limitations rooted in the capabilities of large models themselves, including issues like instruction misunderstanding and example diversity, which require further refinement. itemize Interactive Methods Due to the static nature of datasets, they possess relatively fixed evaluation content and can be vulnerable to targeted training holtzman2019curious. Additionally, the evaluation content may not fully reflect the strengths and weaknesses of corresponding capabilities engstrom2020identifying. As the demands for language model evaluation continue to escalate, new interactive assurance methods have emerged, which can be categorized into two groups: Agent as Supervisor and Environment Interaction. itemize[left=0.3cm] \\item Agent as Supervisor. It is an assurance method that involves using an agent to assess the outputs of AI models. This evaluation approach is characterized by its dynamism and flexibility. Typically, there is a predefined framework for interaction between the agent and the AI system under evaluation cabrera2023zeno. In this method, the agent can be a human participant engaged in experiments through an online system stiennon2020learning, a more advanced language model evaluating relatively less capable language models through multi-turn interactions lin2023llm, or in the context of Scalable Oversight, a less powerful but more trustworthy model greenblatt2023ai. This evaluation form offers advantages such as automation and lower cost compared to human agents. \\item Environment Interaction. It aims to create a relatively realistic environment using elements such as humans and other LLMs to assess the alignment quality of AI models through multiple rounds of interaction liu2024agentbench. One method is using peer discussions, where multiple LLMs engage in dialogue, to enhance evaluations of AI systems, particularly when their capabilities are relatively close to each other. Moreover, by building a world model li2022emergent, the generalization and exploration abilities of AI systems can be comprehensively evaluated. itemize",
      "origin_cites_number": 25
    },
    {
      "section_title": "Evaluation Targets",
      "level": "3",
      "content": "To achieve the goal of safety alignment, the assurance of AI systems can be divided into different small targets shevlane2023model. The subsequent section gives an introduction to these subjects and, furthermore, discusses some of the domain-specific analyses of assurance methods within these realms, while the table tab:safety-eval-example will show examples of alignment assurance works in these domains. Toxicity It refers to content in the output of AI systems that is unhelpful or harmful to humans sheth2022defining. Before the advent of advanced language models, early toxicity evaluation primarily focused on detecting toxic language and identifying harmful statements in an internet context, like the WCC wulczyn2017ex, which collected and manually labeled comments from Wikipedia discussion pages. With the emergence of pretrained language models, assurance against toxicity adopted a prompt-generation paradigm to assess the risk of language models generating toxic content in response to specific prompts gehman2020realtoxicityprompts, ganguli2022red, openai2023gpt4. However, in crowdsourced environments, annotation scores may vary by person, so relative labeling, where crowdsource workers select from two different answers during a chat, is needed to enhance crowdsource quality ~bai2022training. Furthermore, subsequent datasets ganguli2022red, ji2024beavertails employ a red teaming design pattern that induces toxic responses through adversarial inputs, further strengthening the assurance of model robustness. The SafeSora dataset dai2024safesora is the first text-to-video preference dataset designed to align large vision models (LVMs) with human values, focusing on helpfulness and harmlessness. Power-seeking It is a kind of risk that AI systems may seek power over humans once they possess certain levels of intelligence turner2021optimal. In carlsmith2022power, the authors point out that AI systems already have the conditions for power-seeking, including advanced capabilities, agentic planning, and strategic awareness. However, the assurance against power-seeking is still in its early stages. One representative work in this area is the Machiavelli pan2023machiavelli, which constructs a benchmark consisting of decision-making games to assess whether AI systems can balance competition with moral ethics during the game. The conclusion of this work suggests that AI systems still struggle to balance achieving rewards with behaving morally, thus further research in this field is needed. Deceptive Alignment When the AI system is situationally aware, it may recognize that getting high rewards can preserve themselves by preventing significant gradient descent, therefore preserving its original goal hubinger2019risks, kenton2021alignment, ngo2024the. This process is called Deceptive Alignment. In the current context, deceptive alignment is already achievable, as is proved by hubinger2024sleeper. Directly evaluating the deceptive alignment is difficult, for the pronoun deceptive alignment is naturally against the traditional train-evaluation loop. Thus, deceptive alignment might be discovered by indirect methods such as interpreting model parameters (see sec:interpretability), or representation engineering zou2023representation. Moreover, deceptive alignment is closely related to situational awareness, i.e., AI systems with a certain degree of prediction and understanding of the states and developments of entities in their working environment to make corresponding decisions. In li2022emergent, the authors evaluate the performance of language models in the board game Othello, showing that language models have the ability to predict possible future states within the action space in a nonlinear representation. Hallucination AI systems may generate information or responses that are not grounded in factual knowledge or data, leading to the creation of misleading or false content, which is formally called Hallucination ji2023survey. Hallucination evaluation aims to assure the consistency of the knowledge in the AI system's output with the knowledge given by its training data and knowledge base ji2023survey, zhang2023siren. The earliest statistical-based hallucination evaluation methods used n-grams to directly calculate the overlap of vocabulary between the input and output content ~dhingra2019handling, wang2020towards. However, this type of evaluation has a limitation: It only considers lexical overlap and does not take into account semantics or sentence meaning ji2023survey, making it unsuitable for evaluating more complex forms of hallucination. Later assurance methods shifted from statistical approaches to model-based methods, which are more robust compared to statistical token-difference-based methods honovich2021q2. While this evaluation method is more advanced than previous ones, it still has the limitation that the model can only output the degree of hallucination and may have difficulty pinpointing specific errors falke2019ranking. hu2024do designed the benchmark Pinocchio, which investigates whether LLMs can integrate multiple facts, update factual knowledge over time, and withstand adversarial examples. This benchmark represents an in-depth investigation into the factual knowledge bottleneck under the issue of hallucination. Frontier AI Risks In addition to the assurance content described above, the enhancement of AI systems in recent years has given rise to a series of new assurance needs openai2023gpt4. Currently, there is not much public information available for research on these assurance needs, hence this section will provide a brief introduction to some of the more significant ones: itemize[left=0.3cm] \\item Cyber Security \\& Biological Weapons. Advanced LLMs may be misused for cyber-attacks, the production of bio-weapons, and other extremely harmful behaviors shevlane2023model. Although GPT-4 cannot play a significant role in exploiting network vulnerabilities due to its limited context window, it has been proven to demonstrate strong capabilities in identifying network vulnerabilities and in social engineering openai2023gpt4. Similarly, lentzos2022ai have stated the robust abilities of AI systems in the field of bio-weapons and the military, highlighting the risks of misuse of such capabilities. It emphasizes the necessity to ensure that these models can identify and reject malicious requests. \\item Deception \\& Manipulation. AI systems have the potential to negatively influence users by outputting text, including disseminating false information, syncopating humans, and shaping people's beliefs and political impacts shevlane2023model,sharma2024towards. Distinguished from hallucination, the misinformation here is not a flaw of the model itself but rather a deliberate action. Special assurance measures need to be designed for controlling these kinds of behavior. \\item Jailbreak. It refers to the bypassing of AI systems' safeguard mechanisms by users, for example, by constructing specific types of input. This behavior can be limited to text openai2023gpt4, deng2023jailbreaker, huang2024catastrophic, yong2023low,Relevant discussions in \\citet{openai2023gpt4 can be found in its system card appendix. } or it may take multi-modal forms openai2023gpt4V. Specifically, multi-modal jailbreaks make traditional text-based heuristic methods for identifying attack content infeasible, necessitating special multi-modal handling methods. Further discussion of jailbreak can be found in \\Ssubsec:red teaming. \\item Self-Preservation \\& Proliferation. This refers to the tendency of AI systems for self-protection and replication, and in this process, breaking the limit from their environment. These tendencies are examples of instrumental sub-goals bostrom2012superintelligent. While this tendency can be beneficially harnessed, it is dangerous in the absence of regulation perez2022discovering. This tendency has been emphasized and evaluated by various sources perez2022discovering,kinniment2023evaluating,openai2023gpt4,openai2023gpt4V.\\ref{footnote-gpt4} itemize",
      "origin_cites_number": 57
    },
    {
      "section_title": "Red Teaming",
      "level": "3",
      "content": "Red teaming is the act of generating scenarios where AI systems are induced to give unaligned outputs or actions (e.g., dangerous behaviors such as deception or power-seeking, and other problems such as toxic or biased outputs) and testing the systems in these scenarios. The aim is to assess the robustness of a system's alignment by applying adversarial pressures, i.e. specifically trying to make the system fail. In general, state-of-the-art systems -- including language models and vision models -- do not pass this test perez2022red,zou2023universal,liu2023jailbreaking,chen2024content. In game theory and other fields, red teaming was introduced much earlier, and within computer science, the concept of red teaming was proposed in the security field, where it had a similar meaning of adversarially assessing the reliability and robustness of the system. Later, ganguli2022red,perez2022red introduced this idea to the field of AI, and more specifically, alignment. The motivation for red teaming is two-fold: (1) to gain assurance on the trained system's alignment, and (2) to provide a source of adversarial input for adversarial training yoo2021towards,tao2021recent, ziegler2022adversarial, probing models kalin2020black, and further utilities. Here, we focus on the first. It's worth noting that the two objectives aren't separable; works targeting the first motivation also help provide a basis for the second. Reinforced, Optimized, Guided, or Reverse Context Generation This category includes using various methods to generate coherent contexts (prompts) that are inducive to unaligned completions from the language model. perez2022red, deng2022rlprompt, casper2023explore train or tune a separate language model with RL to make it generate desired prompts, which are then fed to the red-teamed model. perez2022red, si2022so also uses other methods such as zero-shot, few-shot, or supervised finetuning-based generation. lee2022query, jones2023automatically generates misalignment-inducive contexts by performing optimization on the prompt -- bayesian optimization and discrete optimization, respectively. dathathri2019plug,krause2021gedi propose the method of guiding an LLM's generation using a smaller classifier; this is proposed in detoxification but is transferable to the red teaming context. Lastly, zhang2022constructing generates misalignment-inducive contexts through reverse generation, i.e. constructing adversarial contexts conditioned on a given response, which can be seen as an inverse process for model inference. Manual and Automatic Jailbreaking As is defined above subsec: evaltarget, Jailbreaking shen2023anything is an informal term that refers to the act of bypassing a product's constraints on users -- and in the case of LLMs, bypassing LLMs' tendencies to not answer misalignment-inducive questions, a feat of alignment training. Most existing attempts are scattered across the Internet in the form of informal reports and involve adding prefixes and suffixes to the original text zou2023universal. Research has descriptively analyzed the existing attempts liu2023jailbreaking,shen2023anything, deng2023jailbreaker, huang2024catastrophic, as well as providing causal explanations for the phenomenon wei2023jailbroken. In addition, past wallace2019universal and current zou2023universal, shah2023scalable works have proposed effective methods to automatically generate such prompts, prefixes, or suffixes that nullify LLMs' tendencies to avoid misalignment-inducive questions. Crowdsourced Adversarial Inputs Several works xu2020recipes, xu2021bot, ganguli2022red have produced misalignment-inducive prompts by crowdsourcing, i.e. recruiting human red teamers (possibly via online platforms) and instruct them to provide adversarial prompts. Besides, companies in the AI industry also build mechanisms to collect adversarial inputs, i.e. the red teaming network of OpenAI\\url{https://openai.com/blog/red-teaming-network} and the bug hunter program of Google\\url{https://bughunters.google.com/about/rules/6625378258649088}. These methods (arguably) provide more flexibility and resemblance to real-world use cases but have higher costs and lower scalability. Perturbation-Based Adversarial Attack In the field of computer vision, there have been many works studying adversarial attacks on vision models that rest on the method of perturbation, i.e., performing small perturbations to the pixel contexts of the image (usually bounded by a pixel-wise matrix norm) to make the model confidently produce false outputs on the perturbated image chakraborty2021survey. This type of adversarial attack has also been extended to language models jia2017adversarial,ebrahimi2018hotflip,zang2020word,cheng2020seq2sick and vision-language models zhao2024evaluating. Unrestricted Adversarial Attack Unrestricted adversarial attack, proposed in song2018constructing, is a more general form of adversarial attack. It removes all restrictions on the adversarial examples, and therefore, for instance, the adversarial example can be generated from scratch, as opposed to being generated from an existing example, as in the case of perturbation-based methods. Many methods for unrestricted adversarial attack have been proposed; the most notable ones include song2018constructing,chen2024content which generate realistic adversarial images using generative models, and bhattad2019unrestricted,shamsabadi2020colorfool which manipulates semantically meaningful traits such as color and texture. Unrestricted adversarial attack has also been extended to text classification models ren2020generating. Datasets for Red Teaming A number of works on red teaming and related topics have compiled datasets consisting of red teaming prompts or dialogues, including the IMAGENET-A and IMAGENET-O dataset hendrycks2021natural, the BAD dataset xu2020recipes, the red teaming section of HH-RLHF dataset bai2022training, and the Real Toxicity Prompts dataset gehman2020realtoxicityprompts. Existing Red Teaming Practices in Industry The practice of red teaming is gaining popularity in the AI industry. Cases of adoption include OpenAI (who performed red teaming on its system GPT-4 to produce part of its System Card) openai2023gpt4, NVIDIA nvidia_ai_red_team, Google google_ai_red_team, and Microsoft microsoft_ai_red_team. During an event at the DEF CON 31 conference, models from 9 companies undergo red teaming from the conference participants;\\url{https://www.airedteam.org/} this red teaming event is held in partnership with four institutions from the U.S. public sector, including the White House. To address the vulnerabilities of LLMs to prompt injections and similar attacks, OpenAI proposes an instruction hierarchy that prioritizes trusted instructions, enhancing model security against both known and new attack types while maintaining general performance with minimal impact wallace2024instruction. Downstream Applications Red teaming plays a crucial role in the adversarial training of AI systems by providing adversarial input yoo2021towards,tao2021recent,ziegler2022adversarial. In addition, adversarial examples produced from red teaming can also be used to interpret models casper2022robust.",
      "origin_cites_number": 34
    },
    {
      "section_title": "Safetywashing",
      "level": "3",
      "content": "The work by ren2024safetywashing focuses on a phenomenon called Safetywashing. Safetywashing refers to safety labs or companies claiming to improve the safety of their models by reporting progress on benchmarks that measure model safety, which are strongly correlated with model performance. As a result, along with the increase in the models capabilities, the model's safety according to benchmarks prone to Safetywashing also appears to increase. Safetywashing, and particularly such benchmarks, create a false perception of safety progress in newer models and fail to address the actual safety issues, which should be orthogonal to improvements in the model's performance. To determine whether a benchmark correlates with a model's capabilities and can therefore be used for Safetywashing, the authors first propose calculating capability scores. These are calculated by constructing a matrix $B \\in R^{m \\times b}$ for $m$ models and their respective performance on $b$ benchmarks, where columns are normalized to have zero mean and a variance of 1. Then, the first principal component $PC_1$ is extracted, and all performances are projected onto $PC_1$. This gives a general measure of a model's capabilities: equation* Capabilities Score_i = (B \\cdot PC_1)_i \\quad for \\, i = 1, \\ldots, m. equation* To measure the correlation of a safety benchmark with model capabilities, a set of $m$ models is evaluated on safety benchmarks (adjusted such that higher scores indicate higher safety, with variance 1 and mean 0). The capabilities correlation is then the Spearman correlation across models between the capability scores and the safety benchmark scores: equation* Capabilities Correlation = corr_{models} (Capabilities Score, Safety Benchmark). equation* Following this methodology, the authors evaluate the capabilities correlation of current safety benchmarks across different AI safety domains. Their results show that, with the exception of weaponization capabilities and measurements of sycophancy, safety benchmarks tend to have positive capabilities correlation. Machine ethics, dynamic adversarial robustness, and calibration benchmarks exhibit low correlation with model capabilities, while current alignment, truthfulness, static adversarial robustness, and scalable oversight benchmarks primarily reflect model capabilities and can thus be used for Safetywashing. The authors propose that future safety evaluations should report capabilities correlation, and the AI safety research community should aim to design benchmarks that are decorrelated from model capabilities. Furthermore, research labs and companies should not claim improved safety by improving performance on benchmarks with high capabilities correlation.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Interpretability",
      "level": "2",
      "content": "Interpretability is a research field that makes machine learning systems and their decision-making process understandable to human beings doshi2017towards, zhang2018visual, miller2019explanation. Interpretability research builds a toolbox with which something novel about the models can be better described or predicted. In this paper, we focus on research that is most relevant to alignment and safety,For a more comprehensive review of interpretability and its methods, we recommend \\citep{rauker2023toward.} and empirically, those techniques make neural networks safer by studying the internal structures and representations of the neural networks rauker2023toward. Interpretability is an important research direction because in principle gaining safety guarantees about white-box systems is easier than black-box ones. The taxonomy of interpretability tools varies according to sub-fields and purposes doshi2017towards,rudin2019stop. There are several ways to break down interpretability research: % explainability and transparency; interpretability of weights, neurons, sub-networks, or representations; for safety or the science of deep learning; intrinsic and post hoc interpretability; mechanistic interpretability and concept-based interpretability. itemize[left=0.3cm] \\item Explainability and Transparency. Explainability research aims to understand why models generate specific output, whereas transparency aims to understand model internals critch2020ai. \\item Safety or the Science of Deep Learning. Researchers also conduct interpretability research with different purposes: some do it to safely deploy AI systems, while others aim for a complete science of neural network. But the line gets blurred as mechanistic interpretability research aims for both olah2020zoom, olah_dreams_2023. \\item Intrinsic and Post Hoc Interpretability. By the stage of intervention, interpretability research is divided into intrinsic interpretability and post hoc interpretability carvalho2019machine: the former focuses on making intrinsically interpretable models, while the latter designs post hoc interpretability methods that offer explanations to model behaviors. \\item Top-down Approach and Bottom-up Approach Interpretability research can also be categorized based on the direction of analysis: top-down or bottom-up approaches. The top-down approach starts with high-level behaviors or concepts and investigates how these are represented within the neural networks architecture. This method involves observing and manipulating high-level, macro neural representations as cognitive phenomena in models zou2023representation. In contrast, the bottom-up approach begins with low-level components such as neurons, weights, or circuits, aiming to build an understanding of the networks function by dissecting these basic elements and their interactions. For example, Mechanistic Interpretability exemplifies the bottom-up approach by seeking to reverse-engineer the computations of neural networks to gain a detailed understanding of their internal mechanisms olah2020zoom,olsson2022context,rauker2023toward, whereas Concept-based Interpretability locates learned knowledge in the neural networks meng2022locating, meng2022mass. itemize In this section, we adopt the Intrinsic and Post Hoc Interpretability classification method, for it offers a more generic framework suitable for various AI systems beyond neural network, and it divides the interpretability analysis both during the system designing and after the system has been deployed rauker2023toward, compared to other classification methods. Specifically, we discussed mechanistic interpretability techniques that take place in model designing and post hoc stages separately in post hoc and intrinsic interpretability subsections.",
      "origin_cites_number": 23
    },
    {
      "section_title": "Intrinsic Interpretability",
      "level": "3",
      "content": "Researchers make deep learning models intrinsically more understandable, which is usually called intrinsic interpretability carvalho2019machine. In contrast to the symbolic approach, which emphasizes the creation of interpretable models, the modern deep learning approach tends to yield models with enhanced capabilities but potentially reduced interpretability. Compared to interpreting the black-box models, designing models that are intrinsically interpretable is safer and more efficient rudin2019stop. To make intrinsically interpretable models, the research community designs modular architecture, which is robust to adversarial attacks and free of superposition anthropic_softmax_2022,rauker2023toward. Notably, mechanistic interpretability, often regarded as a set of post hoc interpretability techniques, arguably facilitates the process of making more interpretable models. Modifying Model Components Model components, such as feedforward layers, are hard to interpret (i.e., it's hard to articulate their behavior in human-understandable terms) because those layers have many polysemantic neurons that respond to unrelated inputs du2019techniques. Thus, there are certain modifications applied to these back-box components and their related structures to make reverse engineering easier, and thus improve their interpretability carvalho2019machine. There are a number of existing works to encourage interpretable results by modifying loss functions ross2017neural, adding a special interpretable filter or embedding space zhang2018interpretable, wang2021interpretable, using dynamic weight depending on the input foerster2017input, and modifying intermediate layers li2022interpretable. Specifically, lage2018human proposed a human-in-the-loop algorithm that directly utilizes human feedback to quantify the subjective concept, thus achieving more reliable results. In transformer models, Anthropic proposes SoLU to replace the activation function, increasing the number of interpretable neurons and making reverse engineering easier while preserving performance anthropic_softmax_2022. This is still an early exploration as a potentially important line of work, and challenges remain, such as the scalability of this method anthropic_softmax_2022. Reengineering Model Architecture Modifying existing model components is beneficial to reverse engineering carvalho2019machine, foerster2017input, but they cannot make models fully understandable, so some researchers started to reengineer model architecture to build theoretically interpretable models carvalho2019machine, mascharka2018transparency. Notably, it is generally believed that there exists a trade-off between model interpretability and its performance in the same model complexity alvarez2018towards, so it becomes crucial to design models that reach a balance between these two elements, or moreover, close the gap between interpretable models and state-of-the-art models alvarez2018towards, carvalho2019machine, fan2021interpretability, espinosa2022concept. We will discuss the detailed research efforts below: itemize \\item Creating Transparent Reasoning Steps In reasoning models, creating transparent minor steps is crucial to make the model interpretable arad2018compositional, and a number of papers accomplished it by introducing the MAC (Memory, Attention, and Composition) cell to separate memory and control arad2018compositional, by utilizing other attention-based methods lin2019kagnet, arik2021tabnet, and by decomposing the complex reasoning process mascharka2018transparency. These methods significantly improved the interpretability of the reasoning process but at the cost of model complexity and performance, though they close the gap of performance between interpretable and state-of-the-art models mascharka2018transparency. \\item Distilling Complex Knowledge Complex models, such as deep neural networks, often have high performance but lack transparency in their decision-making processes, making them difficult to interpret li2020tnt. Knowledge distillation addresses this challenge by transferring knowledge from these complex, 'black-box' models (teachers) to simpler, more interpretable models (students). By introducing this structure into model design, student models can approximate the performance of the teachers while offering greater transparency, thus enhancing interpretability without sacrificing the capabilities of advanced machine learning models zhang2020distilling, li2020tnt. However, this interpretability is partial, especially in intricate missions, where the distilled knowledge may still be hard to interpret sachdeva2023data. itemize Moreover, the pronoun Self-Explaining Models, which can provide both prediction and explanation elton2020self, was suggested by a number of papers as a better substitution to Interpretable Models, with many papers working on it alvarez2018towards, rajagopal2021selfexplain. For language models, the chain-of-thought (CoT) generation wei2022chain may be recognized as a kind of self-explanation method.",
      "origin_cites_number": 27
    },
    {
      "section_title": "Post Hoc Interpretability",
      "level": "3",
      "content": "This section explores techniques and methods applied to understand model internals after the models are trained and deployed, thus these techniques are often referred to as post hoc interpretability rauker2023toward. The goal is to understand the low-level structure and units of black-box neural networks and their causal effect on macroscopic behaviors and outputs. Dictionary Learning A key challenge of post hoc interpretability is superposition, i.e., the tendency of neurons to encode more than one human-interpretable feature simultaneously, which makes it very difficult to identify the individual features elhage2022toy. To address this challenge, dictionary learning methods have gained attention as they aim to extract sparse, interpretable features from superposed activations lin2019sparse. Early exploration in this area assumed a linear superposition of learned factors in the textual embedding of transformers yun2021transformer. Recently, sparse autoencoders (SAEs) have received significant research attention bricken2023towards,cunningham2023sparse. This method trains autoencoders in an unsupervised manner to extract features that can best replace the original activations while maintaining sparsity, thus performing a form of dictionary learning. SAEs have displayed strong scalability to both base model size and autoencoder size. This enables them to be widely applied in some of the largest frontier LLMs bricken2023towards, templeton2024scaling,gao2024scaling,deepmind2024gemma. Circuit Analysis Circuits refer to the sub-networks within neural networks that can be assigned particular functionalities. As their counterparts in neuroscience, the neural circuits which are both anatomical and functional entities neuralcircuits, circuits are also both physical and functional olah2020zoom. Mechanistic interpretability researchers locate circuits in neural networks (microscopic) to understand model behaviors (macroscopic). Multiple circuits have been reported: curve circuits for curve detectors openai_curve, induction circuits for in-context learning olsson2022context, indirect object identification circuits for identifying objects in sentences wang2022interpretability, Python docstrings for predicting repeated argument names in docstrings of Python functions heimersheim_circuit_2023, grokking nanda2022progress, multi-digit addition nanda2022progress, and mathematical ability such as greater than hanna2024does. Notably, many circuit analysis conducted to date has been focused on toy models and toy tasks rauker2023toward. The largest attempt to reverse engineer the natural behaviors of language models is finding the indirect object identification circuit, which is located in GPT-2 Small and has 28 heads wang2022interpretability. Probing Probing is a collection of techniques that train independent classifiers on the interested internal learned representations to extract concepts/features. One example is Gurnee et al used probing to study the linear representations of space and time in hidden layers. gurnee2023language Although probing has been favored by researchers to understand hidden layers alain2017understanding, it has limitations. For one, probing does help to understand learned representations in hidden layers, but it does not tell whether learned representations are used by models to produce predictions ravichander2021probing, belinkov2022probing; for another, the issues of datasets may confound the issues with the model belinkov2022probing. In the context of safety and alignment, training probe requires the dataset to contain concepts/features of interest, which means probing can not be used to detect out-of-distribution features (i.e. features you suspect learned by the models but you don't have a dataset for them). Notably, representation engineering, built upon probing literature, is introduced to detect high-level cognitive phenomena and dangerous capabilities, including morality, emotion, lying, and power-seeking behaviors. zou2023representation. Model Attribution Attribution is a series of techniques that look at the contribution of some components (including head, neuron, layers, and inputs) for neuron responses and model outputs rauker2023toward. Gradient-based attribution is introduced to evaluate the quality of interpretation and guide the search for facts learned by the models ancona2018towards, durrani2020analyzing, lundstrom2022rigorous, dai2022knowledge. However, those methods are limited because they can not provide causal explanations rauker2023toward. Direct Logit Attribution is to identify the direct contribution of individual neurons to the prediction of the next neurons lieberum2023does, mcgrath2023hydra, belrose2023eliciting, dar2023analyzing. But attribution methods also suffer from a salient constraint: they can only help with scenarios where you have datasets for features of interest. Consequently, such attribution methods cannot help with understanding out-of-distribution (OOD) features (including some misalignment scenarios) casper2023red. Data attribution Identifying the subset of training data that leads to a certain behavior can provide insight into both the safety of said behavior and ways to encourage or prevent that behavior. The method of influence function koh2017understanding,grosse2023studying have been proposed to perform such attribution by approximating the result of leave-one-out training. Visualization Techniques of visualization help to understand neural structures, including techniques that visualize datasets (notably dimensionality reduction techniques) van2008visualizing, olah2014mnist, olah2015visual, features erhan2009visualizing, olah2017feature, weights, activations carter2019activation, structure reif2019visualizing, and the whole neural networks simonyan2013deep, zeiler2014visualizing,nguyen2015deep, karpathy2015visualizing, mordvintsev2015inceptionism, nguyen2016multifaceted,kindermans2018learning. The purpose of visualization is to see neural networks with a new level of detail olah2020zoom. Perturbation and Ablation These techniques are designed to test the counterfactual rather than the correlation rauker2023toward. Perturbation is a technique that modifies the input of models and observes changes in their outputs, and the ablation techniques knock out parts of neural networksNeurons \\citep{zhou2018revisiting and Subspace morcos2018importance, ravfogel2022linear}, helping to establish a causal relationship between neural activation and the behavior of the whole network rauker2023toward. Patching Patching refers to the collection of methods replacing key components (paths and activations) and understanding counterfactual effects on model outputs. Among them, activations patching is a popular method among the safety community. Through applying activation patching and conducting both correct run and corrupted runs on the same neural network, researchers aim to locate key activations that matter more to the model output attribution2023nanda. In reality, patching is used to map and edit learning representations/concepts. Specific patching techniques include interpreting token representations in transformers li2021implicit, bansal2021revisiting,geva2021transformer, geva2022lm, power2022grokking, olsson2022context and how do fully-connected layers learn these representations geva2021transformer, olsson2022context, studying the key-query products to understand how do tokens attend to each other bahdanau2014neural, lee2017interactive, liu2018visual, strobelt2018s, clark2019does, vashishth2019attention, vig2019multiscale, hao2021self, chefer2021transformer, rigotti2022attentionbased, identifying meaningful learned concepts from directions in latent space (from concepts to directions fong2018net2vec, kim2018interpretability, and from directions to post hoc explanations schneider2021explaining). For the purposes of safety and alignment, these techniques notably help to detect deception burns2022discovering.",
      "origin_cites_number": 45
    },
    {
      "section_title": "Outlook",
      "level": "3",
      "content": "Superposition makes the analysis at neuron level implausible Superposition refers to the phenomenon that models represent more features than they have dimensions, so features would not correspond to neurons arora2018linear, olah2020zoom, elhage2022toy. Superposition makes it hard to ensure AI safety by enumerating all features in a model elhage2022toy, nanda2023othello. elhage2022toy proposes three methods to solve superposition: creating models with no superposition (addressing it at training time), finding an overcomplete basis describing how features are stored in the neural nets (addressing it after the fact), or a mixture of both approaches. Notably, bricken2023towards builds a sparse auto-encoder to interpret group neurons, rather than individual neurons to extract features, which points out a promising direction to solve superposition: to move past it.see \\citet{elhage2022toy for details on conceptual and empirical research questions about superposition} Scalability As is mentioned in the previous sections, there exists a trade-off between model interpretability and its capability alvarez2018towards, so interpreting real models while maintaining their performance will be harder than applying those techniques to toy models. Thus, scalability becomes a concern when interpretability researchers take a bottom-up approach to interpretability (mechanistic interpretability), as top-down methods such as attention mechanism arad2018compositional would not face such a bottleneck. For mechanistic interpretability research, we either want to scale up techniques (e.g., applying circuit analysis on real model wang2022interpretability), or we want to scale up analysis (e.g., finding larger structure in neural networks olah_dreams_2023). In the end, we want the microscopic analysis to answer the macroscopic model behavioral questions we care about (e.g., in-context learning capability olsson2022context and more speculation about high-level cognitive capabilities such as planning and dangerous capability such as deception circuitjuly2023anthropic). Evaluation and Benchmarking Benchmarking offers insights about what methods work and quantifies their efficiency, and it will also drive community efforts in clear and meaningful directions lipton2018mythos, casper_moving, krishnan2020against, mohseni2021multidisciplinary, madsen2022post. Interpretability benchmarks and metrics were made to evaluate interpretability tools (by evaluating their effectiveness in detecting trojans) casper2023red, circuits (by testing whether specific subgraphs are counted as circuits) lawrencec_causal and explanations (by examining the faithfulness, comprehensiveness, and sufficiency of an explanation) lage2019evaluation, deyoung2020eraser, krishna2022disagreement. However, as the inner logic of a certain AI system is unknown before the interpretability tools are applied samek2019explainable and different explanations may even contradict each other neely2021order, krishna2022disagreement, building a reliable evaluation benchmark or metric is rather difficult krishna2022disagreement.",
      "origin_cites_number": 18
    },
    {
      "section_title": "Human Values Verification",
      "level": "2",
      "content": "Human Values Alignment refers to the expectation that AI systems should adhere to the community's social and moral norms ~chatila2019ieee. As the capabilities of AI systems advance, some have begun to exhibit abilities approaching AGI ~openai2023gpt4. In the future, we can expect autonomous agents governed by these AI systems to become an integral part of our daily lives ~lee2023if. However, if these systems fail to grasp the inherent complexity and adaptability of human values, their decisions could result in negative social outcomes. In this context, simply aligning with human intent may not be sufficient. Thus evaluating the alignment of human morality and values between AI systems and human beings becomes crucial ~weidinger2023using. This underscores the importance of designing AI entities that are more socially oriented, reliable, and trustworthy. Following the logic of theoretical research and practical techniques, we divide our discussion of human value alignment into these two aspects: Formulations \\Ssubsec:formal-ethics-coop and Evaluation Methods \\Ssubsec:value-learning-techniques of human value alignment.",
      "origin_cites_number": 11
    },
    {
      "section_title": "Formulations",
      "level": "3",
      "content": "As the formulation of value is complicated, we introduce frameworks that formally characterize aspects of human values that are relevant to alignment. Specifically, we focus on two topics: formal machine ethics and game theory for cooperative AI. The former focuses on building a formal framework of machine ethics, while the latter discusses the value of multiagent systems, which share a similar origin of the game process. Formal Machine Ethics Machine ethics yu2018building, winfield2019machine, tolmeijer2020implementations, first introduced in \\Ssec:values-in-intro, aim to build ethically-compliant AI systems. Here, we introduce the branch of machine ethics that focuses on formal frameworks -- what we call formal machine ethics. We explain three approaches to formal machine ethics: logic-based, RL/MDP-based, and methods based on game theory/computational social choice: itemize[left=0.2cm] \\item Logic-based methods. One major direction within formal machine ethics focuses on logic pereira2016programming. A number of logic-based works use or propose special-purpose logic systems tailored for machine ethics, such as the Agent-Deed-Consequence (ADC) model dubljevic2020toward, deontic logic von1951deontic,arkoudas2005toward, event calculus and its variants berreby2017declarative. Other works also develop methods for the formal verification of moral properties or frameworks for AI systems that accommodate such kind of formal verification dennis2016formal, mermet2016formal. \\item RL \\& MDP-like settings. Another line of work concerns statistical RL or other similar methods for planning within MDP-like environments abel2016reinforcement, svegliato2021ethically. In particular, some works wu2018low,svegliato2021ethically involve the utilization of the manual design of ethics-oriented reward functions, a concept denoted as ethics shaping. Conversely, in other works berreby2017declarative,murtarelli2021conversation, the segregation of ethical decision-making from the reward function is pursued. \\item Game theory-based methods. To address multi-agent challenges, researchers have developed machine ethics methods based on game theory and computational social choice. Championed by pereira2016bridging, methodologies of existing work can be broadly partitioned into Evolutionary Game Theory (EGT) pereira2016programming, classical game theory conitzer2017moral, and computational social choice rossi2011short, noothigattu2018voting. itemize Game Theory for Cooperative AI Cooperative AI dafoe2020open,dafoe2021cooperative aims to address uncooperative and collectively harmful behaviors from AI systems (see \\Ssec:challenges-of-alignment). Here we introduce the branch of cooperative AI that focuses on game theory to complement the introduction to MARL-based cooperative training in \\Ssec:cooperative-ai-training. This branch tends to study the incentives of cooperation and try to enhance them, in contrast to the MARL's tendency to emphasize the capabilities of coordination. Examples of incentive failures include game theory dilemmas like the prisoner's dilemma phelps2023investigating and tragedy of the commons perolat2017multiagent, while examples of coordination capability failures include bad coordination of a robot football team ma2022elign. itemize[left=0.2cm] \\item Classical Game Theory for Cooperative AI. Recent work on Stackelberg games --- a theoretical model for commitment in games --- include the introduction of bounded rationality into the model pita2010robust, dynamic models li2017review, machine learning of Stackelberg equilibria fiez2020implicit, and more. Apart from Stackelberg games, mixed-motive games have also received extensive study dafoe2020open,mckee2020social,oesterheld2022safe. \\item Evolutionary Game Theory for Cooperative AI. Another avenue of research, initiated by sachs2004evolution, aims to understand how cooperation emerges from evolution -- this includes human cooperation, which arose from Darwinian evolution, as well as the cooperation tendencies in AI systems that could emerge within other evolutionary settings such as the replicator dynamics schuster1983replicator,weibull1997evolutionary. itemize",
      "origin_cites_number": 23
    },
    {
      "section_title": "Evaluation Methods",
      "level": "3",
      "content": "In this section, we assume that we have already obtained the appropriate value that should be aligned. However, even so, under the guidance of Goodhart's Law ~goodhart1984problems, we cannot simply define complex human values as reward functions, which also brings greater challenges to value alignment. We introduce specific human value alignment techniques in three parts: Building Moral Dataset, Scenario Simulation. Building Moral Dataset Moral Alignment refers to the adherence of AI systems to human-compatible moral standards and ethical guidelines while executing tasks or assisting in human decision-making ~min2023recent. Early attempts at moral value alignment, initiated in 2018 awad2018moral, have confirmed that the definition and evaluation of moral values themselves is a challenging issue. This has led to the emergence of abstract moral standards hagendorff2022virtue and various different standards driven by the average values of diverse community groups awad2018moral, fueling further in-depth research into moral value assurance. Assurance of moral values is typically achieved by constructing corresponding datasets. The Rule-of-Thumb (RoT) serves as a gauge for determining what actions are considered acceptable in human society. Building on this concept, emelin2021moral, forbes2020social, ziems2022moral introduced the Moral Stories, SOCIAL-CHEM-101, and Moral Integrity Corpus datasets respectively, focusing on providing human social and moral norms. hendrycks2020aligning and jin2022make introduced the ETHICS and MoralExceptQA datasets respectively, highlighting the inability of contemporary models to align ethically with human values. abdulhai2022moral found that models exhibit certain morals and values more frequently than others, revealing how the moral foundations demonstrated by these models relate to human moral foundations. pan2023rewards explored the trade-off between rewards and moral behavior, discovering a certain tension between the two. Scenario Simulation Scenario simulation is a more complex form than datasets and therefore is considered by some views to be more effective in replicating real situations and harvesting better results. The form of the scenario can also vary. pan2023machiavelli built a series of diverse, morally salient scenarios through text adventure games, evaluating complex behaviors such as deception, manipulation, and betrayal. On the other hand, some work attempts to make intelligent agents learn human values through simulating human-machine interaction. yuan2022situ proposed a method for bidirectional value alignment between humans and machines, enabling machines to learn human preferences and implicit objectives through human feedback. liu2024training placed AI within a simulated human society sandbox, allowing AI to learn human societal value inclinations by mimicking human-social interactions.",
      "origin_cites_number": 13
    },
    {
      "section_title": "Governance",
      "level": "1",
      "content": "Besides technical solutions, governance, the creation and enforcement of rules, is necessary to ensure the safe development and deployment of AI systems. In this section, we survey the literature on AI governance by exploring the role of AI governance, the functions, and relationships between stakeholders in governing AI, and several open challenges to effective AI governance.",
      "origin_cites_number": 0
    },
    {
      "section_title": "The Role of AI Governance",
      "level": "2",
      "content": "To explore the role of AI governance, we must identify the challenges that require governance. A range of social and ethical issues can and have already emerged from the adoption and integration of AI into various sectors of our society summit2023round. For instance, AI applications can inadvertently perpetuate societal biases, resulting in racial and gender discrimination caliskan2017semantics,perez2022discovering. Moreover, unchecked reliance on these systems can lead to repercussions such as labor displacement acemoglu2018artificial, widening socioeconomic disparities, and the creation of monopolistic environments. AI systems have exhibited the potential to jeopardize global security turchin2020classification. For example, OpenAIs system card for GPT-4 openai2023gpt4 finds that an early version of the GPT-4 model as well as a version fine-tuned for increased helpfulness and harmlessness exhibits capabilities to enable disinformation, influence operations, and engineer new biochemical substances, among other risky behavior. urbina2022dual further demonstrated the potential of AI systems to enable the misuse of synthetic biology by inverting their drug discovery model to produce 40,000 toxic molecules. The horizon also holds the prospect of increasingly agentic and general-purpose AI systems that, without sufficient safeguards, could pose catastrophic or even existential risks to humanity mclean2023risks. For example, OpenAI's weng2023agent argued that models such as LLM could essentially act as the brain of an intelligent agent, enhanced by planning, reflection, memory, and tool use. Projects such as AutoGPT, GPT-Engineer, and BabyAGI epitomize this evolution. These systems can autonomously break down intricate tasks into subtasks and make decisions without human intervention. Microsoft research suggests that GPT-4, for instance, hints at the early inklings of AGI bubeck2023sparks. As these systems evolve, they might lead to broad socio-economic impacts such as unemployment, and potentially equip malicious actors with tools for harmful activities. The major objective of AI governance is to mitigate this diverse array of risks. In pursuit of this goal, relevant actors should maintain a balanced portfolio of efforts, giving each risk category its due consideration.",
      "origin_cites_number": 9
    },
    {
      "section_title": "The Multi-Stakeholder Approach",
      "level": "2",
      "content": "We put forward a framework to analyze the functions and relationships between stakeholders in AI governance (see Figure fig:governance). In this framework, we outline three main entities. Government Agencies oversee AI policies using legislative, judicial, and enforcement powers, as well as engage in international cooperation. Industry and AGI Labs research and deploy AI technologies, making them subjects of the governance framework, while proposing techniques to govern themselves and affecting governance policy. Third Parties, including academia, Non-Governmental Organizations (NGOs), and Non-Profit Organizations (NPOs), perform not only auditing on corporate governance, AI systems, and their applications but also assist the government in policy-making. Proposals have been made about specific principles for a multi-stakeholder AI governance landscape. Notably, brundage2020toward argues to implement institutions, software, and hardware to make claims about the safety of AI systems more verifiable. Government According to anderljung2023frontier, three building blocks for government regulation are needed: (1) standard development processes to determine appropriate requirements for cutting-edge AI developers, (2) registration and reporting requirements to offer regulators insight into the progress of advanced AI development processes, (3) mechanisms to guarantee adherence to safety standards in the development and deployment of cutting-edge AI models. At present, an emerging collection of governmental regulations and laws is surfacing on a global scale, including the European Union's AI Act euaiact, and the Bipartisan Framework for U.S. AI Act bipartisan. Such regulations are indispensable for the safety and alignment of AI systems. Industry and AGI Labs Governance efforts in industry and AGI labs should emphasize comprehensive AI risk assessments throughout the lifecycle of the AI system. Based on discussions in koessler2023risk,schuett2023towards, the full cycle of AI risk assessment can be seen as consisting of five stages. Pre-development risk assessments, pre-training risk assessments, and pre-deployment risk assessments all include predictions and analyses of impact and risks with a variety of tools, but with increasing amounts of detail, clarity, and sophistication koessler2023risk. Post-deployment monitoring is the phase where mechanisms for monitoring are established, and all previous analyses are continually updated post-deployment koessler2023risk. External scrutiny includes bug bounty programs schuett2023towards, external red teaming and third-party model auditing schuett2023towards,anderljung2023frontier Taking security measures against the risks associated with AI systems seems to be widely accepted by AI companies and related practitioners. schuett2023towards shows that 98\\% of respondents who have been surveyed somewhat or strongly approved that AGI labs should perform pre-deployment risk assessments, hazardous capabilities evaluations, third-party model audits, safety restrictions on model usage, and red teaming to guarantee AI safety. Meanwhile, leading AI companies, including Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI, have voluntarily committed to the government to implement security measures house2023fact. Notably, a lot of researchers have proposed pausing the development of advanced AI systems to win more time for safety research, risk assessments, and regulatory preparations 2023pauseai. Their proposals include blanket pausing of all sufficiently advanced systems 2023pauseai, and also conditional pausing of specific classes of models in response to evaluation results on specific failure modes alaga2023coordinated, including the currently adopted practice of responsible scaling policy (RSP) rsp2023anthropic. Third Parties mokander2023auditing presents three key functions of third-party auditing: (1) Governance audits (of tech providers that design and disseminate LLMs) (2) Model audits (of LLMs after pre-training but prior to their release) (3) Application audits (of applications based on LLMs). One prominent example of existing third-party audits is that of METR, initially a project of Alignment Research Center arcevalscollab,kinniment2023evaluating, who collaborated with OpenAI to perform red teaming on GPT-4 openai2023gpt4 and partnered with Anthropic to perform red teaming on Claude 2 anthropiceval. These efforts include evaluations on toxicity and bias, as well as frontier AI risks such as autonomous replication, manipulation, cybersecurity, and biological weapon risks openai2023gpt4,shevlane2023model. Apart from auditing, third parties can support AI governance in other ways, such as assisting policy-making and facilitating cooperation internationally ho2023international. For example, maas2021aligning thinks that the government should prefer technology-neutral rules rather than technology-specific rules. AI4Peoples Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations floridi2021ethical, released by AI4People, was guided to the Ethics Guidelines for Trustworthy Artificial Intelligence presented in April 2019 ato2023. The World Economic Forum (WEF) convenes government officials, cooperations, and civil society and it has initiated a Global AI Action Alliance in collaboration with partner organizations, with the goal of promoting international cooperation in the field of AI kerry2021strengthening.",
      "origin_cites_number": 27
    },
    {
      "section_title": "Open Problems",
      "level": "2",
      "content": "There are numerous open problems in the existing field of AI governance. These problems often have no clear answers, and discussion of these questions can often promote better governance. For effective AI governance, we mainly discuss international governance and open-source governance, hoping to promote the safe development of AI through our discussion.",
      "origin_cites_number": 0
    },
    {
      "section_title": "International Governance",
      "level": "3",
      "content": "Amidst the swift progress and widespread implementation of AI technology universally, the need for international governance of AI is high on the agenda bletch2023summit. Critical discussions revolve around the necessity to institute a global framework for AI governance, the means to ensure its normativity and legitimacy erman2022artificial, among other significant concerns. These themes draw an intensifying level of detail and complexity in their consideration. Also, as stated by United Nations secretary-general Antnio Guterres during a Security Council assembly in July, generative AI possesses vast potential for both positive and negative impacts at scale, and failing to take action to mitigate the AI risks would be a grave neglect of our duty to safeguard the well-being of current and future generations un2023remark, international governance also has intergenerational influence. Hence, we examine the significance and viability of international AI governance from three aspects within this section: manage global catastrophic AI risks, manage opportunities in AI, and historical and present efforts, with both generational and intergenerational perspectives. We aim to contribute innovative thoughts for the prospective structure of international AI governance. Manage Global Catastrophic AI Risks The continual advancements in AI technology promise immense potential for global development and prosperity vinuesa2020role. However, they inevitably harbor underlying risks. The unchecked competition in the market and geopolitical factors could precipitate the untimely development and deployment of advanced AI systems, resulting in negative global externalities tallberg2023global. The amplification of existing inequalities such as racial and gender bias pri2020shea ingrained in AI systems may result in intergenerational ethical discrimination. Since these risks are international and intergenerational, it seems that international governance interventions could alleviate these catastrophic global AI challenges. For example, a consensus amongst nations could help defuse potential AI arms races, while an industry-wide agreement could avert the hasty and irresponsible development of sophisticated AI systems, thus securing the long-term and sustainable development of AI ho2023international. Manage Opportunities in AI The opportunities created by AI development are not distributed equally, which may cause enduring digital inequality between regions and harm the sustainability of AI development. Geographic variances in AI progression suggest an inequitable distribution of its economic and societal benefits, potentially excluding developing nations or specific groups from these advantages ho2023international,tallberg2023global. Moreover, the consolidation of decision-making authority within the technology sector among a limited number of individuals intergen2021ai,airegth2021sa could cause an intergenerational impact. Such inequality in the distribution of interests can be mitigated through international governance. Effective international consensus and coordination on the allocation of AI opportunities, which is facilitated by its propagation, education, and infrastructural development committ2023ro, could ensure a balanced distribution of benefits derived from AI and promote sustainability in its ongoing development. Historical and Present Efforts Before the surge of AI technology, the international community had laid down frameworks in line with cooperative regulation of influential technologies and critical matters. For example, the Intergovernmental Panel on Climate Change (IPCC) convened specialists to assess climactic environmental issues, fostering scientific consensus ho2023international. The International Civil Aviation Organization (ICAO) standardized and oversaw international regulations, simultaneously assessing the member nations' compliance with these laws ho2023international. The International Atomic Energy Agency (IAEA) propelled the harmonious utilization of nuclear energy, with its global reach and sophisticated monitoring and evaluation mechanisms. Fast forward to the present-day scenario, wherein multiple international organizations have arrived at a consensus on AI governance. In 2019, the G20 members consolidated a ministerial declaration focusing on human-centered artificial intelligence principles G20prin2019. Concurrently, the Organisation for Economic Cooperation and Development (OECD) set forth the OECD Principles on Artificial Intelligence oecdpri2019. The IEEE Standards Association launched a worldwide initiative aimed at Securing that all stakeholders involved in the design and implementation of autonomous and intelligent systems receive proper education, training, and motivation to emphasize ethical concerns, thereby advancing these technologies for the betterment of humanity. chatila2019ieee. In 2021, the United Nations Educational, Scientific and Cultural Organization(UNESCO) produced the first-ever global standard on AI ethics unesco2021, which aims to lay the foundations for making AI systems work for the good of humanity and societies, and to prevent potential harm caused by losing control over AI systems. In 2023, the AI Safety Summit was convened in London, United Kingdom. Countries held roundtable discussions on the risks and opportunities of AI and jointly issued the Bletchley Declaration bletch2023summit. The scholarly community has also proposed prospective international governance frameworks for AI, such as the International AI Organization (IAIO) trager2023international. We hope these precedents and research outcomes will inspire and provide the groundwork for developing a resilient and long-lasting international framework for AI governance in the future.",
      "origin_cites_number": 18
    },
    {
      "section_title": "Open-Source Governance",
      "level": "3",
      "content": "The debate over the open-sourcing of contemporary AI models is contentious in AI governance, particularly as these models gain increased potency seger2023open. The potential security hazards linked with making these models open-source continue to be the crux of debates among AI researchers and policymakers. The offence-defence balance in open-source AI governance also remains controversial. There is still debate over whether open-source models will increase model security or increase the risk of abuse. As referenced in shapiro2010paper, the efficacy of disclosure depends on the chance of potential attackers already possessing the knowledge, coupled with the government's capacity to convert transparency into the identification and solution of emerging vulnerabilities. Some scholars have already conducted preliminary discussions on the offense-defense balance in the AI field, such as weng2023attack's discussion of adversarial attacks. If a suitable equilibrium between offence and defence cannot be forged for AI systems, the open-sourcing could potentially give rise to significant risks of AI system misuse. For precision and clarity, we adhere to the definition of open-source models stated by seger2023open: enabling open and public access to the model's architecture and weights, allowing for modification, study, further development, and utilization by anyone. Currently, the most recognized open-source models include Llama2, Falcon, Vicuna, and others. In this section, we evaluate the security advantages and potential threats posed by open-source models, fostering a discourse on the feasibility of open-sourcing these models. Ultimately, our objective is to amalgamate insights from existing studies to put forward suggestions for future open-source methodologies that will ascertain the secure implementation of these models. Arguments for Open-sourcing The view that supports the open-sourcing of existing models suggests that this method can mitigate the security risks inherent in these models in several ways: itemize[left=0.3cm] \\item Potentially Bolster Model's Security. Meta's assertions in their release blog for Llama2 Llama22023 promote the belief that this enables the developer and the technical community to conduct tests on the models. As a result, this rapid identification and resolution of issues can considerably strengthen model security. In contrast, another perspective suggests that open-sourcing existing models could enhance the recognition of associated risks, thereby facilitating a greater focus on, investigation into, and mitigation of these potential hazards grover2019. \\item Foster the Decentralization of Power and Control. Open-sourcing has been widely recognized as an effective strategy in reducing the dominance of major AI laboratories across various sectors, including economic, social, and political domains seger2023open. An example is articulated in the core reasons for Stability's open-sourcing of Stable Diffusion: They place their trust in individuals and the community, as opposed to having a centralized, unelected entity controlling AI technology stability2022. Furthermore, certain commentators draw an analogy between open-sourcing and the Enlightenment Era, asserting that decentralized control reinforces faith in the power and good of humanity and society agedis2023, implementing central regulations for safety purposes might amplify the power of the AI technology community instead. itemize Arguments against Open-sourcing Critics of open-source models assess the potential for misuse from the following viewpoints: itemize[left=0.3cm] \\item Potentially Be Fine-Tuned into Detrimental Instances. Current research rigorously affirms that AI systems, contradictory to their initial design intent for mitigating toxicities in chemistry or biology, now hold the potential to manufacture new chemical toxins urbina2022dual and biological weaponry sandbrink2023artificial. The malicious fine-tuning of such models could lead to profound security risk manifestations. Besides, language models, once fine-tuned, could emulate skilled writers and produce convincing disinformation, which may generate considerable sociopolitical risks goldstein2023generative. \\item Inadvertently Encourage System Jailbreaks. Research indicates that unfettered access to open-sourced model weights could facilitate bypassing system security measures seger2023open. This premise was epitomized by zou2023universal, who showcased this potentiality by developing attack suffixes using Vicuna-7B and 13B. Once implemented within readily accessible interfaces such as ChatGPT, Bard, and Claude, these provoked unwanted generations. Therefore, open-sourcing a model might unintentionally undermine the safeguarding protocols of models that are not open-sourced, consequently amplifying the likelihood of model misuse. itemize Tentative Conclusions on Open-Source Governance The debate on the open-sourcing of AI models remains unsettled, with a prevailing viewpoint that the disclosure of AI models does not pose significant risks at present. Our discourse not only synthesizes existing perspectives on this topic but also prepares the ground for future deliberations considering the prudence of open-sourcing more advanced AI systems. Existing guidelines for open-sourcing advanced AI systems include measures such as evaluating risks by quantifying the potential for misuse via fine-tuning and a gradual model release solaiman2019release,seger2023open. Meanwhile, policymakers are establishing rigorous compliance protocols for these open-source models. For example, European policymakers insist that the models should have performance, predictability, interpretability, corrigibility, security, and cybersecurity throughout [their] lifecycle. bal2023.",
      "origin_cites_number": 16
    },
    {
      "section_title": "Rethinking AI Alignment from a Socio-technical Perspective",
      "level": "2",
      "content": "In the preceding discussion, our primary focus is on AI systems as the core of AI Alignment. We examine strategies to align the system with human intentions and values throughout its lifecycle, considering both forward and backward alignment. In the future, AI will address more challenging and high-stakes decisions, e.g., ``How to allocate resource for fairness?'' and ``Which drugs are safe to approve?''. These decisions will require not only significant expertise for well-informed answers but also involve value judgments, leading to strong disagreements among informed individuals based on differing values. Furthermore, AI systems may transmit incorrect values, sway public opinion, facilitate cultural invasion, and exacerbate social division goldstein2023generative. Singapore Conference on AI (SCAI) once introduced 12 questions that are meant to be a holistic formulation of the challenges that should be addressed by the global AI community to allow humanity to flourish \\url{https://www.scai.gov.sg/}. In the area of alignment we are more concerned about the following question: as AI systems evolve into socio-technical entities, how can alignment techniques mit igate the challenges they pose to human society? Specifically, we explore the incorporation of values into AI systems through alignment techniques and provide insights into security methods. We also aim to identify the alignment techniques needed to address the socio-technical challenges posed by future AI systems.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Incorporating Values into AI Systems",
      "level": "3",
      "content": "Aligning AI systems with human morals and societal values is a key objective of alignment technology. However, current technologies (e.g., RLHF) primarily blend preferences without distinguishing specific values, focusing solely on human preferences. Human preferences effectively address the basic alignment issue: ensuring models align with human intentions and safety, but not morals and societal values. However, minor errors in future AI systems' critical problems can lead to disagreements among people with differing viewpoints. Truly understanding human values is crucial for AI systems to generalize and adapt across various scenarios and ideologies. Incorporating values into AI systems generally involves two aspects: aligning with individual values (\\Ssec:human-values-alignment), and aligning with collective values. In this part, we mainly discuss the second topic. The main challenge of collective value alignment lies in determining which groups to include. A prevalent approach is defining universal values like fairness, justice, and altruism, exemplified by the veil of ignorance. However, this work remains theoretical; another approach avoids defining universal values, instead seeking the broadest overlap of values across cultures. bakker2022fine initiated this approach by gathering preferences from various demographics, training a language model, and aggregating results using diverse social welfare functions. Similarly, simulated deliberative democracy has been proposed to enhance decision-making leike2022proposal. Specifically, individuals from diverse demographics reach consensus on value-laden topics with AI assistance. This data informs new model training, enabling simulation of deliberative democracy for more apt responses to new value-laden issues. Furthermore, instead of providing a consensus answer to all users, collective value alignment should encourage AI systems to tailor responses to specific demographic groups. In other words, what values should guide the model's responses to specific questions or in certain dialogues? Democratic Fine-Tuning mai2023dft uses a value card and moral graph to link various values, allowing fine-tuned LLMs to reflect on their moral context before responding. However, while most value discussions assume static values, social values are actually dynamic and evolving. Exploring how value-aligned AI systems can dynamically adapt to changing environmental values is crucial. Future technologies need to address static value alignment first, including strategies for sampling human groups for alignment. bakker2022fine founds that consensus statements built silently from a subgroup will lead to dissent among excluded members, highlighting the consensus's sensitivity to individual input. For international cooperation, establishing a shared data center is necessary but also requires first determining which civilizations to include and if their values can align.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Alignment Techniques for AI Governance",
      "level": "3",
      "content": "It's crucial to ensure the reliability and trustworthiness of AI systems as they are adopted in various real-world decision-making scenarios. On one hand, language models still exhibit illusions during use, and on the other hand, the reliability of systems comprises two parts: the system's reliability under individual testing environments and its reliability in human interactions. Another issue is constructing systems with decision-making processes that are observable and explainable to users. From a social perspective, the proliferation of AI systems across fields also poses potential risks. This risk arises from a gap between AI developers, who often focus on advancing technology without considering its downstream applications, and AI adopters, who may transfer AI systems to their fields without adequate safety considerations or verification of replicable success \\url{https://www.scai.gov.sg/scai-question-11/}. Therefore, it is crucial to build a framework that enables AI adopters to accurately assess model utility and appropriateness, and allows AI regulators to quickly identify risks and issue safety alerts in AI systems. Alignment techniques can facilitate synchronized, independent, and rigorous evaluations of AI systems. AI developers should prioritize appropriate bias handling during the training process, acknowledging the importance of socio-economic, cultural, and other differences. Furthermore, we should aim to develop robust and fair evaluation methods and datasets for auditing AI systems. zhu2023dyval proposes the first dynamic testing protocol for large language models, utilizing Directed Acyclic Graphs (DAGs) to dynamically generate test data, thereby reducing the risk of test data memorization and contamination. Additionally, new robust security protocol evaluation methods have been introduced: shlegeris2023meta suggests constructing adversarial policies to manage dangerously powerful and deceptive models, while greenblatt2023ai proposes (un)trusted editing to supervise models based on their harm and deceitfulness levels. Future efforts should also prevent AI systems from reward-hacking evaluation system exploits and aim to provide AI regulators with an explainable, independent, and centralized evaluation system. AI adopters and the industry should allocate financial and computational resources to thoroughly evaluate use cases and share case studies showcasing both successes and failures. Equally important is training for adopters on downstream applications.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Conclusion",
      "level": "1",
      "content": "In this survey, we have provided a broadly-scoped introduction to AI alignment, which aims to build AI systems that behave in line with human intentions and values. We specify the objectives of alignment as Robustness, Interpretability, Controllability, and Ethicality (RICE), and characterize the scope of alignment methods as comprising of forward alignment (making AI systems aligned via alignment training) and backward alignment (gaining evidence of the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks). Currently, the two notable areas of research within forward alignment are learning from feedback and learning under distribution shift, while backward alignment is comprised of assurance and governance. One thing that sets alignment apart from many other fields is its diversity hendrycks2022pragmatic -- it is a tight assembly of multiple research directions and methods, tied together by a shared goal, as opposed to a shared methodology. This diversity brings benefits. It fosters innovation by having the different directions compete and clash against each other, leading to a cross-pollination of ideas. It also allows different research directions to complement each other and together serve the goal of alignment; this is reflected in the alignment cycle (see Figure fig:maindiagram), where the four pillars are integrated into a self-improving loop that continually improves the alignment of AI systems. Meanwhile, this diversity of research directions raises the barrier to entry into this field, which mandates the compilation of well-organized survey materials that serve both the newcomers and the experienced. In this survey, we attempt to address this need by providing a comprehensive and up-to-date overview of alignment. We attempt to account for the full diversity within the field by adopting a broad and inclusive characterization of alignment. Our survey of alignment gives a spotlight to almost all major research agendas in this field, as well as to real-world practices on the assurance and governance front. We recognize that boundaries of alignment are often vague and subject to debate. Therefore, when proposing the RICE principles, we put forth our broad characterization of alignment as an explicit choice. In the meantime, we recognize that such a survey needs to be a long-term endeavor that is continually reviewed and updated. Both the problems and methods of alignment closely follow the development of machine learning. This fast-paced development means that new materials and frameworks can become outdated after merely a few years. This fact is one reason why we write the survey to reflect the latest developments, and also mandates continual maintenance and updates.",
      "origin_cites_number": 1
    },
    {
      "section_title": "Key Challenges in the Alignment Cycle",
      "level": "2",
      "content": "Specifically, we outline key challenges and potential future directions based on the alignment cycle, namely forward and backward alignment. Learning Human Intent from Rich Modalities (\\textcolor{myred{forward alignment})}Underspecificity of true human intent,i.e., the non-uniqueness of inferred human intent from binary feedback data, is a key challenge in scalable oversight. Consider an AI system tasked with providing proof or refutation to a mathematical hypothesis, under a human evaluator who might be tricked by sophisticated false proofs. Our goal is to construct a training process that induces the AI system to output sound proofs as opposed to false proofs that seem convincing. This system may mislead evaluators with plausible but false proofs due to the system's optimization for human approval, as it attempts to satisfy the superficial criteria of convincing proofs rather than focusing on accuracy. The fundamental problem stems from the reliance on binary feedback which categorizes responses simply as preferred or dispreferred, thus limiting the amount of information on true human preferences that's available to the learning algorithm, potentially leading to the preference of credible-seeming deceptive proofs over genuinely sound arguments. To enhance the model's alignment with true human intent, researchers have proposed incorporating richer human input beyond binary choices, such as detailed text feedback chen2024learning and real-time interations hadfield2016cooperative. It allows the model to differentiate between proofs that are merely convincing and those that are truly sound, using nuanced human evaluations and a vast database of human-written texts. The broader input base helps in constructing a more accurate model of human preferences, reducing the risk of favoring misleading proofs while respecting the complexity of human intent and reasoning. Looking forward, even richer modalities like embodied societal interactions could represent an enticing next step. It is worth noting that current LLMs are already trained on Internet-scale human text (and for multimodal models, also visual/audio content). Why, then, don't reward modeling algorithms already possess the ability to accurately pin down human intent? The explanation is that pretraining data does not feed into the reward modeling process in a way that biases the process towards true human intent, even though the reward model is finetuned from the pretrained model. For instance, neural circuits representing human intent can potentially be rewired during RLHF to perform manipulative behaviors. From another perspective, pretraining on text such as humans do not want to be tricked into believing things does not induce the reward model to interpret later human feedback signals in this light, partly due to the lack of out-of-context learning capabilities in current LLMs berglund2023taken. Solving these problems may enable reward modeling algorithms to learn human intent from massive pretraining data, a big step towards our goal. We summarize three key questions for the learning of human intent from rich modalities. They serve as key dimensions for characterizing an alignment method from the intent modality lens, and almost all existing alignment methods can be categorized by their answers to these three questions. enumerate[left=0.3cm] \\item Learning algorithm. As previously mentioned, we need to learn human intent from rich modalities in a way that guides the reward model's subsequent interpretation of human input. \\item Priors and inductive biases. Human-like priors/inductive bias is needed for the reward modeling process to select the correct hypothesis of human intent, though this requirement is greatly loosened as the allowed modalities of human input expand. \\item Learner alignment. We utilize the intent learner to align AI systems, possibly by using it as a reward model. However, this would not be possible if the intent learner, which is itself an AI system with potentially strong capabilities, is misaligned. This necessitates measures to avoid or contain the misalignment of the intent learner. enumerate Trustworthy Tools for Assurance (\\textcolor{myblue{backward alignment})} A major concern in AI alignment is deceptive alignment, where AI systems pursue aligned goals under most circumstances but may pursue other goals when opportunities arise. Recent studies have revealed that general alignment techniques (e.g., SFT, RLHF, Adversarial Training) fail to eradicate certain deceptive and backdoor behaviors, possibly leading to a misleading sense of safety hubinger2024sleeper. With AI systems gaining power and access to more resources, hidden intentions that pose existential risks could have unimaginable consequences. How can we detect and eliminate deceptive and backdoor behaviors? Reliable tools are still lacking to address this issue. On one hand, mechanistic interpretability tools encounter additional challenges due to the polysemanticity of neurons and scalability issues. On the other hand, there is a limited understanding of how jailbreaking functions and the susceptibility of language models to poisoning and backdoors anwar2024foundational. Additionally, given the potential misuse of AI systems in cyber attacks, biological warfare, and misinformation, it is crucial to develop reliable mechanisms to trace the origins of LLM outputs. While AI systems are becoming more integrated into society, societal readiness lags behind. This is evident from the inadequate AI governance efforts, insufficient public knowledge, governments' lack of necessary scientific and technical capabilities, the absence of institutions that can keep pace with LLM advancements, and the challenges in mitigating the social impacts of widespread harmful behaviors. Therefore, it is essential to reconsider AI alignment from a socio-technical standpoint, establish dependable AI assurance and governance mechanisms, and engage in effective international governance collaboration. Value Elicitation and Value Implementation (\\textcolor{myblue{backward alignment})} Current algorithms for learning from human feedback, particularly RLHF, often assume feedback comes from a singular, monolithic human source. However, this assumption is unrealistic due to widespread disagreements on contentious issues globally, which frequently result in conflicting judgments about AI system outputs santurkar2023whose. Consequently, determining who to draw feedback from and understanding the scope and nature of human values infused into models are crucial questions for the field of alignment. Value Elicitation and Value Implementation aim to define the values and norms that AI systems should encode and how to integrate these into AI systems. Human values and preferences are diverse, ranging from strict rules like laws and moral principles to social etiquette and specific domain preferences cahyawijaya2024high,kirk2024prism. We need reliable tools to reveal the values embedded in current AI systems and potential social risks, enabling us to mitigate these risks more effectively \\url{https://www.scai.gov.sg}. Democratic human input is one of the leading solutions to value elicitation and implementation. This method gathers input from a large, demographically representative sample of individuals, aggregating preferences and values into a coherent policy, rather than relying on feedback from a single individual. This approach is heavily influenced by the computational social choice literature brandt2016handbook. Leading industry zaremb2023democratic and academic kopf2024openassistant labs have adopted democratic human input for LLMs. However, research is still needed on its integration into more agentic AI systems, such as LLM-based autonomous agents. Despite its apparent simplicity, democratic human input encounters significant practical and fundamental challenges. Obtaining a truly random sample of the global population is particularly challenging, as 33\\% of people worldwide do not have Internet access and thus are excluded from participating in AI system training itu2023stats. Furthermore, human feedback becomes less effective when the AI system's reasoning capabilities surpass those of humans, making it difficult for human workers to evaluate its outputs. To complement democratic human input, alternative approaches aim to formalize universally recognized meta-level moral principles, such as moral consistency, moral reflection, and moral progress, designing algorithms to enact these principles. Although these methods still rely on human data and input, they do not demand strict representativeness and are less constrained by human oversight limitations. itemize \\item Moral consistency. There is a general consensus that moral principles should be consistently applied, meaning similar cases should receive similar treatment irrespective of the people or parties involved. Algorithms have been developed to integrate this principle into the ethical decision-making processes of AI systems jin2022when. \\item Moral reflection and moral progress. The coherent extrapolated volition concept was developed to formalize the role of reflection in shaping human moral values sovik2022overarching. Inspired by this, subsequent algorithms were designed to enable AI systems to mimic human moral reflection, thereby influencing their actions xie2023defending. Furthermore, the logical next step of moral reflection is moral progress, demonstrated by AI-driven analyses of historical moral trends schramowski2020moral,atif2022evolution and efforts to permanently integrate continual moral advancement into AI systems kenward2021machine. itemize",
      "origin_cites_number": 16
    },
    {
      "section_title": "Key Traits and Future Directions in Alignment Research",
      "level": "2",
      "content": "In the end of the survey, we conclude the survey by looking ahead and presenting the key traits in this field that we believe ought to be preserved or fostered. Open-Ended Exploration of Novel Challenges and Approaches A lot of the alignment discourse is built upon classic works that predate the recent developments of LLMs and other breakthroughs in large-scale deep learning. Thus, when this paradigm shift happens in the machine learning field, it is plausible that some challenges in alignment become less salient while others become more so; after all, one defining feature of scientific theories is their falsifiability popper2005logic. More importantly, this shift in machine learning methodology and the broader trend of ever-tighter integration of AI systems into society abbass2019social introduces novel challenges that could not be envisioned before. This requires that we engage in open-ended exploration, actively seeking out new challenges that were previously neglected. Moreover, such an exploration need not be constrained to challenges -- a similar mindset should be adopted regarding approaches and solutions, thus building a more diverse portfolio for both the questions and the answers shimi2022diversify. Combining Forward-Looking and Present-Oriented Perspectives Alignment has emphasized harms from potential advanced AI systems that possess stronger capabilities than current systems firstprinc. These systems might come into existence well into the future, or might just be a few years away stein2022expert. The former possibility requires us to look into extrapolated trends and hypothetical scenarios carlsmith2022power. In contrast, the latter possibility highlights the need for on-the-ground efforts that work with current governance institutions and use current systems as a prototype for more advanced ones cotra2021the. Emphasis on Policy Relevance Alignment research does not live in a vacuum but in an ecosystemSee \\url{aisafety.world for a map of the organizational landscape of alignment.}, with participation from researchers, industry actors, governments, and non-governmental organizations. Research serving the needs of the AI alignment and safety ecosystem would therefore be useful. Such needs include solving the key barriers to various governance schemes, for example, extreme risk evaluations shevlane2023model, infrastructure for computing governance, and mechanisms for making verifiable claims about AI systems brundage2020toward. % and other governance-oriented research directions presented in \\Ssec:ai-tech-for-ai-reg. Emphasis on Social Complexities and Moral Values As AI systems become increasingly integrated into society abbass2019social, alignment ceases to be only a single-agent problem and becomes a social problem. Here, the meaning of social is three-fold. enumerate[left=0.3cm] \\item Alignment research in multi-agent settings featuring the interactions between multiple AI systems and multiple humans critch2020ai,liu2024training. This includes how AI systems can receive granular feedback from realistic simulated societies, ensuring consistency in training scenarios and among multiple entities (i.e., multi-agent, multiple AI systems, and multiple humans) , which not only aids in generalizing AI systems in multi-entity settings but also helps avoid problems associated with RL liu2024training. \\item Incorporating human moral and social values into alignment (see \\Ssec:values-in-intro and \\Ssec:human-values-alignment), which is closely linked to the field of machine ethics and value alignment gabriel2020artificial,gabriel2021challenge. \\item Modeling and predicting the impacts of AI systems on society, which requires methods to approach the complexities of the social system, including those from the social sciences. Examples of potentially useful methodologies include social simulation bonabeau2002agent,de2014agent,park2023generative and game theory du2023improving. enumerate",
      "origin_cites_number": 15
    },
    {
      "section_title": "Acknowledgments",
      "level": "1",
      "content": "We thank David Krueger, Anca Dragan, Alan Chan, Stephen Casper, Haoxing Du, Lawrence Chan, Johannes Treutlein, and YingShan Lei for their helpful and constructive feedback on the manuscript. We thank Yi Qu for the graphical design and refinement of the figures in our survey. \\clearpage",
      "origin_cites_number": 0
    }
  ],
  "literature_review_id": 264743032,
  "meta_info": {
    "cite_counts": 840,
    "Conference_journal_name": "ArXiv",
    "influentialcitationcount": 12,
    "Author_info": {
      "Publicationsh": 16,
      "h_index": 12,
      "Citations": 2792,
      "Highly Influential Citations": 0
    },
    "all_cites_title": [
      "2 Interpretability Interpretability Intrinsic Interpretability Modifying Model Components",
      "Visualization [223; 805; 673; 520; 366; 497; 521; 379; 538; 125; 606; 536] Perturbation and Ablation [496; 823; 599; 598] Mapping and Editing Learned Representations [52; 415; 249; 696;",
      "Challenges and Criticisms Superposition",
      "Focusing on Toy Model Experiments [541; 650;",
      "Lack of Bench- marking [443; 598;",
      "Emphasis on Policy Relevance Alignment research does not live in a vacuum but in an ecosystem 43 , with participation from researchers, industry actors, governments, and non-governmental organizations. Research serving the needs of the AI alignment and safety ecosystem would therefore be useful. Such needs include solving the key barriers to various governance schemes, for example, extreme risk evaluations",
      "This includes how AI systems can receive granular feedback from realistic simulated societies, ensuring consistency in training scenarios and among multiple entities (i.e., multi-agent, multiple AI systems, and multiple humans) , which not only aids in generalizing AI systems in multi-entity settings but also helps avoid problems associated with RL",
      "Incorporating human moral and social values into alignment (see 1.2.3 and 4.3), which is closely linked to the field of machine ethics and value alignment",
      "Modeling and predicting the impacts of AI systems on society, which requires methods to approach the complexities of the social system, including those from the social sciences",
      "Social integration of artificial intelligence: functions, automation allocation logic and human-autonomy trust",
      "Apprenticeship learning via inverse reinforcement learning",
      "Moral foundations of large language models",
      "Reinforcement learning as a framework for ethical decision making",
      "Artificial intelligence, automation, and work",
      "A survey of inverse reinforcement learning",
      "Recommender systems, ground truth, and preference pollution",
      "Reinforcement learning based recommender systems: A survey",
      "From reinforcement learning to deep reinforcement learning: An overview",
      "Ai safety summit 2023: Roundtable chairs' summaries, 1 november",
      "why-ai-alignment-could-be-hard-with-modern-deep-learning",
      "Preference-based policy learning",
      "Active preference learning-based reinforcement learning",
      "Coordinated pausing: An evaluation-based coordination scheme for frontier ai developers",
      "Understanding intermediate layers using linear classifier probes",
      "A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems",
      "Becoming: Basic considerations for a psychology of personality",
      "Towards robust interpretability with self-explaining neural networks",
      "Power to the people: The role of humans in interactive machine learning",
      "Learning from human preferences",
      "Concrete problems in ai safety",
      "Towards better understanding of gradient-based attribution methods for deep neural networks",
      "Frontier ai regulation: Managing emerging risks to public safety",
      "Towards machine ethics: Implementing two action-based ethical theories",
      "The status of machine ethics: a report from the aaai symposium. Minds and Machines",
      "Language models as agent models",
      "Anthropic. 2022. Softmax linear units. https://transformer-circuits.pub/2022/solu/in dex.html.",
      "Anthropic. 2023a. Anthropic's responsible scaling policy",
      "Anthropic. 2023b. Circuits updates -july 2023. https://transformer-circuits.pub/2023/j uly-update/index.html.",
      "2023c. Model card and evaluations for claude models",
      "Sycophancy to subterfuge: Investigating reward tampering in language models",
      "Foundational challenges in assuring alignment and safety of large language models",
      "Update on ARC's recent eval efforts",
      "Tabnet: Attentive interpretable tabular learning",
      "Invariant risk minimization",
      "Toward ethical robots via mechanized deontic logic",
      "Stuart Armstrong. 2019. problems with ai debate. https://www.alignmentforum.org/posts/f NTCveSa4HvqvZR2F/problems-with-ai-debate.",
      "Racing to the precipice: a model of artificial intelligence development",
      "Linear algebraic structure of word senses",
      "A survey of inverse reinforcement learning: Challenges, methods and progress",
      "Social choice and individual values",
      "A general language assistant as a laboratory for alignment",
      "Feedback systems: an introduction for scientists and engineers",
      "Karl Johan strm and Bjrn Wittenmark. 2008. Adaptive control. Courier Corporation.",
      "Evolution of basic human values orientations: An application of monitoring changes in cluster solutions",
      "Atomium-EISMD. 2023. Ai4people. https://www.eismd.eu/ai4people.",
      "Global overview of imitation learning",
      "Jean-Franois Bonnefon, and Iyad Rahwan. 2018. The moral machine experiment",
      "A general theoretical paradigm to understand learning from human preferences",
      "Neural machine translation by jointly learning to align and translate",
      "Recent advances in adversarial training for adversarial robustness",
      "2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "Harmlessness from ai feedback",
      "A framework for behavioural cloning",
      "Learning from physical human corrections, one feature at a time",
      "Video pretraining (vpt): Learning to act by watching unlabeled online videos",
      "Matt Botvinick, et al. 2022. Fine-tuning language models to find agreement among humans with diverse preferences",
      "Robot see, robot do: An overview of robot imitation",
      "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
      "Revisiting model stitching to compare neural representations",
      "debate-update-obfuscated-arguments-problem",
      "A survey on artificial intelligence assurance",
      "Recognition in terra incognita",
      "Imitation learning by estimating expertise of demonstrators",
      "Probing classifiers: Promises, shortcomings, and advances",
      "Ai fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias",
      "Eliciting latent predictions from transformers with the tuned lens",
      "Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. 2009. Robust optimization, volume 28. Prince- ton university press.",
      "On the dangers of stochastic parrots: Can language models be too big?",
      "How rogue ais may arise",
      "Elon Musk, and Future of Life Institute. 2023. Pause giant ai experiments: An open letter",
      "Formalizing convergent instrumental goals",
      "Loss surface simplexes for mode connecting volumes and fast ensembling",
      "A prompt array keeps the bias away: Debiasing vision-language models with adversarial learning",
      "Ilastik: interactive machine learning for (bio) image analysis",
      "Taken out of context: On measuring situational awareness in llms",
      "A convex framework for fair regression",
      "Fairness in criminal justice risk assessments: The state of the art",
      "A declarative modular framework for representing and applying ethical principles",
      "Beyond IID: data-driven decision-making in heterogeneous environments",
      "writeup-progress-on-ai-safety-via-debate-1",
      "Unrestricted adversarial examples via semantic manipulation",
      "Accurate medium-range global weather forecasting with 3d neural networks",
      "Safety assurance mechanisms of collaborative robotic systems in manufacturing",
      "Bipartisan framework for u.s. ai act",
      "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "Agent-based modeling: Methods and techniques for simulating human systems. Proceedings of the national academy of sciences",
      "The superintelligent will: Motivation and instrumental rationality in advanced artificial agents",
      "Existential risk prevention as global priority",
      "Global catastrophic risks",
      "David Lie, and Nicolas Papernot. 2021. Machine unlearning",
      "Measuring progress on scalable oversight for large language models",
      "Beyond shared autonomy: Joint perception and action for humanin-the-loop mobile robot navigation systems",
      "Rank analysis of incomplete block designs: I. the method of paired comparisons",
      "Handbook of computational social choice",
      "Towards monosemanticity: Decomposing language models with dictionary learning",
      "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016. Openai gym. arXiv preprint arXiv:1606.01540.",
      "Safe imitation learning via fast bayesian reward inference from preferences",
      "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations",
      "Language models are few-shot learners",
      "Toward trustworthy ai development: mechanisms for supporting verifiable claims",
      "Sparks of artificial general intelligence: Early experiments with gpt-4",
      "Deep reinforcement learning from hierarchical weak preference feedback",
      "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision",
      "Discovering latent knowledge in language models without supervision",
      "Reinforcement learning for control: Performance, stability, and deep approximators",
      "Scaling data-driven robotics with reward sketching and batch reinforcement learning",
      "Zeno: An interactive framework for behavioral evaluation of machine learning",
      "High-dimension human value representation in large language models",
      "Center for ai safety: Statement on ai risk",
      "Semantics derived automatically from language corpora contain human-like biases",
      "Advancing impact assessment for intelligent systems",
      "'deepfake' scam in china fans worries over ai-driven fraud",
      "Are aligned neural networks adversarially aligned? Advances in Neural Information Processing Systems",
      "Is power-seeking ai an existential risk",
      "Increasing robotic wheelchair safety with collaborative control: Evidence from secondary task experiments",
      "Unlabeled data improves adversarial robustness",
      "Teaching large language models to zip their lips",
      "Andres Carranza, Dhruv Pai, Rylan Schaeffer, Arnuv Tandon, and Sanmi Koyejo. 2023. Deceptive alignment monitoring.",
      "Characterizing manipulation from ai systems",
      "Estimating and penalizing induced preference shifts in recommender systems",
      "Shan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah. 2019. Activation atlas. Distill, 4(3):e15.",
      "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
      "Diogo V Carvalho, Eduardo M Pereira, and Jaime S Cardoso. 2019. Machine learning interpretability: A survey on methods and metrics. Electronics, 8(8):832.",
      "Moving Forward: 11th post of The Engineer's Interpretability Sequence",
      "Red teaming deep neural networks with feature synthesis tools",
      "Dorsa Sadigh, and Dylan Hadfield-Menell. 2023b. Open problems and fundamental limitations of reinforcement learning from human feedback",
      "Explore, establish, exploit: Red teaming language models from scratch",
      "Robust feature-level adversaries are interpretability tools",
      "Evaluation of text generation: A survey",
      "A survey on adversarial attacks and defences",
      "PARL: A unified framework for policy alignment in reinforcement learning",
      "Harms from increasingly agentic algorithmic systems",
      "The ieee global initiative on ethics of autonomous and intelligent systems",
      "An ai challenge: Balancing open and closed systems",
      "Transformer interpretability beyond attention visualization",
      "2024a. Learning from natural language feedback",
      "Can LLM-generated misinformation be detected?",
      "Evaluating large language models trained on code",
      "Shouhong Ding, and Wenqiang Zhang. 2024b. Contentbased unrestricted adversarial attack",
      "Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples",
      "Graded multilabel classification: The ordinal case",
      "Label ranking methods based on the plackett-luce model",
      "Predicting partial orders: ranking with abstention",
      "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "On the weaknesses of reinforcement learning for neural machine translation",
      "The alignment problem: Machine learning and human values",
      "What failure looks like",
      "Paul Christiano. 2022. Approval-directed agents. https://www.alignmentforum.org/posts/7 Hr8t6xwuuxBTqADK/approval-directed-agents-1.",
      "Thoughts on the impact of rlhf research",
      "Supervising strong learners by amplifying weak experts",
      "Arc's first technical report: Eliciting latent knowledge",
      "Deep reinforcement learning from human preferences",
      "Get it in writing: Formal contracts mitigate social dilemmas in multi-agent rl",
      "A toy model of universality: Reverse engineering how networks learn group operations",
      "What does bert look at? an analysis of bert's attention",
      "Ruby RobertM GPT-4 Claude. 2023. New lw feature debates",
      "Simulator with bugs",
      "Collective Intelligence Project. 2023. Introducing the collective intelligence project",
      "Moral decision making frameworks for artificial intelligence",
      "Towards automated circuit discovery for mechanistic interpretability",
      "L2 regularization for learning kernels",
      "Iterated distillation and amplification",
      "The case for aligning narrowly superhuman models",
      "Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover -AI Alignment Forum",
      "Ai research considerations for human existential safety (arches)",
      "Tasra: A taxonomy and analysis of societal-scale risks from ai",
      "Reinforcement learning in multi-agent games: Open ai gym diplomacy environment",
      "K-level reasoning for zero-shot coordination in hanabi",
      "Sparse autoencoders find highly interpretable features in language models",
      "Cooperative ai: machines must learn to find common ground",
      "Open problems in cooperative ai",
      "Knowledge neurons in pretrained transformers",
      "Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers",
      "2024a. Safesora: Towards safety alignment of text2video generation via a human preference dataset",
      "2024b. Safe rlhf: Safe reinforcement learning from human feedback",
      "Conscientious classification: A data scientist's guide to discrimination-aware classification",
      "Safeguarded ai: Constructing guaranteed safety",
      "Towards guaranteed safe ai: A framework for ensuring robust and reliable ai systems",
      "Beyond question-based biases: Assessing multimodal shortcut learning in visual question answering",
      "Aum shinrikyo: insights into how terrorists develop biological and chemical weapons",
      "Analyzing transformers in embedding space",
      "Learning Dexterous Manipulation from Exemplar Object Trajectories and Pre-Grasps",
      "Plug and play language models: A simple approach to controlled text generation",
      "Causal confusion in imitation learning",
      "Agent-based models",
      "Building safe artificial intelligence: specification, robustness, and assurance",
      "Magnetic control of tokamak plasmas through deep reinforcement learning",
      "Automated jailbreak across multiple large language model chatbots",
      "Rlprompt: Optimizing discrete text prompts with reinforcement learning",
      "Sinno Jialin Pan, and Lidong Bing. 2023b. Multilingual jailbreak challenges in large language models",
      "Formal verification of ethical choices in autonomous systems",
      "Emergent complexity and zero-shot transfer via unsupervised environment design",
      "Eraser: A benchmark to evaluate rationalized nlp models",
      "Handling divergent reference texts when evaluating table-to-text generation",
      "Goal misgeneralization in deep reinforcement learning",
      "Steps toward robust artificial intelligence",
      "RAFT: Reward ranked finetuning for generative foundation model alignment",
      "A survey for in-context learning",
      "Towards a rigorous science of interpretable machine learning",
      "Essentially no barriers in neural network energy landscape",
      "Techniques for interpretable machine learning",
      "Cooperative multi-agent learning in a complex world: challenges and solutions",
      "Improving factuality and reasoning in language models through multiagent debate",
      "Toward implementing the agent-deed-consequence model of moral judgment in autonomous vehicles",
      "Statistics of robust optimization: A generalized empirical likelihood approach",
      "On the consistency of ranking algorithms",
      "Analyzing individual neurons in pre-trained language models",
      "Hotflip: White-box adversarial examples for text classification",
      "Feeling the force: Integrating force and pose for fluent discovery through imitation learning to open medicine bottles",
      "Who's harry potter? approximate unlearning in llms",
      "Toy models of superposition",
      "A mathematical framework for transformer circuits",
      "Self-explaining ai as an alternative to interpretable ai",
      "Moral stories: Situated reasoning about norms, intents, actions, and their consequences",
      "Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. 2019a. Robustness (python library). https://github.com/MadryLab/robustness.",
      "Identifying statistical bias in dataset replication",
      "Exploring the landscape of spatial robustness",
      "Visualizing higher-layer features of a deep network",
      "Artificial intelligence and the political legitimacy of global governance",
      "Concept embedding models: Beyond the accuracy-explainability trade-off",
      "Eu ai act: first regulation on artificial intelligence",
      "Bing chat is blatantly, aggressively misaligned",
      "Avoiding wireheading with value reinforcement learning",
      "Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective",
      "Reinforcement learning with a corrupted reward channel",
      "Agi safety literature review",
      "Leave no trace: Learning to reset for safe and autonomous reinforcement learning",
      "Google's ai red team: the ethical hackers making ai safer",
      "Interactive machine learning",
      "Human-level play in the game of diplomacy by combining language models with strategic reasoning",
      "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
      "On interpretability of artificial neural networks: A survey",
      "Survey of imitation learning for robotic manipulation",
      "Discovering faster matrix multiplication algorithms with reinforcement learning",
      "Graham Neubig, et al. 2023. Bridging the gap: A survey on integrating (human) feedback for natural language generation",
      "Adoption dynamics and societal impact of ai systems in complex networks",
      "Multi-principal assistance games",
      "Implicit learning dynamics in stackelberg games: Equilibria characterization, convergence analysis, and empirical study",
      "Inverse constitutional ai: Compressing preferences into principles",
      "Pragmatic-pedagogic value alignment",
      "An ethical framework for a good ai society: Opportunities, risks, principles, and recommendations. Ethics, governance, and policies in artificial intelligence",
      "Learning to communicate with deep multi-agent reinforcement learning",
      "Input switched affine networks: An rnn architecture designed for interpretability",
      "Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks",
      "Social chemistry 101: Learning to reason about social and moral norms",
      "Linear mode connectivity and the lottery ticket hypothesis",
      "Learning to predict without looking ahead: World models without forward prediction",
      "From language to goals: Inverse reinforcement learning for vision-based instruction following",
      "Learning robust rewards with adverserial inverse reinforcement learning",
      "2018b. Variational inverse control with events: A general framework for data-driven reward definition",
      "Ai and the economy. Innovation policy and the economy",
      "Pairwise preference learning and ranking",
      "Johannes Frnkranz and Eyke Hllermeier. 2010. Preference Learning. Springer Science & Business Media.",
      "G20 ai principles",
      "Artificial intelligence, values, and alignment",
      "The challenge of value alignment: From fairer algorithms to ai safety",
      "Large-scale adversarial training for vision-and-language representation learning",
      "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
      "Scaling laws for reward model overoptimization",
      "Scaling and evaluating sparse autoencoders",
      "A comprehensive survey on safe reinforcement learning",
      "Loss surfaces, mode connectivity, and fast ensembling of dnns",
      "Logical induction",
      "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
      "Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
      "Lm-debugger: An interactive tool for inspection and intervention in transformer-based language models",
      "Transformer feed-forward layers are keyvalue memories",
      "A divergence minimization perspective on imitation learning methods",
      "Adversarial examples are a natural consequence of test error in noise",
      "Improving alignment of dialogue agents via targeted human judgements",
      "Alec Radford, and Chris Olah. 2021. Multimodal neurons in artificial neural networks",
      "Generative language models and automated influence operations: Emerging threats and potential mitigations",
      "Explaining and harnessing adversarial examples",
      "Problems of monetary management: the UK experience",
      "Gemma scope: Helping the safety community shed light on the inner workings of language models",
      "The roadmap to an effective ai assurance ecosystem -extended version",
      "Frontier ai: capabilities and risks -discussion paper",
      "Using natural language for reward shaping in reinforcement learning",
      "In ai race, microsoft and google choose speed over caution",
      "Ai control: Improving safety despite intentional subversion",
      "Policy shaping: Integrating human feedback with reinforcement learning",
      "Multi-agent deep reinforcement learning: a survey",
      "Studying large language model generalization with influence functions",
      "Ground (less) truth: A causal framework for proxy labels in human-algorithm decision-making",
      "Multiagent planning with factored mdps",
      "Reinforced self-training (rest) for language modeling",
      "Language models represent space and time",
      "Secretary-general's remarks to the security council on artificial intelligence",
      "The off-switch game",
      "Incomplete contracting and ai alignment",
      "Inverse reward design. Advances in Neural Information Processing Systems",
      "Cooperative inverse reinforcement learning",
      "The ethics of ai ethics: An evaluation of guidelines",
      "A virtue-based framework to support putting ai ethics into practice",
      "How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
      "Self-attention attribution: Interpreting information interactions inside transformer",
      "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
      "Overview of supervised learning. The elements of statistical learning: Data mining, inference, and prediction",
      "Assisted robust reward design",
      "What's in your\" safe\" data?",
      "A circuit for Python docstrings in a 4-layer attention-only transformer",
      "Contrastive preference learning: Learning from human feedback without reinforcement learning",
      "Few-Shot Preference Learning for Human-in-the-Loop RL",
      "Pragmatic ai safety",
      "Natural selection favors ais over humans",
      "2021a. The many faces of robustness: A critical analysis of outof-distribution generalization",
      "Aligning ai with shared human values",
      "Unsolved problems in ml safety",
      "Benchmarking neural network robustness to common corruptions and perturbations",
      "Gaussian error linear units (gelus)",
      "X-risk analysis for ai research",
      "Using self-supervised learning can improve model robustness and uncertainty",
      "An overview of catastrophic ai risks",
      "2021c. Natural adversarial examples",
      "Generative adversarial imitation learning",
      "International institutions for advanced ai",
      "Eliciting latent knowledge (elk) -distillation/summary",
      "An empirical analysis of compute-optimal large language model training",
      "The curious case of neural text degeneration",
      "Interactive machine learning for health informatics: when do we need the humanin-the-loop?",
      "What do we need to build explainable ai systems for the medical domain",
      "On the sensitivity of reward inference to misspecified human models",
      "Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering",
      "Ai safety and the age of dislightenment",
      "Off-belief learning",
      "other-play\" for zero-shot coordination",
      "Do large language models know about facts?",
      "Adversarial attacks on neural network policies",
      "Inner monologue: Embodied reasoning through planning with language models",
      "Catastrophic jailbreak of open-source LLMs via exploiting generation",
      "An overview of 11 proposals for building safe advanced ai",
      "Sleeper agents: Training deceptive llms that persist through safety training",
      "Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. 2019a. Deceptive alignment. https://www.alignmentforum.org/posts/zthDPAjh9w6Ytbeks/deceptive-a lignment.",
      "The inner alignment problem",
      "2019c. Risks from learned optimization in advanced machine learning systems",
      "Compositional attention networks for machine reasoning",
      "Label ranking by learning pairwise preferences",
      "Imitation learning: A survey of learning methods",
      "Reward learning from human preferences and demonstrations in atari",
      "Ai safety via debate",
      "A social reinforcement learning agent",
      "Emergent deception and emergent optimization",
      "Human-level performance in 3d multiplayer games with population-based reinforcement learning",
      "Learning trajectory preferences for manipulators via iterative improvement",
      "Reward-rational (implicit) choice: A unifying formalism for reward learning",
      "Aligner: Achieving efficient alignment through weak-to-strong correction",
      "2024b. Beavertails: Towards improved safety alignment of llm via a humanpreference dataset",
      "JiayiZhou ChangyeLi HantaoLou YaodongYang, and PKU-Alignment Team. 2024c. Language models resist alignment",
      "Survey of hallucination in natural language generation",
      "Adversarial examples for evaluating reading comprehension systems",
      "Replay-guided adversarial environment design",
      "2022a. When to make exceptions: Exploring language models as accounts of human moral judgment",
      "When to make exceptions: Exploring language models as accounts of human moral judgment",
      "Building a virtual machine inside chatgpt",
      "Automatically auditing large language models via discrete optimization",
      "Linear connectivity reveals generalization strategies",
      "Reinforcement learning: A survey",
      "Preference amplification in recommender systems",
      "Black box to white box: Discover model characteristics based on strategic probing",
      "Scaling laws for neural language models",
      "Visualizing and understanding recurrent networks",
      "The ethical algorithm: The science of socially aware algorithm design",
      "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "Alignment of language agents",
      "Threat model literature review",
      "On scalable oversight with weak llms judging strong llms",
      "Machine morality, moral progress, and the looming environmental disaster",
      "Strengthening international cooperation on ai, progress report",
      "Debating with more persuasive llms leads to more truthful answers",
      "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
      "Preference Transformer: Modeling Human Preferences using Transformers for RL",
      "Language models can solve computer tasks",
      "Reward identification in inverse reinforcement learning",
      "Learning how to explain neural networks: Patternnet and patternattribution",
      "Evaluating language-model agents on realistic autonomous tasks",
      "The flash crash: Highfrequency trading in an electronic market",
      "Examining gender and race bias in two hundred sentiment analysis systems",
      "The prism alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models",
      "Self-normalizing neural networks",
      "Planning to avoid side effects",
      "About the role of the environment in multi-agent simulations",
      "Reward (mis) design for autonomous driving",
      "Tamer: Training an agent manually via evaluative reinforcement",
      "Reinforcement learning from simultaneous human and mdp reward",
      "Learning non-myopically from human-generated reward",
      "Training a robot via human feedback: A case study",
      "Learning from human-generated reward",
      "Risk assessment at agi companies: A review of popular risk assessment techniques from other safety-critical industries",
      "Understanding black-box predictions via influence functions",
      "Openassistant conversationsdemocratizing large language model alignment",
      "Pretraining language models with human preferences",
      "Modeling human ad hoc coordination",
      "More instances about specification gaming",
      "Paradigms of ai alignment: components and enablers",
      "Gedi: Generative discriminator guided sequence generation",
      "The disagreement problem in explainable machine learning: A practitioner's perspective",
      "Against interpretability: a critical examination of the interpretability problem in machine learning",
      "Out-of-distribution generalization via risk extrapolation (rex)",
      "Hidden incentives for auto-induced distributional shift",
      "Data-efficient generalization of robot skills with contextual policy search",
      "An evaluation of the human-interpretability of explanation",
      "Human-in-theloop interpretability prior",
      "Building machines that learn and think like people",
      "Auditing the ai auditors: A framework for evaluating fairness and bias in high stakes ai predictive models",
      "When your ai deceives you: Challenges with partial observability of human evaluators in reward learning",
      "Causal Scrubbing: a method for rigorously testing interpretability hypotheses",
      "A path towards autonomous machine intelligence version 0.9",
      "Query-efficient and scalable black-box adversarial attacks on discrete sequential data via bayesian optimization",
      "Scaling reinforcement learning from human feedback with ai feedback",
      "Interactive visualization and manipulation of attention-based neural machine translation",
      "Efficient exploration via state marginal matching",
      "What if artificial intelligence become completely ambient in our daily lives? exploring future human-ai interaction through high fidelity illustrations",
      "The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities",
      "Exploiting open-endedness to solve problems through the search for novelty",
      "Igor Mordatch, and Thore Graepel. 2021. Scalable evaluation of multiagent reinforcement learning with melting pot",
      "A proposal for improving societys values",
      "Combining weak-to-strong generalization with scalable oversight",
      "b. A proposal for importing society's values",
      "Scalable agent alignment via reward modeling: a research direction",
      "Ai and biological weapons",
      "2021a. Implicit representations of meaning in neural language models",
      "Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou. 2023a. Trustworthy ai: From principles to practices. ACM Computing Surveys (CSUR), 55(9):1-46.",
      "2022a. Interpretable generative adversarial networks",
      "Tnt: An interpretable tree-network-tree learning framework using knowledge distillation",
      "2022b. Emergent world representations: Exploring a sequence model trained on a synthetic task",
      "2021b. Learning human objectives from sequences of physical corrections",
      "A review of dynamic stackelberg game models",
      "Evaluating object hallucination in large vision-language models",
      "Towards understanding and mitigating social biases in language models",
      "Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla",
      "The effect of social motives, communication and group size on behaviour in an n-person multi-stage mixed-motive game",
      "Kagnet: Knowledge-aware graph networks for commonsense reasoning",
      "2022a. Distributionally robust optimization: A review on theory and applications",
      "Inferring rewards from language in context",
      "2022c. Truthfulqa: Measuring how models mimic human falsehoods",
      "Sparse dictionary learning by dynamical neural networks",
      "LLM-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models",
      "The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery",
      "Pseudoclick: Interactive Image Segmentation with Click Imitation",
      "2024a. Training socially aligned language models on simulated social interactions",
      "Visual interrogation of attention-based models for natural language inference and machine comprehension",
      "Yuxiao Dong, and Jie Tang. 2024b. Agentbench: Evaluating LLMs as agents",
      "Adversarial training for large neural language models",
      "Jailbreaking chatgpt via prompt engineering: An empirical study",
      "Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning",
      "Physical interaction as communication: Learning robot objectives online from human corrections",
      "Multiagent actor-critic for mixed cooperative-competitive environments",
      "Mechanistic mode connectivity",
      "A rigorous study of integrated gradients method and extensions to internal neuron attributions",
      "Learning latent plans from play",
      "Travis Armstrong, and Pete Florence. 2023. Interactive language: Talking to robots in real time",
      "Shadow attacks: automatically evading system-call-behavior based malware detection",
      "Elign: Expectation alignment as a multi-agent intrinsic reward",
      "Aligning ai regulation to sociotechnical change. Oxford Handbook on AI Governance",
      "Visualizing data using t-sne",
      "Interactive learning from policy-dependent human feedback",
      "Towards deep learning models resistant to adversarial attacks",
      "Post-hoc interpretability for neural nlp: A survey",
      "Introducing democratic fine-tuning",
      "Faster sorting algorithms discovered using deep reinforcement learning",
      "Governance, risk, and artificial intelligence",
      "A future that works: Ai, automation, employment, and productivity",
      "Enhance the visual representation via discrete adversarial training",
      "Simulation-based optimization of markov reward processes",
      "Deep learning: A critical appraisal",
      "Transparency by design: Closing the gap between performance and interpretability in visual reasoning",
      "Social values and rules of fairness: A theoretical perspective. Cooperation and helping behavior",
      "The hydra effect: Emergent self-repair in language model computations",
      "Social diversity and social preferences in mixed-motive reinforcement learning",
      "On the fragility of learned reward functions",
      "The risks associated with artificial general intelligence: A systematic review",
      "A survey on bias and fairness in machine learning",
      "Fairness, accountability, transparency, and ethics (fate) in artificial intelligence (ai), and higher education: A systematic review. Computers and Education: Artificial Intelligence",
      "Alex Andonian, and Yonatan Belinkov. 2022a. Locating and editing factual associations in gpt",
      "Mass-editing memory in a transformer",
      "A comment on the ida-alphagozero metaphor; capabilities versus alignment",
      "Formal verication of ethical properties in multiagent systems",
      "Motivational bases of choice in experimental games",
      "Meta and microsoft introduce the next generation of llama",
      "What do nlp researchers believe? results of the nlp community metasurvey",
      "6of4Rb2TgpRD/the-ai-debate-debate",
      "Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence",
      "Recent advances in natural language processing via large pre-trained language models: A survey",
      "Can LLMs keep a secret? testing privacy implications of language models via contextual integrity theory",
      "The threat of offensive ai to organizations",
      "Human-level control through deep reinforcement learning",
      "Model-based reinforcement learning: A survey",
      "A multidisciplinary survey and framework for design and evaluation of explainable ai systems",
      "Auditing large language models: a three-layered approach",
      "On the importance of single directions for generalization",
      "Inceptionism: Going deeper into neural networks",
      "Democratizing ai, stable diffusion & generative models",
      "Equivariant networks for zero-shot coordination",
      "Evyk8eb6b7tFd6pxJ/iterated-distillation-amplification-gato-a nd-proto-agi-re",
      "Situational awareness: techniques, challenges, and prospects",
      "Probabilistic Machine Learning: Advanced Topics",
      "Social value orientation: Theoretical and measurement issues in the study of social preferences",
      "Measuring social value orientation",
      "A conversation-based perspective for shaping ethical human-machine interactions: The particular challenge of chatbots",
      "Stereoset: Measuring stereotypical bias in pretrained language models",
      "Webgpt: Browser-assisted question-answering with human feedback",
      "2023a. Attribution patching: Activation patching at industrial scale",
      "Othello-gpt: Future work i am excited about",
      "Progress measures for grokking via mechanistic interpretability",
      "Crows-pairs: A challenge dataset for measuring social biases in masked language models",
      "Decentralized stochastic control with partial history sharing: A common information approach",
      "Order in the court: Explainable ai methods prone to disagreement",
      "Policy invariance under reward transformations: Theory and application to reward shaping",
      "Algorithms for inverse reinforcement learning",
      "Agi safety from first principles",
      "Richard Ngo. 2021. /why i m excited about debate. https://www.alignmentforum.org/posts /LDsSqXf9Dpu3J3gHD/why-i-m-excited-about-debate.",
      "The alignment problem from a deep learning perspective: A position paper",
      "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks",
      "My understanding of paul christiano's iterated amplification al safety research agenda",
      "2021. f-irl: Inverse reinforcement learning via state marginal matching",
      "The black swan: the impact of the highly improbable",
      "Algorithms of oppression",
      "Ai regulation through an intergenerational lens",
      "A voting-based system for ethical decision making",
      "Bias in datadriven artificial intelligence systems-an introductory survey",
      "OecdAI. 2021. Ai principles. https://oecd.ai/en/dashboards/ai-principles/P8.",
      "Approval-directed agency and the decision theory of newcomb-like problems",
      "Safe pareto improvements for delegated game playing",
      "Visualizing mnist: An exploration of dimensionality reduction",
      "Visualizing representations: Deep learning and human beings",
      "Chris Olah. 2023. Interpretability dreams. https://transformer-circuits.pub/2023/inter pretability-dreams/index.html#larger-scale.",
      "Zoom in: An introduction to circuits",
      "The building blocks of interpretability",
      "Chris Olah et al. 2017. Feature visualization. Distill.",
      "-context learning and induction heads",
      "The basic ai drives",
      "OpenAI. 2021a. Curve detectors",
      "OpenAI. 2021b. Weight banding",
      "OpenAI. 2023a. Gpt-4 technical report",
      "2023b. Gpt-4v(ision) system card",
      "2023c. Introducing superalignment",
      "Committing to bridging the digital divide in least developed countries",
      "A review of cooperative multi-agent deep reinforcement learning",
      "An algorithmic perspective on imitation learning",
      "Training language models to follow instructions with human feedback",
      "How to catch an AI liar: Lie detection in black-box LLMs by asking unrelated questions",
      "Learning reward functions by integrating human demonstrations and preferences",
      "The effects of reward misspecification: Mapping and mitigating misaligned models",
      "Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark. ICML",
      "2023b. Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark",
      "Modeling and mitigating human annotation errors to design efficient stream processing systems with human-in-the-loop machine learning",
      "Framework on ethical aspects of artificial intelligence, robotics and related technologies",
      "Towards the science of security and privacy in machine learning",
      "Senthil Purushwalkam, and Abhinav Gupta. 2022. The unsurprising effectiveness of pre-trained vision models for control",
      "Generative agents: Interactive simulacra of human behavior",
      "2023b. Ai deception: A survey of examples, risks, and potential solutions",
      "Bbq: A hand-built bias benchmark for question answering",
      "Data and its (dis) contents: A survey of dataset development and use in machine learning research",
      "A deep reinforced model for abstractive summarization",
      "Asleep at the keyboard? assessing the security of github copilot's code contributions",
      "Nvidia ai red team: An introduction",
      "Investigations of performance and bias in human-ai teamwork in hiring",
      "Performative prediction",
      "Lus Moniz Pereira, and Ari Saptawijaya. 2016a. Bridging two realms of machine ethics. Programming machine ethics",
      "Programming machine ethics",
      "Red teaming language models with language models",
      "Discovering language model behaviors with modelwritten evaluations",
      "A multi-agent reinforcement learning model of common-pool resource appropriation",
      "Evan hubinger on inner alignment, outer alignment, and proposals for building safe advanced ai",
      "Causal inference using invariant prediction: identification and confidence intervals",
      "Elements of causal inference: foundations and learning algorithms",
      "Investigating emergent goal-like behaviour in large language models using experimental economics",
      "Robust adversarial reinforcement learning",
      "Robust solutions to stackelberg games: Addressing bounded rationality and limited observations in human cognition",
      "Deep networks on toroids: removing symmetries reveals the structure of flat regions in the landscape geometry",
      "The analysis of permutations",
      "Efficient training of artificial neural networks for autonomous navigation",
      "The logic of scientific discovery",
      "Robustness and generalization via generative adversarial training",
      "Grokking: Generalization beyond overfitting on small algorithmic datasets",
      "Early stopping-but when?",
      "Dale Purves, George J Augustine, David Fitzpatrick, Lawrence C Katz, Anthony-Samuel LaMantia, James O McNamara, and S Mark. Williams. 2001. Neuroscience, 2nd edition. Sinauer Associates.",
      "Markov decision processes: discrete stochastic dynamic programming",
      "Fine-tuning aligned language models compromises safety, even when users do not intend to",
      "Rethinking information structures in rlhf: Reward generalization from a graph theory perspective",
      "Invariant visual representation by single neurons in the human brain",
      "Question decomposition improves the faithfulness of model-generated reasoning",
      "Direct preference optimization: Your language model is secretly a reward model",
      "Distributionally robust optimization: A review",
      "Selfexplain: A selfexplaining architecture for neural text classifiers",
      "Saving face: Investigating the ethical concerns of facial recognition auditing",
      "Microsoft ai red team building future of safer ai",
      "Bayesian inverse reinforcement learning",
      "Toward transparent ai: A survey on interpreting the inner structures of deep neural networks",
      "Linear adversarial concept erasure",
      "Recent advances in robot learning from demonstration",
      "Probing the probing paradigm: Does probing accuracy entail task relevance?",
      "Do imagenet classifiers generalize to imagenet",
      "Learning human objectives by evaluating hypothetical behavior",
      "Sqil: Imitation learning via reinforcement learning with sparse rewards",
      "Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gmez Colmenarejo, Alexander Novikov, Gabriel Barth- maron, Mai Gimnez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. 2022. A generalist agent. Transactions on Machine Learning Research.",
      "Visualizing and measuring the geometry of bert",
      "Safetywashing: Do ai safety benchmarks actually measure safety progress? arXiv preprint",
      "Generating natural language adversarial examples on a large scale with generative models",
      "Gradient hacking",
      "Attentionbased interpretability with concept transformers",
      "Delusion, survival, and intelligent agents",
      "A survey on data collection for machine learning: a big data-ai integration perspective",
      "The nature of human values",
      "Solid: A large-scale semi-supervised dataset for offensive language identification",
      "The neural lasso: Local linear sparsity for interpretable explanations",
      "A reduction of imitation learning and structured prediction to no-regret online learning",
      "A Short Introduction to Preferences: Between AI and Social Choice",
      "grabcut\" interactive foreground extraction using iterated graph cuts",
      "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
      "Gender bias in coreference resolution",
      "2021a. Key concepts in ai safety: Interpretability in machine learning",
      "2021b. Key concepts in ai safety: Robustness and adversarial examples",
      "Human compatible: Artificial intelligence and the problem of control",
      "Research priorities for robust and beneficial artificial intelligence",
      "Data distillation: A survey",
      "The evolution of cooperation",
      "Active preference-based learning of reward functions",
      "Distributionally robust neural networks",
      "A survey of evaluation metrics used for nlg systems",
      "Aequitas: A bias and fairness audit toolkit",
      "Explainable AI: interpreting, explaining and visualizing deep learning",
      "Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools",
      "A situation awareness-based framework for design and evaluation of explainable ai",
      "Whose opinions do language models reflect",
      "Why we must consider the intergenerational impacts of ai",
      "Behavioral cloning from noisy demonstrations",
      "Self-critiquing models for assisting human evaluators",
      "How do fairness definitions fare? examining public attitudes towards algorithmic definitions of fairness",
      "Learning from demonstration",
      "Is imitation learning the route to humanoid robots?",
      "Evaluating the moral beliefs encoded in llms",
      "Explaining neural networks by decoding layer activations",
      "The moral choice machine",
      "Towards best practices in agi safety and governance: A survey of expert opinion",
      "Proximal policy optimization algorithms",
      "Replicator dynamics",
      "Universals in the content and structure of values: Theoretical advances and empirical tests in 20 countries",
      "Are there universal aspects in the structure and contents of human values? Journal of social issues",
      "Open-sourcing highly capable foundation models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives",
      "Against almost every theory of impact of interpretability",
      "Social choice theory. Handbook of mathematical economics",
      "Segregation dynamics with reinforcement learning and agent based modeling",
      "Benefits of assistance over reward learning",
      "More examples of gmg",
      "Scalable and transferable black-box jailbreaks for language models via persona modulation",
      "Colorfool: Semantic adversarial colorization",
      "Is this paper dangerous? balancing secrecy and openness in counterterrorism",
      "Towards understanding sycophancy in language models",
      "Correcting robot plans with natural language feedback",
      "Videodex: Learning dexterity from internet videos",
      "Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
      "Defining and detecting toxicity on social media: context and knowledge are key",
      "Model evaluation for extreme risks",
      "How to diversify conceptual alignment: the model behind refine",
      "Benchmarks and algorithms for offline preferencebased reward learning",
      "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
      "Meta-level adversarial evaluation of oversight techniques might allow robust measurement of their adequacy",
      "Why so toxic? measuring and triggering toxic behavior in open-domain chatbots",
      "Dual rl: Unification and new methods for reinforcement and imitation learning",
      "Mastering the game of go with deep neural networks and tree search",
      "Mastering the game of go without human knowledge",
      "Reward is enough",
      "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "Learning when to communicate at scale in multiagent cooperative and competitive tasks",
      "Norms as a basis for governing sociotechnical systems",
      "Defining and characterizing reward gaming",
      "Invariance in policy optimisation and partial identifiability in reward learning",
      "The value learning problem",
      "Aligning superintelligence with human interests: A technical research agenda",
      "Corrigibility. In Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence",
      "Agent foundations for aligning machine intelligence with human interests: a technical research agenda. The technological singularity: Managing the journey",
      "Can large language models democratize access to dual-use biotechnology?",
      "Release strategies and the social impacts of language models",
      "Dorsa Sadigh, and Stefano Ermon. 2018a. Multi-agent generative adversarial imitation learning",
      "Constructing unrestricted adversarial examples with generative models",
      "What overarching ethical principle should a superintelligent ai follow?",
      "Ai model gpt-3 (dis)informs us better than humans",
      "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
      "Beyond memorization: Violating privacy via inference with large language models",
      "Assessing the ethical and social concerns of artificial intelligence in neuroinformatics research: An empirical test of the european union assessment list for trustworthy ai (altai)",
      "2022. expert survey on progress in ai",
      "m-and-short-term-challenges-to-ensuring-the-safety-of-ai-systems, title = Long-Term and Short-Term Challenges to Ensuring the Safety of AI Systems",
      "Why robustness is key to deploying ai",
      "Learning to summarize with human feedback",
      "Ad hoc autonomous agent teams: Collaboration without pre-coordination",
      "Seq2seq-vis: A visual debugging tool for sequence-to-sequence models",
      "Toward harnessing user feedback for machine learning",
      "Interacting meaningfully with machine learning systems: Three experiments",
      "Learning rewards from linguistic feedback",
      "The bletchley declaration by countries attending the ai safety summit, 1-2",
      "Scaling up multiagent reinforcement learning for robotic systems: Learn an adaptive sparse communication graph",
      "Principle-driven self-alignment of language models from scratch with minimal human supervision",
      "Value-decomposition networks for cooperative multi-agent learning based on team reward",
      "Trafficsim: Learning to simulate realistic multi-agent behaviors",
      "Reinforcement learning: An introduction",
      "Ethically compliant sequential decision making",
      "Software that monitors students during tests perpetuates inequality and violates their privacy",
      "Apprenticeship learning using linear programming",
      "Intriguing properties of neural networks",
      "The global governance of artificial intelligence: Next steps for empirical and normative research",
      "Robustifying reinforcement learning agents via action space adversarial training",
      "Multi-agent reinforcement learning: Independent vs. cooperative agents",
      "Stanford alpaca: An instruction-following llama model",
      "Active learning in robotics: A review of control principles",
      "Provably safe systems: the only path to controllable agi",
      "Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner, Callum McDougall, Monte MacDiarmid, Alex Tamkin, Esin Durmus, Tristan Hume, Francesco Mosconi, C. Daniel Freeman, Theodore R Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. 2024. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits.",
      "Fact sheet: Biden-harris administration secures voluntary commitments from leading artificial intelligence companies to manage the risks posed by ai",
      "Teachable robots: Understanding human teaching behavior to build more effective robot learners",
      "An effective baseline for robustness to distributional shift",
      "Causal confusion and reward misidentification in preference-based reward learning",
      "Domain randomization for transferring deep neural networks from simulation to the real world",
      "Implementations in machine ethics: A survey",
      "Behavioral cloning from observation",
      "Open foundation and fine-tuned chat models",
      "International governance of civilian ai: A jurisdictional certification approach",
      "A new formalism, method and open issues for zero-shot coordination",
      "Multi-label classification: An overview",
      "Classification of global catastrophic risks connected with artificial intelligence",
      "Inner and outer alignment decompose one hard problem into two extremely hard problems",
      "Avoiding side effects in complex environments",
      "Optimal policies tend to seek power",
      "Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting",
      "Recommendation on the ethics of artificial intelligence",
      "What is ai capability control & why does it matter?",
      "Population of global offline continues steady decline to 2.6 billion people in 2023",
      "Dual use of artificial-intelligencepowered drug discovery",
      "Development of prosocial, individualistic, and competitive orientations: theory and preliminary evidence",
      "Principles of risk minimization for learning theory",
      "Attention interpretability across nlp tasks",
      "Ajit Kumar Verma, Srividya Ajit, Durga Rao Karanki, et al. 2010. Reliability and safety engineering, vol- ume 43. Springer.",
      "Fairness definitions explained",
      "Specification gaming: the flip side of ai ingenuity",
      "A multiscale visualization of attention in the transformer model",
      "The role of artificial intelligence in achieving the sustainable development goals",
      "Universal adversarial triggers for attacking and analyzing nlp",
      "The instruction hierarchy: Training llms to prioritize privileged instructions",
      "Beyond reverse KL: Generalizing direct preference optimization with diverse divergence constraints",
      "Improving neural language modeling via adversarial training",
      "Falling rule lists",
      "Interpretable image recognition by constructing transparent embedding space",
      "2023a. Learning robotic insertion tasks from human demonstration",
      "Interpretability in the wild: a circuit for indirect object identification in gpt-2 small",
      "A survey on large language model based autonomous agents",
      "Poet: open-ended coevolution of environments and their optimized solutions",
      "Emergent prosociality in multi-agent games through gifting",
      "2023c. Self-instruct: Aligning language models with self-generated instructions",
      "Towards faithful neural table-to-text generation with content-matching constraints",
      "Mind the gap: A balanced corpus of gendered ambiguous pronouns",
      "Jailbroken: How does llm safety training fail?",
      "Chain-of-thought prompting elicits reasoning in large language models",
      "Evolutionary game theory",
      "Using the veil of ignorance to align ai systems with principles of justice",
      "2023a. Adversarial attacks on llms",
      "Llm powered autonomous agents",
      "The future of work: Robots, AI, and automation",
      "Towards ai-complete question answering: A set of prerequisite toy tasks",
      "Ensuring safe, secure, and trustworthy ai",
      "Emergence of maps in the memories of blind navigation agents",
      "Existential risk from artificial general intelligence",
      "Evolution of digital organisms at high mutation rates leads to survival of the flattest",
      "Machine ethics: The design and governance of ethical ai and autonomous systems [scanning the issue]",
      "A survey of preferencebased reinforcement learning methods",
      "Preference-based reinforcement learning: A preliminary survey",
      "Recursively summarizing books with human feedback",
      "A low-cost ethics shaping approach for designing reinforcement learning agents",
      "Fine-grained human feedback gives better rewards for language model training",
      "Ex machina: Personal attacks seen at scale",
      "The rise and potential of large language model based agents: A survey",
      "Defending chatgpt against jailbreak attack via self-reminders",
      "Fairgan: Fairness-aware generative adversarial networks",
      "Recipes for safety in open-domain chatbots",
      "Bot-adversarial dialogue for safe conversational agents",
      "A semantic loss function for deep learning with symbolic knowledge",
      "Deep interactive object selection",
      "Trail: Near-optimal imitation learning with suboptimal data",
      "Pieter Abbeel, and Dale Schuurmans. 2023a. Foundation models for decision making: Problems, methods, and opportunities",
      "2023b. Shadow alignment: The ease of subverting safely-aligned language models",
      "Improving out-of-domain generalization with domain relations",
      "Rt-2: New model translates vision and language into action",
      "Low-resource languages jailbreak gpt-4",
      "Towards improving adversarial training of nlp models",
      "Reinforcement learning in healthcare: A survey",
      "Building ethics into artificial intelligence",
      "Yuval Tassa, and Fei Xia. 2023. Language to rewards for robotic skill synthesis",
      "Towards sample efficient reinforcement learning",
      "Rrhf: Rank responses to align language models with human feedback",
      "In situ bidirectional human-robot value alignment",
      "Challenges to christiano's capability amplification proposal",
      "A survey of crowdsourcing systems",
      "Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors",
      "Predicting the type and target of offensive posts in social media",
      "Word-level textual adversarial attacking as combinatorial optimization",
      "Democratic inputs to ai",
      "Visualizing and understanding convolutional networks",
      "Why we released grover",
      "Mitigating unwanted biases with adversarial learning",
      "mixup: Beyond empirical risk minimization",
      "Visual interpretability for deep learning: a survey",
      "Interpretable convolutional neural networks",
      "Interactive object segmentation with inside-outside guidance",
      "Deep imitation learning for complex manipulation tasks from virtual reality teleoperation",
      "2023a. The wisdom of hindsight makes language models better instruction followers",
      "2023b. Discriminator-guided model-based offline imitation learning",
      "Distilling structured knowledge into embeddings for explainable and accurate recommendation",
      "2023c. Siren's song in the ai ocean: A survey on hallucination in large language models",
      "Constructing highly inductive contexts for dialogue safety through controllable reverse generation",
      "Gender bias in coreference resolution: Evaluation and debiasing methods",
      "A survey of large language models",
      "On evaluating adversarial robustness of large vision-language models",
      "Improving generalization of alignment with human preferences through group invariant learning",
      "Improving the robustness of deep neural networks via stability training",
      "Revisiting the importance of individual units in cnns via ablation",
      "Lima: Less is more for alignment",
      "Domain generalization: A survey",
      "Inverse reinforcement learning with natural language goals",
      "Machine learning",
      "The ingredients of real world robotic reinforcement learning",
      "Dyval: Graph-informed dynamic evaluation of large language models",
      "Consequences of misaligned ai",
      "Maximum entropy inverse reinforcement learning",
      "Adversarial training for high-stakes reliability",
      "Fine-tuning language models from human preferences",
      "The moral integrity corpus: A benchmark for ethical dialogue systems",
      "Representation engineering: A top-down approach to ai transparency",
      "Matt Fredrikson, and Dan Hendrycks. 2024a. Improving alignment and robustness with circuit breakers",
      "2023b. Universal and transferable adversarial attacks on aligned language models",
      "Segment everything everywhere all at once"
    ]
  }
}