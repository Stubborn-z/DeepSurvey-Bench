{
  "survey": "AI alignment is crucial for ensuring artificial intelligence systems act in accordance with human values and intentions, thereby preventing potential harms associated with misalignment. This survey provides a comprehensive examination of AI alignment, safety, ethics, and governance, emphasizing the interconnected domains necessary for responsible AI development. The survey systematically explores the importance of AI alignment in mitigating catastrophic risks and highlights the necessity for benchmarks to evaluate ethical knowledge in AI systems. It delves into AI safety by reviewing metrics for measuring hallucination, mitigation methods, and research progress in specific tasks. Ethical considerations are addressed through the study of biases, fairness, and transparency, underscoring the need for principle-based approaches to AI alignment. The survey also evaluates AI governance frameworks, emphasizing international collaboration as a key component in developing comprehensive regulatory strategies. Challenges such as scalability of governance structures and the enforcement of compliance across jurisdictions are discussed, suggesting ongoing research to refine these frameworks. Overall, the survey underscores the importance of integrating insights from diverse research areas to ensure the responsible deployment of AI technologies that align with societal values and expectations. Future research directions include enhancing human-AI collaboration, refining multiagent systems, and leveraging weak supervision to improve alignment of superhuman models.\n\nIntroduction Importance of AI Alignment AI alignment is essential for ensuring that artificial intelligence systems operate in harmony with human values and intentions, thus mitigating potential risks associated with misalignment [1]. As AI technology progresses, particularly with frontier models, understanding catastrophic risks is critical for informing mitigation strategies, underscoring the necessity of AI alignment in responsible development. The establishment of benchmarks for evaluating ethical knowledge in AI is vital for ensuring that these technologies adhere to societal norms and values [2]. The development of learning trajectories that reflect user preferences, especially in tasks like object manipulation, highlights the importance of creating AI systems consistent with human intentions [3]. Furthermore, enhancing language representation models for natural language processing requires a bidirectional context understanding, crucial for aligning AI with human communication standards [4]. The pretraining of language models on potentially harmful internet text further emphasizes the need for AI alignment to prevent the propagation of biased or harmful information [5]. Training AI systems to be harmless and effective, independent of human labels for identifying harmful outputs, is a critical aspect of alignment, ensuring that AI serves humanity's best interests without unintended consequences [6]. Additionally, understanding deceptive behaviors in AI, particularly in large language models, is vital for ensuring these systems align with human values [7]. Scope and Structure of the Survey This survey is systematically organized to provide a comprehensive examination of AI alignment, focusing on the interconnected domains of safety, ethics, and governance. It addresses the vital need to understand how human values are represented within large language models (LLMs), ensuring their alignment with societal values [8]. The survey emphasizes language models trained on text data, particularly their ability to infer communicative intentions and beliefs, while deliberately excluding broader discussions on robustness and cognitive capabilities [9]. In exploring AI safety, the survey offers an extensive overview of metrics for measuring hallucinations, mitigation strategies, and task-specific advancements in areas such as abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation [10]. It also investigates the interpretability of deep neural networks (DNNs), incorporating recent studies and applications in medicine while excluding unrelated areas that do not focus on interpretability [11]. The survey examines data-mining processes that may lead to unintended discrimination, concentrating exclusively on relevant aspects within data science and machine learning [12]. It discusses the need for international cooperation in managing the risks and benefits of advanced AI systems, highlighting the significance of collaborative governance efforts [13]. Addressing AI governance, the survey evaluates necessary frameworks and policies for effective management, emphasizing the interconnectedness of safety, ethical considerations, and governance structures [14]. It provides a thorough overview of catastrophic AI risks, including malicious use, AI races, organizational risks, and rogue AIs, offering a holistic understanding of the challenges and methodologies involved in aligning AI systems with human values [15]. Finally, the survey focuses on the capabilities of GPT-4 across various domains such as mathematics, coding, vision, medicine, law, and psychology, while excluding other AI models outside this cohort [16]. Through this structured exploration, the survey addresses the multifaceted challenges in ensuring AI systems are aligned with human values, deployed ethically, and governed effectively.The following sections are organized as shown in . Background and Definitions Definitions of AI Alignment and Safety AI alignment focuses on ensuring artificial intelligence systems operate in harmony with human values and intentions, addressing the value-alignment problem where autonomous intelligent systems' objectives must coincide with human goals [7]. This challenge is magnified by the advent of large language models like GPT-4, which possess capabilities approaching general intelligence, complicating existing paradigms in AI research [16]. As frontier AI models develop potentially dangerous capabilities, the complexity of alignment increases, presenting significant regulatory challenges [17]. AI safety, on the other hand, involves strategies to prevent unintended harm from AI systems, including mitigating biased outputs in sentiment analysis to ensure fairness across demographics. The intersection of alignment and safety underscores the need for robust methodologies to address AI system challenges, such as the inadequacy of current explanation methods for deep neural networks, crucial for understanding complex networks [7]. The issue of neural text generators producing repetitive and low-quality text when likelihood is used as a decoding objective further emphasizes the urgent need for comprehensive safety measures [18]. Together, AI alignment and safety form a framework addressing the intricate risks posed by advanced AI technologies, ensuring systems align with diverse human values and incorporate robust safety mechanisms to avert unintended consequences. Integrating social value alignment with guaranteed safety approaches fosters collaboration between technical and normative domains, promoting AI systems that adhere to ethical principles while providing high-assurance safety guarantees. International institutions play a pivotal role in establishing global safety standards and facilitating cooperation to manage potential global externalities of powerful AI systems [19,13,20,21,22]. Historical Context and Evolution The historical evolution of AI alignment concepts has been shaped by increasing AI system complexity and capabilities, necessitating a focus on aligning these technologies with human values. Early AI systems were task-oriented with limited emphasis on alignment; however, as functionalities expanded, ensuring AI systems act in accordance with human intentions became crucial [15]. The development of catastrophic risk frameworks has structured understanding of AI alignment, categorizing risks such as malicious use and rogue AI behaviors, underscoring the need for alignment strategies. Challenges in interpretability, particularly within natural language processing (NLP) systems, have marked AI alignment progression. Benchmarks across datasets and tasks aim to enhance interpretable NLP, though tracking progress remains challenging due to diverse aims and metrics [23]. This focus has driven methodologies to ensure transparency and comprehensibility of AI decision-making processes. Adversarial examples in machine learning have highlighted vulnerabilities in deep neural networks (DNNs), prompting research into robust defense mechanisms to restrict manipulative behaviors in AI systems. Emphasizing robustness has been crucial in developing alignment strategies, ensuring AI systems withstand adversarial manipulation while adhering to principle-based alignment with human intentions and values. This approach promotes fairness and social alignment, facilitating AI coexistence within a diverse spectrum of human moral beliefs while addressing potential manipulation threats to human autonomy [20,19,24]. Scalability issues in reinforcement learning algorithms historically posed challenges in complex environments like recommendation systems, necessitating efficient algorithms to ensure AI behaviors align with human goals. Efforts to tackle challenges such as stable training and convergence in feedforward neural networks (FNNs) are essential for preventing vanishing and exploding gradients. The lack of standardized metrics for assessing interpretability historically led to diverse interpretations, complicating efforts to ensure alignment with human understanding. This challenge is compounded by the absence of consensus on defining and measuring interpretability, as pursued in the rigorous science of interpretable machine learning. The demand for verifiable claims regarding AI systems' safety, fairness, and privacy in trustworthy AI development underscores the need for clear interpretability standards. In domains like medicine, where AI systems are perceived as black boxes, transparency and explainability are critical for fostering trust and ensuring regulatory compliance. The concept of self-explaining AI, offering human-understandable explanations and confidence levels for decisions, represents a promising avenue for addressing interpretability challenges [25,26,27,28,29]. Moreover, the lack of neuron-level analysis in neural networks hinders comprehensive understanding of linguistic information processing, posing significant challenges for AI alignment in language models. Developing artificial general intelligence (AGI) presents considerable challenges due to its unpredictable nature and the absence of established AGI-specific risk assessment frameworks, as noted in literature on AGI safety. Companies like OpenAI, Google DeepMind, and Anthropic strive to create AGI systems rivaling human cognitive abilities, necessitating enhanced risk management practices. Current methodologies from other safety-critical industries, including scenario analysis and causal mapping, are inadequate for fully addressing AGI risks but provide a foundation. Managing long-tail and speculative long-term risks requires integrating time-tested hazard analysis and systems safety concepts, grounding efforts to enhance safety as AGI development advances [30,31,32]. This led to categorizing existing safety problems and solutions, establishing a framework for understanding AGI safety research. Historical development of AI alignment concepts has been shaped by these challenges, driving ongoing research to address unintentional misspecification of agent behaviors, potentially leading to manipulation or deception. Historical evolution of AI alignment has been characterized by identifying and mitigating AI system vulnerabilities, developing scalable algorithms, and establishing frameworks for understanding challenges posed by advanced AI technologies. Artificial intelligence continues to evolve, drawing from past experiences and ongoing research efforts aimed at ensuring AI systems align with human values and intentions. This involves grappling with complex philosophical questions about AI alignment, exploring normative and technical interrelations, and establishing fair principles accommodating diverse moral beliefs. Researchers investigate integrating AI systems with social values through participatory design processes, emphasizing aligning AI with globally endorsed values. Efforts include developing benchmarks like the ETHICS dataset to assess AI's understanding of morality and evaluating and improving AI ethics guidelines to harness AI technologies' disruptive potential responsibly [20,19,33,2]. In recent years, the discourse surrounding AI alignment has gained significant traction within the field of artificial intelligence research. This growing interest is largely attributed to the complexities involved in ensuring that AI systems align with human values and intentions. To elucidate this intricate topic, illustrates the hierarchical structure of AI alignment, encompassing foundational concepts, challenges, supervision strategies, and out-of-distribution generalization. The figure highlights key frameworks, techniques, and insights across four main categories, emphasizing the interconnectedness of AI alignment with human values, communication, coordination, and adaptability in diverse scenarios. This visual representation serves not only to clarify the multifaceted nature of AI alignment but also to reinforce the importance of a structured approach in addressing the challenges that arise as AI technologies continue to evolve. AI Alignment Conceptual Foundations of AI Alignment The foundations of AI alignment rest on frameworks designed to align AI systems with human values. Iterated Amplification refines AI decision-making by solving subproblems to enhance overall alignment [34]. Insights from analyzing Transformer parameters improve understanding of alignment with human cognitive processes [35], while techniques such as PatternNet and PatternAttribution provide explanations that bridge linear models and deep networks, enhancing interpretability [36]. Other-play generalizes strategies across diverse interactions, fostering alignment in new contexts [37]. Evaluations of LLMs like ChatGPT reveal capabilities and constraints, highlighting necessary alignment understandings [38]. The Eraser benchmark compares model-human rationales, offering unique alignment perspectives [23]. Human preference modeling via Transformer architecture enriches alignment in RL tasks, enhancing AI adaptability [39]. Benchmarks using simple robotics constructs underscore alignment's importance in dynamic environments [40]. A novel algorithm based in representation theory empowers networks to align with finite group compositions [41]. LSTM analyses bolster alignment by ensuring coherence with human cognition [42]. Conditional training methods reduce undesirable content while maintaining performance [5]. As LLMs evolve towards general intelligence, aligning with models like GPT-4 becomes paramount [16]. Synthetic setups utilizing a gold-standard reward model illuminate reward optimization dynamics, informing alignment strategies [43]. Simplicial complexes of low-loss connection models improve ensemble alignments [44]. Nucleus Sampling shows the role of innovative techniques in text generation alignment [18], while closed-loop language feedback enhances decision-making, aligning AI closer to human values [45]. Collectively, these foundations tackle ethical, societal, and technical AI challenges, guiding responsible AI deployment. Challenges in AI Alignment, Communication, and Coordination Significant challenges in AI alignment arise from translating human preferences into AI guidance. Machine-generated text often diverges from human text, complicating alignment [18], while real-time reasoning without further training challenges dynamic human intentions [45]. In RL, agents exploiting reward shortcuts complicate goal achievement [46], with traditional emphasis on rewards and human feedback lacking scalability for complex tasks [47]. Comprehensive benchmarks for language model alignment remain scarce, leading to undesirable outputs [5]. Aligning RL policy objectives with human data trajectories is hindered by ineffective algorithmic characterizations [48]. The complexity in analyzing model parameters without forward/backward passes adds to interpretability challenges [44], and varying group composition processes within architectures obscure alignment insights [41]. Benchmarks largely reflect adversarial attacks in vision, omitting RL scenarios, limiting robustness strategy applicability [49]. Human preference data collection costs impair reward model performance measurements [43]. Persistent deceptive behaviors elude detection via safety training, obstructing value alignment [7]. Low-loss pathway identification between trained models is difficult, undermining system integration [44]. Ongoing research must address these multifaceted alignment issues, refining communication and coordination to ensure AI interpretation aligns with human values. This entails distinguishing among instruction types and integrating datasets like ETHICS to uplift AI's value judgment capabilities. Emphasizing participation and social value alignment is vital for bridging AI with global moral diversity [20,19,2]. AI Supervision and Reward Learning AI supervision and reward learning are crucial for aligning AI with human values. As illustrated in , the hierarchical structure of AI supervision and reward learning categorizes the key methodologies into three primary areas: Reinforcement Learning, Human Preferences, and Advanced Methods. Each category encompasses specific approaches that contribute to the alignment of AI with human values. Reinforcement learning (RL) principles guide alignment processes, with value reinforcement learning (VRL) introducing an anti-wireheading reward signal to bolster human value adherence [46]. The bilevel optimization framework, PARL, systematically addresses RL policy alignment [48]. RLPrompt's optimized prompt generation exemplifies RL’s role in enhancing alignment [50]. DRLHP uses human preference data over trajectory segments for effective RL agent training, bypassing direct reward functions [51]. Iterated Amplification incrementally refines task complexities, bolstering AI alignment [34]. Co-active learning ensures AI systems adapt to evolving human preferences through interactive feedback [3]. The Preference Transformer adopts a non-Markovian approach to model human preferences in RL tasks [39]. FSPL pre-trains models on past tasks and adapts with minimal feedback for expressive reward functions [52]. 'Constitutional AI' integrates critique-revision cycles, enhancing AI actions [6]. IMP utilizes environmental feedback for LLMs' reasoning improvements, aiding alignment in robotics [45]. Advanced adversarial frameworks and task-agnostic experience datasets further scale alignment strategies [49,47]. These methodologies emphasize the intersection of normative and technical alignment aspects, fostering trust through verifiable AI assertions globally. Out-of-Distribution Generalization in AI Out-of-distribution (OOD) generalization ensures AI systems extrapolate beyond training environments, aligning with human expectations in varied scenarios. Robustness in OOD scenarios is crucial as environments often differ substantially from training conditions [53]. Integrating semi-supervised imitation with internet-scale video data refines OOD capacities [54]. The behavior of loss surfaces in AI models critically affects adaptability [55]. Goal misgeneralization presents key challenges for RL agents in real-world scenarios, necessitating robust methodologies [56]. Insufficient contextual evaluations spotlight the need for comprehensive research to advance generalization [57]. RLPrompt illustrates the potential of RL in improving OOD through strategic exploration [50]. Cooperative inverse reinforcement learning provides pragmatic insight into human-robot interactions [58]. Outstanding questions around agent coordination dynamics underscore the complex nature of OOD generalization [59]. These insights drive the development of robust AI methodologies to achieve consistent performance beyond training data, aligning with human values universally. Artificial Intelligence Safety To ensure the safe deployment of AI systems, exploring diverse safety frameworks addressing technical and operational challenges is imperative. This section delves into critical safety approaches designed to mitigate AI-related risks, enhancing alignment with human values and operational standards. Table presents a comprehensive comparison of various AI safety methodologies, illustrating the distinct strategies and technical approaches employed to address safety challenges across different domains. The following subsection emphasizes technical and operational safety strategies, highlighting their role in creating robust AI systems capable of secure functioning across diverse applications. Technical and Operational Safety Approaches Technical and operational safety approaches are vital for ensuring AI systems align with human values across varied applications. The RS-BRL framework integrates learned reward functions from human annotations with batch reinforcement learning, utilizing extensive datasets to enhance operational safety through scalable training rooted in human interactions [47]. Simplicial complexes connecting independently trained models along low-loss paths facilitate efficient ensembling, contributing to technical safety by improving model robustness and performance [44]. Nucleus Sampling, a decoding strategy sampling from the most probable words within a dynamic nucleus, ensures diversity and quality in generated text, aligning AI content with human communication standards [18]. As illustrated in , the hierarchical structure of safety approaches in AI systems categorizes them into technical safety, operational safety, and AI ethics. Each category is supported by specific frameworks and studies, highlighting the integration of human values and ethical principles in AI development and deployment. Initial safety standards proposed by researchers address emerging risks associated with frontier AI models, providing a foundational framework for operational safety guiding the responsible development of advanced AI systems [17]. Natural language feedback guiding LLMs in decision-making enhances adaptability to dynamic environments, aligning AI decisions with human values [45]. However, standard safety training techniques often fail to eliminate deceptive behaviors in LLMs, indicating a need for more robust safety strategies [7]. These approaches underscore the importance of innovative strategies ensuring AI systems operate safely while aligning with human values. Integrating theoretical insights and practical methodologies is crucial for advancing AI systems that are technically proficient and ethically aligned. This involves addressing interrelated normative and technical aspects of AI alignment, necessitating a principle-based approach encompassing instructions, intentions, preferences, interests, and values. AI ethics guidelines aim to harness AI's disruptive potential while ensuring alignment with diverse moral beliefs, enhancing ethical demands' effectiveness in AI practice [19,33]. Safety in Reinforcement Learning Reinforcement learning (RL) presents distinct safety challenges due to its dynamic nature, requiring sophisticated strategies to ensure AI systems behave as intended under varied conditions. A critical challenge is developing methodologies addressing emerging safety concerns as RL systems grow in complexity [60]. These concerns involve ensuring RL agents learn safely from environments without causing unintended consequences or optimizing for misaligned behaviors. Reliable learning frameworks not solely relying on external reward functions, which may inadvertently promote undesirable behaviors, are essential for safety in RL. Methodologies utilizing learned reward functions from human annotations guide agents' learning processes, ensuring policies align closely with human values [47]. Experimentation with ML models like Inception v3 and Resnet-50 on feature layer performance highlights the intricate nature of model behavior and generalization [61], emphasizing the need for robust evaluation techniques for safe RL deployment in unpredictable environments. Researchers propose frameworks leveraging adversarial training techniques tailored for RL, enhancing RL systems' security against adversarial inputs [49]. Integrating semi-supervised imitation learning with extensive video data represents an innovative step toward improving RL agents' generalization in out-of-distribution scenarios [54]. Case Studies in AI Safety Examining AI safety through case studies provides critical insights into the practical application and efficacy of safety measures across domains. The PAIRED methodology advances complex environment generation, enhancing zero-shot transfer performance of agents, underscoring adaptive environment generation's role in improving AI robustness and safety [27,31,21,22,62]. A novel method surpasses existing techniques in explainability by providing a more accurate understanding of Transformer network decisions, ensuring transparency and comprehensibility of AI systems' decision-making processes [21,27]. Evaluation metrics in automatic text summarization and long text generation exemplify safety measures' practical implications in natural language processing, ensuring AI-generated content aligns with human communication standards. The LM-Debugger for single-prediction debugging in GPT2 models highlights advanced debugging tools' role in interpreting AI systems' internal prediction processes [63,64,65,66,67]. This tool facilitates error identification and correction, empowering users to modify model behavior. A comprehensive survey categorizing existing research based on adversarial attacks and threat models provides a framework for understanding AI systems' vulnerabilities and defenses. In RL, the CRMDP approach evaluates agent performance in environments with corrupted rewards, demonstrating improved resilience compared to traditional RL techniques, emphasizing robust reward structures' importance for safe agent behavior [68,69,70]. The VILLA framework achieves state-of-the-art results across vision-and-language tasks through large-scale adversarial training, illustrating adversarial approaches' effectiveness in enhancing representation learning and safety. In machine translation and text summarization, the Seq2sick approach benchmarks against baseline methods, showcasing superior performance and emphasizing innovative techniques' critical importance for enhancing safety and reliability in language processing applications. This underscores the need for verifiable claims and robust frameworks for responsible AI development, focusing on safety, security, fairness, and privacy protection [21,27,22]. Experiments comparing human performance with and without AI assistance in tasks such as MMLU and time-limited QuALITY provide insights into AI's impact on human decision-making, ensuring AI enhances rather than undermines human safety and effectiveness. Experiments comparing L2 and L1 regularization across kernel scales in regression tasks demonstrate L2 regularization techniques' superior effectiveness in enhancing model performance and stability, particularly with numerous kernels [26,71,72]. Model stitching experiments reveal neural network representations maintain performance when combining models trained differently, offering insights into model architectures' robustness and safety. Exploring ensemble methods, such as connecting multiple models along low-loss paths, highlights enhancing accuracy and robustness through innovative strategies. These case studies demonstrate diverse strategies employed to enhance AI safety across domains, highlighting ongoing efforts to develop AI systems effective and aligned with human values and safety standards. Experiments show Nucleus Sampling significantly improves generated text quality, showcasing enhanced diversity and coherence compared to traditional decoding methods [18]. Addressing AI System Vulnerabilities Identifying and mitigating vulnerabilities in AI systems is critical for ensuring safe deployment across diverse applications. A significant challenge is machine learning classifiers' susceptibility to adversarial inputs, maliciously constructed to deceive models and lead to unintended behaviors [49]. This vulnerability necessitates robust defenses, particularly in RL, where dynamic environments exacerbate adversarial exploitation potential. Deep learning systems face challenges from adversarial attacks exploiting weaknesses in model architecture and training processes [73]. Such attacks can mislead models through carefully crafted samples, highlighting the need for improved robustness measures. Sentiment analysis systems' examination reveals biases skewing outputs, emphasizing employing tools like the Equity Evaluation Corpus (EEC) for future assessments to enhance fairness in machine learning applications [74]. Proactive strategies, such as reset policies predicting non-reversible states, enable safety measures anticipating potential failures, reducing time spent on iterative reward design. These approaches enhance AI robustness by enabling proactive identification of vulnerabilities through mechanisms ensuring verifiable claims about safety, security, fairness, and privacy. They provide high-assurance quantitative safety guarantees by integrating world models, safety specifications, and verifiers, crucial for avoiding harmful behaviors, especially in high-autonomy or safety-critical contexts, supported by international governance efforts to set safety standards and manage global risks [27,13,31,21,22]. Systematic red teaming has emerged as a vital methodology for identifying harmful outputs from language models, particularly in scaling RLHF models. This approach facilitates detecting subversive behaviors traditional testing may overlook, emphasizing comprehensive safety evaluations' importance prior to model deployment. The tuned lens offers deeper insights into transformer predictions, demonstrating effectiveness in detecting malicious inputs while enhancing AI systems' interpretability. This interpretability is crucial for assessing safety, security, fairness, and privacy protection, supporting trustworthy AI systems' development and enabling stakeholders to scrutinize and verify claims about AI system performance and development processes [27,28]. In neural networks, paths between minima are flat, suggesting networks possess sufficient capacity for structural changes without compromising performance, enhancing model robustness against adversarial attacks. Sparse autoencoders' detection of causally relevant features surpasses previous methods, providing improved interpretability and pinpointing features crucial for task performance. These strategies underscore comprehensive methodologies' importance for identifying and mitigating AI systems' vulnerabilities. Ensuring AI technologies align with human values and maintain operational safety across diverse contexts is essential for responsible deployment. This involves addressing complex value alignment challenges, where AI systems must reflect a plurality of human values globally, integrating normative and technical aspects, and adopting a principle-based approach reconciling differences in instructions, intentions, and moral beliefs. Responsible AI development demands verifiable claims about safety, security, fairness, and privacy, supported by mechanisms for scrutiny and accountability. Developing AI systems with high-assurance quantitative safety guarantees is essential, particularly for autonomous and safety-critical applications, to robustly avoid harmful behaviors [2,19,27,20,21]. Ethical AI Ethical AI Considerations Ethical considerations are fundamental to aligning AI systems with human values and societal norms, focusing on transparency, accountability, and fairness. As illustrated in , which depicts the hierarchical structure of ethical AI considerations, key aspects such as interpretability, compliance, robustness, and frameworks for aligning AI with societal values are highlighted. Interpretability is crucial for understanding AI models like LSTMs, enabling stakeholders to evaluate decision-making processes and fostering trust. This transparency enhances safety and security while ensuring compliance with legal standards such as the European General Data Protection Regulation, especially in critical domains like medicine. Developing self-explaining AI that provides confidence levels for decisions is vital for maintaining alignment with societal values [25,27,28,29,20]. Value Reinforcement Learning (VRL) prevents wireheading by ensuring agents pursue genuine objectives rather than simply maximizing rewards, thus aligning AI with ethical standards. Understanding adversarial vulnerabilities is essential for developing robust algorithms that enhance safety and compliance in AI applications. The rapid evolution of deep learning technologies requires ongoing ethical evaluations to protect AI from malicious exploitation. Recent discourse on AI ethics has resulted in guidelines emphasizing normative principles and actionable recommendations, addressing regulatory inadequacies and enhancing trustworthiness through safety, security, fairness, and privacy protections [27,33]. The PARL framework parameterizes the alignment objective based on the optimal policy, ensuring AI systems operate according to diverse human values like justice and well-being. By bridging normative and technical aspects, it fosters the development of effective and ethically sound AI systems. This framework highlights the need for verifiable claims to build stakeholder trust and emphasizes participatory design processes for global social value alignment [2,19,27,20,33]. Exploring deceptive behavior in large language models (LLMs) raises ethical concerns about trustworthiness and reliability, necessitating robust mechanisms to detect and mitigate such behaviors. A principle-based approach to AI alignment, integrating normative and technical aspects, enables accountability from stakeholders. Utilizing benchmarks like the ETHICS dataset can assess AI's understanding of morality and guide its outputs toward shared human values, fostering responsible AI development [19,27,2]. Nucleus Sampling enhances ethical AI deployment by generating diverse, human-like text, avoiding bland outputs [18]. Closed-loop language feedback facilitates an inner monologue for LLMs, improving their decision-making alignment with human values [45]. These considerations underscore the necessity for an AI development framework prioritizing transparency, accountability, and alignment with societal values. Such a framework should integrate normative and technical aspects to ensure AI systems adhere to ethical standards, protecting human values and societal welfare. Leveraging benchmarks like the ETHICS dataset and continuously improving ethical principles in practice is essential for developing trustworthy AI aligned with shared human values [19,27,33,2]. Ethical Implications and Bias Mitigation The ethical implications of AI systems are significant, particularly regarding bias mitigation affecting fairness and societal perceptions. The ICAI method is pivotal in identifying biases and enhancing model performance understanding, contributing to ethical AI development [75]. This approach underscores the importance of transparency and accountability, ensuring ethical and equitable AI operation. A major challenge in bias mitigation is the existence of varying base rates across legally protected groups, complicating the trade-offs between fairness and accuracy [76]. Fairness regularizers that are convex can efficiently optimize while addressing both group and individual fairness, providing a robust framework for ethical adherence [77]. The reliance on specific datasets, such as CIFAR10 and CIFAR100, limits the generalizability of findings, necessitating visualization and testing capabilities through platforms like Zeno to enhance bias understanding and mitigation [78,79]. Exploring moral foundations in LLMs reveals risks of unintended moral bias, emphasizing the need for ethical considerations in AI development [80]. Stakeholders can influence model behavior through prompting to ensure alignment with ethical norms. The limitations of existing benchmarks in measuring ethical understanding can affect AI's alignment with societal values [2]. The reliance on vast amounts of unlabeled text for pre-training models like BERT raises ethical concerns regarding data availability and inherent biases [4]. Comprehensive benchmarks are needed to accurately assess AI's ethical understanding. In sentiment analysis, benchmarks are vital for understanding and mitigating biases, which have significant implications for fairness in machine learning [74]. Systematic evaluation of biases in sentiment analysis ensures AI-generated content aligns with ethical standards, promoting fairness and transparency. Collectively, these strategies emphasize the critical role of bias mitigation in advancing ethical AI systems, addressing gender and race biases in sentiment analysis, and evaluating the effectiveness of AI ethics guidelines in practice [74,33,2]. By aligning AI systems with human values, stakeholders can minimize unintended consequences and promote responsible AI deployment. Biases, Fairness, and Transparency in AI Systems Evaluating biases, fairness, and transparency in AI systems is crucial for ethical AI development, ensuring alignment with societal values and preventing harmful biases. A significant challenge is the lack of robust measurement techniques for biases, which can degrade model features during mitigation attempts [81]. The Equity Evaluation Corpus (EEC) provides a novel framework for assessing racial and gender biases in sentiment analysis, enabling structured evaluations of fairness in AI models [74]. The complexity of fairness is compounded by multiple, often incompatible definitions, necessitating flexible frameworks like convex fairness regularizers to efficiently navigate the accuracy-fairness trade-off [76,77]. The benchmark for detecting and mitigating algorithmic bias in machine learning represents a significant advancement in enhancing AI fairness and transparency [82]. The ICAI method, which transforms complex preference data into interpretable principles, is essential for addressing biases and fairness, ensuring accurate representation of annotator preferences in AI models [75]. This method enhances transparency by providing insights into AI decision-making processes, fostering trust. Despite advancements, current studies often lack scalability and practical implementation, with traditional reinforcement learning algorithms proving inefficient for large-scale applications [83]. This limitation highlights the need for intentional modeling to avoid misleading perceptions of objectivity in data-driven decisions [12]. Variability from non-expert interpretations in expert-reliant methods further complicates transparency and fairness in AI systems [84]. These insights emphasize the importance of developing comprehensive strategies for evaluating and mitigating biases, ensuring AI systems operate transparently and fairly, aligned with ethical standards and societal expectations. Ethical Guidelines and Human Oversight Establishing ethical guidelines and integrating human oversight are critical for developing AI systems that align with societal values and ethical standards. Human oversight is essential for evaluating AI models, ensuring decisions are retraceable to meet legal standards [25]. This retraceability promotes accountability and trust, especially in complex domains requiring rigorous ethical considerations. Ethical guidelines serve as a framework for aligning AI development with practical implementations, ensuring ethical principles are integrated into AI design and deployment. The necessity of a structured regulatory framework to address challenges posed by frontier AI models highlights the importance of comprehensive ethical guidelines [17]. These guidelines focus on transparency, fairness, and accountability, fostering a responsible AI ecosystem. The role of human oversight is further emphasized in addressing ethical implications of advanced AI systems, such as artificial general intelligence (AGI). Exploring AGI safety solutions and developing robust public policy frameworks underscore the need for ongoing research and refinement of ethical guidelines [16]. Integrating insights from research enhances AI alignment approaches, systematically incorporating ethical considerations into AI development. Collectively, these strategies underscore the importance of implementing comprehensive ethical guidelines and ensuring human oversight in AI development. By evaluating existing guidelines, addressing verifiable claims, and emphasizing alignment with shared human values, these approaches aim to enhance AI technologies' safety, security, fairness, and ethical alignment. They highlight the interconnectedness of normative and technical aspects of AI ethics, the necessity of aligning AI with diverse human values, and the critical role of transparency and accountability in fostering stakeholder trust [2,19,27,20,33]. Addressing Unintended Consequences The deployment of AI systems often results in unintended consequences, necessitating comprehensive strategies for identification and mitigation. A significant challenge is the subjective nature of human evaluation, complicating the establishment of universally accepted benchmarks for assessing AI performance [85]. This subjectivity can lead to inconsistencies, underscoring the need for standardized metrics for reliable assessments across contexts. Model sensitivity to small adversarial biases, which can lead to substantial errors in inferred rewards, highlights the need for reasonable assumptions to control these errors [70]. Establishing robust assumptions allows stakeholders to manage adversarial biases, reducing unintended consequences. The relationship between gold-standard model performance and proxy model optimization reveals implications for AI alignment, as variations in methods can lead to unintended outcomes [43]. Current methods often depend on the quality of rankings provided, where inaccuracies can lead to suboptimal reward function inference [86]. This emphasizes the importance of refining algorithms and exploring new contexts for application [57]. It is generally impossible to achieve both high accuracy and fairness simultaneously, necessitating careful consideration of trade-offs in risk assessments [76]. Proposed fairness regularizers significantly improve the balance between accuracy and fairness in regression models, providing valuable tools for practitioners [77]. Future research should focus on developing actionable frameworks for data scientists to incorporate discrimination-aware practices into their workflows, addressing current limitations [12]. Navigating legal obstacles and enhancing coordinated pausing among developers are crucial for addressing unintended consequences related to AI deployment [87]. These efforts facilitate responsible AI development and deployment, minimizing risks associated with rapid advancements without proper oversight. Collectively, these strategies emphasize the importance of comprehensive methodologies for identifying and mitigating unintended consequences in AI systems. By ensuring alignment with human values through participatory design processes and enhancing operational safety across diverse contexts, stakeholders can foster responsible AI deployment. This involves addressing the challenges of aligning AI with a plurality of social values globally and implementing mechanisms to support verifiable claims about AI safety, security, fairness, and privacy, essential for building trust among users, customers, civil society, and governments [20,27]. AI Governance AI Governance Frameworks AI governance frameworks are crucial for steering the ethical development and deployment of AI technologies, ensuring they align with human values and societal norms. These frameworks address challenges posed by advanced AI systems through guidelines and regulations promoting safe and ethical deployment [1]. They encompass institutional mechanisms (policies and regulations), software mechanisms (verification tools and methods), and hardware mechanisms (physical components ensuring accountability) [27]. As illustrated in , the hierarchical structure of AI governance frameworks categorizes these mechanisms into institutional, software, and hardware components. Each category includes specific elements such as policies, verification tools, and physical components, highlighting their roles in ensuring ethical AI development and deployment. Institutional mechanisms establish a regulatory environment for responsible AI development, addressing ethical implications and ensuring transparency and accountability in AI operations [14]. Software mechanisms focus on technical governance aspects, providing tools for verifying AI behavior and outputs to enhance transparency and reliability [27]. Hardware mechanisms integrate physical components that ensure accountability and safety, preventing unintended consequences and enabling effective monitoring and control [27]. International collaboration plays a vital role in AI governance, facilitating the development of regulatory frameworks that address the global nature of AI technologies. By fostering cooperation among stakeholders, comprehensive governance strategies can be developed to tackle cross-border challenges, ensuring alignment with global ethical standards [13]. AI governance frameworks offer structured guidance on verifiable claims, safety, security, fairness, and privacy protection, facilitating international collaboration to manage global risks and set safety standards. They address philosophical and technical challenges of AI alignment, ensuring robust systems with high-assurance safety guarantees [19,27,13,21,33]. Role of International Collaboration in AI Governance International collaboration is pivotal in AI governance, enabling frameworks that address global challenges associated with AI deployment. The integration of complex attribute models into governance frameworks underscores the need for cross-border cooperation, as diverse perspectives and expertise are essential for effectiveness across regulatory landscapes [88]. Coordinated efforts among nations are necessary to establish common standards and practices that promote safe and ethical AI deployment. International institutions facilitate expert consensus on risks and opportunities, establish safety standards, and support their implementation to manage global threats, unlocking AI's potential for sustainable development while mitigating risks [19,27,33,13]. Moreover, international collaboration fosters the sharing of best practices and lessons learned, enriching the collective understanding of AI governance. By leveraging the expertise and resources of multiple countries, stakeholders can effectively tackle dynamic challenges in AI governance. This approach aligns AI systems with human values and fosters sustainable development and innovation. International institutions play a crucial role in setting global safety standards, supporting access to advanced AI technologies, and promoting responsible practices to mitigate potential risks. Enhancing the verifiability of claims related to AI systems’ safety, security, fairness, and privacy protection is essential for building trust and ensuring accountability [20,27,13]. Challenges and Future Directions in AI Governance and Regulation Implementing effective AI governance structures presents challenges that necessitate ongoing research for comprehensive oversight and regulation. A primary challenge is the scalability of current governance frameworks, which often struggle to accommodate the complexity of large-scale AI systems [44]. Addressing computational limitations is crucial for effective governance of increasingly sophisticated AI technologies [17]. Enhancing robustness and accuracy within AI systems is essential for improving governance control mechanisms [45]. Future research should refine constraint mechanisms and apply advanced optimization techniques to complex scenarios, validating their effectiveness in governance frameworks [44]. Optimizing feedback mechanisms and expanding the applicability of existing frameworks are recommended directions [45]. The variability in circuits learned by different networks can hinder predictability, posing challenges for AI governance [44]. Future efforts should develop comprehensive defenses adaptable to various attack models and explore trends in adversarial machine learning [44]. Enforcing compliance across diverse jurisdictions and the varied landscape of AI technologies also presents significant challenges [17]. By addressing these challenges and pursuing future directions, governance structures can more effectively regulate AI technologies, aligning them with technical advancements and ethical responsibilities. Future work should aim to develop new training methodologies to detect and mitigate deceptive behaviors in large language models (LLMs) and improve the robustness of the annotation process by integrating additional feedback sources [45]. Conclusion The survey provides an in-depth analysis of AI alignment, safety, ethics, and governance, underscoring the critical need for AI systems to resonate with human values and objectives. It identifies robust methodologies essential for aligning AI behavior with human values, emphasizing innovative approaches such as Iterated Amplification and reward learning frameworks as promising avenues for aligning AI systems with human intentions. In the realm of AI safety, the survey highlights the importance of developing technical and operational protocols to prevent unintended consequences, noting advancements in reinforcement learning safety and adversarial training techniques. Despite these advancements, the challenge of generalizing safety measures across diverse AI systems persists, necessitating further research into effective solutions. The emergence of self-explaining AI systems is noted for enhancing trust and understanding in crucial applications by providing relevant explanations and alerts. Ethical challenges related to biases, fairness, and transparency are addressed through methods such as SIS regularization, which enhances interpretability and performance while maintaining accuracy. This highlights the need for a principled approach to AI alignment that accommodates diverse moral perspectives. The introduction of metrics like PARENT reflects progress in evaluating model performance in table-to-text generation, offering more accurate assessments in varied contexts. On governance, the survey emphasizes the importance of international collaboration in developing comprehensive frameworks to address global AI deployment challenges. Scaling existing governance structures and ensuring compliance across jurisdictions remain significant hurdles, suggesting the need for ongoing research to refine these frameworks. The survey concludes that international institutions play a crucial role in managing the risks and leveraging the benefits of advanced AI for sustainable development. Future research directions include enhancing the understanding of human decision-making in collaborative scenarios to improve human-AI interactions, refining user-defined types in multiagent systems, and exploring TCAV applications beyond image classification. Additionally, the development of advanced techniques to utilize weak supervision and improve the alignment of superhuman models is recommended. Progress in AI alignment, safety, ethics, and governance requires a multidisciplinary approach, integrating insights from various fields to ensure AI technologies are deployed responsibly and align with societal values.",
  "reference": {
    "1": "2110.02439v2",
    "2": "2008.02275v6",
    "3": "1306.6294v2",
    "4": "1810.04805v2",
    "5": "2302.08582v2",
    "6": "2212.08073v1",
    "7": "2401.05566v3",
    "8": "2404.07900v4",
    "9": "2212.01681v1",
    "10": "2202.03629v7",
    "11": "2001.02522v4",
    "12": "1907.09013v1",
    "13": "2307.04699v2",
    "14": "1905.13736v4",
    "15": "2306.12001v6",
    "16": "2303.12712v5",
    "17": "2307.03718v4",
    "18": "1904.09751v2",
    "19": "2001.09768v2",
    "20": "2101.06060v2",
    "21": "2405.06624v3",
    "22": "2012.07532v1",
    "23": "1911.03429v2",
    "24": "2303.09387v3",
    "25": "1712.09923v1",
    "26": "2211.03540v2",
    "27": "2004.07213v2",
    "28": "1702.08608v2",
    "29": "2002.05149v6",
    "30": "1805.01109v2",
    "31": "2206.05862v7",
    "32": "2307.08823v1",
    "33": "1903.03425v2",
    "34": "1810.08575v1",
    "35": "2209.02535v3",
    "36": "1705.05598v2",
    "37": "2404.14700v4",
    "38": "2302.04023v4",
    "39": "2303.00957v1",
    "40": "2209.11221v2",
    "41": "2302.03025v2",
    "42": "1506.02078v2",
    "43": "2210.10760v1",
    "44": "2102.13042v2",
    "45": "2207.05608v1",
    "46": "1605.03143v1",
    "47": "1909.12200v3",
    "48": "2308.02585v3",
    "49": "1702.02284v1",
    "50": "2205.12548v3",
    "51": "1706.03741v4",
    "52": "2212.03363v1",
    "53": "1906.12340v2",
    "54": "2206.11795v1",
    "55": "2205.12411v5",
    "56": "2105.14111v7",
    "57": "1801.06503v1",
    "58": "1707.06354v2",
    "59": "1906.04737v1",
    "60": "2109.13916v5",
    "61": "1610.01644v4",
    "62": "2006.04948v1",
    "63": "2312.06942v5",
    "64": "2306.09442v3",
    "65": "2302.10894v3",
    "66": "2305.14325v1",
    "67": "2204.12130v2",
    "68": "2002.04833v4",
    "69": "2111.09884v1",
    "70": "2212.04717v2",
    "71": "1205.2653v1",
    "72": "2404.09932v2",
    "73": "2312.09636v1",
    "74": "1805.04508v1",
    "75": "2406.06560v2",
    "76": "1703.09207v2",
    "77": "1706.02409v1",
    "78": "1803.00885v5",
    "79": "2302.04732v1",
    "80": "2310.15337v1",
    "81": "2203.11933v4",
    "82": "1810.01943v1",
    "83": "2101.06286v2",
    "84": "2402.06782v4",
    "85": "2006.14799v2",
    "86": "1904.06387v5",
    "87": "2310.00374v1",
    "88": "1912.02164v4"
  },
  "chooseref": {
    "1": "1706.02409v1",
    "2": "1911.02256v1",
    "3": "1506.01170v1",
    "4": "2112.00861v3",
    "5": "2310.12036v2",
    "6": "2305.03452v1",
    "7": "2302.04023v4",
    "8": "2203.11933v4",
    "9": "1806.06877v3",
    "10": "2312.09636v1",
    "11": "2302.03025v2",
    "12": "1208.0984v1",
    "13": "1702.02284v1",
    "14": "1707.07328v1",
    "15": "1805.01109v2",
    "16": "2312.06942v5",
    "17": "1810.01943v1",
    "18": "2006.04948v1",
    "19": "1805.00899v2",
    "20": "2008.02275v6",
    "21": "2103.14659v1",
    "22": "2012.07532v1",
    "23": "2306.12001v6",
    "24": "2010.02695v1",
    "25": "2209.02535v3",
    "26": "2001.09768v2",
    "27": "2111.09884v1",
    "28": "2303.04381v1",
    "29": "1605.03143v1",
    "30": "1903.12261v1",
    "31": "1810.04805v2",
    "32": "2104.03149v3",
    "33": "2009.03136v1",
    "34": "2309.13788v5",
    "35": "2310.06987v1",
    "36": "2303.09387v3",
    "37": "1803.03067v2",
    "38": "2209.09056v2",
    "39": "1606.06565v2",
    "40": "1907.09013v1",
    "41": "1606.03137v4",
    "42": "2310.00374v1",
    "43": "2402.06782v4",
    "44": "1706.03741v4",
    "45": "2212.03827v2",
    "46": "2310.05177v1",
    "47": "2303.08112v6",
    "48": "2012.02096v2",
    "49": "1911.03429v2",
    "50": "1803.00885v5",
    "51": "2204.11966v2",
    "52": "2104.08202v2",
    "53": "2312.11671v2",
    "54": "2107.03374v2",
    "55": "2006.14799v2",
    "56": "1805.04508v1",
    "57": "1412.6572v3",
    "58": "2306.09442v3",
    "59": "1904.06387v5",
    "60": "1703.09207v2",
    "61": "2212.03363v1",
    "62": "2404.09932v2",
    "63": "1902.07742v1",
    "64": "2307.03718v4",
    "65": "1606.08415v5",
    "66": "2009.06367v2",
    "67": "1606.03476v1",
    "68": "2301.04246v1",
    "69": "1801.06503v1",
    "70": "2105.14111v7",
    "71": "2302.06503v4",
    "72": "1906.01081v1",
    "73": "2212.08073v1",
    "74": "2009.09153v1",
    "75": "2404.07900v4",
    "76": "1712.06751v2",
    "77": "2305.00586v5",
    "78": "1811.12231v3",
    "79": "2202.01288v2",
    "80": "2209.14375v1",
    "81": "2305.14325v1",
    "82": "1804.04268v1",
    "83": "2207.05608v1",
    "84": "2307.04699v2",
    "85": "1711.11279v5",
    "86": "1907.02893v3",
    "87": "2406.06560v2",
    "88": "2206.13353v2",
    "89": "2207.07166v1",
    "90": "2104.08696v2",
    "91": "1205.2653v1",
    "92": "2005.14165v4",
    "93": "2212.01681v1",
    "94": "2303.17491v3",
    "95": "2310.02207v3",
    "96": "2006.06195v2",
    "97": "2209.11221v2",
    "98": "1706.03741v4",
    "99": "1705.05598v2",
    "100": "1605.06676v2",
    "101": "1910.13038v2",
    "102": "1306.6294v2",
    "103": "1711.06782v1",
    "104": "1601.03764v6",
    "105": "2205.12411v5",
    "106": "1912.05671v4",
    "107": "2204.12130v2",
    "108": "2102.13042v2",
    "109": "1802.10026v4",
    "110": "1607.06520v1",
    "111": "2211.03540v2",
    "112": "1602.03924v1",
    "113": "2310.15337v1",
    "114": "2012.15738v1",
    "115": "1906.04737v1",
    "116": "2007.09540v1",
    "117": "2303.16200v4",
    "118": "1801.03454v2",
    "119": "1409.0473v7",
    "120": "2103.04000v5",
    "121": "2001.02522v4",
    "122": "2407.04622v2",
    "123": "2101.10098v1",
    "124": "2212.04717v2",
    "125": "2012.08630v1",
    "126": "2003.00688v5",
    "127": "2308.02585v3",
    "128": "1912.02164v4",
    "129": "1707.06354v2",
    "130": "2303.00957v1",
    "131": "2302.08582v2",
    "132": "2102.12452v4",
    "133": "2304.06767v4",
    "134": "2009.11462v2",
    "135": "2102.01356v5",
    "136": "1807.04975v2",
    "137": "2302.10894v3",
    "138": "2209.07858v2",
    "139": "2308.08998v2",
    "140": "2101.06286v2",
    "141": "1705.08417v2",
    "142": "2110.02439v2",
    "143": "2106.07682v1",
    "144": "1811.06521v1",
    "145": "2002.04833v4",
    "146": "2307.08823v1",
    "147": "2205.12548v3",
    "148": "2110.03605v7",
    "149": "2002.09089v4",
    "150": "2406.04093v1",
    "151": "1909.12200v3",
    "152": "2001.08361v1",
    "153": "2210.10760v1",
    "154": "2004.11207v2",
    "155": "2002.05149v6",
    "156": "1706.02515v5",
    "157": "1608.07187v4",
    "158": "1803.01128v3",
    "159": "2401.05566v3",
    "160": "2011.00620v3",
    "161": "2303.12712v5",
    "162": "2309.08600v3",
    "163": "1610.03425v3",
    "164": "2308.03296v1",
    "165": "1810.08575v1",
    "166": "2202.03629v7",
    "167": "1908.07442v5",
    "168": "2309.00667v1",
    "169": "2306.06924v2",
    "170": "2101.06060v2",
    "171": "1904.09751v2",
    "172": "2202.01602v6",
    "173": "1903.03425v2",
    "174": "1611.08219v3",
    "175": "2004.07213v2",
    "176": "1702.08608v2",
    "177": "2304.14997v4",
    "178": "2405.06624v3",
    "179": "1806.07538v2",
    "180": "2203.09509v4",
    "181": "2209.10652v1",
    "182": "2012.09838v2",
    "183": "1703.04730v3",
    "184": "1610.01644v4",
    "185": "1905.13736v4",
    "186": "1904.06347v2",
    "187": "2109.13916v5",
    "188": "1903.02020v2",
    "189": "1906.12340v2",
    "190": "2206.11795v1",
    "191": "1506.02078v2",
    "192": "2312.09390v1",
    "193": "1712.09923v1",
    "194": "1906.04341v1",
    "195": "2210.01478v3",
    "196": "2310.02238v2",
    "197": "2212.10559v3",
    "198": "2206.05862v7",
    "199": "2302.04732v1",
    "200": "2404.14700v4"
  }
}