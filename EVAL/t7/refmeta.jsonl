{"paper_id": 205242740, "title": "Human-level control through deep reinforcement learning", "author_names": ["Volodymyr Mnih", "K. Kavukcuoglu", "David Silver", "Andrei A. Rusu", "J. Veness", "Marc G. Bellemare", "Alex Graves", "Martin A. Riedmiller", "A. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charlie Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature", "abstract": null, "year": 2015, "publicationdate": "2015-02-25", "externalids": {"DOI": "10.1038/nature14236"}, "doi_lower": "10.1038/nature14236"}
{"paper_id": 50992617, "title": "Deep Reinforcement Learning for Continuous Control", "author_names": ["Simone Parisi"], "venue": "", "abstract": null, "year": 2016, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 28695052, "title": "Proximal Policy Optimization Algorithms", "author_names": ["John Schulman", "Filip Wolski", "Prafulla Dhariwal", "Alec Radford", "Oleg Klimov"], "venue": "arXiv.org", "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.", "year": 2017, "publicationdate": "2017-07-20", "externalids": {}, "doi_lower": null}
{"paper_id": 218971783, "title": "Language Models are Few-Shot Learners", "author_names": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "R. Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Ma-teusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "I. Sutskever", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "year": 2020, "publicationdate": "2020-05-28", "externalids": {}, "doi_lower": null}
{"paper_id": 257532815, "title": "GPT-4 Technical Report", "author_names": ["OpenAI Josh Achiam", "Steven Adler", "Sandhini Agarwal", "Lama Ahmad", "Ilge Akkaya", "Florencia Leoni Aleman", "Diogo Almeida", "Janko Altenschmidt", "Sam Altman", "Shyamal Anadkat", "Red Avila", "Igor Babuschkin", "S. Balaji", "Valerie Balcom", "Paul Baltescu", "Haim-ing Bao", "Mo Bavarian", "J. Belgum", "Irwan Bello", "Jake Berdine", "Gabriel Bernadett-Shapiro", "Christopher Berner", "Lenny Bogdonoff", "Oleg Boiko", "Made-laine Boyd", "Anna-Luisa Brakman", "Greg Brockman", "Tim Brooks", "Miles Brundage", "Kevin Button", "Trevor Cai", "Rosie Campbell", "Andrew Cann", "Brittany Carey", "Chelsea Carlson", "Rory Carmichael", "Brooke Chan", "Che Chang", "Fotis Chantzis", "Derek Chen", "Sully Chen", "Ruby Chen", "Jason Chen", "Mark Chen", "Benjamin Chess", "Chester Cho", "Casey Chu", "Hyung Won Chung", "Dave Cummings", "Jeremiah Currier", "Yunxing Dai", "Cory Decareaux", "Thomas Degry", "Noah Deutsch", "Damien Deville", "Arka Dhar", "David Dohan", "Steve Dowling", "Sheila Dunning", "Adrien Ecoffet", "Atty Eleti", "Tyna Eloundou", "David Farhi", "L. Fedus", "Niko Felix", "Sim'on Posada Fishman", "Juston Forte", "Is-abella Fulford", "Leo Gao", "Elie Georges", "C. Gibson", "Vik Goel", "Tarun Gogineni", "Gabriel Goh", "Raphael Gontijo-Lopes", "Jonathan Gordon", "Morgan Grafstein", "Scott Gray", "Ryan Greene", "Joshua Gross", "S. Gu", "Yufei Guo", "Chris Hallacy", "Jesse Han", "Jeff Harris", "Yuchen He", "Mike Heaton", "Johannes Heidecke", "Chris Hesse", "Alan Hickey", "W. Hickey", "Peter Hoeschele", "Brandon Houghton", "Kenny Hsu", "Shengli Hu", "Xin Hu", "Joost Huizinga", "Shantanu Jain", "Shawn Jain", "Joanne Jang", "Angela Jiang", "Roger Jiang", "Haozhun Jin", "Denny Jin", "Shino Jomoto", "B. Jonn", "Heewoo Jun", "Tomer Kaftan", "Lukasz Kaiser", "Ali Kamali", "I. Kanitscheider", "N. Keskar", "Tabarak Khan", "Logan Kilpatrick", "Jong Wook Kim", "Christina Kim", "Yongjik Kim", "Hendrik Kirchner", "J. Kiros", "Matthew Knight", "Daniel Kokotajlo", "Lukasz Kondraciuk", "Andrew Kondrich", "Aris Konstantinidis", "Kyle Kosic", "Gretchen Krueger", "Vishal Kuo", "Michael Lampe", "Ikai Lan", "Teddy Lee", "Jan Leike", "Jade Leung", "Daniel Levy", "Chak Li", "Rachel Lim", "Molly Lin", "Stephanie Lin", "Ma-teusz Litwin", "Theresa Lopez", "Ryan Lowe", "Patricia Lue", "A. Makanju", "Kim Malfacini", "Sam Manning", "Todor Markov", "Yaniv Markovski", "Bianca Martin", "Katie Mayer", "Andrew Mayne", "Bob McGrew", "S. McKinney", "Christine McLeavey", "Paul McMillan", "Jake McNeil", "David Medina", "Aalok Mehta", "Jacob Menick", "Luke Metz", "An-drey Mishchenko", "Pamela Mishkin", "Vinnie Monaco", "Evan Morikawa", "Daniel P. Mossing", "Tong Mu", "Mira Murati", "O. Murk", "David M'ely", "Ashvin Nair", "Reiichiro Nakano", "Rajeev Nayak", "Arvind Neelakantan", "Richard Ngo", "Hyeonwoo Noh", "Ouyang Long", "Cullen O'Keefe", "J. Pachocki", "A. Paino", "Joe Palermo", "Ashley Pantuliano", "Giambattista Parascandolo", "J. Parish", "Emy Parparita", "Alexandre Passos", "Mikhail Pavlov", "Andrew Peng", "Adam Perelman", "Filipe de Avila Belbute Peres", "Michael Petrov", "Henrique Pondé de Oliveira Pinto", "Michael Pokorny", "Michelle Pokrass", "Vitchyr H. Pong", "Tolly Powell", "Alethea Power", "Boris Power", "Elizabeth Proehl", "Raul Puri", "Alec Radford", "Jack W. Rae", "Aditya Ramesh", "Cameron Raymond", "Francis Real", "Kendra Rimbach", "Carl Ross", "Bob Rotsted", "Henri Roussez", "N. Ryder", "M. Saltarelli", "Ted Sanders", "Shibani Santurkar", "Girish Sastry", "Heather Schmidt", "David Schnurr", "John Schulman", "Daniel Selsam", "Kyla Sheppard", "Toki Sherbakov", "Jessica Shieh", "Sarah Shoker", "Pranav Shyam", "Szymon Sidor", "Eric Sigler", "Maddie Simens", "Jordan Sitkin", "Katarina Slama", "Ian Sohl", "Benjamin Sokolowsky", "Yang Song", "Natalie Staudacher", "F. Such", "Natalie Summers", "I. Sutskever", "Jie Tang", "N. Tezak", "Madeleine Thompson", "P. Tillet", "Amin Tootoonchian", "Elizabeth Tseng", "Preston Tuggle", "Nick Turley", "Jerry Tworek", "Juan Felipe Cer'on Uribe", "Andrea Vallone", "Arun Vijayvergiya", "Chelsea Voss", "Carroll L. Wainwright", "Justin Jay Wang", "Alvin Wang", "Ben Wang", "Jonathan Ward", "Jason Wei", "CJ Weinmann", "Akila Welihinda", "Peter Welinder", "Jiayi Weng", "Lilian Weng", "Matt Wiethoff", "Dave Willner", "Clemens Winter", "Samuel Wolrich", "Hannah Wong", "Lauren Workman", "Sherwin Wu", "Jeff Wu", "Michael Wu", "Kai Xiao", "Tao Xu", "Sarah Yoo", "Kevin Yu", "Qim-ing Yuan", "Wojciech Zaremba", "Rowan Zellers", "Chong Zhang", "Marvin Zhang", "Shengjia Zhao", "Tianhao Zheng", "Juntang Zhuang", "William Zhuk", "Barret Zoph"], "venue": "", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {}, "doi_lower": null}
{"paper_id": 270667923, "title": "Claude 3.5 Sonnet Model Card Addendum", "author_names": [], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 259950998, "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "author_names": ["Hugo Touvron", "Louis Martin", "Kevin R. Stone", "Peter Albert", "Amjad Almahairi", "Yasmine Babaei", "Niko-lay Bashlykov", "Soumya Batra", "Prajjwal Bhargava", "Shruti Bhosale", "D. Bikel", "Lukas Blecher", "Cris-tian Cantón Ferrer", "Moya Chen", "Guillem Cucurull", "David Esiobu", "Jude Fernandes", "J. Fu", "Wenyin Fu", "Brian Fuller", "Cynthia Gao", "Vedanuj Goswami", "Naman Goyal", "A. Hartshorn", "Saghar Hosseini", "Rui Hou", "Hakan Inan", "Marcin Kardas", "Viktor Kerkez", "Madian Khabsa", "Isabel M. Kloumann", "A. Korenev", "Punit Singh Koura", "M. Lachaux", "Thibaut Lavril", "Jenya Lee", "Diana Liskovich", "Yinghai Lu", "Yuning Mao", "Xavier Martinet", "Todor Mihaylov", "Pushkar Mishra", "Igor Molybog", "Yixin Nie", "Andrew Poulton", "J. Reizenstein", "Rashi Rungta", "Kalyan Saladi", "A. Schelten", "Ruan Silva", "Eric Michael Smith", "R. Subramanian", "Xia Tan", "Binh Tang", "Ross Taylor", "Adina Williams", "Jian Xiang Kuan", "Puxin Xu", "Zhengxu Yan", "Iliyan Zarov", "Yuchen Zhang", "Angela Fan", "M. Kambadur", "Sharan Narang", "Aur'elien Rodriguez", "Robert Stojnic", "Sergey Edunov", "Thomas Scialom"], "venue": "arXiv.org", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "year": 2023, "publicationdate": "2023-07-18", "externalids": {}, "doi_lower": null}
{"paper_id": 67856031, "title": "Generative Adversarial User Model for Reinforcement Learning Based Recommendation System", "author_names": ["Xinshi Chen", "Shuang Li", "Hui Li", "Shaohua Jiang", "Yuan Qi", "Le Song"], "venue": "International Conference on Machine Learning", "abstract": "There are great interests as well as many challenges in applying reinforcement learning (RL) to recommendation systems. In this setting, an online user is the environment; neither the reward function nor the environment dynamics are clearly defined, making the application of RL challenging. In this paper, we propose a novel model-based reinforcement learning framework for recommendation systems, where we develop a generative adversarial network to imitate user behavior dynamics and learn her reward function. Using this user model as the simulation environment, we develop a novel Cascading DQN algorithm to obtain a combinatorial recommendation policy which can handle a large number of candidate items efficiently. In our experiments with real data, we show this generative adversarial user model can better explain user behavior than alternatives, and the RL policy based on this model can lead to a better long-term reward for the user and higher click rate for the system.", "year": 2018, "publicationdate": "2018-12-27", "externalids": {}, "doi_lower": null}
{"paper_id": 258833055, "title": "Reflexion: language agents with verbal reinforcement learning", "author_names": ["Noah Shinn", "Federico Cassano", "Beck Labash", "A. Gopinath", "Karthik Narasimhan", "Shunyu Yao"], "venue": "Neural Information Processing Systems", "abstract": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.", "year": 2023, "publicationdate": "2023-03-20", "externalids": {}, "doi_lower": null}
{"paper_id": 260334759, "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "author_names": ["Yujia Qin", "Shi Liang", "Yining Ye", "Kunlun Zhu", "Lan Yan", "Ya-Ting Lu", "Yankai Lin", "Xin Cong", "Xiangru Tang", "Bill Qian", "Sihan Zhao", "Runchu Tian", "Ruobing Xie", "Jie Zhou", "Marc H. Gerstein", "Dahai Li", "Zhiyuan Liu", "Maosong Sun"], "venue": "International Conference on Learning Representations", "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.", "year": 2023, "publicationdate": "2023-07-31", "externalids": {"DOI": "10.48550/arXiv.2307.16789"}, "doi_lower": "10.48550/arxiv.2307.16789"}
{"paper_id": 256697342, "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "author_names": ["Timo Schick", "Jane Dwivedi-Yu", "Roberto Dessì", "R. Raileanu", "M. Lomeli", "Luke Zettlemoyer", "Nicola Cancedda", "Thomas Scialom"], "venue": "Neural Information Processing Systems", "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.", "year": 2023, "publicationdate": "2023-02-09", "externalids": {"DOI": "10.48550/arXiv.2302.04761"}, "doi_lower": "10.48550/arxiv.2302.04761"}
{"paper_id": 258999153, "title": "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker", "author_names": ["Melanie Sclar", "Sachin Kumar", "Peter West", "Alane Suhr", "Yejin Choi", "Yulia Tsvetkov"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Theory of Mind (ToM)—the ability to reason about the mental states of other people—is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision? We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity’s beliefs, their estimation of other entities’ beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches. Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances off-the-shelf neural networks’ theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines. Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset.", "year": 2023, "publicationdate": "2023-06-01", "externalids": {"DOI": "10.48550/arXiv.2306.00924"}, "doi_lower": "10.48550/arxiv.2306.00924"}
{"paper_id": 259936967, "title": "Communicative Agents for Software Development", "author_names": ["Chen Qian", "Wei Liu", "Xin Cong", "Cheng Yang", "Weize Chen", "Yusheng Su", "Juyuan Xu", "Zhiyuan Liu", "Maosong Sun"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2307.07924"}, "doi_lower": "10.48550/arxiv.2307.07924"}
{"paper_id": 258040990, "title": "Generative Agents: Interactive Simulacra of Human Behavior", "author_names": ["J. Park", "Joseph C. O’Brien", "Carrie J. Cai", "M. Morris", "Percy Liang", "Michael S. Bernstein"], "venue": "ACM Symposium on User Interface Software and Technology", "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.", "year": 2023, "publicationdate": "2023-04-07", "externalids": {"DOI": "10.1145/3586183.3606763"}, "doi_lower": "10.1145/3586183.3606763"}
{"paper_id": 263888378, "title": "RecAgent: A Novel Simulation Paradigm for Recommender Systems", "author_names": ["Lei Wang", "Jingsen Zhang", "Xu Chen", "Yankai Lin", "Ruihua Song", "Wayne Xin Zhao", "Ji-rong Wen"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.02552"}, "doi_lower": "10.48550/arxiv.2306.02552"}
{"paper_id": 265301950, "title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework", "author_names": ["Sirui Hong", "Mingchen Zhuge", "Jonathan Chen", "Xiawu Zheng", "Yuheng Cheng", "Ceyao Zhang", "Jinlin Wang", "Zili Wang", "Steven Ka Shing Yau", "Z. Lin", "Liyang Zhou", "Chenyu Ran", "Lingfeng Xiao", "Chenglin Wu", "Jürgen Schmidhuber"], "venue": "International Conference on Learning Representations", "abstract": "Remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Existing LLM-based multi-agent systems can already solve simple dialogue tasks. Solutions to more complex tasks, however, are complicated through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors. MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together. On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems. Our project can be found at https://github.com/geekan/MetaGPT", "year": 2023, "publicationdate": "2023-08-01", "externalids": {}, "doi_lower": null}
{"paper_id": 258179537, "title": "Self-Collaboration Code Generation via ChatGPT", "author_names": ["Yihong Dong", "Xue Jiang", "Zhi Jin", "Ge Li"], "venue": "ACM Transactions on Software Engineering and Methodology", "abstract": "Although large language models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, (1) Multiple LLM agents act as distinct “experts,” each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other’s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development’s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9–47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.", "year": 2023, "publicationdate": "2023-04-15", "externalids": {"DOI": "10.1145/3672459"}, "doi_lower": "10.1145/3672459"}
{"paper_id": 259317218, "title": "Personality Traits in Large Language Models", "author_names": ["Mustafa Safdari", "Gregory Serapio-Garc'ia", "Clé-ment Crepy", "Stephen Fitz", "P. Romero", "Luning Sun", "Marwa Abdulhai", "Aleksandra Faust", "Maja Matari'c"], "venue": "arXiv.org", "abstract": "The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly powerconversational agents used by the general public world-wide, the synthetic personality traits embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we present a novel and comprehensive psychometrically valid and reliable methodology for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method to 18 LLMs, we found: 1) personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss the application and ethical implications of the measurement and shaping method, in particular regarding responsible AI.", "year": 2023, "publicationdate": "2023-07-01", "externalids": {"DOI": "10.48550/arXiv.2307.00184"}, "doi_lower": "10.48550/arxiv.2307.00184"}
{"paper_id": 140408985, "title": "Big Five Inventory", "author_names": ["O. John", "E. Donahue", "R. L. Kentle"], "venue": "Encyclopedia of Personality and Individual Differences", "abstract": null, "year": 2012, "publicationdate": "2012-02-13", "externalids": {"DOI": "10.1007/978-3-319-28099-8_445-1"}, "doi_lower": "10.1007/978-3-319-28099-8_445-1"}
{"paper_id": 258060002, "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models", "author_names": ["A. Deshpande", "Vishvak Murahari", "Tanmay Rajpurohit", "A. Kalyan", "Karthik Narasimhan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and harmful to an unsuspecting user. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.", "year": 2023, "publicationdate": "2023-04-11", "externalids": {"DOI": "10.48550/arXiv.2304.05335"}, "doi_lower": "10.48550/arxiv.2304.05335"}
{"paper_id": 258832361, "title": "Reflective Linguistic Programming (RLP): A Stepping Stone in Socially-Aware AGI (SocialAGI)", "author_names": ["Kevin Fischer"], "venue": "arXiv.org", "abstract": "This paper presents Reflective Linguistic Programming (RLP), a unique approach to conversational AI that emphasizes self-awareness and strategic planning. RLP encourages models to introspect on their own predefined personality traits, emotional responses to incoming messages, and planned strategies, enabling contextually rich, coherent, and engaging interactions. A striking illustration of RLP's potential involves a toy example, an AI persona with an adversarial orientation, a demon named `Bogus' inspired by the children's fairy tale Hansel&Gretel. Bogus exhibits sophisticated behaviors, such as strategic deception and sensitivity to user discomfort, that spontaneously arise from the model's introspection and strategic planning. These behaviors are not pre-programmed or prompted, but emerge as a result of the model's advanced cognitive modeling. The potential applications of RLP in socially-aware AGI (Social AGI) are vast, from nuanced negotiations and mental health support systems to the creation of diverse and dynamic AI personas. Our exploration of deception serves as a stepping stone towards a new frontier in AGI, one filled with opportunities for advanced cognitive modeling and the creation of truly human `digital souls'.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.48550/arXiv.2305.12647"}, "doi_lower": "10.48550/arxiv.2305.12647"}
{"paper_id": 259837542, "title": "SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning", "author_names": ["Krishan Rana", "Jesse Haviland", "Sourav Garg", "Jad Abou-Chakra", "I. Reid", "Niko Sünderhauf"], "venue": "Conference on Robot Learning", "abstract": "Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic search' for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an 'iterative replanning' pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page https://sayplan.github.io.", "year": 2023, "publicationdate": "2023-07-12", "externalids": {"DOI": "10.48550/arXiv.2307.06135"}, "doi_lower": "10.48550/arxiv.2307.06135"}
{"paper_id": 260900406, "title": "CALYPSO: LLMs as Dungeon Masters' Assistants", "author_names": ["Andrew Zhu", "Lara J. Martin", "Andrew Head", "Chris Callison-Burch"], "venue": "Artificial Intelligence and Interactive Digital Entertainment Conference", "abstract": "The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to perform multiple tasks simultaneously. The DM must digest information about the game setting and monsters, synthesize scenes to present to other players, and respond to the players' interactions with the scene. Doing all of these tasks while maintaining consistency within the narrative and story world is no small feat of human cognition, making the task tiring and unapproachable to new players. Large language models (LLMs) like GPT-3 and ChatGPT have shown remarkable abilities to generate coherent natural language text. In this paper, we conduct a formative evaluation with DMs to establish the use cases of LLMs in D&D and tabletop gaming generally. We introduce CALYPSO, a system of LLM-powered interfaces that support DMs with information and inspiration specific to their own scenario. CALYPSO distills game context into bite-sized prose and helps brainstorm ideas without distracting the DM from the game. When given access to CALYPSO, DMs reported that it generated high-fidelity text suitable for direct presentation to players, and low-fidelity ideas that the DM could develop further while maintaining their creative agency. We see CALYPSO as exemplifying a paradigm of AI-augmented tools that provide synchronous creative assistance within established game worlds, and tabletop gaming more broadly.", "year": 2023, "publicationdate": "2023-08-15", "externalids": {"DOI": "10.1609/aiide.v19i1.27534"}, "doi_lower": "10.1609/aiide.v19i1.27534"}
{"paper_id": 256598146, "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "author_names": ["Zihao Wang", "Shaofei Cai", "Anji Liu", "Xiaojian Ma", "Yitao Liang"], "venue": "arXiv.org", "abstract": "We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and $\\underline{S}$elect\"($\\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of the plan execution process and providing self-$\\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.", "year": 2023, "publicationdate": "2023-02-03", "externalids": {"DOI": "10.48550/arXiv.2302.01560"}, "doi_lower": "10.48550/arxiv.2302.01560"}
{"paper_id": 260704607, "title": "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation", "author_names": ["Jiaju Lin", "Haoran Zhao", "Aochi Zhang", "Yiting Wu", "Huqiuyue Ping", "Qin Chen"], "venue": "arXiv.org", "abstract": "With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https://agentsims.com .", "year": 2023, "publicationdate": "2023-08-08", "externalids": {"DOI": "10.48550/arXiv.2308.04026"}, "doi_lower": "10.48550/arxiv.2308.04026"}
{"paper_id": 260704453, "title": "SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool", "author_names": ["Youyang Ng", "D. Miyashita", "Yasuto Hoshi", "Yasuhiro Morioka", "Osamu Torii", "Tomoya Kodama", "J. Deguchi"], "venue": "arXiv.org", "abstract": "Large Language Model (LLM) based Generative AI systems have seen significant progress in recent years. Integrating a knowledge retrieval architecture allows for seamless integration of private data into publicly available Generative AI systems using pre-trained LLM without requiring additional model fine-tuning. Moreover, Retrieval-Centric Generation (RCG) approach, a promising future research direction that explicitly separates roles of LLMs and retrievers in context interpretation and knowledge memorization, potentially leads to more efficient implementation. SimplyRetrieve is an open-source tool with the goal of providing a localized, lightweight, and user-friendly interface to these sophisticated advancements to the machine learning community. SimplyRetrieve features a GUI and API based RCG platform, assisted by a Private Knowledge Base Constructor and a Retrieval Tuning Module. By leveraging these capabilities, users can explore the potential of RCG for improving generative AI performance while maintaining privacy standards. The tool is available at https://github.com/RCGAI/SimplyRetrieve with an MIT license.", "year": 2023, "publicationdate": "2023-08-08", "externalids": {"DOI": "10.48550/arXiv.2308.03983"}, "doi_lower": "10.48550/arxiv.2308.03983"}
{"paper_id": 260438773, "title": "Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents", "author_names": ["Ziheng Huang", "S. Gutierrez", "Hemanth Kamana", "Stephen MacNeil"], "venue": "ACM Symposium on User Interface Software and Technology", "abstract": "The recent advent of large language models (LLM) has resulted in high-performing conversational agents such as ChatGPT. These agents must remember key information from an ongoing conversation to provide responses that are contextually relevant to the user. However, these agents have limited memory and can be distracted by irrelevant parts of the conversation. While many strategies exist to manage conversational memory, users currently lack affordances for viewing and controlling what the agent remembers, resulting in a poor mental model and conversational breakdowns. In this paper, we present Memory Sandbox, an interactive system and design probe that allows users to manage the conversational memory of LLM-powered agents. By treating memories as data objects that can be viewed, manipulated, recorded, summarized, and shared across conversations, Memory Sandbox provides interaction affordances for users to manage how the agent should ‘see’ the conversation.", "year": 2023, "publicationdate": "2023-08-03", "externalids": {"DOI": "10.1145/3586182.3615796"}, "doi_lower": "10.1145/3586182.3615796"}
{"paper_id": 258887849, "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models", "author_names": ["Guanzhi Wang", "Yuqi Xie", "Yunfan Jiang", "Ajay Mandlekar", "Chaowei Xiao", "Yuke Zhu", "Linxi (Jim) Fan", "Anima Anandkumar"], "venue": "Trans. Mach. Learn. Res.", "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.", "year": 2023, "publicationdate": "2023-05-25", "externalids": {"DOI": "10.48550/arXiv.2305.16291"}, "doi_lower": "10.48550/arxiv.2305.16291"}
{"paper_id": 259088875, "title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory", "author_names": ["Chenxu Hu", "Jie Fu", "Chenzhuang Du", "Simian Luo", "J. Zhao", "Hang Zhao"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .", "year": 2023, "publicationdate": "2023-06-06", "externalids": {"DOI": "10.48550/arXiv.2306.03901"}, "doi_lower": "10.48550/arxiv.2306.03901"}
{"paper_id": 255595513, "title": "Memory Augmented Large Language Models are Computationally Universal", "author_names": ["Dale Schuurmans"], "venue": "arXiv.org", "abstract": "We show that transformer-based large language models are computationally universal when augmented with an external memory. Any deterministic language model that conditions on strings of bounded length is equivalent to a finite automaton, hence computationally limited. However, augmenting such models with a read-write memory creates the possibility of processing arbitrarily large inputs and, potentially, simulating any algorithm. We establish that an existing large language model, Flan-U-PaLM 540B, can be combined with an associative read-write memory to exactly simulate the execution of a universal Turing machine, $U_{15,2}$. A key aspect of the finding is that it does not require any modification of the language model weights. Instead, the construction relies solely on designing a form of stored instruction computer that can subsequently be programmed with a specific set of prompts.", "year": 2023, "publicationdate": "2023-01-10", "externalids": {"DOI": "10.48550/arXiv.2301.04589"}, "doi_lower": "10.48550/arxiv.2301.04589"}
{"paper_id": 261048772, "title": "ExpeL: LLM Agents Are Experiential Learners", "author_names": ["Andrew Zhao", "Daniel Huang", "Quentin Xu", "Matthieu Lin", "Y. Liu", "Gao Huang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.", "year": 2023, "publicationdate": "2023-08-20", "externalids": {"DOI": "10.48550/arXiv.2308.10144"}, "doi_lower": "10.48550/arxiv.2308.10144"}
{"paper_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "author_names": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "Ed H. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "venue": "Neural Information Processing Systems", "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 276928025, "title": "Self-Corrective Task Planning by Inverse Prompting with Large Language Models", "author_names": ["Jiho Lee", "Hayun Lee", "Jonghyeon Kim", "Kyungjae Lee", "Eunwoo Kim"], "venue": "IEEE International Conference on Robotics and Automation", "abstract": "In robot task planning, large language models (LLMs) have shown significant promise in generating complex and long-horizon action sequences. However, it is observed that LLMs often produce responses that sound plausible but are not accurate. To address these problems, existing methods typically employ predefined error sets or external knowledge sources, requiring human efforts and computation resources. Recently, self-correction approaches have emerged, where LLM generates and refines plans, identifying errors by itself. Despite their effectiveness, they are more prone to failures in correction due to insufficient reasoning. In this paper, we introduce InversePrompt, a novel self-corrective task planning approach that leverages inverse prompting to enhance interpretability. Our method incorporates reasoning steps to provide clear, interpretable feedback. It generates inverse actions corresponding to the initially generated actions and verifies whether these inverse actions can restore the system to its original state, explicitly validating the logical coherence of the generated plans. The results on benchmark datasets show an average 16.3% higher success rate over existing LLM-based task planning methods. Our approach offers clearer justifications for feedback in real-world environments, resulting in more successful task completion than existing self-correction approaches across various scenarios.", "year": 2025, "publicationdate": "2025-03-10", "externalids": {"DOI": "10.1109/ICRA55743.2025.11128028"}, "doi_lower": "10.1109/icra55743.2025.11128028"}
{"paper_id": 258967566, "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models", "author_names": ["Binfeng Xu", "Zhiyuan Peng", "Bowen Lei", "Subhabrata Mukherjee", "Yuchen Liu", "Dongkuan Xu"], "venue": "arXiv.org", "abstract": "Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model parameters. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant potential for truly efficient and scalable ALM systems.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.18323"}, "doi_lower": "10.48550/arxiv.2305.18323"}
{"paper_id": 258960143, "title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks", "author_names": ["Bill Yuchen Lin", "Yicheng Fu", "Karina Yang", "Prithviraj Ammanabrolu", "Faeze Brahman", "Shiyu Huang", "Chandra Bhagavatula", "Yejin Choi", "Xiang Ren"], "venue": "Neural Information Processing Systems", "abstract": "We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks.", "year": 2023, "publicationdate": "2023-05-27", "externalids": {"DOI": "10.48550/arXiv.2305.17390"}, "doi_lower": "10.48550/arxiv.2305.17390"}
{"paper_id": 247595263, "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "author_names": ["Xuezhi Wang", "Jason Wei", "D. Schuurmans", "Quoc Le", "Ed H. Chi", "Denny Zhou"], "venue": "International Conference on Learning Representations", "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).", "year": 2022, "publicationdate": "2022-03-21", "externalids": {}, "doi_lower": null}
{"paper_id": 258762525, "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "author_names": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "T. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "venue": "Neural Information Processing Systems", "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {"DOI": "10.48550/arXiv.2305.10601"}, "doi_lower": "10.48550/arxiv.2305.10601"}
{"paper_id": 261030303, "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models", "author_names": ["Maciej Besta", "Nils Blach", "Aleš Kubíček", "Robert Gerstenberger", "Lukas Gianinazzi", "Joanna Gajda", "Tomasz Lehmann", "Michal Podstawski", "H. Niewiadomski", "P. Nyczyk", "Torsten Hoefler"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "We introduce Graph of Thoughts (GoT): a framework that\nadvances prompting capabilities in large language models\n(LLMs) beyond those offered by paradigms such as \nChain-of-Thought or Tree of Thoughts (ToT). The key idea and \nprimary advantage of GoT is the ability to model the information \ngenerated by an LLM as an arbitrary graph, where units of \ninformation (\"LLM thoughts\") are vertices, and edges correspond\nto dependencies between these vertices. This approach enables \ncombining arbitrary LLM thoughts into synergistic outcomes, \ndistilling the essence of whole networks of thoughts,\nor enhancing thoughts using feedback loops. We illustrate\nthat GoT offers advantages over state of the art on different\ntasks, for example increasing the quality of sorting by 62%\nover ToT, while simultaneously reducing costs by >31%.\nWe ensure that GoT is extensible with new thought \ntransformations and thus can be used to spearhead new prompting\nschemes. This work brings the LLM reasoning closer to human \nthinking or brain mechanisms such as recurrence, both\nof which form complex networks", "year": 2023, "publicationdate": "2023-08-18", "externalids": {"DOI": "10.1609/aaai.v38i16.29720"}, "doi_lower": "10.1609/aaai.v38i16.29720"}
{"paper_id": 261049794, "title": "Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models", "author_names": ["Bilgehan Sel", "Ahmad S. Al-Tawaha", "Vanshaj Khattar", "Lucy Wang", "R. Jia", "Ming Jin"], "venue": "International Conference on Machine Learning", "abstract": "Current literature, aiming to surpass the\"Chain-of-Thought\"approach, often resorts to external modi operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. Due to their myopic perspective, they escalate the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways. By employing algorithmic examples fully in-context, this overarching view of the whole process exploits the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and even more recent multi-query strategies that employ an extensive tree search algorithms while using significantly fewer tokens. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application. The code and related content can be found in: https://algorithm-of-thoughts.github.io.", "year": 2023, "publicationdate": "2023-08-20", "externalids": {"DOI": "10.48550/arXiv.2308.10379"}, "doi_lower": "10.48550/arxiv.2308.10379"}
{"paper_id": 258865812, "title": "Reasoning with Language Model is Planning with World Model", "author_names": ["Shibo Hao", "Yi Gu", "Haodi Ma", "Joshua Jiahua Hong", "Zhen Wang", "D. Wang", "Zhiting Hu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.48550/arXiv.2305.14992"}, "doi_lower": "10.48550/arxiv.2305.14992"}
{"paper_id": 258298051, "title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency", "author_names": ["B. Liu", "Yuqian Jiang", "Xiaohan Zhang", "Qian Liu", "Shiqi Zhang", "Joydeep Biswas", "Peter Stone"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems.\\footnote{The code and results are publicly available at https://github.com/Cranial-XIX/llm-pddl.git.", "year": 2023, "publicationdate": "2023-04-22", "externalids": {"DOI": "10.48550/arXiv.2304.11477"}, "doi_lower": "10.48550/arxiv.2304.11477"}
{"paper_id": 260887774, "title": "Dynamic Planning with a LLM", "author_names": ["Gautier Dagan", "Frank Keller", "A. Lascarides"], "venue": "arXiv.org", "abstract": "While Large Language Models (LLMs) can solve many NLP tasks in zero-shot settings, applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows. Planning requires understanding the likely effects of one's actions and identifying whether the current environment satisfies the goal state. While symbolic planners find optimal solutions quickly, they require a complete and accurate representation of the planning problem, severely limiting their use in practical scenarios. In contrast, modern LLMs cope with noisy observations and high levels of uncertainty when reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a neuro-symbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, LLM-DP solves Alfworld faster and more efficiently than a naive LLM ReAct baseline.", "year": 2023, "publicationdate": "2023-08-11", "externalids": {"DOI": "10.48550/arXiv.2308.06391"}, "doi_lower": "10.48550/arxiv.2308.06391"}
{"paper_id": 254408960, "title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models", "author_names": ["Chan Hee Song", "Jiaman Wu", "Clay Washington", "Brian M. Sadler", "Wei-Lun Chao", "Yu Su"], "venue": "IEEE International Conference on Computer Vision", "abstract": "This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. 1", "year": 2022, "publicationdate": "2022-12-08", "externalids": {"DOI": "10.1109/ICCV51070.2023.00280"}, "doi_lower": "10.1109/iccv51070.2023.00280"}
{"paper_id": 250451569, "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models", "author_names": ["Wenlong Huang", "F. Xia", "Ted Xiao", "Harris Chan", "Jacky Liang", "Peter R. Florence", "Andy Zeng", "Jonathan Tompson", "Igor Mordatch", "Yevgen Chebotar", "P. Sermanet", "Noah Brown", "Tomas Jackson", "Linda Luu", "S. Levine", "Karol Hausman", "Brian Ichter"], "venue": "Conference on Robot Learning", "abstract": "Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.", "year": 2022, "publicationdate": "2022-07-12", "externalids": {"DOI": "10.48550/arXiv.2207.05608"}, "doi_lower": "10.48550/arxiv.2207.05608"}
{"paper_id": 257900871, "title": "Self-Refine: Iterative Refinement with Self-Feedback", "author_names": ["Aman Madaan", "Niket Tandon", "Prakhar Gupta", "Skyler Hallinan", "Luyu Gao", "Sarah Wiegreffe", "Uri Alon", "Nouha Dziri", "Shrimai Prabhumoye", "Yiming Yang", "S. Welleck", "Bodhisattwa Prasad Majumder", "Shashank Gupta", "A. Yazdanbakhsh", "Peter Clark"], "venue": "Neural Information Processing Systems", "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {"DOI": "10.48550/arXiv.2303.17651"}, "doi_lower": "10.48550/arxiv.2303.17651"}
{"paper_id": 260350986, "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning", "author_names": ["Ning Miao", "Y. Teh", "Tom Rainforth"], "venue": "International Conference on Learning Representations", "abstract": "The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on three datasets (GSM8K, MathQA, and MATH) and find that it successfully recognizes errors and, in turn, increases final answer accuracies.", "year": 2023, "publicationdate": "2023-08-01", "externalids": {"DOI": "10.48550/arXiv.2308.00436"}, "doi_lower": "10.48550/arxiv.2308.00436"}
{"paper_id": 260438734, "title": "InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent", "author_names": ["Po-Lin Chen", "Cheng-Shang Chang"], "venue": "arXiv.org", "abstract": "This research paper delves into the integration of OpenAI's ChatGPT into embodied agent systems, evaluating its influence on interactive decision-making benchmark. Drawing a parallel to the concept of people assuming roles according to their unique strengths, we introduce InterAct. In this approach, we feed ChatGPT with varied prompts, assigning it a numerous roles like a checker and a sorter, then integrating them with the original language model. Our research shows a remarkable success rate of 98% in AlfWorld, which consists of 6 different tasks in a simulated household environment, emphasizing the significance of proficient prompt engineering. The results highlight ChatGPT's competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.", "year": 2023, "publicationdate": "2023-08-03", "externalids": {"DOI": "10.48550/arXiv.2308.01552"}, "doi_lower": "10.48550/arxiv.2308.01552"}
{"paper_id": 258841374, "title": "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models", "author_names": ["Z. Chen", "Kun Zhou", "Beichen Zhang", "Zheng Gong", "Wayne Xin Zhao", "Ji-rong Wen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose \\textbf{ChatCoT}, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs. In ChatCoT, we model the chain-of-thought~(CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting. At each turn, LLMs can either interact with tools or perform the reasoning. Our approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. Specially, we initialize the early turns of the conversation by the tools, tasks and reasoning format, and propose an iterative \\emph{tool-augmented reasoning} step to perform step-by-step tool-augmented reasoning. The experiment results on two complex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of ChatCoT on complex reasoning tasks, achieving a 6.8\\% relative improvement over the state-of-the-art baseline. Our code and data are available at: \\url{https://github.com/RUCAIBOX/ChatCoT}.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14323"}, "doi_lower": "10.48550/arxiv.2305.14323"}
{"paper_id": 245329531, "title": "WebGPT: Browser-assisted question-answering with human feedback", "author_names": ["Reiichiro Nakano", "Jacob Hilton", "S. Balaji", "Jeff Wu", "Ouyang Long", "Christina Kim", "Christopher Hesse", "Shantanu Jain", "Vineet Kosaraju", "W. Saunders", "Xu Jiang", "K. Cobbe", "Tyna Eloundou", "Gretchen Krueger", "Kevin Button", "Matthew Knight", "Benjamin Chess", "John Schulman"], "venue": "arXiv.org", "abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.", "year": 2021, "publicationdate": "2021-12-17", "externalids": {}, "doi_lower": null}
{"paper_id": 265381326, "title": "TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents", "author_names": ["Jingqing Ruan", "Yihong Chen", "Bin Zhang", "Zhiwei Xu", "Tianpeng Bao", "Guoqing Du", "Shiwei Shi", "Hangyu Mao", "Xingyu Zeng", "Rui Zhao"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2308.03427"}, "doi_lower": "10.48550/arxiv.2308.03427"}
{"paper_id": 258865184, "title": "Gorilla: Large Language Model Connected with Massive APIs", "author_names": ["Shishir G. Patil", "Tianjun Zhang", "Xin Wang", "Joseph E. Gonzalez"], "venue": "Neural Information Processing Systems", "abstract": "Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu", "year": 2023, "publicationdate": "2023-05-24", "externalids": {"DOI": "10.52202/079017-4020"}, "doi_lower": "10.52202/079017-4020"}
{"paper_id": 276344352, "title": "API-Bank: A Benchmark for Tool-Augmented LLMs", "author_names": ["Minghao Li", "Feifan Song", "Bowen Yu", "Haiyang Yu", "Zhoujun Li", "Fei Huang", "Yongbin Li"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2304.08244"}, "doi_lower": "10.48550/arxiv.2304.08244"}
{"paper_id": 259138886, "title": "RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs", "author_names": ["Yifan Song", "Weimin Xiong", "Dawei Zhu", "Cheng Li", "Ke Wang", "Ye Tian", "Sujian Li"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.06624"}, "doi_lower": "10.48550/arxiv.2306.06624"}
{"paper_id": 257804802, "title": "TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs", "author_names": ["Yaobo Liang", "Chenfei Wu", "Ting Song", "Wenshan Wu", "Yan Xia", "Yu Liu", "Yangyiwen Ou", "Shuai Lu", "Lei Ji", "Shaoguang Mao", "Yun Wang", "Linjun Shou", "Ming Gong", "Nan Duan"], "venue": "Intelligent Computing", "abstract": "Artificial Intelligence (AI) has made incredible progress recently. On the one hand, advanced foundation models like ChatGPT can offer powerful conversation, in-context learning and code generation abilities on a broad range of open-domain tasks. They can also generate high-level solution outlines for domain-specific tasks based on the common sense knowledge they have acquired. However, they still face difficulties with some specialized tasks because they lack enough domain-specific data during pre-training or they often have errors in their neural network computations on those tasks that need accurate executions. On the other hand, there are also many existing models and systems (symbolic-based or neural-based) that can do some domain-specific tasks very well. However, due to the different implementation or working mechanisms, they are not easily accessible or compatible with foundation models. Therefore, there is a clear and pressing need for a mechanism that can leverage foundation models to propose task solution outlines and then automatically match some of the sub-tasks in the outlines to the off-the-shelf models and systems with special functionalities to complete them. Inspired by this, we introduce TaskMatrix.AI as a new AI ecosystem that connects foundation models with millions of APIs for task completion. Unlike most previous work that aimed to improve a single AI model, TaskMatrix.AI focuses more on using existing foundation models (as a brain-like central system) and APIs of other AI models and systems (as sub-task solvers) to achieve diversified tasks in both digital and physical domains. As a position paper, we will present our vision of how to build such an ecosystem, explain each key component, and use study cases to illustrate both the feasibility of this vision and the main challenges we need to address next.", "year": 2023, "publicationdate": "2023-03-29", "externalids": {"DOI": "10.48550/arXiv.2303.16434"}, "doi_lower": "10.48550/arxiv.2303.16434"}
{"paper_id": 258049306, "title": "OpenAGI: When LLM Meets Domain Experts", "author_names": ["Yingqiang Ge", "Wenyue Hua", "Kai Mei", "Jianchao Ji", "Juntao Tan", "Shuyuan Xu", "Zelong Li", "Yongfeng Zhang"], "venue": "Neural Information Processing Systems", "abstract": "Human intelligence excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive intelligent models, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research platform designed for multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.", "year": 2023, "publicationdate": "2023-04-10", "externalids": {"DOI": "10.48550/arXiv.2304.04370"}, "doi_lower": "10.48550/arxiv.2304.04370"}
{"paper_id": 257505358, "title": "ViperGPT: Visual Inference via Python Execution for Reasoning", "author_names": ["D'idac Sur'is", "Sachit Menon", "Carl Vondrick"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, limiting interpretability and generalization. Learning modular programs presents a promising alternative, but has proven challenging due to the difficulty of learning both the programs and modules simultaneously. We introduce ${\\color{green}{\\text{ViperGPT}}}$, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query. ${\\color{green}{\\text{ViperGPT}}}$ utilizes a provided API to access the available modules, and composes them by generating Python code that is later executed. This simple approach requires no further training, and achieves state-of-the-art results across various complex visual tasks.", "year": 2023, "publicationdate": "2023-03-14", "externalids": {"DOI": "10.1109/ICCV51070.2023.01092"}, "doi_lower": "10.1109/iccv51070.2023.01092"}
{"paper_id": 257637012, "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action", "author_names": ["Zhengyuan Yang", "Linjie Li", "Jianfeng Wang", "Kevin Lin", "E. Azarnasab", "Faisal Ahmed", "Zicheng Liu", "Ce Liu", "Michael Zeng", "Lijuan Wang"], "venue": "arXiv.org", "abstract": "We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence, MM-REACT introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and aligned file names for dense visual signals such as images and videos. MM-REACT's prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding. Furthermore, we discuss and compare MM-REACT's system paradigm with an alternative approach that extends language models for multimodal scenarios through joint finetuning. Code, demo, video, and visualization are available at https://multimodal-react.github.io/", "year": 2023, "publicationdate": "2023-03-20", "externalids": {"DOI": "10.48550/arXiv.2303.11381"}, "doi_lower": "10.48550/arxiv.2303.11381"}
{"paper_id": 260202947, "title": "S3: Social-network Simulation System with Large Language Model-Empowered Agents", "author_names": ["Chen Gao", "Xiaochong Lan", "Zhi-jie Lu", "Jinzhu Mao", "J. Piao", "Huandong Wang", "Depeng Jin", "Yong Li"], "venue": "Social Science Research Network", "abstract": "Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\\textbf{S}$ocial network $\\textbf{S}$imulation $\\textbf{S}$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.", "year": 2023, "publicationdate": "2023-07-27", "externalids": {"DOI": "10.48550/arXiv.2307.14984"}, "doi_lower": "10.48550/arxiv.2307.14984"}
{"paper_id": 251403008, "title": "Social Simulacra: Creating Populated Prototypes for Social Computing Systems", "author_names": ["J. Park", "Lindsay Popowski", "Carrie J. Cai", "M. Morris", "Percy Liang", "Michael S. Bernstein"], "venue": "ACM Symposium on User Interface Software and Technology", "abstract": "Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many challenges do not arise until a system is populated at a larger scale. Can a designer understand how a social system might behave when populated, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping technique that generates a breadth of realistic social interactions that may emerge when a social computing system is populated. Social simulacra take as input the designer’s description of a community’s design—goal, rules, and member personas—and produce as output an instance of that design with simulated behavior, including posts, replies, and anti-social behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of “what if?” scenarios where community members or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large language models’ training data already includes a wide variety of positive and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacra from actual community behavior and that social computing designers successfully refine their social computing designs when using social simulacra.", "year": 2022, "publicationdate": "2022-08-08", "externalids": {"DOI": "10.1145/3526113.3545616"}, "doi_lower": "10.1145/3526113.3545616"}
{"paper_id": 257900712, "title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society", "author_names": ["G. Li", "Hasan Hammoud", "Hani Itani", "Dmitrii Khizbullin", "Bernard Ghanem"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2303.17760"}, "doi_lower": "10.48550/arxiv.2303.17760"}
{"paper_id": 258947756, "title": "Training Socially Aligned Language Models in Simulated Human Society", "author_names": ["Ruibo Liu", "Ruixin Yang", "Chenyan Jia", "Ge Zhang", "Denny Zhou", "Andrew M. Dai", "Diyi Yang", "Soroush Vosoughi"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2305.16960"}, "doi_lower": "10.48550/arxiv.2305.16960"}
{"paper_id": 258823336, "title": "Introspective Tips: Large Language Model for In-Context Decision Making", "author_names": ["Liting Chen", "Lu Wang", "Hang Dong", "Yali Du", "Jie Yan", "Fan Yang", "Shuang Li", "Pu Zhao", "Si Qin", "S. Rajmohan", "Qingwei Lin", "Dongmei Zhang"], "venue": "arXiv.org", "abstract": "The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks. In this study, we employ ``Introspective Tips\"to facilitate LLMs in self-optimizing their decision-making. By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips. Our method enhances the agent's performance in both few-shot and zero-shot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games. Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations. Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making. Experiments involving over 100 games in TextWorld illustrate the superior performance of our approach.", "year": 2023, "publicationdate": "2023-05-19", "externalids": {"DOI": "10.48550/arXiv.2305.11598"}, "doi_lower": "10.48550/arxiv.2305.11598"}
{"paper_id": 250264533, "title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents", "author_names": ["Shunyu Yao", "Howard Chen", "John Yang", "Karthik Narasimhan"], "venue": "Neural Information Processing Systems", "abstract": "Existing benchmarks for grounding language in interactive environments either lack real-world linguistic elements, or prove difficult to scale up due to substantial human involvement in the collection of data or feedback signals. To bridge this gap, we develop WebShop -- a simulated e-commerce website environment with $1.18$ million real-world products and $12,087$ crowd-sourced text instructions. Given a text instruction specifying a product requirement, an agent needs to navigate multiple types of webpages and issue diverse actions to find, customize, and purchase an item. WebShop provides several challenges for language grounding including understanding compositional instructions, query (re-)formulation, comprehending and acting on noisy text in webpages, and performing strategic exploration. We collect over $1,600$ human demonstrations for the task, and train and evaluate a diverse range of agents using reinforcement learning, imitation learning, and pre-trained image and language models. Our best model achieves a task success rate of $29\\%$, which outperforms rule-based heuristics ($9.6\\%$) but is far lower than human expert performance ($59\\%$). We also analyze agent and human trajectories and ablate various model components to provide insights for developing future agents with stronger language understanding and decision making abilities. Finally, we show that agents trained on WebShop exhibit non-trivial sim-to-real transfer when evaluated on amazon.com and ebay.com, indicating the potential value of WebShop in developing practical web-based agents that can operate in the wild.", "year": 2022, "publicationdate": "2022-07-04", "externalids": {"DOI": "10.48550/arXiv.2207.01206"}, "doi_lower": "10.48550/arxiv.2207.01206"}
{"paper_id": 260681803, "title": "EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education", "author_names": ["Yuhao Dan", "Zhikai Lei", "Yiyang Gu", "Yong Li", "Jia-Peng Yin", "Jiaju Lin", "Linhao Ye", "Zhiyan Tie", "Yougen Zhou", "Yilei Wang", "Aimin Zhou", "Zeyang Zhou", "Qin Chen", "Jie Zhou", "Liang He", "Xipeng Qiu"], "venue": "arXiv.org", "abstract": "EduChat (https://www.educhat.top/) is a large-scale language model (LLM)-based chatbot system in the education domain. Its goal is to support personalized, fair, and compassionate intelligent education, serving teachers, students, and parents. Guided by theories from psychology and education, it further strengthens educational functions such as open question answering, essay assessment, Socratic teaching, and emotional support based on the existing basic LLMs. Particularly, we learn domain-specific knowledge by pre-training on the educational corpus and stimulate various skills with tool use by fine-tuning on designed system prompts and instructions. Currently, EduChat is available online as an open-source project, with its code, data, and model parameters available on platforms (e.g., GitHub https://github.com/icalk-nlp/EduChat, Hugging Face https://huggingface.co/ecnu-icalk ). We also prepare a demonstration of its capabilities online (https://vimeo.com/851004454). This initiative aims to promote research and applications of LLMs for intelligent education.", "year": 2023, "publicationdate": "2023-08-05", "externalids": {"DOI": "10.48550/arXiv.2308.02773"}, "doi_lower": "10.48550/arxiv.2308.02773"}
{"paper_id": 259129428, "title": "Mind2Web: Towards a Generalist Agent for the Web", "author_names": ["Xiang Deng", "Yu Gu", "Boyuan Zheng", "Shijie Chen", "Samuel Stevens", "Boshi Wang", "Huan Sun", "Yu Su"], "venue": "Neural Information Processing Systems", "abstract": "We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.", "year": 2023, "publicationdate": "2023-06-09", "externalids": {"DOI": "10.48550/arXiv.2306.06070"}, "doi_lower": "10.48550/arxiv.2306.06070"}
{"paper_id": 260611249, "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization", "author_names": ["Weiran Yao", "Shelby Heinecke", "Juan Carlos Niebles", "Zhiwei Liu", "Yihao Feng", "Le Xue", "Rithesh Murthy", "Zeyuan Chen", "Jianguo Zhang", "Devansh Arpit", "Ran Xu", "P. Mùi", "Haiquan Wang", "Caiming Xiong", "S. Savarese"], "venue": "International Conference on Learning Representations", "abstract": "Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.", "year": 2023, "publicationdate": "2023-08-04", "externalids": {"DOI": "10.48550/arXiv.2308.02151"}, "doi_lower": "10.48550/arxiv.2308.02151"}
{"paper_id": 261048771, "title": "RAH! RecSys-Assistant-Human: A Human-Central Recommendation Framework with Large Language Models", "author_names": ["Yubo Shu", "Hansu Gu", "Peng Zhang", "Haonan Zhang", "T. Lu", "Dongsheng Li", "Ning Gu"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2308.09904"}, "doi_lower": "10.48550/arxiv.2308.09904"}
{"paper_id": 259501567, "title": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models", "author_names": ["Zhao Mandi", "Shreeya Jain", "Shuran Song"], "venue": "IEEE International Conference on Robotics and Automation", "abstract": "We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset that evaluates LLMs’ agent representation and reasoning capability. We experimentally demonstrate the effectiveness of our approach — it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility — in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together.", "year": 2023, "publicationdate": "2023-07-10", "externalids": {"DOI": "10.1109/ICRA57147.2024.10610855"}, "doi_lower": "10.1109/icra57147.2024.10610855"}
{"paper_id": 258841118, "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate", "author_names": ["Yilun Du", "Shuang Li", "A. Torralba", "J. Tenenbaum", "Igor Mordatch"], "venue": "International Conference on Machine Learning", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14325"}, "doi_lower": "10.48550/arxiv.2305.14325"}
{"paper_id": 266435868, "title": "AppAgent: Multimodal Agents as Smartphone Users", "author_names": ["C. Zhang", "Zhao Yang", "Jiaxuan Liu", "Yanda Li", "Yucheng Han", "Xin Chen", "Zebiao Huang", "Bin Fu", "Gang Yu"], "venue": "International Conference on Human Factors in Computing Systems", "abstract": "Recent advancements in large language models (LLMs) have led to the creation of intelligent agents capable of performing complex tasks. This paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications. Our framework allows the agent to mimic human-like interactions such as tapping and swiping through a simplified action space, eliminating the need for system back-end access and enhancing its versatility across various apps. Central to the agent’s functionality is an innovative in-context learning method, where it either autonomously explores or learns from human demonstrations, creating a knowledge base used to execute complex tasks across diverse applications. We conducted extensive testing with our agent on over 50 tasks spanning 10 applications, ranging from social media to sophisticated image editing tools. Additionally, a user study confirmed the agent’s superior performance and practicality in handling a diverse array of high-level tasks, demonstrating its effectiveness in real-world settings. Our project page is available at https://appagent-official.github.io/.", "year": 2023, "publicationdate": "2023-12-21", "externalids": {"DOI": "10.1145/3706598.3713600"}, "doi_lower": "10.1145/3706598.3713600"}
{"paper_id": 258832563, "title": "Augmenting Autotelic Agents with Large Language Models", "author_names": ["Cédric Colas", "Laetitia Teodorescu", "Pierre-Yves Oudeyer", "Xingdi Yuan", "Marc-Alexandre Côté"], "venue": "CoLLAs", "abstract": "Humans learn to master open-ended repertoires of skills by imagining and practicing their own goals. This autotelic learning process, literally the pursuit of self-generated (auto) goals (telos), becomes more and more open-ended as the goals become more diverse, abstract and creative. The resulting exploration of the space of possible skills is supported by an inter-individual exploration: goal representations are culturally evolved and transmitted across individuals, in particular using language. Current artificial agents mostly rely on predefined goal representations corresponding to goal spaces that are either bounded (e.g. list of instructions), or unbounded (e.g. the space of possible visual inputs) but are rarely endowed with the ability to reshape their goal representations, to form new abstractions or to imagine creative goals. In this paper, we introduce a language model augmented autotelic agent (LMA3) that leverages a pretrained language model (LM) to support the representation, generation and learning of diverse, abstract, human-relevant goals. The LM is used as an imperfect model of human cultural transmission; an attempt to capture aspects of humans' common-sense, intuitive physics and overall interests. Specifically, it supports three key components of the autotelic architecture: 1)~a relabeler that describes the goals achieved in the agent's trajectories, 2)~a goal generator that suggests new high-level goals along with their decomposition into subgoals the agent already masters, and 3)~reward functions for each of these goals. Without relying on any hand-coded goal representations, reward functions or curriculum, we show that LMA3 agents learn to master a large diversity of skills in a task-agnostic text-based environment.", "year": 2023, "publicationdate": "2023-05-21", "externalids": {"DOI": "10.48550/arXiv.2305.12487"}, "doi_lower": "10.48550/arxiv.2305.12487"}
{"paper_id": 259836864, "title": "Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems", "author_names": ["N. Nascimento", "Paulo Alencar", "Donald D. Cowan"], "venue": "2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C)", "abstract": "The complexity of managing multiagent systems (MASs) in autonomic computing can be mitigated using a self-adaptation approach, where systems are equipped to monitor and adjust themselves based on specific concerns. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, the tasks of boosting communication expressiveness within MASs and logically processing a multitude of variables in dynamic environments are still challenging. This paper presents a novel strategy: integrating large language models (LLMs) like GPT-based technologies into MASs to boost communication and agent autonomy. Our proposal encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models. This is grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments. We illustrate our approach through a marketplace scenario. This work represents a paradigm shift in MAS self-adaptation, utilizing LLMs' capabilities and indicating further research opportunities to assess LLMs' applicability in more complex MAS scenarios. This could pave the way for more potent problem-solving capabilities and refined communication within MASs.", "year": 2023, "publicationdate": "2023-07-12", "externalids": {"DOI": "10.1109/ACSOS-C58168.2023.00048"}, "doi_lower": "10.1109/acsos-c58168.2023.00048"}
{"paper_id": 259165464, "title": "Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind", "author_names": ["Swarnadeep Saha", "Peter Hase", "Mohit Bansal"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2306.09299"}, "doi_lower": "10.48550/arxiv.2306.09299"}
{"paper_id": 258947227, "title": "Mindstorms in Natural Language-Based Societies of Mind", "author_names": ["Mingchen Zhuge", "Haozhe Liu", "Francesco Faccio", "Dylan R. Ashley", "R'obert Csord'as", "Anand Gopalakrishnan", "Abdullah Hamdi", "Hasan Hammoud", "Vincent Herrmann", "Kazuki Irie", "Louis Kirsch", "Bing-chuan Li", "G. Li", "Shuming Liu", "Jinjie Mai", "Piotr Pikekos", "A. Ramesh", "Imanol Schlag", "Weimin Shi", "Aleksandar Stani'c", "Wenyi Wang", "Yu‐Han Wang", "Mengmeng Xu", "Deng-Ping Fan", "Bernard Ghanem", "J. Schmidhuber"], "venue": "Computational Visual Media", "abstract": null, "year": 2025, "publicationdate": "2025-02-01", "externalids": {"DOI": "10.26599/cvm.2025.9450460"}, "doi_lower": "10.26599/cvm.2025.9450460"}
{"paper_id": 251719353, "title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies", "author_names": ["Gati Aher", "RosaI. Arriaga", "A. Kalai"], "venue": "International Conference on Machine Learning", "abstract": "We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a\"hyper-accuracy distortion\"present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.", "year": 2022, "publicationdate": "2022-08-18", "externalids": {}, "doi_lower": null}
{"paper_id": 258947115, "title": "Playing repeated games with large language models", "author_names": ["Elif Akata", "Lion Schulz", "Julian Coda-Forno", "Seong Joon Oh", "M. Bethge", "Eric Schulz"], "venue": "Nature Human Behaviour", "abstract": "Large language models (LLMs) are increasingly used in applications where they interact with humans and other agents. We propose to use behavioural game theory to study LLMs’ cooperation and coordination behaviour. Here we let different LLMs play finitely repeated 2 × 2 games with each other, with human-like strategies, and actual human players. Our results show that LLMs perform particularly well at self-interested games such as the iterated Prisoner’s Dilemma family. However, they behave suboptimally in games that require coordination, such as the Battle of the Sexes. We verify that these behavioural signatures are stable across robustness checks. We also show how GPT-4’s behaviour can be modulated by providing additional information about its opponent and by using a ‘social chain-of-thought’ strategy. This also leads to better scores and more successful coordination when interacting with human players. These results enrich our understanding of LLMs’ social behaviour and pave the way for a behavioural game theory for machines. Large language models perform well in self-interested games such as the iterated Prisoner’s Dilemma but struggle in games that require coordination. Social reasoning strategies can improve cooperative outcomes with both other models and human players.", "year": 2023, "publicationdate": "2023-05-26", "externalids": {"DOI": "10.1038/s41562-025-02172-y"}, "doi_lower": "10.1038/s41562-025-02172-y"}
{"paper_id": 260333873, "title": "Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support", "author_names": ["Zilin Ma", "Yiyang Mei", "Zhaoyuan Su"], "venue": "AMIA ... Annual Symposium proceedings. AMIA Symposium", "abstract": "Conversational agents powered by large language models (LLM) have increasingly been utilized in the realm of mental well-being support. However, the implications and outcomes associated with their usage in such a critical field remain somewhat ambiguous and unexplored. We conducted a qualitative analysis of 120 posts, encompassing 2917 user comments, drawn from the most popular subreddit focused on mental health support applications powered by large language models (u/Replika). This exploration aimed to shed light on the advantages and potential pitfalls associated with the integration of these sophisticated models in conversational agents intended for mental health support. We found the app (Replika) beneficial in offering on-demand, non-judgmental support, boosting user confidence, and aiding self-discovery. Yet, it faced challenges in filtering harmful content, sustaining consistent communication, remembering new information, and mitigating users' overdependence. The stigma attached further risked isolating users socially. We strongly assert that future researchers and designers must thoroughly evaluate the appropriateness of employing LLMs for mental well-being support, ensuring their responsible and effective application.", "year": 2023, "publicationdate": "2023-07-28", "externalids": {"DOI": "10.48550/arXiv.2307.15810"}, "doi_lower": "10.48550/arxiv.2307.15810"}
{"paper_id": 255152420, "title": "Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?", "author_names": ["Apostolos Filippas", "J. Horton", "Benjamin S. Manning"], "venue": "ACM Conference on Economics and Computation", "abstract": "Large language models---because of how they are trained and designed---are implicit computational models of humans---a homo silicus. Social scientists can use LLMs like economists use homo economicus: LLMs can be given endowments, information, preferences, and so on, and then their behavior can be explored in scenarios via simulation. We replicate four experiments using this approach and find qualitatively similar results to the original. Benefits of this approach include trying new variations for fresh insights, piloting studies via simulation, and searching for novel social science insights to test in the real world. The full version of the paper can be accessed at https://apostolos-filippas.com/papers/hs.pdf.", "year": 2023, "publicationdate": "2023-01-18", "externalids": {"DOI": "10.1145/3670865.3673513"}, "doi_lower": "10.1145/3670865.3673513"}
{"paper_id": 259991646, "title": "Are you in a Masquerade? Exploring the Behavior and Impact of Large Language Model Driven Social Bots in Online Social Networks", "author_names": ["Siyu Li", "Jin Yang", "Kui Zhao"], "venue": "arXiv.org", "abstract": "As the capabilities of Large Language Models (LLMs) emerge, they not only assist in accomplishing traditional tasks within more efficient paradigms but also stimulate the evolution of social bots. Researchers have begun exploring the implementation of LLMs as the driving core of social bots, enabling more efficient and user-friendly completion of tasks like profile completion, social behavior decision-making, and social content generation. However, there is currently a lack of systematic research on the behavioral characteristics of LLMs-driven social bots and their impact on social networks. We have curated data from Chirper, a Twitter-like social network populated by LLMs-driven social bots and embarked on an exploratory study. Our findings indicate that: (1) LLMs-driven social bots possess enhanced individual-level camouflage while exhibiting certain collective characteristics; (2) these bots have the ability to exert influence on online communities through toxic behaviors; (3) existing detection methods are applicable to the activity environment of LLMs-driven social bots but may be subject to certain limitations in effectiveness. Moreover, we have organized the data collected in our study into the Masquerade-23 dataset, which we have publicly released, thus addressing the data void in the subfield of LLMs-driven social bots behavior datasets. Our research outcomes provide primary insights for the research and governance of LLMs-driven social bots within the research community.", "year": 2023, "publicationdate": "2023-07-19", "externalids": {"DOI": "10.48550/arXiv.2307.10337"}, "doi_lower": "10.48550/arxiv.2307.10337"}
{"paper_id": 260681901, "title": "Quantifying the Impact of Large Language Models on Collective Opinion Dynamics", "author_names": ["Chao Li", "Xingye Su", "Haoying Han", "Cong Xue", "Chunmo Zheng", "C. Fan"], "venue": "arXiv.org", "abstract": "The process of opinion expression and exchange is a critical component of democratic societies. As people interact with large language models (LLMs) in the opinion shaping process different from traditional media, the impacts of LLMs are increasingly recognized and being concerned. However, the knowledge about how LLMs affect the process of opinion expression and exchange of social opinion networks is very limited. Here, we create an opinion network dynamics model to encode the opinions of LLMs, cognitive acceptability and usage strategies of individuals, and simulate the impact of LLMs on opinion dynamics in a variety of scenarios. The outcomes of the simulations inform about effective demand-oriented opinion network interventions. The results from this study suggested that the output opinion of LLMs has a unique and positive effect on the collective opinion difference. The marginal effect of cognitive acceptability on collective opinion formation is nonlinear and shows a decreasing trend. When people partially rely on LLMs, the exchange process of opinion becomes more intense and the diversity of opinion becomes more favorable. In fact, there is 38.6% more opinion diversity when people all partially rely on LLMs, compared to prohibiting the use of LLMs entirely. The optimal diversity of opinion was found when the fractions of people who do not use, partially rely on, and fully rely on LLMs reached roughly 4:12:1. Our experiments also find that introducing extra agents with opposite/neutral/random opinions, we can effectively mitigate the impact of biased/toxic output from LLMs. Our findings provide valuable insights into opinion dynamics in the age of LLMs, highlighting the need for customized interventions tailored to specific scenarios to address the drawbacks of improper output and use of LLMs.", "year": 2023, "publicationdate": "2023-08-07", "externalids": {"DOI": "10.48550/arXiv.2308.03313"}, "doi_lower": "10.48550/arxiv.2308.03313"}
{"paper_id": 259937409, "title": "The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents", "author_names": ["Grgur Kovač", "Rémy Portelas", "Peter Ford Dominey", "Pierre-Yves Oudeyer"], "venue": "arXiv.org", "abstract": "Developmental psychologists have long-established the importance of socio-cognitive abilities in human intelligence. These abilities enable us to enter, participate and benefit from human culture. AI research on social interactive agents mostly concerns the emergence of culture in a multi-agent setting (often without a strong grounding in developmental psychology). We argue that AI research should be informed by psychology and study socio-cognitive abilities enabling to enter a culture too. We discuss the theories of Michael Tomasello and Jerome Bruner to introduce some of their concepts to AI and outline key concepts and socio-cognitive abilities. We present The SocialAI school - a tool including a customizable parameterized uite of procedurally generated environments, which simplifies conducting experiments regarding those concepts. We show examples of such experiments with RL agents and Large Language Models. The main motivation of this work is to engage the AI community around the problem of social intelligence informed by developmental psychology, and to provide a tool to simplify first steps in this direction. Refer to the project website for code and additional information: https://sites.google.com/view/socialai-school.", "year": 2023, "publicationdate": "2023-07-15", "externalids": {"DOI": "10.48550/arXiv.2307.07871"}, "doi_lower": "10.48550/arxiv.2307.07871"}
{"paper_id": 259766713, "title": "Epidemic Modeling with Generative Agents", "author_names": ["Ross Williams", "Niyousha Hosseinichimeh", "A. Majumdar", "Navid Ghaffarzadegan"], "venue": "arXiv.org", "abstract": "This study offers a new paradigm of individual-level modeling to address the grand challenge of incorporating human behavior in epidemic models. Using generative artificial intelligence in an agent-based epidemic model, each agent is empowered to make its own reasonings and decisions via connecting to a large language model such as ChatGPT. Through various simulation experiments, we present compelling evidence that generative agents mimic real-world behaviors such as quarantining when sick and self-isolation when cases rise. Collectively, the agents demonstrate patterns akin to multiple waves observed in recent pandemics followed by an endemic period. Moreover, the agents successfully flatten the epidemic curve. This study creates potential to improve dynamic system modeling by offering a way to represent human brain, reasoning, and decision making.", "year": 2023, "publicationdate": "2023-07-11", "externalids": {"DOI": "10.48550/arXiv.2307.04986"}, "doi_lower": "10.48550/arxiv.2307.04986"}
{"paper_id": 261101102, "title": "CGMI: Configurable General Multi-Agent Interaction Framework", "author_names": ["Jinxin Shi", "Jiabao Zhao", "Yilei Wang", "Xingjiao Wu", "Jiawen Li", "Liangbo He"], "venue": "arXiv.org", "abstract": "Benefiting from the powerful capabilities of large language models (LLMs), agents based on LLMs have shown the potential to address domain-specific tasks and emulate human behaviors. However, the content generated by these agents remains somewhat superficial, owing to their limited domain expertise and the absence of an effective cognitive architecture. To address this, we present the Configurable General Multi-Agent Interaction (CGMI) framework, designed to replicate human interactions in real-world scenarios. Specifically, we propose a tree-structured methodology for the assignment, detection, and maintenance of agent personality. Additionally, we designed a cognitive architecture equipped with a skill library based on the ACT* model, which contains memory, reflection, and planning modules. We have also integrated general agents to augment the virtual environment's realism. Using the CGMI framework, we simulated numerous classroom interactions between teacher and students. The experiments indicate that aspects such as the teaching methodology, curriculum, and student performance closely mirror real classroom settings. We will open source our work.", "year": 2023, "publicationdate": "2023-08-24", "externalids": {"DOI": "10.48550/arXiv.2308.12503"}, "doi_lower": "10.48550/arxiv.2308.12503"}
{"paper_id": 275547939, "title": "Research on Large Language Models for Elderly Travel Education Integrated with External Knowledge Bases", "author_names": ["Wei Wang", "Yefeng Chen", "Xinqi Pan"], "venue": "2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE)", "abstract": "A Large Language Model has demonstrated exceptional performance across a variety of tasks. However, when deployed in specific domains like smart elderly care, the models still face challenges due to a deficiency in domain-specific knowledge and encounter performance bottlenecks in providing services for older people. In this paper, we introduce an LLM named ChatDriver, which is tailored for elderly travel education. Considering the importance of data quality, we have carefully designed a fine-tuning dataset for the older population care domain. Additionally, we propose a method that combines vector database retrieval with keyword retrieval to effectively address the uncertainty issues caused by vector similarity matching in vector databases. Furthermore, by employing a novel attention mechanism and improving memory management and computational resource utilization, we have significantly enhanced the model's performance.", "year": 2024, "publicationdate": "2024-10-11", "externalids": {"DOI": "10.1109/CBASE64041.2024.10824480"}, "doi_lower": "10.1109/cbase64041.2024.10824480"}
{"paper_id": 269646697, "title": "Can Generative AI improve social science?", "author_names": ["Christopher A Bail"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "abstract": "Generative AI that can produce realistic text, images, and other human-like outputs is currently transforming many different industries. Yet it is not yet known how such tools might influence social science research. I argue Generative AI has the potential to improve survey research, online experiments, automated content analyses, agent-based models, and other techniques commonly used to study human behavior. In the second section of this article, I discuss the many limitations of Generative AI. I examine how bias in the data used to train these tools can negatively impact social science research—as well as a range of other challenges related to ethics, replication, environmental impact, and the proliferation of low-quality research. I conclude by arguing that social scientists can address many of these limitations by creating open-source infrastructure for research on human behavior. Such infrastructure is not only necessary to ensure broad access to high-quality research tools, I argue, but also because the progress of AI will require deeper understanding of the social forces that guide human behavior.", "year": 2024, "publicationdate": "2024-05-09", "externalids": {"DOI": "10.1073/pnas.2314021121"}, "doi_lower": "10.1073/pnas.2314021121"}
{"paper_id": 258059651, "title": "Emergent autonomous scientific research capabilities of large language models", "author_names": ["Daniil A. Boiko", "R. MacKnight", "Gabe Gomes"], "venue": "arXiv.org", "abstract": "Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.", "year": 2023, "publicationdate": "2023-04-11", "externalids": {"DOI": "10.48550/arXiv.2304.05332"}, "doi_lower": "10.48550/arxiv.2304.05332"}
{"paper_id": 260438479, "title": "ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks", "author_names": ["Y. Kang", "Jihan Kim"], "venue": "arXiv.org", "abstract": "ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate metal-organic frameworks (MOFs). By leveraging a large-scale language model (GPT-4 and GPT-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generations. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.", "year": 2023, "publicationdate": "2023-08-01", "externalids": {"DOI": "10.48550/arXiv.2308.01423"}, "doi_lower": "10.48550/arxiv.2308.01423"}
{"paper_id": 259360593, "title": "Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics", "author_names": ["M. Swan", "Takashi Kido", "Eric Roland", "R. P. D. Santos"], "venue": "arXiv.org", "abstract": "The advancement in generative AI could be boosted with more accessible mathematics. Beyond human-AI chat, large language models (LLMs) are emerging in programming, algorithm discovery, and theorem proving, yet their genomics application is limited. This project introduces Math Agents and mathematical embedding as fresh entries to the\"Moore's Law of Mathematics\", using a GPT-based workflow to convert equations from literature into LaTeX and Python formats. While many digital equation representations exist, there's a lack of automated large-scale evaluation tools. LLMs are pivotal as linguistic user interfaces, providing natural language access for human-AI chat and formal languages for large-scale AI-assisted computational infrastructure. Given the infinite formal possibility spaces, Math Agents, which interact with math, could potentially shift us from\"big data\"to\"big math\". Math, unlike the more flexible natural language, has properties subject to proof, enabling its use beyond traditional applications like high-validation math-certified icons for AI alignment aims. This project aims to use Math Agents and mathematical embeddings to address the ageing issue in information systems biology by applying multiscalar physics mathematics to disease models and genomic data. Generative AI with episodic memory could help analyse causal relations in longitudinal health records, using SIR Precision Health models. Genomic data is suggested for addressing the unsolved Alzheimer's disease problem.", "year": 2023, "publicationdate": "2023-07-04", "externalids": {"DOI": "10.48550/arXiv.2307.02502"}, "doi_lower": "10.48550/arxiv.2307.02502"}
{"paper_id": 247997008, "title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level", "author_names": ["Iddo Drori", "Sarah J. Zhang", "Reece Shuttleworth", "Leonard Tang", "Albert Lu", "Elizabeth Ke", "Kevin Liu", "Linda Chen", "Sunny Tran", "Newman Cheng", "Roman Wang", "Nikhil Singh", "T. Patti", "J. Lynch", "A. Shporer", "Nakul Verma", "Eugene Wu", "G. Strang"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "abstract": "Significance We demonstrate that a neural network automatically solves, explains, and generates university-level problems from the largest Massachusetts Institute of Technology (MIT) mathematics courses at a human level. Our methods combine three innovations: 1) using recent neural networks pretrained on text and fine-tuned on code rather than pretrained on text; 2) few-shot learning synthesizing programs that correctly solve course problems automatically; and 3) a pipeline to solve questions, explain solutions, and generate new questions indistinguishable by students from course questions. Our work solves university-level mathematics courses and improves upon state-of-the-art, increasing automatic accuracy on randomly sampled questions on a benchmark by order of magnitude. Implications for higher education include roles of artificial intelligence (AI) in automated course evaluation and content generation.", "year": 2021, "publicationdate": "2021-12-31", "externalids": {"DOI": "10.1073/pnas.2123433119"}, "doi_lower": "10.1073/pnas.2123433119"}
{"paper_id": 235755472, "title": "Evaluating Large Language Models Trained on Code", "author_names": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Pondé", "Jared Kaplan", "Harrison Edwards", "Yura Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "Scott Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mo Bavarian", "Clemens Winter", "P. Tillet", "F. Such", "D. Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William H. Guss", "Alex Nichol", "Igor Babuschkin", "S. Balaji", "Shantanu Jain", "A. Carr", "Jan Leike", "Josh Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "M. Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "I. Sutskever", "Wojciech Zaremba"], "venue": "arXiv.org", "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.", "year": 2021, "publicationdate": "2021-07-07", "externalids": {}, "doi_lower": null}
{"paper_id": 260886891, "title": "CodeHelp: Using Large Language Models with Guardrails for Scalable Support in Programming Classes", "author_names": ["Mark H. Liffiton", "Brad E. Sheese", "Jaromir Savelka", "Paul Denny"], "venue": "European Conference on Modelling and Simulation", "abstract": "Computing educators face significant challenges in providing timely support to students, especially in large class settings. Large language models (LLMs) have emerged recently and show great promise for providing on-demand help at a large scale, but there are concerns that students may over-rely on the outputs produced by these models. In this paper, we introduce CodeHelp, a novel LLM-powered tool designed with guardrails to provide on-demand assistance to programming students without directly revealing solutions. We detail the design of the tool, which incorporates a number of useful features for instructors, and elaborate on the pipeline of prompting strategies we use to ensure generated outputs are suitable for students. To evaluate CodeHelp, we deployed it in a first-year computer and data science course with 52 students and collected student interactions over a 12-week period. We examine students’ usage patterns and perceptions of the tool, and we report reflections from the course instructor and a series of recommendations for classroom use. Our findings suggest that CodeHelp is well-received by students who especially value its availability and help with resolving errors, and that for instructors it is easy to deploy and complements, rather than replaces, the support that they provide to students.", "year": 2023, "publicationdate": "2023-08-14", "externalids": {"DOI": "10.1145/3631802.3631830"}, "doi_lower": "10.1145/3631802.3631830"}
{"paper_id": 260611570, "title": "A large language model-assisted education tool to provide feedback on open-ended responses", "author_names": ["Jordan K Matelsky", "Felipe Parodi", "Tony Liu", "Richard D. Lange", "K. Kording"], "venue": "arXiv.org", "abstract": "Open-ended questions are a favored tool among instructors for assessing student understanding and encouraging critical exploration of course material. Providing feedback for such responses is a time-consuming task that can lead to overwhelmed instructors and decreased feedback quality. Many instructors resort to simpler question formats, like multiple-choice questions, which provide immediate feedback but at the expense of personalized and insightful comments. Here, we present a tool that uses large language models (LLMs), guided by instructor-defined criteria, to automate responses to open-ended questions. Our tool delivers rapid personalized feedback, enabling students to quickly test their knowledge and identify areas for improvement. We provide open-source reference implementations both as a web application and as a Jupyter Notebook widget that can be used with instructional coding or math notebooks. With instructor guidance, LLMs hold promise to enhance student learning outcomes and elevate instructional methodologies.", "year": 2023, "publicationdate": "2023-07-25", "externalids": {"DOI": "10.48550/arXiv.2308.02439"}, "doi_lower": "10.48550/arxiv.2308.02439"}
{"paper_id": 259166062, "title": "AI and the transformation of social science research", "author_names": ["I. Grossmann", "M. Feinberg", "D. C. Parker", "N. Christakis", "P. Tetlock", "William A. Cunningham"], "venue": "Science", "abstract": "Careful bias management and data fidelity are key Advances in artificial intelligence (AI), particularly large language models (LLMs), are substantially affecting social science research. These transformer-based machine-learning models pretrained on vast amounts of text data are increasingly capable of simulating human-like responses and behaviors (1, 2), offering opportunities to test theories and hypotheses about human behavior at great scale and speed. This presents urgent challenges: How can social science research practices be adapted, even reinvented, to harness the power of foundational AI? And how can this be done while ensuring transparent and replicable research?", "year": 2023, "publicationdate": "2023-06-16", "externalids": {"DOI": "10.1126/science.adi1778"}, "doi_lower": "10.1126/science.adi1778"}
{"paper_id": 273102361, "title": "ChatEDA: A Large Language Model Powered Autonomous Agent for EDA", "author_names": ["Zhuolun He", "Haoyuan Wu", "Xinyun Zhang", "Xufeng Yao", "Su Zheng", "Haisheng Zheng", "Bei Yu"], "venue": "Workshop on Machine Learning for CAD", "abstract": "The integration of a complex set of Electronic Design Automation (EDA) tools to enhance interoperability is a critical concern for circuit designers. Recent advancements in large language models (LLMs) have showcased their exceptional capabilities in natural language processing and comprehension, offering a novel approach to interfacing with EDA tools. This research paper introduces ChatEDA, an autonomous agent for EDA empowered by a large language model, AutoMage, complemented by EDA tools serving as executors. ChatEDA streamlines the design flow from the Register-Transfer Level (RTL) to the Graphic Data System Version II (GDSII) by effectively managing task planning, script generation, and task execution. Through comprehensive experimental evaluations, ChatEDA has demonstrated its proficiency in handling diverse requirements, and our fine-tuned AutoMage model has exhibited superior performance compared to GPT-4 and other similar LLMs.", "year": 2023, "publicationdate": "2023-09-10", "externalids": {"DOI": "10.1109/MLCAD58807.2023.10299852"}, "doi_lower": "10.1109/mlcad58807.2023.10299852"}
{"paper_id": 261395685, "title": "Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations", "author_names": ["Xu Huang", "Jianxun Lian", "Yuxuan Lei", "Jing Yao", "Defu Lian", "Xing Xie"], "venue": "ACM Trans. Inf. Syst.", "abstract": "Recommender models capture ever-changing user preferences by training with in-domain user behavior data. These models are typically lightweight, facilitating real-time and large-scale online services. However, these models often falter when tasked with providing more sophisticated functionalities, such as offering explanations or engaging in conversations. Recently, large language models (LLMs) have emerged as a significant advancement towards artificial general intelligence, demonstrating impressive capabilities in instruction comprehension, reasoning, and human interaction. Unfortunately, LLMs lack the understanding of domain-specific item catalogs and behavioral patterns, especially in areas that deviate from general world knowledge, such as online e-commerce. This limitation makes them unsuitable to function as recommender models directly. In this article, we bridge the gap between recommender models and LLMs, combining their respective strengths to create an interactive recommender system. We present an efficient framework, termed as InteRecAgent, which utilizes LLMs as the brain and recommender models as instrumental tools. We first outline a minimal set of essential tools required to transform LLMs into InteRecAgent. To overcome specific challenges associated with LLM-based agents for recommender systems, we enhance three core components, covering memory mechanism, task planning, and tool learning abilities. The InteRecAgent empowers traditional recommender systems, like ID-based matrix factorization models, to evolve into versatile and interactive systems with a natural language interface through the integration of LLMs. Experimental results derived from three public datasets demonstrate that the InteRecAgent delivers strong performance as a conversational recommender system, surpassing general LLMs such as GPT-4.", "year": 2023, "publicationdate": "2023-08-31", "externalids": {"DOI": "10.1145/3731446"}, "doi_lower": "10.1145/3731446"}
{"paper_id": 260887370, "title": "PentestGPT: An LLM-empowered Automatic Penetration Testing Tool", "author_names": ["Gelei Deng", "Yi Liu", "V'ictor Mayoral-Vilches", "Peng Liu", "Yuekang Li", "Yuan Xu", "Tianwei Zhang", "Yang Liu", "M. Pinzger", "S. Rass"], "venue": "arXiv.org", "abstract": "Penetration testing, a crucial industrial practice for ensuring system security, has traditionally resisted automation due to the extensive expertise required by human professionals. Large Language Models (LLMs) have shown significant advancements in various domains, and their emergent abilities suggest their potential to revolutionize industries. In this research, we evaluate the performance of LLMs on real-world penetration testing tasks using a robust benchmark created from test machines with platforms. Our findings reveal that while LLMs demonstrate proficiency in specific sub-tasks within the penetration testing process, such as using testing tools, interpreting outputs, and proposing subsequent actions, they also encounter difficulties maintaining an integrated understanding of the overall testing scenario. In response to these insights, we introduce PentestGPT, an LLM-empowered automatic penetration testing tool that leverages the abundant domain knowledge inherent in LLMs. PentestGPT is meticulously designed with three self-interacting modules, each addressing individual sub-tasks of penetration testing, to mitigate the challenges related to context loss. Our evaluation shows that PentestGPT not only outperforms LLMs with a task-completion increase of 228.6\\% compared to the \\gptthree model among the benchmark targets but also proves effective in tackling real-world penetration testing challenges. Having been open-sourced on GitHub, PentestGPT has garnered over 4,700 stars and fostered active community engagement, attesting to its value and impact in both the academic and industrial spheres.", "year": 2023, "publicationdate": "2023-08-13", "externalids": {"DOI": "10.48550/arXiv.2308.06782"}, "doi_lower": "10.48550/arxiv.2308.06782"}
{"paper_id": 11419044, "title": "VerifiedDSP: Verifying Digital Signal Processing Designs in Coq https://github.com/JeremyRubin/VerifiedDSP", "author_names": ["Jeremy Rubin"], "venue": "", "abstract": null, "year": 2015, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 11419044, "title": "VerifiedDSP: Verifying Digital Signal Processing Designs in Coq https://github.com/JeremyRubin/VerifiedDSP", "author_names": ["Jeremy Rubin"], "venue": "", "abstract": null, "year": 2015, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 258417925, "title": "Towards autonomous system: flexible modular production system enhanced with large language model agents", "author_names": ["Yuchen Xia", "Manthan Shenoy", "Nasser Jazdi", "M. Weyrich"], "venue": "IEEE International Conference on Emerging Technologies and Factory Automation", "abstract": "In this paper, we present a novel framework that combines large language models (LLMs), digital twins and industrial automation system to enable intelligent planning and control of production processes. We retrofit the automation system for a modular production facility and create executable control interfaces of fine-granular functionalities and coarse-granular skills. Low-level functionalities are executed by automation components, and high-level skills are performed by automation modules. Subsequently, a digital twin system is developed, registering these interfaces and containing additional descriptive information about the production system. Based on the retrofitted automation system and the created digital twins, LLM-agents are designed to interpret descriptive information in the digital twins and control the physical system through service interfaces. These LLM-agents serve as intelligent agents on different levels within an automation system, enabling autonomous planning and control of flexible production. Given a task instruction as input, the LLM-agents orchestrate a sequence of atomic functionalities and skills to accomplish the task. We demonstrate how our implemented prototype can handle un-predefined tasks, plan a production process, and execute the operations. This research highlights the potential of integrating LLMs into industrial automation systems in the context of smart factory for more agile, flexible, and adaptive production processes, while it also underscores the critical insights and limitations for future work. Demos at: https://github.com/YuchenXia/GPT4IndustrialAutomation", "year": 2023, "publicationdate": "2023-04-28", "externalids": {"DOI": "10.1109/ETFA54631.2023.10275362"}, "doi_lower": "10.1109/etfa54631.2023.10275362"}
{"paper_id": 258352210, "title": "Industrial Engineering with Large Language Models: A Case Study of ChatGPT's Performance on Oil & Gas Problems", "author_names": ["O. Ogundare", "S. Madasu", "N. Wiggins"], "venue": "International Conference on Control, Mechatronics and Automation", "abstract": "Large Language Models (LLMs) have shown great potential in solving complex problems in various fields, including oil and gas engineering and other industrial engineering disciplines like factory automation, PLC programming etc. However, automatic identification of strong and weak solutions to fundamental physics equations governing several industrial processes remain a challenging task. This paper identifies the limitation of current LLM approaches, particularly ChatGPT in selected practical problems native to oil and gas engineering but not exclusively. The performance of ChatGPT in solving complex problems in oil and gas engineering is discussed and the areas where LLMs are most effective are presented.", "year": 2023, "publicationdate": "2023-04-27", "externalids": {"DOI": "10.1109/ICCMA59762.2023.10374622"}, "doi_lower": "10.1109/iccma59762.2023.10374622"}
{"paper_id": 261064959, "title": "ProAgent: Building Proactive Cooperative Agents with Large Language Models", "author_names": ["Ceyao Zhang", "Kaijie Yang", "Siyi Hu", "Zihao Wang", "Guanghe Li", "Y. Sun", "Chen Zhang", "Zhaowei Zhang", "Anji Liu", "Song-Chun Zhu", "Xiaojun Chang", "Junge Zhang", "F. Yin", "Yitao Liang", "Yaodong Yang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Building agents with adaptive behavior in cooperative tasks stands as a paramount goal in the realm of multi-agent systems. Current approaches to developing cooperative agents rely primarily on learning-based methods, whose policy generalization depends heavily on the diversity of teammates they interact with during the training phase. Such reliance, however, constrains the agents' capacity for strategic adaptation when cooperating with unfamiliar teammates, which becomes a significant challenge in zero-shot coordination scenarios. To address this challenge, we propose ProAgent, a novel framework that harnesses large language models (LLMs) to create proactive agents capable of dynamically adapting their behavior to enhance cooperation with teammates. ProAgent can analyze the present state, and infer the intentions of teammates from observations. It then updates its beliefs in alignment with the teammates' subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of modularity and interpretability, making it easily integrated into various of coordination scenarios. Experimental evaluations conducted within the Overcooked-AI environment unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training when cooperating with AI agents. Furthermore, in partnered with human proxy models, its performance exhibits an average improvement exceeding 10% compared to the current state-of-the-art method. For more information about our project, please visit https://pku-proagent.github.io.", "year": 2023, "publicationdate": "2023-08-22", "externalids": {"DOI": "10.1609/aaai.v38i16.29710"}, "doi_lower": "10.1609/aaai.v38i16.29710"}
{"paper_id": 259108425, "title": "Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach", "author_names": ["Bin Hu", "Chenyang Zhao", "Pushi Zhang", "Zihao Zhou", "Yuanhang Yang", "Zenglin Xu", "Bin Liu"], "venue": "RLJ", "abstract": "Large language models (LLMs) encode a vast amount of world knowledge acquired from massive text datasets. Recent studies have demonstrated that LLMs can assist an embodied agent in solving complex sequential decision making tasks by providing high-level instructions. However, interactions with LLMs can be time-consuming. In many practical scenarios, it requires a significant amount of storage space that can only be deployed on remote cloud servers. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency. In this paper, we explore how to enable intelligent cost-effective interactions between a down stream task oriented agent and an LLM. We find that this problem can be naturally formulated by a Markov decision process (MDP), and propose When2Ask, a reinforcement learning based approach that learns when it is necessary to query LLMs for high-level instructions to accomplish a target task. On one side, When2Ask discourages unnecessary redundant interactions, while on the other side, it enables the agent to identify and follow useful instructions from the LLM. This enables the agent to halt an ongoing plan and transition to a more suitable one based on new environmental observations. Experiments on MiniGrid and Habitat environments that entail planning sub-goals demonstrate that When2Ask learns to solve target tasks with only a few necessary interactions with the LLM, significantly reducing interaction costs in testing environments compared with baseline methods. Our code is available at: https://github.com/ZJLAB-AMMI/LLM4RL.", "year": 2023, "publicationdate": "2023-06-06", "externalids": {"DOI": "10.48550/arXiv.2306.03604"}, "doi_lower": "10.48550/arxiv.2306.03604"}
{"paper_id": 258480064, "title": "Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents", "author_names": ["Yue Wu", "So Yeon Min", "Yonatan Bisk", "R. Salakhutdinov", "A. Azaria", "Yuan-Fang Li", "Tom M. Mitchell", "Shrimai Prabhumoye"], "venue": "arXiv.org", "abstract": "Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.", "year": 2023, "publicationdate": "2023-05-03", "externalids": {"DOI": "10.48550/arXiv.2305.02412"}, "doi_lower": "10.48550/arxiv.2305.02412"}
{"paper_id": 259145016, "title": "Large Language Models Are Semi-Parametric Reinforcement Learning Agents", "author_names": ["Danyang Zhang", "Lu Chen", "Situo Zhang", "Hongshen Xu", "Zihan Zhao", "Kai Yu"], "venue": "Neural Information Processing Systems", "abstract": "Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate on two task sets and demonstrate the superiority and robustness of REMEMBERER.", "year": 2023, "publicationdate": "2023-06-09", "externalids": {}, "doi_lower": null}
{"paper_id": 259311695, "title": "Towards A Unified Agent with Foundation Models", "author_names": ["Norman Di Palo", "Arunkumar Byravan", "Leonard Hasenclever", "Markus Wulfmeier", "N. Heess", "Martin A. Riedmiller"], "venue": "arXiv.org", "abstract": "Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms. We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve novel tasks or imitate videos of human experts.", "year": 2023, "publicationdate": "2023-07-18", "externalids": {"DOI": "10.48550/arXiv.2307.09668"}, "doi_lower": "10.48550/arxiv.2307.09668"}
{"paper_id": 258564887, "title": "TidyBot: Personalized Robot Assistance with Large Language Models", "author_names": ["Jimmy Wu", "Rika Antonova", "Adam Kan", "Marion Lepert", "Andy Zeng", "Shuran Song", "Jeannette Bohg", "S. Rusinkiewicz", "T. Funkhouser"], "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems", "abstract": "For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.", "year": 2023, "publicationdate": "2023-05-09", "externalids": {"DOI": "10.1007/s10514-023-10139-z"}, "doi_lower": "10.1007/s10514-023-10139-z"}
{"paper_id": 259342896, "title": "Embodied Task Planning with Large Language Models", "author_names": ["Zhenyu Wu", "Ziwei Wang", "Xiuwei Xu", "Jiwen Lu", "Haibin Yan"], "venue": "arXiv.org", "abstract": "Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions. The generated data is leveraged for grounded plan tuning of pre-trained LLMs. During inference, we discover the objects in the scene by extending open-vocabulary object detectors to multi-view RGB images collected in different achievable locations. Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.", "year": 2023, "publicationdate": "2023-07-04", "externalids": {"DOI": "10.48550/arXiv.2307.01848"}, "doi_lower": "10.48550/arxiv.2307.01848"}
{"paper_id": 253180684, "title": "Collaborating with language models for embodied reasoning", "author_names": ["Ishita Dasgupta", "Christine Kaeser-Chen", "Kenneth Marino", "Arun Ahuja", "Sheila Babayan", "Felix Hill", "R. Fergus"], "venue": "arXiv.org", "abstract": "Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.", "year": 2023, "publicationdate": "2023-02-01", "externalids": {"DOI": "10.48550/arXiv.2302.00763"}, "doi_lower": "10.48550/arxiv.2302.00763"}
{"paper_id": 256389514, "title": "Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling", "author_names": ["Kolby Nottingham", "Prithviraj Ammanabrolu", "Alane Suhr", "Yejin Choi", "Hannaneh Hajishirzi", "Sameer Singh", "Roy Fox"], "venue": "International Conference on Machine Learning", "abstract": "Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.", "year": 2023, "publicationdate": "2023-01-28", "externalids": {"DOI": "10.48550/arXiv.2301.12050"}, "doi_lower": "10.48550/arxiv.2301.12050"}
{"paper_id": 260333931, "title": "Dialogue Shaping: Empowering Agents through NPC Interaction", "author_names": ["Wei Zhou", "Xiangyu Peng", "Mark O. Riedl"], "venue": "EXAG@AIIDE", "abstract": "One major challenge in reinforcement learning (RL) is the large amount of steps for the RL agent needs to converge in the training process and learn the optimal policy, especially in text-based game environments where the action space is extensive. However, non-player characters (NPCs) sometimes hold some key information about the game, which can potentially help to train RL agents faster. Thus, this paper explores how to interact and converse with NPC agents to get the key information using large language models (LLMs), as well as incorporate this information to speed up RL agent's training using knowledge graphs (KGs) and Story Shaping.", "year": 2023, "publicationdate": "2023-07-28", "externalids": {"DOI": "10.48550/arXiv.2307.15833"}, "doi_lower": "10.48550/arxiv.2307.15833"}
{"paper_id": 260351308, "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models", "author_names": ["Haonan Li", "Yu Hao", "Yizhuo Zhai", "Zhiyun Qian"], "venue": "arXiv.org", "abstract": "Static analysis is a widely used technique in software engineering for identifying and mitigating bugs. However, a significant hurdle lies in achieving a delicate balance between precision and scalability. Large Language Models (LLMs) offer a promising alternative, as recent advances demonstrate remarkable capabilities in comprehending, generating, and even debugging code. Yet, the logic of bugs can be complex and require sophisticated reasoning and a large analysis scope spanning multiple functions. Therefore, at this point, LLMs are better used in an assistive role to complement static analysis. In this paper, we take a deep dive into the open space of LLM-assisted static analysis, using use-before-initialization (UBI) bugs as a case study. To this end, we develop LLift, a fully automated framework that interfaces with both a static analysis tool and an LLM. By carefully designing the framework and the prompts, we are able to overcome a number of challenges, including bug-specific modeling, the large problem scope, the non-deterministic nature of LLMs, etc. Tested in a real-world scenario analyzing nearly a thousand potential UBI bugs produced by static analysis, LLift demonstrates a potent capability, showcasing a reasonable precision (50%) and appearing to have no missing bugs. It even identified 13 previously unknown UBI bugs in the Linux kernel. This research paves the way for new opportunities and methodologies in using LLMs for bug discovery in extensive, real-world datasets.", "year": 2023, "publicationdate": "2023-08-01", "externalids": {"DOI": "10.48550/arXiv.2308.00245"}, "doi_lower": "10.48550/arxiv.2308.00245"}
{"paper_id": 11419044, "title": "VerifiedDSP: Verifying Digital Signal Processing Designs in Coq https://github.com/JeremyRubin/VerifiedDSP", "author_names": ["Jeremy Rubin"], "venue": "", "abstract": null, "year": 2015, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 11419044, "title": "VerifiedDSP: Verifying Digital Signal Processing Designs in Coq https://github.com/JeremyRubin/VerifiedDSP", "author_names": ["Jeremy Rubin"], "venue": "", "abstract": null, "year": 2015, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 271753702, "title": "https://jafrchir.org/docs/1643408796.pdf", "author_names": ["AB Diouf , et al..."], "venue": "Journal Africain de Chirurgie", "abstract": "Objective : To make our contribution in the coeliosurgical management of the cornual pregnancies by proposing a minimal invasive technique of resection when the indication arises. Patients and methods : It is a simpler, less mutilating than conventional resection technique, which is to release the cornual pregnancy its peripheral attachments before coagulation-section of the base. Results: We performed this technique in four patients. They all had a right cornual ectopic pregnancy. A rupture was observed in two cases with significant haemoperitoneum. The mean age of the patients was 38.5 years (37-42 years). With regard to the history, salpingectomy was already performed on one patient for heterotopic pregnancy, another patient had a uterine myomatosis. The surgical follow-up was simple in all cases on a 2-day hospital stay. A twin pregnancy after IVF with vaginal delivery was observed in the 38-year-old female patient. Conclusion: Resection by coagulation section once cornual pregnancy has occurred is an effective and safe technique to include in the surgical treatment of interstitial pregnancy. Keys words : Ectopic pregnancy ; Cornual pregnancy ; Interstitial pregnancy ; Laparoscopy.", "year": 2017, "publicationdate": "2017-12-05", "externalids": {"DOI": "10.61585/pud-jafrchir-v4n302"}, "doi_lower": "10.61585/pud-jafrchir-v4n302"}
{"paper_id": 11419044, "title": "VerifiedDSP: Verifying Digital Signal Processing Designs in Coq https://github.com/JeremyRubin/VerifiedDSP", "author_names": ["Jeremy Rubin"], "venue": "", "abstract": null, "year": 2015, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 11419044, "title": "VerifiedDSP: Verifying Digital Signal Processing Designs in Coq https://github.com/JeremyRubin/VerifiedDSP", "author_names": ["Jeremy Rubin"], "venue": "", "abstract": null, "year": 2015, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 280011990, "title": "HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval", "author_names": ["Kevin Duh", "Eugene Yang", "Orion Weller", "Andrew Yates", "Dawn J. Lawrie"], "venue": "arXiv.org", "abstract": "The HLTCOE LiveRAG submission utilized the GPT-researcher framework for researching the context of the question, filtering the returned results, and generating the final answer. The retrieval system was a ColBERT bi-encoder architecture, which represents a passage with many dense tokens. Retrieval used a local, compressed index of the FineWeb10-BT collection created with PLAID-X, using a model fine-tuned for multilingual retrieval. Query generation from context was done with Qwen2.5-7B-Instruct, while filtering was accomplished with m2-bert-80M-8k-retrieval. Up to nine passages were used as context to generate an answer using Falcon3-10B. This system placed 5th in the LiveRAG automatic evaluation for correctness with a score of 1.07.", "year": 2025, "publicationdate": "2025-06-27", "externalids": {"DOI": "10.48550/arXiv.2506.22356"}, "doi_lower": "10.48550/arxiv.2506.22356"}
{"paper_id": 258179336, "title": "Tool Learning with Foundation Models", "author_names": ["Yujia Qin", "Shengding Hu", "Yankai Lin", "Weize Chen", "Ning Ding", "Ganqu Cui", "Zheni Zeng", "Yufei Huang", "Chaojun Xiao", "Chi Han", "Y. Fung", "Yusheng Su", "Huadong Wang", "Cheng Qian", "Runchu Tian", "Kunlun Zhu", "Shi Liang", "Xingyu Shen", "Bokai Xu", "Zhen Zhang", "Yining Ye", "Bowen Li", "Ziwei Tang", "Jing Yi", "Yu Zhu", "Zhenning Dai", "Lan Yan", "Xin Cong", "Ya-Ting Lu", "Weilin Zhao", "Yuxiang Huang", "Junxi Yan", "Xu Han", "Xian Sun", "Dahai Li", "Jason Phang", "Cheng Yang", "Tongshuang Wu", "Heng Ji", "Zhiyuan Liu", "Maosong Sun"], "venue": "ACM Computing Surveys", "abstract": "Humans possess an extraordinary ability to create and utilize tools. With the advent of foundation models, artificial intelligence systems have the potential to be equally adept in tool use as humans. This paradigm, which is dubbed as tool learning with foundation models, combines the strengths of tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. This article presents a systematic investigation and comprehensive review of tool learning. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research and formulate a general framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate generalization in tool learning. Finally, we discuss several open problems that require further investigation, such as ensuring trustworthy tool use, enabling tool creation with foundation models, and addressing personalization challenges. Overall, we hope this article could inspire future research in integrating tools with foundation models.", "year": 2023, "publicationdate": "2023-04-17", "externalids": {"DOI": "10.1145/3704435"}, "doi_lower": "10.1145/3704435"}
{"paper_id": 260925901, "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework", "author_names": ["Qingyun Wu", "Gagan Bansal", "Jieyu Zhang", "Yiran Wu", "Shaokun Zhang", "Erkang Zhu", "Beibin Li", "Li Jiang", "Xiaoyun Zhang", "Chi Wang"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 261048935, "title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents", "author_names": ["Weize Chen", "Yusheng Su", "Jingwei Zuo", "Cheng Yang", "Chenfei Yuan", "Cheng Qian", "Chi-Min Chan", "Yujia Qin", "Ya-Ting Lu", "Ruobing Xie", "Zhiyuan Liu", "Maosong Sun", "Jie Zhou"], "venue": "International Conference on Learning Representations", "abstract": "Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework \\framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \\framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \\framework will soon be released at \\url{https://github.com/OpenBMB/AgentVerse}.", "year": 2023, "publicationdate": "2023-08-21", "externalids": {"DOI": "10.48550/arXiv.2308.10848"}, "doi_lower": "10.48550/arxiv.2308.10848"}
{"paper_id": 271745584, "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate", "author_names": ["Chi-Min Chan", "Weize Chen", "Yusheng Su", "Jianxuan Yu", "Wei Xue", "Shanghang Zhang", "Jie Fu", "Zhiyuan Liu"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 256627809, "title": "ChatGPT and Software Testing Education: Promises & Perils", "author_names": ["Sajed Jalil", "Suzzana Rafi", "Thomas D. Latoza", "Kevin Moran", "Wing Lam"], "venue": "International Conference on Software Testing, Verification and Validation Workshops", "abstract": "Over the past decade, predictive language modeling for code has proven to be a valuable tool for enabling new forms of automation for developers. More recently, we have seen the ad-vent of general purpose \"large language models\", based on neural transformer architectures, that have been trained on massive datasets of human written text, which includes code and natural language. However, despite the demonstrated representational power of such models, interacting with them has historically been constrained to specific task settings, limiting their general applicability. Many of these limitations were recently overcome with the introduction of ChatGPT, a language model created by OpenAI and trained to operate as a conversational agent, enabling it to answer questions and respond to a wide variety of commands from end users.The introduction of models, such as ChatGPT, has already spurred fervent discussion from educators, ranging from fear that students could use these AI tools to circumvent learning, to excitement about the new types of learning opportunities that they might unlock. However, given the nascent nature of these tools, we currently lack fundamental knowledge related to how well they perform in different educational settings, and the potential promise (or danger) that they might pose to traditional forms of instruction. As such, in this paper, we examine how well ChatGPT performs when tasked with answering common questions in a popular software testing curriculum. We found that given its current capabilities, ChatGPT is able to respond to 77.5% of the questions we examined and that, of these questions, it is able to provide correct or partially correct answers in 55.6% of cases, provide correct or partially correct explanations of answers in 53.0% of cases, and that prompting the tool in a shared question context leads to a marginally higher rate of correct answers and explanations. Based on these findings, we discuss the potential promises and perils related to the use of ChatGPT by students and instructors.", "year": 2023, "publicationdate": "2023-02-07", "externalids": {"DOI": "10.1109/ICSTW58534.2023.00078"}, "doi_lower": "10.1109/icstw58534.2023.00078"}
{"paper_id": 258291842, "title": "Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback", "author_names": ["Nikhil Mehta", "Milagro Teruel", "Patricio Figueroa Sanz", "Xinwei Deng", "A. Awadallah", "Julia Kiseleva"], "venue": "Findings", "abstract": "Many approaches to Natural Language Processing tasks often treat them as single-step problems, where an agent receives an instruction, executes it, and is evaluated based on the final outcome. However, language is inherently interactive, as evidenced by the back-and-forth nature of human conversations. In light of this, we posit that human-AI collaboration should also be interactive, with humans monitoring the work of AI agents and providing feedback that the agent can understand and utilize. Further, the AI agent should be able to detect when it needs additional information and proactively ask for help. Enabling this scenario would lead to more natural, efficient, and engaging human-AI collaboration.In this paper, we investigate these directions using the challenging task established by the IGLU competition, an interactive grounded language understanding task in a MineCraft-like world. We delve into multiple types of help players can give to the AI to guide it and analyze the impact of this help on behavior, resulting in performance improvements and an end-to-end interactive system.", "year": 2023, "publicationdate": "2023-04-21", "externalids": {"DOI": "10.48550/arXiv.2304.10750"}, "doi_lower": "10.48550/arxiv.2304.10750"}
{"paper_id": 258841038, "title": "Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs", "author_names": ["Angelica Chen", "Jason Phang", "Alicia Parrish", "Vishakh Padmakumar", "Chen Zhao", "Sam Bowman", "Kyunghyun Cho"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -- hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14279"}, "doi_lower": "10.48550/arxiv.2305.14279"}
{"paper_id": 259164957, "title": "Mobile-Env: An Evaluation Platform and Benchmark for Interactive Agents in LLM Era", "author_names": ["Danyang Zhang", "Lu Chen", "Zihan Zhao", "Ruisheng Cao", "Kai Yu"], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 258841392, "title": "clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents", "author_names": ["Chalamalasetti Kranti", "Jana Gotze", "Sherzod Hakimov", "Brielen Madureira", "Philipp Sadler", "David Schlangen"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent work has proposed a methodology for the systematic evaluation of\"Situated Language Understanding Agents\"-agents that operate in rich linguistic and non-linguistic contexts-through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities? As a proof of concept, this paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable to follow game-play instructions. Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models performing better. The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value. Our general framework for implementing and evaluating games with LLMs is available at https://github.com/clembench .", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.48550/arXiv.2305.13455"}, "doi_lower": "10.48550/arxiv.2305.13455"}
{"paper_id": 258987554, "title": "Decision-Oriented Dialogue for Human-AI Collaboration", "author_names": ["Jessy Lin", "Nicholas Tomlin", "Jacob Andreas", "J. Eisner"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Abstract We describe a class of tasks called decision-oriented dialogues, in which AI assistants such as large language models (LMs) must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: Assistants can access and process large amounts of information, while users have preferences and constraints external to the system. For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach. We evaluate LMs in self-play and in collaboration with humans and find that they fall short compared to human assistants, achieving much lower rewards despite engaging in longer dialogues. We highlight a number of challenges models face in decision-oriented dialogues, ranging from goal-directed behavior to reasoning and optimization, and release our environments as a testbed for future work.", "year": 2023, "publicationdate": "2023-05-31", "externalids": {"DOI": "10.1162/tacl_a_00679"}, "doi_lower": "10.1162/tacl_a_00679"}
{"paper_id": 259108951, "title": "Towards Autonomous Testing Agents via Conversational Large Language Models", "author_names": ["R. Feldt", "Sungmin Kang", "Juyeon Yoon", "Shin Yoo"], "venue": "International Conference on Automated Software Engineering", "abstract": "Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized “hallucination” of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.1109/ASE56229.2023.00148"}, "doi_lower": "10.1109/ase56229.2023.00148"}
{"paper_id": 260125274, "title": "Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models", "author_names": ["Yuanzhi Liang", "Linchao Zhu", "Yezhou Yang"], "venue": "arXiv.org", "abstract": "Recent advancements in natural language and Large Language Models (LLMs) have enabled AI agents to simulate human-like interactions within virtual worlds. However, these interactions still face limitations in complexity and flexibility, particularly in scenarios involving multiple characters and novel objects. Pre-defining all interactable objects in the agent's world model presents challenges, and conveying implicit intentions to multiple characters through complex interactions remains difficult. To address these issues, we propose integrating virtual Game Masters (GMs) into the agent's world model, drawing inspiration from Tabletop Role-Playing Games (TRPGs). GMs play a crucial role in overseeing information, estimating players' intentions, providing environment descriptions, and offering feedback, compensating for current world model deficiencies. To facilitate future explorations for complex interactions, we introduce a benchmark named Tachikuma, comprising a Multiple character and novel Object based interaction Estimation (MOE) task and a supporting dataset. MOE challenges models to understand characters' intentions and accurately determine their actions within intricate contexts involving multi-character and novel object interactions. Besides, the dataset captures log data from real-time communications during gameplay, providing diverse, grounded, and complex interactions for further explorations. Finally, we present a simple prompting baseline and evaluate its performance, demonstrating its effectiveness in enhancing interaction understanding. We hope that our dataset and task will inspire further research in complex interactions with natural language, fostering the development of more advanced AI agents.", "year": 2023, "publicationdate": "2023-07-24", "externalids": {"DOI": "10.48550/arXiv.2307.12573"}, "doi_lower": "10.48550/arxiv.2307.12573"}
{"paper_id": 260682249, "title": "AgentBench: Evaluating LLMs as Agents", "author_names": ["Xiao Liu", "Hao Yu", "Hanchen Zhang", "Yifan Xu", "Xuanyu Lei", "Hanyu Lai", "Yu Gu", "Yuxian Gu", "Hangliang Ding", "Kai Men", "Kejuan Yang", "Shudan Zhang", "Xiang Deng", "Aohan Zeng", "Zhengxiao Du", "Chenhui Zhang", "Shengqi Shen", "Tianjun Zhang", "Sheng Shen", "Yu Su", "Huan Sun", "Minlie Huang", "Yuxiao Dong", "Jie Tang"], "venue": "International Conference on Learning Representations", "abstract": "The potential of Large Language Model (LLM) as agents has been widely acknowledged recently. Thus, there is an urgent need to quantitatively \\textit{evaluate LLMs as agents} on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities. Our extensive test over \\num API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Improving instruction following and training on high quality multi-round alignment data could improve agent performance. And different from existing assumptions, training on code present ambivalent impacts on different agent tasks. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.", "year": 2023, "publicationdate": "2023-08-07", "externalids": {"DOI": "10.48550/arXiv.2308.03688"}, "doi_lower": "10.48550/arxiv.2308.03688"}
{"paper_id": 260865960, "title": "BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents", "author_names": ["Zhiwei Liu", "Weiran Yao", "Jianguo Zhang", "Le Xue", "Shelby Heinecke", "Rithesh Murthy", "Yihao Feng", "Zeyuan Chen", "Juan Carlos Niebles", "Devansh Arpit", "Ran Xu", "P. Mùi", "Haiquan Wang", "Caiming Xiong", "S. Savarese"], "venue": "arXiv.org", "abstract": "The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, \\textit{i.e.} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at \\url{https://github.com/salesforce/BOLAA}.", "year": 2023, "publicationdate": "2023-08-11", "externalids": {"DOI": "10.48550/arXiv.2308.05960"}, "doi_lower": "10.48550/arxiv.2308.05960"}
{"paper_id": 260682960, "title": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench", "author_names": ["Jen-Tse Huang", "Man Ho Lam", "E. Li", "Shujie Ren", "Wenxuan Wang", "Wenxiang Jiao", "Zhaopeng Tu", "Michael R. Lyu"], "venue": "arXiv.org", "abstract": "Evaluating Large Language Models' (LLMs) anthropomorphic capabilities has become increasingly important in contemporary discourse. Utilizing the emotion appraisal theory from psychology, we propose to evaluate the empathy ability of LLMs, i.e., how their feelings change when presented with specific situations. After a careful and comprehensive survey, we collect a dataset containing over 400 situations that have proven effective in eliciting the eight emotions central to our study. Categorizing the situations into 36 factors, we conduct a human evaluation involving more than 1,200 subjects worldwide. With the human evaluation results as references, our evaluation includes seven LLMs, covering both commercial and open-source models, including variations in model sizes, featuring the latest iterations, such as GPT-4, Mixtral-8x22B, and LLaMA-3.1. We find that, despite several misalignments, LLMs can generally respond appropriately to certain situations. Nevertheless, they fall short in alignment with the emotional behaviors of human beings and cannot establish connections between similar situations. Our collected dataset of situations, the human evaluation results, and the code of our testing framework, i.e., EmotionBench, are publicly available at https://github.com/CUHK-ARISE/EmotionBench.", "year": 2023, "publicationdate": "2023-08-07", "externalids": {"DOI": "10.48550/arXiv.2308.03656"}, "doi_lower": "10.48550/arxiv.2308.03656"}
{"paper_id": 260735941, "title": "Benchmarking LLM powered Chatbots: Methods and Metrics", "author_names": ["D. Banerjee", "Pooja Singh", "Arjun Avadhanam", "Shashank Srivastava"], "venue": "arXiv.org", "abstract": "Autonomous conversational agents, i.e. chatbots, are becoming an increasingly common mechanism for enterprises to provide support to customers and partners. In order to rate chatbots, especially ones powered by Generative AI tools like Large Language Models (LLMs) we need to be able to accurately assess their performance. This is where chatbot benchmarking becomes important. In this paper, we propose the use of a novel benchmark that we call the E2E (End to End) benchmark, and show how the E2E benchmark can be used to evaluate accuracy and usefulness of the answers provided by chatbots, especially ones powered by LLMs. We evaluate an example chatbot at different levels of sophistication based on both our E2E benchmark, as well as other available metrics commonly used in the state of art, and observe that the proposed benchmark show better results compared to others. In addition, while some metrics proved to be unpredictable, the metric associated with the E2E benchmark, which uses cosine similarity performed well in evaluating chatbots. The performance of our best models shows that there are several benefits of using the cosine similarity score as a metric in the E2E benchmark.", "year": 2023, "publicationdate": "2023-08-08", "externalids": {"DOI": "10.48550/arXiv.2308.04624"}, "doi_lower": "10.48550/arxiv.2308.04624"}
{"paper_id": 257900969, "title": "A Survey of Large Language Models", "author_names": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Z. Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "J. Nie", "Ji-rong Wen"], "venue": "arXiv.org", "abstract": "Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.", "year": 2023, "publicationdate": "2023-03-31", "externalids": {}, "doi_lower": null}
{"paper_id": 258331833, "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond", "author_names": ["Jingfeng Yang", "Hongye Jin", "Ruixiang Tang", "Xiaotian Han", "Qizhang Feng", "Haoming Jiang", "Bing Yin", "Xia Hu"], "venue": "ACM Transactions on Knowledge Discovery from Data", "abstract": "This article presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream Natural Language Processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. First, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at https://github.com/Mooler0410/LLMsPracticalGuide. An LLMs evolutionary tree, editable yet regularly updated, can be found at llmtree.ai.", "year": 2023, "publicationdate": "2023-04-26", "externalids": {"DOI": "10.1145/3649506"}, "doi_lower": "10.1145/3649506"}
{"paper_id": 275570199, "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models", "author_names": ["Fengli Xu", "Qianyue Hao", "Zefang Zong", "Jingwei Wang", "Yunke Zhang", "Jingyi Wang", "Xiaochong Lan", "Jiahui Gong", "Tianjian Ouyang", "Fanjin Meng", "Chenyang Shao", "Yuwei Yan", "Qinglong Yang", "Yiwen Song", "Sijian Ren", "Xinyuan Hu", "Yu Li", "J. Feng", "Chen Gao", "Yong Li"], "venue": "arXiv.org", "abstract": "Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of\"thought\"-- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to\"think\"with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI's o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.", "year": 2025, "publicationdate": "2025-01-16", "externalids": {"DOI": "10.48550/arXiv.2501.09686"}, "doi_lower": "10.48550/arxiv.2501.09686"}
{"paper_id": 259360395, "title": "A Survey on Evaluation of Large Language Models", "author_names": ["Yu-Chu Chang", "Xu Wang", "Jindong Wang", "Yuan Wu", "Kaijie Zhu", "Hao Chen", "Linyi Yang", "Xiaoyuan Yi", "Cunxiang Wang", "Yidong Wang", "Weirong Ye", "Yue Zhang", "Yi Chang", "Philip S. Yu", "Qian Yang", "Xingxu Xie"], "venue": "ACM Transactions on Intelligent Systems and Technology", "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey", "year": 2023, "publicationdate": "2023-07-06", "externalids": {"DOI": "10.1145/3641289"}, "doi_lower": "10.1145/3641289"}
{"paper_id": 257636789, "title": "Language Model Behavior: A Comprehensive Survey", "author_names": ["Tyler A. Chang", "B. Bergen"], "venue": "International Conference on Computational Logic", "abstract": "Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning. Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.", "year": 2023, "publicationdate": "2023-03-20", "externalids": {"DOI": "10.1162/coli_a_00492"}, "doi_lower": "10.1162/coli_a_00492"}
{"paper_id": 263864128, "title": "EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus", "author_names": ["Cheng Li", "Jindong Wang", "Kaijie Zhu", "Yixuan Zhang", "Wenxin Hou", "Jianxun Lian", "Xing Xie"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2307.11760"}, "doi_lower": "10.48550/arxiv.2307.11760"}
{"paper_id": 256389762, "title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex", "author_names": ["Terry Yue Zhuo", "Zhuang Li", "Yujin Huang", "Yuan-Fang Li", "Weiqing Wang", "Gholamreza Haffari", "Fatemeh Shiri"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "Semantic parsing is a technique aimed at constructing a structured representation of the meaning of a natural-language question. Recent advances in language models trained on code have shown superior performance in generating these representations compared to language models trained solely on natural language text. The existing fine-tuned neural semantic parsers are vulnerable to adversarial attacks on natural-language inputs. While it has been established that the robustness of smaller semantic parsers can be enhanced through adversarial training, this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data. This paper presents the first empirical study on the adversarial robustness of a prompt-based semantic parser based on CODEX, a stateof-the-art (SOTA) language model trained on code. Our results demonstrate that the large language model of code is vulnerable to carefully crafted adversarial examples. To overcome this challenge, we propose methods for enhancing robustness without requiring substantial amounts of labelled data or intensive computational resources.", "year": 2023, "publicationdate": "2023-01-30", "externalids": {"DOI": "10.48550/arXiv.2301.12868"}, "doi_lower": "10.48550/arxiv.2301.12868"}
{"paper_id": 250113278, "title": "On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method", "author_names": ["Zorik Gekhman", "Nadav Oved", "Orgad Keller", "Idan Szpektor", "Roi Reichart"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "Most work on modeling the conversation history in Conversational Question Answering (CQA) reports a single main result on a common CQA benchmark. While existing models show impressive results on CQA leaderboards, it remains unclear whether they are robust to shifts in setting (sometimes to more realistic ones), training data size (e.g., from large to small sets) and domain. In this work, we design and conduct the first large-scale robustness study of history modeling approaches for CQA. We find that high benchmark scores do not necessarily translate to strong robustness, and that various methods can perform extremely differently under different settings. Equipped with the insights from our study, we design a novel prompt-based history modeling approach and demonstrate its strong robustness across various settings. Our approach is inspired by existing methods that highlight historic answers in the passage. However, instead of highlighting by modifying the passage token embeddings, we add textual prompts directly in the passage text. Our approach is simple, easy to plug into practically any model, and highly effective, thus we recommend it as a starting point for future model developers. We also hope that our study and insights will raise awareness to the importance of robustness-focused evaluation, in addition to obtaining high leaderboard scores, leading to better CQA systems.1", "year": 2022, "publicationdate": "2022-06-29", "externalids": {"DOI": "10.1162/tacl_a_00549"}, "doi_lower": "10.1162/tacl_a_00549"}
{"paper_id": 271859930, "title": "Causal Agent based on Large Language Model", "author_names": ["Kairong Han", "Kun Kuang", "Ziyu Zhao", "Junjian Ye", "Fei Wu"], "venue": "arXiv.org", "abstract": "The large language model (LLM) has achieved significant success across various domains. However, the inherent complexity of causal problems and causal theory poses challenges in accurately describing them in natural language, making it difficult for LLM to comprehend and use them effectively. Causal methods are not easily conveyed through natural language, which hinders LLM's ability to apply them accurately. Additionally, causal datasets are typically tabular, while LLM excels in handling natural language data, creating a structural mismatch that impedes effective reasoning with tabular data. To address these challenges, we have equipped the LLM with causal tools within an agent framework, named the Causal Agent, enabling it to tackle causal problems. The causal agent comprises tools, memory, and reasoning modules. In the tool module, the causal agent calls Python code and uses the encapsulated causal function module to align tabular data with natural language. In the reasoning module, the causal agent performs reasoning through multiple iterations with the tools. In the memory module, the causal agent maintains a dictionary instance where the keys are unique names and the values are causal graphs. To verify the causal ability of the causal agent, we established a Causal Tabular Question Answer (CausalTQA) benchmark consisting of four levels of causal problems: variable level, edge level, causal graph level, and causal effect level. CausalTQA consists of about 1.4K for these four levels questions. Causal agent demonstrates remarkable efficacy on the four-level causal problems, with accuracy rates all above 80\\%. Through verification on the real-world dataset QRData, the causal agent is 6\\% higher than the original SOTA. For further insights and implementation details, our code is accessible via the GitHub repository https://github.com/kairong-han/causal_agent.", "year": 2024, "publicationdate": "2024-08-13", "externalids": {"DOI": "10.48550/arXiv.2408.06849"}, "doi_lower": "10.48550/arxiv.2408.06849"}
