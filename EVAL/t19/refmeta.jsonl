{"paper_id": 257532815, "title": "GPT-4 Technical Report", "author_names": ["OpenAI Josh Achiam", "Steven Adler", "Sandhini Agarwal", "Lama Ahmad", "Ilge Akkaya", "Florencia Leoni Aleman", "Diogo Almeida", "Janko Altenschmidt", "Sam Altman", "Shyamal Anadkat", "Red Avila", "Igor Babuschkin", "S. Balaji", "Valerie Balcom", "Paul Baltescu", "Haim-ing Bao", "Mo Bavarian", "J. Belgum", "Irwan Bello", "Jake Berdine", "Gabriel Bernadett-Shapiro", "Christopher Berner", "Lenny Bogdonoff", "Oleg Boiko", "Made-laine Boyd", "Anna-Luisa Brakman", "Greg Brockman", "Tim Brooks", "Miles Brundage", "Kevin Button", "Trevor Cai", "Rosie Campbell", "Andrew Cann", "Brittany Carey", "Chelsea Carlson", "Rory Carmichael", "Brooke Chan", "Che Chang", "Fotis Chantzis", "Derek Chen", "Sully Chen", "Ruby Chen", "Jason Chen", "Mark Chen", "Benjamin Chess", "Chester Cho", "Casey Chu", "Hyung Won Chung", "Dave Cummings", "Jeremiah Currier", "Yunxing Dai", "Cory Decareaux", "Thomas Degry", "Noah Deutsch", "Damien Deville", "Arka Dhar", "David Dohan", "Steve Dowling", "Sheila Dunning", "Adrien Ecoffet", "Atty Eleti", "Tyna Eloundou", "David Farhi", "L. Fedus", "Niko Felix", "Sim'on Posada Fishman", "Juston Forte", "Is-abella Fulford", "Leo Gao", "Elie Georges", "C. Gibson", "Vik Goel", "Tarun Gogineni", "Gabriel Goh", "Raphael Gontijo-Lopes", "Jonathan Gordon", "Morgan Grafstein", "Scott Gray", "Ryan Greene", "Joshua Gross", "S. Gu", "Yufei Guo", "Chris Hallacy", "Jesse Han", "Jeff Harris", "Yuchen He", "Mike Heaton", "Johannes Heidecke", "Chris Hesse", "Alan Hickey", "W. Hickey", "Peter Hoeschele", "Brandon Houghton", "Kenny Hsu", "Shengli Hu", "Xin Hu", "Joost Huizinga", "Shantanu Jain", "Shawn Jain", "Joanne Jang", "Angela Jiang", "Roger Jiang", "Haozhun Jin", "Denny Jin", "Shino Jomoto", "B. Jonn", "Heewoo Jun", "Tomer Kaftan", "Lukasz Kaiser", "Ali Kamali", "I. Kanitscheider", "N. Keskar", "Tabarak Khan", "Logan Kilpatrick", "Jong Wook Kim", "Christina Kim", "Yongjik Kim", "Hendrik Kirchner", "J. Kiros", "Matthew Knight", "Daniel Kokotajlo", "Lukasz Kondraciuk", "Andrew Kondrich", "Aris Konstantinidis", "Kyle Kosic", "Gretchen Krueger", "Vishal Kuo", "Michael Lampe", "Ikai Lan", "Teddy Lee", "Jan Leike", "Jade Leung", "Daniel Levy", "Chak Li", "Rachel Lim", "Molly Lin", "Stephanie Lin", "Ma-teusz Litwin", "Theresa Lopez", "Ryan Lowe", "Patricia Lue", "A. Makanju", "Kim Malfacini", "Sam Manning", "Todor Markov", "Yaniv Markovski", "Bianca Martin", "Katie Mayer", "Andrew Mayne", "Bob McGrew", "S. McKinney", "Christine McLeavey", "Paul McMillan", "Jake McNeil", "David Medina", "Aalok Mehta", "Jacob Menick", "Luke Metz", "An-drey Mishchenko", "Pamela Mishkin", "Vinnie Monaco", "Evan Morikawa", "Daniel P. Mossing", "Tong Mu", "Mira Murati", "O. Murk", "David M'ely", "Ashvin Nair", "Reiichiro Nakano", "Rajeev Nayak", "Arvind Neelakantan", "Richard Ngo", "Hyeonwoo Noh", "Ouyang Long", "Cullen O'Keefe", "J. Pachocki", "A. Paino", "Joe Palermo", "Ashley Pantuliano", "Giambattista Parascandolo", "J. Parish", "Emy Parparita", "Alexandre Passos", "Mikhail Pavlov", "Andrew Peng", "Adam Perelman", "Filipe de Avila Belbute Peres", "Michael Petrov", "Henrique Pondé de Oliveira Pinto", "Michael Pokorny", "Michelle Pokrass", "Vitchyr H. Pong", "Tolly Powell", "Alethea Power", "Boris Power", "Elizabeth Proehl", "Raul Puri", "Alec Radford", "Jack W. Rae", "Aditya Ramesh", "Cameron Raymond", "Francis Real", "Kendra Rimbach", "Carl Ross", "Bob Rotsted", "Henri Roussez", "N. Ryder", "M. Saltarelli", "Ted Sanders", "Shibani Santurkar", "Girish Sastry", "Heather Schmidt", "David Schnurr", "John Schulman", "Daniel Selsam", "Kyla Sheppard", "Toki Sherbakov", "Jessica Shieh", "Sarah Shoker", "Pranav Shyam", "Szymon Sidor", "Eric Sigler", "Maddie Simens", "Jordan Sitkin", "Katarina Slama", "Ian Sohl", "Benjamin Sokolowsky", "Yang Song", "Natalie Staudacher", "F. Such", "Natalie Summers", "I. Sutskever", "Jie Tang", "N. Tezak", "Madeleine Thompson", "P. Tillet", "Amin Tootoonchian", "Elizabeth Tseng", "Preston Tuggle", "Nick Turley", "Jerry Tworek", "Juan Felipe Cer'on Uribe", "Andrea Vallone", "Arun Vijayvergiya", "Chelsea Voss", "Carroll L. Wainwright", "Justin Jay Wang", "Alvin Wang", "Ben Wang", "Jonathan Ward", "Jason Wei", "CJ Weinmann", "Akila Welihinda", "Peter Welinder", "Jiayi Weng", "Lilian Weng", "Matt Wiethoff", "Dave Willner", "Clemens Winter", "Samuel Wolrich", "Hannah Wong", "Lauren Workman", "Sherwin Wu", "Jeff Wu", "Michael Wu", "Kai Xiao", "Tao Xu", "Sarah Yoo", "Kevin Yu", "Qim-ing Yuan", "Wojciech Zaremba", "Rowan Zellers", "Chong Zhang", "Marvin Zhang", "Shengjia Zhao", "Tianhao Zheng", "Juntang Zhuang", "William Zhuk", "Barret Zoph"], "venue": "", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "year": 2023, "publicationdate": "2023-03-15", "externalids": {}, "doi_lower": null}
{"paper_id": 267060969, "title": "Structured Code Representations Enable Data-Efficient Adaptation of Code Language Models", "author_names": ["Mayank Agarwal", "Yikang Shen", "Bailin Wang", "Yoon Kim", "Jie Chen"], "venue": "arXiv.org", "abstract": "Current language models tailored for code tasks often adopt the pre-training-then-fine-tuning paradigm from natural language processing, modeling source code as plain text. This approach, however, overlooks the unambiguous structures inherent in programming languages. In this work, we explore data-efficient adaptation of pre-trained code models by further pre-training and fine-tuning them with program structures. Specifically, we represent programs as parse trees -- also known as concrete syntax trees (CSTs) -- and adapt pre-trained models on serialized CSTs. Although the models that we adapt have been pre-trained only on the surface form of programs, we find that a small amount of continual pre-training and fine-tuning on CSTs without changing the model architecture yields improvements over the baseline approach across various code tasks. The improvements are found to be particularly significant when there are limited training examples, demonstrating the effectiveness of integrating program structures with plain-text representation even when working with backbone models that have not been pre-trained with structures.", "year": 2024, "publicationdate": "2024-01-19", "externalids": {"DOI": "10.48550/arXiv.2401.10716"}, "doi_lower": "10.48550/arxiv.2401.10716"}
{"paper_id": 4254748, "title": "Memory Aware Synapses: Learning what (not) to forget", "author_names": ["Rahaf Aljundi", "F. Babiloni", "Mohamed Elhoseiny", "Marcus Rohrbach", "T. Tuytelaars"], "venue": "European Conference on Computer Vision", "abstract": "Humans can learn in a continuous manner. Old rarely utilized knowledge can be overwritten by new incoming information while important, frequently used knowledge is prevented from being erased. In artificial learning systems, lifelong learning so far has focused mainly on accumulating knowledge over tasks and overcoming catastrophic forgetting. In this paper, we argue that, given the limited model capacity and the unlimited new information to be learned, knowledge has to be preserved or erased selectively. Inspired by neuroplasticity, we propose a novel approach for lifelong learning, coined Memory Aware Synapses (MAS). It computes the importance of the parameters of a neural network in an unsupervised and online manner. Given a new sample which is fed to the network, MAS accumulates an importance measure for each parameter of the network, based on how sensitive the predicted output function is to a change in this parameter. When learning a new task, changes to important parameters can then be penalized, effectively preventing important knowledge related to previous tasks from being overwritten. Further, we show an interesting connection between a local version of our method and Hebb's rule,which is a model for the learning process in the brain. We test our method on a sequence of object recognition tasks and on the challenging problem of learning an embedding for predicting $ $ triplets. We show state-of-the-art performance and, for the first time, the ability to adapt the importance of the parameters based on unlabeled data towards what the network needs (not) to forget, which may vary depending on test conditions.", "year": 2017, "publicationdate": "2017-11-27", "externalids": {"DOI": "10.1007/978-3-030-01219-9_9"}, "doi_lower": "10.1007/978-3-030-01219-9_9"}
{"paper_id": 235417543, "title": "Dynamic Language Models for Continuously Evolving Content", "author_names": ["Spurthi Amba Hombaiah", "Tao Chen", "Mingyang Zhang", "Michael Bendersky", "Marc Najork"], "venue": "Knowledge Discovery and Data Mining", "abstract": "The content on the web is in a constant state of flux. New entities,issues, and ideas continuously emerge, while the semantics of the existing conversation topics gradually shift. In recent years, pre-trained language models like BERT greatly improved the state-of-the-art for a large spectrum of content understanding tasks.Therefore, in this paper, we aim to study how these language models can be adapted to better handle continuously evolving web content.In our study, we first analyze the evolution of 2013 - 2019 Twitter data, and unequivocally confirm that a BERT model trained on past tweets would heavily deteriorate when directly applied to data from later years. Then, we investigate two possible sources of the deterioration: the semantic shift of existing tokens and the sub-optimal or failed understanding of new tokens. To this end, we both explore two different vocabulary composition methods, as well as propose three sampling methods which help in efficient incremental training for BERT-like models. Compared to a new model trained from scratch offline, our incremental training (a) reduces the training costs, (b) achieves better performance on evolving content, and (c)is suitable for online deployment. The superiority of our methods is validated using two downstream tasks. We demonstrate significant improvements when incrementally evolving the model from a particular base year, on the task of Country Hashtag Prediction, as well as on the OffensEval 2019 task.", "year": 2021, "publicationdate": "2021-06-11", "externalids": {"DOI": "10.1145/3447548.3467162"}, "doi_lower": "10.1145/3447548.3467162"}
{"paper_id": 258740735, "title": "PaLM 2 Technical Report", "author_names": ["Rohan Anil", "Andrew M. Dai", "Orhan Firat", "Melvin Johnson", "Dmitry Lepikhin", "Alexandre Passos", "Siamak Shakeri", "Emanuel Taropa", "Paige Bailey", "Z. Chen", "Eric Chu", "J. Clark", "Laurent El Shafey", "Yanping Huang", "K. Meier-Hellstern", "Gaurav Mishra", "Erica Moreira", "Mark Omernick", "Kevin Robinson", "Sebastian Ruder", "Yi Tay", "Kefan Xiao", "Yuanzhong Xu", "Yujing Zhang", "Gustavo Hernández Abrego", "Junwhan Ahn", "Jacob Austin", "P. Barham", "Jan A. Botha", "James Bradbury", "Siddhartha Brahma", "K. Brooks", "Michele Catasta", "Yongzhou Cheng", "Colin Cherry", "Christopher A. Choquette-Choo", "A. Chowdhery", "C. Crépy", "Shachi Dave", "Mostafa Dehghani", "Sunipa Dev", "Jacob Devlin", "M. D'iaz", "Nan Du", "Ethan Dyer", "Vladimir Feinberg", "Fan Feng", "Vlad Fienber", "Markus Freitag", "Xavier García", "Sebastian Gehrmann", "Lucas González", "Guy Gur-Ari", "S. Hand", "Hadi Hashemi", "Le Hou", "Joshua Howland", "A. Hu", "Jef-frey Hui", "Jeremy Hurwitz", "M. Isard", "Abe Ittycheriah", "Matthew Jagielski", "W. Jia", "Kathleen Kenealy", "M. Krikun", "Sneha Kudugunta", "Chang Lan", "Katherine Lee", "Benjamin Lee", "Eric Li", "Mu-Li Li", "Wei Li", "Yaguang Li", "Jun Yu Li", "Hyeontaek Lim", "Han Lin", "Zhong-Zhong Liu", "Frederick Liu", "M. Maggioni", "Aroma Mahendru", "Joshua Maynez", "Vedant Misra", "Maysam Moussalem", "Zachary Nado", "John Nham", "Eric Ni", "A. Nystrom", "Alicia Parrish", "Marie Pellat", "M. Polacek", "Oleksandr Polozov", "Reiner Pope", "Siyuan Qiao", "Emily Reif", "Bryan Richter", "Parker Riley", "A. Ros", "Aurko Roy", "Brennan Saeta", "R. Samuel", "R. Shelby", "Ambrose Slone", "D. Smilkov", "David R. So", "Daniela Sohn", "Simon Tokumine", "Dasha Valter", "Vijay Vasudevan", "Kiran Vodrahalli", "Xuezhi Wang", "Pidong Wang", "Zirui Wang", "Tao Wang", "J. Wieting", "Yuhuai Wu", "Ke Xu", "Yunhan Xu", "L. Xue", "Pengcheng Yin", "Jiahui Yu", "Qiaoling Zhang", "Steven Zheng", "Ce Zheng", "Wei Zhou", "Denny Zhou", "Slav Petrov", "Yonghui Wu"], "venue": "arXiv.org", "abstract": "We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities. When discussing the PaLM 2 family, it is important to distinguish between pre-trained models (of various sizes), fine-tuned variants of these models, and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {}, "doi_lower": null}
{"paper_id": 258480391, "title": "Is It Worth the (Environmental) Cost? Limited Evidence for Temporal Adaptation via Continuous Training", "author_names": ["Giuseppe Attanasio", "Debora Nozza", "Federico Bianchi", "Dirk Hovy"], "venue": "", "abstract": "Language is constantly changing and evolving, leaving language models to become quickly outdated. Consequently, we should continuously update our models with new data to expose them to new events and facts. However, that requires additional computing, which means new carbon emissions. Do any measurable benefits justify this cost? This paper looks for empirical evidence to support continuous training. We reproduce existing benchmarks and extend them to include additional time periods, models, and tasks. Our results show that the downstream task performance of temporally adapted English models for social media data do not improve over time. Pretrained models without temporal adaptation are actually significantly more effective and efficient. However, we also note a lack of suitable temporal benchmarks. Our findings invite a critical reflection on when and how to temporally adapt language models, accounting for sustainability.", "year": 2022, "publicationdate": "2022-10-13", "externalids": {}, "doi_lower": null}
{"paper_id": 264172303, "title": "Llemma: An Open Language Model For Mathematics", "author_names": ["Zhangir Azerbayev", "Hailey Schoelkopf", "Keiran Paster", "Marco Dos Santos", "S. McAleer", "Albert Q. Jiang", "Jia Deng", "Stella Biderman", "S. Welleck"], "venue": "International Conference on Learning Representations", "abstract": "We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.", "year": 2023, "publicationdate": "2023-10-16", "externalids": {"DOI": "10.48550/arXiv.2310.10631"}, "doi_lower": "10.48550/arxiv.2310.10631"}
{"paper_id": 257834207, "title": "Enhancing Continual Learning with Global Prototypes: Counteracting Negative Representation Drift", "author_names": ["Xueying Bai", "Jinghuan Shang", "Yifan Sun", "Niranjan Balasubramanian"], "venue": "", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 248118878, "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "author_names": ["Yuntao Bai", "Andy Jones", "Kamal Ndousse", "Amanda Askell", "Anna Chen", "Nova Dassarma", "Dawn Drain", "Stanislav Fort", "Deep Ganguli", "T. Henighan", "Nicholas Joseph", "Saurav Kadavath", "John Kernion", "Tom Conerly", "S. El-Showk", "Nelson Elhage", "Zac Hatfield-Dodds", "Danny Hernandez", "Tristan Hume", "Scott Johnston", "Shauna Kravec", "Liane Lovitt", "Neel Nanda", "Catherine Olsson", "Dario Amodei", "Tom B. Brown", "Jack Clark", "Sam McCandlish", "Chris Olah", "Benjamin Mann", "Jared Kaplan"], "venue": "arXiv.org", "abstract": "We apply preference modeling and reinforcement learning from human feedback (RLHF) to ﬁnetune language models to act as helpful and harmless assistants. We ﬁnd this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efﬁciently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work. Figure These plots show that PM accuracy decreases as we focus exclusively on comparisons between pairs of samples with high score. We have normalized all preference models to have the same mean score on a held-out dataset so that they’re directly comparable, and then plotted accuracy for the comparisons where both samples have scores above a speciﬁc threshold.", "year": 2022, "publicationdate": "2022-04-12", "externalids": {"DOI": "10.48550/arXiv.2204.05862"}, "doi_lower": "10.48550/arxiv.2204.05862"}
{"paper_id": 7164502, "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments", "author_names": ["Satanjeev Banerjee", "A. Lavie"], "venue": "IEEvaluation@ACL", "abstract": null, "year": 2005, "publicationdate": "2005-06-01", "externalids": {}, "doi_lower": null}
{"paper_id": 210868223, "title": "The Pushshift Reddit Dataset", "author_names": ["Jason Baumgartner", "Savvas Zannettou", "Brian Keegan", "Megan Squire", "Jeremy Blackburn"], "venue": "International Conference on Web and Social Media", "abstract": "Social media data has become crucial to the advancement of scientific understanding. However, even though it has become ubiquitous, just collecting large-scale social media data involves a high degree of engineering skill set and computational resources. In fact, research is often times gated by data engineering problems that must be overcome before analysis can proceed. This has resulted recognition of datasets as meaningful research contributions in and of themselves.Reddit, the so called “front page of the Internet,” in particular has been the subject of numerous scientific studies. Although Reddit is relatively open to data acquisition compared to social media platforms like Facebook and Twitter, the technical barriers to acquisition still remain. Thus, Reddit's millions of subreddits, hundreds of millions of users, and billions of comments are at the same time relatively accessible, but time consuming to collect and analyze systematically.In this paper, we present the Pushshift Reddit dataset. Pushshift is a social media data collection, analysis, and archiving platform that since 2015 has collected Reddit data and made it available to researchers. Pushshift's Reddit dataset is updated in real-time, and includes historical data back to Reddit's inception. In addition to monthly dumps, Pushshift provides computational tools to aid in searching, aggregating, and performing exploratory analysis on the entirety of the dataset. The Pushshift Reddit dataset makes it possible for social media researchers to reduce time spent in the data collection, cleaning, and storage phases of their projects.", "year": 2020, "publicationdate": "2020-01-23", "externalids": {"DOI": "10.5281/ZENODO.3608135"}, "doi_lower": "10.5281/zenodo.3608135"}
{"paper_id": 263608392, "title": "OceanGPT: A Large Language Model for Ocean Science Tasks", "author_names": ["Zhen Bi", "Ningyu Zhang", "Yida Xue", "Yixin Ou", "Daxiong Ji", "Guozhou Zheng", "Huajun Chen"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reasons are the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever large language model in the ocean domain, which is expert in various ocean science tasks. We also propose OceanGPT, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though comprehensive experiments, OceanGPT not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.", "year": 2023, "publicationdate": "2023-10-03", "externalids": {"DOI": "10.48550/arXiv.2310.02031"}, "doi_lower": "10.48550/arxiv.2310.02031"}
{"paper_id": 227231454, "title": "Continual Lifelong Learning in Natural Language Processing: A Survey", "author_names": ["Magdalena Biesialska", "Katarzyna Biesialska", "M. Costa-jussà"], "venue": "International Conference on Computational Linguistics", "abstract": "Continual learning (CL) aims to enable information systems to learn from a continuous data stream across time. However, it is difficult for existing deep learning architectures to learn a new task without largely forgetting previously acquired knowledge. Furthermore, CL is particularly challenging for language learning, as natural language is ambiguous: it is discrete, compositional, and its meaning is context-dependent. In this work, we look at the problem of CL through the lens of various NLP tasks. Our survey discusses major challenges in CL and current methods applied in neural network models. We also provide a critical review of the existing CL evaluation methods and datasets in NLP. Finally, we present our outlook on future research directions.", "year": 2020, "publicationdate": "2020-12-17", "externalids": {"DOI": "10.18653/v1/2020.coling-main.574"}, "doi_lower": "10.18653/v1/2020.coling-main.574"}
{"paper_id": 208290939, "title": "PIQA: Reasoning about Physical Commonsense in Natural Language", "author_names": ["Yonatan Bisk", "Rowan Zellers", "Ronan Le Bras", "Jianfeng Gao", "Yejin Choi"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains – such as news articles and encyclopedia entries, where text is plentiful – in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical commonsense questions without experiencing the physical world?In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95% accuracy), large pretrained models struggle (∼75%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.", "year": 2019, "publicationdate": "2019-11-26", "externalids": {"DOI": "10.1609/AAAI.V34I05.6239"}, "doi_lower": "10.1609/aaai.v34i05.6239"}
{"paper_id": 15535376, "title": "Findings of the 2014 Workshop on Statistical Machine Translation", "author_names": ["Ondrej Bojar", "C. Buck", "C. Federmann", "B. Haddow", "Philipp Koehn", "Johannes Leveling", "Christof Monz", "Pavel Pecina", "Matt Post", "Herve Saint-Amand", "Radu Soricut", "Lucia Specia", "Ales Tamchyna"], "venue": "WMT@ACL", "abstract": "This paper presents the results of the WMT14 shared tasks, which included a standard news translation task, a separate medical translation task, a task for run-time estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries", "year": 2014, "publicationdate": "2014-06-01", "externalids": {"DOI": "10.3115/v1/W14-3302"}, "doi_lower": "10.3115/v1/w14-3302"}
{"paper_id": 268248289, "title": "Transformers for Supervised Online Continual Learning", "author_names": ["J. Bornschein", "Yazhe Li", "Amal Rannen-Triki"], "venue": "arXiv.org", "abstract": "Transformers have become the dominant architecture for sequence modeling tasks such as natural language processing or audio processing, and they are now even considered for tasks that are not naturally sequential such as image classification. Their ability to attend to and to process a set of tokens as context enables them to develop in-context few-shot learning abilities. However, their potential for online continual learning remains relatively unexplored. In online continual learning, a model must adapt to a non-stationary stream of data, minimizing the cumulative nextstep prediction loss. We focus on the supervised online continual learning setting, where we learn a predictor $x_t \\rightarrow y_t$ for a sequence of examples $(x_t, y_t)$. Inspired by the in-context learning capabilities of transformers and their connection to meta-learning, we propose a method that leverages these strengths for online continual learning. Our approach explicitly conditions a transformer on recent observations, while at the same time online training it with stochastic gradient descent, following the procedure introduced with Transformer-XL. We incorporate replay to maintain the benefits of multi-epoch training while adhering to the sequential protocol. We hypothesize that this combination enables fast adaptation through in-context learning and sustained longterm improvement via parametric learning. Our method demonstrates significant improvements over previous state-of-the-art results on CLOC, a challenging large-scale real-world benchmark for image geo-localization.", "year": 2024, "publicationdate": "2024-03-03", "externalids": {"DOI": "10.48550/arXiv.2403.01554"}, "doi_lower": "10.48550/arxiv.2403.01554"}
{"paper_id": 155390561, "title": "IN B V N BAC BLAC VALUE C LIE", "author_names": ["E. Pr"], "venue": "", "abstract": null, "year": 2011, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 218971783, "title": "Language Models are Few-Shot Learners", "author_names": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "J. Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "T. Henighan", "R. Child", "A. Ramesh", "Daniel M. Ziegler", "Jeff Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Ma-teusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "I. Sutskever", "Dario Amodei"], "venue": "Neural Information Processing Systems", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "year": 2020, "publicationdate": "2020-05-28", "externalids": {}, "doi_lower": null}
{"paper_id": 29658212, "title": "DBpedia Abstracts: A Large-Scale, Open, Multilingual NLP Training Corpus", "author_names": ["Martin Brümmer", "Milan Dojchinovski", "Sebastian Hellmann"], "venue": "International Conference on Language Resources and Evaluation", "abstract": null, "year": 2016, "publicationdate": "2016-05-01", "externalids": {}, "doi_lower": null}
{"paper_id": 215768806, "title": "Dark Experience for General Continual Learning: a Strong, Simple Baseline", "author_names": ["Pietro Buzzega", "Matteo Boschini", "Angelo Porrello", "Davide Abati", "S. Calderara"], "venue": "Neural Information Processing Systems", "abstract": "Neural networks struggle to learn continuously, as they forget the old knowledge catastrophically whenever the data distribution changes over time. Recently, Continual Learning has inspired a plethora of approaches and evaluation settings; however, the majority of them overlooks the properties of a practical scenario, where the data stream cannot be shaped as a sequence of tasks and offline training is not viable. We work towards General Continual Learning (GCL), where task boundaries blur and the domain and class distributions shift either gradually or suddenly. We address it through Dark Experience Replay, namely matching the network's logits sampled throughout the optimization trajectory, thus promoting consistency with its past. By conducting an extensive analysis on top of standard benchmarks, we show that such a seemingly simple baseline outperforms consolidated approaches and leverages limited resources. To provide a better understanding, we further introduce MNIST-360, a novel GCL evaluation setting.", "year": 2020, "publicationdate": "2020-04-15", "externalids": {}, "doi_lower": null}
{"paper_id": 265466509, "title": "InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery", "author_names": ["He Cao", "Zijing Liu", "Xingyu Lu", "Yuan Yao", "Yu Li"], "venue": "International Conference on Computational Linguistics", "abstract": "The rapid evolution of artificial intelligence in drug discovery encounters challenges with generalization and extensive training, yet Large Language Models (LLMs) offer promise in reshaping interactions with complex molecular data. Our novel contribution, InstructMol, a multi-modal LLM, effectively aligns molecular structures with natural language via an instruction-tuning approach, utilizing a two-stage training strategy that adeptly combines limited domain-specific data with molecular and textual information. InstructMol showcases substantial performance improvements in drug discovery-related molecular tasks, surpassing leading LLMs and significantly reducing the gap with specialized models, thereby establishing a robust foundation for a versatile and dependable drug discovery assistant.", "year": 2023, "publicationdate": "2023-11-27", "externalids": {"DOI": "10.48550/arXiv.2311.16208"}, "doi_lower": "10.48550/arxiv.2311.16208"}
{"paper_id": 268724307, "title": "Generative Multi-modal Models are Good Class-Incremental Learners", "author_names": ["Xusheng Cao", "Haori Lu", "Linlan Huang", "Xialei Liu", "Ming-Ming Cheng"], "venue": "Computer Vision and Pattern Recognition", "abstract": "In class-incremental learning (CIL) scenarios, the phe-nomenon of catastrophic forgetting caused by the classi-fier's bias towards the current task has long posed a signif-icant challenge. It is mainly caused by the characteristic of discriminative models. With the growing popularity of the generative multi-modal models, we would explore replacing discriminative models with generative ones for CIL How-ever, transitioning from discriminative to generative mod-els requires addressing two key challenges. The primary challenge lies in transferring the generated textual infor-mation into the classification of distinct categories. Ad-ditionally, it requires formulating the task of CIL within a generative framework. To this end, we propose a novel generative multi-modal model (GMM) framework for class-incremental learning. Our approach directly generates la-bels for images using an adapted generative model. After obtaining the detailed text, we use a text encoder to ex-tract text features and employ feature matching to deter-mine the most similar label as the classification prediction. In the conventional CIL settings, we achieve signifi-cantly better results in long-sequence task scenarios. Un-der the Few-shot CIL setting, we have improved by at least 14% accuracy over all the current state-of-the-art methods with significantly less forgetting. Our code is available at https://github.com/DoubleClass/GMM.", "year": 2024, "publicationdate": "2024-03-27", "externalids": {"DOI": "10.1109/CVPR52733.2024.02712"}, "doi_lower": "10.1109/cvpr52733.2024.02712"}
{"paper_id": 213849817, "title": "Mining the Harvard Caselaw Access Project", "author_names": ["Felix B. Chang", "Erin E. McCabe", "Zhaowei Ren", "Joshua Beckelhimer", "J. Lee"], "venue": "", "abstract": null, "year": 2020, "publicationdate": "2020-01-31", "externalids": {"DOI": "10.2139/ssrn.3529257"}, "doi_lower": "10.2139/ssrn.3529257"}
{"paper_id": 247447641, "title": "FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing", "author_names": ["Ilias Chalkidis", "Tommaso Pasini", "Shenmin Zhang", "Letizia Tomada", "Sebastian Felix Schwemer", "Anders Søgaard"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present a benchmark suite of four datasets for evaluating the fairness of pre-trained language models and the techniques used to fine-tune them for downstream tasks. Our benchmarks cover four jurisdictions (European Council, USA, Switzerland, and China), five languages (English, German, French, Italian and Chinese) and fairness across five attributes (gender, age, region, language, and legal area). In our experiments, we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases, while none of these techniques guarantee fairness, nor consistently mitigate group disparities. Furthermore, we provide a quantitative and qualitative analysis of our results, highlighting open challenges in the development of robustness methods in legal NLP.", "year": 2022, "publicationdate": "2022-03-14", "externalids": {"DOI": "10.48550/arXiv.2203.07228"}, "doi_lower": "10.48550/arxiv.2203.07228"}
{"paper_id": 54443381, "title": "Efficient Lifelong Learning with A-GEM", "author_names": ["Arslan Chaudhry", "Marc'Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny"], "venue": "International Conference on Learning Representations", "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.", "year": 2018, "publicationdate": "2018-09-27", "externalids": {}, "doi_lower": null}
{"paper_id": 14136307, "title": "One billion word benchmark for measuring progress in statistical language modeling", "author_names": ["Ciprian Chelba", "Tomas Mikolov", "M. Schuster", "Qi Ge", "T. Brants", "P. Koehn", "T. Robinson"], "venue": "Interspeech", "abstract": "We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned KneserNey 5-gram model achieves perplexity 67.6. A combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. The benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.", "year": 2013, "publicationdate": "2013-12-10", "externalids": {"DOI": "10.21437/Interspeech.2014-564"}, "doi_lower": "10.21437/interspeech.2014-564"}
{"paper_id": 268379670, "title": "CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model", "author_names": ["Cheng Chen", "Junchen Zhu", "Xu Luo", "Hengtao Shen", "Lianli Gao", "Jingkuan Song"], "venue": "Neural Information Processing Systems", "abstract": "Instruction tuning represents a prevalent strategy employed by Multimodal Large Language Models (MLLMs) to align with human instructions and adapt to new tasks. Nevertheless, MLLMs encounter the challenge of adapting to users' evolving knowledge and demands. Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated. In this paper, we present a comprehensive benchmark, namely Continual Instruction tuNing (CoIN), to assess existing MLLMs in the sequential instruction tuning paradigm. CoIN comprises 10 commonly used datasets spanning 8 task categories, ensuring a diverse range of instructions and tasks. Besides, the trained model is evaluated from two aspects: Instruction Following and General Knowledge, which assess the alignment with human intention and knowledge preserved for reasoning, respectively. Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and the failure in intention alignment assumes the main responsibility, instead of the knowledge forgetting. To this end, we introduce MoELoRA to MLLMs which is effective to retain the previous instruction alignment. Experimental results consistently illustrate the forgetting decreased from this method on CoIN.", "year": 2024, "publicationdate": "2024-03-13", "externalids": {"DOI": "10.48550/arXiv.2403.08350"}, "doi_lower": "10.48550/arxiv.2403.08350"}
{"paper_id": 265221365, "title": "HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs", "author_names": ["Junying Chen", "Xidong Wang", "Anningzhe Gao", "Feng Jiang", "Shunian Chen", "Hongbo Zhang", "Dingjie Song", "Wenya Xie", "Chuyi Kong", "Jianquan Li", "Xiang Wan", "Haizhou Li", "Benyou Wang"], "venue": "arXiv.org", "abstract": "Adapting a language model into a specific domain, a.k.a `domain adaption', is a common practice when specialized knowledge, e.g. medicine, is not encapsulated in a general language model like Llama2. The challenge lies in the heterogeneity of data across the two training stages, as it varies in languages, genres, or formats. To tackle this and simplify the learning protocol, we propose to transform heterogeneous data, from the both pre-training and supervised stages, into a unified, simple input-output pair format. We validate the new protocol in the domains where proprietary LLMs like ChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The developed model, HuatuoGPT-II, has shown state-of-the-art performance in Chinese medicine domain on a number of benchmarks, e.g. medical licensing exams. It even outperforms proprietary models like ChatGPT and GPT-4 in some aspects, especially in Traditional Chinese Medicine. Expert manual evaluations further validate HuatuoGPT-II's advantages over existing LLMs. Notably, HuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing Examination where it achieved the best performance, showcasing not only its effectiveness but also its generalization capabilities.", "year": 2023, "publicationdate": "2023-11-16", "externalids": {"DOI": "10.48550/arXiv.2311.09774"}, "doi_lower": "10.48550/arxiv.2311.09774"}
{"paper_id": 216553067, "title": "Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting", "author_names": ["Sanyuan Chen", "Yutai Hou", "Yiming Cui", "Wanxiang Che", "Ting Liu", "Xiangzhan Yu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we propose a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.", "year": 2020, "publicationdate": "2020-04-27", "externalids": {"DOI": "10.18653/v1/2020.emnlp-main.634"}, "doi_lower": "10.18653/v1/2020.emnlp-main.634"}
{"paper_id": 258833488, "title": "Lifelong Language Pretraining with Distribution-Specialized Experts", "author_names": ["Wuyang Chen", "Yan-Quan Zhou", "Nan Du", "Yanping Huang", "J. Laudon", "Z. Chen", "Claire Cu"], "venue": "International Conference on Machine Learning", "abstract": "Pretraining on a large-scale corpus has become a standard method to build general language models (LMs). Adapting a model to new data distributions targeting different downstream tasks poses significant challenges. Naive fine-tuning may incur catastrophic forgetting when the over-parameterized LMs overfit the new data but fail to preserve the pretrained features. Lifelong learning (LLL) aims to enable information systems to learn from a continuous data stream across time. However, most prior work modifies the training recipe assuming a static fixed network architecture. We find that additional model capacity and proper regularization are key elements to achieving strong LLL performance. Thus, we propose Lifelong-MoE, an extensible MoE (Mixture-of-Experts) architecture that dynamically adds model capacity via adding experts with regularized pretraining. Our results show that by only introducing a limited number of extra experts while keeping the computation cost constant, our model can steadily adapt to data distribution shifts while preserving the previous knowledge. Compared to existing lifelong learning approaches, Lifelong-MoE achieves better few-shot performance on 19 downstream NLP tasks.", "year": 2023, "publicationdate": "2023-05-20", "externalids": {"DOI": "10.48550/arXiv.2305.12281"}, "doi_lower": "10.48550/arxiv.2305.12281"}
{"paper_id": 267782519, "title": "Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization", "author_names": ["Xuxi Chen", "Zhendong Wang", "Daouda Sow", "Junjie Yang", "Tianlong Chen", "Yingbin Liang", "Mingyuan Zhou", "Zhangyang Wang"], "venue": "arXiv.org", "abstract": "In the rapidly advancing arena of large language models (LLMs), a key challenge is to enhance their capabilities amid a looming shortage of high-quality training data. Our study starts from an empirical strategy for the light continual training of LLMs using their original pre-training data sets, with a specific focus on selective retention of samples that incur moderately high losses. These samples are deemed informative and beneficial for model refinement, contrasting with the highest-loss samples, which would be discarded due to their correlation with data noise and complexity. We then formalize this strategy into a principled framework of Instance-Reweighted Distributionally Robust Optimization (IR-DRO). IR-DRO is designed to dynamically prioritize the training focus on informative samples through an instance reweighting mechanism, streamlined by a closed-form solution for straightforward integration into established training protocols. Through rigorous experimentation with various models and datasets, our findings indicate that our sample-targeted methods significantly improve LLM performance across multiple benchmarks, in both continual pre-training and instruction tuning scenarios. Our codes are available at https://github.com/VITA-Group/HardFocusTraining.", "year": 2024, "publicationdate": "2024-02-22", "externalids": {"DOI": "10.48550/arXiv.2402.14270"}, "doi_lower": "10.48550/arxiv.2402.14270"}
{"paper_id": 263829616, "title": "Parameterizing Context: Unleashing the Power of Parameter-Efficient Fine-Tuning and In-Context Tuning for Continual Table Semantic Parsing", "author_names": ["Yongrui Chen", "Shenyu Zhang", "Guilin Qi", "Xinnan Guo"], "venue": "Neural Information Processing Systems", "abstract": "Continual table semantic parsing aims to train a parser on a sequence of tasks, where each task requires the parser to translate natural language into SQL based on task-specific tables but only offers limited training examples. Conventional methods tend to suffer from overfitting with limited supervision, as well as catastrophic forgetting due to parameter updates. Despite recent advancements that partially alleviate these issues through semi-supervised data augmentation and retention of a few past examples, the performance is still limited by the volume of unsupervised data and stored examples. To overcome these challenges, this paper introduces a novel method integrating \\textit{parameter-efficient fine-tuning} (PEFT) and \\textit{in-context tuning} (ICT) for training a continual table semantic parser. Initially, we present a task-adaptive PEFT framework capable of fully circumventing catastrophic forgetting, which is achieved by freezing the pre-trained model backbone and fine-tuning small-scale prompts. Building on this, we propose a teacher-student framework-based solution. The teacher addresses the few-shot problem using ICT, which procures contextual information by demonstrating a few training examples. In turn, the student leverages the proposed PEFT framework to learn from the teacher's output distribution, and subsequently compresses and saves the contextual information to the prompts, eliminating the need to store any training examples. Experimental evaluations on two benchmarks affirm the superiority of our method over prevalent few-shot and continual learning baselines across various metrics.", "year": 2023, "publicationdate": "2023-10-07", "externalids": {"DOI": "10.48550/arXiv.2310.04801"}, "doi_lower": "10.48550/arxiv.2310.04801"}
{"paper_id": 264145480, "title": "Lifelong Machine Learning", "author_names": ["Zhiyuan Chen", "Bing Liu"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning", "abstract": null, "year": 2016, "publicationdate": "2016-11-07", "externalids": {"DOI": "10.1007/978-3-031-01575-5"}, "doi_lower": "10.1007/978-3-031-01575-5"}
{"paper_id": 271745635, "title": "Adapting Large Language Models via Reading Comprehension", "author_names": ["Daixuan Cheng", "Shaohan Huang", "Furu Wei"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2309.09530"}, "doi_lower": "10.48550/arxiv.2309.09530"}
{"paper_id": 247951931, "title": "PaLM: Scaling Language Modeling with Pathways", "author_names": ["A. Chowdhery", "Sharan Narang", "Jacob Devlin", "Maarten Bosma", "Gaurav Mishra", "Adam Roberts", "P. Barham", "Hyung Won Chung", "Charles Sutton", "Sebastian Gehrmann", "Parker Schuh", "Kensen Shi", "Sasha Tsvyashchenko", "Joshua Maynez", "Abhishek Rao", "Parker Barnes", "Yi Tay", "Noam Shazeer", "Vinodkumar Prabhakaran", "Emily Reif", "Nan Du", "Ben Hutchinson", "Reiner Pope", "James Bradbury", "Jacob Austin", "M. Isard", "Guy Gur-Ari", "Pengcheng Yin", "Toju Duke", "Anselm Levskaya", "S. Ghemawat", "Sunipa Dev", "H. Michalewski", "Xavier García", "Vedant Misra", "Kevin Robinson", "L. Fedus", "Denny Zhou", "Daphne Ippolito", "D. Luan", "Hyeontaek Lim", "Barret Zoph", "A. Spiridonov", "Ryan Sepassi", "David Dohan", "Shivani Agrawal", "Mark Omernick", "Andrew M. Dai", "Thanumalayan Sankaranarayana Pillai", "Marie Pellat", "Aitor Lewkowycz", "Erica Moreira", "R. Child", "Oleksandr Polozov", "Katherine Lee", "Zongwei Zhou", "Xuezhi Wang", "Brennan Saeta", "Mark Díaz", "Orhan Firat", "Michele Catasta", "Jason Wei", "K. Meier-Hellstern", "D. Eck", "J. Dean", "Slav Petrov", "Noah Fiedel"], "venue": "Journal of machine learning research", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "year": 2022, "publicationdate": "2022-04-05", "externalids": {}, "doi_lower": null}
{"paper_id": 3922816, "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge", "author_names": ["Peter Clark", "Isaac Cowhey", "Oren Etzioni", "Tushar Khot", "Ashish Sabharwal", "Carissa Schoenick", "Oyvind Tafjord"], "venue": "arXiv.org", "abstract": "We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering. Together, these constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the SQuAD and SNLI tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the ARC Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose ARC as a challenge to the community.", "year": 2018, "publicationdate": "2018-03-14", "externalids": {}, "doi_lower": null}
{"paper_id": 282854460, "title": "RedPajama: an Open Dataset for Training Large Language Models", "author_names": ["Shane Adams", "Virginia Adams", "Anton Alexandrov", "Quentin Anthony", "Ben Athiwaratkun", "Rahul Chalamala", "Kezhen Chen", "Tri Dao", "Daniel Fu", "Percy Liang", "Xiaozhong Lyu", "Huu Nguyen", "Yonatan Oren", "Christopher Ré", "Irina Rish", "Max Ryabinin", "Maurice Weber", "Xiaozhe Yao", "Ce Zhang"], "venue": "Advances in Neural Information Processing Systems 37", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.52202/079017-3697"}, "doi_lower": "10.52202/079017-3697"}
{"paper_id": 248887419, "title": "Continual Pre-Training Mitigates Forgetting in Language and Vision", "author_names": ["Andrea Cossu", "T. Tuytelaars", "Antonio Carta", "Lucia C. Passaro", "Vincenzo Lomonaco", "D. Bacciu"], "venue": "Neural Networks", "abstract": "Pre-trained models are commonly used in Continual Learning to initialize the model before training on the stream of non-stationary data. However, pre-training is rarely applied during Continual Learning. We investigate the characteristics of the Continual Pre-Training scenario, where a model is continually pre-trained on a stream of incoming data and only later fine-tuned to different downstream tasks. We introduce an evaluation protocol for Continual Pre-Training which monitors forgetting against a Forgetting Control dataset not present in the continual stream. We disentangle the impact on forgetting of 3 main factors: the input modality (NLP, Vision), the architecture type (Transformer, ResNet) and the pre-training protocol (supervised, self-supervised). Moreover, we propose a Sample-Efficient Pre-training method (SEP) that speeds up the pre-training phase. We show that the pre-training protocol is the most important factor accounting for forgetting. Surprisingly, we discovered that self-supervised continual pre-training in both NLP and Vision is sufficient to mitigate forgetting without the use of any Continual Learning strategy. Other factors, like model depth, input modality and architecture type are not as crucial.", "year": 2022, "publicationdate": "2022-05-19", "externalids": {"DOI": "10.48550/arXiv.2205.09357"}, "doi_lower": "10.48550/arxiv.2205.09357"}
{"paper_id": 268532114, "title": "Larimar: Large Language Models with Episodic Memory Control", "author_names": ["Payel Das", "Subhajit Chaudhury", "Elliot Nelson", "Igor Melnyk", "Sarath Swaminathan", "Sihui Dai", "Aurélie C. Lozano", "Georgios Kollias", "Vijil Chenthamarakshan", "Jirí Navrátil", "Soham Dan", "Pin-Yu Chen"], "venue": "International Conference on Machine Learning", "abstract": "Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 8-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting, information leakage prevention, and input context length generalization with Larimar and show their effectiveness. Our code is available at https://github.com/IBM/larimar", "year": 2024, "publicationdate": "2024-03-18", "externalids": {"DOI": "10.48550/arXiv.2403.11901"}, "doi_lower": "10.48550/arxiv.2403.11901"}
{"paper_id": 201058596, "title": "Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning", "author_names": ["Pradeep Dasigi", "Nelson F. Liu", "Ana Marasović", "Noah A. Smith", "Matt Gardner"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Machine comprehension of texts longer than a single sentence often requires coreference resolution. However, most current reading comprehension benchmarks do not contain complex coreferential phenomena and hence fail to evaluate the ability of models to resolve coreference. We present a new crowdsourced dataset containing more than 24K span-selection questions that require resolving coreference among entities in over 4.7K English paragraphs from Wikipedia. Obtaining questions focused on such phenomena is challenging, because it is hard to avoid lexical cues that shortcut complex reasoning. We deal with this issue by using a strong baseline model as an adversary in the crowdsourcing loop, which helps crowdworkers avoid writing questions with exploitable surface cues. We show that state-of-the-art reading comprehension models perform significantly worse than humans on this benchmark—the best model performance is 70.5 F1, while the estimated human performance is 93.4 F1.", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/D19-1606"}, "doi_lower": "10.18653/v1/d19-1606"}
{"paper_id": 233289412, "title": "Editing Factual Knowledge in Language Models", "author_names": ["Nicola De Cao", "Wilker Aziz", "Ivan Titov"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix ‘bugs’ or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor’s efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a ‘probe’ revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor", "year": 2021, "publicationdate": "2021-04-16", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.522"}, "doi_lower": "10.18653/v1/2021.emnlp-main.522"}
{"paper_id": 259108887, "title": "K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization", "author_names": ["Cheng Deng", "Tianhang Zhang", "Zhongmou He", "Yi Xu", "Qiyuan Chen", "Yuanyuan Shi", "Le Zhou", "Luoyi Fu", "Weinan Zhang", "Xinbing Wang", "Cheng Zhou", "Zhouhan Lin", "Junxian He"], "venue": "Web Search and Data Mining", "abstract": "Large language models (LLMs) have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBench, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pre-trained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on 5.5B tokens of geoscience text corpus, including over 1 million pieces of geoscience literature, and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can efficiently gather domain-specific data and construct domain-supervised data, even in situations where manpower is scarce. Meanwhile, we equip K2 with the abilities of using tools to be a naive geoscience aide. Experiments conducted on the GeoBench demonstrate the effectiveness of our approach and datasets on geoscience knowledge understanding and utilization.We open-source all the training data and K2 model checkpoints at https://github.com/davendw49/k2", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.1145/3616855.3635772"}, "doi_lower": "10.1145/3616855.3635772"}
{"paper_id": 57246310, "title": "ImageNet: A large-scale hierarchical image database", "author_names": ["Jia Deng", "Wei Dong", "R. Socher", "Li-Jia Li", "K. Li", "Li Fei-Fei"], "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition", "abstract": null, "year": 2009, "publicationdate": "2009-06-20", "externalids": {"DOI": "10.1109/CVPR.2009.5206848"}, "doi_lower": "10.1109/cvpr.2009.5206848"}
{"paper_id": 258841328, "title": "QLoRA: Efficient Finetuning of Quantized LLMs", "author_names": ["Tim Dettmers", "Artidoro Pagnoni", "Ari Holtzman", "Luke Zettlemoyer"], "venue": "Neural Information Processing Systems", "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.", "year": 2023, "publicationdate": "2023-05-23", "externalids": {"DOI": "10.48550/arXiv.2305.14314"}, "doi_lower": "10.48550/arxiv.2305.14314"}
{"paper_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "author_names": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "year": 2019, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/N19-1423"}, "doi_lower": "10.18653/v1/n19-1423"}
{"paper_id": 252762125, "title": "Calibrating Factual Knowledge in Pretrained Language Models", "author_names": ["Qingxiu Dong", "Damai Dai", "Yifan Song", "Jingjing Xu", "Zhifang Sui", "Lei Li"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Previous literature has proved that Pretrained Language Models (PLMs) can store factual knowledge. However, we find that facts stored in the PLMs are not always correct. It motivates us to explore a fundamental question: How do we calibrate factual knowledge in PLMs without re-training from scratch? In this work, we propose a simple and lightweight method CaliNet to achieve this goal. To be specific, we first detect whether PLMs can learn the right facts via a contrastive score between right and fake facts. If not, we then use a lightweight method to add and adapt new parameters to specific factual texts. Experiments on the knowledge probing task show the calibration effectiveness and efficiency. In addition, through closed-book question answering, we find that the calibrated PLM possesses knowledge generalization ability after fine-tuning. Beyond the calibration performance, we further investigate and visualize the knowledge calibration mechanism.", "year": 2022, "publicationdate": "2022-10-07", "externalids": {"DOI": "10.48550/arXiv.2210.03329"}, "doi_lower": "10.48550/arxiv.2210.03329"}
{"paper_id": 225039882, "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "author_names": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "M. Minderer", "G. Heigold", "S. Gelly", "Jakob Uszkoreit", "N. Houlsby"], "venue": "International Conference on Learning Representations", "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.", "year": 2020, "publicationdate": "2020-10-22", "externalids": {}, "doi_lower": null}
{"paper_id": 67855846, "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs", "author_names": ["Dheeru Dua", "Yizhong Wang", "Pradeep Dasigi", "Gabriel Stanovsky", "Sameer Singh", "Matt Gardner"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 38.4% F1 on our generalized accuracy metric, while expert human performance is 96%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51% F1.", "year": 2019, "publicationdate": "2019-03-01", "externalids": {"DOI": "10.18653/v1/N19-1246"}, "doi_lower": "10.18653/v1/n19-1246"}
{"paper_id": 174802369, "title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "author_names": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "venue": "International Conference on Learning Representations", "abstract": "Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms need an external representation and extra computation to measure the parameters' \\textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in networks. Uncertainty is a natural way to identify \\textit{what to remember} and \\textit{what to change} as we continually learn, and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning and retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.", "year": 2019, "publicationdate": "2019-06-06", "externalids": {}, "doi_lower": null}
{"paper_id": 4612975, "title": "T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples", "author_names": ["Hady ElSahar", "P. Vougiouklis", "Arslen Remaci", "C. Gravier", "Jonathon S. Hare", "F. Laforest", "E. Simperl"], "venue": "International Conference on Language Resources and Evaluation", "abstract": null, "year": 2018, "publicationdate": "2018-05-07", "externalids": {}, "doi_lower": null}
{"paper_id": 269449465, "title": "Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities", "author_names": ["Kazuki Fujii", "Taishi Nakamura", "Mengsay Loem", "Hiroki Iida", "Masanari Ohi", "Kakeru Hattori", "Hirai Shota", "Sakae Mizuki", "Rio Yokota", "Naoaki Okazaki"], "venue": "arXiv.org", "abstract": "Cross-lingual continual pre-training of large language models (LLMs) initially trained on English corpus allows us to leverage the vast amount of English language resources and reduce the pre-training cost. In this study, we constructed Swallow, an LLM with enhanced Japanese capability, by extending the vocabulary of Llama 2 to include Japanese characters and conducting continual pre-training on a large Japanese web corpus. Experimental results confirmed that the performance on Japanese tasks drastically improved through continual pre-training, and the performance monotonically increased with the amount of training data up to 100B tokens. Consequently, Swallow achieved superior performance compared to other LLMs that were trained from scratch in English and Japanese. An analysis of the effects of continual pre-training revealed that it was particularly effective for Japanese question answering tasks. Furthermore, to elucidate effective methodologies for cross-lingual continual pre-training from English to Japanese, we investigated the impact of vocabulary expansion and the effectiveness of incorporating parallel corpora. The results showed that the efficiency gained through vocabulary expansion had no negative impact on performance, except for the summarization task, and that the combined use of parallel corpora enhanced translation ability.", "year": 2024, "publicationdate": "2024-04-27", "externalids": {"DOI": "10.48550/arXiv.2404.17790"}, "doi_lower": "10.48550/arxiv.2404.17790"}
{"paper_id": 2871880, "title": "Domain-Adversarial Training of Neural Networks", "author_names": ["Yaroslav Ganin", "E. Ustinova", "Hana Ajakan", "Pascal Germain", "H. Larochelle", "François Laviolette", "M. Marchand", "V. Lempitsky"], "venue": "Journal of machine learning research", "abstract": "We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. \n \nThe approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. \n \nWe demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.", "year": 2015, "publicationdate": "2015-05-28", "externalids": {"DOI": "10.1007/978-3-319-58347-1_10"}, "doi_lower": "10.1007/978-3-319-58347-1_10"}
{"paper_id": 264487212, "title": "TiC-CLIP: Continual Training of CLIP Models", "author_names": ["Saurabh Garg", "Mehrdad Farajtabar", "Hadi Pouransari", "Raviteja Vemulapalli", "Sachin Mehta", "Oncel Tuzel", "Vaishaal Shankar", "Fartash Faghri"], "venue": "International Conference on Learning Representations", "abstract": "Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataComp, TiC-YFCC, and TiC-Redcaps. TiC-DataComp, our largest dataset, contains over 12.7B timestamped image-text pairs spanning 9 years (2014-2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\\approx 8\\%$ zero-shot accuracy on our curated retrieval task from 2021-2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the last checkpoint and replays old data reduces compute by $2.5\\times$ when compared to the standard practice of retraining from scratch. Code is available at https://github.com/apple/ml-tic-clip.", "year": 2023, "publicationdate": "2023-10-24", "externalids": {"DOI": "10.48550/arXiv.2310.16226"}, "doi_lower": "10.48550/arxiv.2310.16226"}
{"paper_id": 264935145, "title": "Continual Learning Under Language Shift", "author_names": ["Evangelia Gogoulou", "Timothée Lesort", "Magnus Boman", "Joakim Nivre"], "venue": "International Conference on Text, Speech and Dialogue", "abstract": "The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. We study the pros and cons of updating a language model when new data comes from new languages -- the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Danish, Icelandic, and Norwegian to investigate how forward and backward transfer effects depend on pre-training order and characteristics of languages, for three different model sizes. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be positive or negative depending on the order and characteristics of new languages. We explore a number of potentially explanatory factors and find that a combination of language contamination and syntactic similarity best fits our results.", "year": 2023, "publicationdate": "2023-11-02", "externalids": {}, "doi_lower": null}
{"paper_id": 244983614, "title": "Wikitext-103 and OpenWebText Models", "author_names": ["Forrest Davis"], "venue": "", "abstract": null, "year": 2020, "publicationdate": "2020-09-27", "externalids": {"DOI": "10.5281/ZENODO.4053572"}, "doi_lower": "10.5281/zenodo.4053572"}
{"paper_id": 220919723, "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing", "author_names": ["Yu Gu", "Robert Tinn", "Hao Cheng", "Michael R. Lucas", "N. Usuyama", "Xiaodong Liu", "Tristan Naumann", "Jianfeng Gao", "Hoifung Poon"], "venue": "ACM Trans. Comput. Heal.", "abstract": "Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding & Reasoning Benchmark) at https://aka.ms/BLURB.", "year": 2020, "publicationdate": "2020-07-31", "externalids": {"DOI": "10.1145/3458754"}, "doi_lower": "10.1145/3458754"}
{"paper_id": 264832958, "title": "Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering", "author_names": ["Zhen Guo", "Yining Hua"], "venue": "arXiv.org", "abstract": "Large language models exhibit promising general capabilities but often lack specialized knowledge for domain-specific tasks. Developing domain experts from a base model enables a range of applications without prohibitive training costs. This work demonstrates a method using continuous training and instruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese medical domain. We first conduct continuous training on 1B tokens from Chinese medical references to teach relevant vocabulary and knowledge. The models are then fine-tuned on 54K examples sourced from the Chinese National Medical Licensing Examination. Experiments on Chinese medical data confirm the effectiveness of this approach, producing a model comparable to GPT-3.5-turbo while using way less computational resource. The resulting domain-specific model could be useful for various Chinese medical applications. More broadly, this provides a template for domain-specific training of large language models in areas where pre-trained models lack the required expertise, such as law, science, and engineering.", "year": 2023, "publicationdate": "2023-11-01", "externalids": {"DOI": "10.48550/arXiv.2311.00204"}, "doi_lower": "10.48550/arxiv.2311.00204"}
{"paper_id": 260704601, "title": "Continual Pre-Training of Large Language Models: How to (re)warm your model?", "author_names": ["Kshitij Gupta", "Benjamin Th'erien", "Adam Ibrahim", "Mats L. Richter", "Quentin Anthony", "Eugene Belilovsky", "I. Rish", "Timothée Lesort"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and evaluate performance through validation perplexity. We experiment with different pre-training checkpoints, various maximum learning rates, and various warmup lengths. Our results show that while rewarming models first increases the loss on upstream and downstream data, in the longer run it improves the downstream performance, outperforming models trained from scratch$\\unicode{x2013}$even for a large downstream dataset.", "year": 2023, "publicationdate": "2023-08-08", "externalids": {"DOI": "10.48550/arXiv.2308.04014"}, "doi_lower": "10.48550/arxiv.2308.04014"}
{"paper_id": 3831582, "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People", "author_names": ["D. Gurari", "Qing Li", "Abigale Stangl", "Anhong Guo", "Chi Lin", "K. Grauman", "Jiebo Luo", "Jeffrey P. Bigham"], "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "abstract": "The study of algorithms to automatically answer visual questions currently is motivated by visual question answering (VQA) datasets constructed in artificial VQA settings. We propose VizWiz, the first goal-oriented VQA dataset arising from a natural VQA setting. VizWiz consists of over 31,000 visual questions originating from blind people who each took a picture using a mobile phone and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. VizWiz differs from the many existing VQA datasets because (1) images are captured by blind photographers and so are often poor quality, (2) questions are spoken and so are more conversational, and (3) often visual questions cannot be answered. Evaluation of modern algorithms for answering visual questions and deciding if a visual question is answerable reveals that VizWiz is a challenging dataset. We introduce this dataset to encourage a larger community to develop more generalized algorithms that can assist blind people.", "year": 2018, "publicationdate": "2018-02-22", "externalids": {"DOI": "10.1109/CVPR.2018.00380"}, "doi_lower": "10.1109/cvpr.2018.00380"}
{"paper_id": 236976189, "title": "DEMix Layers: Disentangling Domains for Modular Language Modeling", "author_names": ["Suchin Gururangan", "Michael Lewis", "Ari Holtzman", "Noah A. Smith", "Luke Zettlemoyer"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer includes a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added, or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity (especially for out-of-domain data), increase training efficiency, and enable rapid adaptation. Mixing experts during inference, using a parameter-free weighted ensemble, enables better generalization to heterogeneous or unseen domains. We also show it is possible to add experts to adapt to new domains without forgetting older ones, and remove experts to restrict access to unwanted domains. Overall, these results demonstrate benefits of domain modularity in language models.", "year": 2021, "publicationdate": "2021-08-11", "externalids": {"DOI": "10.18653/v1/2022.naacl-main.407"}, "doi_lower": "10.18653/v1/2022.naacl-main.407"}
{"paper_id": 216080466, "title": "Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks", "author_names": ["Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.", "year": 2020, "publicationdate": "2020-04-23", "externalids": {"DOI": "10.18653/v1/2020.acl-main.740"}, "doi_lower": "10.18653/v1/2020.acl-main.740"}
{"paper_id": 253735429, "title": "Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors", "author_names": ["Thomas Hartvigsen", "S. Sankaranarayanan", "Hamid Palangi", "Yoon Kim", "M. Ghassemi"], "venue": "Neural Information Processing Systems", "abstract": "Deployed language models decay over time due to shifting inputs, changing user needs, or emergent world-knowledge gaps. When such problems are identified, we want to make targeted edits while avoiding expensive retraining. However, current model editors, which modify such behaviors of pre-trained models, degrade model performance quickly across multiple, sequential edits. We propose GRACE, a lifelong model editing method, which implements spot-fixes on streaming errors of a deployed model, ensuring minimal impact on unrelated inputs. GRACE writes new mappings into a pre-trained model's latent space, creating a discrete, local codebook of edits without altering model weights. This is the first method enabling thousands of sequential edits using only streaming errors. Our experiments on T5, BERT, and GPT models show GRACE's state-of-the-art performance in making and retaining edits, while generalizing to unseen inputs. Our code is available at https://www.github.com/thartvigsen/grace}.", "year": 2022, "publicationdate": "2022-11-20", "externalids": {"DOI": "10.48550/arXiv.2211.11031"}, "doi_lower": "10.48550/arxiv.2211.11031"}
{"paper_id": 255595518, "title": "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models", "author_names": ["Peter Hase", "Mohit Bansal", "Been Kim", "Asma Ghandeharioun"], "venue": "Neural Information Processing Systems", "abstract": "Language models learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights. In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific model parameters would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit. Next, we consider several variants of the editing problem, including erasing and amplifying facts. For one of our editing problems, editing performance does relate to localization results from representation denoising, but we find that which layer we edit is a far better predictor of performance. Our results suggest, counterintuitively, that better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior. Our code is available at https://github.com/google/belief-localization", "year": 2023, "publicationdate": "2023-01-10", "externalids": {"DOI": "10.48550/arXiv.2301.04213"}, "doi_lower": "10.48550/arxiv.2301.04213"}
{"paper_id": 244709666, "title": "Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs", "author_names": ["Peter Hase", "Mona T. Diab", "Asli Celikyilmaz", "Xian Li", "Zornitsa Kozareva", "Veselin Stoyanov", "Mohit Bansal", "Srini Iyer"], "venue": "arXiv.org", "abstract": "Do language models have beliefs about the world? Dennett (1995) famously argues that even thermostats have beliefs, on the view that a belief is simply an informational state decoupled from any motivational state. In this paper, we discuss approaches to detecting when models have beliefs about the world, and we improve on methods for updating model beliefs to be more truthful, with a focus on methods based on learned optimizers or hypernetworks. Our main contributions include: (1) new metrics for evaluating belief-updating methods that focus on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing model updates (SLAG) that improves the performance of learned optimizers, and (3) the introduction of the belief graph, which is a new form of interface with language models that shows the interdependencies between model beliefs. Our experiments suggest that models possess belief-like qualities to only a limited extent, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work. Code is available at https://github.com/peterbhase/SLAG-Belief-Updating", "year": 2021, "publicationdate": "2021-11-26", "externalids": {}, "doi_lower": null}
{"paper_id": 265466199, "title": "Continual Instruction Tuning for Large Multimodal Models", "author_names": ["Jinghan He", "Haiyun Guo", "Ming Tang", "Jinqiao Wang"], "venue": "arXiv.org", "abstract": "Instruction tuning is now a widely adopted approach to aligning large multimodal models (LMMs) to follow human intent. It unifies the data format of vision-language tasks, enabling multi-task joint training. However, vision-language tasks are constantly being created in practice. Instead of always re-training LMMs when new tasks arrive, continual learning offers flexibility for models to continually and efficiently exploit the evolving data. This work aims to explore the following two questions: 1) Do LMMs still suffer from catastrophic forgetting in continual instruction tuning? 2) Are the existing three classes of continual learning methods still applicable to the continual instruction tuning of LMMs? An extensive study is conducted to address the above questions. First, we establish the first benchmark in this setting and reveal that catastrophic forgetting is still observed when continually instruction-tuning LMMs. However, the multi-task joint instruction tuning can facilitate the model's continual learning ability and mitigate forgetting. Second, we integrate and adapt classic continual learning methods to our context, demonstrating the efficacy of data replay and model expansion strategies across diverse scenarios. In contrast, regularization-based methods only perform well on models that have been jointly instruction-tuned on multiple tasks. Third, we delve into the correlation and forgetting dynamics between vision-language task pairs and propose task-similarity-informed regularization and model expansion methods for continual instruction tuning of LMMs. Experimental results show that our approach consistently boosts the model's performance.", "year": 2023, "publicationdate": "2023-11-27", "externalids": {"DOI": "10.48550/arXiv.2311.16206"}, "doi_lower": "10.48550/arxiv.2311.16206"}
{"paper_id": 233189563, "title": "Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models", "author_names": ["Myle Ott"], "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "abstract": "In this work, we study how the finetuning stage in the pretrain-finetune framework changes the behavior of a pretrained neural language generator. We focus on the transformer encoder-decoder model for the open-domain dialogue response generation task. Our major finding is that after standard finetuning, the model forgets some of the important language generation skills acquired during large-scale pretraining. We demonstrate the forgetting phenomenon through a set of detailed behavior analysis from the perspectives of knowledge transfer, context sensitivity, and function space projection. As a preliminary attempt to alleviate the forgetting problem, we propose an intuitive finetuning strategy named “mix-review”. We find that mix-review effectively regularizes the finetuning process, and the forgetting problem is alleviated to some extent. Finally, we discuss interesting behavior of the resulting dialogue model and its implications.", "year": 2021, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2021.eacl-main.95"}, "doi_lower": "10.18653/v1/2021.eacl-main.95"}
{"paper_id": 268510177, "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning", "author_names": ["Yongquan He", "Xuancheng Huang", "Minghao Tang", "Lingxun Meng", "Xiang Li", "Wei Lin", "Wenyuan Zhang", "Yifu Gao"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score, to measure the generalization and instruction-following abilities of LLMs. Experiments demonstrate our method achieves superior performance on both seen and held-out tasks.", "year": 2024, "publicationdate": "2024-03-15", "externalids": {"DOI": "10.48550/arXiv.2403.10056"}, "doi_lower": "10.48550/arxiv.2403.10056"}
{"paper_id": 220968818, "title": "Aligning AI With Shared Human Values", "author_names": ["Dan Hendrycks", "Collin Burns", "Steven Basart", "Andrew Critch", "J. Li", "D. Song", "J. Steinhardt"], "venue": "International Conference on Learning Representations", "abstract": "We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete understanding of basic ethical knowledge. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.", "year": 2020, "publicationdate": "2020-08-05", "externalids": {}, "doi_lower": null}
{"paper_id": 15870937, "title": "WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia", "author_names": ["Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "I. Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "We present WIKIREADING, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNNbased architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%.", "year": 2016, "publicationdate": "2016-08-01", "externalids": {"DOI": "10.18653/v1/P16-1145"}, "doi_lower": "10.18653/v1/p16-1145"}
{"paper_id": 247778764, "title": "Training Compute-Optimal Large Language Models", "author_names": ["Jordan Hoffmann", "Sebastian Borgeaud", "A. Mensch", "Elena Buchatskaya", "Trevor Cai", "Eliza Rutherford", "Diego de Las Casas", "Lisa Anne Hendricks", "Johannes Welbl", "Aidan Clark", "Tom Hennigan", "Eric Noland", "Katie Millican", "George van den Driessche", "Bogdan Damoc", "Aurelia Guy", "Simon Osindero", "K. Simonyan", "Erich Elsen", "Jack W. Rae", "O. Vinyals", "L. Sifre"], "venue": "arXiv.org", "abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.", "year": 2022, "publicationdate": "2022-03-29", "externalids": {}, "doi_lower": null}
{"paper_id": 267751068, "title": "WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing", "author_names": ["Chenhui Hu", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named Wise-Layer Knowledge Editor (WilKE), which selects editing layer based on the pattern matching degree of editing knowledge across different layers in language models. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.", "year": 2024, "publicationdate": "2024-02-16", "externalids": {"DOI": "10.48550/arXiv.2402.10987"}, "doi_lower": "10.48550/arxiv.2402.10987"}
{"paper_id": 235458009, "title": "LoRA: Low-Rank Adaptation of Large Language Models", "author_names": ["J. E. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Weizhu Chen"], "venue": "International Conference on Learning Representations", "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.", "year": 2021, "publicationdate": "2021-06-17", "externalids": {}, "doi_lower": null}
{"paper_id": 235458009, "title": "LoRA: Low-Rank Adaptation of Large Language Models", "author_names": ["J. E. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Weizhu Chen"], "venue": "International Conference on Learning Representations", "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.", "year": 2021, "publicationdate": "2021-06-17", "externalids": {}, "doi_lower": null}
{"paper_id": 268230393, "title": "Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal", "author_names": ["Jianheng Huang", "Leyang Cui", "Ante Wang", "Chengyi Yang", "Xinting Liao", "Linfeng Song", "Junfeng Yao", "Jinsong Su"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains.", "year": 2024, "publicationdate": "2024-03-02", "externalids": {"DOI": "10.48550/arXiv.2403.01244"}, "doi_lower": "10.48550/arxiv.2403.01244"}
{"paper_id": 202540590, "title": "Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning", "author_names": ["Lifu Huang", "Ronan Le Bras", "Chandra Bhagavatula", "Yejin Choi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Understanding narratives requires reading between the lines, which in turn, requires interpreting the likely causes and effects of events, even when they are not mentioned explicitly. In this paper, we introduce Cosmos QA, a large-scale dataset of 35,600 problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. In stark contrast to most existing reading comprehension datasets where the questions focus on factual and literal understanding of the context paragraph, our dataset focuses on reading between the lines over a diverse collection of people’s everyday narratives, asking such questions as “what might be the possible reason of ...?\", or “what would have happened if ...\" that require reasoning beyond the exact text spans in the context. To establish baseline performances on Cosmos QA, we experiment with several state-of-the-art neural architectures for reading comprehension, and also propose a new architecture that improves over the competitive baselines. Experimental results demonstrate a significant gap between machine (68.4%) and human performance (94%), pointing to avenues for future research on commonsense machine comprehension. Dataset, code and leaderboard is publicly available at https://wilburone.github.io/cosmos.", "year": 2019, "publicationdate": "2019-08-31", "externalids": {"DOI": "10.18653/v1/D19-1243"}, "doi_lower": "10.18653/v1/d19-1243"}
{"paper_id": 258865862, "title": "Lawyer LLaMA Technical Report", "author_names": ["Quzhe Huang", "Mingxu Tao", "Zhenwei An", "Chen Zhang", "Cong Jiang", "Zhibin Chen", "Zirui Wu", "Yansong Feng"], "venue": "", "abstract": "Large Language Models (LLMs), like LLaMA, have exhibited remarkable performance across various tasks. Nevertheless, when deployed to specific domains such as law or medicine, the models still confront the challenge of a deficiency in domain-specific knowledge and an inadequate capability to leverage that knowledge to resolve domain-related problems. In this paper, we propose a new framework to adapt LLMs to specific domains and build Lawyer LLaMA, a legal domain LLM, based on this framework. Specifically, we inject domain knowledge during the continual training stage and teach the model to learn professional skills using properly designed supervised fine-tuning tasks. Moreover, to alleviate the hallucination problem during the model's generation, we add a retrieval module and extract relevant legal articles before the model answers any queries. When learning domain-specific skills, we find that experts' experience is much more useful than experiences distilled from ChatGPT, where hundreds of expert-written data outperform tens of thousands of ChatGPT-generated ones. We will release our model and data.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {}, "doi_lower": null}
{"paper_id": 152282269, "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering", "author_names": ["Drew A. Hudson", "Christopher D. Manning"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages Visual Genome scene graph structures to create 22M diverse reasoning questions, which all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate question biases. Accompanying the dataset is a suite of new metrics that evaluate essential qualities such as consistency, grounding and plausibility. A careful analysis is performed for baselines as well as state-of-the-art models, providing fine-grained results for different question types and topologies. Whereas a blind LSTM obtains a mere 42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%, offering ample opportunity for new research to explore. We hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding of vision and language.", "year": 2019, "publicationdate": "2019-02-25", "externalids": {"DOI": "10.1109/CVPR.2019.00686"}, "doi_lower": "10.1109/cvpr.2019.00686"}
{"paper_id": 248476156, "title": "TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models", "author_names": ["Joel Jang", "Seonghyeon Ye", "Changho Lee", "Sohee Yang", "Joongbo Shin", "Janghoon Han", "Gyeonghun Kim", "Minjoon Seo"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Language Models (LMs) become outdated as the world changes; they often fail to perform tasks requiring recent factual information which was absent or different during training, a phenomenon called temporal misalignment. This is especially a challenging problem because the research community still lacks a coherent dataset for assessing the adaptability of LMs to frequently-updated knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, respectively. The benchmark hence allows researchers to periodically track an LM’s ability to retain previous knowledge and acquire updated/new knowledge at each point in time. We also find that training an LM on the diff data through continual learning methods achieves similar or better perplexity than on the entire snapshot in our benchmark with 12 times less computational cost, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning.", "year": 2022, "publicationdate": "2022-04-29", "externalids": {"DOI": "10.48550/arXiv.2204.14211"}, "doi_lower": "10.48550/arxiv.2204.14211"}
{"paper_id": 238419458, "title": "Towards Continual Knowledge Learning of Language Models", "author_names": ["Joel Jang", "Seonghyeon Ye", "Sohee Yang", "Joongbo Shin", "Janghoon Han", "Gyeonghun Kim", "Stanley Jungkyu Choi", "Minjoon Seo"], "venue": "International Conference on Learning Representations", "abstract": "Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs. The benchmark datasets, evaluation script, and baseline code to reproduce our results are available at https://github.com/joeljang/continual-knowledge-learning.", "year": 2021, "publicationdate": "2021-10-07", "externalids": {}, "doi_lower": null}
{"paper_id": 267759882, "title": "Instruction-tuned Language Models are Better Knowledge Learners", "author_names": ["Zhengbao Jiang", "Zhiqing Sun", "Weijia Shi", "Pedro Rodriguez", "Chunting Zhou", "Graham Neubig", "Xi Victoria Lin", "Wen-tau Yih", "Srinivasan Iyer"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes on questions prior to training on documents. This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents. Extensive experiments and ablation studies demonstrate that pre-instruction-tuning significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%.", "year": 2024, "publicationdate": "2024-02-20", "externalids": {"DOI": "10.48550/arXiv.2402.12847"}, "doi_lower": "10.48550/arxiv.2402.12847"}
{"paper_id": 267411970, "title": "What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement", "author_names": ["Xisen Jin", "Xiang Ren"], "venue": "International Conference on Machine Learning", "abstract": "Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products of example representations achieves better forecasting performance over a series of setups. Finally, we show that we reduce forgetting of upstream pretraining examples by replaying examples that are forecasted to be forgotten, demonstrating the practical utility of forecasting example forgetting.", "year": 2024, "publicationdate": "2024-02-02", "externalids": {"DOI": "10.48550/arXiv.2402.01865"}, "doi_lower": "10.48550/arxiv.2402.01865"}
{"paper_id": 250390567, "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora", "author_names": ["Xisen Jin", "Dejiao Zhang", "Henghui Zhu", "Wei Xiao", "Shang-Wen Li", "Xiaokai Wei", "Andrew O. Arnold", "Xiang Ren"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Pretrained language models (PTLMs) are typically learned over a large, static corpus and further fine-tuned for various downstream tasks. However, when deployed in the real world, a PTLM-based model must deal with data distributions that deviates from what the PTLM was initially trained on. In this paper, we study a lifelong language model pretraining challenge where a PTLM is continually updated so as to adapt to emerging data. Over a domain-incremental research paper stream and a chronologically-ordered tweet stream, we incrementally pretrain a PTLM with different continual learning algorithms, and keep track of the downstream task performance (after fine-tuning). We evaluate PTLM’s ability to adapt to new corpora while retaining learned knowledge in earlier corpora. Our experiments show distillation-based approaches to be most effective in retaining downstream performance in earlier domains. The algorithms also improve knowledge transfer, allowing models to achieve better downstream performance over latest data, and improve temporal generalization when distribution gaps exist between training and evaluation because of time. We believe our problem formulation, methods, and analysis will inspire future studies towards continual pretraining of language models.", "year": 2022, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2022.bigscience-1.1"}, "doi_lower": "10.18653/v1/2022.bigscience-1.1"}
{"paper_id": 71295871, "title": "Principles of Neural Science", "author_names": ["V. Calabrese"], "venue": "", "abstract": null, "year": 1992, "publicationdate": "1992-12-16", "externalids": {"DOI": "10.1001/JAMA.1992.03490230111048"}, "doi_lower": "10.1001/jama.1992.03490230111048"}
{"paper_id": 210861095, "title": "Scaling Laws for Neural Language Models", "author_names": ["J. Kaplan", "Sam McCandlish", "T. Henighan", "Tom B. Brown", "Benjamin Chess", "R. Child", "Scott Gray", "Alec Radford", "Jeff Wu", "Dario Amodei"], "venue": "arXiv.org", "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.", "year": 2020, "publicationdate": "2020-01-23", "externalids": {}, "doi_lower": null}
{"paper_id": 252815848, "title": "Continual Training of Language Models for Few-Shot Learning", "author_names": ["Zixuan Ke", "Haowei Lin", "Yijia Shao", "Hu Xu", "Lei Shu", "Bin Liu"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications. Adapting or posttraining an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills. The goal is to improve the few-shot end-task learning in these domains. The resulting system is called CPT (Continual PostTraining), which to our knowledge, is the first continual post-training system. Experimental results verify its effectiveness.", "year": 2022, "publicationdate": "2022-10-11", "externalids": {"DOI": "10.48550/arXiv.2210.05549"}, "doi_lower": "10.48550/arxiv.2210.05549"}
{"paper_id": 253801772, "title": "Continual Learning of Natural Language Processing Tasks: A Survey", "author_names": ["Zixuan Ke", "Bin Liu"], "venue": "arXiv.org", "abstract": "Continual learning (CL) is a learning paradigm that emulates the human capability of learning and accumulating knowledge continually without forgetting the previously learned knowledge and also transferring the learned knowledge to help learn new tasks better. This survey presents a comprehensive review and analysis of the recent progress of CL in NLP, which has significant differences from CL in computer vision and machine learning. It covers (1) all CL settings with a taxonomy of existing techniques; (2) catastrophic forgetting (CF) prevention, (3) knowledge transfer (KT), which is particularly important for NLP tasks; and (4) some theory and the hidden challenge of inter-task class separation (ICS). (1), (3) and (4) have not been included in the existing survey. Finally, a list of future directions is discussed.", "year": 2022, "publicationdate": "2022-11-23", "externalids": {"DOI": "10.48550/arXiv.2211.12701"}, "doi_lower": "10.48550/arxiv.2211.12701"}
{"paper_id": 244908578, "title": "Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning", "author_names": ["Zixuan Ke", "Bing Liu", "Nianzu Ma", "Hu Xu", "Lei Shu"], "venue": "Neural Information Processing Systems", "abstract": "Continual learning (CL) learns a sequence of tasks incrementally with the goal of achieving two main objectives: overcoming catastrophic forgetting (CF) and encouraging knowledge transfer (KT) across tasks. However, most existing techniques focus only on overcoming CF and have no mechanism to encourage KT, and thus do not do well in KT. Although several papers have tried to deal with both CF and KT, our experiments show that they suffer from serious CF when the tasks do not have much shared knowledge. Another observation is that most current CL methods do not use pre-trained models, but it has been shown that such models can significantly improve the end task performance. For example, in natural language processing, fine-tuning a BERT-like pre-trained language model is one of the most effective approaches. However, for CL, this approach suffers from serious CF. An interesting question is how to make the best use of pre-trained models for CL. This paper proposes a novel model called CTR to solve these problems. Our experimental results demonstrate the effectiveness of CTR", "year": 2021, "publicationdate": "2021-12-05", "externalids": {}, "doi_lower": null}
{"paper_id": 258079422, "title": "Continual Pre-training of Language Models", "author_names": ["Zixuan Ke", "Yijia Shao", "Haowei Lin", "Tatsuya Konishi", "Gyuhak Kim", "Bin Liu"], "venue": "International Conference on Learning Representations", "abstract": "Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances. Empirical evaluation demonstrates the effectiveness of the proposed method.", "year": 2023, "publicationdate": "2023-02-07", "externalids": {}, "doi_lower": null}
{"paper_id": 5112038, "title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences", "author_names": ["Daniel Khashabi", "Snigdha Chaturvedi", "Michael Roth", "Shyam Upadhyay", "D. Roth"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 88.1%. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.", "year": 2018, "publicationdate": "2018-06-01", "externalids": {"DOI": "10.18653/v1/N18-1023"}, "doi_lower": "10.18653/v1/n18-1023"}
{"paper_id": 33698356, "title": "Learning What is Essential in Questions", "author_names": ["Daniel Khashabi", "Tushar Khot", "Ashish Sabharwal", "D. Roth"], "venue": "Conference on Computational Natural Language Learning", "abstract": "Question answering (QA) systems are easily distracted by irrelevant or redundant words in questions, especially when faced with long or multi-sentence questions in difficult domains. This paper introduces and studies the notion of essential question terms with the goal of improving such QA solvers. We illustrate the importance of essential question terms by showing that humans’ ability to answer questions drops significantly when essential terms are eliminated from questions.We then develop a classifier that reliably (90% mean average precision) identifies and ranks essential terms in questions. Finally, we use the classifier to demonstrate that the notion of question term essentiality allows state-of-the-art QA solver for elementary-level science questions to make better and more informed decisions,improving performance by up to 5%.We also introduce a new dataset of over 2,200 crowd-sourced essential terms annotated science questions.", "year": 2017, "publicationdate": "2017-08-01", "externalids": {"DOI": "10.18653/v1/K17-1010"}, "doi_lower": "10.18653/v1/k17-1010"}
{"paper_id": 204915921, "title": "QASC: A Dataset for Question Answering via Sentence Composition", "author_names": ["Tushar Khot", "Peter Clark", "Michal Guerquin", "Peter Alexander Jansen", "Ashish Sabharwal"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Composing knowledge from multiple pieces of texts is a key challenge in multi-hop question answering. We present a multi-hop reasoning dataset, Question Answering via Sentence Composition (QASC), that requires retrieving facts from a large corpus and composing them to answer a multiple-choice question. QASC is the first dataset to offer two desirable properties: (a) the facts to be composed are annotated in a large corpus, and (b) the decomposition into these facts is not evident from the question itself. The latter makes retrieval challenging as the system must introduce new concepts or relations in order to discover potential decompositions. Further, the reasoning model must then learn to identify valid compositions of these retrieved facts using common-sense reasoning. To help address these challenges, we provide annotation for supporting facts as well as their composition. Guided by these annotations, we present a two-step approach to mitigate the retrieval challenges. We use other multiple-choice datasets as additional training data to strengthen the reasoning model. Our proposed approach improves over current state-of-the-art language models by 11% (absolute). The reasoning and retrieval problems, however, remain unsolved as this model still lags by 20% behind human performance.", "year": 2019, "publicationdate": "2019-10-25", "externalids": {"DOI": "10.1609/AAAI.V34I05.6319"}, "doi_lower": "10.1609/aaai.v34i05.6319"}
{"paper_id": 4704285, "title": "Overcoming catastrophic forgetting in neural networks", "author_names": ["J. Kirkpatrick", "Razvan Pascanu", "Neil C. Rabinowitz", "J. Veness", "Guillaume Desjardins", "Andrei A. Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "A. Grabska-Barwinska", "D. Hassabis", "C. Clopath", "D. Kumaran", "R. Hadsell"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "abstract": "Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.", "year": 2016, "publicationdate": "2016-12-02", "externalids": {"DOI": "10.1073/pnas.1611835114"}, "doi_lower": "10.1073/pnas.1611835114"}
{"paper_id": 86611921, "title": "Natural Questions: A Benchmark for Question Answering Research", "author_names": ["T. Kwiatkowski", "J. Palomaki", "Olivia Redfield", "Michael Collins", "Ankur P. Parikh", "Chris Alberti", "D. Epstein", "I. Polosukhin", "Jacob Devlin", "Kenton Lee", "Kristina Toutanova", "Llion Jones", "Matthew Kelcey", "Ming-Wei Chang", "Andrew M. Dai", "Jakob Uszkoreit", "Quoc V. Le", "Slav Petrov"], "venue": "Transactions of the Association for Computational Linguistics", "abstract": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.", "year": 2019, "publicationdate": "2019-08-01", "externalids": {"DOI": "10.1162/tacl_a_00276"}, "doi_lower": "10.1162/tacl_a_00276"}
{"paper_id": 239886013, "title": "Mind the Gap: Assessing Temporal Generalization in Neural Language Models", "author_names": ["Angeliki Lazaridou", "A. Kuncoro", "E. Gribovskaya", "Devang Agrawal", "Adam Liska", "Tayfun Terzi", "Mai Giménez", "Cyprien de Masson d'Autume", "Tomás Kociský", "Sebastian Ruder", "Dani Yogatama", "Kris Cao", "Susannah Young", "Phil Blunsom"], "venue": "Neural Information Processing Systems", "abstract": "Our world is open-ended, non-stationary, and constantly evolving; thus what we talk about and how we talk about it change over time. This inherent dynamic nature of language contrasts with the current static language modelling paradigm, which trains and evaluates models on utterances from overlapping time periods. Despite impressive recent progress, we demonstrate that Transformer-XL language models perform worse in the realistic setup of predicting future utterances from beyond their training period, and that model performance becomes increasingly worse with time. We find that, while increasing model size alone -- a key driver behind recent progress -- does not solve this problem, having models that continually update their knowledge with new information can indeed mitigate this performance degradation over time. Hence, given the compilation of ever-larger language modelling datasets, combined with the growing list of language-model-based NLP applications that require up-to-date factual knowledge about the world, we argue that now is the right time to rethink the static way in which we currently train and evaluate our language models, and develop adaptive language models that can remain up-to-date with respect to our ever-changing and non-stationary world. We publicly release our dynamic, streaming language modelling benchmarks for WMT and arXiv to facilitate language model evaluation that takes temporal dynamics into account.", "year": 2021, "publicationdate": "2021-02-03", "externalids": {}, "doi_lower": null}
{"paper_id": 793385, "title": "Zero-Shot Relation Extraction via Reading Comprehension", "author_names": ["Omer Levy", "Minjoon Seo", "Eunsol Choi", "Luke Zettlemoyer"], "venue": "Conference on Computational Natural Language Learning", "abstract": "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.", "year": 2017, "publicationdate": "2017-06-13", "externalids": {"DOI": "10.18653/v1/K17-1034"}, "doi_lower": "10.18653/v1/k17-1034"}
{"paper_id": 266844262, "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language Models", "author_names": ["Chen-An Li", "Hung-yi Lee"], "venue": "arXiv.org", "abstract": "Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue.", "year": 2024, "publicationdate": "2024-01-06", "externalids": {"DOI": "10.48550/arXiv.2401.03129"}, "doi_lower": "10.48550/arxiv.2401.03129"}
{"paper_id": 253420654, "title": "Large Language Models with Controllable Working Memory", "author_names": ["Daliang Li", "A. Rawat", "M. Zaheer", "Xin Wang", "Michal Lukasik", "Andreas Veit", "Felix X. Yu", "Surinder Kumar"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), owing to their excellent understanding and generation abilities. Remarkably, what further sets these models apart is the massive amounts of world knowledge they internalize during pretraining. While many downstream applications provide the model with an informational context to aid its performance on the underlying task, how the model's world knowledge interacts with the factual information presented in the context remains under explored. As a desirable behavior, an LLM should give precedence to the context whenever it contains task-relevant information that conflicts with the model's memorized knowledge. This enables model predictions to be grounded in the context, which can then be used to update or correct specific model predictions without frequent retraining. By contrast, when the context is irrelevant to the task, the model should ignore it and fall back on its internal knowledge. In this paper, we undertake a first joint study of the aforementioned two properties, namely controllability and robustness, in the context of LLMs. We demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned) could exhibit poor controllability and robustness, which do not scale with increasing model size. As a solution, we propose a novel method - Knowledge Aware FineTuning (KAFT) - to strengthen both controllability and robustness by incorporating counterfactual and irrelevant contexts to standard supervised datasets. Our comprehensive evaluation showcases the utility of KAFT across model architectures and sizes.", "year": 2022, "publicationdate": "2022-11-09", "externalids": {"DOI": "10.48550/arXiv.2211.05110"}, "doi_lower": "10.48550/arxiv.2211.05110"}
{"paper_id": 262054189, "title": "CFGPT: Chinese Financial Assistant with Large Language Model", "author_names": ["Jiangtong Li", "Yuxuan Bian", "Guoxuan Wang", "Yang Lei", "Dawei Cheng", "Zhijun Ding", "Changjun Jiang"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have demonstrated great potential in natural language processing tasks within the financial domain. In this work, we present a Chinese Financial Generative Pre-trained Transformer framework, named CFGPT, which includes a dataset~(CFData) for pre-training and supervised fine-tuning, a financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment framework~(CFAPP) designed to navigate real-world financial applications. The CFData comprising both a pre-training dataset and a supervised fine-tuning dataset, where the pre-training dataset collates Chinese financial data and analytics, alongside a smaller subset of general-purpose text with 584M documents and 141B tokens in total, and the supervised fine-tuning dataset is tailored for six distinct financial tasks, embodying various facets of financial analysis and decision-making with 1.5M instruction pairs and 1.5B tokens in total. The CFLLM, which is based on InternLM-7B to balance the model capability and size, is trained on CFData in two stage, continued pre-training and supervised fine-tuning. The CFAPP is centered on large language models (LLMs) and augmented with additional modules to ensure multifaceted functionality in real-world application. Our codes are released at https://github.com/TongjiFinLab/CFGPT.", "year": 2023, "publicationdate": "2023-09-19", "externalids": {"DOI": "10.48550/arXiv.2309.10654"}, "doi_lower": "10.48550/arxiv.2309.10654"}
{"paper_id": 256390509, "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "author_names": ["Junnan Li", "Dongxu Li", "S. Savarese", "Steven C. H. Hoi"], "venue": "International Conference on Machine Learning", "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.", "year": 2023, "publicationdate": "2023-01-30", "externalids": {"DOI": "10.48550/arXiv.2301.12597"}, "doi_lower": "10.48550/arxiv.2301.12597"}
{"paper_id": 112339826, "title": "Product Model Evolvement and Configuration Knowledge Reuse Method", "author_names": ["Wei", "Feng", "Yi-xiong", "Tan", "Jianrong"], "venue": "", "abstract": null, "year": 2009, "publicationdate": null, "externalids": {"DOI": "10.3901/CJME.2009.06.856"}, "doi_lower": "10.3901/cjme.2009.06.856"}
{"paper_id": 4853851, "title": "Learning without Forgetting", "author_names": ["Zhizhong Li", "Derek Hoiem"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.", "year": 2016, "publicationdate": "2016-06-29", "externalids": {"DOI": "10.1007/978-3-319-46493-0_37"}, "doi_lower": "10.1007/978-3-319-46493-0_37"}
{"paper_id": 964287, "title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author_names": ["Chin-Yew Lin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": null, "year": 2004, "publicationdate": "2004-07-25", "externalids": {}, "doi_lower": null}
{"paper_id": 261697277, "title": "Mitigating the Alignment Tax of RLHF", "author_names": ["Yong Lin", "Lu Tan", "Hangyu Lin", "Wei Xiong", "Zeming Zheng", "Renjie Pi", "Jipeng Zhang", "Shizhe Diao", "Haoxiang Wang", "Hanze Dong", "Han Zhao", "Yuan Yao", "T. Zhang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "LLMs acquire a wide range of abilities during pre-training, but aligning LLMs under Reinforcement Learning with Human Feedback (RLHF) can lead to forgetting pretrained abilities, which is also known as the alignment tax. To investigate alignment tax, we conducted experiments with existing RLHF algorithms using OpenLLaMA-3B, which revealed a pronounced alignment tax in NLP tasks. Whereas, despite various techniques to mitigate forgetting, they are often at odds with the RLHF performance, leading to a trade-off between alignment performance and forgetting mitigation, leading to an alignment-forgetting trade-off. In this paper we show that model averaging, which simply interpolates between pre and post RLHF model weights, surprisingly achieves the most strongest alignment-forgetting Pareto front among a wide range of competing methods. To understand its effectiveness, we offer theoretical insights into model averaging, revealing that it enhances performance Pareto front by increasing feature diversity on the layers where tasks share overlapped feature spaces. Empirical evidence corroborates our analysis by showing the benefits of averaging low-level transformer layers. Building on the analysis and the observation that averaging different layers of the transformer leads to significantly different alignment-forgetting trade-offs, we propose Heterogeneous Model Averaging (HMA) to Heterogeneously find various combination ratios of model layers. HMA seeks to maximize the alignment performance while incurring minimal alignment tax. Moreover, we validate HMA’s performance across a range of RLHF algorithms over OpenLLaMA-3B and further extend our findings to Mistral-7B which is evaluated by open-sourced preference model and GPT4. Code available here.", "year": 2023, "publicationdate": "2023-09-12", "externalids": {"DOI": "10.48550/arXiv.2309.06256"}, "doi_lower": "10.48550/arxiv.2309.06256"}
{"paper_id": 266693296, "title": "GeoGalactica: A Scientific Large Language Model in Geoscience", "author_names": ["Zhouhan Lin", "Cheng Deng", "Le Zhou", "Tianhang Zhang", "Yi Xu", "Yutong Xu", "Zhongmou He", "Yuanyuan Shi", "Beiya Dai", "Yunchong Song", "Boyi Zeng", "Qiyuan Chen", "Tao Shi", "Tianyu Huang", "Yiwei Xu", "Shu Wang", "Luoyi Fu", "Weinan Zhang", "Junxian He", "Chao Ma", "Yunqiang Zhu", "Xinbing Wang", "Cheng Zhou"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) have achieved huge success for their general knowledge and ability to solve a wide spectrum of tasks in natural language processing (NLP). Due to their impressive abilities, LLMs have shed light on potential inter-discipline applications to foster scientific discoveries of a specific domain by using artificial intelligence (AI for science, AI4S). In the meantime, utilizing NLP techniques in geoscience research and practice is wide and convoluted, contributing from knowledge extraction and document classification to question answering and knowledge discovery. In this work, we take the initial step to leverage LLM for science, through a rather straightforward approach. We try to specialize an LLM into geoscience, by further pre-training the model with a vast amount of texts in geoscience, as well as supervised fine-tuning (SFT) the resulting model with our custom collected instruction tuning dataset. These efforts result in a model GeoGalactica consisting of 30 billion parameters. To our best knowledge, it is the largest language model for the geoscience domain. More specifically, GeoGalactica is from further pre-training of Galactica. We train GeoGalactica over a geoscience-related text corpus containing 65 billion tokens, preserving as the largest geoscience-specific text corpus. Then we fine-tune the model with 1 million pairs of instruction-tuning data consisting of questions that demand professional geoscience knowledge to answer. In this technical report, we will illustrate in detail all aspects of GeoGalactica, including data collection, data cleaning, base model selection, pre-training, SFT, and evaluation. We open-source our data curation tools and the checkpoints of GeoGalactica during the first 3/4 of pre-training.", "year": 2023, "publicationdate": "2023-12-31", "externalids": {"DOI": "10.48550/arXiv.2401.00434"}, "doi_lower": "10.48550/arxiv.2401.00434"}
{"paper_id": 269042762, "title": "Rho-1: Not All Tokens Are What You Need", "author_names": ["Zheng-Wen Lin", "Zhibin Gou", "Yeyun Gong", "Xiao Liu", "Yelong Shen", "Ruochen Xu", "Chen Lin", "Yujiu Yang", "Jian Jiao", "Nan Duan", "Weizhu Chen"], "venue": "arXiv.org", "abstract": "Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that\"9l training\". Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.", "year": 2024, "publicationdate": "2024-04-11", "externalids": {"DOI": "10.48550/arXiv.2404.07965"}, "doi_lower": "10.48550/arxiv.2404.07965"}
{"paper_id": 258179774, "title": "Visual Instruction Tuning", "author_names": ["Haotian Liu", "Chunyuan Li", "Qingyang Wu", "Yong Jae Lee"], "venue": "Neural Information Processing Systems", "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.", "year": 2023, "publicationdate": "2023-04-17", "externalids": {"DOI": "10.48550/arXiv.2304.08485"}, "doi_lower": "10.48550/arxiv.2304.08485"}
{"paper_id": 248693283, "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning", "author_names": ["Haokun Liu", "Derek Tam", "Mohammed Muqeeth", "Jay Mohta", "Tenghao Huang", "Mohit Bansal", "Colin Raffel"], "venue": "Neural Information Processing Systems", "abstract": "Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA)$^3$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6% absolute. All of the code used in our experiments is publicly available.", "year": 2022, "publicationdate": "2022-05-11", "externalids": {"DOI": "10.48550/arXiv.2205.05638"}, "doi_lower": "10.48550/arxiv.2205.05638"}
{"paper_id": 198953378, "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "author_names": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "M. Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "venue": "arXiv.org", "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.", "year": 2019, "publicationdate": "2019-07-26", "externalids": {}, "doi_lower": null}
{"paper_id": 216036341, "title": "Rehearsal-Free Continual Learning over Small Non-I.I.D. Batches", "author_names": ["Vincenzo Lomonaco", "D. Maltoni", "Lorenzo Pellegrini"], "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "abstract": "Robotic vision is a field where continual learning can play a significant role. An embodied agent operating in a complex environment subject to frequent and unpredictable changes is required to learn and adapt continuously. In the context of object recognition, for example, a robot should be able to learn (without forgetting) objects of never before seen classes as well as improving its recognition capabilities as new instances of already known classes are discovered. Ideally, continual learning should be triggered by the availability of short videos of single objects and performed on-line on on-board hardware with fine-grained updates. In this paper, we introduce a novel continual learning protocol based on the CORe50 benchmark and propose two rehearsal-free continual learning techniques, CWR* and AR1*, that can learn effectively even in the challenging case of nearly 400 small non-i.i.d. incremental batches. In particular, our experiments show that AR1* can outperform other state-of-the-art rehearsal-free techniques by more than 15% accuracy in some cases, with a very light and constant computational and memory overhead across training batches.", "year": 2019, "publicationdate": "2019-07-08", "externalids": {"DOI": "10.1109/CVPRW50498.2020.00131"}, "doi_lower": "10.1109/cvprw50498.2020.00131"}
{"paper_id": 37308416, "title": "Gradient Episodic Memory for Continual Learning", "author_names": ["David Lopez-Paz", "Marc'Aurelio Ranzato"], "venue": "Neural Information Processing Systems", "abstract": "One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.", "year": 2017, "publicationdate": "2017-06-01", "externalids": {}, "doi_lower": null}
{"paper_id": 257038067, "title": "BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark", "author_names": ["Dakuan Lu", "Jiaqing Liang", "Yipei Xu", "Qi He", "Yipeng Geng", "Mengkun Han", "Ying Xin", "Hengkui Wu", "Yanghua Xiao"], "venue": "arXiv.org", "abstract": "To advance Chinese financial natural language processing (NLP), we introduce BBT-FinT5, a new Chinese financial pre-training language model based on the T5 model. To support this effort, we have built BBT-FinCorpus, a large-scale financial corpus with approximately 300GB of raw text from four different sources. In general domain NLP, comprehensive benchmarks like GLUE and SuperGLUE have driven significant advancements in language model pre-training by enabling head-to-head comparisons among models. Drawing inspiration from these benchmarks, we propose BBT-CFLEB, a Chinese Financial Language understanding and generation Evaluation Benchmark, which includes six datasets covering both understanding and generation tasks. Our aim is to facilitate research in the development of NLP within the Chinese financial domain. Our model, corpus and benchmark are released at https://github.com/ssymmetry/BBT-FinCUGE-Applications. Our work belongs to the Big Bang Transformer (BBT), a large-scale pre-trained language model project.", "year": 2023, "publicationdate": "2023-02-18", "externalids": {"DOI": "10.48550/arXiv.2302.09432"}, "doi_lower": "10.48550/arxiv.2302.09432"}
{"paper_id": 263622229, "title": "IBCL: Zero-shot Model Generation under Stability-Plasticity Trade-offs", "author_names": ["Pengyuan Lu", "Michele Caprio", "Eric Eaton", "Insup Lee"], "venue": "", "abstract": "Algorithms that balance the stability-plasticity trade-off are well studied in the Continual Learning literature. However, only a few focus on obtaining models for specified trade-off preferences. When solving the problem of continual learning under specific trade-offs (CLuST), state-of-the-art techniques leverage rehearsal-based learning, which requires retraining when a model corresponding to a new trade-off preference is requested. This is inefficient, since there potentially exists a significant number of different trade-offs, and a large number of models may be requested. As a response, we propose Imprecise Bayesian Continual Learning (IBCL), an algorithm that tackles CLuST efficiently. IBCL replaces retraining with a constant-time convex combination. Given a new task, IBCL (1) updates the knowledge base as a convex hull of model parameter distributions, and (2) generates one Pareto-optimal model per given trade-off via convex combination without additional training. That is, obtaining models corresponding to specified trade-offs via IBCL is zero-shot. Experiments whose baselines are current CLuST algorithms show that IBCL improves classification by at most 44% on average per task accuracy, and by 45% on peak per task accuracy while maintaining a near-zero to positive backward transfer, with memory overheads converging to constants. In addition, its training overhead, measured by the number of batch updates, remains constant at every task, regardless of the number of preferences requested. IBCL also improves multi-objective reinforcement learning tasks by maintaining the same Pareto front hypervolume, while significantly reducing the training cost. Details can be found at: https://github.com/ibcl-anon/ibcl.", "year": 2023, "publicationdate": "2023-05-24", "externalids": {}, "doi_lower": null}
{"paper_id": 252383606, "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering", "author_names": ["Pan Lu", "Swaroop Mishra", "Tony Xia", "Liang Qiu", "Kai-Wei Chang", "Song-Chun Zhu", "Oyvind Tafjord", "Peter Clark", "A. Kalyan"], "venue": "Neural Information Processing Systems", "abstract": "When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40% of the data. The data and code are available at https://scienceqa.github.io.", "year": 2022, "publicationdate": "2022-09-20", "externalids": {"DOI": "10.48550/arXiv.2209.09513"}, "doi_lower": "10.48550/arxiv.2209.09513"}
{"paper_id": 231855531, "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation", "author_names": ["Shuai Lu", "Daya Guo", "Shuo Ren", "Junjie Huang", "Alexey Svyatkovskiy", "Ambrosio Blanco", "Colin B. Clement", "Dawn Drain", "Daxin Jiang", "Duyu Tang", "Ge Li", "Lidong Zhou", "Linjun Shou", "Long Zhou", "Michele Tufano", "Ming Gong", "Ming Zhou", "Nan Duan", "Neel Sundaresan", "Shao Kun Deng", "Shengyu Fu", "Shujie Liu"], "venue": "NeurIPS Datasets and Benchmarks", "abstract": "Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.", "year": 2021, "publicationdate": "2021-02-09", "externalids": {}, "doi_lower": null}
{"paper_id": 252542956, "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining", "author_names": ["Renqian Luo", "Liai Sun", "Yingce Xia", "Tao Qin", "Sheng Zhang", "Hoifung Poon", "Tie-Yan Liu"], "venue": "Briefings Bioinform.", "abstract": "Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.", "year": 2022, "publicationdate": "2022-09-24", "externalids": {"DOI": "10.1093/bib/bbac409"}, "doi_lower": "10.1093/bib/bbac409"}
{"paper_id": 258587921, "title": "Investigating Forgetting in Pre-Trained Representations Through Continual Learning", "author_names": ["Yun Luo", "Zhen Yang", "Xuefeng Bai", "Fandong Meng", "Jie Zhou", "Yue Zhang"], "venue": "arXiv.org", "abstract": "Representation forgetting refers to the drift of contextualized representations during continual training. Intuitively, the representation forgetting can influence the general knowledge stored in pre-trained language models (LMs), but the concrete effect is still unclear. In this paper, we study the effect of representation forgetting on the generality of pre-trained language models, i.e. the potential capability for tackling future downstream tasks. Specifically, we design three metrics, including overall generality destruction (GD), syntactic knowledge forgetting (SynF), and semantic knowledge forgetting (SemF), to measure the evolution of general knowledge in continual learning. With extensive experiments, we find that the generality is destructed in various pre-trained LMs, and syntactic and semantic knowledge is forgotten through continual learning. Based on our experiments and analysis, we further get two insights into alleviating general knowledge forgetting: 1) training on general linguistic tasks at first can mitigate general knowledge forgetting; 2) the hybrid continual learning method can mitigate the generality destruction and maintain more general knowledge compared with those only considering rehearsal or regularization.", "year": 2023, "publicationdate": "2023-05-10", "externalids": {"DOI": "10.48550/arXiv.2305.05968"}, "doi_lower": "10.48550/arxiv.2305.05968"}
{"paper_id": 261030404, "title": "BioMedGPT: Open Multimodal Generative Pre-trained Transformer for BioMedicine", "author_names": ["Yi Luo", "Jiahuan Zhang", "Siqi Fan", "Kai Yang", "Yushuai Wu", "Mu Qiao", "Zaiqing Nie"], "venue": "arXiv.org", "abstract": "Foundation models (FMs) have exhibited remarkable performance across a wide range of downstream tasks in many domains. Nevertheless, general-purpose FMs often face challenges when confronted with domain-specific problems, due to their limited access to the proprietary training data in a particular domain. In biomedicine, there are various biological modalities, such as molecules, proteins, and cells, which are encoded by the language of life and exhibit significant modality gaps with human natural language. In this paper, we introduce BioMedGPT, an open multimodal generative pre-trained transformer (GPT) for biomedicine, to bridge the gap between the language of life and human natural language. BioMedGPT allows users to easily ``communicate'' with diverse biological modalities through free text, which is the first of its kind. BioMedGPT aligns different biological modalities with natural language via a large generative language model, namely, BioMedGPT-LM. We publish BioMedGPT-10B, which unifies the feature spaces of molecules, proteins, and natural language via encoding and alignment. Through fine-tuning, BioMedGPT-10B outperforms or is on par with human and significantly larger general-purpose foundation models on the biomedical QA task. It also demonstrates promising performance in the molecule QA and protein QA tasks, which could greatly accelerate the discovery of new drugs and therapeutic targets. In addition, BioMedGPT-LM-7B is the first large generative language model based on Llama2 in the biomedical domain, therefore is commercial friendly. Both BioMedGPT-10B and BioMedGPT-LM-7B are open-sourced to the research community. In addition, we publish the datasets that are meticulously curated for the alignment of multi-modalities, i.e., PubChemQA and UniProtQA. All the models, codes, and datasets are available at \\url{https://github.com/PharMolix/OpenBioMed}.", "year": 2023, "publicationdate": "2023-08-18", "externalids": {"DOI": "10.48550/arXiv.2308.09442"}, "doi_lower": "10.48550/arxiv.2308.09442"}
{"paper_id": 259164815, "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "author_names": ["Ziyang Luo", "Can Xu", "Pu Zhao", "Qingfeng Sun", "Xiubo Geng", "Wenxiang Hu", "Chongyang Tao", "Jing Ma", "Qingwei Lin", "Daxin Jiang"], "venue": "International Conference on Learning Representations", "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM", "year": 2023, "publicationdate": "2023-06-14", "externalids": {}, "doi_lower": null}
{"paper_id": 1428702, "title": "Learning Word Vectors for Sentiment Analysis", "author_names": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "A. Ng", "Christopher Potts"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": null, "year": 2011, "publicationdate": "2011-06-19", "externalids": {}, "doi_lower": null}
{"paper_id": 231709504, "title": "Online Continual Learning in Image Classification: An Empirical Survey", "author_names": ["Zheda Mai", "Ruiwen Li", "Jihwan Jeong", "David Quispe", "Hyunwoo J. Kim", "S. Sanner"], "venue": "Neurocomputing", "abstract": null, "year": 2021, "publicationdate": "2021-01-25", "externalids": {"DOI": "10.1016/j.neucom.2021.10.021"}, "doi_lower": "10.1016/j.neucom.2021.10.021"}
{"paper_id": 8745888, "title": "Generation and Comprehension of Unambiguous Object Descriptions", "author_names": ["Junhua Mao", "Jonathan Huang", "Alexander Toshev", "Oana-Maria Camburu", "A. Yuille", "K. Murphy"], "venue": "Computer Vision and Pattern Recognition", "abstract": "We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MSCOCO. We have released the dataset and a toolbox for visualization and evaluation, see https://github.com/ mjhucla/Google_Refexp_toolbox.", "year": 2015, "publicationdate": "2015-11-07", "externalids": {"DOI": "10.1109/CVPR.2016.9"}, "doi_lower": "10.1109/cvpr.2016.9"}
{"paper_id": 264820150, "title": "A Survey on Knowledge Editing of Neural Networks", "author_names": ["Vittorio Mazzia", "Alessandro Pedrani", "Andrea Caciolai", "Kay Rottmann", "Davide Bernardi"], "venue": "IEEE Transactions on Neural Networks and Learning Systems", "abstract": "Deep neural networks are becoming increasingly pervasive in academia and industry, matching and surpassing human performance in a wide variety of fields and related tasks. However, just as humans, even the largest artificial neural networks (ANNs) make mistakes, and once-correct predictions can become invalid as the world progresses in time. Augmenting datasets with samples that account for mistakes or up-to-date information has become a common workaround in practical applications. However, the well-known phenomenon of catastrophic forgetting poses a challenge in achieving precise changes in the implicitly memorized knowledge of neural network parameters, often requiring a full model retraining to achieve desired behaviors. That is expensive, unreliable, and incompatible with the current trend of large self-supervised pretraining, making it necessary to find more efficient and effective methods for adapting neural network models to changing data. To address this need, knowledge editing (KE) is emerging as a novel area of research that aims to enable reliable, data-efficient, and fast changes to a pretrained target model, without affecting model behaviors on previously learned tasks. In this survey, we provide a brief review of this recent artificial intelligence field of research. We first introduce the problem of editing neural networks, formalize it in a common framework and differentiate it from more notorious branches of research such as continuous learning. Next, we provide a review of the most relevant KE approaches and datasets proposed so far, grouping works under four different families: regularization techniques, meta-learning, direct model editing, and architectural strategies. Finally, we outline some intersections with other fields of research and potential directions for future works.", "year": 2023, "publicationdate": "2023-10-30", "externalids": {"DOI": "10.1109/TNNLS.2024.3498935"}, "doi_lower": "10.1109/tnnls.2024.3498935"}
{"paper_id": 245537398, "title": "Towards continual task learning in artificial neural networks: current approaches and insights from neuroscience", "author_names": ["David McCaffary"], "venue": "arXiv.org", "abstract": "The innate capacity of humans and other animals to learn a diverse, and often interfering, range of knowledge and skills throughout their lifespan is a hallmark of natural intelligence, with obvious evolutionary motivations. In parallel, the ability of artificial neural networks (ANNs) to learn across a range of tasks and domains, combining and re-using learned representations where required, is a clear goal of artificial intelligence. This capacity, widely described as continual learning, has become a prolific subfield of research in machine learning. Despite the numerous successes of deep learning in recent years, across domains ranging from image recognition to machine translation, such continual task learning has proved challenging. Neural networks trained on multiple tasks in sequence with stochastic gradient descent often suffer from representational interference, whereby the learned weights for a given task effectively overwrite those of previous tasks in a process termed catastrophic forgetting. This represents a major impediment to the development of more generalised artificial learning systems, capable of accumulating knowledge over time and task space, in a manner analogous to humans. A repository of selected papers and implementations accompanying this review can be found at https://github.com/mccaffary/continual-learning.", "year": 2021, "publicationdate": "2021-12-28", "externalids": {}, "doi_lower": null}
{"paper_id": 2832081, "title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.", "author_names": ["James L. McClelland", "B. McNaughton", "R. O’Reilly"], "venue": "Psychology Review", "abstract": null, "year": 1995, "publicationdate": "1995-07-01", "externalids": {"DOI": "10.1037/0033-295X.102.3.419"}, "doi_lower": "10.1037/0033-295x.102.3.419"}
{"paper_id": 61019113, "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem", "author_names": ["M. McCloskey", "N. J. Cohen"], "venue": "", "abstract": null, "year": 1989, "publicationdate": null, "externalids": {"DOI": "10.1016/S0079-7421(08)60536-8"}, "doi_lower": "10.1016/s0079-7421(08)60536-8"}
{"paper_id": 245329773, "title": "An Empirical Investigation of the Role of Pre-training in Lifelong Learning", "author_names": ["Sanket Vaibhav Mehta", "Darshan Patil", "Sarath Chandar", "Emma Strubell"], "venue": "Journal of machine learning research", "abstract": "The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel data set of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized models. We then further investigate why pre-training alleviates forgetting in this setting. We study this phenomenon by analyzing the loss landscape, finding that pre-trained weights appear to ease forgetting by leading to wider minima. Based on this insight, we propose jointly optimizing for current task loss and loss basin sharpness to explicitly encourage wider basins during sequential fine-tuning. We show that this optimization approach outperforms several state-of-the-art task-sequential continual learning algorithms across multiple settings, occasionally even without retaining a memory that scales in size with the number of tasks.", "year": 2021, "publicationdate": "2021-12-16", "externalids": {}, "doi_lower": null}
{"paper_id": 255825985, "title": "Locating and Editing Factual Associations in GPT", "author_names": ["Kevin Meng", "David Bau", "A. Andonian", "Yonatan Belinkov"], "venue": "Neural Information Processing Systems", "abstract": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/", "year": 2022, "publicationdate": "2022-02-10", "externalids": {}, "doi_lower": null}
{"paper_id": 247155069, "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?", "author_names": ["Sewon Min", "Xinxi Lyu", "Ari Holtzman", "Mikel Artetxe", "M. Lewis", "Hannaneh Hajishirzi", "Luke Zettlemoyer"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.", "year": 2022, "publicationdate": "2022-02-25", "externalids": {"DOI": "10.18653/v1/2022.emnlp-main.759"}, "doi_lower": "10.18653/v1/2022.emnlp-main.759"}
{"paper_id": 209413409, "title": "OCR-VQA: Visual Question Answering by Reading Text in Images", "author_names": ["Anand Mishra", "Shashank Shekhar", "A. Singh", "Anirban Chakraborty"], "venue": "IEEE International Conference on Document Analysis and Recognition", "abstract": "The problem of answering questions about an image is popularly known as visual question answering (or VQA in short). It is a well-established problem in computer vision. However, none of the VQA methods currently utilize the text often present in the image. These \"texts in images\" provide additional useful cues and facilitate better understanding of the visual content. In this paper, we introduce a novel task of visual question answering by reading text in images, i.e., by optical character recognition or OCR. We refer to this problem as OCR-VQA. To facilitate a systematic way of studying this new problem, we introduce a large-scale dataset, namely OCRVQA-200K. This dataset comprises of 207,572 images of book covers and contains more than 1 million question-answer pairs about these images. We judiciously combine well-established techniques from OCR and VQA domains to present a novel baseline for OCR-VQA-200K. The experimental results and rigorous analysis demonstrate various challenges present in this dataset leaving ample scope for the future research. We are optimistic that this new task along with compiled dataset will open-up many exciting research avenues both for the document image analysis and the VQA communities.", "year": 2019, "publicationdate": "2019-09-01", "externalids": {"DOI": "10.1109/ICDAR.2019.00156"}, "doi_lower": "10.1109/icdar.2019.00156"}
{"paper_id": 248118588, "title": "NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks", "author_names": ["Swaroop Mishra", "Arindam Mitra", "Neeraj Varshney", "Bhavdeep Singh Sachdeva", "Peter Clark", "Chitta Baral", "A. Kalyan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 % on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.", "year": 2022, "publicationdate": "2022-04-12", "externalids": {"DOI": "10.48550/arXiv.2204.05660"}, "doi_lower": "10.48550/arxiv.2204.05660"}
{"paper_id": 239050360, "title": "Fast Model Editing at Scale", "author_names": ["E. Mitchell", "Charles Lin", "Antoine Bosselut", "Chelsea Finn", "Christopher D. Manning"], "venue": "International Conference on Learning Representations", "abstract": "While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact is desirable. However, the distributed, black-box nature of the representations learned by large neural networks makes producing such targeted edits difficult. If presented with only a single problematic input and new desired output, fine-tuning approaches tend to overfit; other editing algorithms are either computationally infeasible or simply ineffective when applied to very large models. To enable easy post-hoc editing at scale, we propose Model Editor Networks using Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model's behavior. MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre-trained model. Our experiments with T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters. Code and data available at https://sites.google.com/view/mend-editing.", "year": 2021, "publicationdate": "2021-10-21", "externalids": {}, "doi_lower": null}
{"paper_id": 249642147, "title": "Memory-Based Model Editing at Scale", "author_names": ["E. Mitchell", "Charles Lin", "Antoine Bosselut", "Christopher D. Manning", "Chelsea Finn"], "venue": "International Conference on Machine Learning", "abstract": "Even the largest neural networks make errors, and once-correct predictions can become invalid as the world changes. Model editors make local updates to the behavior of base (pre-trained) models to inject updated knowledge or correct undesirable behaviors. Existing model editors have shown promise, but also suffer from insufficient expressiveness: they struggle to accurately model an edit's intended scope (examples affected by the edit), leading to inaccurate predictions for test inputs loosely related to the edit, and they often fail altogether after many edits. As a higher-capacity alternative, we propose Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC), which stores edits in an explicit memory and learns to reason over them to modulate the base model's predictions as needed. To enable more rigorous evaluation of model editors, we introduce three challenging language model editing problems based on question answering, fact-checking, and dialogue generation. We find that only SERAC achieves high performance on all three problems, consistently outperforming existing approaches to model editing by a significant margin. Code, data, and additional project information will be made available at https://sites.google.com/view/serac-editing.", "year": 2022, "publicationdate": "2022-06-13", "externalids": {}, "doi_lower": null}
{"paper_id": 259370786, "title": "Large-scale Lifelong Learning of In-context Instructions and How to Tackle It", "author_names": ["J. Mok", "Jaeyoung Do", "Sungjin Lee", "Tara Taghavi", "Seunghak Yu", "Sungroh Yoon"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Jointly fine-tuning a Pre-trained Language Model (PLM) on a pre-defined set of tasks with in-context instructions has been proven to improve its generalization performance, allowing us to build a universal language model that can be deployed across task boundaries. In this work, we explore for the first time whether this attractive property of in-context instruction learning can be extended to a scenario in which tasks are fed to the target PLM in a sequential manner. The primary objective of so-called lifelong in-context instruction learning is to improve the target PLM’s instance- and task-level generalization performance as it observes more tasks. DynaInst, the proposed method to lifelong in-context instruction learning, achieves noticeable improvements in both types of generalization, nearly reaching the upper bound performance obtained through joint training.", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.acl-long.703"}, "doi_lower": "10.18653/v1/2023.acl-long.703"}
{"paper_id": 278498336, "title": "Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order", "author_names": ["Taishi Nakamura", "Mayank Mishra", "Simone Tedeschi", "Yekun Chai", "Jason T. Stillerman", "Felix Friedrich", "Prateek Yadav", "Tanmay Laud", "Minh Chien Vu", "Terry Yue Zhuo", "Diganta Misra", "Ben Bogin", "Xuan-Son Vu", "Marzena Karpinska", "A. Dantuluri", "Wojciech Kusa", "Tommaso Furlanello", "Rio Yokota", "Niklas Muennighoff", "Suhas Pai", "Tosin P. Adewumi", "Veronika Laippala", "Xiaozhe Yao", "Adalberto Junior", "Alpay Ariyak", "Aleksandr Drozd", "Jordan Clive", "Kshitij Gupta", "Liangyu Chen", "Qi Sun", "Ken Tsui", "Noah Persaud", "Nour Moustafa-Fahmy", "Tian-Xiang Chen", "Mohit Bansal", "Nicolo Monti", "Tai Dang", "Ziyang Luo", "Tien-Tung Bui", "Roberto Navigli", "Virendra Mehta", "Matthew Blumberg", "Victor May", "Huu Nguyen", "Sampo Pyysalo"], "venue": "arXiv.org", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2404.00399"}, "doi_lower": "10.48550/arxiv.2404.00399"}
{"paper_id": 261696577, "title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy", "author_names": ["Tuan Dung Nguyen", "Yuan-Sen Ting", "I. Ciucă", "Charlie O'Neill", "Ze-Chang Sun", "Maja Jabłońska", "S. Kruk", "Ernest Perkowski", "Jack William Miller", "Jason Li", "J. Peek", "Kartheik G. Iyer", "Tomasz R'o.za'nski", "P. Khetarpal", "Sharaf Zaman", "D. Brodrick", "Sergio J. Rodr'iguez M'endez", "Thang Bui", "Alyssa Goodman", "A. Accomazzi", "J. P. Naiman", "Jesse Cranney", "K. Schawinski", "UniverseTBD"], "venue": "WIESP", "abstract": "Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development.", "year": 2023, "publicationdate": "2023-09-12", "externalids": {"DOI": "10.48550/arXiv.2309.06126"}, "doi_lower": "10.48550/arxiv.2309.06126"}
{"paper_id": 202621357, "title": "Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects", "author_names": ["Jianmo Ni", "Jiacheng Li", "Julian McAuley"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Several recent works have considered the problem of generating reviews (or ‘tips’) as a form of explanation as to why a recommendation might match a customer’s interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justifications that are relevant to users’ decision-making process. We seek to introduce new datasets and methods to address the recommendation justification task. In terms of data, we first propose an ‘extractive’ approach to identify review segments which justify users’ intentions; this approach is then used to distantly label massive review corpora and construct large-scale personalized recommendation justification datasets. In terms of generation, we are able to design two personalized generation models with this data: (1) a reference-based Seq2Seq model with aspect-planning which can generate justifications covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justifications based on templates extracted from justification histories. We conduct experiments on two real-world datasets which show that our model is capable of generating convincing and diverse justifications.", "year": 2019, "publicationdate": "2019-11-01", "externalids": {"DOI": "10.18653/v1/D19-1018"}, "doi_lower": "10.18653/v1/d19-1018"}
{"paper_id": 244400669, "title": "Revisiting Catastrophic Forgetting in Class Incremental Learning", "author_names": ["Zixuan Ni", "Haizhou Shi", "Siliang Tang", "Longhui Wei", "Qi Tian", "Yueting Zhuang"], "venue": "", "abstract": "Although the concept of catastrophic forgetting is straightforward, there is a lack of study on its causes. In this paper, we systematically explore and reveal three causes for catastrophic forgetting in Class Incremental Learning(CIL). From the perspective of representation learning,(i) intra-phase forgetting happens when the learner fails to correctly align the same-phase data as training proceeds and (ii) inter-phase confusion happens when the learner confuses the current-phase data with the previous-phase. From the task-specific point of view, the CIL model suffers from the problem of (iii) classifier deviation. After investigating existing strategies, we observe that there is a lack of study on how to prevent the inter-phase confusion. To initiate the research on this specific issue, we propose a simple yet effective framework, Contrastive Class Concentration for CIL (C4IL). Our framework leverages the class concentration effect of contrastive learning, yielding a representation distribution with better intra-class compactibility and inter-class separability. Empirically, we observe that C4IL significantly lowers the probability of inter-phase confusion and as a result improves the performance on multiple CIL settings of multiple datasets.", "year": 2021, "publicationdate": "2021-07-26", "externalids": {}, "doi_lower": null}
{"paper_id": 258676581, "title": "Continual Vision-Language Representation Learning with Off-Diagonal Information", "author_names": ["Zixuan Ni", "Longhui Wei", "Siliang Tang", "Yueting Zhuang", "Qi Tian"], "venue": "International Conference on Machine Learning", "abstract": "Large-scale multi-modal contrastive learning frameworks like CLIP typically require a large amount of image-text samples for training. However, these samples are always collected continuously in real scenarios. This paper discusses the feasibility of continual CLIP training using streaming data. Unlike continual learning based on self-supervised learning methods for pure images, which is empirically robust against catastrophic forgetting, CLIP's performance degeneration in the continual setting is significant and non-neglectable. By analyzing the changes in the model's representation space during continual CLIP training from a spatial geometry perspective, we explore and summarize these spatial variations as Spatial Disorder (SD), which can be divided into Intra-modal Rotation and Inter-modal Deviation. Moreover, we empirically and theoretically demonstrate how SD leads to a performance decline for CLIP on cross-modal retrieval tasks. To alleviate SD, we propose a new continual vision-language representation learning framework Mod-X: Maintain off-diagonal information-matriX. By selectively aligning the off-diagonal information distribution of contrastive matrices, the Mod-X improves the capability of the multi-modal model by maintaining the multi-modal representation space alignment on the old data domain during continuously fitting the new training data domain. Experiments on commonly used datasets with different scales and scopes have demonstrated the effectiveness of our method.", "year": 2023, "publicationdate": "2023-05-11", "externalids": {"DOI": "10.48550/arXiv.2305.07437"}, "doi_lower": "10.48550/arxiv.2305.07437"}
{"paper_id": 276163907, "title": "Introducing ChatGPT to Education in Science Museums: A Conceptual Framework", "author_names": ["Mohamed W. Fareed"], "venue": "Journal of Museum Education", "abstract": "ABSTRACT Museum education plays a pivotal role in enriching the visitor experience by promoting learning and engagement across all age groups. This article introduces a framework that envisions the integration of technology, particularly natural language processing (NLP) models like ChatGPT, as a transformative force in the field of museum education. The framework examines the strengths and weaknesses of ChatGPT, its diverse applications, and the potential advantages and drawbacks it presents, with a particular emphasis on science museums as an illustrative example. This framework revolves around the identification of key stakeholders in museum education, including visitors, educators, curators, and administrators. It delves into the myriad ways ChatGPT can be employed to address various aspects and functions within museum education. These applications encompass tasks such as responding to visitor queries, offering tailored recommendations, and facilitating constructive dialogues between visitors and museum personnel. The insights gained from this exploration hold the promise of positively impacting both the theoretical underpinnings and practical implementations of museum education in the foreseeable future.", "year": 2025, "publicationdate": "2025-02-05", "externalids": {"DOI": "10.1080/10598650.2024.2440287"}, "doi_lower": "10.1080/10598650.2024.2440287"}
{"paper_id": 1753586, "title": "Brain imaging of language plasticity in adopted adults: can a second language replace the first?", "author_names": ["Christophe Pallier", "S. Dehaene", "J B Poline", "D. LeBihan", "Emmanuel Dupoux"], "venue": "NeuroImage", "abstract": null, "year": 2001, "publicationdate": "2001-06-01", "externalids": {"DOI": "10.1016/S1053-8119(01)91925-1"}, "doi_lower": "10.1016/s1053-8119(01)91925-1"}
{"paper_id": 11080756, "title": "Bleu: a Method for Automatic Evaluation of Machine Translation", "author_names": ["Kishore Papineni", "Salim Roukos", "T. Ward", "Wei-Jing Zhu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.", "year": 2002, "publicationdate": "2002-07-06", "externalids": {"DOI": "10.3115/1073083.1073135"}, "doi_lower": "10.3115/1073083.1073135"}
{"paper_id": 268253688, "title": "IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators", "author_names": ["Indraneil Paul", "Jun Luo", "Goran Glavas", "Iryna Gurevych"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations (IR) - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer. To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled with respective intermediate representations. Next, starting from various base Code-LMs (ranging in size from 1.1B to 7.3B parameters), we carry out continued causal language modelling training on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2) align the IR constructs with respective constructs of various programming languages. Our resulting models, dubbed IRCoder, display sizeable and consistent gains across a wide variety of code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following.", "year": 2024, "publicationdate": "2024-03-06", "externalids": {"DOI": "10.48550/arXiv.2403.03894"}, "doi_lower": "10.48550/arxiv.2403.03894"}
{"paper_id": 67329015, "title": "Theoretical foundations of multi-task lifelong learning", "author_names": ["Anastasia Pentina"], "venue": "", "abstract": null, "year": 2016, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 202539551, "title": "Language Models as Knowledge Bases?", "author_names": ["F. Petroni", "Tim Rocktäschel", "Patrick Lewis", "A. Bakhtin", "Yuxiang Wu", "Alexander H. Miller", "Sebastian Riedel"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.", "year": 2019, "publicationdate": "2019-09-01", "externalids": {"DOI": "10.18653/v1/D19-1250"}, "doi_lower": "10.18653/v1/d19-1250"}
{"paper_id": 257631846, "title": "Computationally Budgeted Continual Learning: What Does Matter?", "author_names": ["Ameya Prabhu", "Hasan Hammoud", "P. Dokania", "Philip H. S. Torr", "S. Lim", "Bernard Ghanem", "Adel Bibi"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Continual Learning (CL) aims to sequentially train models on streams of incoming data that vary in distribution by preserving previous knowledge while adapting to new data. Current CL literature focuses on restricted access to previously seen data, while imposing no constraints on the computational budget for training. This is unreasonable for applications in-the-wild, where systems are primarily constrained by computational and time budgets, not storage. We revisit this problem with a large-scale benchmark and analyze the performance of traditional CL approaches in a compute-constrained setting, where effective memory samples used in training can be implicitly restricted as a consequence of limited computation. We conduct experiments evaluating various CL sampling strategies, distillation losses, and partial fine-tuning on two large-scale datasets, namely ImageNet2K and Continual Google Landmarks V2 in data incremental, class incremental, and time incremental settings. Through extensive experiments amounting to a total of over 1500 GPU-hours, we find that, under compute-constrained setting, traditional CL approaches, with no exception, fail to outperform a simple minimal baseline that samples uniformly from memory. Our conclusions are consistent in a different number of stream time steps, e.g., 20 to 200, and under several computational budgets. This suggests that most existing CL methods are particularly too computationally expensive for realistic budgeted deployment. Code for this project is available at: https://github.com/drimpossible/BudgetCL.", "year": 2023, "publicationdate": "2023-03-20", "externalids": {"DOI": "10.1109/CVPR52729.2023.00360"}, "doi_lower": "10.1109/cvpr52729.2023.00360"}
{"paper_id": 258715081, "title": "Online Continual Learning Without the Storage Constraint", "author_names": ["Ameya Prabhu", "Zhipeng Cai", "P. Dokania", "Philip H. S. Torr", "V. Koltun", "Ozan Sener"], "venue": "arXiv.org", "abstract": "Traditional online continual learning (OCL) research has primarily focused on mitigating catastrophic forgetting with fixed and limited storage allocation throughout an agent's lifetime. However, a broad range of real-world applications are primarily constrained by computational costs rather than storage limitations. In this paper, we target such applications, investigating the online continual learning problem under relaxed storage constraints and limited computational budgets. We contribute a simple algorithm, which updates a kNN classifier continually along with a fixed, pretrained feature extractor. We selected this algorithm due to its exceptional suitability for online continual learning. It can adapt to rapidly changing streams, has zero stability gap, operates within tiny computational budgets, has low storage requirements by only storing features, and has a consistency property: It never forgets previously seen data. These attributes yield significant improvements, allowing our proposed algorithm to outperform existing methods by over 20% in accuracy on two large-scale OCL datasets: Continual LOCalization (CLOC) with 39M images and 712 classes and Continual Google Landmarks V2 (CGLM) with 580K images and 10,788 classes, even when existing methods retain all previously seen images. Furthermore, we achieve this superior performance with considerably reduced computational and storage expenses. We provide code to reproduce our results at github.com/drimpossible/ACM.", "year": 2023, "publicationdate": "2023-05-16", "externalids": {"DOI": "10.48550/arXiv.2305.09253"}, "doi_lower": "10.48550/arxiv.2305.09253"}
{"paper_id": 238856821, "title": "LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5", "author_names": ["Chengwei Qin", "Shafiq R. Joty"], "venue": "International Conference on Learning Representations", "abstract": "Existing approaches to lifelong language learning rely on plenty of labeled data for learning a new task, which is hard to obtain in most real scenarios. Considering that humans can continually learn new tasks from a handful of examples, we expect the models also to be able to generalize well on new few-shot tasks without forgetting the previous ones. In this work, we define this more challenging yet practical problem as Lifelong Few-shot Language Learning (LFLL) and propose a unified framework for it based on prompt tuning of T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot learning ability, and simultaneously trains the model as a task solver and a data generator. Before learning a new domain of the same task type, LFPT5 generates pseudo (labeled) samples of previously learned domains, and later gets trained on those samples to alleviate forgetting of previous knowledge as it learns the new domain. In addition, a KL divergence loss is minimized to achieve label consistency between the previous and the current model. While adapting to a new task type, LFPT5 includes and tunes additional prompt embeddings for the new task. With extensive experiments, we demonstrate that LFPT5 can be applied to various different types of tasks and significantly outperform previous methods in different LFLL settings.", "year": 2021, "publicationdate": "2021-10-14", "externalids": {}, "doi_lower": null}
{"paper_id": 258686422, "title": "Recyclable Tuning for Continual Pre-training", "author_names": ["Yujia Qin", "Cheng Qian", "Xu Han", "Yankai Lin", "Huadong Wang", "Ruobing Xie", "Zhiyuan Liu", "Maosong Sun", "Jie Zhou"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources. We bring this issue to the forefront and contend that proper algorithms for recycling outdated adapted weights should be developed. To this end, we formulate the task of recyclable tuning for continual pre-training. In pilot studies, we find that after continual pre-training, the upgraded PLM remains compatible with the outdated adapted weights to some extent. Motivated by this finding, we analyze the connection between continually pre-trained PLMs from two novel aspects, i.e., mode connectivity, and functional similarity. Based on the corresponding findings, we propose both an initialization-based method and a distillation-based method for our task. We demonstrate their feasibility in improving the convergence and performance for tuning the upgraded PLM. We also show that both methods can be combined to achieve better performance. The source codes are publicly available at https://github.com/thunlp/RecyclableTuning.", "year": 2023, "publicationdate": "2023-05-15", "externalids": {"DOI": "10.48550/arXiv.2305.08702"}, "doi_lower": "10.48550/arxiv.2305.08702"}
{"paper_id": 231591445, "title": "Learning Transferable Visual Models From Natural Language Supervision", "author_names": ["Alec Radford", "Jong Wook Kim", "Chris Hallacy", "A. Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark", "Gretchen Krueger", "I. Sutskever"], "venue": "International Conference on Machine Learning", "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.", "year": 2021, "publicationdate": "2021-02-26", "externalids": {}, "doi_lower": null}
{"paper_id": 160025533, "title": "Language Models are Unsupervised Multitask Learners", "author_names": ["Alec Radford", "Jeff Wu", "R. Child", "D. Luan", "Dario Amodei", "I. Sutskever"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 258959321, "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "author_names": ["Rafael Rafailov", "Archit Sharma", "E. Mitchell", "Stefano Ermon", "Christopher D. Manning", "Chelsea Finn"], "venue": "Neural Information Processing Systems", "abstract": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {}, "doi_lower": null}
{"paper_id": 258959321, "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "author_names": ["Rafael Rafailov", "Archit Sharma", "E. Mitchell", "Stefano Ermon", "Christopher D. Manning", "Chelsea Finn"], "venue": "Neural Information Processing Systems", "abstract": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.", "year": 2023, "publicationdate": "2023-05-29", "externalids": {}, "doi_lower": null}
{"paper_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "author_names": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "venue": "Journal of machine learning research", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.", "year": 2019, "publicationdate": "2019-10-23", "externalids": {}, "doi_lower": null}
{"paper_id": 47018994, "title": "Know What You Don’t Know: Unanswerable Questions for SQuAD", "author_names": ["Pranav Rajpurkar", "Robin Jia", "Percy Liang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.", "year": 2018, "publicationdate": "2018-06-11", "externalids": {"DOI": "10.18653/v1/P18-2124"}, "doi_lower": "10.18653/v1/p18-2124"}
{"paper_id": 245007201, "title": "Model Zoo: A Growing Brain That Learns Continually", "author_names": ["Rahul Ramesh", "P. Chaudhari"], "venue": "International Conference on Learning Representations", "abstract": "This paper argues that continual learning methods can benefit by splitting the capacity of the learner across multiple models. We use statistical learning theory and experimental analysis to show how multiple tasks can interact with each other in a non-trivial fashion when a single model is trained on them. The generalization error on a particular task can improve when it is trained with synergistic tasks, but can also deteriorate when trained with competing tasks. This theory motivates our method named Model Zoo which, inspired from the boosting literature, grows an ensemble of small models, each of which is trained during one episode of continual learning. We demonstrate that Model Zoo obtains large gains in accuracy on a variety of continual learning benchmark problems. Code is available at https://github.com/grasp-lyrl/modelzoo_continual.", "year": 2021, "publicationdate": "2021-06-06", "externalids": {}, "doi_lower": null}
{"paper_id": 85616828, "title": "A J M Watson, EJElliott, D D K Rolston, M M Borodo, M J G Farthing, PD Fairclough", "author_names": ["London J M Watson", "D. Rolston", "J. M. Watson"], "venue": "", "abstract": null, "year": 1990, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 206596260, "title": "iCaRL: Incremental Classifier and Representation Learning", "author_names": ["Sylvestre-Alvise Rebuffi", "Alexander Kolesnikov", "G. Sperl", "Christoph H. Lampert"], "venue": "Computer Vision and Pattern Recognition", "abstract": "A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.", "year": 2016, "publicationdate": "2016-11-23", "externalids": {"DOI": "10.1109/CVPR.2017.587"}, "doi_lower": "10.1109/cvpr.2017.587"}
{"paper_id": 268297180, "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context", "author_names": ["Machel Reid", "Nikolay Savinov", "Denis Teplyashin", "Dmitry Lepikhin", "T. Lillicrap", "Jean-Baptiste Alayrac", "Radu Soricut", "Angeliki Lazaridou", "Orhan Firat", "Julian Schrittwieser", "Ioannis Antonoglou", "Rohan Anil", "Sebastian Borgeaud", "Andrew M. Dai", "Katie Millican", "Ethan Dyer", "Mia Glaese", "Thibault Sottiaux", "Ben-jamin Lee", "Fabio Viola", "Malcolm Reynolds", "Yuanzhong Xu", "James Molloy", "Jilin Chen", "M. Isard", "P. Barham", "Tom Hennigan", "Ross Mcilroy", "Melvin Johnson", "J. Schalkwyk", "Eli Collins", "Eliza Rutherford", "Erica Moreira", "Kareem W. Ayoub", "Megha Goel", "Clemens Meyer", "Gregory Thornton", "Zhen Yang", "H. Michalewski", "Zaheer Abbas", "Nathan Schucher", "Ankesh Anand", "Richard Ives", "James Keeling", "Karel Lenc", "S. Haykal", "Siamak Shakeri", "Pranav Shyam", "A. Chowdhery", "Roman Ring", "Stephen Spencer", "Eren Sezener", "Luke Vilnis", "Os-car Chang", "Nobuyuki Morioka", "George Tucker", "Ce Zheng", "Oliver Woodman", "Nithya Attaluri", "Tomás Kociský", "Evgenii Eltyshev", "Xi Chen", "Timothy Chung", "Vittorio Selo", "Siddhartha Brahma", "Petko Georgiev", "Ambrose Slone", "Zhenkai Zhu", "James Lottes", "Siyuan Qiao", "Ben Caine", "Sebastian Riedel", "Alex Tomala", "Martin Chadwick", "J Christopher Love", "Peter Choy", "Sid Mittal", "N. Houlsby", "Yunhao Tang", "Matthew Lamm", "Libin Bai", "Qiao Zhang", "Luheng He", "Yong Cheng", "Peter Humphreys", "Yujia Li", "Sergey Brin", "Albin Cassirer", "Ying-Qi Miao", "Lukás Zilka", "Taylor Tobin", "Kelvin Xu", "Lev Proleev", "Daniel Sohn", "Al-berto Magni", "L. Hendricks", "Isabel Gao", "Santiago Ontan'on", "Oskar Bunyan", "Nathan Byrd", "Abhanshu Sharma", "Biao Zhang", "Mario Pinto", "Rishika Sinha", "Harsh Mehta", "Dawei Jia", "Sergi Caelles", "Albert Webson", "Alex Morris", "Becca Roelofs", "Yifan Ding", "Robin Strudel", "Xuehan Xiong", "Marvin Ritter", "Mostafa Dehghani", "R. Chaabouni", "Abhijit Karmarkar", "Guangda Lai", "Fabian Mentzer", "Bibo Xu", "YaGuang Li", "Yujing Zhang", "T. Paine", "Alex Goldin", "Behnam Neyshabur", "Kate Baumli", "Anselm Levskaya", "Michael Laskin", "Wenhao Jia", "Jack W. Rae", "Kefan Xiao", "Antoine He", "Skye Giordano", "Lakshman Yagati", "Jean-Baptiste Lespiau", "Paul Natsev", "Sanjay Ganapathy", "Fangyu Liu", "Danilo Martins", "Nanxin Chen", "Yunhan Xu", "Megan Barnes", "Rhys May", "Arpi Vezer", "Junhyuk Oh", "Ken Franko", "Sophie Bridgers", "Ruizhe Zhao", "Boxi Wu", "Basil Mustafa", "Sean Sechrist", "Emilio Parisotto", "Thanumalayan Sankaranarayana Pillai", "Chris Larkin", "Chenjie Gu", "Christina Sorokin", "M. Krikun", "Alexey Guseynov", "Jessica Landon", "Romina Datta", "A. Pritzel", "Phoebe Thacker", "Fan Yang", "Kevin Hui", "A.E. Hauth", "C. Yeh", "David Barker", "J. Mao-Jones", "Sophia Austin", "Hannah Sheahan", "Parker Schuh", "James Svensson", "Rohan Jain", "V. Ramasesh", "Anton Briukhov", "D. Chung", "Tamara von Glehn", "Christina Butterfield", "Priya Jhakra", "Matt Wiethoff", "Justin Frye", "Jordan Grimstad", "Beer Changpinyo", "Charline Le Lan", "Anna Bortsova", "Yonghui Wu", "P. Voigtlaender", "Tara N. Sainath", "Charlotte Smith", "Will Hawkins", "Kris Cao", "James Besley", "S. Srinivasan", "Mark Omernick", "Colin Gaffney", "G. Surita", "Ryan Burnell", "Bogdan Damoc", "Junwhan Ahn", "Andrew Brock", "Mantas Pajarskas", "Anastasia Petrushkina", "Seb Noury", "Lorenzo Blanco", "Kevin Swersky", "Arun Ahuja", "Thi Avrahami", "Vedant Misra", "Raoul de Liedekerke", "Mariko Iinuma", "A. Polozov", "Sarah York", "George van den Driessche", "Paul Michel", "Justin Chiu", "Rory Blevins", "Zach Gleicher", "Adrià Recasens", "Alban Rrustemi", "E. Gribovskaya", "Au-rko Roy", "Wiktor Gworek", "Sébastien M. R. Arnold", "Lisa Lee", "James Lee-Thorp", "M. Maggioni", "Enrique Piqueras", "Kartikeya Badola", "S. Vikram", "Lucas Gonzalez", "Anirudh Baddepudi", "Evan Senter", "J. Devlin", "James Qin", "Michael Azzam", "Maja Trebacz", "M. Polacek", "Kashyap Krishnakumar", "Shuo-Yiin Chang", "Matthew Tung", "Ivo Penchev", "Rishabh Joshi", "Kate Olszewska", "Carrie Muir", "Mateo Wirth", "A. Hartman", "Joshua Newlan", "S. Kashem", "Vijay Bolina", "Elahe Dabir", "Joost R. van Amersfoort", "Zafarali Ahmed", "James Cobon-Kerr", "Aishwarya B Kamath", "A. M. Hrafnkelsson", "Le Hou", "Ian Mackinnon", "Alexandre Frechette", "Eric Noland", "Xi-ance Si", "Emanuel Taropa", "Dong Li", "Phil Crone", "Anmol Gulati", "S'ebastien Cevey", "Jonas Adler", "Ada Ma", "David Silver", "Simon Tokumine", "Richard Powell", "Stephan Lee", "Michael B. Chang", "Samer Hassan", "Diana Mincu", "Antoine Yang", "Nir Levine", "Jenny Brennan", "Mingqiu Wang", "Sarah Hodkinson", "Jeffrey Zhao", "Josh Lipschultz", "Aedan Pope", "Michael B. Chang", "Cheng Li", "Laurent El Shafey", "M. Paganini", "Sholto Douglas", "Bernd Bohnet", "Fabio Pardo", "Seth Odoom", "Mihaela Roșca", "Cicero Nogueira dos Santos", "Kedar Soparkar", "A. Guez", "Tom Hudson", "Steven Hansen", "Chulayuth Asawaroengchai", "Ravichandra Addanki", "Tianhe Yu", "Wojciech Stokowiec", "Mina Khan", "Justin Gilmer", "Jaehoon Lee", "Carrie Grimes Bostock", "Keran Rong", "Jonathan Caton", "Pedram Pejman", "Filip Pavetic", "Geoff Brown", "Vivek Sharma", "Mario Luvci'c", "Rajku-mar Samuel", "J. Djolonga", "Amol Mandhane", "Lars Lowe Sjosund", "Elena Buchatskaya", "Elspeth White", "Natalie Clay", "Jiepu Jiang", "Hyeontaek Lim", "Ross Hemsley", "Jane Labanowski", "Nicola De Cao", "David Steiner", "Sayed Hadi Hashemi", "Jacob Austin", "Anita Gergely", "Tim Blyth", "Joe Stanton", "K. Shivakumar", "Aditya Siddhant", "Anders Andreassen", "Carlos L. Araya", "Nikhil Sethi", "Rakesh Shivanna", "Steven Hand", "Ankur Bapna", "A. Khodaei", "Antoine Miech", "Garrett Tanzer", "Andy Swing", "S. Thakoor", "Zhufeng Pan", "Zachary Nado", "Stephanie Winkler", "Dian Yu", "Mohammad Saleh", "Lorenzo Maggiore", "Iain Barr", "Minh Giang", "Thais Kagohara", "Ivo Danihelka", "Amit Marathe", "Vladimir Feinberg", "Mohamed Elhawaty", "Nimesh Ghelani", "Dan Horgan", "Helen Miller", "Lexi Walker", "Richard Tanburn", "Mukarram Tariq", "Disha Shrivastava", "Fei Xia", "Chung-Cheng Chiu", "Zoe Ashwood", "Khuslen Baatarsukh", "Sina Samangooei", "Fred Alcober", "Axel Stjerngren", "P. Komarek", "Katerina Tsihlas", "Anudhyan Boral", "R. Comanescu", "Jeremy Chen", "Ruibo Liu", "Dawn Bloxwich", "Charlie Chen", "Yanhua Sun", "Fangxi-aoyu Feng", "M. Mauger", "Xerxes Dotiwalla", "V. Hellendoorn", "Michael Sharman", "Ivy Zheng", "Krishna Haridasan", "Gabriel Barth-Maron", "Craig Swanson", "Dominika Rogozi'nska", "Alek Andreev", "P. Rubenstein", "Ruoxin Sang", "Dan Hurt", "Gamaleldin Elsayed", "Ren-shen Wang", "Dave Lacey", "Anastasija Ili'c", "Yao Zhao", "Woohyun Han", "Lora Aroyo", "Chimezie Iwuanyanwu", "Vitaly Nikolaev", "Balaji Lakshminarayanan", "Sadegh Jazayeri", "Raphael Lopez Kaufman", "Mani Varadarajan", "Chetan Tekur", "Doug Fritz", "Misha Khalman", "David Reitter", "Kingshuk Dasgupta", "Shourya Sarcar", "T. Ornduff", "Javier Snaider", "Fantine Huot", "Johnson Jia", "Rupert Kemp", "Nejc Trdin", "Anitha Vijayakumar", "Lucy Kim", "Christof Angermueller", "Li Lao", "Tianqi Liu", "Haibin Zhang", "David Engel", "Somer Greene", "Anais White", "Jessica Austin", "Lilly Taylor", "Shereen Ashraf", "Dangyi Liu", "Maria Georgaki", "Irene Cai", "Yana Kulizhskaya", "Sonam Goenka", "Brennan Saeta", "Kiran Vodrahalli", "Christian Frank", "D. Cesare", "Brona Robenek", "Harry Richardson", "Mah-moud Alnahlawi", "Christo-pher Yew", "Priya Ponnapalli", "M. Tagliasacchi", "Alex Korchemniy", "Yelin Kim", "Dinghua Li", "B. Rosgen", "Kyle Levin", "Jeremy Wiesner", "Praseem Banzal", "Praveen Srinivasan", "Hongkun Yu", "cCauglar Unlu", "David Reid", "Zora Tung", "D. Finchelstein", "Ravin Kumar", "A. Elisseeff", "Jin Huang", "Ming Zhang", "Rui Zhu", "Ricardo Aguilar", "Mai Gim'enez", "Jiawei Xia", "Olivier Dousse", "W. Gierke", "S. Yeganeh", "Damion Yates", "Komal Jalan", "Lu Li", "Eri Latorre-Chimoto", "D. D. Nguyen", "Ken Durden", "Praveen Kallakuri", "Yaxin Liu", "Matthew Johnson", "Tomy Tsai", "Alice Talbert", "Jasmine Liu", "Alexander Neitz", "C. Elkind", "Marco Selvi", "Mimi Jasarevic", "Livio Baldini Soares", "Livio Baldini Soares", "Pidong Wang", "A. Wang", "Xinyu Ye", "Krystal Kallarackal", "Lucia Loher", "Hoi Lam", "Josef Broder", "D. Holtmann-Rice", "Nina Martin", "Bramandia Ramadhana", "Daniel Toyama", "Mrinal Shukla", "Sujoy Basu", "Abhi Mohan", "Nicholas Fernando", "Noah Fiedel", "Kim Paterson", "Hui Li", "Ankush Garg", "Jane Park", "Donghyun Choi", "Diane Wu", "Sankalp Singh", "Zhishuai Zhang", "Amir Globerson", "Lily Yu", "John Carpenter", "F. D. C. Quitry", "Carey Radebaugh", "Chu-Cheng Lin", "Alex Tudor", "Prakash Shroff", "D. Garmon", "Dayou Du", "Neera Vats", "Han Lu", "Shariq Iqbal", "A. Yakubovich", "Nilesh Tripuraneni", "J. Manyika", "Ha-roon Qureshi", "Nan Hua", "Christel Ngani", "Maria Abi Raad", "Hannah Forbes", "Anna Bulanova", "J. Stanway", "Mukund Sundararajan", "Victor Ungureanu", "Colton Bishop", "Yunjie Li", "Balaji Venkatraman", "Bo Li", "Chloe Thornton", "Salvatore Scellato", "Nishesh Gupta", "Yicheng Wang", "Ian Tenney", "Xihui Wu", "Ashish Shenoy", "Gabriel Carvajal", "Diana Gage Wright", "Ben Bariach", "Zhuyun Xiao", "Peter Hawkins", "Sid Dalmia", "Clément Farabet", "Pedro Valenzuela", "Quan Yuan", "Christoper A. Welty", "Ananth Agarwal", "Mianna Chen", "Wooyeol Kim", "Brice Hulse", "Nandita Dukkipati", "Adam Paszke", "Andrew Bolt", "Elnaz Davoodi", "Kiam Choo", "Jennifer Beattie", "J. Prendki", "Harsha Vashisht", "Re-beca Santamaria-Fernandez", "Luis C. Cobo", "Jarek Wilkiewicz", "David Madras", "Ali Elqursh", "Grant Uy", "Kevin Ramirez", "Matt Harvey", "T. Liechty", "H. Zen", "Jeff Seibert", "Clara Huiyi Hu", "A. Ya. Khorlin", "Maigo Le", "A. Aharoni", "Megan Li", "Lily Wang", "Sandeep Kumar", "Alejandro Lince", "Norman Casagrande", "Jay Hoover", "Dalia El Badawy", "David Soergel", "D. Vnukov", "Matt Miecnikowski", "Jiří Šimša", "Anna Koop", "Praveen Kumar", "Thibault Sellam", "Daniel Vlasic", "Samira Daruki", "Nir Shabat", "John Zhang", "Guolong Su", "Kalpesh Krishna", "Jiageng Zhang", "Jeremiah Liu", "Yi Sun", "Evan Palmer", "Alireza Ghaffarkhah", "Xi Xiong", "Victor Cotruta", "Michael Fink", "Lucas Dixon", "Ashwin Sreevatsa", "Lucas Dixon", "Alek Dimitriev", "Mohsen Jafari", "Remi Crocker", "Nicholas Fitzgerald", "Aviral Kumar", "Nicholas FitzGerald", "Ivan Philips", "Frederick Liu", "Yannie Liang", "Rachel Sterneck", "Alena Repina", "Marcus Wu", "Laura Knight", "Marin Georgiev", "Hyo Lee", "Harry Askham", "A. Chakladar", "Annie Louis", "C. Crous", "Hardie Cate", "Dessie Petrova", "Michael Quinn", "Denese Owusu-Afriyie", "Achintya Singhal", "Nan Wei", "Solomon Kim", "Damien Vincent", "Milad Nasr", "Ilia Shumailov", "Christopher A. Choquette-Choo", "Reiko Tojo", "Shawn Lu", "Diego de Las Casas", "Yuchung Cheng", "Tolga Bolukbasi", "Kather-ine Lee", "S. Fatehi", "R. Ananthanarayanan", "Miteyan Patel", "C. Kaed", "Jing Li", "Jakub Sygnowski", "S. Belle", "Zhe Chen", "Jaclyn Konzelmann", "Siim Põder", "Roopal Garg", "Vinod Koverkathu", "Adam Brown", "Chris Dyer", "Rosanne Liu", "Azade Nova", "Jun Xu", "Junwen Bai", "Slav Petrov", "D. Hassabis", "K. Kavukcuoglu", "Jeffrey Dean", "O. Vinyals", "Alexandra Chronopoulou"], "venue": "arXiv.org", "abstract": "In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.", "year": 2024, "publicationdate": "2024-03-08", "externalids": {}, "doi_lower": null}
{"paper_id": 53100211, "title": "Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference", "author_names": ["M. Riemer", "Ignacio Cases", "R. Ajemian", "Miao Liu", "I. Rish", "Y. Tu", "G. Tesauro"], "venue": "International Conference on Learning Representations", "abstract": "Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.", "year": 2018, "publicationdate": "2018-09-27", "externalids": {}, "doi_lower": null}
{"paper_id": 29169199, "title": "Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting", "author_names": ["H. Ritter", "Aleksandar Botev", "D. Barber"], "venue": "Neural Information Processing Systems", "abstract": "We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework, where we recursively approximate the posterior after every task with a Gaussian, leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode, which is typically intractable for modern architectures. In order to make our method scalable, we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90% test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset, substantially outperforming related methods for overcoming catastrophic forgetting.", "year": 2018, "publicationdate": "2018-05-01", "externalids": {}, "doi_lower": null}
{"paper_id": 234570337, "title": "Adversarial Fine-Tuning of Pretrained Language Models", "author_names": ["Γεώργιος Βερνικός", "Georgios S. Vernikos"], "venue": "", "abstract": null, "year": 2020, "publicationdate": "2020-12-03", "externalids": {"DOI": "10.26240/HEAL.NTUA.19888"}, "doi_lower": "10.26240/heal.ntua.19888"}
{"paper_id": 238743808, "title": "Time Masking for Temporal Language Models", "author_names": ["Guy D. Rosin", "Ido Guy", "Kira Radinsky"], "venue": "Web Search and Data Mining", "abstract": "Our world is constantly evolving, and so is the content on the web. Consequently, our languages, often said to mirror the world, are dynamic in nature. However, most current contextual language models are static and cannot adapt to changes over time. In this work, we propose a temporal contextual language model called TempoBERT, which uses time as an additional context of texts. Our technique is based on modifying texts with temporal information and performing time masking - specific masking for the supplementary time information. We leverage our approach for the tasks of semantic change detection and sentence time prediction, experimenting on diverse datasets in terms of time, size, genre, and language. Our extensive evaluation shows that both tasks benefit from exploiting time masking.", "year": 2021, "publicationdate": "2021-10-12", "externalids": {"DOI": "10.1145/3488560.3498529"}, "doi_lower": "10.1145/3488560.3498529"}
{"paper_id": 261100919, "title": "Code Llama: Open Foundation Models for Code", "author_names": ["Baptiste Rozière", "Jonas Gehring", "Fabian Gloeckle", "Sten Sootla", "Itai Gat", "Xiaoqing Tan", "Yossi Adi", "Jingyu Liu", "Tal Remez", "J. Rapin", "Artyom Kozhevnikov", "I. Evtimov", "Joanna Bitton", "Manish P Bhatt", "Cris-tian Cantón Ferrer", "Aaron Grattafiori", "Wenhan Xiong", "Alexandre D'efossez", "Jade Copet", "Faisal Azhar", "Hugo Touvron", "Louis Martin", "Nicolas Usunier", "Thomas Scialom", "Gabriel Synnaeve"], "venue": "arXiv.org", "abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.", "year": 2023, "publicationdate": "2023-08-24", "externalids": {"DOI": "10.48550/arXiv.2308.12950"}, "doi_lower": "10.48550/arxiv.2308.12950"}
{"paper_id": 264426172, "title": "LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions", "author_names": ["Andre Niyongabo Rubungo", "Craig Arnold", "Barry P. Rand", "Adji Bousso Dieng"], "venue": "arXiv.org", "abstract": "The prediction of crystal properties plays a crucial role in the crystal design process. Current methods for predicting crystal properties focus on modeling crystal structures using graph neural networks (GNNs). Although GNNs are powerful, accurately modeling the complex interactions between atoms and molecules within a crystal remains a challenge. Surprisingly, predicting crystal properties from crystal text descriptions is understudied, despite the rich information and expressiveness that text data offer. One of the main reasons is the lack of publicly available data for this task. In this paper, we develop and make public a benchmark dataset (called TextEdge) that contains text descriptions of crystal structures with their properties. We then propose LLM-Prop, a method that leverages the general-purpose learning capabilities of large language models (LLMs) to predict the physical and electronic properties of crystals from their text descriptions. LLM-Prop outperforms the current state-of-the-art GNN-based crystal property predictor by about 4% in predicting band gap, 3% in classifying whether the band gap is direct or indirect, and 66% in predicting unit cell volume. LLM-Prop also outperforms a finetuned MatBERT, a domain-specific pre-trained BERT model, despite having 3 times fewer parameters. Our empirical results may highlight the current inability of GNNs to capture information pertaining to space group symmetry and Wyckoff sites for accurate crystal property prediction.", "year": 2023, "publicationdate": "2023-10-21", "externalids": {"DOI": "10.48550/arXiv.2310.14029"}, "doi_lower": "10.48550/arxiv.2310.14029"}
{"paper_id": 15350923, "title": "Progressive Neural Networks", "author_names": ["Andrei A. Rusu", "Neil C. Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "J. Kirkpatrick", "K. Kavukcuoglu", "Razvan Pascanu", "R. Hadsell"], "venue": "arXiv.org", "abstract": "Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.", "year": 2016, "publicationdate": "2016-06-15", "externalids": {}, "doi_lower": null}
{"paper_id": 199370376, "title": "An Adversarial Winograd Schema Challenge at Scale", "author_names": ["Keisuke Sakaguchi", "Ronan Le Bras", "Chandra Bhagavatula", "Yejin Choi"], "venue": "", "abstract": null, "year": 2019, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 239009562, "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "author_names": ["Victor Sanh", "Albert Webson", "Colin Raffel", "Stephen H. Bach", "Lintang Sutawika", "Zaid Alyafeai", "Antoine Chaffin", "Arnaud Stiegler", "Teven Le Scao", "Arun Raja", "Manan Dey", "M Saiful Bari", "Canwen Xu", "Urmish Thakker", "S. Sharma", "Eliza Szczechla", "Taewoon Kim", "Gunjan Chhablani", "Nihal V. Nayak", "Debajyoti Datta", "Jonathan D. Chang", "Mike Tian-Jian Jiang", "Han Wang", "Matteo Manica", "Sheng Shen", "Zheng-Xin Yong", "Harshit Pandey", "Rachel Bawden", "Thomas Wang", "Trishala Neeraj", "Jos Rozen", "Abheesht Sharma", "Andrea Santilli", "Thibault Févry", "Jason Alan Fries", "R. Teehan", "Stella Biderman", "Leo Gao", "T. Bers", "Thomas Wolf", "Alexander M. Rush"], "venue": "arXiv.org", "abstract": "Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.", "year": 2021, "publicationdate": "2021-10-15", "externalids": {}, "doi_lower": null}
{"paper_id": 28695052, "title": "Proximal Policy Optimization Algorithms", "author_names": ["John Schulman", "Filip Wolski", "Prafulla Dhariwal", "Alec Radford", "Oleg Klimov"], "venue": "arXiv.org", "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.", "year": 2017, "publicationdate": "2017-07-20", "externalids": {}, "doi_lower": null}
{"paper_id": 232233599, "title": "Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence", "author_names": ["Tal Schuster", "Adam Fisch", "R. Barzilay"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness—improving accuracy by 10% on adversarial fact verification and 6% on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.", "year": 2021, "publicationdate": "2021-03-15", "externalids": {"DOI": "10.18653/V1/2021.NAACL-MAIN.52"}, "doi_lower": "10.18653/v1/2021.naacl-main.52"}
{"paper_id": 260547030, "title": "Progress & Compress : A scalable framework for continual learning", "author_names": [], "venue": "", "abstract": null, "year": 2018, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 252815378, "title": "Fine-tuned Language Models are Continual Learners", "author_names": ["Thomas Scialom", "Tuhin Chakrabarty", "S. Muresan"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions and that models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets.To address this limitation, we argue that a model should be able to keep extending its knowledge and abilities, without forgetting previous skills. In spite of the limited success of Continual Learning, we show that Fine-tuned Language Models can be continual learners.We empirically investigate the reason for this success and conclude that Continual Learning emerges from self-supervision pre-training. Our resulting model Continual-T0 (CT0) is able to learn 8 new diverse language generation tasks, while still maintaining good performance on previous tasks, spanning in total of 70 datasets. Finally, we show that CT0 is able to combine instructions in ways it was never trained for, demonstrating some level of instruction compositionality.", "year": 2022, "publicationdate": "2022-05-24", "externalids": {"DOI": "10.18653/v1/2022.emnlp-main.410"}, "doi_lower": "10.18653/v1/2022.emnlp-main.410"}
{"paper_id": 258685646, "title": "Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis", "author_names": ["Agam Shah", "Suvan Paturi", "S. Chava"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a major driver of financial market returns. We construct the largest tokenized and annotated dataset of FOMC speeches, meeting minutes, and press conference transcripts in order to understand how monetary policy influences financial markets. In this study, we develop a novel task of hawkish-dovish classification and benchmark various pre-trained language models on the proposed dataset. Using the best-performing model (RoBERTa-large), we construct a measure of monetary policy stance for the FOMC document release days. To evaluate the constructed measure, we study its impact on the treasury market, stock market, and macroeconomic indicators. Our dataset, models, and code are publicly available on Huggingface and GitHub under CC BY-NC 4.0 license.", "year": 2023, "publicationdate": "2023-05-13", "externalids": {"DOI": "10.48550/arXiv.2305.07972"}, "doi_lower": "10.48550/arxiv.2305.07972"}
{"paper_id": 259224492, "title": "Class-Incremental Learning based on Label Generation", "author_names": ["Yijia Shao", "Yiduo Guo", "Dongyan Zhao", "Bin Liu"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.", "year": 2023, "publicationdate": "2023-06-22", "externalids": {"DOI": "10.48550/arXiv.2306.12619"}, "doi_lower": "10.48550/arxiv.2306.12619"}
{"paper_id": 12462234, "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "author_names": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc V. Le", "Geoffrey E. Hinton", "J. Dean"], "venue": "International Conference on Learning Representations", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "year": 2017, "publicationdate": "2017-01-23", "externalids": {}, "doi_lower": null}
{"paper_id": 267547938, "title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains", "author_names": ["Junhong Shen", "Neil Tenenholtz", "James Hall", "David Alvarez-Melis", "Nicoló Fusi"], "venue": "International Conference on Machine Learning", "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from task functions, our method enables zero-shot generalization to unseen problems through diverse combinations of the input tags. It also boosts LLM's performance in various specialized domains, such as predicting protein or chemical properties and modeling drug-target interactions, outperforming expert models tailored to these tasks.", "year": 2024, "publicationdate": "2024-02-06", "externalids": {"DOI": "10.48550/arXiv.2402.05140"}, "doi_lower": "10.48550/arxiv.2402.05140"}
{"paper_id": 85553602, "title": "Towards VQA Models That Can Read", "author_names": ["Amanpreet Singh", "Vivek Natarajan", "Meet Shah", "Yu Jiang", "Xinlei Chen", "Dhruv Batra", "Devi Parikh", "Marcus Rohrbach"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Studies have shown that a dominant class of questions asked by visually impaired users on images of their surroundings involves reading text in the image. But today’s VQA models can not read! Our paper takes a first step towards addressing this problem. First, we introduce a new “TextVQA” dataset to facilitate progress on this important problem. Existing datasets either have a small proportion of questions about text (e.g., the VQA dataset) or are too small (e.g., the VizWiz dataset). TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Second, we introduce a novel model architecture that reads text in the image, reasons about it in the context of the image and the question, and predicts an answer which might be a deduction based on the text and the image or composed of the strings found in the image. Consequently, we call our approach Look, Read, Reason & Answer (LoRRA). We show that LoRRA outperforms existing state-of-the-art VQA models on our TextVQA dataset. We find that the gap between human performance and machine performance is significantly larger on TextVQA than on VQA 2.0, suggesting that TextVQA is well-suited to benchmark progress along directions complementary to VQA 2.0.", "year": 2019, "publicationdate": "2019-04-18", "externalids": {"DOI": "10.1109/CVPR.2019.00851"}, "doi_lower": "10.1109/cvpr.2019.00851"}
{"paper_id": 213938729, "title": "Editable Neural Networks", "author_names": ["A. Sinitsin", "Vsevolod Plokhotnyuk", "Dmitriy V. Pyrkin", "Sergei Popov", "Artem Babenko"], "venue": "International Conference on Learning Representations", "abstract": "These days deep neural networks are ubiquitously used in a wide range of tasks, from image classification and machine translation to face identification and self-driving cars. In many applications, a single model error can lead to devastating financial, reputational and even life-threatening consequences. Therefore, it is crucially important to correct model mistakes quickly as they appear. In this work, we investigate the problem of neural network editing - how one can efficiently patch a mistake of the model on a particular sample, without influencing the model behavior on other samples. Namely, we propose Editable Training, a model-agnostic training technique that encourages fast editing of the trained model. We empirically demonstrate the effectiveness of this method on large-scale image classification and machine translation tasks.", "year": 2020, "publicationdate": "2020-04-01", "externalids": {}, "doi_lower": null}
{"paper_id": 266184455, "title": "D I C A L J O U R N A L R H O D E I S L A N D", "author_names": ["MD GYAN PAREEK", "MD OLIVE W. TANG", "MBChB CHRISTOPHER OWINO", "MBChB ANN MUTUGI", "MD Mph JIE TANG", "ScD CHRISTINA A. RAKER", "MD MATTHEW R. LYNCH", "MD SARAH MOORE", "MD Gyan EMILY BARRY", "M. Pareek", "BA REBECCA WALES", "MD Gyan DANIEL SANFORD", "M. Pareek", "M. Tang", "D. Ournalrhodeislan", "December", "Rhode Island Medical Journal", "MD SANDIPAN SHRINGI", "Mbbs SAIRAH SHARIF", "M. R", "MD IHA KAUL", "MD Pgy Faizanahmed Munshi", "MD Pgy Suhas Penukonda", "Resident"], "venue": "", "abstract": null, "year": null, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 262822572, "title": "ConPET: Continual Parameter-Efficient Tuning for Large Language Models", "author_names": ["Chenyan Song", "Xu Han", "Zheni Zeng", "Kuai Li", "Chen Chen", "Zhiyuan Liu", "Maosong Sun", "Taojiannan Yang"], "venue": "arXiv.org", "abstract": "Continual learning necessitates the continual adaptation of models to newly emerging tasks while minimizing the catastrophic forgetting of old ones. This is extremely challenging for large language models (LLMs) with vanilla full-parameter tuning due to high computation costs, memory consumption, and forgetting issue. Inspired by the success of parameter-efficient tuning (PET), we propose Continual Parameter-Efficient Tuning (ConPET), a generalizable paradigm for continual task adaptation of LLMs with task-number-independent training complexity. ConPET includes two versions with different application scenarios. First, Static ConPET can adapt former continual learning methods originally designed for relatively smaller models to LLMs through PET and a dynamic replay strategy, which largely reduces the tuning costs and alleviates the over-fitting and forgetting issue. Furthermore, to maintain scalability, Dynamic ConPET adopts separate PET modules for different tasks and a PET module selector for dynamic optimal selection. In our extensive experiments, the adaptation of Static ConPET helps multiple former methods reduce the scale of tunable parameters by over 3,000 times and surpass the PET-only baseline by at least 5 points on five smaller benchmarks, while Dynamic ConPET gains its advantage on the largest dataset. The codes and datasets are available at https://github.com/Raincleared-Song/ConPET.", "year": 2023, "publicationdate": "2023-09-26", "externalids": {"DOI": "10.48550/arXiv.2309.14763"}, "doi_lower": "10.48550/arxiv.2309.14763"}
{"paper_id": 267760221, "title": "Code Needs Comments: Enhancing Code LLMs with Comment Augmentation", "author_names": ["Demin Song", "Honglin Guo", "Yunhua Zhou", "Shuhao Xing", "Yudong Wang", "Zifan Song", "Wenwei Zhang", "Qipeng Guo", "Hang Yan", "Xipeng Qiu", "Dahua Lin"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The programming skill is one crucial ability for Large Language Models (LLMs), necessitating a deep understanding of programming languages (PLs) and their correlation with natural languages (NLs). We examine the impact of pre-training data on code-focused LLMs' performance by assessing the comment density as a measure of PL-NL alignment. Given the scarcity of code-comment aligned data in pre-training corpora, we introduce a novel data augmentation method that generates comments for existing code, coupled with a data filtering strategy that filters out code data poorly correlated with natural language. We conducted experiments on three code-focused LLMs and observed consistent improvements in performance on two widely-used programming skill benchmarks. Notably, the model trained on the augmented data outperformed both the model used for generating comments and the model further trained on the data without augmentation.", "year": 2024, "publicationdate": "2024-02-20", "externalids": {"DOI": "10.48550/arXiv.2402.13013"}, "doi_lower": "10.48550/arxiv.2402.13013"}
{"paper_id": 266177048, "title": "Efficient Continue Training of Temporal Language Model with Structural Information", "author_names": ["Zhao-yu Su", "Juntao Li", "Zikang Zhang", "Zihan Zhou", "Min Zhang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": ",", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.findings-emnlp.418"}, "doi_lower": "10.18653/v1/2023.findings-emnlp.418"}
{"paper_id": 254877742, "title": "A Survey on Pretrained Language Models for Neural Code Intelligence", "author_names": ["Yichen Xu", "Yanqiao Zhu"], "venue": "arXiv.org", "abstract": "As the complexity of modern software continues to escalate, software engineering has become an increasingly daunting and error-prone endeavor. In recent years, the field of Neural Code Intelligence (NCI) has emerged as a promising solution, leveraging the power of deep learning techniques to tackle analytical tasks on source code with the goal of improving programming efficiency and minimizing human errors within the software industry. Pretrained language models have become a dominant force in NCI research, consistently delivering state-of-the-art results across a wide range of tasks, including code summarization, generation, and translation. In this paper, we present a comprehensive survey of the NCI domain, including a thorough review of pretraining techniques, tasks, datasets, and model architectures. We hope this paper will serve as a bridge between the natural language and programming language communities, offering insights for future research in this rapidly evolving field.", "year": 2022, "publicationdate": "2022-12-20", "externalids": {"DOI": "10.48550/arXiv.2212.10079"}, "doi_lower": "10.48550/arxiv.2212.10079"}
{"paper_id": 198968327, "title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "author_names": ["Yu Sun", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Hao Tian", "Hua Wu", "Haifeng Wang"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 model outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.", "year": 2019, "publicationdate": "2019-07-29", "externalids": {"DOI": "10.1609/AAAI.V34I05.6428"}, "doi_lower": "10.1609/aaai.v34i05.6428"}
{"paper_id": 269137552, "title": "Pretraining and Updates of Domain-Specific LLM: A Case Study in the Japanese Business Domain", "author_names": ["Kosuke Takahashi", "Takahiro Omi", "Kosuke Arima", "Tatsuya Ishigaki"], "venue": "Pacific Asia Conference on Language, Information and Computation", "abstract": "The development of Large Language Models (LLMs) in various languages has been advancing, but the combination of non-English languages with domain-specific contexts remains underexplored. This paper presents our findings from training and evaluating a Japanese business domain-specific LLM designed to better understand business-related documents, such as the news on current affairs, technical reports, and patents. Additionally, LLMs in this domain require regular updates to incorporate the most recent knowledge. Therefore, we also report our findings from the first experiments and evaluations involving updates to this LLM using the latest article data, which is an important problem setting that has not been addressed in previous research. From our experiments on a newly created benchmark dataset for question answering in the target domain, we found that (1) our pretrained model improves QA accuracy without losing general knowledge, and (2) a proper mixture of the latest and older texts in the training data for the update is necessary. Our pretrained model and business domain benchmark are publicly available to support further studies.", "year": 2024, "publicationdate": "2024-04-12", "externalids": {}, "doi_lower": null}
{"paper_id": 257279790, "title": "Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study", "author_names": ["Mingxu Tao", "Yansong Feng", "Dongyan Zhao"], "venue": "International Conference on Learning Representations", "abstract": "Large pre-trained language models help to achieve state of the art on a variety of natural language processing (NLP) tasks, nevertheless, they still suffer from forgetting when incrementally learning a sequence of tasks. To alleviate this problem, recent works enhance existing models by sparse experience replay and local adaption, which yield satisfactory performance. However, in this paper we find that pre-trained language models like BERT have a potential ability to learn sequentially, even without any sparse memory replay. To verify the ability of BERT to maintain old knowledge, we adopt and re-finetune single-layer probe networks with the parameters of BERT fixed. We investigate the models on two types of NLP tasks, text classification and extractive question answering. Our experiments reveal that BERT can actually generate high quality representations for previously learned tasks in a long term, under extremely sparse replay or even no replay. We further introduce a series of novel methods to interpret the mechanism of forgetting and how memory rehearsal plays a significant role in task incremental learning, which bridges the gap between our new discovery and previous studies about catastrophic forgetting.", "year": 2023, "publicationdate": "2023-03-02", "externalids": {"DOI": "10.48550/arXiv.2303.01081"}, "doi_lower": "10.48550/arxiv.2303.01081"}
{"paper_id": 266818336, "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism", "author_names": ["DeepSeek-AI Xiao Bi", "Deli Chen", "Guanting Chen", "Shanhuang Chen", "Damai Dai", "C. Deng", "Honghui Ding", "Kai Dong", "Qiushi Du", "Zhe Fu", "Huazuo Gao", "Kaige Gao", "Wenjun Gao", "Ruiqi Ge", "Kang Guan", "Daya Guo", "Jianzhong Guo", "Guangbo Hao", "Zhewen Hao", "Ying He", "Wen-Hui Hu", "Panpan Huang", "Erhang Li", "Guowei Li", "Jiashi Li", "Yao Li", "Y. K. Li", "W. Liang", "Fangyun Lin", "A. Liu", "Bo Liu (Benjamin Liu)", "Wen Liu", "Xiaodong Liu", "Xin Liu", "Yiyuan Liu", "Haoyu Lu", "Shanghao Lu", "Fuli Luo", "Shirong Ma", "X. Nie", "Tian Pei", "Yishi Piao", "Junjie Qiu", "Hui Qu", "Tongzheng Ren", "Z. Ren", "C. Ruan", "Zhangli Sha", "Zhihong Shao", "Jun-Mei Song", "Xuecheng Su", "Jingxiang Sun", "Yaofeng Sun", "Min Tang", "Bing-Li Wang", "Peiyi Wang", "Shiyu Wang", "Yaohui Wang", "Yongji Wang", "Tong Wu", "Yu Wu", "Xin Xie", "Zhenda Xie", "Ziwei Xie", "Yi Xiong", "Hanwei Xu", "R. X. Xu", "Yanhong Xu", "Dejian Yang", "Yu-mei You", "Shuiping Yu", "Xin-yuan Yu", "Bo Zhang", "Haowei Zhang", "Lecong Zhang", "Liyue Zhang", "Mingchuan Zhang", "Minghu Zhang", "Wentao Zhang", "Yichao Zhang", "Chenggang Zhao", "Yao Zhao", "Shangyan Zhou", "Shunfeng Zhou", "Qihao Zhu", "Yuheng Zou"], "venue": "arXiv.org", "abstract": "The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.", "year": 2024, "publicationdate": "2024-01-05", "externalids": {}, "doi_lower": null}
{"paper_id": 269921376, "title": "Imp: Highly Capable Large Multimodal Models for Mobile Devices", "author_names": ["Zhenwei Shao", "Zhou Yu", "Jun Yu", "Xuecheng Ouyang", "Lihao Zheng", "Zhenbiao Gai", "Mingyang Wang", "Zhenzhong Kuang", "Jiajun Ding"], "venue": "IEEE transactions on multimedia", "abstract": "By harnessing the capabilities of large language models (LLMs), recent large multimodal models (LMMs) have shown remarkable versatility in open-world multimodal understanding. Nevertheless, they are usually parameter-heavy and computation-intensive, thus hindering their applicability in resource-constrained scenarios. To this end, several lightweight LMMs have been proposed successively to maximize the capabilities under constrained scale (e.g., 3B). Despite the encouraging results achieved by these methods, most of them only focus on one or two aspects of the design space, and the key design choices that influence model capability have not yet been thoroughly investigated. In this paper, we conduct a systematic study for lightweight LMMs from the aspects of model architecture, training strategy, and training data. Based on our findings, we obtain Imp—a family of highly capable LMMs at the 2B$\\sim$4B scales. Notably, our Imp-3B model steadily outperforms all the existing lightweight LMMs of similar size, and even surpasses the state-of-the-art LMMs at the 13B scale. With low-bit quantization and resolution reduction techniques, our Imp model can be deployed on a Qualcomm Snapdragon 8Gen3 mobile chip with a high inference speed of about 13 tokens/s.", "year": 2024, "publicationdate": "2024-05-20", "externalids": {"DOI": "10.1109/TMM.2025.3557680"}, "doi_lower": "10.1109/tmm.2025.3557680"}
{"paper_id": 258588247, "title": "StarCoder: may the source be with you!", "author_names": ["Raymond Li", "Loubna Ben Allal", "Yangtian Zi", "Niklas Muennighoff", "Denis Kocetkov", "Chenghao Mou", "Marc Marone", "Christopher Akiki", "Jia Li", "Jenny Chim", "Qian Liu", "Evgenii Zheltonozhskii", "Terry Yue Zhuo", "Thomas Wang", "Olivier Dehaene", "Mishig Davaadorj", "J. Lamy-Poirier", "João Monteiro", "Oleh Shliazhko", "Nicolas Gontier", "Nicholas Meade", "A. Zebaze", "Ming-Ho Yee", "Logesh Kumar Umapathi", "Jian Zhu", "Benjamin Lipkin", "Muhtasham Oblokulov", "Zhiruo Wang", "Rudra Murthy", "J. Stillerman", "Siva Sankalp Patel", "Dmitry Abulkhanov", "Marco Zocca", "Manan Dey", "Zhihan Zhang", "N. Fahmy", "Urvashi Bhattacharyya", "W. Yu", "Swayam Singh", "Sasha Luccioni", "Paulo Villegas", "M. Kunakov", "Fedor Zhdanov", "Manuel Romero", "Tony Lee", "Nadav Timor", "Jennifer Ding", "Claire Schlesinger", "Hailey Schoelkopf", "Jana Ebert", "Tri Dao", "Mayank Mishra", "A. Gu", "Jennifer Robinson", "Carolyn Jane Anderson", "Brendan Dolan-Gavitt", "Danish Contractor", "Siva Reddy", "Daniel Fried", "Dzmitry Bahdanau", "Yacine Jernite", "Carlos Muñoz Ferrandis", "Sean M. Hughes", "Thomas Wolf", "Arjun Guha", "L. V. Werra", "H. D. Vries"], "venue": "Trans. Mach. Learn. Res.", "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.", "year": 2023, "publicationdate": "2023-05-09", "externalids": {}, "doi_lower": null}
{"paper_id": 268063676, "title": "StarCoder 2 and The Stack v2: The Next Generation", "author_names": ["Anton Lozhkov", "Raymond Li", "Loubna Ben Allal", "Federico Cassano", "J. Lamy-Poirier", "Nouamane Tazi", "Ao Tang", "Dmytro Pykhtar", "Jiawei Liu", "Yuxiang Wei", "Tianyang Liu", "Max Tian", "Denis Kocetkov", "Arthur Zucker", "Younes Belkada", "Zijian Wang", "Qian Liu", "Dmitry Abulkhanov", "Indraneil Paul", "Zhuang Li", "Wen-Ding Li", "Megan L. Risdal", "Jia Li", "Jian Zhu", "Terry Yue Zhuo", "Evgenii Zheltonozhskii", "Nii Osae Osae Dade", "W. Yu", "Lucas Krauss", "Naman Jain", "Yixuan Su", "Xuanli He", "Manan Dey", "Edoardo Abati", "Yekun Chai", "Niklas Muennighoff", "Xiangru Tang", "Muhtasham Oblokulov", "C. Akiki", "Marc Marone", "Chenghao Mou", "Mayank Mishra", "A. Gu", "Binyuan Hui", "Tri Dao", "A. Zebaze", "Olivier Dehaene", "N. Patry", "Canwen Xu", "Julian J. McAuley", "Han Hu", "Torsten Scholak", "Sébastien Paquet", "Jennifer Robinson", "C. Anderson", "Nicolas Chapados", "M. Patwary", "Nima Tajbakhsh", "Yacine Jernite", "Carlos Muñoz Ferrandis", "Lingming Zhang", "Sean Hughes", "Thomas Wolf", "Arjun Guha", "L. V. Werra", "H. D. Vries"], "venue": "arXiv.org", "abstract": "The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.", "year": 2024, "publicationdate": "2024-02-29", "externalids": {"DOI": "10.48550/arXiv.2402.19173"}, "doi_lower": "10.48550/arxiv.2402.19173"}
{"paper_id": 4711425, "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification", "author_names": ["James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.", "year": 2018, "publicationdate": "2018-03-14", "externalids": {"DOI": "10.18653/v1/N18-1074"}, "doi_lower": "10.18653/v1/n18-1074"}
{"paper_id": 267034871, "title": "ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change", "author_names": ["David Thulke", "Yingbo Gao", "Petrus Pelser", "Rein Brune", "Rricha Jalota", "Floris Fok", "Michael Ramos", "Ian van Wyk", "Abdallah Nasir", "Hayden Goldstein", "Taylor Tragemann", "Katie Nguyen", "Ariana Fowler", "Andrew Stanco", "Jon Gabriel", "Jordan Taylor", "Dean Moro", "Evgenii Tsymbalov", "Juliette de Waal", "E. Matusov", "Mudar Yaghi", "Mohammad Shihadah", "Hermann Ney", "Christian Dugast", "Jonathan Dotan", "Daniel Erasmus"], "venue": "arXiv.org", "abstract": "This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to natively multilingual models while being easier to scale to a large number of languages. Further, to address the intrinsic interdisciplinary aspect of climate change we consider different research perspectives. Therefore, the model can produce in-depth answers focusing on different perspectives in addition to an overall answer. We propose a suite of automatic climate-specific benchmarks to evaluate LLMs. On these benchmarks, ClimateGPT-7B performs on par with the ten times larger Llama-2-70B Chat model while not degrading results on general domain benchmarks. Our human evaluation confirms the trends we saw in our benchmarks. All models were trained and evaluated using renewable energy and are released publicly.", "year": 2024, "publicationdate": "2024-01-17", "externalids": {"DOI": "10.48550/arXiv.2401.09646"}, "doi_lower": "10.48550/arxiv.2401.09646"}
{"paper_id": 257219404, "title": "LLaMA: Open and Efficient Foundation Language Models", "author_names": ["Hugo Touvron", "Thibaut Lavril", "Gautier Izacard", "Xavier Martinet", "M. Lachaux", "Timothée Lacroix", "Baptiste Rozière", "Naman Goyal", "Eric Hambro", "Faisal Azhar", "Aur'elien Rodriguez", "Armand Joulin", "Edouard Grave", "Guillaume Lample"], "venue": "arXiv.org", "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.", "year": 2023, "publicationdate": "2023-02-27", "externalids": {}, "doi_lower": null}
{"paper_id": 259950998, "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "author_names": ["Hugo Touvron", "Louis Martin", "Kevin R. Stone", "Peter Albert", "Amjad Almahairi", "Yasmine Babaei", "Niko-lay Bashlykov", "Soumya Batra", "Prajjwal Bhargava", "Shruti Bhosale", "D. Bikel", "Lukas Blecher", "Cris-tian Cantón Ferrer", "Moya Chen", "Guillem Cucurull", "David Esiobu", "Jude Fernandes", "J. Fu", "Wenyin Fu", "Brian Fuller", "Cynthia Gao", "Vedanuj Goswami", "Naman Goyal", "A. Hartshorn", "Saghar Hosseini", "Rui Hou", "Hakan Inan", "Marcin Kardas", "Viktor Kerkez", "Madian Khabsa", "Isabel M. Kloumann", "A. Korenev", "Punit Singh Koura", "M. Lachaux", "Thibaut Lavril", "Jenya Lee", "Diana Liskovich", "Yinghai Lu", "Yuning Mao", "Xavier Martinet", "Todor Mihaylov", "Pushkar Mishra", "Igor Molybog", "Yixin Nie", "Andrew Poulton", "J. Reizenstein", "Rashi Rungta", "Kalyan Saladi", "A. Schelten", "Ruan Silva", "Eric Michael Smith", "R. Subramanian", "Xia Tan", "Binh Tang", "Ross Taylor", "Adina Williams", "Jian Xiang Kuan", "Puxin Xu", "Zhengxu Yan", "Iliyan Zarov", "Yuchen Zhang", "Angela Fan", "M. Kambadur", "Sharan Narang", "Aur'elien Rodriguez", "Robert Stojnic", "Sergey Edunov", "Thomas Scialom"], "venue": "arXiv.org", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "year": 2023, "publicationdate": "2023-07-18", "externalids": {}, "doi_lower": null}
{"paper_id": 254335115, "title": "Three types of incremental learning", "author_names": ["Gido M. van de Ven", "T. Tuytelaars", "A. Tolias"], "venue": "Nat. Mac. Intell.", "abstract": "Incrementally learning new information from a non-stationary stream of data, referred to as ‘continual learning’, is a key feature of natural intelligence, but a challenging problem for deep neural networks. In recent years, numerous deep learning methods for continual learning have been proposed, but comparing their performances is difficult due to the lack of a common framework. To help address this, we describe three fundamental types, or ‘scenarios’, of continual learning: task-incremental, domain-incremental and class-incremental learning. Each of these scenarios has its own set of challenges. To illustrate this, we provide a comprehensive empirical comparison of currently used continual learning strategies, by performing the Split MNIST and Split CIFAR-100 protocols according to each scenario. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of the effectiveness of different strategies. The proposed categorization aims to structure the continual learning field, by forming a key foundation for clearly defining benchmark problems. A challenge for any machine learning system is to continually adapt to new data. While methods to address this issue are developed, their performance is hard to compare. A new framework to facilitate benchmarking divides approaches into three categories, defined by whether models need to adapt to new tasks, domains or classes.", "year": 2022, "publicationdate": "2022-12-01", "externalids": {"DOI": "10.1038/s42256-022-00568-3"}, "doi_lower": "10.1038/s42256-022-00568-3"}
{"paper_id": 36417848, "title": "S T R A N G E A T T R A C T O R S A N D D Y N A M I C A L M O D E L S", "author_names": ["L. P. Shil'nikov", "L. P. Skil'nikov"], "venue": "", "abstract": null, "year": 2010, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 2204603, "title": "TL;DR: Mining Reddit to Learn Automatic Summarization", "author_names": ["Michael Völske", "Martin Potthast", "S. Syed", "Benno Stein"], "venue": "NFiS@EMNLP", "abstract": "Recent advances in automatic text summarization have used deep neural networks to generate high-quality abstractive summaries, but the performance of these models strongly depends on large amounts of suitable training data. We propose a new method for mining social media for author-provided summaries, taking advantage of the common practice of appending a “TL;DR” to long posts. A case study using a large Reddit crawl yields the Webis-TLDR-17 dataset, complementing existing corpora primarily from the news genre. Our technique is likely applicable to other social media sites and general web crawls.", "year": 2017, "publicationdate": "2017-09-01", "externalids": {"DOI": "10.18653/v1/W17-4508"}, "doi_lower": "10.18653/v1/w17-4508"}
{"paper_id": 258832435, "title": "GPT-SW3: An Autoregressive Language Model for the Nordic Languages", "author_names": ["Ariel Ekgren", "Amaru Cuba Gyllensten", "F. Stollenwerk", "Joey Öhman", "T. Isbister", "Evangelia Gogoulou", "F. Carlsson", "Alice Heiman", "Judit Casademont", "Magnus Sahlgren"], "venue": "arXiv.org", "abstract": "This paper details the process of developing the first native large generative language model for the Nordic languages, GPT-SW3. We cover all parts of the development process, from data collection and processing, training configuration and instruction finetuning, to evaluation and considerations for release strategies. We hope that this paper can serve as a guide and reference for other researchers that undertake the development of large generative models for smaller languages.", "year": 2023, "publicationdate": "2023-05-22", "externalids": {"DOI": "10.48550/arXiv.2305.12987"}, "doi_lower": "10.48550/arxiv.2305.12987"}
{"paper_id": 250526805, "title": "CoSCL: Cooperation of Small Continual Learners is Stronger than a Big One", "author_names": ["Liyuan Wang", "Xingxing Zhang", "Qian Li", "Jun Zhu", "Yi Zhong"], "venue": "European Conference on Computer Vision", "abstract": "Continual learning requires incremental compatibility with a sequence of tasks. However, the design of model architecture remains an open question: In general, learning all tasks with a shared set of parameters suffers from severe interference between tasks; while learning each task with a dedicated parameter subspace is limited by scalability. In this work, we theoretically analyze the generalization errors for learning plasticity and memory stability in continual learning, which can be uniformly upper-bounded by (1) discrepancy between task distributions, (2) flatness of loss landscape and (3) cover of parameter space. Then, inspired by the robust biological learning system that processes sequential experiences with multiple parallel compartments, we propose Cooperation of Small Continual Learners (CoSCL) as a general strategy for continual learning. Specifically, we present an architecture with a fixed number of narrower sub-networks to learn all incremental tasks in parallel, which can naturally reduce the two errors through improving the three components of the upper bound. To strengthen this advantage, we encourage to cooperate these sub-networks by penalizing the difference of predictions made by their feature representations. With a fixed parameter budget, CoSCL can improve a variety of representative continual learning approaches by a large margin (e.g., up to 10.64% on CIFAR-100-SC, 9.33% on CIFAR-100-RS, 11.45% on CUB-200-2011 and 6.72% on Tiny-ImageNet) and achieve the new state-of-the-art performance.", "year": 2022, "publicationdate": "2022-07-13", "externalids": {"DOI": "10.48550/arXiv.2207.06543"}, "doi_lower": "10.48550/arxiv.2207.06543"}
{"paper_id": 256459333, "title": "A Comprehensive Survey of Continual Learning: Theory, Method and Application", "author_names": ["Liyuan Wang", "Xingxing Zhang", "Hang Su", "Jun Zhu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance drop of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.", "year": 2023, "publicationdate": "2023-01-31", "externalids": {"DOI": "10.1109/TPAMI.2024.3367329"}, "doi_lower": "10.1109/tpami.2024.3367329"}
{"paper_id": 269982715, "title": "WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models", "author_names": ["Peng Wang", "Zexi Li", "Ningyu Zhang", "Ziwen Xu", "Yunzhi Yao", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "venue": "Neural Information Processing Systems", "abstract": "Large language models (LLMs) need knowledge updates to meet the ever-growing world facts and correct the hallucinated responses, facilitating the methods of lifelong model editing. Where the updated knowledge resides in memories is a fundamental question for model editing. In this paper, we find that editing either long-term memory (direct model parameters) or working memory (non-parametric knowledge of neural network activations/representations by retrieval) will result in an impossible triangle -- reliability, generalization, and locality can not be realized together in the lifelong editing settings. For long-term memory, directly editing the parameters will cause conflicts with irrelevant pretrained knowledge or previous edits (poor reliability and locality). For working memory, retrieval-based activations can hardly make the model understand the edits and generalize (poor generalization). Therefore, we propose WISE to bridge the gap between memories. In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge. We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query. For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters, and are subsequently merged into a shared memory without conflicts. Extensive experiments show that WISE can outperform previous model editing methods and overcome the impossible triangle under lifelong model editing of question answering, hallucination, and out-of-distribution settings across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is available at https://github.com/zjunlp/EasyEdit.", "year": 2024, "publicationdate": "2024-05-23", "externalids": {"DOI": "10.48550/arXiv.2405.14768"}, "doi_lower": "10.48550/arxiv.2405.14768"}
{"paper_id": 211031933, "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters", "author_names": ["Ruize Wang", "Duyu Tang", "Nan Duan", "Zhongyu Wei", "Xuanjing Huang", "Jianshu Ji", "Guihong Cao", "Daxin Jiang", "Ming Zhou"], "venue": "Findings", "abstract": "We study the problem of injecting knowledge into large pre-trained models like BERT and RoBERTa. Existing methods typically update the original parameters of pre-trained models when injecting knowledge. However, when multiple kinds of knowledge are injected, they may suffer from catastrophic forgetting. To address this, we propose K-Adapter, which remains the original parameters of the pre-trained model fixed and supports continual knowledge infusion. Taking RoBERTa as the pre-trained model, K-Adapter has a neural adapter for each kind of infused knowledge, like a plug-in connected to RoBERTa. There is no information flow between different adapters, thus different adapters are efficiently trained in a distributed way. We inject two kinds of knowledge, including factual knowledge obtained from automatically aligned text-triplets on Wikipedia and Wikidata, and linguistic knowledge obtained from dependency parsing. Results on three knowledge-driven tasks (total six datasets) including relation classification, entity typing and question answering demonstrate that each adapter improves the performance, and the combination of both adapters brings further improvements. Probing experiments further indicate that K-Adapter captures richer factual and commonsense knowledge than RoBERTa.", "year": 2020, "publicationdate": "2020-02-05", "externalids": {"DOI": "10.18653/v1/2021.findings-acl.121"}, "doi_lower": "10.18653/v1/2021.findings-acl.121"}
{"paper_id": 264426441, "title": "Orthogonal Subspace Learning for Language Model Continual Learning", "author_names": ["Xiao Wang", "Tianze Chen", "Qiming Ge", "Han Xia", "Rong Bao", "Rui Zheng", "Qi Zhang", "Tao Gui", "Xuanjing Huang"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Benefiting from massive corpora and advanced hardware, large language models (LLMs) exhibit remarkable capabilities in language understanding and generation. However, their performance degrades in scenarios where multiple tasks are encountered sequentially, also known as catastrophic forgetting. In this paper, we propose orthogonal low-rank adaptation (O-LoRA), a simple and efficient approach for continual learning in language models, effectively mitigating catastrophic forgetting while learning new tasks. Specifically, O-LoRA learns tasks in different (low-rank) vector subspaces that are kept orthogonal to each other in order to minimize interference. Our method induces only marginal additional parameter costs and requires no user data storage for replay. Experimental results on continual learning benchmarks show that our method outperforms state-of-the-art methods. Furthermore, compared to previous approaches, our method excels in preserving the generalization ability of LLMs on unseen tasks.", "year": 2023, "publicationdate": "2023-10-22", "externalids": {"DOI": "10.48550/arXiv.2310.14152"}, "doi_lower": "10.48550/arxiv.2310.14152"}
{"paper_id": 263830425, "title": "TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models", "author_names": ["Xiao Wang", "Yuan Zhang", "Tianze Chen", "Songyang Gao", "Senjie Jin", "Xianjun Yang", "Zhiheng Xi", "Rui Zheng", "Yicheng Zou", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "venue": "arXiv.org", "abstract": "Aligned large language models (LLMs) demonstrate exceptional capabilities in task-solving, following instructions, and ensuring safety. However, the continual learning aspect of these aligned LLMs has been largely overlooked. Existing continual learning benchmarks lack sufficient challenge for leading aligned LLMs, owing to both their simplicity and the models' potential exposure during instruction tuning. In this paper, we introduce TRACE, a novel benchmark designed to evaluate continual learning in LLMs. TRACE consists of 8 distinct datasets spanning challenging tasks including domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. All datasets are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Our experiments show that after training on TRACE, aligned LLMs exhibit significant declines in both general ability and instruction-following capabilities. For example, the accuracy of llama2-chat 13B on gsm8k dataset declined precipitously from 28.8\\% to 2\\% after training on our datasets. This highlights the challenge of finding a suitable tradeoff between achieving performance on specific tasks while preserving the original prowess of LLMs. Empirical findings suggest that tasks inherently equipped with reasoning paths contribute significantly to preserving certain capabilities of LLMs against potential declines. Motivated by this, we introduce the Reasoning-augmented Continual Learning (RCL) approach. RCL integrates task-specific cues with meta-rationales, effectively reducing catastrophic forgetting in LLMs while expediting convergence on novel tasks.", "year": 2023, "publicationdate": "2023-10-10", "externalids": {"DOI": "10.48550/arXiv.2310.06762"}, "doi_lower": "10.48550/arxiv.2310.06762"}
{"paper_id": 258685677, "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation", "author_names": ["Yue Wang", "Hung Le", "Akhilesh Deepak Gotmare", "Nghi D. Q. Bui", "Junnan Li", "Steven C. H. Hoi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.", "year": 2023, "publicationdate": "2023-05-13", "externalids": {"DOI": "10.48550/arXiv.2305.07922"}, "doi_lower": "10.48550/arxiv.2305.07922"}
{"paper_id": 268513020, "title": "InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions", "author_names": ["Yifan Wang", "Yafei Liu", "Chufan Shi", "Haoling Li", "Chen Chen", "H. Lu", "Yujiu Yang"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Instruction tuning effectively optimizes Large Language Models (LLMs) for downstream tasks. Due to the changing environment in real-life applications, LLMs necessitate continual task-specific adaptation without catastrophic forgetting. Considering the heavy computational cost, replay-based Continual Learning (CL) methods are the simplest and most widely used for LLMs to address the forgetting issue. However, traditional replay-based methods do not fully utilize instructions to customize the replay strategy. In this work, we propose a novel paradigm called Instruction-based Continual Learning (InsCL). InsCL dynamically replays previous data based on task similarity, calculated by Wasserstein Distance with instructions. Moreover, we further introduce an Instruction Information Metric (InsInfo) to quantify the complexity and diversity of instructions. According to InsInfo, InsCL guides the replay process more inclined to high-quality data. We conduct extensive experiments over 16 tasks with different training orders, observing consistent performance improvements of InsCL. When all tasks have been trained, InsCL achieves performance gains of 3.0 Relative Gain compared with Random Replay, and 27.96 Relative Gain compared with No Replay.", "year": 2024, "publicationdate": "2024-03-18", "externalids": {"DOI": "10.48550/arXiv.2403.11435"}, "doi_lower": "10.48550/arxiv.2403.11435"}
{"paper_id": 242657828, "title": "C h a m p i o n ’ s K i t K i c k s t a r t s ‘ B e t t e r ’ C a m p a i g n", "author_names": [], "venue": "Volume 31, Number 3, June 2004", "abstract": null, "year": 2020, "publicationdate": "2020-03-13", "externalids": {"DOI": "10.1287/orms.2004.03.07"}, "doi_lower": "10.1287/orms.2004.03.07"}
{"paper_id": 237386541, "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation", "author_names": ["Yue Wang", "Weishi Wang", "Shafiq R. Joty", "S. Hoi"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.", "year": 2021, "publicationdate": "2021-09-02", "externalids": {"DOI": "10.18653/v1/2021.emnlp-main.685"}, "doi_lower": "10.18653/v1/2021.emnlp-main.685"}
{"paper_id": 269009666, "title": "CodecLM: Aligning Language Models with Tailored Synthetic Data", "author_names": ["Zifeng Wang", "Chun-Liang Li", "Vincent Perot", "Long T. Le", "Jin Miao", "Zizhao Zhang", "Chen-Yu Lee", "Tomas Pfister"], "venue": "NAACL-HLT", "abstract": "Instruction tuning has emerged as the key in aligning large language models (LLMs) with specific task instructions, thereby mitigating the discrepancy between the next-token prediction objective and users' actual goals. To reduce the labor and time cost to collect or annotate data by humans, researchers start to explore the use of LLMs to generate instruction-aligned synthetic data. Recent works focus on generating diverse instructions and applying LLM to increase instruction complexity, often neglecting downstream use cases. It remains unclear how to tailor high-quality data to elicit better instruction-following abilities in different target instruction distributions and LLMs. To this end, we introduce CodecLM, a general framework for adaptively generating high-quality synthetic data for LLM alignment with different downstream instruction distributions and LLMs. Drawing on the Encode-Decode principles, we use LLMs as codecs to guide the data generation process. We first encode seed instructions into metadata, which are concise keywords generated on-the-fly to capture the target instruction distribution, and then decode metadata to create tailored instructions. We also introduce Self-Rubrics and Contrastive Filtering during decoding to tailor data-efficient samples. Extensive experiments on four open-domain instruction following benchmarks validate the effectiveness of CodecLM over the current state-of-the-arts.", "year": 2024, "publicationdate": "2024-04-08", "externalids": {"DOI": "10.48550/arXiv.2404.05875"}, "doi_lower": "10.48550/arxiv.2404.05875"}
{"paper_id": 252383174, "title": "SparCL: Sparse Continual Learning on the Edge", "author_names": ["Zifeng Wang", "Zheng Zhan", "Yifan Gong", "Geng Yuan", "Wei Niu", "T. Jian", "Bin Ren", "Stratis Ioannidis", "Yanzhi Wang", "Jennifer G. Dy"], "venue": "Neural Information Processing Systems", "abstract": "Existing work in continual learning (CL) focuses on mitigating catastrophic forgetting, i.e., model performance deterioration on past tasks when learning a new task. However, the training efficiency of a CL system is under-investigated, which limits the real-world application of CL systems under resource-limited scenarios. In this work, we propose a novel framework called Sparse Continual Learning(SparCL), which is the first study that leverages sparsity to enable cost-effective continual learning on edge devices. SparCL achieves both training acceleration and accuracy preservation through the synergy of three aspects: weight sparsity, data efficiency, and gradient sparsity. Specifically, we propose task-aware dynamic masking (TDM) to learn a sparse network throughout the entire CL process, dynamic data removal (DDR) to remove less informative training data, and dynamic gradient masking (DGM) to sparsify the gradient updates. Each of them not only improves efficiency, but also further mitigates catastrophic forgetting. SparCL consistently improves the training efficiency of existing state-of-the-art (SOTA) CL methods by at most 23X less training FLOPs, and, surprisingly, further improves the SOTA accuracy by at most 1.7%. SparCL also outperforms competitive baselines obtained from adapting SOTA sparse training methods to the CL setting in both efficiency and accuracy. We also evaluate the effectiveness of SparCL on a real mobile phone, further indicating the practical potential of our method.", "year": 2022, "publicationdate": "2022-09-20", "externalids": {"DOI": "10.48550/arXiv.2209.09476"}, "doi_lower": "10.48550/arxiv.2209.09476"}
{"paper_id": 248085201, "title": "DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning", "author_names": ["Zifeng Wang", "Zizhao Zhang", "Sayna Ebrahimi", "Ruoxi Sun", "Han Zhang", "Chen-Yu Lee", "Xiaoqi Ren", "Guolong Su", "Vincent Perot", "Jennifer G. Dy", "Tomas Pfister"], "venue": "European Conference on Computer Vision", "abstract": "Continual learning aims to enable a single model to learn a sequence of tasks without catastrophic forgetting. Top-performing methods usually require a rehearsal buffer to store past pristine examples for experience replay, which, however, limits their practical value due to privacy and memory constraints. In this work, we present a simple yet effective framework, DualPrompt, which learns a tiny set of parameters, called prompts, to properly instruct a pre-trained model to learn tasks arriving sequentially without buffering past examples. DualPrompt presents a novel approach to attach complementary prompts to the pre-trained backbone, and then formulates the objective as learning task-invariant and task-specific\"instructions\". With extensive experimental validation, DualPrompt consistently sets state-of-the-art performance under the challenging class-incremental setting. In particular, DualPrompt outperforms recent advanced continual learning methods with relatively large buffer sizes. We also introduce a more challenging benchmark, Split ImageNet-R, to help generalize rehearsal-free continual learning research. Source code is available at https://github.com/google-research/l2p.", "year": 2022, "publicationdate": "2022-04-10", "externalids": {"DOI": "10.48550/arXiv.2204.04799"}, "doi_lower": "10.48550/arxiv.2204.04799"}
{"paper_id": 245218925, "title": "Learning to Prompt for Continual Learning", "author_names": ["Zifeng Wang", "Zizhao Zhang", "Chen-Yu Lee", "Han Zhang", "Ruoxi Sun", "Xiaoqi Ren", "Guolong Su", "Vincent Perot", "Jennifer G. Dy", "Tomas Pfister"], "venue": "Computer Vision and Pattern Recognition", "abstract": "The mainstream paradigm behind continual learning has been to adapt the model parameters to non-stationary data distributions, where catastrophic forgetting is the central challenge. Typical methods rely on a rehearsal buffer or known task identity at test time to retrieve learned knowl-edge and address forgetting, while this work presents a new paradigm for continual learning that aims to train a more succinct memory system without accessing task identity at test time. Our method learns to dynamically prompt (L2P) a pre-trained model to learn tasks sequen-tially under different task transitions. In our proposed framework, prompts are small learnable parameters, which are maintained in a memory space. The objective is to optimize prompts to instruct the model prediction and ex-plicitly manage task-invariant and task-specific knowledge while maintaining model plasticity. We conduct comprehen-sive experiments under popular image classification bench-marks with different challenging continual learning set-tings, where L2P consistently outperforms prior state-of-the-art methods. Surprisingly, L2P achieves competitive results against rehearsal-based methods even without a re-hearsal buffer and is directly applicable to challenging task-agnostic continual learning. Source code is available at https://github.com/google-research/12p.", "year": 2021, "publicationdate": "2021-12-16", "externalids": {"DOI": "10.1109/CVPR52688.2022.00024"}, "doi_lower": "10.1109/cvpr52688.2022.00024"}
{"paper_id": 237416585, "title": "Finetuned Language Models Are Zero-Shot Learners", "author_names": ["Jason Wei", "Maarten Bosma", "Vincent Zhao", "Kelvin Guu", "Adams Wei Yu", "Brian Lester", "Nan Du", "Andrew M. Dai", "Quoc V. Le"], "venue": "International Conference on Learning Representations", "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.", "year": 2021, "publicationdate": "2021-09-03", "externalids": {}, "doi_lower": null}
{"paper_id": 237416585, "title": "Finetuned Language Models Are Zero-Shot Learners", "author_names": ["Jason Wei", "Maarten Bosma", "Vincent Zhao", "Kelvin Guu", "Adams Wei Yu", "Brian Lester", "Nan Du", "Andrew M. Dai", "Quoc V. Le"], "venue": "International Conference on Learning Representations", "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.", "year": 2021, "publicationdate": "2021-09-03", "externalids": {}, "doi_lower": null}
{"paper_id": 249674500, "title": "Emergent Abilities of Large Language Models", "author_names": ["Jason Wei", "Yi Tay", "Rishi Bommasani", "Colin Raffel", "Barret Zoph", "Sebastian Borgeaud", "Dani Yogatama", "Maarten Bosma", "Denny Zhou", "Donald Metzler", "Ed H. Chi", "Tatsunori Hashimoto", "O. Vinyals", "P. Liang", "J. Dean", "W. Fedus"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.", "year": 2022, "publicationdate": "2022-06-15", "externalids": {"DOI": "10.48550/arXiv.2206.07682"}, "doi_lower": "10.48550/arxiv.2206.07682"}
{"paper_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "author_names": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "Ed H. Chi", "F. Xia", "Quoc Le", "Denny Zhou"], "venue": "Neural Information Processing Systems", "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "year": 2022, "publicationdate": "2022-01-28", "externalids": {}, "doi_lower": null}
{"paper_id": 258556987, "title": "On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code", "author_names": ["M. Weyssow", "Xin Zhou", "Kisub Kim", "David Lo", "H. Sahraoui"], "venue": "ESEC/SIGSOFT FSE", "abstract": "Pre-trained language models (PLMs) have become a prevalent technique in deep learning for code, utilizing a two-stage pre-training and fine-tuning procedure to acquire general knowledge about code and specialize in a variety of downstream tasks. However, the dynamic nature of software codebases poses a challenge to the effectiveness and robustness of PLMs. In particular, world-realistic scenarios potentially lead to significant differences between the distribution of the pre-training and test data, i.e., distribution shift, resulting in a degradation of the PLM's performance on downstream tasks. In this paper, we stress the need for adapting PLMs of code to software data whose distribution changes over time, a crucial problem that has been overlooked in previous works. The motivation of this work is to consider the PLM in a non-stationary environment, where fine-tuning data evolves over time according to a software evolution scenario. Specifically, we design a scenario where the model needs to learn from a stream of programs containing new, unseen APIs over time. We study two widely used PLM architectures, i.e., a GPT2 decoder and a RoBERTa encoder, on two downstream tasks, API call and API usage prediction. We demonstrate that the most commonly used fine-tuning technique from prior work is not robust enough to handle the dynamic nature of APIs, leading to the loss of previously acquired knowledge i.e., catastrophic forgetting. To address these issues, we implement five continual learning approaches, including replay-based and regularization-based methods. Our findings demonstrate that utilizing these straightforward methods effectively mitigates catastrophic forgetting in PLMs across both downstream tasks while achieving comparable or superior performance.", "year": 2023, "publicationdate": "2023-05-06", "externalids": {"DOI": "10.1145/3611643.3616244"}, "doi_lower": "10.1145/3611643.3616244"}
{"paper_id": 258887506, "title": "Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning", "author_names": ["Genta Indra Winata", "Lingjue Xie", "Karthik Radhakrishnan", "Shijie Wu", "Xisen Jin", "Pengxiang Cheng", "Mayank Kulkarni", "Daniel Preotiuc-Pietro"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Real-life multilingual systems should be able to efficiently incorporate new languages as data distributions fed to the system evolve and shift over time. To do this, systems need to handle the issue of catastrophic forgetting, where the model performance drops for languages or tasks seen further in its past. In this paper, we study catastrophic forgetting, as well as methods to minimize this, in a massively multilingual continual learning framework involving up to 51 languages and covering both classification and sequence labeling tasks. We present LR ADJUST, a learning rate scheduling method that is simple, yet effective in preserving new information without strongly overwriting past knowledge. Furthermore, we show that this method is effective across multiple continual learning approaches. Finally, we provide further insights into the dynamics of catastrophic forgetting in this massively multilingual setup.", "year": 2023, "publicationdate": "2023-05-25", "externalids": {"DOI": "10.48550/arXiv.2305.16252"}, "doi_lower": "10.48550/arxiv.2305.16252"}
{"paper_id": 265498940, "title": "Continual Learning with Low Rank Adaptation", "author_names": ["Martin Wistuba", "Prabhu Teja Sivaprasad", "Lukas Balles", "Giovanni Zappella"], "venue": "arXiv.org", "abstract": "Recent work using pretrained transformers has shown impressive performance when fine-tuned with data from the downstream problem of interest. However, they struggle to retain that performance when the data characteristics changes. In this paper, we focus on continual learning, where a pre-trained transformer is updated to perform well on new data, while retaining its performance on data it was previously trained on. Earlier works have tackled this primarily through methods inspired from prompt tuning. We question this choice, and investigate the applicability of Low Rank Adaptation (LoRA) to continual learning. On a range of domain-incremental learning benchmarks, our LoRA-based solution, CoLoR, yields state-of-the-art performance, while still being as parameter efficient as the prompt tuning based methods.", "year": 2023, "publicationdate": "2023-11-29", "externalids": {"DOI": "10.48550/arXiv.2311.17601"}, "doi_lower": "10.48550/arxiv.2311.17601"}
{"paper_id": 266755997, "title": "LLaMA Pro: Progressive LLaMA with Block Expansion", "author_names": ["Chengyue Wu", "Yukang Gan", "Yixiao Ge", "Zeyu Lu", "Jiahao Wang", "Ye Feng", "Ping Luo", "Ying Shan"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.", "year": 2024, "publicationdate": "2024-01-04", "externalids": {"DOI": "10.48550/arXiv.2401.02415"}, "doi_lower": "10.48550/arxiv.2401.02415"}
{"paper_id": 258417843, "title": "PMC-LLaMA: Towards Building Open-source Language Models for Medicine", "author_names": ["Chaoyi Wu", "Xiaoman Zhang", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "venue": "", "abstract": "Recently, Large Language Models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge. In this paper, we describe the procedure for building a powerful, open-source language model specifically designed for medicine applications, termed as PMC-LLaMA. Our contributions are threefold: (i) we systematically investigate the process of adapting a general-purpose foundation language model towards medical domain, this involves data-centric knowledge injection through the integration of 4.8M biomedical academic papers and 30K medical textbooks, as well as comprehensive fine-tuning for alignment with domain-specific instructions; (ii) we contribute a large-scale, comprehensive dataset for instruction tuning. This dataset encompasses medical question-answering (QA), rationale for reasoning, and conversational dialogues, comprising a total of 202M tokens; (iii) we conduct thorough ablation studies to demonstrate the effectiveness of each proposed component. While evaluating on various public medical question-answering benchmarks, our lightweight PMCLLaMA, which consists of only 13 billion parameters, exhibits superior performance, even surpassing ChatGPT. All models, codes, datasets can be found in https://github.com/chaoyi-wu/PMC-LLaMA.", "year": 2023, "publicationdate": "2023-04-27", "externalids": {}, "doi_lower": null}
{"paper_id": 257833842, "title": "BloombergGPT: A Large Language Model for Finance", "author_names": ["Shijie Wu", "Ozan Irsoy", "Steven Lu", "Vadim Dabravolski", "Mark Dredze", "Sebastian Gehrmann", "P. Kambadur", "D. Rosenberg", "Gideon Mann"], "venue": "arXiv.org", "abstract": "The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.", "year": 2023, "publicationdate": "2023-03-30", "externalids": {}, "doi_lower": null}
{"paper_id": 247717213, "title": "Pretrained Language Model in Continual Learning: A Comparative Study", "author_names": ["Tongtong Wu", "Massimo Caccia", "Zhuang Li", "Yuan-Fang Li", "G. Qi", "Gholamreza Haffari"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2022, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 267406164, "title": "Continual Learning for Large Language Models: A Survey", "author_names": ["Tongtong Wu", "Linhao Luo", "Yuan-Fang Li", "Shirui Pan", "Thuy-Trang Vu", "Gholamreza Haffari"], "venue": "arXiv.org", "abstract": "Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.", "year": 2024, "publicationdate": "2024-02-02", "externalids": {"DOI": "10.48550/arXiv.2402.01364"}, "doi_lower": "10.48550/arxiv.2402.01364"}
{"paper_id": 173187918, "title": "Large Scale Incremental Learning", "author_names": ["Yue Wu", "Yinpeng Chen", "Lijuan Wang", "Yuancheng Ye", "Zicheng Liu", "Yandong Guo", "Y. Fu"], "venue": "Computer Vision and Pattern Recognition", "abstract": "Modern machine learning suffers from \\textit{catastrophic forgetting} when learning new classes incrementally. The performance dramatically degrades due to the missing data of old classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to \\textbf{scale up to a large number of classes}. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. We propose a simple and effective method to address this data imbalance issue. We found that the last fully connected layer has a strong bias towards the new classes, and this bias can be corrected by a linear model. With two bias parameters, our method performs remarkably well on two large datasets: ImageNet (1000 classes) and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms by 11.1\\% and 13.2\\% respectively.", "year": 2019, "publicationdate": "2019-05-30", "externalids": {"DOI": "10.1109/CVPR.2019.00046"}, "doi_lower": "10.1109/cvpr.2019.00046"}
{"paper_id": 3524564, "title": "The Kanerva Machine: A Generative Distributed Memory", "author_names": ["Yan Wu", "Greg Wayne", "Alex Graves", "T. Lillicrap"], "venue": "International Conference on Learning Representations", "abstract": "We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva's sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train.", "year": 2018, "publicationdate": "2018-02-15", "externalids": {}, "doi_lower": null}
{"paper_id": 259137655, "title": "QUERT: Continual Pre-training of Language Model for Query Understanding in Travel Domain Search", "author_names": ["Jian Xie", "Yidan Liang", "Jingping Liu", "Yanghua Xiao", "Baohua Wu", "Shenghua Ni"], "venue": "Knowledge Discovery and Data Mining", "abstract": "In light of the success of the pre-trained language models (PLMs), continual pre-training of generic PLMs has been the paradigm of domain adaption. In this paper, we propose QUERT, A Continual Pre-trained Language Model for QUERy Understanding in Travel Domain Search. QUERT is jointly trained on four tailored pre-training tasks to the characteristics of query in travel domain search: Geography-aware Mask Prediction, Geohash Code Prediction, User Click Behavior Learning, and Phrase and Token Order Prediction. Performance improvement of downstream tasks and ablation experiment demonstrate the effectiveness of our proposed pre-training tasks. To be specific, the average performance of downstream tasks increases by 2.02% and 30.93% in supervised and unsupervised settings, respectively. To check on the improvement of QUERT to online business, we deploy QUERT and perform A/B testing on Fliggy APP. The feedback results show that QUERT increases the Unique Click-Through Rate and Page Click-Through Rate by 0.89% and 1.03% when applying QUERT as the encoder. Resources are available at https://github.com/hsaest/QUERT", "year": 2023, "publicationdate": "2023-06-11", "externalids": {"DOI": "10.1145/3580305.3599891"}, "doi_lower": "10.1145/3580305.3599891"}
{"paper_id": 273812708, "title": "Me LLaMA: Foundation Large Language Models for Medical Applications", "author_names": ["Qianqian Xie", "Qingyu Chen", "Aokun Chen", "C.A.I. Peng", "Yan Hu", "Fongci Lin", "Xueqing Peng", "Jimin Huang", "Jeffrey Zhang", "V. Keloth", "Xinyu Zhou", "Lingfei Qian", "Huan He", "Dennis Shung", "Lucila Ohno-Machado", "Yonghui Wu", "Hua Xu", "Jiang Bian"], "venue": "", "abstract": "Recent advancements in large language models (LLMs) like ChatGPT and LLaMA show promise in medical applications, yet challenges remain in medical language comprehension. This study presents Me-LLaMA, a new medical LLM family based on open-source LLaMA models, optimized for medical text analysis and diagnosis by leveraging large-scale, domain-specific datasets. The Me-LLaMA family, including foundation models Me-LLaMA 13/70B and their chat-enhanced versions, was developed through continued pre-training and instruction tuning with 129B tokens and 214K samples from biomedical and clinical sources. Training the 70B models required over 100,000 A100 GPU hours. Me-LLaMA's performance was evaluated across six medical text analysis tasks using 12 benchmark datasets and complex clinical case diagnosis, with automatic and human evaluations. Results indicate Me-LLaMA outperforms LLaMA and other open-source medical LLMs in zero-shot and supervised settings. Task-specific tuning further boosts performance, surpassing ChatGPT on 7 of 8 datasets and GPT-4 on 5 of 8. For complex clinical cases, Me-LLaMA achieves performance comparable to ChatGPT and GPT-4. This work underscores the importance of domain-specific data in developing medical LLMs and addresses the high computational costs involved in training, highlighting a balance between pre-training and fine-tuning strategies. Me-LLaMA models are now accessible under user agreements, providing a valuable resource for advancing medical AI.", "year": 2024, "publicationdate": "2024-02-20", "externalids": {}, "doi_lower": null}
{"paper_id": 259129602, "title": "PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance", "author_names": ["Qianqian Xie", "Weiguang Han", "Xiao Zhang", "Yanzhao Lai", "Min Peng", "Alejandro Lopez-Lira", "Jimin Huang"], "venue": "arXiv.org", "abstract": "Although large language models (LLMs) has shown great performance on natural language processing (NLP) in the financial domain, there are no publicly available financial tailtored LLMs, instruction tuning datasets, and evaluation benchmarks, which is critical for continually pushing forward the open-source development of financial artificial intelligence (AI). This paper introduces PIXIU, a comprehensive framework including the first financial LLM based on fine-tuning LLaMA with instruction data, the first instruction data with 136K data samples to support the fine-tuning, and an evaluation benchmark with 5 tasks and 9 datasets. We first construct the large-scale multi-task instruction data considering a variety of financial tasks, financial document types, and financial data modalities. We then propose a financial LLM called FinMA by fine-tuning LLaMA with the constructed dataset to be able to follow instructions for various financial tasks. To support the evaluation of financial LLMs, we propose a standardized benchmark that covers a set of critical financial tasks, including five financial NLP tasks and one financial prediction task. With this benchmark, we conduct a detailed analysis of FinMA and several existing LLMs, uncovering their strengths and weaknesses in handling critical financial tasks. The model, datasets, benchmark, and experimental results are open-sourced to facilitate future research in financial AI.", "year": 2023, "publicationdate": "2023-06-08", "externalids": {"DOI": "10.48550/arXiv.2306.05443"}, "doi_lower": "10.48550/arxiv.2306.05443"}
{"paper_id": 256627727, "title": "Data Selection for Language Models via Importance Resampling", "author_names": ["Sang Michael Xie", "Shibani Santurkar", "Tengyu Ma", "Percy Liang"], "venue": "Neural Information Processing Systems", "abstract": "Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given unlabeled target samples. Due to the scale and dimensionality of the raw text data, existing methods use simple heuristics or require human experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. We instantiate the DSIR framework with hashed n-gram features for efficiency, enabling the selection of 100M documents from the full Pile dataset in 4.5 hours. To measure whether hashed n-gram features preserve the aspects of the data that are relevant to the target, we define KL reduction, a data metric that measures the proximity between the selected pretraining data and the target on some feature space. Across 8 data selection methods (including expert selection), KL reduction on hashed n-gram features highly correlates with average downstream accuracy (r=0.82). When selecting data for continued pretraining on a specific domain, DSIR performs comparably to expert curation across 8 target distributions. When pretraining general-domain models (target is Wikipedia and books), DSIR improves over random selection and heuristic filtering baselines by 2-2.5% on the GLUE benchmark. Code is available at https://github.com/p-lambda/dsir.", "year": 2023, "publicationdate": "2023-02-06", "externalids": {"DOI": "10.48550/arXiv.2302.03169"}, "doi_lower": "10.48550/arxiv.2302.03169"}
{"paper_id": 265213147, "title": "Efficient Continual Pre-training for Building Domain Specific Large Language Models", "author_names": ["Yong Xie", "Karan Aggarwal", "Aitzaz Ahmad"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Large language models (LLMs) have demonstrated remarkable open-domain capabilities. Traditionally, LLMs tailored for a domain are trained from scratch to excel at handling domain-specific tasks. In this work, we explore an alternative strategy of continual pre-training as a means to develop domain-specific LLMs. We introduce FinPythia-6.9B, developed through domain-adaptive continual pre-training on the financial domain. Continual pre-trained FinPythia showcases consistent improvements on financial tasks over the original foundational model. We further explore simple but effective data selection strategies for continual pre-training. Our data selection strategies outperforms vanilla continual pre-training's performance with just 10% of corpus size and cost, without any degradation on open-domain standard tasks. Our work proposes an alternative solution to building domain-specific LLMs from scratch in a cost-effective manner.", "year": 2023, "publicationdate": "2023-11-14", "externalids": {"DOI": "10.48550/arXiv.2311.08545"}, "doi_lower": "10.48550/arxiv.2311.08545"}
{"paper_id": 258298159, "title": "WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions", "author_names": ["Can Xu", "Qingfeng Sun", "Kai Zheng", "Xiubo Geng", "Pu Zhao", "Jiazhan Feng", "Chongyang Tao", "Daxin Jiang"], "venue": "International Conference on Learning Representations", "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM", "year": 2023, "publicationdate": "2023-04-24", "externalids": {}, "doi_lower": null}
{"paper_id": 102353837, "title": "BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis", "author_names": ["Hu Xu", "Bing Liu", "Lei Shu", "Philip S. Yu"], "venue": "North American Chapter of the Association for Computational Linguistics", "abstract": "Question-answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making. Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions. We call this problem Review Reading Comprehension (RRC). To the best of our knowledge, no existing work has been done on RRC. In this work, we first build an RRC dataset called ReviewRC based on a popular benchmark for aspect-based sentiment analysis. Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC. To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis. Experimental results demonstrate that the proposed post-training is highly effective.", "year": 2019, "publicationdate": "2019-04-03", "externalids": {"DOI": "10.18653/v1/N19-1242"}, "doi_lower": "10.18653/v1/n19-1242"}
{"paper_id": 260775975, "title": "WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine", "author_names": ["Siqiao Xue", "Fan Zhou", "Y. Xu", "Hongyu Zhao", "Shuo Xie", "Caigao Jiang", "James Y. Zhang", "Jun Zhou", "P. Xu", "D. Xiu", "Hongyuan Mei"], "venue": "arXiv.org", "abstract": "We present WeaverBird, an intelligent dialogue system designed specifically for the finance domain. Our system harnesses a large language model of GPT architecture that has been tuned using extensive corpora of finance-related text. As a result, our system possesses the capability to understand complex financial queries, such as\"How should I manage my investments during inflation?\", and provide informed responses. Furthermore, our system incorporates a local knowledge base and a search engine to retrieve relevant information. The final responses are conditioned on the search results and include proper citations to the sources, thus enjoying an enhanced credibility. Through a range of finance-related questions, we have demonstrated the superior performance of our system compared to other models. To experience our system firsthand, users can interact with our live demo at https://weaverbird.ttic.edu, as well as watch our 2-min video illustration at https://www.youtube.com/watch?v=yofgeqnlrMc.", "year": 2023, "publicationdate": "2023-08-10", "externalids": {"DOI": "10.48550/arXiv.2308.05361"}, "doi_lower": "10.48550/arxiv.2308.05361"}
{"paper_id": 253735287, "title": "AF Adapter: Continual Pretraining for Building Chinese Biomedical Language Model", "author_names": ["Yong-ping Yan", "Kui Xue", "Xiaoming Shi", "Qi Ye", "Jingping Liu", "Tong Ruan"], "venue": "IEEE International Conference on Bioinformatics and Biomedicine", "abstract": "Continual pretraining is a popular way of building a domain-specific pretrained language model from a general-domain language model. In spite of its high efficiency, continual pretraining suffers from catastrophic forgetting, which may harm the model’s performance in downstream tasks. To alleviate the issue, in this paper, we propose a continual pretraining method for the BERT-based model, named Attention-FFN Adapter. Its main idea is to introduce a small number of attention heads and hidden units inside each self-attention layer and feed-forward network. Furthermore, we train a domain-specific language model named AF Adapter based RoBERTa for the Chinese biomedical domain. In experiments, models are applied to downstream tasks for evaluation. The results demonstrate that with only about 17% of model parameters trained, AF Adapter achieves 0.6%, 2% gain in performance on average, compared to strong baselines. Further experimental results show that our method alleviates the catastrophic forgetting problem by 11% compared to the fine-tuning method. Code is available at https://github.com/yanyongyu/AF-Adapter.", "year": 2022, "publicationdate": "2022-11-21", "externalids": {"DOI": "10.1109/BIBM58861.2023.10385733"}, "doi_lower": "10.1109/bibm58861.2023.10385733"}
{"paper_id": 267751418, "title": "MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning", "author_names": ["Shu Yang", "Muhammad Asif Ali", "Cheng-Long Wang", "Lijie Hu", "Di Wang"], "venue": "arXiv.org", "abstract": "Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper, we propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for Lifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the fine-tuning abilities of LoRA for effective life-long learning of LLMs. In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question-answer pairs, which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings, we introduce a new evaluation benchmark namely: Life Long Learning of LLM (5L-bench) encompassing a newly curated dataset of question-answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book and closed-book settings. Experimental evaluation shows (i) LLMs learn fast in open-book settings with up to 30.15% improvement in\"RA\"for Phi-2-2.7B compared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL shows higher performance improvement for models with a greater number of parameters; (iii) MoRAL is robust to catastrophic forgetting offering better knowledge retention compared to baselines.", "year": 2024, "publicationdate": "2024-02-17", "externalids": {"DOI": "10.48550/arXiv.2402.11260"}, "doi_lower": "10.48550/arxiv.2402.11260"}
{"paper_id": 266741610, "title": "PLLaMa: An Open-source Large Language Model for Plant Science", "author_names": ["Xianjun Yang", "Junfeng Gao", "Wenxin Xue", "Erik Alexandersson"], "venue": "arXiv.org", "abstract": "Large Language Models (LLMs) have exhibited remarkable capabilities in understanding and interacting with natural language across various sectors. However, their effectiveness is limited in specialized areas requiring high accuracy, such as plant science, due to a lack of specific expertise in these fields. This paper introduces PLLaMa, an open-source language model that evolved from LLaMa-2. It's enhanced with a comprehensive database, comprising more than 1.5 million scholarly articles in plant science. This development significantly enriches PLLaMa with extensive knowledge and proficiency in plant and agricultural sciences. Our initial tests, involving specific datasets related to plants and agriculture, show that PLLaMa substantially improves its understanding of plant science-related topics. Moreover, we have formed an international panel of professionals, including plant scientists, agricultural engineers, and plant breeders. This team plays a crucial role in verifying the accuracy of PLLaMa's responses to various academic inquiries, ensuring its effective and reliable application in the field. To support further research and development, we have made the model's checkpoints and source codes accessible to the scientific community. These resources are available for download at \\url{https://github.com/Xianjun-Yang/PLLaMa}.", "year": 2024, "publicationdate": "2024-01-03", "externalids": {"DOI": "10.48550/arXiv.2401.01600"}, "doi_lower": "10.48550/arxiv.2401.01600"}
{"paper_id": 268385313, "title": "Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training", "author_names": ["Yanlai Yang", "Matt Jones", "M. Mozer", "Mengye Ren"], "venue": "Neural Information Processing Systems", "abstract": "We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence. Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of LLMs finetuned sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again. This behavior occurs even though the documents are never presented in context together. The behavior emerges and becomes more robust as the architecture scales up its number of parameters. Through comprehensive experiments and visualizations, we demonstrate a new mechanism by which over-parametrized neural networks can recover from catastrophic interference and uncover new insights into training over-parameterized networks in cyclically structured environments.", "year": 2024, "publicationdate": "2024-03-14", "externalids": {"DOI": "10.48550/arXiv.2403.09613"}, "doi_lower": "10.48550/arxiv.2403.09613"}
{"paper_id": 270095224, "title": "Recent Advances of Foundation Language Models-based Continual Learning: A Survey", "author_names": ["Yutao Yang", "Jie Zhou", "Xuanwen Ding", "Tianyu Huai", "Shunyu Liu", "Qin Chen", "Liang He", "Yuan Xie"], "venue": "ACM Computing Surveys", "abstract": "Recently, foundation language models (LMs) have marked significant achievements in the domains of natural language processing and computer vision. Unlike traditional neural network models, foundation LMs obtain a great ability for transfer learning by acquiring rich common sense knowledge through pre-training on extensive unsupervised datasets with a vast number of parameters. Despite these capabilities, LMs still struggle with catastrophic forgetting, hindering their ability to learn continuously like humans. To address this, continual learning (CL) methodologies have been introduced, allowing LMs to adapt to new tasks while retaining learned knowledge. However, a systematic taxonomy of existing approaches and a comparison of their performance are still lacking. In this article, we delve into a comprehensive review, summarization, and classification of the existing literature on CL-based approaches applied to foundation language models, such as pre-trained language models, large language models, and vision-language models. We divide these studies into offline and online CL, which consist of traditional methods, parameter-efficient-based methods, instruction tuning-based methods and continual pre-training methods. Additionally, we outline the typical datasets and metrics employed in CL research and provide a detailed analysis of the challenges and future work for LMs-based continual learning.", "year": 2024, "publicationdate": "2024-05-28", "externalids": {"DOI": "10.1145/3705725"}, "doi_lower": "10.1145/3705725"}
{"paper_id": 258762525, "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "author_names": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "T. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "venue": "Neural Information Processing Systems", "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.", "year": 2023, "publicationdate": "2023-05-17", "externalids": {"DOI": "10.48550/arXiv.2305.10601"}, "doi_lower": "10.48550/arxiv.2305.10601"}
{"paper_id": 268032887, "title": "Investigating Continual Pretraining in Large Language Models: Insights and Implications", "author_names": ["cCaugatay Yildiz", "Nishaanth Kanna Ravichandran", "Prishruit Punia", "Matthias Bethge", "B. Ermiş"], "venue": "Trans. Mach. Learn. Res.", "abstract": "Continual learning (CL) in large language models (LLMs) is an evolving domain that focuses on developing efficient and sustainable training strategies to adapt models to emerging knowledge and achieve robustness in dynamic environments. Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge. Since existing works concentrate mostly on continual fine-tuning for a limited selection of downstream tasks or training domains, we introduce a new benchmark designed to measure the adaptability of LLMs to changing pretraining data landscapes. We further examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models. Our findings uncover several key insights: (i) continual pretraining consistently improves<1.5B models studied in this work and is also superior to domain adaptation, (ii) larger models always achieve better perplexity than smaller ones when continually pretrained on the same corpus, (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both learning and forgetting, (iv) continual pretraining boosts downstream task performance of GPT-2 family, (v) continual pretraining enables LLMs to specialize better when the sequence of domains shows semantic similarity while randomizing training domains leads to better transfer and final performance otherwise. We posit that our research establishes a new benchmark for CL in LLMs, providing a more realistic evaluation of knowledge retention and transfer across diverse domains.", "year": 2024, "publicationdate": "2024-02-27", "externalids": {"DOI": "10.48550/arXiv.2402.17400"}, "doi_lower": "10.48550/arxiv.2402.17400"}
{"paper_id": 247476090, "title": "ConTinTin: Continual Learning from Task Instructions", "author_names": ["Wenpeng Yin", "Jia Li", "Caiming Xiong"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The mainstream machine learning paradigms for NLP often work with two underlying presumptions. First, the target task is predefined and static; a system merely needs to learn to solve it exclusively. Second, the supervision of a task mainly comes from a set of labeled examples. A question arises: how to build a system that can keep learning new tasks from their instructions?This work defines a new learning paradigm ConTinTin (Continual Learning from Task Instructions), in which a system should learn a sequence of new tasks one by one, each task is explained by a piece of textual instruction. The system is required to (i) generate the expected outputs of a new task by learning from its instruction, (ii) transfer the knowledge acquired from upstream tasks to help solve downstream tasks (i.e., forward-transfer), and (iii) retain or even improve the performance on earlier tasks after learning new tasks (i.e., backward-transfer). This new problem is studied on a stream of more than 60 tasks, each equipped with an instruction. Technically, our method InstructionSpeak contains two strategies that make full use of task instructions to improve forward-transfer and backward-transfer: one is to learn from negative outputs, the other is to re-visit instructions of previous tasks. To our knowledge, this is the first time to study ConTinTin in NLP. In addition to the problem formulation and our promising approach, this work also contributes to providing rich analyses for the community to better understand this novel learning problem.", "year": 2022, "publicationdate": "2022-03-16", "externalids": {"DOI": "10.48550/arXiv.2203.08512"}, "doi_lower": "10.48550/arxiv.2203.08512"}
{"paper_id": 266362196, "title": "MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA", "author_names": ["Lang Yu", "Qin Chen", "Jie Zhou", "Liang He"], "venue": "AAAI Conference on Artificial Intelligence", "abstract": "Large language models (LLMs) have shown great success in various Natural Language Processing (NLP) tasks, whist they still need updates after deployment to fix errors or keep pace with the changing knowledge in the world. Researchers formulate such problem as Model Editing and have developed various editors focusing on different axes of editing properties. However, current editors can hardly support all properties and rely on heavy computational resources. In this paper, we propose a plug-in Model Editing method based on neuron-indexed dynamic LoRA (MELO), which alters the behavior of language models by dynamically activating certain LoRA blocks according to the index built in an inner vector database. Our method satisfies various editing properties with high efficiency and can be easily integrated into multiple LLM backbones. Experimental results show that our proposed MELO achieves state-of-the-art editing performance on three sequential editing tasks (document classification, question answering and hallucination correction), while requires the least trainable parameters and computational cost.", "year": 2023, "publicationdate": "2023-12-19", "externalids": {"DOI": "10.48550/arXiv.2312.11795"}, "doi_lower": "10.48550/arxiv.2312.11795"}
{"paper_id": 248986947, "title": "CIRCLE: continual repair across programming languages", "author_names": ["Wei Yuan", "Quanjun Zhang", "Tieke He", "Chunrong Fang", "Nguyen Quoc Viet Hung", "X. Hao", "Hongzhi Yin"], "venue": "International Symposium on Software Testing and Analysis", "abstract": "Automatic Program Repair (APR) aims at fixing buggy source code with less manual debugging efforts, which plays a vital role in improving software reliability and development productivity. Recent APR works have achieved remarkable progress via applying deep learning (DL), particularly neural machine translation (NMT) techniques. However, we observe that existing DL-based APR models suffer from at least two severe drawbacks: (1) Most of them can only generate patches for a single programming language, as a result, to repair multiple languages, we have to build and train many repairing models. (2) Most of them are developed offline. Therefore, they won’t function when there are new-coming requirements. To address the above problems, a T5-based APR framework equipped with continual learning ability across multiple programming languages is proposed, namely ContInual Repair aCross Programming LanguagEs (CIRCLE). Specifically, (1) CIRCLE utilizes a prompting function to narrow the gap between natural language processing (NLP) pre-trained tasks and APR. (2) CIRCLE adopts a difficulty-based rehearsal strategy to achieve lifelong learning for APR without access to the full historical data. (3) An elastic regularization method is employed to strengthen CIRCLE’s continual learning ability further, preventing it from catastrophic forgetting. (4) CIRCLE applies a simple but effective re-repairing method to revise generated errors caused by crossing multiple programming languages. We train CIRCLE for four languages (i.e., C, JAVA, JavaScript, and Python) and evaluate it on five commonly used benchmarks. The experimental results demonstrate that CIRCLE not only effectively and efficiently repairs multiple programming languages in continual learning settings, but also achieves state-of-the-art performance (e.g., fixes 64 Defects4J bugs) with a single repair model.", "year": 2022, "publicationdate": "2022-05-22", "externalids": {"DOI": "10.1145/3533767.3534219"}, "doi_lower": "10.1145/3533767.3534219"}
{"paper_id": 261696697, "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning", "author_names": ["Xiang Yue", "Xingwei Qu", "Ge Zhang", "Yao Fu", "Wenhao Huang", "Huan Sun", "Yu Su", "Wenhu Chen"], "venue": "International Conference on Learning Representations", "abstract": "We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT-4's CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.", "year": 2023, "publicationdate": "2023-09-11", "externalids": {"DOI": "10.48550/arXiv.2309.05653"}, "doi_lower": "10.48550/arxiv.2309.05653"}
{"paper_id": 168169824, "title": "Defending Against Neural Fake News", "author_names": ["Rowan Zellers", "Ari Holtzman", "Hannah Rashkin", "Yonatan Bisk", "Ali Farhadi", "Franziska Roesner", "Yejin Choi"], "venue": "Neural Information Processing Systems", "abstract": "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \nModern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \nDeveloping robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.", "year": 2019, "publicationdate": "2019-05-29", "externalids": {}, "doi_lower": null}
{"paper_id": 262055661, "title": "Investigating the Catastrophic Forgetting in Multimodal Large Language Models", "author_names": ["Yuexiang Zhai", "Shengbang Tong", "Xiao Li", "Mu Cai", "Qing Qu", "Yong Jae Lee", "Y. Ma"], "venue": "arXiv.org", "abstract": "Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.", "year": 2023, "publicationdate": "2023-09-19", "externalids": {"DOI": "10.48550/arXiv.2309.10313"}, "doi_lower": "10.48550/arxiv.2309.10313"}
{"paper_id": 266999634, "title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models", "author_names": ["Dan Zhang", "Ziniu Hu", "Sining Zhoubian", "Zhengxiao Du", "Kaiyu Yang", "Zihan Wang", "Yisong Yue", "Yuxiao Dong", "Jie Tang"], "venue": "Neural Information Processing Systems", "abstract": "Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciInstruct, a suite of scientific instructions for training scientific language models capable of college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We analyze the curated SciInstruct from multiple interesting perspectives (e.g., domain, scale, source, question type, answer length, etc.). To verify the effectiveness of SciInstruct, we fine-tuned different language models with SciInstruct, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B: MetaMath, enhancing their scientific and mathematical reasoning capabilities, without sacrificing the language understanding capabilities of the base model. We release all codes and SciInstruct at https://github.com/THUDM/SciGLM.", "year": 2024, "publicationdate": "2024-01-15", "externalids": {"DOI": "10.52202/079017-0046"}, "doi_lower": "10.52202/079017-0046"}
{"paper_id": 264439275, "title": "COPF: Continual Learning Human Preference through Optimal Policy Fitting", "author_names": ["Han Zhang", "Lin Gui", "Yuanzhao Zhai", "Hui Wang", "Yu Lei", "Ruifeng Xu"], "venue": "arXiv.org", "abstract": null, "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.48550/arXiv.2310.15694"}, "doi_lower": "10.48550/arxiv.2310.15694"}
{"paper_id": 271746234, "title": "CPPO: Continual Learning for Reinforcement Learning with Human Feedback", "author_names": ["Han Zhang", "Yu Lei", "Lin Gui", "Min Yang", "Yulan He", "Hui Wang", "Ruifeng Xu"], "venue": "International Conference on Learning Representations", "abstract": null, "year": 2024, "publicationdate": null, "externalids": {}, "doi_lower": null}
{"paper_id": 261049152, "title": "Instruction Tuning for Large Language Models: A Survey", "author_names": ["Shengyu Zhang", "Linfeng Dong", "Xiaoya Li", "Sen Zhang", "Xiaofei Sun", "Shuhe Wang", "Jiwei Li", "Runyi Hu", "Tianwei Zhang", "Fei Wu", "Guoyin Wang"], "venue": "ACM Computing Surveys", "abstract": "This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of (instruction, output) pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users’ objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and application, along with analysis of aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.", "year": 2023, "publicationdate": "2023-08-21", "externalids": {"DOI": "10.1145/3777411"}, "doi_lower": "10.1145/3777411"}
{"paper_id": 258833440, "title": "XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters", "author_names": ["Xuanyu Zhang", "Qing Yang", "Dongliang Xu"], "venue": "International Conference on Information and Knowledge Management", "abstract": "Recently, with the popularity of ChatGPT, large-scale language models have experienced rapid development. However, there is a scarcity of open-sourced chat models specifically designed for the Chinese language, especially in the field of Chinese finance, at the scale of hundreds of billions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese chat model to date, built upon the BLOOM-176B architecture. Additionally, we propose a novel training method called hybrid-tuning to mitigate catastrophic forgetting. By integrating general and domain-specific knowledge, as well as combining the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable of providing accurate and contextually appropriate responses in the Chinese financial domain.", "year": 2023, "publicationdate": "2023-05-19", "externalids": {"DOI": "10.1145/3583780.3615285"}, "doi_lower": "10.1145/3583780.3615285"}
{"paper_id": 368182, "title": "Character-level Convolutional Networks for Text Classification", "author_names": ["Xiang Zhang", "J. Zhao", "Yann LeCun"], "venue": "Neural Information Processing Systems", "abstract": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.", "year": 2015, "publicationdate": "2015-09-04", "externalids": {}, "doi_lower": null}
{"paper_id": 247594532, "title": "Continual Sequence Generation with Adaptive Compositional Modules", "author_names": ["Yanzhe Zhang", "Xuezhi Wang", "Diyi Yang"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Continual learning is essential for real-world deployment when there is a need to quickly adapt the model to new tasks without forgetting knowledge of old tasks. Existing work on continual sequence generation either always reuses existing parameters to learn new tasks, which is vulnerable to catastrophic forgetting on dissimilar tasks, or blindly adds new parameters for every new task, which could prevent knowledge sharing between similar tasks. To get the best of both worlds, in this work, we propose continual sequence generation with adaptive compositional modules to adaptively add modules in transformer architectures and compose both old and new modules for new tasks. We also incorporate pseudo experience replay to facilitate knowledge transfer in those shared modules. Experiment results on various sequences of generation tasks show that our framework can adaptively add modules or reuse modules based on task similarity, outperforming state-of-the-art baselines in terms of both performance and parameter efficiency. We make our code public at https://github.com/GT-SALT/Adaptive-Compositional-Modules.", "year": 2022, "publicationdate": "2022-03-20", "externalids": {"DOI": "10.48550/arXiv.2203.10652"}, "doi_lower": "10.48550/arxiv.2203.10652"}
{"paper_id": 264426357, "title": "CITB: A Benchmark for Continual Instruction Tuning", "author_names": ["Zihan Zhang", "Meng Fang", "Ling Chen", "Mohammad-Reza Namazi-Rad"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Continual learning (CL) is a paradigm that aims to replicate the human ability to learn and accumulate knowledge continually without forgetting previous knowledge and transferring it to new tasks. Recent instruction tuning (IT) involves fine-tuning models to make them more adaptable to solving NLP tasks in general. However, it is still uncertain how instruction tuning works in the context of CL tasks. This challenging yet practical problem is formulated as Continual Instruction Tuning (CIT). In this work, we establish a CIT benchmark consisting of learning and evaluation protocols. We curate two long dialogue task streams of different types, InstrDialog and InstrDialog++, to study various CL methods systematically. Our experiments show that existing CL methods do not effectively leverage the rich natural language instructions, and fine-tuning an instruction-tuned model sequentially can yield similar or better results. We further explore different aspects that might affect the learning of CIT. We hope this benchmark will facilitate more research in this direction.", "year": 2023, "publicationdate": "2023-10-23", "externalids": {"DOI": "10.48550/arXiv.2310.14510"}, "doi_lower": "10.48550/arxiv.2310.14510"}
{"paper_id": 259370556, "title": "C-STANCE: A Large Dataset for Chinese Zero-Shot Stance Detection", "author_names": ["Chenye Zhao", "Yingjie Li", "Cornelia Caragea"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Zero-shot stance detection (ZSSD) aims to determine whether the author of a text is in favor of, against, or neutral toward a target that is unseen during training. Despite the growing attention on ZSSD, most recent advances in this task are limited to English and do not pay much attention to other languages such as Chinese. To support ZSSD research, in this paper, we present C-STANCE that, to our knowledge, is the first Chinese dataset for zero-shot stance detection. We introduce two challenging subtasks for ZSSD: target-based ZSSD and domain-based ZSSD. Our dataset includes both noun-phrase targets and claim targets, covering a wide range of domains. We provide a detailed description and analysis of our dataset. To establish results on C-STANCE, we report performance scores using state-of-the-art deep learning models. We publicly release our dataset and code to facilitate future research.", "year": 2023, "publicationdate": null, "externalids": {"DOI": "10.18653/v1/2023.acl-long.747"}, "doi_lower": "10.18653/v1/2023.acl-long.747"}
{"paper_id": 282949668, "title": "CEM: A Data-Efficient Method for Large Language Models to Continue Evolving From Mistakes", "author_names": ["Haokun Zhao", "Jinyi Han", "Jie Shi", "Chengyu Du", "Jiaqing Liang", "Yanghua Xiao", "Weikang Zhou", "Zeye Sun", "Fei Yu"], "venue": "Proceedings of the 34th ACM International Conference on Information and Knowledge Management", "abstract": "Large Language Models (LLMs) achieve remarkable success, but their static nature leads to inherent limitations and persistent mistakes in dynamic real-world scenarios. While Continual Instruction Tuning (CIT) and Continual Pre-training (CPT) are primary continual learning approaches, they struggle with scalable knowledge acquisition and maintaining model capabilities. To address these, we propose the Continue Evolving from Mistakes (CEM) method, a novel and data-efficient framework for continuous LLM evolution. Inspired by human learning, CEM establishes an iterative process: it efficiently collects targeted CPT data by robustly identifying LLM mistakes and uncertainties (via an Ambiguity-Aware Knowledge Collection (AAKC) algorithm), and employs a novel joint training paradigm that leverages CIT and CPT to assimilate knowledge efficiently while maintaining existing capabilities and mitigating catastrophic forgetting. Extensive experiments confirm CEM's effectiveness, yielding substantial accuracy gains for multiple models, increasing accuracy by up to 29.63%. Code and datasets are available on GitHub https://anonymous.4open.science/r/cem-BB25.", "year": 2025, "publicationdate": "2025-11-10", "externalids": {"DOI": "10.1145/3746252.3761096"}, "doi_lower": "10.1145/3746252.3761096"}
{"paper_id": 220961483, "title": "Memory-Efficient Class-Incremental Learning for Image Classification", "author_names": ["Hanbin Zhao", "Haibo Wang", "Yongjian Fu", "Fei Wu", "Xi Li"], "venue": "IEEE Transactions on Neural Networks and Learning Systems", "abstract": "With the memory-resource-limited constraints, class-incremental learning (CIL) usually suffers from the “catastrophic forgetting” problem when updating the joint classification model on the arrival of newly added classes. To cope with the forgetting problem, many CIL methods transfer the knowledge of old classes by preserving some exemplar samples into the size-constrained memory buffer. To utilize the memory buffer more efficiently, we propose to keep more auxiliary low-fidelity exemplar samples, rather than the original real-high-fidelity exemplar samples. Such a memory-efficient exemplar preserving scheme makes the old-class knowledge transfer more effective. However, the low-fidelity exemplar samples are often distributed in a different domain away from that of the original exemplar samples, that is, a domain shift. To alleviate this problem, we propose a duplet learning scheme that seeks to construct domain-compatible feature extractors and classifiers, which greatly narrows down the above domain gap. As a result, these low-fidelity auxiliary exemplar samples have the ability to moderately replace the original exemplar samples with a lower memory cost. In addition, we present a robust classifier adaptation scheme, which further refines the biased classifier (learned with the samples containing distillation label knowledge about old classes) with the help of the samples of pure true class labels. Experimental results demonstrate the effectiveness of this work against the state-of-the-art approaches. We will release the code, baselines, and training statistics for all models to facilitate future research.", "year": 2020, "publicationdate": "2020-08-04", "externalids": {"DOI": "10.1109/TNNLS.2021.3072041"}, "doi_lower": "10.1109/tnnls.2021.3072041"}
{"paper_id": 268513563, "title": "Reconstruct before Query: Continual Missing Modality Learning with Decomposed Prompt Collaboration", "author_names": ["Shu Zhao", "Xiaohan Zou", "Tan Yu", "Huijuan Xu"], "venue": "arXiv.org", "abstract": "Pre-trained large multi-modal models (LMMs) exploit fine-tuning to adapt diverse user applications. Nevertheless, fine-tuning may face challenges due to deactivated sensors (e.g., cameras turned off for privacy or technical issues), yielding modality-incomplete data and leading to inconsistency in training data and the data for inference. Additionally, continuous training leads to catastrophic forgetting, diluting the knowledge in pre-trained LMMs. To overcome these challenges, we introduce a novel task, Continual Missing Modality Learning (CMML), to investigate how models can generalize when data of certain modalities is missing during continual fine-tuning. Our preliminary benchmarks reveal that existing methods suffer from a significant performance drop in CMML, even with the aid of advanced continual learning techniques. Therefore, we devise a framework termed Reconstruct before Query (RebQ). It decomposes prompts into modality-specific ones and breaks them into components stored in pools accessible via a key-query mechanism, which facilitates ParameterEfficient Fine-Tuning and enhances knowledge transferability for subsequent tasks. Meanwhile, our RebQ leverages extensive multi-modal knowledge from pre-trained LMMs to reconstruct the data of missing modality. Comprehensive experiments demonstrate that RebQ effectively reconstructs the missing modality information and retains pre-trained knowledge. Specifically, compared with the baseline, RebQ improves average precision from 20.00 to 50.92 and decreases average forgetting from 75.95 to 8.56. Code and datasets are available on https://github.com/Tree-Shu-Zhao/RebQ.pytorch", "year": 2024, "publicationdate": "2024-03-17", "externalids": {"DOI": "10.48550/arXiv.2403.11373"}, "doi_lower": "10.48550/arxiv.2403.11373"}
{"paper_id": 267027714, "title": "SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models", "author_names": ["Weixiang Zhao", "Shilong Wang", "Yulin Hu", "Yanyan Zhao", "Bing Qin", "Xuanyu Zhang", "Qing Yang", "Dongliang Xu", "Wanxiang Che"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Existing methods devise the learning module to acquire task-specific knowledge with parameter-efficient tuning (PET) block and the selection module to pick out the corresponding one for the testing input, aiming at handling the challenges of catastrophic forgetting and knowledge transfer in CL. However, these methods tend to address only one of the challenges, ignoring the potential of aligning the two modules to effectively address catastrophic forgetting and knowledge transfer simultaneously. To this end, we propose a novel Shared Attention Framework (SAPT), to align the PET learning and selection via the Shared Attentive Learning \\&Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of SAPT. Moreover, SAPT consistently demonstrates its superiority when we scale it to different model sizes (from 770M to 13B), different model architectures (T5 and LLaMA-2) and unseen tasks.", "year": 2024, "publicationdate": "2024-01-16", "externalids": {"DOI": "10.18653/v1/2024.acl-long.625"}, "doi_lower": "10.18653/v1/2024.acl-long.625"}
{"paper_id": 267028471, "title": "Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer", "author_names": ["Junhao Zheng", "Qianli Ma", "Zhen Liu", "Binquan Wu", "Hu Feng"], "venue": "arXiv.org", "abstract": "Multimodal Continual Instruction Tuning (MCIT) enables Multimodal Large Language Models (MLLMs) to meet continuously emerging requirements without expensive retraining. MCIT faces two major obstacles: catastrophic forgetting (where old knowledge is forgotten) and negative forward transfer (where the performance of future tasks is degraded). Although existing methods have greatly alleviated catastrophic forgetting, they still suffer from negative forward transfer. We discover a large discrepancy in different input embeddings by performing singular value decomposition (SVD) on input embeddings. This discrepancy results in the model learning irrelevant information for old and pre-trained tasks, leading to catastrophic forgetting and negative forward transfer. To address these issues, we propose Prompt Tuning with Positive Forward Transfer (Fwd-Prompt), a prompt-based method that projects the prompt gradient to the residual space to minimize interference between tasks and to the pre-trained subspace for reusing pre-trained knowledge. Our experiments demonstrate that Fwd-Prompt achieves state-of-the-art performance while updating fewer parameters and requiring no old samples. Our research illuminates the potential of continuously adapting MLLMs to new tasks under the instruction tuning paradigm and encourages future studies to explore MCIT.", "year": 2024, "publicationdate": "2024-01-17", "externalids": {"DOI": "10.48550/arXiv.2401.09181"}, "doi_lower": "10.48550/arxiv.2401.09181"}
{"paper_id": 266191340, "title": "Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models", "author_names": ["Junhao Zheng", "Shengjie Qiu", "Qianli Ma"], "venue": "Annual Meeting of the Association for Computational Linguistics", "abstract": "Incremental Learning (IL) has been a long-standing problem in both vision and Natural Language Processing (NLP) communities. In recent years, as Pre-trained Language Models (PLMs) have achieved remarkable progress in various NLP downstream tasks, utilizing PLMs as backbones has become a common practice in recent research of IL in NLP. Most assume that catastrophic forgetting is the biggest obstacle to achieving superior IL performance and propose various techniques to overcome this issue. However, we find that this assumption is problematic. Specifically, we revisit more than 20 methods on four classification tasks (Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition) under the two most popular IL settings (Class-Incremental and Task-Incremental) and reveal that most of them severely underestimate the inherent anti-forgetting ability of PLMs. Based on the observation, we propose a frustratingly easy method called SEQ* for IL with PLMs. The results show that SEQ* has competitive or superior performance compared to state-of-the-art (SOTA) IL methods and requires considerably less trainable parameters and training time. These findings urge us to revisit the IL with PLMs and encourage future studies to have a fundamental understanding of the catastrophic forgetting in PLMs. The data, code and scripts are publicly available at https://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.", "year": 2023, "publicationdate": "2023-12-13", "externalids": {"DOI": "10.48550/arXiv.2312.07887"}, "doi_lower": "10.48550/arxiv.2312.07887"}
{"paper_id": 257496481, "title": "Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models", "author_names": ["Zangwei Zheng", "Mingyu Ma", "Kai Wang", "Ziheng Qin", "Xiangyu Yue", "Yang You"], "venue": "IEEE International Conference on Computer Vision", "abstract": "Continual learning (CL) can help pre-trained vision-language models efficiently adapt to new or under-trained data distributions without re-training. Nevertheless, during the continual training of the Contrastive Language-Image Pre-training (CLIP) model, we observe that the model’s zero-shot transfer ability significantly degrades due to catastrophic forgetting. Existing CL methods can mitigate forgetting by replaying previous data. However, since the CLIP dataset is private, replay methods cannot access the pre-training dataset. In addition, replaying data of previously learned downstream tasks can enhance their performance but comes at the cost of sacrificing zero-shot performance. To address this challenge, we propose a novel method ZSCL to prevent zero-shot transfer degradation in the continual learning of vision-language models in both feature and parameter space. In the feature space, a reference dataset is introduced for distillation between the current and initial models. The reference dataset should have semantic diversity but no need to be labeled, seen in pre-training, or matched image-text pairs. In parameter space, we prevent a large parameter shift by averaging weights during the training. We propose a more challenging Multi-domain Task Incremental Learning (MTIL) benchmark to evaluate different methods, where tasks are from various domains instead of class-separated in a single dataset. Our method outperforms other methods in the traditional class-incremental learning setting and the MTIL by 9.7% average score. Our code locates at https: //github.com/Thunderbeee/ZSCL.", "year": 2023, "publicationdate": "2023-03-12", "externalids": {"DOI": "10.1109/ICCV51070.2023.01752"}, "doi_lower": "10.1109/iccv51070.2023.01752"}
{"paper_id": 264405801, "title": "MarineGPT: Unlocking Secrets of Ocean to the Public", "author_names": ["Ziqiang Zheng", "Jipeng Zhang", "Tuan-Anh Vu", "Shizhe Diao", "Yue Him Wong Tim", "Sai-Kit Yeung"], "venue": "arXiv.org", "abstract": "Large language models (LLMs), such as ChatGPT/GPT-4, have proven to be powerful tools in promoting the user experience as an AI assistant. The continuous works are proposing multi-modal large language models (MLLM), empowering LLMs with the ability to sense multiple modality inputs through constructing a joint semantic space (e.g. visual-text space). Though significant success was achieved in LLMs and MLLMs, exploring LLMs and MLLMs in domain-specific applications that required domain-specific knowledge and expertise has been less conducted, especially for \\textbf{marine domain}. Different from general-purpose MLLMs, the marine-specific MLLM is required to yield much more \\textbf{sensitive}, \\textbf{informative}, and \\textbf{scientific} responses. In this work, we demonstrate that the existing MLLMs optimized on huge amounts of readily available general-purpose training data show a minimal ability to understand domain-specific intents and then generate informative and satisfactory responses. To address these issues, we propose \\textbf{MarineGPT}, the first vision-language model specially designed for the marine domain, unlocking the secrets of the ocean to the public. We present our \\textbf{Marine-5M} dataset with more than 5 million marine image-text pairs to inject domain-specific marine knowledge into our model and achieve better marine vision and language alignment. Our MarineGPT not only pushes the boundaries of marine understanding to the general public but also offers a standard protocol for adapting a general-purpose assistant to downstream domain-specific experts. We pave the way for a wide range of marine applications while setting valuable data and pre-trained models for future research in both academic and industrial communities.", "year": 2023, "publicationdate": "2023-10-20", "externalids": {"DOI": "10.48550/arXiv.2310.13596"}, "doi_lower": "10.48550/arxiv.2310.13596"}
{"paper_id": 202541184, "title": "“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding", "author_names": ["Ben Zhou", "Daniel Khashabi", "Qiang Ning", "D. Roth"], "venue": "Conference on Empirical Methods in Natural Language Processing", "abstract": "Understanding time is crucial for understanding events expressed in natural language. Because people rarely say the obvious, it is often necessary to have commonsense knowledge about various temporal aspects of events, such as duration, frequency, and temporal order. However, this important problem has so far received limited attention. This paper systematically studies this temporal commonsense problem. Specifically, we define five classes of temporal commonsense, and use crowdsourcing to develop a new dataset, MCTACO, that serves as a test set for this task. We find that the best current methods used on MCTACO are still far behind human performance, by about 20%, and discuss several directions for improvement. We hope that the new dataset and our study here can foster more future research on this topic.", "year": 2019, "publicationdate": "2019-09-06", "externalids": {"DOI": "10.18653/v1/D19-1332"}, "doi_lower": "10.18653/v1/d19-1332"}
{"paper_id": 226964491, "title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "author_names": ["Wangchunshu Zhou", "Dong-Ho Lee", "Ravi Kiran Selvam", "Seyeon Lee", "Bill Yuchen Lin", "Xiang Ren"], "venue": "International Conference on Learning Representations", "abstract": "Pre-trained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational commonsense knowledge about everyday concepts, which is crucial to many downstream tasks that need common sense to understand or generate. To augment PTLMs with concept-centric commonsense knowledge, in this paper, we propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self-supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream datasets). Furthermore, we develop a joint pre-training framework to unify generative and contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that our method, concept-aware language model (CALM), can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks. We show that while only incrementally pre-trained on a relatively small corpus for a few steps, CALM outperforms baseline methods by a consistent margin and even comparable with some larger PTLMs, which suggests that CALM can serve as a general, plug-and-play method for improving the commonsense reasoning ability of a PTLM.", "year": 2020, "publicationdate": "2020-10-24", "externalids": {}, "doi_lower": null}
{"paper_id": 267751282, "title": "Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models", "author_names": ["Didi Zhu", "Zhongyi Sun", "Zexi Li", "Tao Shen", "Ke Yan", "Shouhong Ding", "Kun Kuang", "Chao Wu"], "venue": "International Conference on Machine Learning", "abstract": "Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks. This paper presents a comprehensive analysis of catastrophic forgetting in MLLMs and introduces a post-training adjustment method called Model Tailor. Our method primarily preserves the pre-trained parameters while replacing a small number ($\\leq$ 10\\%) of fine-tuned parameters, maintaining $\\sim$ 99\\% effectiveness on original tasks versus pre-training, and achieving $\\sim$ 97\\% on new tasks compared to standard fine-tuning. Specifically, we derive a sparse mask to identify the\"model patch\", based on a fusion strategy that integrates salience and sensitivity analysis. Subsequently, a compensation mechanism is introduced to\"decorate the patch\", enhancing the model's performance on both target and original tasks. Additionally, our method is adaptable to multi-task scenarios. Through extensive experiments on InstructBLIP and LLaVA-1.5 in both image captioning and visual question answering tasks, our approach demonstrates significant task adaptability while preserving inherent pre-trained capabilities.", "year": 2024, "publicationdate": "2024-02-19", "externalids": {"DOI": "10.48550/arXiv.2402.12048"}, "doi_lower": "10.48550/arxiv.2402.12048"}
