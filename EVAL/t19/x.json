{
  "survey": "Continual learning is an increasingly vital paradigm for large language models (LLMs), addressing significant challenges such as catastrophic forgetting and enabling models to adapt to new information while retaining previously acquired knowledge. This comprehensive survey examines various methodologies, including memory-based, architecture-based, regularization-based, and hybrid approaches, that underpin continual learning in LLMs. The survey highlights the importance of scalable and efficient neural architectures, emphasizing techniques like Gradient Episodic Memory (GEM) and orthogonal low-rank adaptation (O-LoRA) that mitigate forgetting and enhance knowledge retention. It explores adaptive algorithms, such as dynamic learning rate strategies and efficient model update mechanisms, which optimize learning processes in resource-constrained environments. Challenges such as scalability, catastrophic forgetting, adaptability, and ethical considerations are thoroughly analyzed, with future directions proposed to advance the field. The survey underscores the potential of continual learning strategies to propel LLMs toward becoming robust, adaptable models capable of seamlessly integrating new knowledge while preserving essential prior information across diverse applications. By focusing on these areas, researchers can address existing gaps and foster the development of more efficient and responsive models, ensuring their relevance and efficacy in evolving AI landscapes.\n\nIntroduction Importance of Continual Learning for LLMs Continual learning is vital for large language models (LLMs), addressing challenges such as catastrophic forgetting and adaptation to new data distributions. This paradigm is particularly crucial in fields like biomedical natural language processing, where pre-trained models often lack generative capabilities. By facilitating the integration of information from continuously growing data sources, continual learning ensures LLMs remain relevant and effective [1]. The necessity for continual learning is underscored in situations where neural networks must acquire new capabilities without access to original training data, overcoming traditional retraining limitations [2]. This approach aligns language models with user intent, addressing the frequent misalignment between model outputs and user expectations [3]. Additionally, continual learning enables efficient fine-tuning of large models while minimizing memory usage, which is essential for optimizing computational resources [4]. In specialized domains such as financial language understanding in the Chinese context, continual learning is indispensable for adapting LLMs to specific requirements [5]. It also plays a critical role in correcting prediction errors while preventing catastrophic forgetting in out-of-distribution data streams [6]. By allowing LLMs to assimilate new tasks while retaining previously acquired knowledge, continual learning enhances their effectiveness in addressing complex, evolving challenges across various applications. Challenges Addressed by Continual Learning Continual learning in LLMs is essential for overcoming several inherent challenges, with catastrophic forgetting being a primary concern. This phenomenon occurs when models forget previously learned information upon receiving new data, particularly in class incremental learning scenarios. The WIKIREADING benchmark addresses this by integrating unstructured text with structured knowledge bases, facilitating better retention of past knowledge [7]. Maintaining a network's original capabilities while training on new tasks without prior data access is another significant challenge [2]. The misalignment between model outputs and user intent can lead to undesirable results from large models [3]. This issue is compounded by the substantial memory requirements for fine-tuning models like GPT-3, which pose practical deployment challenges [4]. Furthermore, the computational costs associated with exhaustive pre-training on all existing data add to the complexity [1], as does the necessity to process incoming data streams while retaining past knowledge within computational limits [8]. In specialized domains, such as financial language processing in Chinese, existing benchmarks often fail to address unique aspects, complicating model adaptation [5]. Similarly, in biomedicine, the lack of effective generative models restricts the application of discriminative models in producing coherent biomedical content [9]. Challenges related to boundary-agnostic and non-stationary distribution shifts further complicate continual learning efforts [6]. These issues highlight the urgent need for advanced continual learning strategies to enhance LLM adaptability and efficiency in retaining and applying knowledge across diverse tasks. Scope of the Survey This survey provides a comprehensive exploration of continual learning methodologies tailored for LLMs, focusing on overcoming catastrophic forgetting and facilitating effective knowledge transfer across various domains. It critically examines the limitations of current continual learning frameworks, which often fail to account for the complexities of real-world scenarios [6]. Emphasizing the need for benchmarks that evaluate continual learning strategies under realistic constraints, this survey promotes efficient lifelong pre-training. Central to the survey is the investigation of domain-specific pretraining techniques, particularly in specialized fields like biomedical natural language processing, where benchmarks such as BLURB are crucial [10]. The survey underscores the importance of optimizing memory usage during the fine-tuning of large models, highlighting approaches like QLoRA that are instrumental in achieving this optimization [4]. Additionally, it explores the integration of human feedback in fine-tuning processes to enhance model alignment with user intent, addressing prevalent misalignments in outputs [3]. The survey also examines methodologies that enhance natural language understanding capabilities, reflecting the increasing demand for models adept at processing large-scale data [7]. It introduces innovative approaches for assessing the impact of continual pre-training on LLMs, particularly concerning knowledge retention and model reliability, areas traditionally underexplored [11]. In the context of financial language models, benchmarks such as BBT-CFLEB are considered essential for evaluating the efficacy of continual learning techniques [5]. Through a detailed analysis of these methodologies and evaluation criteria, the survey aims to advance the field of continual learning in LLMs, bridging existing gaps and fostering the development of robust, adaptable models capable of seamlessly integrating new knowledge while preserving previously acquired information [12]. Structure of the Survey The survey is organized to systematically address the multifaceted challenges of continual learning in LLMs, categorizing current methodologies into distinct stages based on their effectiveness in mitigating issues such as catastrophic forgetting and enhancing knowledge transfer [13]. This organization provides a comprehensive overview of existing strategies while emphasizing resource efficiency and generalizability across diverse tasks. Initially, the survey delineates methods applicable across various domains, including software engineering, where code summarization, generation, and translation are critical for continual learning applications [14]. This segmentation allows for focused analyses of techniques tailored to specific task requirements, facilitating a nuanced understanding of the adaptability and effectiveness of different approaches. Subsequent sections delve into online learning strategies, neural network architectures, and adaptive algorithms, each explored in detail to highlight their contributions to the continual learning paradigm. By systematically categorizing these methodologies, the survey aims to provide a clear roadmap for advancing the field, ultimately fostering the development of robust LLMs capable of seamlessly integrating new information while preserving previously acquired knowledge.The following sections are organized as shown in . Background and Definitions Continual Learning Continual learning enables AI models, particularly large language models (LLMs), to assimilate new tasks while preserving existing knowledge, crucial for benchmarks like WIKIREADING [12,7]. This approach enhances task adaptation efficiency by leveraging empirical scaling laws [15]. Pre-training is instrumental in lifelong learning, mitigating catastrophic forgetting in sequential task learning, as seen in BioGPT's adaptation to biomedical literature [16,9]. Continual learning addresses theoretical challenges in class incremental learning (CIL), allowing incremental task acquisition without knowledge loss [17]. In specialized domains such as financial NLP, continual learning supports benchmarks like BBT-CFLEB [5] and enhances coreference resolution in reading comprehension tasks, exemplified by Quoref [18]. This paradigm ensures a stability-plasticity trade-off, promoting generalizability across tasks and enabling LLMs to maintain and update world knowledge for tasks like question answering and fact-checking. Benchmarks for Continual Knowledge Learning (CKL) emphasize retaining time-invariant knowledge, updating outdated information, and acquiring new insights, crucial for LLM adaptability in dynamic contexts [19,13,20]. Continual learning is vital for models leveraging past experiences to enhance performance across diverse tasks. Large Language Models (LLMs) Large Language Models (LLMs) are pivotal in AI, facilitating robust natural language processing across diverse applications. Models like GPT-4 handle multimodal inputs, enhancing applicability in complex tasks [21]. Their adaptability is demonstrated in multilingual processing, aided by datasets like Sailor [22]. BERT advances language representation through deep bidirectional pre-training, enhancing comprehension and generation capabilities [23]. Expanding datasets, encompassing trillions of tokens, are essential for LLMs to adapt to evolving linguistic environments [24]. Llama 2 focuses on dialogue applications, optimizing communication in AI interactions [25]. LLMs extend beyond general-purpose applications, with FinBERT addressing financial tasks [26], SaulLM-7B optimizing legal text comprehension [27], and Llemma specializing in mathematical problem-solving [28]. In healthcare, models like Hippocrates innovate medical diagnostics [29]. Despite their potential, LLMs face challenges in continual learning, particularly when pre-training is not leveraged effectively [30]. Addressing these challenges is crucial for optimizing task performance and advancing LLMs within modern AI systems. Online Learning Online learning is a dynamic approach enabling models to adapt incrementally to new data, beneficial for LLMs requiring continuous updates [12]. It manages non-stationary data distributions, allowing parameter updates without full retraining [8]. For LLMs, online learning is essential for maintaining relevance in rapidly changing environments, such as social media analysis and real-time translation [24]. Implementing online learning involves techniques addressing catastrophic forgetting and computational efficiency, enabling models to process streaming data and adapt to evolving linguistic patterns [6]. This adaptability is vital in domains like digital marketing, where language usage changes rapidly [13]. Online learning supports effective operation in resource-constrained settings, employing adaptive algorithms to optimize performance while minimizing resource consumption [1,8]. This adaptability is crucial for deploying LLMs in edge computing environments, where real-time processing is essential [11]. Beyond practical applications, online learning advances continual learning frameworks, providing insights into LLM scalability and robustness [6]. By incorporating online learning techniques, models can retain knowledge and apply it to new tasks, enhancing utility across applications [15]. Neural Networks Deep neural networks (DNNs) are fundamental to LLM architecture, facilitating hierarchical feature extraction for advanced language understanding and generation. In continual learning, DNNs maintain performance while adapting to new information and correcting errors [31]. They capture complex linguistic and relational knowledge from extensive textual data, enabling human-like text generation and understanding. Pretraining on vast world knowledge aids various tasks, including domain-specific applications in law and medicine. Challenges arise in adapting models to specific domains due to potential hallucinations and lack of domain-specific knowledge. Frameworks like Lawyer LLaMA incorporate domain knowledge during training, utilizing retrieval modules for grounding outputs [32,33,34]. Techniques like fine-tuning and transfer learning enhance neural network adaptability, leveraging existing knowledge while integrating new insights. Neural networks advance continual learning by supporting incremental updates and error correction, addressing catastrophic forgetting and enabling task adaptation without full retraining. Insights from neuroscience and fields like knowledge editing focus on precise parameter modifications to maintain performance across tasks [35,36]. This adaptability is critical in dynamic environments, such as real-time data streams, where adjusting to new information without forgetting prior knowledge is essential. Memory-enhanced architectures and adaptive algorithms contribute to LLM robustness and scalability, ensuring efficacy across applications. Adaptive Algorithms Adaptive algorithms are key in continual learning, allowing LLMs to adjust dynamically to new data while preserving prior knowledge. These algorithms enhance LLM robustness and utility across applications by managing evolving tasks and data distributions. For instance, Rho-1 optimizes learning by selectively training on useful tokens [37]. InsCL's Instruction Information Metric (InsInfo) quantifies instruction complexity and diversity, facilitating adaptive strategies [38]. Continual Parameter-Efficient Tuning (ConPET) offers generalizable continual task adaptation, maintaining low training complexity [39]. LoRA exemplifies adaptive algorithms by freezing model weights and training low-rank matrices, enabling task adaptation without forgetting [40]. Adaptive algorithms also address spatial disorder, as seen in Mod-X, aligning representation space with learned data to mitigate performance decline [41]. HMA increases feature diversity in overlapping feature spaces, maintaining performance across alignment scenarios [42]. The GQA dataset emphasizes developing models that adapt to diverse reasoning tasks, ensuring performance across question types [43]. In specialized domains, domain-specific pretraining enhances performance in biomedical NLP tasks, highlighting the need for adaptive algorithms to adjust to new information without forgetting [10]. Lifelong-MoE facilitates adaptive learning by expanding model expertise while retaining prior knowledge [44]. QLoRA demonstrates efficient finetuning, achieving state-of-the-art results with reduced memory requirements [4]. Despite their impact, existing continual learning methods face challenges under realistic constraints, necessitating more efficient approaches [8]. Integrating adaptive algorithms allows models to retain knowledge and apply it to new tasks, enhancing utility across applications [6]. Techniques in Continual Learning Continual learning is a vital paradigm in AI, especially for large language models (LLMs), facilitating the acquisition of new tasks while retaining prior knowledge. This section explores techniques enhancing continual learning, focusing on memory-based methods, which are crucial for improving knowledge retention and adaptability, mitigating catastrophic forgetting, and enabling effective knowledge transfer across tasks. illustrates the hierarchical classification of techniques in continual learning for LLMs, categorized into memory-based, architecture-based, regularization-based, and hybrid methods. Each category is further divided into key techniques or approaches that enhance knowledge retention, adaptability, and effective knowledge transfer across tasks, thereby providing a comprehensive overview of the strategies available for advancing continual learning in this context. Additionally, Table presents a detailed comparison of different methods in continual learning for large language models, illustrating their unique features and domain applications. Memory-Based Methods Memory-based methods are essential in the continual learning framework for LLMs, offering mechanisms for knowledge storage and retrieval to mitigate catastrophic forgetting and enhance adaptability. As illustrated in , which categorizes memory-based methods in continual learning, key approaches in knowledge storage, frameworks, and learning paradigms are highlighted. Table presents a comprehensive overview of memory-based methods employed in the continual learning framework for large language models, emphasizing their roles in knowledge storage, learning paradigms, and domain-specific applications. The Gradient Episodic Memory (GEM) method leverages episodic memory to reduce forgetting and promote knowledge transfer [12]. WIKIREADING, with its extensive dataset, exemplifies a memory-based approach supporting models in learning from vast data [7]. BioGPT employs extensive pre-training on biomedical literature, representing a memory-based strategy that bolsters continual learning and enhances text generation [9]. The theoretical framework by [17] aligns class incremental learning (CIL) with out-of-distribution (OOD) detection, emphasizing within-task prediction (WP) and task-id prediction (TP) as critical components. BBT-FinT5 enhances financial language understanding and generation, demonstrating memory-based methods' utility in specialized domains [5]. TextVQA and Quoref benchmarks exemplify memory-based techniques that utilize datasets to support continual learning in text-based reasoning and coreference resolution [45,18]. InstructGPT uses supervised learning and reinforcement learning from human feedback to enhance continual learning, highlighting memory's role in aligning model outputs with user expectations [3]. QLoRA, a finetuning method, utilizes a frozen, quantized pretrained model to backpropagate gradients into Low Rank Adapters, reducing memory requirements while enhancing performance [4]. ELLE integrates model expansion and domain-specific prompts to optimize continual learning [1]. These methods are indispensable for supporting the continual learning paradigm, enabling LLMs to retain and apply knowledge across diverse tasks and domains, thereby enhancing adaptability and robustness. Architecture-Based Methods Architecture-based methods in continual learning for LLMs focus on structural modifications that enhance adaptability and mitigate catastrophic forgetting. The DEMix layer facilitates continual learning by allowing the addition and removal of domain-specific expert networks, improving adaptability across domains while retaining acquired information [46]. CodeT5+ enhances its ability to learn continuously and adapt to new programming languages and paradigms [47]. Editable Training introduces architectural modifications for targeted corrections within neural networks, maintaining performance while adapting to new data [31]. Domain-adversarial training methods incorporate a gradient reversal layer, facilitating learning across diverse environments [48]. An ensemble approach involves training multiple small models simultaneously, promoting knowledge transfer and retention [49]. The Vision Transformer (ViT) supports continual learning by efficiently processing visual data and adapting to new classification challenges [50]. These methods optimize neural network structures, fostering continual learning and knowledge editing, supporting adaptable, efficient models capable of overcoming challenges like catastrophic forgetting. This progress aids in creating systems that dynamically address evolving AI application demands, such as reinforcement learning and neural code intelligence [51,35,36,14]. Regularization-Based Methods Regularization-based methods are integral to the continual learning framework for LLMs, offering strategies to mitigate forgetting and preserve knowledge across tasks. Learning without Forgetting allows models to train on new task data while preserving original capabilities [2]. The decomposition of CIL into WP and TP highlights regularization techniques' importance in continual learning [17]. Lifelong-MoE facilitates adaptation to data shifts while preserving learned knowledge, emphasizing dynamic adjustment mechanisms [44]. QLoRA introduces quantization techniques enhancing memory efficiency [4]. ELLE allows continuous knowledge integration without full retraining, emphasizing efficient integration's importance [1]. These methods enhance continual learning by facilitating new domain-specific knowledge integration while preserving existing information, particularly valuable in specialized fields like law and medicine. LLMs benefit from expert knowledge injection and supervised fine-tuning tasks, improving problem-solving abilities and mitigating hallucination issues by incorporating retrieval modules for relevant information [33,34]. Hybrid Methods Hybrid methods in continual learning for LLMs synthesize multiple strategies to enhance adaptability and generalization across tasks. CITF enables zero-shot generalization and improved performance through explicit separation of task domains from task functions [52]. The benchmark by [53] provides a framework for assessing language models' temporal reasoning capabilities. [54] introduces a hybrid method combining multitask learning with zero-shot evaluation, enhancing generalization across tasks. BioGPT combines generative capabilities with continual learning in the biomedical field [9]. InstructGPT integrates supervised and reinforcement learning to enhance performance [3]. Lifelong-MoE incorporates new experts while employing regularized pretraining [44]. The dataset by [6] provides dynamic out-of-distribution data streams for evaluating continual learning capabilities. TextVQA combines approaches to improve learning outcomes in visual question answering tasks [45]. These hybrid methods advance continual learning capabilities, enabling LLMs to integrate new knowledge while preserving existing information, contributing to adaptable and efficient models addressing dynamic AI application demands. Online Learning Strategies Incremental Learning Techniques Incremental learning techniques are essential for LLMs to continuously adapt to new data while maintaining prior knowledge, thereby reducing computational and memory overhead and enhancing efficiency in dynamic contexts. As illustrated in , which depicts the hierarchical categorization of incremental learning techniques, these approaches focus on model expansion, class-incremental learning, and optimization methods, each contributing to the continuous adaptation and efficiency of language models in dynamic environments. ELLE exemplifies this by expanding model width and depth to integrate new information effectively [1]. iCaRL starts with a limited set of classes, gradually incorporating new ones to foster robust classifiers and data representations, outperforming traditional methods in experiments on CIFAR-100 and ImageNet ILSVRC 2012 data [55,56,38,17,57]. FLAN fine-tunes LLMs across diverse tasks using natural language instructions, ensuring adaptability across linguistic inputs. MER optimizes parameters through gradient alignment, enhancing the model's capacity to learn from new data. Proximal Policy Optimization (PPO) in reinforcement learning alternates between sampling data via environmental interactions and optimizing a surrogate objective function using stochastic gradient ascent, allowing multiple epochs of minibatch updates for improved sample efficiency [58,59,60,61]. The Lifelong-MoE method dynamically integrates additional experts based on data distribution shifts, while LoRA facilitates efficient adaptation to downstream tasks without full model retraining. Incrementally trained models, such as those using the Model Zoo approach, employing ensembles of small models for continual learning, outperform traditional models, reducing training costs and enhancing performance in tasks like Country Hashtag Prediction and OffensEval [62,49,63]. However, the environmental costs of continuous training require careful evaluation, as temporal adaptation does not always guarantee improved performance. In NLP, studies on BERT emphasize incremental learning's significance, especially with sparse or absent memory replay, in text classification and extractive question answering. The WIKIREADING dataset underscores incremental learning's necessity for models to adapt continuously while maintaining performance on previous tasks. Continual pre-training techniques optimized for Japanese tasks illustrate incremental learning's broad applicability across linguistic contexts. Incremental learning techniques enhance LLMs' continual learning capabilities, enabling them to integrate new knowledge while preserving existing information. This adaptability is crucial for AI applications facing rapidly changing data and knowledge domains, addressing challenges like catastrophic forgetting and allowing LLMs to retain time-invariant knowledge, update outdated information, and acquire new insights, thereby improving performance across dynamic tasks and scenarios [19,20,13,56,64]. Streaming Data Processing Streaming data processing is crucial for maintaining LLMs' relevance and accuracy in dynamic environments with continuous data generation. This process allows LLMs to adapt to evolving content landscapes, including semantic shifts and new entities, by employing incremental training methods and continual learning algorithms to update understanding of new tokens while retaining earlier knowledge [62,64]. Real-time integration of new information into existing models without extensive retraining ensures LLMs remain effective. Continual training of CLIP models using streaming data highlights the unique challenges posed by adapting to non-stationary data distributions, critical in streaming contexts [41]. Efficient streaming data processing is vital for applications like real-time language translation, social media analysis, and personalized content recommendations, where timely model parameter updates are essential. Techniques for processing streaming data in LLMs often incorporate sophisticated incremental learning strategies to address continuously evolving content challenges. Implementing methods like continual pretraining and dynamic vocabulary composition allows LLMs to handle non-stationary data streams effectively, enhancing performance on out-of-distribution tasks while improving temporal generalization [56,62,6,64]. Adaptive algorithms dynamically adjust learning rates and optimize model architectures to accommodate information influx without degrading performance. Memory-based methods play a crucial role in streaming data processing, enabling models to selectively retain and recall relevant information from past data streams, thus mitigating catastrophic forgetting. Integrating streaming data processing methodologies into LLMs significantly enhances their operational effectiveness in real-time applications. This approach ensures LLMs remain robust and adaptable, continually refining models to address prediction errors in out-of-distribution data streams and overcoming challenges like boundary-agnostic shifts and diverse data clusters. Augmenting code-focused LLMs with comment generation and filtering strategies enhances programming skills by aligning programming languages with natural languages, boosting performance in dynamic environments [65,6]. Addressing streaming data challenges enables researchers to develop more efficient and responsive models capable of meeting modern AI applications' demands. Neural Network Architectures Transformer-Based Architectures Transformer-based architectures are central to large language models (LLMs), leveraging attention mechanisms for effective continual learning. As illustrated in , key aspects of these architectures are depicted, particularly their applications in biomedical text generation, learning strategies, and ethical considerations. BioGPT exemplifies the utility of transformers in biomedical text generation and mining, showcasing adaptability in domain-specific applications [9]. ELLE further enhances models like BERT and GPT by facilitating efficient new data integration [1]. The figure highlights the importance of scaling laws and MER in learning strategies, providing theoretical insights into transformer performance in continual learning and predicting scalability through power-law relationships [15]. MER emphasizes gradient alignment within transformer architectures to reduce interference and promote positive transfer [59]. Evaluations of moral understanding in language models, as seen in [66], illustrate the capacity of transformer-based models to integrate complex concepts, underscoring the integration of ethical concepts as demonstrated by moral understanding evaluations. The evolution of transformer architectures enhances LLMs' ability to process non-stationary data streams, integrating new information while preserving existing knowledge. Techniques such as incremental training, replay strategies, and continual pretraining enable rapid adaptation to semantic shifts, improving knowledge transfer and achieving state-of-the-art performance in tasks like image geo-localization, country hashtag prediction, and text classification [67,62,68,64]. Modular and Adaptive Architectures Modular and adaptive architectures bolster LLM continual learning by dynamically adjusting model structures to incorporate new data. Lifelong-MoE exemplifies this, expanding model capacity through expert addition, optimizing adaptability while preserving prior knowledge [44]. These architectures facilitate new information integration without extensive retraining, ensuring efficiency in rapidly changing environments. Modular components enable targeted updates that reduce catastrophic forgetting, utilizing lateral connections and knowledge editing techniques [51,35,36]. This approach is beneficial in domains like finance and biomedicine, where domain-specific expertise is crucial. Dynamic learning algorithms further enhance adaptability by adjusting model parameters based on incoming data streams, maintaining performance in non-stationary data distributions. Structured code representations and ensemble methods like Model Zoo, inspired by boosting techniques, improve accuracy in continual learning scenarios, optimizing generalization and adaptability to new challenges [69,49]. Memory-Enhanced Architectures Memory-enhanced architectures are crucial for augmenting LLM continual learning, integrating mechanisms for information retention and retrieval across tasks. These architectures use memory modules to facilitate knowledge storage, mitigating catastrophic forgetting and enhancing adaptability in dynamic environments. Gradient Episodic Memory (GEM) exemplifies this approach, employing episodic memory to support task knowledge transfer and retention [12]. Benchmarks like WIKIREADING leverage large datasets to improve models' information processing and recall capabilities [7]. In biomedicine, BioGPT utilizes extensive pre-training as a memory-based strategy to enhance continual learning and text generation [9]. The theoretical framework by [17] aligns class incremental learning (CIL) with out-of-distribution (OOD) detection, emphasizing within-task prediction (WP) and task-id prediction (TP) as critical components. Memory-enhanced architectures advance LLM continual learning by enabling dynamic integration of new information through distributed episodic memory systems like Larimar, facilitating efficient updates without extensive retraining. This approach addresses challenges like catastrophic forgetting and outdated information, evidenced by benchmarks like TRACE and Continual Knowledge Learning (CKL), ensuring LLMs maintain general abilities while adapting to new tasks [70,19,71,72]. Multimodal and Specialized Architectures Multimodal and specialized architectures enhance LLM capabilities by integrating diverse data inputs and tailoring structures to specific tasks. These architectures process complex data formats, such as text, images, and audio, broadening applicability. CLIP employs contrastive learning to align visual and textual representations, achieving robust multimodal understanding through pre-training on 400 million image-text pairs, enabling effective zero-shot transfer to various tasks. Challenges in continual learning, such as spatial disorder in representation space, are addressed by frameworks like Mod-X to maintain alignment during training [73,20,74,41,30]. In specialized domains, architectures like FinBERT cater to financial sector tasks, where general-purpose models often underperform [26]. SaulLM-7B, optimized for legal text comprehension, demonstrates proficiency in domain-specific tasks using the Mistral 7B architecture [27]. Specialized architectures like Llemma for mathematical problem-solving further illustrate customization for unique domain requirements [28]. These architectures leverage advanced techniques to enhance performance in processing multimodal inputs and specialized tasks. Attention mechanisms and modular components within neural networks improve efficiency in handling diverse data types. This approach allows adaptation to specific domain requirements while maintaining generalization capabilities, addressing challenges like catastrophic forgetting and enabling continual learning across tasks. Techniques like knowledge editing and domain-adaptive pretraining facilitate precise, data-efficient updates without full retraining, ensuring relevance and accuracy in dynamic environments. Multimodal reasoning and thought chains enhance reasoning and interpretability, demonstrated in benchmarks like ScienceQA, where language models achieve superior performance with less data through explanatory frameworks [75,14,74,36,35]. Sophisticated architectural modifications enable models to process multimodal data while addressing specialized application demands, advancing continual learning in LLMs. Adaptive Algorithms Adaptive algorithms play a pivotal role in optimizing large language models (LLMs) by dynamically adjusting learning parameters to accommodate evolving datasets and tasks. Traditional static learning rates often fail to address these complexities, necessitating adaptive strategies that enhance model responsiveness and accuracy. This section delves into adaptive learning rate strategies within the continual learning framework, underscoring their significance in enabling LLMs to assimilate new information efficiently while preserving existing knowledge. Adaptive Learning Rate Strategies Adaptive learning rate strategies are crucial for LLMs in continual learning, facilitating effective updates while retaining past knowledge. The Hippocrates framework exemplifies the need for adapting to new medical data without forgetting prior information [29]. Similarly, BioGPT utilizes these strategies to optimize performance across diverse biomedical NLP tasks, ensuring the integration of new knowledge while maintaining generative capabilities [9]. SaulLM-7B employs instructional fine-tuning tailored for legal texts, highlighting the importance of domain-specific adaptations in learning rate strategies [27]. As illustrated in , the application of adaptive learning rate strategies spans various domains, emphasizing key methods and metrics. In the medical domain, strategies are exemplified by the Hippocrates framework and the optimization achieved with BioGPT. The legal domain benefits from the tuning capabilities of SaulLM-7B. Furthermore, efficiency improvements and performance metrics are demonstrated through the (IA)$^3$ method and the ROUGE metric. In few-shot learning, the (IA)$^3$ method demonstrates the efficacy of fine-tuning limited parameters, significantly enhancing performance through adaptive learning rate strategies [76]. Metrics like ROUGE assess the overlap between generated and reference summaries, providing a quantitative measure of summarization effectiveness and underscoring the role of adaptive strategies in optimizing model performance [77]. These strategies enable models to dynamically adjust learning rates in response to new data, enhancing their ability to learn efficiently from limited inputs. Efficient Model Update Mechanisms Efficient model update mechanisms are vital for LLMs to adapt to new data while retaining prior knowledge, thereby improving performance in dynamic environments. The kNN-OCL method exemplifies this efficiency by continuously adapting the kNN classifier with features extracted from a fixed feature extractor, ensuring responsiveness to evolving data streams [78]. IT-BERT similarly leverages existing knowledge to adapt efficiently to new information, maintaining high performance amidst changing content [62]. Editable Training allows localized adjustments to neural network parameters, correcting specific errors without extensive retraining, thus maintaining accuracy while minimizing computational overhead [31]. The Vision Transformer showcases the potential for efficient updates, achieving excellent results in image classification with reduced computational resources, suggesting applicability to LLMs [50]. KFO-LA employs recursive updates to the weight posterior after each task, facilitating efficient model adaptation and ensuring integration of new knowledge without compromising previously learned information [79]. Collectively, these mechanisms emphasize the importance of efficient update strategies in enhancing the adaptability and robustness of LLMs as they encounter new data and tasks across various applications. Selective Knowledge Retention Selective knowledge retention is crucial in continual learning for LLMs, ensuring models preserve essential information from previous tasks while effectively integrating new data. The Gradient Episodic Memory (GEM) method exemplifies this by maintaining a memory of past tasks, facilitating beneficial knowledge transfer [12]. This approach is vital for mitigating catastrophic forgetting, a significant challenge in real-world LLM applications [11]. The WIKIREADING benchmark highlights the importance of selective knowledge retention by assessing how well models retain critical information while integrating extensive new data, such as that from Wikipedia [7]. The Learning without Forgetting approach emphasizes maintaining performance on old tasks without access to their training data, addressing a notable limitation of existing methods [2]. An empirical study indicates that generic pre-training can alleviate catastrophic forgetting, with proposed optimization approaches outperforming existing methods without requiring memory that scales with the number of tasks [16]. Metrics such as accuracy and F1-score are pivotal in evaluating the effectiveness of selective knowledge retention methods in continual learning contexts [80]. Incorporating direct human feedback in training processes is emphasized as a means to guide model outputs, ensuring selective retention aligns with user expectations and enhances reliability [3]. The development and implementation of selective knowledge retention strategies are essential for fostering LLMs' adaptability and robustness, enabling effective application of prior experiences to new tasks and environments. These strategies enhance LLMs' capability to manage temporal knowledge by integrating domain-specific expertise while maintaining up-to-date factual information, providing a robust foundation for future advancements in continual learning paradigms, such as Continual Knowledge Learning (CKL), which aims to prevent catastrophic forgetting and efficiently update models with new information without extensive retraining [19,32,81,34,53]. Challenges and Future Directions Scalability and Efficiency in Neural Architectures Scalability and efficiency are crucial challenges in neural architecture development for LLMs, affecting their adaptability and dataset processing capabilities. Lifelong-MoE exemplifies scalable architecture by maintaining prior knowledge without increasing computational costs, thereby enhancing continual learning [44]. However, current benchmarks often fail to capture real-world complexities, limiting their effectiveness in evaluating scalability and efficiency [8]. Tailored benchmarks like BBT-FinT5 are vital in financial language processing to address specific overlooked aspects, highlighting the need for specialized evaluation frameworks [5]. Oncontinua99 fills evaluation gaps by providing a framework for realistic scenarios, aiding the development of scalable models that efficiently integrate new information [6]. Efficient alternatives such as QLoRA optimize computational resources by enabling fine-tuning on limited hardware while maintaining performance [4]. Optimizing model size and data allocation maximizes performance without overfitting, underscoring the necessity for scalable architectures [15]. Addressing these challenges is essential for advancing continual learning, particularly for LLMs like LLaMA, to incorporate domain-specific knowledge within realistic computational constraints. This involves developing frameworks that inject domain knowledge during training, leveraging expert-written data, and incorporating retrieval modules to mitigate hallucination while retaining prior knowledge [34,20]. Emphasizing architectural innovations and evaluation strategies will enable scalable and efficient models capable of complex reasoning across diverse domains. Catastrophic Forgetting and Knowledge Retention Catastrophic forgetting is a significant challenge in LLM continual learning, where new data integration often degrades existing knowledge. In financial sentiment analysis, models like FinBERT enhance knowledge retention while mitigating forgetting [26]. The orthogonal low-rank adaptation (O-LoRA) approach advances retention by preserving LLM generalization on unseen tasks [82]. LoRA minimizes trainable parameters, retaining essential knowledge without full model fine-tuning [83]. In cross-modal retrieval tasks, models like CLIP face challenges in maintaining performance during continual training, necessitating improved strategies [41]. The GEM framework targets catastrophic forgetting by retaining past task memory, enabling beneficial knowledge transfer [12]. WIKIREADING serves as a critical evaluation tool for understanding these challenges [7]. Class incremental learning (CIL) illustrates LLM challenges regarding catastrophic forgetting and the need for effective retention strategies [17]. Despite advancements, limitations persist in methodologies like ELLE, which addresses forgetting through model expansion and domain prompts but struggles with generalization across diverse tasks [1]. Existing benchmarks inadequately test complex coreferential phenomena, limiting effectiveness in assessing model performance [18]. Ensuring truthful and helpful model outputs relates to broader challenges of forgetting and retention [3]. Effective knowledge transfer and retention strategies are crucial for improving LLM adaptability, allowing retention and application of prior experiences across diverse applications. The TextVQA benchmark highlights gaps between human and machine performance in reading and reasoning, underscoring ongoing challenges in catastrophic forgetting and knowledge retention [45]. Adaptability and Robustness Adaptability and robustness are vital for LLMs navigating diverse tasks and environments. The Imp model family exemplifies these attributes by adapting to multimodal tasks, enhancing LLM applicability across contexts [84]. Achieving these requires addressing challenges like hyperparameter tuning in domain-adversarial training methods, essential for maintaining cross-domain performance [48]. Flexible benchmarks, such as those proposed by [73], are crucial for evaluating adaptability and robustness, offering insights into generalization across tasks. Lifelong-MoE contributes by integrating new experts, allowing adaptation to dynamic environments [44]. Future research should prioritize optimizing ensemble models to improve task selection and interaction strategies, bolstering adaptability and robustness [49]. Exploring emerging trends in model architectures, particularly in programming environments, may yield insights into enhancing neural code intelligence across domains [14]. Theoretical frameworks of continual learning emphasize effective within-task prediction (WP) and task-id prediction (TP) for adaptability and robustness [17]. Further optimizations in quantization techniques through QLoRA could enhance adaptability in larger models or alternative architectures [4]. Addressing adaptability and robustness challenges requires optimizing model architectures, refining benchmarks, and exploring innovative training methods. Focusing on these areas will develop LLMs that are adaptable, robust, and capable of integrating new knowledge while retaining prior information across applications [85]. Generalization and Domain Adaptation LLM generalization across domains is crucial for utility and effectiveness in diverse applications. PIQA dataset challenges in physical commonsense reasoning reveal AI limitations in domain generalization [86]. OceanBench aims to enhance applicability in ocean science, demonstrating the need for domain-specific knowledge integration [87]. Future research should expand benchmarks to include more languages and tasks, improving generalization in linguistic and contextual environments. Sailor dataset suggests integrating more languages and dialects for enhanced applicability [22]. Extending benchmarks, as proposed by [67], could significantly enhance generalization capabilities. Improving adaptability and robustness in continual learning can be achieved through localization and editing methods, contributing to effective generalization [88]. Future research may explore refining technology for visitor needs and museum education applications, providing insights into model generalization [89]. In programming, expanding benchmarks to include more languages and refining intermediate representations are crucial for domain adaptation [90]. Enhancing model capabilities for domain generalization, particularly in multi-hop reasoning tasks, is a vital area for exploration [91]. Addressing generalization and domain adaptation challenges requires expanding benchmarks, refining evaluation techniques, and exploring innovative knowledge integration methods. Focusing on these areas will develop LLMs capable of domain generalization while integrating new knowledge and preserving prior information [35]. Ethical Considerations and Fairness Ethical considerations and fairness are integral to LLM development and deployment within continual learning frameworks, encompassing responsible data use, bias mitigation, and equitable AI advancement distribution. CodeT5 highlights the need to address biases in code understanding and generation, suggesting exploration of ethical considerations in programming tasks [92]. Applying continual learning methods in law and engineering fields necessitates focusing on ethical implications for fair model performance [93]. Models like Llemma emphasize fairness and accuracy in mathematical problem-solving and theorem proving, underscoring ethical considerations in development [28]. Expanding medical NLP datasets to include diverse scenarios and languages is crucial for fairness and inclusivity in AI applications [94]. Ethical considerations in continual learning highlight model responsibility in addressing climate change, ensuring fairness in synthesizing research perspectives [95]. Model performance across tasks necessitates careful ethical consideration for equitable outcomes [96]. Future research should explore vocabulary adaptation for evolving content beyond social media, raising ethical considerations in content adaptation [62]. Environmental costs associated with continuous training present an ethical dilemma, underscoring sustainable approaches in model adaptation [63]. Expanding datasets and investigating programming tasks in open-source code intelligence raise ethical considerations for inclusive benchmarks [97]. Fostering ethical responsibility and fairness in LLMs involves addressing biases, promoting inclusivity, and ensuring sustainable AI development practices. Integrating domain-specific knowledge and professional skills, as demonstrated in developing Lawyer LLaMA, while ensuring fairness across attributes like gender, age, and region—highlighted by the Fairlex benchmark—better addresses ethical challenges in legal NLP. Aligning AI with shared human values, explored with the ETHICS dataset, is essential for guiding models toward morally informed decisions, enhancing reliability and fairness in applications [98,34,66]. Conclusion Continual learning stands as a pivotal advancement for large language models (LLMs), addressing critical issues like catastrophic forgetting and improving task performance across diverse domains. Techniques such as Gradient Episodic Memory (GEM) play a crucial role in minimizing forgetting while enhancing task performance, thereby contributing significantly to the field. The HMA method further advances alignment performance, offering promising improvements in reinforcement learning with human feedback (RLHF) for LLMs. The insights drawn from scaling laws highlight the significance of model scalability, revealing that larger models demonstrate superior sample efficiency, which implies that training strategies should prioritize larger models even when data is scarce. In visual question answering, advancements like TextVQA enhance models' ability to interpret and reason about text, thus contributing to the continual learning capabilities of LLMs. The WIKIREADING findings underscore the importance of continual learning in augmenting natural language understanding and suggest avenues for future research to expand model capabilities. In specialized fields, models like BioGPT illustrate exceptional performance in biomedical text generation, highlighting the essential role of continual learning in the progression of LLMs. Aligning language models with user intent remains a crucial focus, necessitating ongoing refinement of fine-tuning processes. The ELLE method demonstrates improvements in pre-training efficiency and downstream performance, emphasizing the importance of continual learning for LLMs. Moreover, the BBT-CFLEB framework offers a robust approach for assessing Chinese financial NLP models, underscoring the significance of continual learning in developing specialized benchmarks. These advancements illustrate the transformative potential of addressing existing challenges within the field, indicating that continual learning strategies are vital for evolving LLMs into more robust, adaptable, and efficient models capable of integrating new knowledge while retaining essential prior information.",
  "reference": {
    "1": "2203.06311v2",
    "2": "1606.09282v3",
    "3": "2203.02155v1",
    "4": "2305.14314v1",
    "5": "2302.09432v2",
    "6": "2205.02014v1",
    "7": "1608.03542v2",
    "8": "2303.11165v2",
    "9": "2210.10341v3",
    "10": "2007.15779v6",
    "11": "2401.03129v1",
    "12": "1706.08840v6",
    "13": "2302.00487v3",
    "14": "2403.14734v5",
    "15": "2001.08361v1",
    "16": "2112.09153v2",
    "17": "2211.02633v1",
    "18": "1908.05803v2",
    "19": "2110.03215v4",
    "20": "2012.09823v1",
    "21": "2303.08774v6",
    "22": "2404.03608v1",
    "23": "1810.04805v2",
    "24": "2401.02954v1",
    "25": "2307.09288v2",
    "26": "1908.10063v1",
    "27": "2403.03883v2",
    "28": "2310.10631v3",
    "29": "2404.16621v1",
    "30": "2112.02706v1",
    "31": "2004.00345v2",
    "32": "2211.05110v1",
    "33": "1909.01066v2",
    "34": "2305.15062v2",
    "35": "2310.19704v3",
    "36": "2112.14146v1",
    "37": "2404.07965v4",
    "38": "2403.11435v1",
    "39": "2309.14763v1",
    "40": "2106.09685v2",
    "41": "2305.07437v5",
    "42": "2309.06256v4",
    "43": "1902.09506v3",
    "44": "2305.12281v1",
    "45": "1904.08920v2",
    "46": "2108.05036v2",
    "47": "2305.07922v2",
    "48": "1505.07818v4",
    "49": "2106.03027v3",
    "50": "2010.11929v2",
    "51": "1606.04671v4",
    "52": "2402.05140v3",
    "53": "2106.15110v2",
    "54": "2110.08207v3",
    "55": "2306.12619v2",
    "56": "2405.08015v1",
    "57": "1611.07725v2",
    "58": "2305.18290v3",
    "59": "1810.11910v3",
    "60": "1707.06347v2",
    "61": "2204.05862v1",
    "62": "2106.06297v1",
    "63": "2210.07365v2",
    "64": "2110.08534v3",
    "65": "2402.13013v1",
    "66": "2008.02275v6",
    "67": "1910.10683v4",
    "68": "2403.01554v1",
    "69": "2401.10716v1",
    "70": "2003.09553v2",
    "71": "2403.11901v4",
    "72": "2310.06762v1",
    "73": "2103.00020v1",
    "74": "2209.09513v2",
    "75": "2004.10964v3",
    "76": "2205.05638v2",
    "77": "2305.17529v1",
    "78": "2305.09253v2",
    "79": "1805.07810v1",
    "80": "2401.00434v2",
    "81": "2204.14211v3",
    "82": "2310.14152v1",
    "83": "2106.09685v2",
    "84": "2312.11805v5",
    "85": "2009.03300v3",
    "86": "1911.11641v1",
    "87": "2310.02031v8",
    "88": "2301.04213v2",
    "89": "2403.07082v1",
    "90": "2403.03894v3",
    "91": "1910.11473v2",
    "92": "2109.00859v1",
    "93": "2311.00204v1",
    "94": "2311.09774v2",
    "95": "2401.09646v1",
    "96": "2311.16206v1",
    "97": "2401.14196v2",
    "98": "2203.07228v1"
  },
  "chooseref": {
    "1": "2302.00487v3",
    "2": "1907.11692v1",
    "3": "2401.00434v2",
    "4": "2403.14734v5",
    "5": "2310.19704v3",
    "6": "2211.02633v1",
    "7": "2310.12244v1",
    "8": "2112.02706v1",
    "9": "2003.09553v2",
    "10": "2211.11031v5",
    "11": "2008.02275v6",
    "12": "2112.09153v2",
    "13": "2308.08747v5",
    "14": "2010.11929v2",
    "15": "2302.09432v2",
    "16": "2210.10341v3",
    "17": "2308.09442v2",
    "18": "2403.18365v1",
    "19": "2301.12597v3",
    "20": "2210.03329v2",
    "21": "2303.01081v1",
    "22": "2309.10654v2",
    "23": "2306.12619v2",
    "24": "2308.12950v3",
    "25": "2402.13013v1",
    "26": "2404.05875v1",
    "27": "2203.13474v5",
    "28": "2305.07922v2",
    "29": "2109.00859v1",
    "30": "2102.04664v2",
    "31": "2403.08350v2",
    "32": "2303.11165v2",
    "33": "2309.14763v1",
    "34": "2311.16206v1",
    "35": "2211.12701v2",
    "36": "2311.01200v4",
    "37": "2012.09823v1",
    "38": "2404.17790v1",
    "39": "2205.09357v1",
    "40": "2302.03241v4",
    "41": "2308.04014v2",
    "42": "2210.05549v1",
    "43": "2305.07437v5",
    "44": "2311.00204v1",
    "45": "2207.06543v1",
    "46": "1909.00277v2",
    "47": "2108.05036v2",
    "48": "2004.07211v2",
    "49": "2401.02954v1",
    "50": "2401.14196v2",
    "51": "2305.18290v3",
    "52": "2111.13654v1",
    "53": "2301.04213v2",
    "54": "1505.07818v4",
    "55": "2007.15779v6",
    "56": "2403.10056v4",
    "57": "2004.10964v3",
    "58": "1903.00161v2",
    "59": "2204.04799v2",
    "60": "2106.06297v1",
    "61": "2012.15283v3",
    "62": "2203.06311v2",
    "63": "2312.15696v1",
    "64": "2004.00345v2",
    "65": "2104.08164v2",
    "66": "1812.00420v2",
    "67": "2206.07682v2",
    "68": "1907.12412v2",
    "69": "2302.11344v1",
    "70": "2401.03129v1",
    "71": "1910.10683v4",
    "72": "2203.07228v1",
    "73": "2110.11309v2",
    "74": "1803.05355v3",
    "75": "2205.05638v2",
    "76": "1908.10063v1",
    "77": "2205.12393v4",
    "78": "2109.01652v5",
    "79": "2403.05530v5",
    "80": "1511.02283v3",
    "81": "2403.18383v1",
    "82": "2103.08541v1",
    "83": "2206.15331v2",
    "84": "2303.08774v6",
    "85": "1902.09506v3",
    "86": "1706.08840v6",
    "87": "2404.16621v1",
    "88": "2311.09774v2",
    "89": "2310.02995v3",
    "90": "2002.01808v5",
    "91": "2403.11435v1",
    "92": "2402.12847v2",
    "93": "2311.16208v2",
    "94": "2403.07082v1",
    "95": "2305.05968v1",
    "96": "2403.03894v3",
    "97": "2210.07365v2",
    "98": "2306.05064v2",
    "99": "1806.03822v1",
    "100": "2005.14165v4",
    "101": "1909.01066v2",
    "102": "2211.05110v1",
    "103": "2403.11901v4",
    "104": "2305.15062v2",
    "105": "2209.09513v2",
    "106": "1810.11910v3",
    "107": "2112.08654v2",
    "108": "2103.00020v1",
    "109": "1606.09282v3",
    "110": "2110.07298v3",
    "111": "2305.12281v1",
    "112": "2110.08534v3",
    "113": "2307.09288v2",
    "114": "2310.10631v3",
    "115": "2310.14029v1",
    "116": "2106.09685v2",
    "117": "2202.05262v5",
    "118": "2106.09685v2",
    "119": "2210.07229v2",
    "120": "2009.03300v3",
    "121": "2305.17529v1",
    "122": "1711.09601v4",
    "123": "2206.06520v1",
    "124": "2102.01951v2",
    "125": "2403.01244v2",
    "126": "2309.06256v4",
    "127": "2106.03027v3",
    "128": "2110.08207v3",
    "129": "2204.05660v1",
    "130": "2310.02031v8",
    "131": "2205.02014v1",
    "132": "1312.3005v3",
    "133": "2305.09253v2",
    "134": "1805.07810v1",
    "135": "2302.13971v1",
    "136": "2310.14152v1",
    "137": "1612.00796v2",
    "138": "2305.10403v3",
    "139": "2204.02311v5",
    "140": "2310.04801v1",
    "141": "1911.11641v1",
    "142": "1810.04805v2",
    "143": "1606.04671v4",
    "144": "1707.06347v2",
    "145": "2304.01373v2",
    "146": "1910.11473v2",
    "147": "2305.14314v1",
    "148": "1908.05803v2",
    "149": "1704.04683v5",
    "150": "1908.05852v2",
    "151": "2004.12651v1",
    "152": "2305.08702v1",
    "153": "1907.03799v3",
    "154": "2107.12308v5",
    "155": "2404.07965v4",
    "156": "1911.02782v3",
    "157": "2404.03608v1",
    "158": "2403.03883v2",
    "159": "2001.08361v1",
    "160": "2403.08763v4",
    "161": "2209.09476v1",
    "162": "2402.19173v1",
    "163": "2305.06161v2",
    "164": "2401.10716v1",
    "165": "2402.05140v3",
    "166": "2402.14270v2",
    "167": "2204.14211v3",
    "168": "2001.08435v1",
    "169": "1803.05457v1",
    "170": "2405.08015v1",
    "171": "2310.16226v3",
    "172": "2110.06366v4",
    "173": "2106.15110v2",
    "174": "2202.03829v2",
    "175": "2401.09646v1",
    "176": "2110.03215v4",
    "177": "2112.14146v1",
    "178": "2309.06126v1",
    "179": "1904.08920v2",
    "180": "2310.06762v1",
    "181": "2204.05862v1",
    "182": "2203.15556v1",
    "183": "2203.02155v1",
    "184": "2301.09785v1",
    "185": "2403.01554v1",
    "186": "2305.07972v1",
    "187": "1906.02425v2",
    "188": "1602.01585v1",
    "189": "2304.08485v2",
    "190": "1802.08218v4",
    "191": "2402.01865v3",
    "192": "1608.03542v2",
    "193": "2405.14768v3",
    "194": "2306.08568v2",
    "195": "2308.09583v3",
    "196": "1706.04115v1",
    "197": "2312.11805v5",
    "198": "1611.07725v2"
  }
}