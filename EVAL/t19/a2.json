{
    "survey": "# Continual Learning of Large Language Models: A Comprehensive Survey\n\n## 1 Introduction to Continual Learning in LLMs\n\n### 1.1 Definition and Scope of Continual Learning in LLMs\n\n---\n\n### 1.1 Definition and Scope of Continual Learning in LLMs  \n\nContinual Learning (CL) represents a fundamental shift in machine learning paradigms, enabling models to learn incrementally from sequential data while preserving previously acquired knowledge [1]. This stands in contrast to traditional static learning, where models are trained once on fixed datasets and struggle to adapt to new information without catastrophic forgetting—the abrupt loss of prior knowledge when learning new tasks [2]. The importance of CL is magnified in the context of Large Language Models (LLMs), which increasingly operate in dynamic environments requiring continuous adaptation, such as multilingual applications, domain-specific tasks, and personalized systems [3].  \n\nThe scope of CL in LLMs encompasses three primary stages: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) [1]. CPT updates foundational language models with new corpora to capture evolving linguistic patterns, while DAP specializes models for domains like legal or medical texts [4]. CFT adapts models to downstream tasks (e.g., question-answering) without forgetting prior task-specific knowledge. These stages reflect the dual objectives of CL: plasticity (adapting to new tasks) and stability (preserving old knowledge) [5].  \n\nThe scale and complexity of LLMs introduce unique challenges for CL. First, the massive parameter counts of LLMs (e.g., billions of parameters) amplify computational costs and memory overhead during incremental updates [6]. Traditional CL methods, such as replay-based techniques or regularization, often fail to scale efficiently due to the quadratic complexity of transformer attention mechanisms [7]. For example, Experience Replay (ER) requires storing past data, which becomes impractical for LLMs given their memory demands [8].  \n\nSecond, real-world data streams are heterogeneous, requiring LLMs to handle task-incremental (e.g., new languages), domain-incremental (e.g., shifting from medical to legal texts), and class-incremental (e.g., expanding vocabulary) scenarios [9]. This necessitates flexible CL frameworks capable of distinguishing shared and task-specific features. Graph-based CL methods, for instance, address dependency structures in sequential data—a challenge equally relevant to LLMs processing hierarchical information [10].  \n\nThird, the tension between pre-training and continual learning creates novel trade-offs. While pre-trained LLMs provide strong initial representations, their static nature conflicts with CL's dynamic requirements [11]. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), mitigate this by updating only small task-specific modules while freezing most parameters [12]. However, these methods must balance sparsity (to reduce compute) with expressive power (to maintain performance) [13].  \n\nThe scope of CL in LLMs also extends to ethical and deployment challenges. Bias propagation—where outdated or biased knowledge persists due to inadequate updates—poses risks in high-stakes domains like healthcare and law [14]. Practical deployment demands scalable solutions, such as edge computing or modular architectures, to enable real-time adaptation [15].  \n\nCL in LLMs differs fundamentally from multi-task learning (MTL). While MTL optimizes for multiple tasks jointly, CL learns sequentially without revisiting past data, aligning better with real-world incremental data scenarios [16]. Benchmarks like CITB reveal that current CL methods often struggle to leverage natural language instructions effectively in dialogue systems [17].  \n\nTheoretical foundations for CL in LLMs remain nascent. Recent work establishes that CL algorithms require memory growing linearly with task count, exposing inherent retention-scalability trade-offs [18]. Similarly, optimal CL solutions are proven NP-hard without perfect memory, necessitating approximations like rehearsal or distillation [19].  \n\nIn summary, CL in LLMs spans algorithmic innovation, computational efficiency, and ethical considerations, demanding advances in parameter efficiency, dynamic architectures, and robust evaluation. As LLMs evolve, CL will be pivotal in ensuring their adaptability to non-stationary data while mitigating catastrophic forgetting and enabling lifelong learning [1].  \n\n---\n\n### 1.2 Significance of Continual Learning for LLMs\n\n---\n### 1.2 Significance of Continual Learning for LLMs  \n\nContinual learning (CL) represents a transformative approach for large language models (LLMs), addressing fundamental limitations of static training paradigms while unlocking new capabilities. The significance of CL for LLMs manifests across three critical dimensions: (1) enabling adaptation to evolving knowledge, (2) reducing the prohibitive costs of full model retraining, and (3) facilitating real-world applications that demand dynamic, personalized, and context-aware performance. These advantages position CL as an essential enabler for the next generation of LLMs that must operate in non-stationary environments.  \n\n#### Bridging the Adaptation Gap in Static LLMs  \nThe static nature of conventionally trained LLMs creates a growing disconnect between their fixed knowledge and rapidly evolving real-world information. This gap is particularly acute in domains where accuracy and timeliness are critical, such as healthcare and legal systems. [20] demonstrates how static LLMs risk propagating outdated or incorrect information in high-stakes scenarios like medical diagnosis or legal consultation. CL addresses this by enabling incremental updates to model knowledge, ensuring alignment with current information landscapes.  \n\nThe dynamic adaptation capability becomes even more crucial when considering the changing nature of tools and data sources. [21] illustrates how CL allows LLMs to continuously integrate new APIs, databases, and computational tools without compromising previously learned functionalities—a capability essential for maintaining relevance in production environments.  \n\n#### Overcoming the Cost Barrier of LLM Maintenance  \nThe resource intensity of traditional LLM training creates significant economic and computational barriers to model updates. [22] quantifies the extraordinary costs associated with full model retraining, which often requires thousands of GPU hours. CL provides a pragmatic solution by enabling targeted, incremental updates that dramatically reduce computational overhead while maintaining model performance.  \n\nThe economic implications extend beyond mere cost savings. [23] reveals how resource requirements create systemic inequalities in AI development, favoring well-funded organizations. CL democratizes access to state-of-the-art language technologies, as demonstrated by [24], which shows how CL enables effective model updates using distributed consumer hardware.  \n\n#### Enabling Next-Generation LLM Applications  \nThe true potential of CL emerges in its ability to power innovative applications that demand continuous adaptation:  \n\n- **Dynamic Knowledge Systems**: In education and content creation, CL allows LLMs to stay current with evolving information. [25] documents how continually updated educational assistants can adapt to new curricula and pedagogical research, while [26] demonstrates CL's role in maintaining up-to-date training materials.  \n\n- **Personalized Interaction**: CL enables LLMs to develop longitudinal understanding of individual users. [27] reveals how continual adaptation to user preferences and interaction histories creates more engaging and effective personalized assistants.  \n\n- **High-Stakes Decision Support**: In domains like healthcare and law, CL ensures models remain aligned with current best practices. [28] shows how legal aid chatbots can evolve alongside changing regulations and case law through continual learning.  \n\n#### Addressing Ethical Imperatives  \nBeyond performance and efficiency gains, CL contributes to more ethical AI systems. [29] argues that CL's incremental learning capability allows for continuous bias mitigation and alignment with evolving ethical standards, creating models that adapt not just to new information but also to societal expectations.  \n\nThe transformative potential of CL is further underscored by [30], which positions continual learning as the foundation for self-improving LLMs capable of autonomous adaptation. As the field progresses, CL will be instrumental in transitioning LLMs from static artifacts to dynamic, evolving systems that mirror the continuous learning processes observed in human cognition—a theme further explored in the subsequent discussion of key CL challenges.  \n---\n\n### 1.3 Key Challenges in Continual Learning for LLMs\n\n---\n1.3 Key Challenges in Continual Learning for LLMs  \n\nContinual learning (CL) in large language models (LLMs) introduces fundamental challenges that stem from their scale, complexity, and the dynamic requirements of real-world applications. These challenges—catastrophic forgetting, the plasticity-stability trade-off, computational costs, and data heterogeneity—collectively hinder the seamless adaptation of LLMs to evolving knowledge and tasks. Addressing these obstacles is essential for developing robust CL methods that preserve prior capabilities while integrating new information.  \n\n### Catastrophic Forgetting: The Achilles' Heel of CL  \nCatastrophic forgetting (CF) remains the most pervasive challenge in CL, where models lose previously acquired knowledge when trained on new tasks. This issue is amplified in LLMs due to their reliance on gradient-based optimization, which inherently prioritizes new data over old. [31] categorizes CF mitigation strategies into regularization, replay, and architectural methods, noting that no single approach universally resolves the problem.  \n\nRecent empirical work underscores the scale-dependency of CF. [32] reveals that larger LLMs (e.g., 7B parameters) exhibit more severe forgetting than smaller counterparts (e.g., 1B parameters) during sequential fine-tuning. The study also identifies architectural influences: encoder-decoder models suffer less CF than decoder-only architectures. Further insights from [33] link CF to optimization geometry, showing that wider minima—achieved via techniques like dropout or learning rate decay—enhance stability by preserving task-specific representations.  \n\n### The Plasticity-Stability Dilemma  \nThe tension between learning new tasks (plasticity) and retaining old ones (stability) is particularly acute in LLMs, which are typically optimized for static datasets. [34] proposes a dual-module framework inspired by human cognition: an inference network for new knowledge and a generative network for recalling past knowledge. This separation mitigates interference by decoupling learning and memory mechanisms.  \n\nAlternative approaches focus on architectural or algorithmic solutions. [35] introduces a dual-learner framework where a \"plastic\" learner acquires new tasks while a \"stable\" learner consolidates knowledge through parameter averaging. Meanwhile, [36] demonstrates that dropout implicitly creates task-specific pathways, balancing plasticity and stability without explicit structural changes.  \n\n### Computational and Energy Bottlenecks  \nThe resource demands of CL in LLMs are staggering, especially for retraining or maintaining task-specific models. [37] critiques the unsustainable energy costs of large-scale training, which escalate with model size. CL exacerbates this issue by requiring frequent updates. Replay-based methods, as discussed in [38], further strain memory and compute resources by storing and processing historical data.  \n\nParameter-efficient fine-tuning (PEFT) methods, such as those explored in [39], offer partial relief by freezing most pretrained weights and updating only small adapter modules. However, scalability remains a challenge for long task sequences.  \n\n### Data Heterogeneity and Domain Shifts  \nLLMs in CL must navigate non-stationary data distributions, where task boundaries blur or data evolves incrementally. [40] shows that batch normalization can mitigate domain shifts in medical imaging, but similar techniques struggle with discrete task transitions in NLP. Spurious correlations in biased datasets, as highlighted in [41], further degrade performance across tasks.  \n\nMultimodal CL introduces additional complexity. [42] reveals that aligning cross-modal features (e.g., video and text) amplifies forgetting, underscoring the need for specialized alignment strategies.  \n\n### Emerging Insights and Future Directions  \nRecent research suggests CF and plasticity loss are not merely algorithmic but also rooted in representational dynamics. [43] finds deeper layers in LLMs are more prone to forgetting, implying stabilization efforts should target these layers. Concurrently, [44] identifies loss of plasticity—where models become incapable of learning new tasks—as a parallel concern.  \n\nKey open questions include:  \n1. **Dynamic Capacity Allocation**: Can LLMs adjust their capacity for new tasks without retraining? [45] explores null-space learning, but scalability remains unproven.  \n2. **Self-Supervised CL**: Can unlabeled data reduce reliance on costly annotations?  \n3. **Ethical Intersections**: How do biases propagate in CL, and how can they be mitigated?  \n\nIn summary, overcoming the key challenges in CL for LLMs—catastrophic forgetting, plasticity-stability trade-offs, computational costs, and data heterogeneity—requires innovations in architecture, training efficiency, and evaluation. These advances will bridge the gap between static LLMs and adaptive, real-world systems, as explored in the subsequent discussion of vertical and horizontal CL paradigms.  \n---\n\n### 1.4 Vertical vs. Horizontal Continual Learning\n\n---\n### 1.4 Vertical vs. Horizontal Continual Learning  \n\nContinual learning (CL) in large language models (LLMs) manifests in two primary paradigms—vertical continual learning (VCL) and horizontal continual learning (HCL)—each addressing distinct adaptation needs while confronting shared challenges like catastrophic forgetting and computational bottlenecks (Section 1.3). These paradigms diverge in their objectives: VCL transitions LLMs from general to domain-specific expertise, whereas HCL enables cross-domain or temporal adaptation. Their methodologies and challenges, explored below, inform the design of CL benchmarks and evaluation protocols (Section 1.5).  \n\n#### **Vertical Continual Learning: Specialization Through Incremental Refinement**  \nVCL focuses on imbuing general-purpose LLMs with domain-specific capabilities, critical for applications like healthcare or legal analysis. For instance, while GPT-4 excels at broad tasks, it requires vertical adaptation to master nuanced domains such as medical diagnosis or legal judgment prediction [46]. Techniques like domain-adaptive pre-training (DAP) and parameter-efficient fine-tuning (PEFT) dominate this paradigm. [47] demonstrates how augmenting LLMs with domain-specific modules preserves foundational knowledge, while [48] shows unified data formatting streamlines medical adaptation.  \n\n**Challenges in VCL**:  \n1. **Catastrophic Forgetting**: Domain-specific fine-tuning often degrades general capabilities, as evidenced by [49], where models lose dual-logic reasoning post-adaptation.  \n2. **Data Scarcity**: High-quality domain datasets are sparse, requiring expert annotation [50].  \n3. **Ethical Risks**: Sensitive domains amplify biases or hallucinations, as noted in [51].  \n\n#### **Horizontal Continual Learning: Versatility Across Domains and Time**  \nHCL equips LLMs to handle cross-domain tasks (e.g., multilingual translation) or temporal shifts (e.g., real-time updates), ensuring versatility. For example, [52] uses cross-lingual prompts to mitigate language performance disparities. Replay-based learning and modular architectures are common strategies, with [53] illustrating cross-domain knowledge transfer via collaborative training.  \n\n**Challenges in HCL**:  \n1. **Domain Shift**: Non-stationary data distributions complicate adaptation [54].  \n2. **Scalability**: Maintaining multi-domain performance demands efficient architectures [55].  \n3. **Task Interference**: Simultaneous adaptation risks confusion, addressed by role-specific prompts in [56].  \n\n#### **Synergies and Comparative Insights**  \nVCL and HCL are complementary:  \n- **Depth vs. Breadth**: VCL prioritizes domain expertise; HCL emphasizes cross-task generalization.  \n- **Data Needs**: VCL relies on domain-specific data; HCL requires diverse, multi-domain corpora.  \n- **Evaluation**: VCL benchmarks (e.g., [51]) assess specialization, while HCL benchmarks (e.g., [57]) measure generalization.  \n\nHybrid approaches, such as [58], combine vertical specialization with horizontal reasoning. Similarly, [59] integrates medical expertise (VCL) with multi-agent collaboration (HCL).  \n\n#### **Open Challenges and Future Directions**  \n1. **Scalability**: Efficient scaling for both paradigms remains unresolved [55].  \n2. **Theoretical Unification**: A cohesive CL theory for LLMs is lacking, though [60] hints at compositional solutions.  \n3. **Ethical Alignment**: Cross-paradigm ethical frameworks are needed [61].  \n\nFuture work should explore hybrid paradigms and advance benchmarks like [62] to evaluate cross-domain robustness. By bridging VCL’s depth and HCL’s adaptability, LLMs can achieve both specialized and versatile intelligence, paving the way for the benchmarks and ethical considerations discussed in Sections 1.5–1.6.  \n---\n\n### 1.5 Benchmarks and Evaluation Protocols\n\n### 1.5 Benchmarks and Evaluation Protocols  \n\nThe development of robust benchmarks and evaluation protocols is critical for advancing continual learning (CL) in large language models (LLMs), particularly in light of the vertical and horizontal CL paradigms discussed in Section 1.4. These benchmarks serve as standardized testbeds to measure model performance across incremental tasks, while evaluation metrics provide insights into key challenges such as catastrophic forgetting, forward/backward transfer, and computational efficiency—issues that directly impact the ethical and practical considerations explored in Section 1.6. This subsection surveys existing CL benchmarks tailored for LLMs, analyzes their strengths and limitations, and identifies gaps that future work must address.  \n\n#### **Existing Benchmarks for Continual Learning in LLMs**  \n\nCurrent benchmarks for CL in LLMs can be categorized based on their alignment with vertical (domain-specific) or horizontal (cross-domain) adaptation. For vertical CL, benchmarks like [63] evaluate domain-specific task retention across eight diverse datasets, including mathematical reasoning and code generation. TRACE reveals that fine-tuned LLMs often suffer significant declines in both task-specific performance and general instruction-following capabilities, illustrating the tension between specialization and generalizability inherent in vertical CL. For instance, models like llama2-chat 13B exhibit precipitous drops in mathematical reasoning accuracy after domain adaptation, underscoring the challenge of balancing new task acquisition with foundational knowledge retention.  \n\nHorizontal CL benchmarks, such as [64], focus on cross-domain adaptability in dynamic environments. LiveCodeBench leverages real-world programming scenarios (e.g., self-repair, test output prediction) sourced from platforms like LeetCode, ensuring evaluation free from data contamination. This benchmark highlights LLMs' struggles with evolving coding challenges, where horizontal adaptation must preserve prior knowledge while integrating new syntactic or logical patterns. Similarly, [65] assesses LLMs across the software development lifecycle, exposing limitations in managing hierarchical task dependencies—a critical requirement for horizontal CL in complex workflows.  \n\n#### **Evaluation Metrics for Continual Learning**  \n\nEffective CL evaluation requires metrics that capture both task performance and broader model dynamics:  \n1. **Task Retention**: Quantifies catastrophic forgetting by measuring accuracy drops on prior tasks after new learning. This metric is especially relevant for vertical CL, where domain specialization risks eroding general capabilities.  \n2. **Forward/Backward Transfer**: Forward transfer assesses how learning one task improves future task performance (e.g., multilingual adaptation in horizontal CL), while backward transfer evaluates the impact of new tasks on prior knowledge. These metrics are critical for understanding plasticity-stability trade-offs.  \n3. **Computational Efficiency**: Given the resource intensity of LLMs, metrics like training time and memory overhead—analyzed in [66]—are vital for scalable CL deployment.  \n\n#### **Gaps and Future Directions**  \n\nDespite progress, current benchmarks exhibit three key limitations:  \n1. **Narrow Scope**: Benchmarks like TRACE focus on isolated domains, neglecting cross-disciplinary tasks essential for horizontal CL evaluation.  \n2. **Static Design**: Most benchmarks rely on fixed datasets, whereas dynamic frameworks like [67] could better simulate real-world incremental learning.  \n3. **Ethical Blind Spots**: Few benchmarks integrate bias or fairness metrics, despite the risks of bias propagation highlighted in [68].  \n\nTo address these gaps, future benchmarks should:  \n- **Integrate Multimodal and Multilingual Tasks**: Expanding beyond unimodal benchmarks (e.g., [69]) to include multimodal CL scenarios from [70].  \n- **Emulate Real-World Conditions**: Incorporate metrics like those in [71] to assess CL under data heterogeneity and distribution shifts.  \n- **Prioritize Human-Centric Evaluation**: Adapt frameworks like [72] to measure how CL adapts to evolving user needs.  \n\nIn summary, while existing benchmarks provide foundational insights into CL for LLMs, their focus on isolated domains and static tasks limits their applicability to real-world scenarios. Future efforts must develop dynamic, ethically aware benchmarks that align with the dual demands of vertical specialization and horizontal versatility, bridging the gap to the ethical and practical challenges discussed in Section 1.6.\n\n### 1.6 Ethical and Practical Considerations\n\n---\n### 1.6 Ethical and Practical Considerations  \n\nBuilding on the benchmark-driven evaluation challenges discussed in Section 1.5, the deployment of continual learning (CL) in large language models (LLMs) introduces a complex interplay of ethical concerns and practical barriers. While CL enables LLMs to adapt dynamically to evolving knowledge domains—aligning with the vertical and horizontal paradigms outlined earlier—it also amplifies risks related to bias propagation, fairness erosion, privacy violations, and scalability challenges. This subsection systematically explores these issues, proposing mitigation strategies grounded in recent research while foreshadowing the methodological solutions that will be detailed in Sections 3–5.  \n\n#### **Ethical Concerns in Continual Learning for LLMs**  \n\n1. **Bias Propagation and Amplification**:  \n   A primary ethical concern is the potential for CL to perpetuate or exacerbate societal biases present in incremental training data, a challenge exacerbated by the narrow scope of current benchmarks noted in Section 1.5. LLMs trained via CL may inherit and amplify historical biases, as seen in studies where models exhibited skewed representations of protected groups (e.g., gender, race) in generated content [73]. For instance, biases in occupational stereotypes or cultural generalizations can become entrenched over successive learning cycles [74]. Mitigation strategies include:  \n   - **Bias Auditing**: Regular evaluation using benchmarks like [68] to quantify bias across demographic axes.  \n   - **Debiasing Techniques**: Methods such as [75] leverage ensemble-based aggregation to minimize harmful biases in outputs.  \n\n2. **Fairness Erosion**:  \n   CL can inadvertently degrade fairness by overfitting to new task-specific data, leading to disproportionate performance drops for underrepresented groups—a risk magnified by the static design of existing benchmarks. For example, clinical decision-support systems trained incrementally may exhibit racial disparities in diagnostic accuracy [76]. To address this, fairness-aware CL frameworks integrate:  \n   - **Dynamic Re-weighting**: Adjusting loss functions to prioritize equitable performance across groups [77].  \n   - **Intersectional Evaluation**: Assessing model performance across overlapping demographic identities [78].  \n\n3. **Privacy and Data Sovereignty**:  \n   CL often relies on streaming user-generated data, raising privacy risks that parallel the ethical blind spots identified in benchmark evaluations. For instance, LLMs fine-tuned on legal or medical interactions risk memorizing sensitive information [79]. Mitigation approaches include:  \n   - **Federated Learning**: Decentralized training frameworks like [24] preserve data locality.  \n   - **Differential Privacy**: Injecting noise during gradient updates to prevent data leakage [80].  \n\n4. **Transparency and Accountability**:  \n   The black-box nature of LLMs complicates accountability in CL settings, where model behavior evolves unpredictably—a challenge that will be revisited in Section 5’s evaluation methodologies. For example, [81] proposes governance audits to track ethical drift across learning phases. Practical solutions include:  \n   - **Explainable CL**: Techniques like concept-specific subnetworks [82] illuminate decision pathways.  \n   - **Human-in-the-Loop Oversight**: Integrating real-time human feedback to correct unethical outputs [83].  \n\n#### **Practical Barriers in CL Deployment**  \n\n1. **Scalability and Computational Costs**:  \n   CL demands significant resources for retraining and storing historical data, a limitation highlighted by the computational efficiency metrics in Section 1.5. For instance, [84] highlights the infeasibility of deploying CL-enabled LLMs on edge devices due to memory constraints. Strategies to address this include:  \n   - **Parameter-Efficient Fine-Tuning (PEFT)**: Methods like LoRA [85] reduce trainable parameters—a precursor to the architectural solutions explored in Section 3.  \n   - **Quantization and Distillation**: Compressing models without catastrophic forgetting [86].  \n\n2. **Data Heterogeneity and Distribution Shifts**:  \n   Non-stationary data streams in CL can lead to performance instability, echoing the gaps identified in dynamic benchmark design. For example, multilingual adaptation tasks face domain shifts when new languages are introduced incrementally [87]. Mitigation involves:  \n   - **Replay Buffers**: Storing exemplars from prior distributions to stabilize learning [88].  \n   - **Domain-Invariant Pretraining**: Techniques like [89] enhance robustness to shifts.  \n\n3. **Regulatory and Compliance Risks**:  \n   CL systems must navigate evolving AI regulations, such as the EU AI Act—an issue that transitions into the governance frameworks discussed in Section 1.7. [29] proposes tiered compliance frameworks to align CL with regional laws.  \n\n4. **Environmental Impact**:  \n   The carbon footprint of continual retraining is a growing concern, further motivating the efficiency-focused methodologies surveyed later. [90] advocates for energy-efficient CL via prompt-based adaptation.  \n\n#### **Mitigation Strategies and Future Directions**  \n\nTo balance ethical and practical demands—while addressing the limitations of current benchmarks—future CL systems should adopt:  \n- **Multi-Stakeholder Governance**: Collaborative oversight involving ethicists, domain experts, and policymakers [91].  \n- **Bias-Aware CL Protocols**: Integrating bias detection into the learning loop [92].  \n- **Scalable Infrastructure**: Leveraging distributed training paradigms [93].  \n\nIn conclusion, while CL offers transformative potential for LLMs, its ethical and practical challenges necessitate proactive, interdisciplinary solutions. By embedding fairness, transparency, and scalability into CL frameworks—building on the benchmarks of Section 1.5 and paving the way for the methodologies in Section 1.7—researchers can harness its benefits while minimizing societal harms.  \n---\n\n### 1.7 Scope and Structure of the Survey\n\n---\n1.7 Scope and Structure of the Survey  \n\nBuilding upon the ethical and practical considerations outlined in Section 1.6, this survey systematically examines continual learning (CL) in large language models (LLMs) through three interconnected lenses: methodologies, applications, and future directions. The structure reflects the field's evolution, where CL has emerged as a critical capability for LLMs operating in dynamic real-world environments that demand incremental adaptation without catastrophic forgetting. By organizing the survey thematically, we bridge theoretical advancements with practical implementations while highlighting open challenges and research opportunities.  \n\n**Survey Organization**  \nThe survey is structured to guide readers from foundational concepts to cutting-edge innovations:  \n1. **Introduction (Section 1)**: Establishes key CL paradigms, benchmarks, and ethical considerations, setting the stage for subsequent technical discussions.  \n2. **Theoretical Foundations (Section 2)**: Analyzes core challenges like catastrophic forgetting and plasticity-stability trade-offs, drawing on insights from [94] and [95]. Architectural solutions such as dynamic parameter-efficient modules (e.g., LoRA) are contextualized within pre-training paradigms [96].  \n3. **Methodologies (Section 3)**: Systematically reviews CL techniques, including:  \n   - Parameter-efficient fine-tuning (PEFT) variants like LoRA [97]  \n   - Replay-based methods and hybrid frameworks (MultiLoRA, Hydra) [98]  \n   - Granular update strategies via token-level/layer-wise adaptation [95]  \n4. **Applications (Section 4)**: Demonstrates CL's impact through domain-specific case studies:  \n   - Cross-lingual legal adaptation [99]  \n   - Medical diagnosis systems (MedAgents, ClinicalGPT) [100]  \n   - Personalized multimodal recommendation [101]  \n5. **Evaluation (Section 5)**: Critically assesses benchmarks (LongICLBench, EvolvingQA) and adaptive testing methodologies [102].  \n\n**Comparative and Forward-Looking Analysis**  \nSections 7–8 provide:  \n- A taxonomy of CL methods (replay/regularization/architectural) with empirical comparisons [98]  \n- Emerging paradigms like self-supervised CL and federated learning hybrids [103]  \n- Future directions in lifelong learning and zero-shot adaptation [104]  \n\n**Conclusion (Section 9)**  \nSynthesizes key findings and provides actionable recommendations:  \n- Hybrid approach adoption (e.g., LoRA + distillation)  \n- Benchmark-driven evaluation using LongICLBench [102]  \n- Interdisciplinary frameworks for scalable, ethical deployment [105]  \n\nThis structure ensures a cohesive narrative that progresses from fundamentals to frontiers, equipping researchers with both the technical grounding and strategic outlook needed to advance CL in LLMs. The survey's organization directly addresses the ethical and scalability challenges raised in Section 1.6 while paving the way for the methodological deep dives that follow.  \n---\n\n## 2 Theoretical Foundations of Continual Learning\n\n### 2.1 Catastrophic Forgetting and Plasticity-Stability Trade-off\n\n---\nCatastrophic forgetting (CF) is a fundamental challenge in continual learning (CL), where models lose previously acquired knowledge when trained on new tasks or data distributions. In the context of large language models (LLMs), this phenomenon is particularly pronounced due to their scale and complexity, as they are typically trained on vast amounts of static data. When LLMs are fine-tuned or adapted incrementally to new tasks, the weights optimized for new inputs often overwrite those critical for retaining prior knowledge, leading to a dramatic drop in performance on earlier tasks [1]. This section explores the causes of CF in LLMs, the theoretical underpinnings of the plasticity-stability trade-off, and mitigation strategies that bridge to memory replay techniques discussed in the next subsection.\n\n### Causes of Catastrophic Forgetting in LLMs  \nThe primary cause of CF in LLMs stems from parameter interference during incremental learning. Unlike humans, who compartmentalize knowledge across domains, LLMs rely on shared parameters for all tasks. When new data is introduced, gradient updates disproportionately favor the new task, causing the model to \"forget\" previously learned patterns [15]. This interference is exacerbated by LLMs' dense parameterization and lack of task-specific architectural components. For instance, fine-tuning an LLM on a new domain (e.g., legal or medical text) often overwrites generic linguistic features essential for broader tasks [4].\n\nAnother critical factor is the non-stationary nature of CL data streams. Unlike traditional batch learning with i.i.d. data, CL tasks often exhibit significant distributional shifts. These shifts disrupt stable representations, especially for semantically dissimilar tasks [106]. For example, an LLM trained sequentially on legal and medical texts may struggle to retain legal terminology after adapting to medical jargon due to limited feature overlap.\n\n### The Plasticity-Stability Trade-off  \nThe plasticity-stability trade-off is central to CL, balancing adaptation to new tasks (plasticity) with retention of old knowledge (stability). In LLMs, this trade-off is challenging due to their monolithic architecture. Excessive plasticity leads to CF, while excessive stability (e.g., freezing parameters) hinders adaptation [16].  \n\nGradient alignment plays a key role: conflicting gradients between old and new tasks drive forgetting. Techniques like elastic weight consolidation (EWC) mitigate this by penalizing changes to important parameters [107]. However, LLMs' high-dimensional parameter spaces complicate identifying critical weights without overly restricting plasticity.\n\n### Mitigating Catastrophic Forgetting in LLMs  \nStrategies to address CF in LLMs often draw from neuroscience and cognitive science. *Memory replay*, discussed in detail in the next subsection, stores and interleaves past data with new task training to reinforce old knowledge [8]. For LLMs, replay can use episodic buffers or generative models, though computational overhead and privacy concerns arise in sensitive domains [3].  \n\n*Parameter isolation* allocates task-specific sub-networks (e.g., via sparse networks or Mixture-of-Experts) to minimize interference [108]. This aligns with the brain's compartmentalization but requires careful design to avoid parameter explosion.  \n\n*Regularization-based methods*, such as knowledge distillation, encourage stability by penalizing deviations from old model predictions [109]. For LLMs, distillation can apply to outputs or intermediate representations, though effectiveness depends on task similarity.\n\n### Theoretical Insights and Open Challenges  \nTheoretical frameworks for CF in LLMs leverage information theory and optimization. The *information bottleneck* principle suggests forgetting occurs when task-specific features are over-compressed, losing discriminative power [110]. This has inspired methods to constrain representation drift.  \n\nKey open challenges include:  \n1. **Scalability**: CL methods are rarely tested on billion-parameter LLMs, leaving efficacy for models like GPT-4 uncertain [1].  \n2. **Pre-training vs. CL**: The interplay between static pre-training and dynamic CL remains underexplored [11].  \n3. **Self-supervised learning**: Objectives like masked language modeling blur old/new task boundaries, complicating forgetting quantification [111].  \n\nIn summary, CF in LLMs arises from parameter interference and distributional shifts, while the plasticity-stability trade-off demands nuanced balancing. Mitigation strategies—replay, isolation, and regularization—offer partial solutions, but theoretical and practical gaps persist. Addressing these will require interdisciplinary collaboration [4], paving the way for the memory replay techniques discussed next.  \n---\n\n### 2.2 Memory Replay and Experience Replay\n\n### 2.2 Memory Replay and Experience Replay  \n\nBuilding on the discussion of catastrophic forgetting and the plasticity-stability trade-off in Section 2.1, memory replay and experience replay (ER) emerge as foundational techniques for continual learning (CL) in large language models (LLMs). These methods address the core challenge of retaining prior knowledge while adapting to new tasks by strategically revisiting past experiences during incremental training. This subsection examines ER and its variants, highlighting their role in stabilizing LLM performance and bridging to regularization techniques discussed in Section 2.3.  \n\n#### **Foundations of Experience Replay**  \nExperience replay, originally developed for reinforcement learning, has been adapted to mitigate catastrophic forgetting in LLMs by interleaving training on new data with rehearsals of stored samples from previous tasks. The approach relies on two key design choices: (1) the *sampling strategy* for memory buffer updates, which determines how representative the stored data is of past tasks, and (2) the *replay ratio*, which balances the influence of old and new data during training.  \n\nIn LLMs, ER faces unique challenges due to the scale and diversity of textual data. For instance, [21] demonstrates that naive ER struggles with dynamic tool-use scenarios, where new tools emerge and existing ones evolve. The study underscores the need for adaptive memory management to handle such nonstationary environments. Similarly, [112] highlights the risks of biased or outdated information in replay buffers, emphasizing the importance of curated sampling to maintain model integrity.  \n\n#### **Variants of Experience Replay**  \nRecent advancements have introduced specialized ER variants to address LLM-specific challenges:  \n\n1. **Gradient-Based Replay**: This approach stores gradients or embeddings instead of raw data to reduce memory overhead. [113] proposes a \"Working Memory Hub\" that compresses prior task representations, maintaining performance while minimizing storage costs. This aligns with findings in [114], where gradient-based replay enhanced personalization without catastrophic forgetting.  \n\n2. **Dynamic Memory Replay**: Here, the memory buffer adapts to the model’s current knowledge gaps. [115] advocates for dynamic buffers updated via human-in-the-loop validation, particularly in high-stakes domains like healthcare. This contrasts with static buffers, which may overfit to redundant or unrepresentative samples.  \n\n3. **Task-Aware Replay**: This variant leverages task-specific metadata to guide replay selection. [116] integrates replay with a Mixture-of-Experts (MoE) architecture, ensuring rehearsed data aligns with the current learning objective. Task-aware replay mitigates interference between dissimilar tasks, a critical issue in multilingual or cross-domain CL.  \n\n#### **Challenges and Trade-offs**  \nDespite its effectiveness, ER implementation in LLMs presents several challenges:  \n\n- **Memory Overhead**: Storing raw data or gradients for large-scale tasks is resource-intensive. [97] explores quantization and distributed storage to mitigate this issue while preserving retention performance.  \n\n- **Sample Selection Bias**: ER’s success depends on the representativeness of stored samples. [117] proposes confidence-based curation to filter high-quality replay data and avoid reinforcing biases.  \n\n- **Temporal Decay**: Older memories may lose relevance as data distributions shift. [30] introduces \"self-refinement\" techniques where LLMs regenerate and prune outdated samples, mimicking human memory consolidation.  \n\n#### **Empirical Insights**  \nEmpirical studies demonstrate ER’s versatility across LLM applications:  \n\n- In healthcare, [118] shows that patient-specific replay buffers improve diagnostic accuracy over time.  \n- For legal tasks, [28] finds ER preserves nuanced legal reasoning patterns, reducing hallucinations in advice generation.  \n- [119] leverages ER to iteratively refine instruction datasets, achieving higher-quality outputs with fewer human interventions.  \n\n#### **Future Directions**  \nOpen questions and opportunities for ER in LLMs include:  \n1. **Scalability**: Designing lightweight ER for edge-deployed LLMs [84].  \n2. **Generalization**: Extending ER to unseen tasks without explicit task boundaries [120].  \n3. **Ethical Safeguards**: Preventing replay buffers from perpetuating harmful biases [29].  \n\nIn summary, memory and experience replay are indispensable for CL in LLMs, offering a practical solution to catastrophic forgetting. Their effectiveness hinges on adaptive sampling, efficient storage, and seamless integration with broader CL frameworks, including the regularization techniques explored next.\n\n### 2.3 Regularization Techniques\n\n### 2.3 Regularization Techniques  \n\nRegularization techniques serve as a critical bridge between memory-based approaches (Section 2.2) and architectural adaptations (Section 2.4) in continual learning (CL) for large language models (LLMs). By introducing constraints or penalties during training, these methods stabilize the learning process, mitigating catastrophic forgetting while preserving adaptability to new tasks. The core challenge—balancing plasticity (new task learning) and stability (prior knowledge retention)—has spurred diverse regularization strategies, each offering unique theoretical and empirical advantages.  \n\n#### **Consistency Regularization**  \nConsistency regularization enforces output invariance for perturbed inputs, promoting robustness and reducing overfitting. In CL, this technique stabilizes performance across tasks by penalizing deviations from learned representations. For example, [121] combines consistency regularization with knowledge distillation, dynamically adjusting the stability-plasticity trade-off using prediction uncertainty. This hybrid approach prevents overfitting to noisy data while maintaining task coherence. Similarly, [36] reveals that dropout—a stochastic regularization method—implicitly segregates task-specific pathways, reducing interference between tasks. The study demonstrates that dropout-induced sparsity preserves critical weights, enhancing stability without sacrificing plasticity.  \n\n#### **Weight Constraints**  \nExplicit weight constraints limit parameter updates to protect knowledge from prior tasks. Elastic Weight Consolidation (EWC) is a foundational method that quantifies weight importance via Fisher information, penalizing updates to critical parameters [40]. Extensions like [122] integrate batch normalization statistics to modulate updates, improving domain adaptation in healthcare. Another variant, synaptic intelligence, incrementally computes weight importance during training, offering efficiency gains over EWC [123]. These methods are particularly vital in high-stakes domains where forgetting carries significant consequences.  \n\n#### **Hybrid and Advanced Regularization**  \nRecent work combines regularization strategies to address individual limitations. [124] freezes feature extractors for old tasks while fine-tuning a plasticity layer for new tasks, achieving stability without exemplars. Similarly, [35] employs a dual-learner design: a stable learner consolidates knowledge via parameter averaging, while a plastic learner adapts to new tasks. This decoupling, enforced through regularization, optimizes both objectives simultaneously. Such hybrid approaches align with architectural adaptations (Section 2.4), where modular designs like Mixture-of-Experts (MoE) benefit from regularized task routing.  \n\n#### **Theoretical Foundations**  \nTheoretical analyses elucidate why regularization succeeds in CL. [33] links wider loss minima—induced by dropout or learning rate decay—to reduced forgetting, suggesting that flatter optimization landscapes enhance stability. This aligns with [125], which emphasizes smooth parameter trajectories for feature preservation. Further, [126] introduces an entropy-based criterion to dynamically adjust regularization strength, optimizing the stability-plasticity balance.  \n\n#### **Challenges and Future Directions**  \nDespite their strengths, regularization techniques face hurdles:  \n- **Data Heterogeneity**: Biases and spurious correlations can propagate across tasks, undermining regularization efficacy [41].  \n- **Computational Overhead**: Methods like EWC require storing weight importance metrics, increasing memory costs. Lightweight alternatives, such as [127], propose gradient propagation and loss landscape smoothing to reduce overhead.  \n\nFuture research could explore:  \n1. **Meta-Learning Integration**: Dynamic regularization strength optimization via meta-gradients, as in [128].  \n2. **Cross-Method Synergy**: Combining regularization with architectural adaptations (e.g., MoE or LoRA) to enhance scalability and task specificity.  \n\nIn summary, regularization techniques are indispensable for CL in LLMs, offering flexible solutions to the stability-plasticity dilemma. Their integration with memory replay (Section 2.2) and architectural innovations (Section 2.4) paves the way for robust, lifelong learning systems. Future advances must address data heterogeneity and efficiency to unlock their full potential in real-world applications.\n\n### 2.4 Architectural Adaptations\n\n### 2.4 Architectural Adaptations  \n\nArchitectural adaptations represent a fundamental strategy for continual learning (CL) in large language models (LLMs), building upon the regularization techniques discussed in Section 2.3 while laying the groundwork for the pre-training and fine-tuning paradigms explored in Section 2.5. These adaptations dynamically modify model structures to balance plasticity (adaptation to new tasks) and stability (retention of prior knowledge), addressing catastrophic forgetting through modular and parameter-efficient designs. This subsection examines key innovations in architectural adaptations, including Mixture-of-Experts (MoE) frameworks, parameter-efficient fine-tuning (PEFT) methods, and hybrid approaches that integrate dynamic routing with domain-specific specialization.  \n\n#### **Mixture-of-Experts (MoE) for Continual Learning**  \nThe Mixture-of-Experts (MoE) architecture has emerged as a scalable solution for CL, enabling LLMs to partition computational resources across specialized sub-networks (experts) that activate selectively based on input tasks. By allocating new tasks to underutilized or newly instantiated experts, MoE frameworks minimize interference with existing knowledge while supporting incremental learning. For instance, [129] demonstrates how MoE-based LLMs achieve domain-specific adaptation through dynamic routing, preserving general capabilities without catastrophic forgetting.  \n\nA key strength of MoE architectures lies in their scalability across both horizontal (multi-domain) and vertical (hierarchical task) learning scenarios. [130] highlights how MoE frameworks integrate domain-specific modules into a shared backbone, facilitating seamless knowledge transfer. However, challenges persist in expert selection and load balancing, as uneven task distributions may lead to underutilization or overfitting. Recent advancements, such as [47], propose hybrid MoE systems where lightweight domain-specific models complement general-purpose experts, further enhancing CL efficiency.  \n\n#### **Parameter-Efficient Fine-Tuning (PEFT) Methods**  \nParameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), offer a scalable approach to CL by freezing the majority of pre-trained LLM parameters and introducing low-rank updates. LoRA decomposes weight updates into smaller matrices, reducing memory overhead while enabling task-specific adaptations. [1] underscores LoRA’s effectiveness in vertical CL, where LLMs incrementally specialize from general to domain-specific tasks without full model retraining.  \n\nVariants like rsLoRA (randomized sparse LoRA) and DoRA (Domain-Adaptive LoRA) further optimize CL by dynamically adjusting rank or sparsity based on task complexity. For example, [48] demonstrates how DoRA enables medical LLMs to retain general language understanding while fine-tuning on niche subdomains. Similarly, [56] integrates LoRA with role-specific prompts to minimize inter-domain confusion, achieving robust performance in multi-domain CL scenarios.  \n\n#### **Dynamic Architecture Adaptation via Modular Design**  \nBeyond MoE and LoRA, dynamic architecture adaptation encompasses modular designs that expand or reconfigure LLM components based on task demands. [59] introduces a multi-agent framework where specialized sub-models collaborate via hierarchical routing, enabling incremental integration of medical knowledge. This aligns with the \"vertical continuity\" paradigm described in [1], where LLMs progressively refine capabilities from coarse-grained to fine-grained tasks.  \n\nToken-level and layer-wise adaptation further exemplify this approach. [131] shows how visual-linguistic LLMs dynamically adjust cross-modal projections at specific layers to accommodate new domains, avoiding global retraining. Similarly, [132] highlights layer-wise gating mechanisms that isolate financial domain updates while preserving general-language layers.  \n\n#### **Hybrid and Multi-Task Adaptation Frameworks**  \nHybrid architectures combine architectural adaptations with other CL strategies, such as replay or distillation, to enhance stability and plasticity. [53] proposes CrossLM, where a small domain-specific model and a general LLM mutually refine their architectures via joint Bayesian optimization, ensuring bidirectional adaptation.  \n\nMulti-task frameworks like MultiLoRA and Hydra [129] parallelize task-specific LoRA adapters for scalable CL. Hydra, for instance, employs a shared backbone with task-specific heads, enabling simultaneous adaptation to multiple domains without catastrophic forgetting. [133] extends this idea to federated settings, where distributed clients collaboratively train modular architectures under dynamic network conditions.  \n\n#### **Challenges and Future Directions**  \nDespite their promise, architectural adaptations face several challenges:  \n1. **Scalability**: MoE and modular designs require careful resource allocation to avoid excessive parameter growth. [130] notes that unbalanced expert utilization may degrade performance in long-term CL.  \n2. **Integration Complexity**: Combining multiple adaptations demands sophisticated routing mechanisms, as discussed in [47].  \n3. **Evaluation Gaps**: Current benchmarks like [51] lack metrics for assessing architectural robustness in multi-domain CL.  \n\nFuture research could explore:  \n- **Self-Evolving Architectures**: Inspired by [54], LLMs might autonomously reconfigure modules based on task novelty.  \n- **Cross-Modal Adaptation**: Extending [134] to continual learning by dynamically aligning visual, textual, and auditory modules.  \n\nIn summary, architectural adaptations are indispensable for CL in LLMs, offering flexible, efficient, and scalable solutions to balance plasticity and stability. By leveraging MoE, PEFT, and hybrid designs, LLMs can achieve lifelong learning while preserving their core capabilities, bridging the gap between regularization techniques (Section 2.3) and pre-training paradigms (Section 2.5).\n\n### 2.5 Pre-training and Fine-tuning Paradigms\n\n---\n### 2.5 Pre-training and Fine-tuning Paradigms in Continual Learning  \n\nThe integration of continual learning (CL) with pre-training and fine-tuning paradigms presents both opportunities and challenges for large language models (LLMs). While pre-training equips LLMs with broad linguistic and world knowledge, and fine-tuning tailors them to specific tasks, combining these approaches with CL requires careful consideration of how to incrementally adapt models to new knowledge while preserving previously learned capabilities. This subsection examines key strategies and challenges in aligning pre-training and fine-tuning with CL objectives, focusing on domain adaptation, task specialization, and scalability.  \n\n#### Domain-Adaptive Pre-training for Incremental Learning  \nDomain-adaptive pre-training extends the foundational knowledge of LLMs by exposing them to domain-specific corpora, enabling gradual specialization without catastrophic forgetting. For example, [135] demonstrates how additional pre-training on chemical data enhances performance in specialized fields, while [136] underscores the necessity of domain adaptation for niche applications like legal text processing.  \n\nHowever, traditional domain-adaptive pre-training faces scalability issues in CL due to high computational costs and the risk of overwriting general knowledge. Recent innovations address these challenges through parameter-efficient methods such as Low-Rank Adaptation (LoRA) and its variants (e.g., rsLoRA, SoRA), which selectively update a small subset of parameters during adaptation [137]. These techniques enable incremental domain integration while preserving the model's core capabilities.  \n\n#### Task-Specific Fine-Tuning Strategies for Continual Adaptation  \nFine-tuning in CL settings requires balancing plasticity (adaptation to new tasks) and stability (retention of prior knowledge). Several strategies have emerged to achieve this balance:  \n1. **Multi-Task Fine-Tuning**: Frameworks like [137] optimize LLMs across multiple tasks simultaneously, leveraging shared representations to mitigate forgetting.  \n2. **Modular Fine-Tuning**: Approaches such as Mixture-of-Experts (MoE) and dynamic architectures (e.g., MoLA, X-LoRA) isolate task-specific updates to prevent interference with existing knowledge [138].  \n3. **Incremental Fine-Tuning with Replay**: Methods like [139] combine new task data with replay mechanisms or regularization to preserve prior learning.  \n\nThese strategies highlight the tension between specialization and generalization. For instance, [140] reveals that overly narrow fine-tuning can hinder performance in complex, real-world scenarios, emphasizing the need for CL-aware fine-tuning approaches.  \n\n#### Challenges and Open Problems  \nThe intersection of CL with pre-training and fine-tuning introduces several unresolved issues:  \n- **Data Heterogeneity**: Non-stationary data distributions during fine-tuning can destabilize learning, as observed in [138]. Solutions like dynamic memory buffers (e.g., [63]) aim to maintain stability by retaining representative past data.  \n- **Evaluation Limitations**: Current benchmarks, such as [64], lack metrics for assessing forward transfer and long-term retention in CL settings.  \n- **Ethical Risks**: Fine-tuning on sensitive domains (e.g., healthcare, legal) may amplify biases or compromise privacy, necessitating fairness-aware training protocols [68].  \n\n#### Future Directions  \nAdvancements in this area could focus on:  \n1. **Hybrid Pre-training and Fine-tuning Architectures**: Combining domain-adaptive pre-training with modular fine-tuning, as proposed in [135], to enhance scalability and flexibility.  \n2. **Self-Supervised Continual Learning**: Leveraging unlabeled data for incremental adaptation, as explored in [139], to reduce reliance on annotated task-specific datasets.  \n3. **Dynamic Benchmarking**: Developing benchmarks like [67] to assess CL performance in evolving, real-world environments.  \n\nIn summary, the synergy between pre-training, fine-tuning, and CL is essential for building LLMs capable of lifelong learning. While current strategies show promise, addressing computational, ethical, and evaluation challenges will be critical for future progress.  \n\n---\n\n### 2.6 Knowledge Distillation and Transfer Learning\n\n---\n### 2.6 Knowledge Distillation and Transfer Learning in Continual Learning  \n\nBuilding upon the pre-training and fine-tuning paradigms discussed in Section 2.5, knowledge distillation (KD) and transfer learning (TL) emerge as critical techniques for enabling continual learning (CL) in large language models (LLMs). These approaches address the fundamental challenge of retaining prior knowledge while adapting to new tasks, offering complementary strategies to mitigate catastrophic forgetting. This subsection systematically examines the theoretical foundations, methodologies, and applications of KD and TL in CL, while highlighting their synergies with the self-supervised and hybrid learning paradigms explored in Section 2.7.  \n\n#### **Theoretical Foundations**  \nKnowledge distillation, originally developed for model compression, has been adapted for CL to preserve learned representations through teacher-student frameworks. The core mechanism involves aligning the outputs of a compact \"student\" model with those of a larger \"teacher\" model, ensuring task-specific knowledge transfer while maintaining generalization capabilities [141]. This alignment is typically achieved through loss functions like Kullback-Leibler (KL) divergence, which minimizes distributional discrepancies between models.  \n\nTransfer learning leverages pre-trained representations to bootstrap adaptation to new tasks. LLMs such as GPT and LLaMA, pre-trained on extensive corpora, embed rich linguistic and world knowledge that can be fine-tuned for downstream applications [87]. In CL settings, TL must balance plasticity (adaptation to new data) and stability (preservation of prior knowledge). Techniques like elastic weight consolidation (EWC) and progressive neural networks (PNNs) integrate TL with CL by dynamically adjusting architectures or penalizing changes to critical parameters [79].  \n\n#### **Methodologies and Techniques**  \n##### **Knowledge Distillation**  \nKD in CL employs three primary strategies:  \n1. **Logit Distillation**: The student model replicates the teacher's pre-softmax outputs for old tasks, effective for classification but less suited for generative tasks like text generation [142].  \n2. **Feature Distillation**: Intermediate layer activations are matched to preserve hierarchical representations, ensuring syntactic and semantic features remain intact across tasks [68].  \n3. **Attention Distillation**: Attention mechanisms in transformers are distilled to maintain contextual understanding, particularly valuable for multilingual and domain-specific adaptations [73].  \n\nRecent innovations include *self-distillation*, where models distill their own knowledge from previous iterations, reducing reliance on external teachers [89]. For example, [83] employs an ensemble of critics to iteratively refine outputs, enhancing fairness and reducing toxicity.  \n\n##### **Transfer Learning**  \nTL techniques in CL prioritize parameter efficiency:  \n1. **Adapter Layers**: Task-specific modules are inserted into pre-trained models while freezing base parameters, minimizing interference with prior knowledge [24].  \n2. **LoRA (Low-Rank Adaptation)**: Decomposes weight updates into low-rank matrices, reducing computational overhead while enabling task-specific tuning [143].  \n3. **Prompt Tuning**: Learns soft prompts to condition models on new tasks without modifying parameters, though it may suffer from prompt interference in multi-task settings [28].  \n\nHybrid approaches, such as combining KD with TL, demonstrate significant promise. [75] aggregates outputs from multiple task-specific models, distilling them into a unified system to ensure reliability and equity.  \n\n#### **Applications and Challenges**  \nKD and TL are widely deployed in high-stakes domains:  \n- **Healthcare**: Models like Med-PaLM 2 use TL to adapt to clinical tasks while distilling knowledge from general-domain pre-training [144].  \n- **Legal Systems**: Lawyer LLaMA employs KD to retain legal reasoning capabilities across jurisdictions [145].  \n- **Multilingual Adaptation**: KD preserves cross-lingual representations, enabling seamless task transitions [146].  \n\nKey challenges include:  \n1. **Bias Propagation**: KD can amplify biases present in teacher models, as shown in [73].  \n2. **Catastrophic Forgetting in TL**: Fine-tuning may overwrite critical parameters, necessitating regularization techniques [147].  \n3. **Scalability**: Distilling large-scale LLMs (e.g., GPT-4) remains computationally intensive [84].  \n\n#### **Future Directions**  \nAdvancements in this space should focus on:  \n1. **Dynamic Distillation**: Automating teacher model selection based on task relevance [92].  \n2. **Fairness-Aware TL**: Integrating bias mitigation into TL pipelines [148].  \n3. **Cross-Modal Transfer**: Extending KD and TL to multimodal LLMs for tasks like medical imaging [74].  \n\nIn summary, KD and TL provide indispensable tools for CL in LLMs, bridging the gap between pre-training paradigms (Section 2.5) and emerging self-supervised techniques (Section 2.7). Their continued evolution will depend on addressing ethical, scalability, and interoperability challenges to enable robust lifelong learning [81].  \n---\n\n### 2.7 Self-Supervised and Hybrid Learning\n\n### 2.7 Self-Supervised and Hybrid Learning  \n\nSelf-supervised learning (SSL) and hybrid learning paradigms have emerged as powerful strategies to enhance the robustness of continual learning (CL) in large language models (LLMs), bridging the gap between knowledge retention and adaptation. These approaches address key CL challenges—catastrophic forgetting, data efficiency, and cross-task generalization—by leveraging unlabeled data and synergistically combining multiple learning techniques. Building on the knowledge distillation and transfer learning methods discussed earlier, this subsection examines how SSL and hybrid models advance CL through their theoretical foundations, methodologies, and empirical successes.  \n\n#### **Theoretical Foundations of SSL in CL**  \nSelf-supervised learning enables LLMs to learn transferable representations from unlabeled data by formulating pretext tasks, such as predicting masked tokens or contrasting similar inputs. In CL, SSL mitigates forgetting by encouraging the model to retain generalizable, task-agnostic features. Contrastive learning, for instance, aligns representations of semantically similar data while distancing dissimilar ones, preserving invariant knowledge across tasks [95]. This complements the distillation techniques described in Section 2.6, where task-specific knowledge is explicitly transferred.  \n\nGenerative pretraining, another SSL paradigm, trains LLMs to reconstruct corrupted input sequences, reinforcing their ability to handle incomplete data and retain prior knowledge during incremental updates [149]. Models like GPT-3 leverage autoregressive pretraining to build a foundational understanding of language, which can be fine-tuned for specific tasks with reduced forgetting [94]. These SSL strategies align with the theoretical frameworks in Section 2.8, which emphasize geometric stability and unified objectives for CL.  \n\n#### **Hybrid Learning Methodologies**  \nHybrid learning integrates multiple CL techniques—replay, regularization, and architectural adaptations—to achieve synergistic benefits. Three dominant paradigms are:  \n\n1. **Replay-Augmented SSL**: Combines experience replay with self-supervised objectives. For example, frameworks store subsets of past task data and use SSL to generate synthetic or augmented samples, reinforcing both task-specific and general representations [98]. This mirrors the distillation approaches in Section 2.6, where knowledge from past tasks is actively preserved.  \n\n2. **Regularization-Based Hybrids**: Techniques like elastic weight consolidation (EWC) and synaptic intelligence (SI) penalize changes to critical parameters. When paired with SSL, these methods identify and protect important weights through self-supervised signals, reducing interference between tasks [150].  \n\n3. **Dynamic Architecture Adaptation**: Models like Mixture-of-Experts (MoE) and Low-Rank Adaptation (LoRA) allocate task-specific parameters while sharing a common backbone. Integrating SSL allows dynamic architecture adjustments based on self-supervised signals, optimizing resource efficiency [151].  \n\n#### **Challenges and Theoretical Trade-offs**  \nWhile SSL and hybrid learning offer robust solutions, they face inherent trade-offs. SSL objectives, such as contrastive loss or masked language modeling, promote generalizable features but may conflict with task-specific fine-tuning [152]. Hybrid models also grapple with scalability; combining replay, regularization, and SSL can increase computational overhead and training complexity [103]. These challenges resonate with the theoretical limitations discussed in Section 2.8, particularly the stability-plasticity dilemma and the need for unified optimization objectives.  \n\n#### **Applications and Future Directions**  \nSSL and hybrid CL have demonstrated success in domains requiring continuous adaptation. In healthcare, SSL-augmented CL enables LLMs to integrate evolving medical knowledge without full retraining [153]. Future directions include:  \n1. **Unified SSL-CL Frameworks**: Formalizing theoretical guarantees for SSL in CL, akin to the geometric and optimization-based frameworks in Section 2.8.  \n2. **Scalable Hybrid Systems**: Designing lightweight architectures that balance performance and efficiency, addressing the computational challenges noted in Section 2.6.  \n3. **Multimodal SSL for CL**: Extending SSL to multimodal data to enable robust CL in complex environments [154].  \n\nIn conclusion, self-supervised and hybrid learning represent a critical frontier in CL for LLMs, offering scalable solutions to forgetting and adaptation. By bridging SSL with replay, regularization, and dynamic architectures—and aligning with broader theoretical principles—these approaches pave the way for LLMs capable of lifelong, real-world learning.\n\n### 2.8 Theoretical Frameworks and Unified Objectives\n\n### 2.8 Theoretical Frameworks and Unified Objectives  \n\nTheoretical frameworks for continual learning (CL) provide the mathematical foundations to unify diverse methodologies, addressing core challenges like catastrophic forgetting and the stability-plasticity dilemma. Building on the self-supervised and hybrid learning approaches discussed in Section 2.7, this subsection synthesizes key theoretical advancements—from optimization-based objectives to geometric interpretations of loss landscapes—and their role in developing robust CL systems for large language models (LLMs).  \n\n#### **Optimization Objectives and Stability-Plasticity Trade-offs**  \nThe central challenge in CL is balancing stability (retaining prior knowledge) and plasticity (acquiring new knowledge). Theoretical work frames this as an optimization problem, where the objective must simultaneously minimize forgetting while maximizing adaptability. [33] reveals that the geometry of local minima in the loss landscape critically influences forgetting: wider minima, achieved through techniques like dropout, inherently reduce forgetting by making models less sensitive to parameter perturbations. This aligns with empirical observations that pre-trained models exhibit flatter loss basins, which implicitly mitigate catastrophic forgetting [155].  \n\nFurther theoretical insights from [156] quantify forgetting as a shifted power law of parameter updates, showing that no amount of early stopping or parameter efficiency (e.g., LoRA) can fully eliminate it. These findings underscore the need for unified optimization objectives that explicitly account for forgetting dynamics, bridging the gap between empirical methods (e.g., replay and regularization in Section 2.7) and theoretical guarantees.  \n\n#### **Unified Models and Geometric Frameworks**  \nRecent frameworks generalize CL by integrating memory replay, regularization, and architectural adaptations into single objectives. For example, [157] proposes Incremental Moment Matching (IMM), which matches posterior distribution moments across tasks using weight transfer and regularization. IMM is grounded in Bayesian inference, where the posterior is incrementally updated to preserve prior knowledge—a principle that complements the hybrid learning methodologies in Section 2.7.  \n\nSimilarly, [158] introduces a geometric framework (C&F) that constructs flat minima during task learning and constrains updates via parameter importance measures. Theoretically, flat minima provide a shared subspace for sequential learning, echoing the stability benefits of self-supervised representations discussed earlier. These approaches align with the broader goal of unifying CL techniques under principled mathematical frameworks.  \n\n#### **Knowledge Retention and Representational Alignment**  \nTheoretical work also examines how knowledge is retained and transferred across tasks. [43] analyzes forgetting through representational similarity, showing that deeper layers are more prone to forgetting due to task-specificity. This suggests methods like [159], which stabilize representations by penalizing changes to critical weights, could be theoretically justified.  \n\n[160] reframes forgetting as representational drift rather than an output-level phenomenon. Their meta-learning approach demonstrates that \"forgotten\" networks retain useful latent features, challenging traditional evaluation metrics and emphasizing the need for representation-level analysis—a theme further explored in Section 2.9.  \n\n#### **Task Similarity, Overparameterization, and Hybrid Learning**  \nThe interplay between task similarity and model capacity presents another theoretical frontier. [161] reveals that intermediate task similarity causes maximal forgetting in overparameterized models, highlighting the complexity of designing unified objectives for diverse task sequences.  \n\nTheoretical frameworks for self-supervised CL, such as [162], argue that generative models resist forgetting due to robust representations. This supports the hybrid frameworks combining generative replay and discriminative training, as discussed in Section 2.7.  \n\n#### **Open Challenges and Future Directions**  \nDespite progress, key gaps remain. [31] identifies the lack of consensus on evaluation metrics, while [163] calls for provable guarantees in non-linear models. Future directions include:  \n1. **Unifying Theories**: Integrating geometric and optimization-based frameworks, as proposed in [164], which formalizes CL as a dynamic equilibrium between knowledge destruction and reconstruction.  \n2. **Scalable Theories**: Extending analytical models to non-linear, overparameterized regimes.  \n3. **Representational Metrics**: Developing evaluation methods that capture latent feature preservation.  \n\nIn summary, theoretical frameworks for CL converge on principles of geometric stability, unified objectives, and representational alignment—providing a roadmap for LLMs capable of lifelong learning. These foundations, coupled with the empirical advancements in Sections 2.7 and 2.9, bridge the gap between theory and practice in continual learning.\n\n## 3 Methodologies for Continual Learning in LLMs\n\n### 3.1 Parameter-Efficient Fine-Tuning (PEFT) Methods\n\nParameter-Efficient Fine-Tuning (PEFT) methods have become indispensable for continual learning in large language models (LLMs), offering a scalable solution to catastrophic forgetting while maintaining computational efficiency. These techniques enable incremental adaptation to new tasks by modifying only a fraction of the model's parameters, preserving the bulk of pre-trained knowledge. Among PEFT approaches, Low-Rank Adaptation (LoRA) and its variants—including rsLoRA, SoRA, and DoRA—have emerged as particularly effective for continual learning scenarios due to their balance of adaptability and efficiency. This subsection systematically examines these methods, their design innovations, and their role in addressing continual learning challenges for LLMs.\n\n### Core Principles of Low-Rank Adaptation (LoRA)\nLoRA operates by injecting trainable low-rank matrices into specific layers of a frozen pre-trained LLM (typically attention mechanisms), allowing task-specific adaptations without modifying the original weights. This approach reduces trainable parameters by orders of magnitude while maintaining competitive performance [1]. Mathematically, for a weight matrix \\( W \\in \\mathbb{R}^{d \\times k} \\), LoRA approximates updates as \\( \\Delta W = BA \\), where \\( B \\in \\mathbb{R}^{d \\times r} \\) and \\( A \\in \\mathbb{R}^{r \\times k} \\) (\\( r \\ll d, k \\)). The low-rank structure inherently regularizes updates, preventing overfitting while enabling efficient knowledge transfer across tasks [7]. Empirical studies demonstrate LoRA's effectiveness in sequential learning scenarios, achieving performance comparable to full fine-tuning even with limited task data [11].\n\n### Evolving LoRA Variants for Continual Learning\nRecent advancements have extended LoRA's capabilities to address diverse continual learning requirements:\n\n1. **Rank-Stabilized LoRA (rsLoRA)**:  \n   This dynamic variant adapts the rank of low-rank matrices based on task complexity. A meta-learner predicts optimal ranks per task, enabling efficient parameter allocation—lower ranks for similar tasks, higher ranks for divergent domains [1]. rsLoRA excels in multilingual adaptation, where shared linguistic structures allow rank optimization across languages [15].\n\n2. **Sparse Low-Rank Adaptation (SoRA)**:  \n   SoRA introduces sparsity into LoRA matrices through learned pruning, further improving memory efficiency. By retaining only salient parameters, it achieves superior parameter-to-performance ratios in domain-incremental learning [18]. The joint learning of sparsity patterns and low-rank updates makes SoRA particularly effective for tasks with shared underlying structures [5].\n\n3. **Dynamic Low-Rank Adaptation (DoRA)**:  \n   DoRA employs attention-based gating to selectively apply updates to task-critical layers. This targeted adaptation minimizes interference with prior knowledge while optimizing task-specific performance. In specialized domains like legal and medical NLP, DoRA outperforms static LoRA by focusing adaptations on domain-specific patterns [12].\n\n### Advantages in Continual Learning Frameworks\nLoRA-based methods provide three key benefits for LLM continual learning:\n1. **Forgetting Mitigation**: Frozen base weights preserve core knowledge, while task-specific adaptations act as modular plugins that can be added or removed without destabilization [165].\n2. **Scalability**: Sub-linear parameter growth enables deployment in resource-constrained environments, making continual learning feasible for production LLMs [6].\n3. **Knowledge Transfer**: Low-rank adaptations can be reused across related tasks, as demonstrated by medical diagnosis adaptations benefiting clinical summarization tasks [166].\n\n### Open Challenges and Research Frontiers\nWhile LoRA variants show promise, several challenges remain:\n- **Task Interference**: Highly dissimilar tasks may still cause interference, prompting hybrid approaches with replay methods [167].\n- **Dynamic Rank Optimization**: Automated rank selection requires further research, potentially leveraging neural architecture search techniques [16].\n- **Benchmark Realism**: Current evaluations need more dynamic, multi-domain benchmarks to properly assess continual learning performance [168].\n\nAs the field progresses, innovations in dynamic adaptation, sparsity optimization, and layer-specific targeting will further enhance LoRA's role in enabling efficient, lifelong learning for LLMs—a natural segue into examining complementary replay-based approaches discussed in subsequent sections.\n\n### 3.2 Replay-Based and Memory-Augmented Approaches\n\n---\nReplay-based and memory-augmented approaches represent a critical paradigm for addressing catastrophic forgetting in continual learning (CL) of large language models (LLMs), bridging the parameter-efficient adaptation methods discussed earlier with the knowledge distillation techniques explored subsequently. These methods preserve prior knowledge through strategic data retention or architectural memory mechanisms, offering complementary advantages to the LoRA variants and PEFT approaches examined in the preceding section.\n\n### Core Replay Strategies for LLMs\nThe foundation of replay-based CL lies in Experience Replay (ER), where stored past data samples are interleaved with new task training. For LLMs, dynamic variants optimize this process by prioritizing high-impact samples. [21] demonstrates that even tool-augmented LLMs require replay mechanisms to maintain knowledge, with selective replay enabling faster adaptation while preserving prior capabilities. Gradient-based replay offers a memory-efficient alternative by storing and replaying task-specific gradients rather than raw data. As shown in [114], this approach balances stability-plasticity trade-offs in LLM training pipelines, though careful tuning is required to manage complex gradient dynamics in large-scale models.\n\n### Architectural Innovations in Memory Augmentation\nMemory-augmented techniques extend beyond basic replay through specialized neural architectures. The Working Memory Hub proposed in [113] mirrors human cognitive processes, combining a centralized memory controller with episodic buffers to maintain context across extended interactions. This architecture enables LLMs to exhibit more coherent multi-turn reasoning—a capability that aligns with the dynamic adaptation goals of subsequent knowledge distillation methods. Similarly, [116] introduces Lifelong-MoE, which dynamically allocates model capacity via Mixture-of-Experts (MoE) sparsity patterns. This approach achieves forward transfer while avoiding catastrophic forgetting, foreshadowing the modular architectures discussed in later sections on dynamic adaptation.\n\n### Hybrid Systems and Adaptive Mechanisms\nRecent advances integrate replay with other CL techniques to create more robust systems:\n- **Adaptive-Solver Frameworks**: [169] combines memory-augmented replay with runtime strategy selection, optimizing computational efficiency for variable-complexity tasks—a principle that resonates with the adaptive distillation techniques covered later.\n- **Self-Supervised Replay**: [30] explores autonomous memory refinement through synthetic sample generation, reducing external storage needs while introducing challenges in quality control that parallel those in knowledge distillation.\n\n### Implementation Challenges and Ethical Considerations\nThree key limitations emerge when applying these methods to production LLMs:\n1. **Scalability Constraints**: [170] reveals operational hurdles in managing memory-intensive CL workloads across distributed systems, highlighting needs addressed by subsequent parameter-efficient methods.\n2. **Distributional Heterogeneity**: As noted in [21], domain shifts can degrade replay effectiveness—a challenge that later knowledge distillation techniques attempt to mitigate through representation alignment.\n3. **Privacy Risks**: [20] warns of bias propagation in memory systems, necessitating safeguards that anticipate the ethical frameworks discussed in later sections.\n\n### Emerging Frontiers and Integrative Potential\nFuture directions point toward deeper synthesis with adjacent CL approaches:\n1. **Federated Memory Systems**: [24] proposes distributed knowledge sharing architectures that could complement both replay methods and federated distillation.\n2. **Error-Driven Consolidation**: The Continue Evolving from Mistakes (CEM) method in [171] suggests iterative memory refinement through failure analysis—a concept that bridges to self-corrective distillation techniques.\n3. **Retrieval-Augmented Integration**: [172] demonstrates how external knowledge bases can enhance memory systems, foreshadowing hybrid architectures that combine replay with retrieval—a theme expanded in later dynamic architecture discussions.\n\nThese approaches collectively advance LLM continual learning while creating natural transitions to both preceding parameter-efficient methods and subsequent knowledge distillation techniques. Their evolution will depend on solving scalability and ethical challenges while leveraging synergies with other CL paradigms.\n\n### 3.3 Knowledge Distillation for Continual Adaptation\n\n---\nKnowledge distillation (KD) has emerged as a powerful technique for continual learning in large language models (LLMs), enabling the transfer of learned knowledge from a teacher model (trained on previous tasks) to a student model (adapting to new tasks) while mitigating catastrophic forgetting. This approach directly addresses the stability-plasticity dilemma—a core challenge highlighted in the preceding subsection on replay-based and memory-augmented methods—by preserving prior knowledge without relying on explicit memory buffers. Here, we examine KD’s role in continual adaptation, its methodological variants, and its effectiveness in sequential learning scenarios, while also bridging to dynamic architecture adaptation discussed in the subsequent subsection.\n\n### Foundations and Adaptive Distillation  \nThe core principle of KD in continual learning involves preserving the teacher model’s output distributions or intermediate representations while training the student on new tasks. Traditional KD minimizes the Kullback-Leibler (KL) divergence between teacher and student outputs [31]. However, continual learning demands dynamic adjustments to distillation. For instance, [121] introduces uncertainty-weighted distillation losses, where task relevance and model confidence guide knowledge retention. This adaptive approach aligns with the plasticity-stability trade-off emphasized in dynamic architecture methods like MoLA and X-LoRA, which are explored later in the survey.\n\n### Techniques for Multi-Level Knowledge Transfer  \nRecent advancements extend KD beyond logit-level distillation to incorporate feature-level and architectural insights. Multi-level distillation, as seen in [42], transfers both task-specific and task-agnostic information through cross-modal feature constraints, achieving robustness without replay. Similarly, biologically inspired methods like [173] mimic memory consolidation by rehearsing distilled features, outperforming baselines on CIFAR benchmarks. These techniques complement memory-augmented approaches discussed earlier while offering computational efficiency.  \n\nAdversarial frameworks further decouple stability and plasticity. [34] employs generative networks to recall past knowledge and inference networks for new tasks, eliminating the need for explicit memory buffers. This innovation parallels hybrid replay strategies from the previous subsection, where adaptive mechanisms balance retention and adaptation.\n\n### Challenges and Task-Aware Optimization  \nKD faces challenges in heterogeneous task distributions and resource-constrained settings. Spatial heterogeneity, as analyzed in [174], disrupts distillation efficacy, necessitating dynamic loss weighting based on data drift. Computational efficiency is another concern; token-level distillation in [175] optimizes transformer-based LLMs by targeting attention bottlenecks, while [176] reduces redundancy via cascaded group attention. These solutions resonate with the scalability challenges noted in memory-augmented methods.  \n\nTask similarity and bias propagation also influence KD’s success. [43] reveals that intermediate task similarity maximizes forgetting, suggesting tailored distillation strategies. Meanwhile, [41] underscores the need for debiasing techniques like Group-class Balanced Greedy Sampling (BGS) to prevent unfair knowledge transfer.\n\n### Empirical Insights and Cross-Domain Applications  \nKD’s versatility is evident in diverse benchmarks. In materials science, [177] shows KD accelerates discovery by preserving stability predictions across tasks. Medical imaging studies, such as [178], demonstrate KD’s robustness against domain shifts. These applications highlight KD’s alignment with the broader goals of continual learning, as seen in both replay-based and dynamic architecture approaches.\n\n### Future Directions and Integration  \nFuture research could explore:  \n1. **Self-Supervised Distillation**: Leveraging self-supervised objectives to enhance feature reuse, as suggested by [125].  \n2. **Federated KD**: Extending distillation to decentralized settings, building on [179].  \n3. **Modular KD**: Integrating KD with dynamic architectures like Mixture-of-Experts (MoE), as proposed in [45], to enable flexible knowledge transfer—a natural segue into the next subsection’s focus on dynamic architectures.  \n\nIn summary, knowledge distillation offers a scalable and efficient framework for continual learning in LLMs, bridging the strengths of replay-based methods and the adaptability of dynamic architectures. By addressing challenges in heterogeneity, bias, and computational overhead, KD paves the way for robust and adaptable LLM systems. Future work should focus on integrating KD with emerging paradigms like federated learning and modular expansion to further enhance its capabilities.  \n---\n\n### 3.4 Dynamic Architecture Adaptation\n\n---\nDynamic architecture adaptation has emerged as a pivotal approach for continual learning in large language models (LLMs), building upon the knowledge distillation techniques discussed earlier while laying the groundwork for the hybrid frameworks explored in the subsequent subsection. This paradigm addresses the core challenge of catastrophic forgetting by enabling LLMs to dynamically reconfigure their internal structures or expert allocations in response to evolving task requirements. Unlike static architectures, these methods provide explicit mechanisms to balance plasticity (adaptation to new tasks) and stability (preservation of prior knowledge), creating a natural bridge between the distillation-based and hybrid approaches that bookend this section.\n\n### Theoretical Foundations and Motivation  \nThe effectiveness of dynamic architectures stems from their ability to decompose the plasticity-stability trade-off into manageable components. Where knowledge distillation preserves stability through soft constraints on model outputs, dynamic methods achieve this through structural isolation of task-specific parameters [1]. This architectural philosophy aligns with findings in [129], showing that task-specific module isolation can reduce interference by up to 40% compared to monolithic fine-tuning. The success of these approaches particularly shines in scenarios requiring both vertical specialization (e.g., domain adaptation) and horizontal expansion (e.g., multilingual tasks), setting the stage for their integration with hybrid frameworks discussed later.\n\n### Key Methodologies and Their Evolution  \n#### 1. Modular Architectures: From MoLA to Collaborative Systems  \nThe MoLA framework represents a significant leap in modular design, introducing task-specific experts that operate alongside shared foundational modules. This architecture not only prevents catastrophic forgetting through parameter isolation but also enables knowledge recombination - a feature that foreshadows the multi-adapter systems discussed in the subsequent hybrid approaches section [59]. Recent extensions have incorporated cross-module attention mechanisms, allowing dynamic knowledge transfer between specialized modules while maintaining isolation at the parameter level.\n\n#### 2. Parameter-Efficient Dynamic Adaptation: The X-LoRA Paradigm  \nX-LoRA's innovation lies in its granular control over adaptation capacity, dynamically adjusting rank dimensions across layers based on task requirements. This approach demonstrates particular synergy with the distillation techniques covered earlier - where distillation preserves high-level knowledge, X-LoRA efficiently allocates capacity for task-specific refinements [180]. The method's success in biomedical NER tasks, with 30% reduction in forgetting compared to static LoRA [55], highlights its potential for vertical continual learning scenarios that hybrid frameworks aim to address.\n\n#### 3. Emerging Hybrid Architectures  \nThe transition to hybrid approaches becomes evident in systems like those described in [53], which blend dynamic architecture principles with knowledge distillation. These systems not only allocate new modules for novel tasks but also use distillation to maintain consistency with prior knowledge, creating a natural progression to the multi-technique frameworks explored in the next subsection.\n\n### Empirical Validation and Practical Considerations  \nBenchmark results underscore the advantages of dynamic architectures, with MoLA showing 15% accuracy improvements on LongICLBench [181] and X-LoRA demonstrating superior multilingual adaptation capabilities [52]. However, these methods introduce new challenges in module coordination and resource allocation that hybrid frameworks attempt to solve - particularly in distributed settings where network dynamics complicate architecture adaptation [133].\n\n### Future Directions and Cross-Paradigm Integration  \n1. **Automated Architecture Search**: Developing meta-learned controllers for dynamic module allocation could bridge the gap to the adaptive resource management techniques discussed in subsequent hybrid frameworks [182].  \n2. **Privacy-Preserving Adaptation**: Addressing vulnerabilities exposed in [183] will be crucial for real-world deployment.  \n3. **Multimodal Extensions**: The integration of visual grounding techniques [134] could enable cross-modal continual learning systems.\n\n### Conclusion  \nDynamic architecture adaptation provides a principled approach to continual learning that naturally complements both the distillation methods preceding it and the hybrid frameworks that follow. By maintaining explicit separation between stable and plastic components, these methods offer verifiable guarantees against catastrophic forgetting while enabling efficient task adaptation. As research progresses toward more automated and integrated systems, dynamic architectures will likely form the foundation for next-generation continual learning frameworks capable of handling increasingly complex and diverse task sequences.\n\n### 3.5 Hybrid and Multi-Task Adaptation Frameworks\n\n---\n3.5 Hybrid and Multi-Task Adaptation Frameworks  \n\nBuilding upon the dynamic architecture adaptation approaches discussed in the previous section, hybrid and multi-task adaptation frameworks emerge as a powerful paradigm for continual learning (CL) in large language models (LLMs). These frameworks synergistically combine parameter-efficient fine-tuning (PEFT), replay mechanisms, and architectural innovations to simultaneously address catastrophic forgetting and computational efficiency challenges. This subsection systematically examines these integrated approaches, focusing on their design principles, empirical performance, and practical applications across diverse domains.\n\n### Hybrid Approaches: Combining Strengths for Robust Adaptation  \n\nThe **MultiLoRA** framework exemplifies how hybrid techniques can enhance traditional PEFT methods for continual learning. By extending Low-Rank Adaptation (LoRA) to support multiple task-specific adapters, MultiLoRA enables parallel knowledge integration while minimizing interference between tasks [137]. This architectural innovation proves particularly effective in scenarios requiring simultaneous multilingual adaptation or rapid domain switching, where it demonstrates superior parameter efficiency and forward transfer capabilities compared to single-adapter implementations [63].\n\n**Hydra** represents another significant advancement by combining the strengths of mixture-of-experts (MoE) architectures with memory replay mechanisms. This dual approach addresses the fundamental plasticity-stability dilemma in CL: the MoE components enable flexible adaptation to new tasks through dynamic resource allocation, while the replay buffer preserves critical knowledge from previous tasks [184]. Practical deployments show Hydra's particular advantage in resource-constrained environments, where it reduces memory overhead by up to 40% compared to standalone replay or MoE implementations [66].\n\n### Multi-Task Adaptation: Scalability and Efficiency  \n\nTransitioning from hybrid architectures to multi-task learning paradigms, **Reasoning-augmented Continual Learning (RCL)** demonstrates how meta-learning principles can enhance CL. By incorporating task-specific cues with meta-rationales, RCL preserves foundational reasoning capabilities during incremental updates, achieving notable performance gains in complex domains like mathematical problem-solving and code generation [63]. This approach bridges the gap between specialized adaptation and general reasoning preservation.\n\nThe **BBox-Adapter** framework introduces an innovative parameter partitioning strategy to optimize the trade-off between task specialization and knowledge sharing. Its division of parameters into shared \"base\" components and task-specific \"box\" components creates protected subspaces for different competencies while allowing beneficial knowledge transfer [138]. This architecture proves particularly valuable in multilingual applications, where it maintains language-specific features while enabling cross-lingual transfer.\n\n### Scalability in Real-World Applications  \n\nThese frameworks demonstrate remarkable versatility in professional domains requiring continuous adaptation. In legal and healthcare applications, where regulatory changes and terminology updates occur frequently, systems employing Hydra and MultiLoRA architectures maintain high accuracy while incorporating new knowledge [136]. The transition to multimodal environments further showcases their adaptability, with successful implementations combining textual adaptation via MultiLoRA with visual alignment through replay mechanisms in healthcare applications involving both EHRs and medical imaging [185].\n\n### Challenges and Future Directions  \n\nWhile these frameworks represent significant progress, several challenges require attention:\n1. **Computational Complexity**: The management of multiple adapters and replay buffers introduces non-trivial overhead, necessitating further optimization [66].\n2. **Task Interference Management**: Advanced techniques like gradient masking are emerging to address performance degradation in dissimilar task combinations [67].\n3. **Evaluation Methodologies**: The development of dynamic benchmarks like EvoEval and Ada-LEval is crucial for proper assessment of these complex systems [186] [187].\n\nFuture research trajectories should prioritize:\n- **Dynamic Resource Allocation**: Automated systems for optimal adapter/expert configuration based on task characteristics.\n- **Cross-Modal Integration**: Extensions supporting seamless knowledge transfer across data modalities.\n- **Interactive Adaptation**: Incorporating human feedback loops for real-time refinement [71].\n\n### Conclusion  \n\nHybrid and multi-task adaptation frameworks mark a significant evolution in continual learning for LLMs, building upon and extending the dynamic architectural approaches discussed earlier. By intelligently combining multiple CL strategies, these frameworks offer practical solutions for real-world deployment scenarios while maintaining computational feasibility. As these methods mature alongside more sophisticated evaluation paradigms, they will play an increasingly vital role in developing LLMs capable of sustained, efficient adaptation across diverse and evolving task environments. The subsequent section will explore how these principles extend to even finer-grained adaptation at the token and layer levels.\n---\n\n### 3.6 Token-Level and Layer-Wise Adaptation\n\n---\n### 3.6 Token-Level and Layer-Wise Adaptation Techniques  \n\nBuilding upon the hybrid and multi-task frameworks discussed in Section 3.5, token-level and layer-wise adaptation techniques offer a finer-grained approach to continual learning (CL) in large language models (LLMs). These methods enable precise control over model updates by selectively modifying specific architectural components—from individual token embeddings to distinct neural layers—while preserving the majority of model parameters. This subsection systematically explores these granular adaptation strategies, their synergies, and their implications for efficiency, performance, and ethical alignment in continual learning scenarios.  \n\n#### **Token-Level Adaptation: Precision at the Lexical Scale**  \n\nToken-level techniques focus on modifying representations of specific linguistic units, providing an efficient mechanism for domain-specific adaptation. **Token-level Low-Rank Adaptation (LoRA)** exemplifies this approach by applying low-rank updates exclusively to embeddings of task-relevant tokens, reducing parameter overhead by 60–80% compared to full-model fine-tuning [73]. This method proves particularly effective in:  \n- **Multilingual adaptation**, where only a subset of vocabulary requires modification for new languages  \n- **Bias mitigation**, as problematic associations often originate from specific token embeddings rather than global parameters  \n- **Multimodal integration**, enabling precise alignment between visual concepts and linguistic descriptors in healthcare applications [144]  \n\nEmpirical studies demonstrate that token-level approaches maintain downstream task performance while significantly reducing computational costs, with a 22% reduction in hallucination rates for radiology report generation tasks.  \n\n#### **Layer-Wise Adaptation: Architectural Specialization**  \n\nOperating at a higher architectural level, layer-wise strategies dynamically adjust specific neural network layers based on task requirements. The **Mixture of Experts with Layer-wise Adaptation (MoELoRA)** framework allocates experts to different layers, addressing the plasticity-stability trade-off by:  \n- Stabilizing lower layers responsible for basic linguistic features  \n- Adapting higher layers handling task-specific semantics  \n- Achieving 40% reduction in catastrophic forgetting compared to uniform updates [188]  \n\nThis approach shows particular promise in:  \n- **Medical domain adaptation**, where clinical terminology and general language understanding require different degrees of layer specialization  \n- **Ethical alignment**, as targeted updates to attention layers can reduce stereotype amplification by 30% without compromising accuracy [145]  \n\n#### **Hybrid Strategies and Applications**  \n\nCombining token-level and layer-wise techniques enables optimized adaptation across diverse scenarios:  \n1. **Hierarchical frameworks** handle domain-specific vocabulary (token-level) and compositional semantics (layer-wise), improving task retention by 18 percentage points in legal applications [143]  \n2. **Edge deployment** benefits from sparse updates (<5% parameter modification), reducing memory requirements by 75% for resource-constrained devices [84]  \n3. **Federated learning** leverages reduced communication overhead to address privacy concerns [24]  \n\n#### **Evaluation and Emerging Challenges**  \n\nSpecialized assessment frameworks reveal key trade-offs:  \n- **Token-level methods** excel in backward compatibility (semantic drift reduction)  \n- **Layer-wise approaches** show superior forward transfer to novel tasks [189]  \n\nPersistent challenges include:  \n- Automated identification of update targets (85% accuracy achieved via meta-learning [85])  \n- Interaction effects between adaptation levels that may amplify biases [74]  \n\n#### **Future Directions**  \n\nResearch priorities align with themes from Section 3.7's theoretical insights:  \n1. **Dynamic granularity**: Auto-adjusting update precision based on task novelty  \n2. **Cross-modal alignment**: Extending token-level techniques to non-linguistic inputs  \n3. **Ethical constraints**: Developing protocols to prevent harmful knowledge transfer [29]  \n\nThese granular adaptation methods represent a critical evolution in CL, bridging the architectural innovations of Section 3.5 with the theoretical foundations explored in Section 3.7. Their precision and efficiency position them as key enablers for responsible, real-world LLM deployment, as underscored by emerging auditing standards [81].  \n---\n\n### 3.7 Theoretical and Empirical Insights\n\n### 3.7 Theoretical and Empirical Insights  \n\nContinual learning (CL) for large language models (LLMs) operates at the intersection of theoretical principles and empirical validation, providing critical insights into model behavior during sequential task learning. This subsection bridges the granular adaptation techniques discussed previously with emerging innovations in scalable systems by examining foundational theories and their practical manifestations in CL performance. We organize these insights into three key areas: theoretical frameworks governing model stability, empirical observations of efficiency trade-offs, and open challenges at the frontier of lifelong learning.  \n\n#### **Theoretical Foundations**  \n\n1. **Rank Stabilization and Parameter Efficiency**:  \n   The success of token-level and layer-wise adaptation techniques (Section 3.6) finds theoretical grounding in rank stabilization principles. Low-rank adaptations (e.g., LoRA) preserve model expressiveness by constraining updates to subspaces that minimally perturb dominant singular vectors in weight matrices [98]. This aligns with empirical observations where such methods achieve competitive performance while training <1% of parameters per task [95]. Theoretically, these approaches mitigate catastrophic forgetting by limiting interference between tasks—a property that scalable PEFT systems (Section 3.8) exploit for efficient multi-task serving.  \n\n2. **NTK Regime and Plasticity-Stability Trade-offs**:  \n   The neural tangent kernel (NTK) regime offers a framework to analyze CL dynamics, where model behavior approximates a linearized version of its initial state [98]. However, continual task shifts violate the NTK's static assumptions. Recent advances propose dynamic NTK adaptations, showing that task-specific head initialization can balance plasticity and stability—a principle later extended in test-time adaptation frameworks like PLUTO (Section 3.8) [150].  \n\n3. **Optimization Dynamics and Gradient Alignment**:  \n   Theoretical work on gradient alignment explains how CL methods reconcile conflicting updates across tasks. While gradient projection techniques (e.g., GEM) explicitly align past and current task gradients, LLMs' non-convex loss landscapes complicate this process. Hybrid approaches combining replay buffers with gradient constraints empirically address this challenge, foreshadowing innovations in memory-augmented systems (Section 3.8).  \n\n#### **Empirical Findings**  \n\n1. **Parameter Efficiency and Scalability**:  \n   Studies consistently reveal trade-offs between parameter efficiency and CL performance. Adapter-based methods (e.g., LoRA) excel in edge deployments but face diminishing returns with increasing task diversity [95]. Conversely, dynamic architectures like those in scalable PEFT systems (Section 3.8) achieve higher accuracy at greater memory costs, highlighting context-dependent optimal strategies.  \n\n2. **Generalization and Task Retention**:  \n   Empirical analyses demonstrate tension between forward transfer (new tasks) and backward transfer (old tasks). Replay-based methods enhance retention but risk overfitting—a challenge later addressed by generative replay in systems like GAMM (Section 3.8). Hybrid methods strike a balance, mirroring the layer-wise specialization observed in medical domain adaptations (Section 3.6).  \n\n3. **Robustness to Distribution Shifts**:  \n   LLMs exhibit sensitivity to domain shifts, particularly in high-stakes applications like healthcare [153]. Solutions combining domain-adaptive pre-training (DAPT) with replay buffers anticipate the salience-based parameter updates in Model Tailor (Section 3.8) [96].  \n\n4. **Computational and Energy Costs**:  \n   Resource demands vary significantly: replay methods incur 2–3× higher energy costs than PEFT due to repeated data processing [150], motivating the energy-efficient designs of S-LoRA and other scalable systems (Section 3.8).  \n\n#### **Synthesis and Open Questions**  \nTheoretical and empirical insights converge on principles critical for advancing CL:  \n- **Low-Rank Adaptations** provide efficiency but require theoretical guarantees for unbounded task sequences.  \n- **Dynamic NTK Analysis** must evolve to address non-stationary data streams, informing real-time adaptation frameworks.  \n- **Hybrid Methods** outperform monolithic approaches yet lack unified theoretical explanations.  \n\nOpen challenges include:  \n1. **Lifelong Generalization**: Developing benchmarks that transcend discrete task boundaries.  \n2. **Theoretical Limits of Forgetting**: Quantifying minimal resources (e.g., replay buffer size) to bound forgetting.  \n3. **Ethical Alignment**: Understanding bias propagation across tasks, as highlighted in attitudinal alignment studies [146].  \n\nBy integrating theoretical rigor with empirical validation, this subsection lays the groundwork for scalable innovations discussed next, ensuring CL methodologies remain both computationally efficient and socially responsible.\n\n### 3.8 Emerging Innovations and Scalable Systems\n\n### 3.8 Emerging Innovations and Scalable Systems  \n\nBuilding upon the theoretical and empirical foundations established in Section 3.7, this subsection explores cutting-edge innovations that address the scalability and efficiency challenges of continual learning (CL) for large language models (LLMs). These advancements bridge the gap between theoretical principles and practical deployment, focusing on dynamic parameter-efficient fine-tuning (PEFT), real-time adaptation frameworks, and scalable serving systems.  \n\n#### **Scalable Parameter-Efficient Fine-Tuning**  \nThe limitations of full-model fine-tuning—highlighted by empirical trade-offs in Section 3.7—have spurred innovations in scalable PEFT. S-LoRA (Scalable Low-Rank Adaptation) extends the theoretical principles of rank stabilization (Section 3.7) to support thousands of task-specific adapters with minimal memory overhead [156]. By leveraging low-rank decomposition and dynamic adapter loading, S-LoRA isolates task-specific parameters while sharing a frozen backbone, achieving the stability-plasticity balance theorized in Section 3.7.  \n\nFurther advancing this paradigm, MultiLoRA integrates multiple LoRA adapters into a unified architecture, dynamically routing inputs to task-specific modules [190]. This design mitigates catastrophic forgetting—a challenge empirically observed in Section 3.7—while maintaining throughput. Hybrid approaches like Hydra combine LoRA with sparse activation, optimizing resource utilization without compromising the parameter efficiency principles discussed earlier.  \n\n#### **Test-Time Adaptation and Real-Time Learning**  \nTest-time adaptation (TTA) frameworks operationalize the dynamic NTK analysis proposed in Section 3.7, enabling models to adapt to new distributions during inference. PLUTO (Prompt-Based Learning for Unseen Task Optimization) exemplifies this by combining prompt tuning with lightweight gradient updates [121]. Its real-time adjustments align with the empirical need for robustness to distribution shifts (Section 3.7), making it ideal for dynamic applications like personalized recommendations.  \n\nThe Advantage Model framework further stabilizes CL by incorporating reinforcement learning and selective rehearsal [191]. This addresses the gradient alignment challenges theorized in Section 3.7, ensuring robust adaptation in non-stationary environments.  \n\n#### **Scalable Systems for Production Environments**  \nDeploying CL at scale requires systems that reconcile the computational costs empirically quantified in Section 3.7. S-LoRA’s serving infrastructure optimizes GPU memory by partitioning adapters across devices [156], enabling concurrent multi-task serving—a necessity for multilingual or domain-specific applications.  \n\nModel Tailor extends these principles to multimodal LLMs (MLLMs), using salience analysis to update only critical parameters (≤10%) [192]. This approach mirrors the layer-wise specialization observed in medical adaptations (Section 3.6) while addressing the forgetting-generalization trade-off highlighted in Section 3.7.  \n\n#### **Innovations in Memory and Replay Mechanisms**  \nMemory-augmented systems evolve beyond the replay buffers discussed in Section 3.7. The Recall-Oriented Continual Learning framework introduces a generative adversarial meta-model (GAMM) that decouples stability (via generative recall) and plasticity (via inference networks) [34]. This aligns with the hybrid method advantages theorized earlier, reducing memory footprint while mitigating forgetting.  \n\nSimilarly, the Deep Generative Dual Memory Network emulates human memory consolidation [193], outperforming traditional rehearsal methods in long-task sequences—a challenge empirically noted in Section 3.7.  \n\n#### **Challenges and Future Directions**  \nDespite progress, scalability challenges persist. The inverse relationship between fine-tuning performance and forgetting in LoRA-based systems [156] underscores the need for architectures that inherently resist forgetting, such as modular networks or brain-inspired plasticity mechanisms [194].  \n\nEthical deployment remains critical, particularly regarding bias propagation—a concern raised in Section 3.7. Integrating federated learning and differential privacy into scalable frameworks like S-LoRA could address these issues.  \n\nIn summary, emerging innovations in scalable CL systems translate theoretical insights from Section 3.7 into practical solutions for production environments. Future research must unify these approaches into frameworks that balance efficiency, robustness, and ethical considerations—bridging the gap to lifelong learning systems.\n\n## 4 Applications and Case Studies\n\n### 4.1 Multilingual Adaptation in Legal and Medical Domains\n\n---\n### 4.1 Multilingual Adaptation in Legal and Medical Domains  \n\nContinual learning (CL) has become indispensable for large language models (LLMs) operating in multilingual legal and healthcare contexts, where domain-specific expertise must be preserved across language boundaries. These high-stakes domains require models to maintain accuracy while adapting to diverse linguistic and regulatory landscapes—a challenge that CL addresses by mitigating catastrophic forgetting during cross-lingual adaptation. This subsection explores CL’s role in enabling multilingual proficiency for legal and medical LLMs, supported by case studies, evaluation frameworks, and ethical considerations.  \n\n#### Challenges in Multilingual Adaptation  \nLegal and medical workflows often involve documents and records in multiple languages, yet static LLMs struggle with low-resource languages or evolving terminologies. Catastrophic forgetting further complicates adaptation, as fine-tuning for new languages may degrade performance in previously learned ones. [4] underscores the difficulty of preserving discrete, compositional language structures during cross-lingual shifts, while [15] highlights the need for memory- and compute-efficient CL systems—a critical requirement for real-world legal and medical deployments.  \n\n#### Legal Domain: Adapting Across Languages and Specializations  \nThe legal field demands precision in jurisdiction-specific language and adaptability to legislative updates. [1] introduces \"vertical continual learning,\" where LLMs transition from general legal knowledge to specialized subdomains (e.g., contract law to criminal law) across languages. For instance, replay-based CL methods store critical legal precedents in memory buffers, enabling models to fine-tune for multilingual tasks like non-English judgment prediction without losing prior capabilities.  \n\nFurther advancing this approach, [195] demonstrates CL’s effectiveness in legal document classification across languages. By dynamically inferring task identities, the model adapts seamlessly to new languages without explicit task boundaries—a common scenario in legal practice. This method achieves a 15% improvement in cross-lingual accuracy over traditional fine-tuning, proving CL’s value in preserving legal semantics.  \n\n#### Medical Domain: Bridging Language Gaps in Healthcare  \nIn healthcare, LLMs must process multilingual patient records, literature, and diagnostic guidelines. [1] shows how CL combines domain-adaptive pre-training (DAP) with memory replay to retain rare medical terms in low-resource languages, reducing diagnostic errors by 20% in multilingual EHRs. Another study highlights CL’s role in medical summarization, where self-supervised learning and task-specific adapters enable models to generate summaries in Spanish and French without forgetting English proficiency. This aligns with [111], which finds that unsupervised CL techniques excel at handling language shifts in medical text due to their reliance on invariant features.  \n\n#### Evaluation Frameworks for Multilingual CL  \nRobust benchmarks are essential to assess multilingual CL systems. [17] measures forward transfer (leveraging prior languages for new tasks) and backward transfer (retaining old languages) in legal and medical instruction tuning. Models like [12] outperform static fine-tuning by 12% in cross-lingual legal QA, thanks to shared attention mechanisms.  \n\nFor graph-structured medical data, [9] evaluates CL in multilingual patient networks. Dynamic architectures, such as Mixture-of-Experts (MoE) layers in [1], allocate language-specific sub-networks, improving diagnostic code prediction by 30% across five languages.  \n\n#### Ethical and Practical Barriers  \nMultilingual CL raises ethical concerns, such as bias propagation from dominant-language training data to low-resource contexts. [1] proposes fairness-aware replay buffers to balance language representation during retraining. Practical challenges include computational costs and data heterogeneity. [6] advocates for partial fine-tuning strategies, exemplified by [167], which prioritizes high-impact multilingual samples to minimize overhead.  \n\n#### Future Directions  \nHybrid CL frameworks combining multilingual and multimodal learning—as suggested in [1]—could enhance applications like multilingual medical imaging analysis. [196] also calls for lightweight CL algorithms for edge devices in global healthcare, where resources are limited.  \n\nIn summary, CL enables LLMs to navigate multilingual legal and medical landscapes by balancing adaptation and retention. Case studies and benchmarks demonstrate its viability, while ethical and computational challenges guide future research toward fairness-aware and efficient CL methods.  \n---\n\n### 4.2 Domain-Specific Applications in Healthcare\n\n---\n### 4.2 Domain-Specific Applications in Healthcare  \n\nContinual learning (CL) has become a cornerstone for large language models (LLMs) in healthcare, bridging the gap between static models and the dynamic nature of medical knowledge. As highlighted in Section 4.1, where CL enables multilingual adaptation in high-stakes domains, healthcare similarly demands models that can evolve with new clinical guidelines, research findings, and patient data while maintaining precision and compliance. This subsection examines how CL empowers healthcare LLMs across clinical diagnosis, medical documentation, and personalized care, while addressing challenges like catastrophic forgetting and ethical risks—themes that resonate with the legal domain adaptations discussed in Section 4.3.  \n\n#### Clinical Diagnosis and Decision Support  \nThe ability to integrate real-time medical updates makes CL indispensable for diagnostic LLMs. Unlike static models that risk obsolescence, CL frameworks like those in [118] preserve long-term patient histories while adapting to new protocols, reducing diagnostic errors by 18% in longitudinal studies. This mirrors the \"vertical continual learning\" approach in legal LLMs (Section 4.1), where specialization deepens without forgetting foundational knowledge.  \n\nMultimodal CL further enhances diagnostic precision. For instance, [197] demonstrates how LLMs dynamically correlate radiology reports with emerging imaging techniques—paralleling the cross-jurisdictional adaptation challenges in legal AI (Section 4.3). However, distribution shifts in patient demographics remain a hurdle, necessitating human-AI collaboration to refine outputs, as noted in [198].  \n\n#### Medical Summarization and Documentation  \nAutomating clinical documentation is another breakthrough. CL allows LLMs to adapt to hospital-specific formats like SOAP notes while minimizing hallucinations—a critical concern in legal contexts (Section 4.3). [120] shows how replay-based CL preserves documentation styles, while [82] introduces self-correcting mechanisms that cross-reference authoritative sources, reducing factual errors by 22%.  \n\n#### Patient Consultations and Personalized Care  \nPersonalization in healthcare LLMs, exemplified by MedAgents in [113], leverages CL to retain patient histories across interactions—akin to the ethical bounded personalization discussed in [29]. Privacy concerns are mitigated through federated CL, as proposed in [199], echoing the decentralized solutions explored for legal data (Section 4.3).  \n\n#### Challenges and Future Directions  \nKey challenges include computational costs, addressed by parameter-efficient techniques like those in [97], and dataset curation, emphasized in [115]. Future work must prioritize:  \n1. **Interdisciplinary Benchmarks**: Developing healthcare-specific CL metrics, similar to legal domain evaluations (Section 4.3).  \n2. **Bias Mitigation**: Extending fairness-aware methods from [200] to incremental updates.  \n3. **Regulatory Alignment**: Ensuring CL adaptations comply with evolving healthcare standards, a challenge also faced in legal AI deployments.  \n\nIn summary, CL transforms healthcare LLMs into adaptive, precise tools, but their success depends on solving technical and ethical challenges—lessons that resonate across high-stakes domains like law (Section 4.3) and multilingual applications (Section 4.1).  \n---\n\n### 4.3 Legal Domain Adaptation and Ethical Considerations\n\n---\n### 4.3 Legal Domain Adaptation and Ethical Considerations  \n\nContinual learning (CL) in legal applications presents unique challenges due to the dynamic nature of legal systems, the need for precise domain adaptation, and the ethical implications of deploying AI in high-stakes decision-making. Legal tasks such as judgment prediction, legal advice generation, and document analysis require models to adapt to evolving statutes, precedents, and jurisdictional nuances while retaining prior knowledge. This subsection explores CL methodologies tailored for legal domains, their performance benchmarks, and the ethical dilemmas they introduce, bridging insights from healthcare CL (Section 4.2) and foreshadowing personalized recommendation systems (Section 4.4).  \n\n#### Legal Domain Adaptation Challenges  \nLegal texts are characterized by their complexity, ambiguity, and domain-specific terminology, making adaptation particularly challenging for CL systems. Unlike static models that require full retraining for new legal rulings or legislative changes, CL-enabled large language models (LLMs) can incrementally update their knowledge. However, this risks catastrophic forgetting—overwriting prior legal reasoning patterns when integrating new case law. Replay-based methods and parameter-efficient fine-tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), mitigate this by selectively updating model parameters while preserving foundational legal knowledge [121].  \n\nJurisdictional heterogeneity further complicates adaptation, mirroring challenges in healthcare CL (Section 4.2). Models trained on one jurisdiction’s data often fail to generalize to others due to domain shifts. Techniques like elastic weight consolidation (EWC) and gradient-based replay stabilize performance across legal systems, as demonstrated in [40]. Hybrid approaches combining CL with retrieval-augmented generation (RAG) dynamically incorporate jurisdiction-specific references, addressing data uniformity gaps.  \n\n#### Benchmarks and Evaluation  \nStandardized benchmarks are critical for evaluating CL in legal tasks, simulating scenarios like multi-task learning (e.g., contract analysis followed by judgment prediction) and cross-jurisdictional adaptation. Metrics such as task retention (accuracy on past legal queries) and forward transfer (improved performance on new tasks due to prior learning) are essential, though [201] critiques their inability to account for task difficulty. Adjusted measures like \"stability gap\" better quantify forgetting in legal contexts where task complexity varies widely.  \n\nEmpirical studies reveal uneven CL performance across legal subdomains. For instance, [31] categorizes legal tasks into \"vertical\" (e.g., deepening criminal law expertise) and \"horizontal\" (e.g., expanding to civil law) CL scenarios. Architectural adaptations like dynamic sparse networks excel in vertical settings, while replay-based methods suit horizontal expansion. However, biased datasets—such as those overrepresenting certain demographics—can propagate spurious correlations during incremental updates, as noted in [41], leading to unfair outcomes.  \n\n#### Ethical Considerations  \nThe deployment of CL systems in legal workflows raises profound ethical questions, echoing concerns in healthcare (Section 4.2) and anticipating challenges in personalized recommendations (Section 4.4). First, the \"black-box\" nature of many CL models complicates accountability, especially when generating legal advice or predicting case outcomes. [43] highlights how deeper neural network layers disproportionately contribute to forgetting, making long-term decision audits difficult.  \n\nSecond, bias amplification is a persistent risk. [41] shows that CL models trained on biased task sequences (e.g., racially skewed sentencing data) can forward-transfer biases to new tasks. While techniques like [202] propose debiasing during incremental updates, their legal efficacy remains underexplored.  \n\nThird, privacy concerns arise when processing sensitive legal data. [179] explores federated CL as a solution, enabling localized updates without centralized data storage. Yet, legal workflows often require cross-referencing global precedents, creating tension between privacy and performance—a challenge also relevant to personalized recommendations (Section 4.4).  \n\n#### Case Studies and Future Directions  \nPractical CL implementations in legal settings show promise. Hybrid systems combining LoRA and knowledge distillation for legal advice generation achieve high retention on historical queries post-update. Meanwhile, [42]’s two-stage knowledge distillation approach inspires frameworks for legal document summarization.  \n\nFuture research must address:  \n1. **Fairness Metrics**: Developing CL-specific fairness metrics for legal AI, as suggested by [203].  \n2. **Human-in-the-Loop Validation**: Integrating clinician-like corrections (Section 4.2) to mitigate model drift, akin to [173].  \n3. **Cross-Jurisdictional Benchmarks**: Standardizing evaluation, paralleling efforts in healthcare CL.  \n\nIn conclusion, CL holds transformative potential for legal AI but requires careful navigation of technical and ethical trade-offs, mirroring challenges in adjacent domains. Interdisciplinary collaboration will be key to advancing robust, fair, and adaptable legal systems.  \n---\n\n### 4.4 Personalized Recommendation Systems\n\n---\n### 4.4 Personalized Recommendation Systems  \n\nContinual learning (CL) in personalized recommendation systems represents a critical application of large language models (LLMs), enabling dynamic adaptation to evolving user preferences while maintaining performance on historical data. Building on the ethical and domain-specific challenges of legal CL (Section 4.3), this subsection explores how CL techniques address the unique requirements of recommendation systems—balancing personalization, privacy, and scalability. The discussion transitions naturally into multimodal CL (Section 4.5) by highlighting the need for integrating diverse data modalities in user modeling.  \n\n#### User Behavior Modeling with CL in LLMs  \n\nThe temporal nature of user behavior poses significant challenges for recommendation systems, including concept drift and cold-start problems. CL-equipped LLMs address these by incrementally updating user representations without catastrophic forgetting. For instance, [50] demonstrates how zero-shot outputs can refine user-specific knowledge—a principle applicable to recommendation tasks. Memory replay techniques, such as Experience Replay (ER), store past interactions to mitigate forgetting [1], while regularization methods like elastic weight consolidation (EWC) preserve critical preference patterns. Hybrid knowledge transfer, as proposed in [53], further enhances personalization by distilling insights from domain-specific models trained on private user data.  \n\n#### Hybrid Approaches: Combining Domain-Specific Models with LLMs  \n\nHybrid architectures bridge the gap between generalization and specialization in recommendation systems. [47] introduces a framework where domain-specific models (e.g., for e-commerce attributes) complement LLMs through joint Bayesian optimization. Similarly, CrossLM [53] enables bidirectional knowledge transfer between global LLMs and localized user behavior models. These approaches mirror the multimodal integration challenges discussed in Section 4.5, where heterogeneous data streams must be unified for coherent decision-making.  \n\n#### Challenges and Mitigation Strategies  \n\nKey challenges include:  \n1. **Data Heterogeneity**: User interactions span diverse domains with non-stationary distributions. Vertical federated learning (VFL) addresses this by partitioning features across clients [55], though privacy risks like input reconstruction attacks persist [183]. Mitigations include differential privacy and secure multi-party computation (SMPC).  \n2. **Computational Efficiency**: Parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaptation (LoRA) reduce overhead by updating only low-rank matrices [1]. Dynamic architectures such as Mixture-of-Experts (MoE) further optimize resource usage [129].  \n\n#### Case Studies and Applications  \n\nReal-world implementations highlight CL’s impact:  \n- **E-commerce**: [58] augments LLMs with knowledge graphs to improve item relevance.  \n- **Healthcare**: [59] uses multi-agent LLMs to generate adaptive patient recommendations, aligning with the longitudinal adaptation needs discussed in Section 4.5.  \n\n#### Future Directions  \n\nResearch priorities include:  \n1. **Lifelong Learning Frameworks**: Enabling LLMs to adapt over extended user interaction cycles.  \n2. **Ethical Alignment**: Preventing bias amplification in recommendations.  \n3. **Multimodal Integration**: Incorporating text, image, and audio data for richer user modeling [134], foreshadowing the multimodal CL challenges in Section 4.5.  \n\nIn summary, CL techniques empower recommendation systems to balance dynamic adaptation with historical performance. Hybrid approaches and efficient fine-tuning methods offer scalable solutions, while emerging challenges in privacy and multimodal integration set the stage for advancements explored in subsequent sections.  \n---\n\n### 4.5 Multimodal Continual Learning\n\n### 4.5 Multimodal Continual Learning  \n\nMultimodal continual learning (CL) represents a critical frontier in the evolution of large language models (LLMs), enabling them to process and integrate diverse data modalities such as text, images, electronic health records (EHRs), and structured legal documents. This capability is particularly vital in high-stakes domains like healthcare and legal systems, where multimodal data fusion enhances decision-making, accuracy, and adaptability. Building on the challenges of personalized recommendation systems (Section 4.4), multimodal CL introduces additional complexities, including catastrophic forgetting, modality alignment, and scalable retrieval-augmented frameworks. This subsection explores the applications, methodologies, and case studies of multimodal CL in healthcare and legal domains, supported by recent research, while setting the stage for the high-risk domain challenges discussed in Section 4.6.  \n\n#### Challenges and Opportunities in Multimodal CL  \nMultimodal CL extends traditional CL paradigms by requiring models to retain and incrementally update knowledge across heterogeneous data streams. For instance, in healthcare, LLMs must process EHRs, medical imaging, and clinical notes while adapting to new diagnostic protocols or emerging diseases—similar to how personalized recommendation systems adapt to evolving user preferences. Similarly, legal applications demand the integration of semi-structured data (e.g., contracts, case law) with unstructured text to support dynamic retrieval and reasoning. The primary challenges include:  \n1. **Modality Heterogeneity**: Aligning textual, visual, and tabular data into a unified representation without performance degradation [204].  \n2. **Catastrophic Forgetting**: Preserving proficiency in older modalities (e.g., radiology reports) while learning new ones (e.g., genomic data) [63].  \n3. **Retrieval-Augmented Frameworks**: Ensuring efficient cross-modal retrieval to support real-time decision-making [205].  \n\n#### Healthcare Applications  \nIn healthcare, multimodal CL enables LLMs to synthesize patient histories, imaging data, and lab results into cohesive diagnostic tools. For example, retrieval-augmented generation (RAG) frameworks have been employed to link EHRs with medical literature, reducing hallucinations and improving diagnostic accuracy [206]. This mirrors the hybrid approaches in recommendation systems (Section 4.4), where domain-specific knowledge is integrated with LLMs. The integration of imaging data, such as X-rays or MRIs, with textual reports further exemplifies multimodal CL’s potential. Studies like [138] demonstrate how LLMs fine-tuned on multimodal clinical datasets achieve superior performance in tasks like tumor classification or treatment recommendation.  \n\nA notable case is the application of CL in longitudinal patient care, where models must adapt to evolving patient conditions without retraining from scratch. For instance, [140] highlights how incremental updates to LLMs using EHR streams improve predictive accuracy for chronic disease management. However, ethical concerns, such as bias propagation from imbalanced datasets, persist [68], foreshadowing the high-risk challenges discussed in Section 4.6.  \n\n#### Legal Domain Applications  \nLegal systems increasingly rely on LLMs to process semi-structured data (e.g., contracts, court rulings) alongside unstructured text, paralleling the need for dynamic adaptation in recommendation systems. Multimodal CL here involves parsing tables, clauses, and citations while adapting to new jurisdictions or legislative updates. The [136] benchmark reveals that while LLMs struggle with raw legal text retrieval, retrieval-augmented frameworks significantly enhance performance. For example, [207] demonstrates how CL-equipped LLMs dynamically index legal precedents and statutes to support judicial decision-making.  \n\nA key innovation is the use of hybrid architectures, such as Mixture-of-Experts (MoE), to partition legal knowledge by modality (e.g., text vs. structured data) [137]. This approach mitigates forgetting while enabling specialization, similar to the parameter-efficient techniques in personalized recommendations. However, challenges like confidentiality breaches and reasoning gaps in high-risk scenarios persist, as noted in [208], directly linking to the ethical and safety concerns in Section 4.6.  \n\n#### Retrieval-Augmented Frameworks  \nRetrieval-augmented frameworks are pivotal for multimodal CL, as they decouple memory storage from model parameters, allowing dynamic updates. For instance, [64] introduces a system where LLMs retrieve relevant medical images or legal clauses during inference, reducing the burden of memorization. Similarly, [209] showcases how multimodal RAG systems improve accuracy in cross-modal tasks like patent analysis or compliance checking. These frameworks echo the bidirectional knowledge transfer in recommendation systems (Section 4.4), where localized data refines global models.  \n\n#### Case Studies and Empirical Insights  \n1. **Healthcare**: [135] illustrates how CL-enabled LLMs integrate molecular structures (SMILES notations) with textual research papers to predict drug interactions incrementally. The model’s performance degrades by only 12% after 10 updates, compared to 40% in non-CL baselines.  \n2. **Legal**: [210] evaluates LLMs in legal document summarization, showing that CL models with RAG achieve 78% retention in accuracy over six months of legislative updates, versus 50% for static models.  \n\n#### Future Directions  \n1. **Cross-Modal Alignment**: Techniques like contrastive learning could bridge gaps between modalities [211].  \n2. **Dynamic Architecture Adaptation**: Expanding MoE or LoRA variants to handle multimodal streams [138].  \n3. **Ethical Safeguards**: Addressing bias and privacy risks through federated CL frameworks [208], aligning with the mitigation strategies for high-risk domains in Section 4.6.  \n\nIn conclusion, multimodal CL in LLMs represents a transformative approach for healthcare and legal domains, but its success hinges on overcoming heterogeneity, forgetting, and retrieval efficiency challenges. The integration of retrieval-augmented systems and dynamic architectures offers promising pathways, as evidenced by recent benchmarks and case studies, while bridging the themes of adaptation and risk mitigation explored in adjacent sections.\n\n### 4.6 Case Studies on High-Risk Domain Challenges\n\n### 4.6 Case Studies on High-Risk Domain Challenges  \n\nThe deployment of large language models (LLMs) in high-stakes domains such as healthcare and legal systems presents unique challenges that necessitate robust continual learning (CL) solutions. Building on the multimodal CL challenges discussed in Section 4.5—particularly catastrophic forgetting and modality alignment—this subsection examines how LLMs face critical limitations like hallucinations, biases, and confidentiality breaches in high-risk scenarios. Through empirical case studies, we analyze these challenges and demonstrate how CL methodologies can mitigate risks while enabling dynamic adaptation.  \n\n#### **Hallucinations in Medical Self-Diagnosis**  \nA persistent issue in healthcare applications is LLMs' tendency to generate factually incorrect or misleading information, known as \"hallucinations.\" Studies such as [79] reveal that LLMs like ChatGPT often provide inaccurate medical advice, posing risks for self-diagnosis without professional oversight. This challenge mirrors the modality alignment difficulties in multimodal CL (Section 4.5), where heterogeneous data streams require careful integration.  \n\nFurther exacerbating this issue, [144] demonstrates that LLMs like Med-PaLM 2 exhibit biases in medical QA tasks, disproportionately misdiagnosing conditions for marginalized groups due to imbalanced training data. These findings align with the ethical concerns raised in multimodal healthcare applications (Section 4.5), underscoring the need for CL techniques such as dynamic memory replay and domain-adaptive fine-tuning. By iteratively correcting errors and adapting to new clinical guidelines, CL can reduce hallucination rates while preserving accuracy across diverse patient demographics.  \n\n#### **Legal Confidentiality and Ethical Gaps**  \nIn the legal domain, LLMs struggle with safeguarding sensitive information and maintaining ethical standards, paralleling the retrieval-augmented challenges in multimodal CL (Section 4.5). For instance, [28] critiques LLMs for generating unreliable legal advice and breaching client confidentiality, often misinterpreting jurisdictional nuances. Similarly, [145] highlights cases where LLMs inadvertently disclose privileged information during inference.  \n\nTo address these gaps, CL frameworks incorporating regularization and knowledge distillation have shown promise. [82] introduces a self-correction mechanism that dynamically prunes \"bias neurons\" responsible for confidentiality violations. This approach aligns with CL's stability-plasticity trade-off, ensuring models adapt without compromising ethical constraints—a theme further explored in emerging niche applications (Section 4.7).  \n\n#### **Bias Amplification in High-Stakes Decision-Making**  \nLLMs deployed in judicial or hiring systems often perpetuate societal biases, a challenge that extends beyond the multimodal alignment issues discussed in Section 4.5. For example, [73] found that LLMs disproportionately associate certain demographics with negative stereotypes, potentially influencing parole or employment decisions. [68] quantifies these biases across 12 demographic axes, showing that adversarial debiasing alone is insufficient without continual feedback loops.  \n\nCL offers solutions through hybrid adaptation frameworks. [92] demonstrates how iterative pruning of biased neurons combined with replay-based learning can reduce discriminatory outputs. Similarly, [75] proposes equity-aware aggregation to ensure minority group representation—a strategy that bridges the gap between high-risk challenges and niche domain adaptations (Section 4.7).  \n\n#### **Case Studies and Mitigation Strategies**  \n1. **Clinical Workflows**: [76] evaluated eight LLMs using clinical vignettes, revealing diagnostic disparities across racial and gender lines. The study advocates for CL-driven \"reflection\" techniques, where models iteratively refine outputs based on human feedback—a method that complements the retrieval-augmented frameworks in Section 4.5.  \n2. **Legal Advisory Systems**: [28] examines LLM-based legal tools, highlighting tensions between accessibility and reliability. By fine-tuning on region-specific corpora and integrating real-time feedback, CL can enhance jurisdictional precision, foreshadowing the knowledge injection techniques discussed in Section 4.7.  \n\nTo operationalize these insights, recent CL methodologies include:  \n- **Dynamic Architecture Adaptation**: [24] enables domain-specific expert routing for high-risk queries.  \n- **Ethical Auditing**: [81] proposes layered audits to monitor biases during continual updates.  \n- **Self-Supervised CL**: [89] uses token-level confidence scores to filter unethical outputs.  \n\n#### **Future Directions**  \nScaling CL for real-time high-risk applications remains a challenge. [85] calls for interdisciplinary collaboration to address data heterogeneity, while [212] emphasizes human-in-the-loop feedback. Key priorities include:  \n- **Real-Time Adaptation**: Federated CL frameworks like [24] for secure, low-latency updates.  \n- **Cross-Domain Knowledge Transfer**: Leveraging techniques from niche domains (Section 4.7) to enhance generalization.  \n\nIn conclusion, high-risk domains demand CL solutions that balance adaptation with accountability. By building on multimodal CL challenges (Section 4.5) and bridging to niche applications (Section 4.7), this subsection highlights CL's role in ensuring LLMs operate safely, equitably, and reliably in critical settings.\n\n### 4.7 Emerging Applications in Niche Domains\n\n---\n### 4.7 Emerging Applications in Niche Domains  \n\nContinual learning (CL) in large language models (LLMs) is proving transformative for specialized domains where domain-specific knowledge adaptation is critical. These applications demand models to handle unique terminologies, evolving regulations, and highly contextualized tasks, making CL indispensable for maintaining accuracy and relevance. Building on the high-risk domain challenges discussed in Section 4.6, this subsection explores how CL enables LLMs to address niche requirements in transportation, insurance QA, healthcare, and industrial applications, while highlighting knowledge injection techniques like Unified Medical Language System (UMLS) and enterprise knowledge graphs.  \n\n#### **Transportation and Logistics**  \nThe transportation sector leverages CL-enabled LLMs for dynamic tasks such as route optimization, demand forecasting, and autonomous vehicle decision-making. Unlike static models, CL allows LLMs to adapt to real-time traffic data, regional logistics patterns, and evolving safety protocols. For instance, [213] demonstrates how LLMs can process structured behavior graphs—a technique extendable to transportation networks for congestion prediction or delivery optimization. However, the safety-critical nature of this domain necessitates rigorous output validation to prevent hallucinations, as highlighted in [214], which analyzes numeric hallucination risks in LLMs.  \n\n#### **Insurance and Legal QA**  \nIn insurance, CL-equipped LLMs automate claim processing, policy recommendations, and fraud detection while adapting to regulatory changes. The precision and interpretability requirements of this domain align with findings from [99], which emphasizes LLMs’ ability to parse complex contractual language. To enhance output reliability, [215] proposes decomposing generation into content creation and structuring—a method applicable to insurance QA where ambiguous outputs could have financial repercussions.  \n\n#### **Healthcare and Biomedical Specializations**  \nCL addresses critical gaps in niche medical applications, such as rare disease diagnosis and personalized treatment planning. [153] reveals performance variability across demographics, underscoring the need for localized CL fine-tuning. Knowledge injection via UMLS and biomedical knowledge graphs, as explored in [216], standardizes terminologies and improves interoperability—key for domains like oncology or genomics where precision is paramount.  \n\n#### **Enterprise and Industrial Applications**  \nIndustrial sectors deploy CL-enhanced LLMs for predictive maintenance, supply chain optimization, and technical documentation. Enterprise knowledge graphs, discussed in [217], enable continuous integration of domain-specific jargon and protocols. This aligns with efficiency considerations from [98], particularly relevant for resource-constrained industrial settings.  \n\n#### **Lessons from Domain-Specific Knowledge Injection**  \nEffective CL in niche domains relies on structured knowledge integration:  \n1. **UMLS and Biomedical Knowledge Graphs**: Standardize medical terminologies, as demonstrated in [216].  \n2. **Enterprise Knowledge Graphs**: Capture organizational workflows, enabling LLMs to reflect updated policies, per [217].  \n3. **Dynamic Benchmarking**: Tailored evaluation metrics, as proposed in [102], address the lack of standardized benchmarks in specialized fields.  \n\n#### **Challenges and Future Directions**  \nKey hurdles include data scarcity and overfitting to domain-specific patterns. [96] offers data curation strategies, while [98] advocates hybrid CL-transfer learning approaches. Future work should prioritize:  \n- **Cross-Domain Knowledge Transfer**: Leveraging multi-agent collaboration frameworks like [218].  \n- **Human-in-the-Loop Validation**: Integrating expert feedback, as suggested in [219].  \n- **Scalable Knowledge Injection**: Lightweight methods to reduce computational overhead.  \n\nIn conclusion, CL empowers LLMs to tackle niche domain challenges through adaptive knowledge integration. By bridging insights from high-risk applications (Section 4.6) and advancing domain-specific techniques, continual learning demonstrates its versatility in enabling precise, context-aware AI solutions. Future progress hinges on interdisciplinary collaboration and tailored evaluation frameworks.  \n---\n\n## 5 Evaluation Metrics and Benchmarks\n\n### 5.1 Key Metrics for Continual Learning Performance\n\n### 5.1 Key Metrics for Continual Learning Performance  \n\nContinual learning (CL) in large language models (LLMs) presents unique evaluation challenges, as models must simultaneously retain past knowledge while adapting to new information. This subsection systematically examines the key metrics for assessing CL performance, organized into four core dimensions: retention and forgetting, knowledge transfer, domain robustness, and computational efficiency. These metrics provide a comprehensive framework for evaluating CL approaches across diverse scenarios.\n\n#### **Retention and Forgetting Metrics**  \nThe ability to retain previously learned knowledge while acquiring new skills is fundamental to CL. Catastrophic forgetting (CF) remains a primary challenge, where models lose proficiency on earlier tasks during sequential training. Key metrics include:  \n- **Average Accuracy (AA)**: The mean accuracy across all tasks after sequential training, providing an overall measure of retention.  \n- **Forgetting Measure (FM)**: Quantifies the performance drop for each task between its peak and post-update accuracy [5].  \n- **Retention Rate (RR)**: The ratio of final to initial accuracy per task, aggregated across tasks.  \n\nRecent studies demonstrate that rehearsal-based methods, such as memory replay, effectively mitigate CF by preserving old task samples [8]. However, these approaches often introduce computational overhead, necessitating careful evaluation of their efficiency trade-offs.\n\n#### **Knowledge Transfer Metrics**  \nCL systems must leverage learned knowledge to improve performance on both new and previous tasks:  \n- **Forward Transfer (FWT)**: Measures how prior knowledge accelerates learning of new tasks. For instance, LLMs pre-trained on general language tasks often show improved adaptation to specialized domains like legal or medical text analysis [15].  \n- **Backward Transfer (BWT)**: Evaluates the impact of new learning on previous tasks, where positive values indicate synergistic knowledge integration [16].  \n\nParameter-efficient fine-tuning (PEFT) methods, such as LoRA, have shown superior transfer capabilities compared to full-model retraining [7], though their effectiveness depends on task similarity and adaptation granularity.\n\n#### **Domain Robustness Metrics**  \nCL models must maintain performance across shifting data distributions:  \n- **Domain Adaptation Gap (DAG)**: The performance difference between in-domain and out-of-domain evaluations, highlighting challenges in scenarios like transitioning from medical to legal text [1].  \n- **Generalization Error (GE)**: Measures performance drops on unseen data from the same domain, revealing overfitting tendencies [220].  \n\nBenchmarks like TRACE and EvolvingQA explicitly evaluate domain shift handling by curating task sequences with varied distributions [1]. Dynamic architectural approaches, such as MoLA, address these challenges by isolating domain-specific parameters [1].\n\n#### **Computational Efficiency Metrics**  \nGiven practical resource constraints, efficiency is critical for scalable CL:  \n- **Training Time per Task**: Wall-clock time required for model updates, with replay-based methods incurring additional overhead from memory operations [18].  \n- **Parameter Efficiency**: Ratio of trainable to total parameters, where PEFT methods like adapters or LoRA excel by freezing most model weights [7].  \n- **Memory Footprint**: Storage requirements for replay buffers or task-specific modules, optimized by techniques like gradient coreset selection (GCR) [167].  \n\nEmerging solutions like S-LoRA enhance scalability through parallel adaptation of low-rank modules [1], though efficiency-performance trade-offs remain crucial for edge deployment.\n\n#### **Emerging and Composite Metrics**  \nRecent advances introduce metrics addressing broader CL challenges:  \n- **Task Similarity Index (TSI)**: Quantifies inter-task overlap to predict transfer potential and reduce redundant training [221].  \n- **Calibration Error (CE)**: Assesses alignment between model confidence and accuracy, particularly important for detecting outdated knowledge [222].  \n- **Lifelong Generalization Score (LGS)**: Unifies retention, transfer, and robustness into a single long-term performance metric [196].  \n\n#### **Open Challenges and Future Directions**  \nCurrent metrics face limitations in:  \n1. **Temporal Dynamics**: Most assume discrete task boundaries, unlike real-world continuous data streams [11].  \n2. **Bias and Fairness**: Few metrics track ethical risks like bias accumulation across tasks [1].  \n3. **Human Interaction**: Interactive CL systems lack standardized metrics for user feedback integration [223].  \n\nFuture work should unify evaluation protocols, as seen in frameworks like Sequoia, which standardize CL settings across domains [224].  \n\nIn summary, effective CL evaluation requires balancing retention, transfer, robustness, and efficiency metrics. While existing measures provide a foundation, adapting them to LLMs in dynamic, real-world environments remains an ongoing research frontier.\n\n### 5.2 Established Benchmarks for Continual Learning\n\n---\n### 5.2 Established Benchmarks for Continual Learning  \n\nBuilding upon the metrics framework established in Section 5.1, this subsection analyzes three prominent benchmarks—LongICLBench, EvolvingQA, and TRACE—that operationalize these evaluation principles for continual learning (CL) in large language models (LLMs). These benchmarks translate theoretical metrics into concrete testing scenarios, addressing critical challenges like catastrophic forgetting, domain adaptation, and computational efficiency while paving the way for domain-specific protocols discussed in Section 5.3.\n\n#### LongICLBench: Evaluating Long-Context Retention  \nDesigned to assess retention metrics (Section 5.1), LongICLBench tests LLMs' ability to maintain coherence across extended sequences—a capability crucial for applications like legal analysis or medical documentation. The benchmark employs multi-turn tasks (e.g., progressive summarization) where models must integrate new information while preserving earlier context [28]. Its dynamic task injection mechanism aligns with horizontal CL scenarios, evaluating cross-domain shifts through metrics like Domain Adaptation Gap (DAG). However, synthetic data limitations reduce its applicability to real-world multilingual settings, highlighting a gap addressed by emerging benchmarks in Section 5.3.\n\n#### EvolvingQA: Dynamic Question-Answering for Lifelong Learning  \nEvolvingQA implements knowledge transfer metrics (FWT/BWT) through incrementally updated QA tasks that mirror real-world knowledge evolution (e.g., integrating latest scientific findings) [225]. The benchmark's granular evaluation separates \"old\" versus \"new\" question accuracy, directly measuring forgetting (FM) and adaptation—critical for vertical CL scenarios where models specialize progressively. While its text-centric design limits multimodal evaluation, proposed extensions with retrieval-augmented generation (RAG) components could bridge this gap [172].\n\n#### TRACE: Task Retention and Catastrophic Forgetting Evaluation  \nTRACE operationalizes retention and computational efficiency metrics through controlled task-sequence experiments. Its modular design evaluates parameter-efficient methods (e.g., LoRA's rank stabilization) [22], providing empirical validation for techniques discussed in Section 5.1's efficiency analysis. Studies using TRACE demonstrate that Low-Rank Adaptation variants like DoRA reduce forgetting by 30% compared to full fine-tuning [117]. However, its assumption of discrete task boundaries contrasts with real-world gradual shifts—a limitation addressed by dynamic evaluation paradigms in Section 5.4.\n\n#### Comparative Analysis and Benchmark Evolution  \nCurrent benchmarks exhibit complementary strengths:  \n- **LongICLBench** excels in retention metrics but lacks multimodal support  \n- **EvolvingQA** optimizes transfer metrics but is text-limited  \n- **TRACE** provides granular forgetting analysis but assumes abrupt task transitions  \n\nEmerging solutions like CodeTask-CL attempt synthesis by evaluating API adaptation in code generation [226], while self-evolving benchmarks propose dynamic difficulty adjustment [30]. These innovations anticipate the adaptive benchmarking principles explored in Section 5.4, suggesting future benchmarks should integrate:  \n1. **Multimodal evaluation** (extending LongICLBench's retention focus)  \n2. **Continuous shift simulation** (bridging TRACE's granularity with real-world fluidity)  \n3. **Decentralized validation** through federated protocols [199]  \n\nThis evolution will better align benchmark design with the composite metrics (LGS, TSI) introduced in Section 5.1, ultimately supporting the domain-specific evaluation needs outlined in Section 5.3.\n\n### 5.3 Task-Specific Evaluation Protocols\n\n### 5.3 Task-Specific Evaluation Protocols  \n\nContinual learning (CL) in large language models (LLMs) demands specialized evaluation frameworks tailored to distinct application domains, as generic benchmarks often fail to capture the nuanced challenges of real-world deployment. Building on the foundation of established benchmarks like LongICLBench and TRACE (discussed in Section 5.2), this subsection examines domain-specific protocols that address unique requirements in multilingual adaptation, healthcare, legal domains, and high-risk applications—while anticipating the dynamic benchmarking paradigms explored in Section 5.4.  \n\n#### Multilingual Adaptation  \nMultilingual CL evaluation focuses on mitigating catastrophic forgetting across languages, particularly in low-resource settings. Unlike static benchmarks, task-specific protocols must account for cross-lingual transfer and code-switching scenarios. Key innovations include:  \n1. **Cross-lingual Transfer Accuracy**: Measures how LLMs leverage high-resource language knowledge (e.g., English) to enhance low-resource language performance, addressing gaps identified in [32].  \n2. **Language-Specific Retention**: Quantifies forgetting during sequential fine-tuning, as observed in BLOOMZ and mT0 models, where performance drops up to 40% on prior languages [42].  \n3. **Code-Switching Robustness**: Evaluates adaptability to mixed-language inputs—a critical capability for real-world applications like multilingual customer support.  \n\n#### Healthcare Domain  \nHealthcare CL protocols must balance diagnostic accuracy with regulatory compliance, extending the temporal generalization challenges highlighted in EvolvingQA (Section 5.2) to clinical settings:  \n1. **Diagnostic Consistency**: Ensures models maintain tumor detection accuracy after learning new tasks like pathology classification, as demonstrated in [40].  \n2. **Temporal Generalization**: Tracks performance on longitudinal patient data, where frameworks like [122] use elastic weight consolidation (EWC) to preserve knowledge across hospital datasets.  \n3. **Ethical Compliance**: Measures bias propagation in sequential tasks, revealing that spurious correlations (e.g., demographic biases) exacerbate forgetting [41].  \n\n#### Legal Domain  \nLegal CL benchmarks build on TRACE’s forgetting metrics (Section 5.2) but introduce domain-specific requirements:  \n1. **Precedent Retention**: Evaluates recall of legal precedents after fine-tuning—critical for applications like [51].  \n2. **Confidentiality Leakage**: Quantifies unintended memorization of sensitive case details, a risk amplified by rehearsal-based methods [179].  \n3. **Cross-Jurisdictional Adaptation**: Assesses performance shifts between legal systems, where client drift worsens forgetting [174].  \n\n#### High-Risk Applications  \nFor autonomous systems and finance, protocols emphasize real-time robustness, bridging gaps between static benchmarks and dynamic evaluation (Section 5.4):  \n1. **Failure Mode Analysis**: Identifies critical forgetting scenarios (e.g., autonomous vehicles losing pedestrian detection capabilities).  \n2. **Latency-Aware Metrics**: Measures inference speed degradation, as attention bottlenecks reduce throughput by 30% after 10 tasks [175].  \n\n#### Emerging Trends and Challenges  \nRecent advances align with the adaptive benchmarking principles discussed in Section 5.4:  \n1. **Dynamic Difficulty Adjustment**: Benchmarks like [201] link forgetting to task dissimilarity rather than sequence length.  \n2. **Self-Evolving Protocols**: Frameworks such as [177] simulate real-world material discovery, balancing stability predictions with efficiency.  \n\nPersistent challenges include:  \n- **Metric Standardization**: Heterogeneous metrics (e.g., F1-score vs. CIDER_t) hinder cross-domain comparisons [42].  \n- **Bias Quantification**: Few protocols measure how forgetting exacerbates biases [41].  \n- **Deployment Gaps**: Simulated benchmarks overlook hardware constraints [227].  \n\nFuture directions should unify evaluation protocols, as proposed in [228], to bridge domain-specific and general-purpose CL benchmarks while preparing for dynamic and adaptive testing paradigms.\n\n### 5.4 Dynamic and Adaptive Benchmarking\n\n---\n### 5.4 Dynamic and Adaptive Benchmarking  \n\nBuilding upon the domain-specific evaluation protocols discussed in Section 5.3, this subsection examines advanced benchmarking paradigms that address the temporal and contextual dynamics of continual learning (CL) in large language models (LLMs). Traditional static benchmarks often fail to capture the evolving nature of real-world deployment scenarios, necessitating adaptive testing frameworks and dynamic benchmarks that can simulate shifting conditions—a critical foundation for addressing the methodological challenges explored in Section 5.5.  \n\n#### Adaptive Testing Frameworks for CL in LLMs  \nAdaptive testing frameworks, inspired by Computerized Adaptive Testing (CAT) principles, dynamically adjust evaluation sequences based on real-time model performance. These frameworks address key limitations of static benchmarks by:  \n1. **Task Difficulty Adaptation**: Tailoring evaluations to boundary tasks that probe the edges of a model's capabilities, as demonstrated in vertical continual learning (VCL) scenarios where models transition from general to domain-specific knowledge [129].  \n2. **Efficient Resource Utilization**: Prioritizing evaluations in low-resource languages or novel subdomains where LLMs exhibit inconsistent performance, as seen in multilingual adaptation studies [52].  \n3. **Hybrid Assessment Models**: Combining LLM confidence scores with task-specific metrics to refine adaptive sequencing, addressing the challenge of applying traditional item response theories (IRT) to probabilistic model outputs [57].  \n\n#### Dynamic Benchmarks for Real-Time Assessment  \nDynamic benchmarks simulate real-world distribution shifts through controlled perturbations, bridging the gap between laboratory evaluations and production environments:  \n1. **Self-Evolving Tasks**: Benchmarks like EvoEval progressively modify tasks based on model performance, mimicking non-stationary environments encountered in domains such as healthcare [229].  \n2. **Adversarial Evaluation**: Frameworks like RefuteBench generate counterfactual examples to test logical consistency, revealing vulnerabilities in legal and financial reasoning tasks [51].  \n3. **Multimodal Adaptation**: Extending benchmarks to incorporate emerging modalities (e.g., EHRs or imaging data) while preserving existing capabilities, as explored in multimodal CL studies [134].  \n\n#### Integration Challenges and Emerging Solutions  \nThe deployment of dynamic benchmarking faces three key challenges:  \n1. **Scalability**: Computational overhead of real-time task generation, addressed through distributed pipelines [55] and lightweight proxy metrics [230].  \n2. **Ethical Alignment**: Risk of bias amplification during dynamic task selection, mitigated by fairness-aware sampling [62] and human-in-the-loop validation [61].  \n3. **Interpretability**: Need for transparent failure modes, supported by structured prompting techniques [231] and causal analysis frameworks [232].  \n\n#### Future Directions  \nAdvancing dynamic benchmarking requires:  \n- **Unified Metrics**: Cross-benchmark generalization standards to compare adaptive and dynamic evaluations [130].  \n- **Lifelong Simulation**: Benchmarks that model recurring task variations to assess long-term knowledge retention.  \n- **Expert-Informed Design**: Integration of domain specialist feedback for high-stakes applications [48].  \n\nThis paradigm shift toward dynamic and adaptive benchmarking provides a critical link between controlled CL evaluation and real-world applicability, setting the stage for addressing the reproducibility and scalability challenges discussed in Section 5.5.\n\n### 5.5 Challenges in Evaluation Design\n\n---\n### 5.5 Methodological Challenges in Continual Learning Evaluation  \n\nThe evaluation of continual learning (CL) in large language models (LLMs) faces significant methodological challenges that undermine the reliability and reproducibility of benchmark results. These challenges span four key dimensions: data contamination, benchmark overfitting, scalability/reproducibility issues, and metric granularity, each requiring targeted solutions to ensure robust CL assessment.  \n\n#### Data Contamination and Temporal Validity  \nA critical issue is data contamination, where evaluation datasets inadvertently overlap with the pretraining corpora of LLMs, leading to inflated performance metrics. As demonstrated in [233], proprietary LLMs can guess missing options in test sets with high accuracy (e.g., 57% exact match rate in MMLU for GPT-4), suggesting prior exposure to benchmark content. This problem is exacerbated by the lack of transparency in pretraining data sources, particularly for closed-source models. Proposed solutions include:  \n- **Detection Protocols**: TS-Guessing methods that mask incorrect multiple-choice options or unlikely words to identify contamination patterns [233].  \n- **Temporal Separation**: Benchmarks like [64] that continuously collect fresh problems (e.g., from programming contests) to ensure temporal separation from pretraining data.  \n- **Longitudinal Analysis**: Techniques from [234] that statistically verify leakage through GitHub popularity trends and model cutoff dates.  \n\n#### Benchmark Overfitting and Dynamic Adaptation  \nStatic benchmarks often fail to capture real-world generalization due to overfitting, where models memorize patterns rather than develop robust reasoning skills. Key findings include:  \n- **Overfitting Gap**: [235] shows performance drops of 19.6–47.7% when evolving coding benchmarks into new domains, exposing the fragility of static evaluations.  \n- **Real-World Disconnect**: Studies like [140] reveal that GPT-4 solves only 33.82% of repository-level ML tasks despite strong performance on isolated coding challenges [140].  \n- **Human-Aligned Validation**: [236] demonstrates that static benchmark scores (e.g., HumanEval) poorly correlate with programmer productivity gains in practice.  \n\nEmerging solutions leverage dynamic frameworks such as [67], which uses multi-agent systems to continuously reframe test instances through context perturbation and adversarial questioning.  \n\n#### Scalability and Reproducibility Challenges  \nThe computational demands of CL evaluation introduce scalability bottlenecks, while inconsistent execution environments hinder reproducibility:  \n- **Hardware Variance**: [66] reveals orders-of-magnitude throughput differences across hardware for 7B–70B parameter models.  \n- **Standardization Gaps**: Initiatives like [237] address this through open graph schemas for workload specification.  \n- **Multilingual Biases**: GPT-4-based evaluators inflate ratings for Latin-script languages unless calibrated with native speaker judgments [238].  \n\nDomain-specific reproducibility frameworks are emerging, such as:  \n- [239]’s execution-based multilingual engine for 14 programming languages.  \n- [240]’s human-aligned protocols for Chinese LLMs across 400+ dimensions.  \n\n#### Granularity and Specialized Domain Evaluation  \nCurrent metrics often obscure critical variations by focusing on aggregate scores. Advanced frameworks address this through:  \n- **Multidimensional Ability Maps**: [204] evaluates structured data generation across six dimensions (e.g., pragmatics, reasoning), revealing GPT-4’s weaknesses in pragmatic constraints despite formatting strengths.  \n- **Fine-Grained Agent Diagnostics**: [71] decomposes performance into 12 progress indicators like tool usage efficiency and multi-turn coherence.  \n- **Ethical Multi-Axis Assessment**: [241] measures bias across 12 demographic dimensions and 5 toxicity metrics, uncovering inconsistent mitigation across protected groups.  \n\nSpecialized domains face unique challenges:  \n- Legal: GPT-4 struggles with zero-shot deposition line lookup despite general benchmark prowess [136].  \n- ML Engineering: LLMs fail to navigate repository dependencies in [138].  \n- Psychology: Performance varies up to 40% across subdomains in [242].  \n\n#### Emerging Solutions and Future Directions  \nInnovative approaches are bridging these gaps:  \n- **Efficient Sampling**: [243] shows 100-example subsets can estimate full benchmark performance with 95% correlation.  \n- **Peer-Review Mechanisms**: [244] achieves 90% human-judge agreement via LLM cross-evaluation.  \n- **Hybrid Human-AI Frameworks**: [245] combines LLM refinement with NLI verification, while [246] adapts psychometric principles for population-referenced scoring.  \n\nThese advancements collectively enable more robust, scalable, and reproducible CL evaluation methodologies that align with real-world deployment needs—a critical foundation for the dynamic benchmarking paradigms discussed in Section 5.4 and the emerging trends explored in subsequent sections.\n\n### 5.6 Emerging Trends in Benchmark Development\n\n---\n### 5.6 Emerging Trends in Continual Learning Benchmark Development  \n\nBuilding upon the methodological challenges outlined in Section 5.5, the field of continual learning (CL) for large language models (LLMs) is witnessing transformative shifts in benchmark design. These emerging trends directly address scalability, real-world alignment, and evaluation robustness—key themes that will inform the comparative analysis in Section 5.7. Three paradigm-shifting approaches are redefining CL assessment: self-evolving benchmarks, multimodal evaluation frameworks, and federated learning-based assessment.\n\n#### Self-Evolving Benchmarks  \nStatic benchmarks struggle to keep pace with the rapid evolution of LLM capabilities and real-world task requirements. Self-evolving frameworks like those in [67] introduce dynamic adaptation mechanisms that:  \n1. **Automatically generate test cases** based on identified model weaknesses, as demonstrated in [247]'s iterative red-teaming approach.  \n2. **Incorporate temporal validity checks** to prevent data obsolescence, particularly crucial for domains like healthcare where [79] shows LLMs must adapt to new medical knowledge.  \n\nChallenges include:  \n- Computational costs of continuous test generation  \n- Risk of adversarial feedback loops, necessitating ethical safeguards as discussed in [145].  \n\n#### Multimodal Evaluation  \nThe integration of LLMs with visual, auditory, and sensory modalities demands evaluation frameworks that transcend textual analysis. Key developments include:  \n- **Cross-modal retention metrics** assessing how CL affects performance when switching between modalities, aligning with [84]'s vision for edge AI.  \n- **Domain-specific multimodal benchmarks**, such as medical imaging paired with EHR analysis highlighted in [79].  \n\nCritical limitations involve:  \n- Semantic coherence evaluation across modalities, with [248] warning against superficial multimodal assessments.  \n- Standardization gaps in scoring heterogeneous data types.  \n\n#### Federated Learning-Based Assessment  \nDecentralized evaluation paradigms address privacy and distributional heterogeneity—two key concerns from Section 5.5's reproducibility discussion. FL-based benchmarks enable:  \n- **Privacy-preserving CL evaluation** for sensitive domains like healthcare [29].  \n- **Cross-institutional adaptability testing**, measuring how LLMs reconcile regional data variations as examined in [76].  \n\nImplementation challenges mirror those in federated training:  \n- Synchronization overhead mitigated by techniques in [24].  \n- Trust mechanisms explored in [93].  \n\n#### Convergent Innovations and Future Outlook  \nThe integration of these trends points toward next-generation benchmarks that:  \n1. Combine dynamic test generation with multimodal tasks and federated validation.  \n2. Adopt FAIR principles for reproducibility, as advocated in [249].  \n3. Incorporate security-aware evaluation protocols from [80].  \n\nThese advancements directly set the stage for Section 5.7's comparative analysis by:  \n- Providing the methodological foundation for granularity vs. scalability trade-off discussions.  \n- Informing real-world applicability assessments through multimodal and federated paradigms.  \n- Highlighting unresolved challenges that future benchmark standardization must address.  \n\nThe trajectory suggests a paradigm shift toward living benchmarks that evolve alongside LLMs—a necessity for meaningful CL evaluation as models permeate increasingly complex, multimodal, and privacy-sensitive domains.\n\n### 5.7 Comparative Analysis of Evaluation Approaches\n\n---\n### 5.7 Comparative Analysis of Evaluation Approaches  \n\nThe evaluation of continual learning (CL) in large language models (LLMs) presents a complex landscape where methodologies must balance competing priorities. Building on the emerging trends in benchmark development discussed earlier—such as self-evolving, multimodal, and federated approaches—this subsection provides a systematic comparison of evaluation frameworks, focusing on their trade-offs in granularity, scalability, and real-world applicability.  \n\n#### Granularity vs. Scalability Trade-offs  \nEvaluation frameworks for CL in LLMs often face a fundamental tension between granularity (depth of analysis) and scalability (breadth of application). Fine-grained benchmarks like [250] excel at measuring task retention and transfer learning through metrics such as catastrophic forgetting and plasticity-stability trade-offs. However, their reliance on domain-specific datasets, as seen in [152], limits their scalability across diverse applications. On the other hand, scalable benchmarks like [251] aggregate performance across broad task categories, sacrificing nuanced insights into model behavior.  \n\nDynamic benchmarks, such as [235], attempt to bridge this gap by adaptively testing LLMs, but they risk overfitting to superficial cues, as critiqued in [252]. Hybrid approaches, like the hierarchical evaluation in [253], offer a promising middle ground by combining entity-level analysis with scalable frameworks, albeit at higher computational costs.  \n\n#### Real-World Applicability  \nThe utility of CL evaluation hinges on its alignment with practical deployment scenarios. Domain-specific benchmarks, such as [153], provide valuable insights into LLM performance in high-stakes settings like healthcare. However, their narrow focus, as noted in [103], limits their generalizability to other domains.  \n\nUser-centric approaches, such as the user-reported scenarios (URS) in [72], ground evaluations in real-world needs but face scalability challenges due to manual curation. Similarly, [254] leverages survey data for realistic evaluation but is constrained by biases in self-reported responses. The tension between controlled lab evaluations and real-world variability is further explored in [98], which advocates for a balance between synthetic benchmarks and field studies.  \n\n#### Methodological Comparisons  \n1. **Task Retention vs. Computational Cost**: Metrics like accuracy drop (post-CL vs. initial performance) in [102] offer detailed insights but require extensive retesting. Scalable alternatives, such as parameter efficiency scores in [150], prioritize speed over depth, potentially overlooking task-specific forgetting.  \n2. **Forward/Backward Transfer**: Benchmarks like [255] measure knowledge transfer across tasks but are computationally intensive. [151] critiques their reliance on pre-defined task sequences, which may not reflect real-world incremental learning.  \n3. **Robustness to Distribution Shifts**: Evaluations in [153] emphasize domain-specific robustness but lack cross-domain validation. Legal-domain benchmarks, as proposed in [99], face similar limitations in broader applicability.  \n\n#### Emerging Trends and Challenges  \n1. **Self-Evolving Benchmarks**: While [67] introduces adaptive testing, concerns about data leakage, as raised in [256], remain unresolved.  \n2. **Multimodal Evaluation**: The need for multimodal benchmarks is highlighted in [154], but their development is still in early stages.  \n3. **Federated Evaluation**: Decentralized approaches, as explored in [218], show promise but require further scalability testing.  \n\n#### Recommendations for Future Work  \n1. **Hybrid Evaluation Frameworks**: Combine granular benchmarks (e.g., [219]) with scalable protocols (e.g., [95]) to achieve balanced assessments.  \n2. **Domain-Specific Validation**: Extend the approach of [257] to other high-stakes domains, such as law [99].  \n3. **Human-in-the-Loop Metrics**: Integrate user-centric frameworks like [72] with automated metrics to enhance real-world relevance.  \n\n#### Future Directions  \n1. **Standardization**: Disparate metrics, as noted in [94], hinder cross-study comparisons and must be addressed through unified frameworks.  \n2. **Bias Mitigation**: Inclusive evaluation practices, as advocated in [87], require actionable benchmarks to ensure fairness.  \n3. **Dynamic Environments**: Real-time evaluation, proposed in [258], presents implementation challenges but is critical for adapting to evolving CL demands.  \n\nIn summary, this comparative analysis underscores the need for CL evaluation frameworks that harmonize granularity, scalability, and real-world fidelity. Future efforts should prioritize interdisciplinary collaboration, as suggested in [105], to develop holistic and adaptable benchmarks for LLMs.  \n---\n\n## 6 Challenges and Limitations\n\n### 6.1 Computational and Resource Constraints\n\n---\n### 6.1 Computational and Resource Constraints  \n\nContinual learning (CL) for large language models (LLMs) presents significant computational and resource challenges that hinder both training and deployment. The massive scale of modern LLMs, often comprising billions of parameters, exacerbates these issues, as incremental updates demand substantial computational power, memory, and energy. This subsection examines the primary bottlenecks—computational costs, energy consumption, and financial barriers—and discusses mitigation strategies such as quantization, distributed training, and parameter-efficient methods.  \n\n#### Computational Costs  \nThe computational overhead of CL in LLMs arises from the need to process sequential tasks while retaining prior knowledge. Unlike static training, CL requires frequent model updates, involving backpropagation through the entire network for each new task. This process is highly resource-intensive, particularly for LLMs where even a single forward pass demands significant GPU/TPU resources [1]. Fine-tuning models like GPT-3 or LLaMA-2 on new tasks can consume thousands of GPU hours, making it impractical for real-world applications with limited budgets [12].  \n\nThe computational burden is further compounded by techniques to mitigate catastrophic forgetting. Replay-based methods, which store and retrain on past data, require additional forward and backward passes, increasing runtime [8]. Similarly, regularization-based approaches like elastic weight consolidation (EWC) introduce auxiliary loss terms that complicate optimization [107]. These methods often sacrifice computational efficiency for stability, creating a trade-off between performance and practicality.  \n\n#### Energy Consumption  \nThe energy demands of CL for LLMs are substantial. Training a single LLM from scratch can emit CO₂ equivalent to five cars over their lifetimes, and CL exacerbates this by requiring repeated training cycles [6]. Energy consumption scales linearly with model size and task frequency, making it unsustainable for long-term deployment. For instance, replay-based methods consume 2–3× more energy than static training due to redundant computations [2].  \n\nEnergy inefficiency is particularly problematic in edge devices or IoT systems, where battery life and thermal constraints limit computational capacity [3]. While lightweight architectures like adapters or LoRA (Low-Rank Adaptation) reduce energy use, they still require periodic full-model updates, undermining their efficiency [12].  \n\n#### Financial Barriers  \nThe financial cost of CL for LLMs is prohibitive for many organizations. Training and maintaining LLMs in production require expensive hardware (e.g., A100/H100 GPUs) and cloud infrastructure. For example, deploying a continually updated LLM like ChatGPT can cost millions annually in cloud fees alone [21]. Smaller entities, such as academic labs or startups, are often priced out of CL research due to these barriers.  \n\nAdditionally, data storage for replay buffers or episodic memory adds another layer of expense. Memory-augmented methods, such as those in [167], require high-speed storage (e.g., NVMe SSDs) to minimize latency during retrieval, further inflating costs.  \n\n#### Mitigation Strategies  \nTo address these constraints, researchers have proposed several strategies:  \n\n1. **Quantization**: Reducing the precision of model weights (e.g., from 32-bit to 8-bit) can cut memory usage and energy consumption by up to 75% without significant performance loss [259]. Quantization-aware training (QAT) techniques, such as those in [260], enable CL models to adapt to lower precision dynamically.  \n\n2. **Distributed Training**: Frameworks like Federated Learning (FL) distribute computational loads across devices, reducing the burden on any single node [7]. However, FL introduces communication overhead, which can negate gains if not optimized [261].  \n\n3. **Parameter-Efficient Methods**: Techniques like LoRA and adapters freeze most pretrained weights, updating only small task-specific modules. For example, [12] reduces trainable parameters by 90%, slashing computational costs.  \n\n4. **Dynamic Sparsity**: Methods like [108] selectively activate subsets of neurons per task, reducing FLOPs. Similarly, [13] shows that sparse networks can achieve competitive accuracy with 50% fewer computations.  \n\n5. **Replay Optimization**: Instead of storing raw data, [8] compresses past tasks into prototypes, cutting memory usage by 80%. [167] further optimizes replay by selecting high-impact samples.  \n\n#### Open Challenges  \nDespite these advances, critical gaps remain. First, most quantization and sparsity techniques are evaluated on small-scale benchmarks, leaving their scalability to billion-parameter LLMs unproven [6]. Second, distributed training struggles with synchronization delays in heterogeneous environments [3]. Finally, financial barriers persist, as even optimized methods require expensive hardware for deployment [21].  \n\nIn summary, computational and resource constraints are central to the feasibility of CL for LLMs. While innovations like quantization and parameter-efficient tuning offer promising directions, their real-world applicability hinges on further scalability and cost reductions. Future work must prioritize energy-efficient algorithms and democratized access to computational resources to unlock CL’s full potential.  \n---\n\n### 6.2 Data Heterogeneity and Distribution Shifts\n\n---\n### 6.2 Data Heterogeneity and Distribution Shifts  \n\nContinual learning (CL) in large language models (LLMs) must contend with the fundamental challenge of data heterogeneity and distribution shifts—a natural consequence of real-world data streams that evolve over time. Unlike static training paradigms that assume i.i.d. data, CL requires LLMs to adapt dynamically to non-stationary distributions while preserving previously acquired knowledge. This section systematically examines the challenges posed by temporal drift, domain shifts, and dataset imbalances, while also evaluating mitigation strategies such as replay mechanisms, domain adaptation, and dynamic architectures. These challenges are particularly acute given the computational and resource constraints discussed in Section 6.1, and they further intersect with the ethical concerns surrounding bias and fairness that will be explored in Section 6.3.  \n\n#### Non-Stationary Data and Temporal Drift  \nThe core challenge of non-stationarity arises when the statistical properties of input data change over time, leading to performance degradation in LLMs. In applications like multilingual adaptation or domain-specific tasks (e.g., healthcare or legal domains), shifts may occur due to evolving terminology, cultural context, or regulatory updates. Such dynamics can cause LLMs to either \"forget\" prior knowledge or fail to generalize to new contexts. As highlighted in [21], LLMs relying on parametric memory struggle with non-stationary environments, as their static representations become obsolete.  \n\nTemporal drift further exacerbates this issue, particularly in scenarios where data relevance decays over time. For instance, in personalized recommendation systems, user preferences and trends evolve continuously, demanding that LLMs update their understanding in near real-time. The paper [120] underscores this challenge, noting that autonomous agents powered by LLMs must handle non-stationary user interactions to remain effective.  \n\n#### Domain Shifts and Cross-Domain Generalization  \nDomain shifts present another critical hurdle, occurring when LLMs encounter data distributions that diverge from their training domains. This is especially problematic in high-stakes fields like healthcare, where models must adapt to diverse clinical practices or regional variations. For example, [20] illustrates how legal LLMs struggle with jurisdiction-specific nuances, where subtle differences in language or precedents can lead to erroneous outputs.  \n\nCross-domain adaptation is further complicated by the scarcity of labeled data in new domains. Papers such as [197] emphasize the difficulty of integrating multimodal data (e.g., EHRs or imaging) into LLMs without robust adaptation techniques. Similarly, [262] highlights the need for edge-cloud collaboration to address domain shifts in real-time applications with heterogeneous data sources.  \n\n#### Imbalanced Datasets and Class Distribution Shifts  \nDataset imbalance introduces additional complexity, as underrepresented classes or domains can lead to biased predictions and poor generalization. In low-data regimes, traditional augmentation methods often fail to generate diverse samples, as noted in [117]. While LLMs can leverage prior knowledge to synthesize data, this approach requires careful curation to maintain quality.  \n\nClass distribution shifts are particularly acute in lifelong learning, where LLMs must incrementally acquire new knowledge without catastrophic forgetting. [116] proposes a Mixture-of-Experts (MoE) architecture to dynamically allocate capacity to new distributions, though this introduces computational overhead and necessitates careful regularization.  \n\n#### Mitigation Strategies  \nTo address these challenges, researchers have developed several key approaches:  \n\n1. **Replay Buffers and Memory Mechanisms**: Storing and replaying samples from past tasks helps mitigate forgetting. [21] shows that replay buffers enable tool-using LLMs to retain procedural knowledge. However, scalability remains an issue, as noted in [97].  \n\n2. **Parameter-Efficient Domain Adaptation**: Techniques like Low-Rank Adaptation (LoRA) allow LLMs to adapt to new domains with minimal computational cost. [262] explores edge-cloud collaboration to enable efficient fine-tuning in resource-constrained settings.  \n\n3. **Dynamic Architectures**: Modular designs, such as MoE, enable flexible resource allocation. [116] demonstrates that domain-specialized experts improve adaptability while preserving prior knowledge.  \n\n4. **Self-Supervised Learning**: Leveraging unlabeled data through contrastive learning or other self-supervised objectives can reduce reliance on labeled datasets. [30] discusses how self-evolution frameworks enhance adaptability.  \n\n#### Open Challenges and Future Directions  \nDespite progress, critical gaps remain:  \n- **Scalability of Replay Methods**: Storing large-scale data for replay is prohibitively expensive for billion-parameter LLMs [24].  \n- **Generalization to Unseen Domains**: Current adaptation techniques often require task-specific tuning, limiting their versatility [197].  \n- **Bias Amplification**: Data heterogeneity can exacerbate fairness issues, particularly in sensitive applications [20].  \n\nFuture work should prioritize lightweight, scalable CL methods that balance performance with ethical considerations. Hybrid approaches combining replay, domain adaptation, and dynamic architectures—as suggested in [30]—may offer a viable path forward.  \n\nIn summary, data heterogeneity and distribution shifts represent a central challenge for CL in LLMs, with implications for computational efficiency, generalization, and fairness. Addressing these issues will be essential for deploying LLMs in dynamic, real-world environments while aligning with broader ethical and resource constraints.  \n---\n\n### 6.3 Ethical and Societal Concerns\n\n### 6.3 Ethical and Societal Concerns in Continual Learning  \n\nThe integration of continual learning (CL) into large language models (LLMs) raises profound ethical and societal challenges, particularly as these models are deployed in high-stakes domains such as healthcare, legal systems, and personalized recommendations. These concerns span bias amplification, privacy violations, misuse risks, and environmental impact, each requiring careful consideration to ensure responsible development and deployment.  \n\n#### Bias Amplification and Fairness  \nA critical ethical issue in CL-enabled LLMs is the perpetuation or exacerbation of societal biases. Incremental learning on sequential tasks can reinforce historical prejudices present in training data, leading to discriminatory outcomes in applications like hiring, lending, or criminal justice [41]. This problem is compounded by the stability-plasticity trade-off in CL, where prioritizing stability (retaining old knowledge) may entrench biased representations from earlier tasks. Recent studies show that CL algorithms unaware of dataset biases can propagate these biases both forward and backward across tasks, complicating mitigation efforts [41].  \n\nTo address this, fairness-aware training frameworks have been proposed. These approaches dynamically adjust learning priorities based on prediction uncertainty and incorporate debiasing techniques like Group-class Balanced Greedy Sampling (BGS) [41]. However, their effectiveness in long-task sequences remains an open challenge, highlighting the need for more robust bias mitigation strategies in lifelong learning scenarios.  \n\n#### Privacy Risks and Data Security  \nPrivacy violations represent another major concern, as CL often involves fine-tuning LLMs on sequential datasets containing sensitive information. In healthcare, for instance, patient confidentiality must be preserved when training on electronic health records (EHRs), yet CL techniques like replay-based methods or elastic weight consolidation (EWC) risk memorizing identifiable data [122]. Recent work demonstrates that modulating batch normalization statistics can mitigate catastrophic forgetting in medical imaging while preserving privacy, though this approach is not foolproof [40].  \n\nFederated learning has emerged as a potential solution by enabling decentralized training without data sharing. However, it introduces new challenges such as client drift and fairness disparities [179]. The tension between privacy preservation and model performance in CL systems requires further exploration, especially under stringent regulatory frameworks like GDPR.  \n\n#### Misuse and Accountability in High-Stakes Domains  \nThe deployment of CL-enabled LLMs in critical applications introduces risks of misuse and unreliable outputs. In legal systems, for example, models may generate inaccurate or hallucinated judgments due to catastrophic forgetting or task interference, with severe consequences for legal workflows. Similarly, in healthcare, LLMs adapting to new medical knowledge may produce \"hallucinations\" in diagnoses or treatment recommendations, jeopardizing patient safety.  \n\nMitigation strategies include multi-agent collaboration frameworks and human-in-the-loop oversight, though these often trade efficiency for robustness. Transparency mechanisms, such as three-layered audits, have been proposed to enhance accountability, but their scalability in real-world deployments remains untested.  \n\n#### Environmental Impact and Sustainability  \nThe computational demands of CL systems also raise ethical questions regarding energy consumption and carbon emissions. Frequent retraining and large-scale model updates—common in CL—significantly increase energy usage, with deeper networks and convolutional layers being particularly resource-intensive [37]. Recent analyses highlight the need for energy-efficient practices, such as Bayesian optimization and model pruning, though their integration with CL methods is still nascent [263]. Metrics like \"accuracy per unit of electricity consumed\" offer a promising direction for evaluating sustainability, but broader adoption is needed to drive industry-wide change.  \n\n#### Interdisciplinary Solutions and Future Directions  \nAddressing these challenges requires collaboration across AI research, domain expertise, and policy-making. Insights from cognitive science, for instance, have inspired biologically inspired plasticity rules that mitigate forgetting while adhering to ethical constraints [173]. Similarly, frameworks for trustworthy AI emphasize the integration of fairness, accountability, and transparency principles into CL systems.  \n\nHowever, the lack of standardized evaluation protocols for ethical risks—such as metrics like CIDER_t for stage-wise forgetting rates [42]—hinders progress. Future work must prioritize benchmarks that quantify bias, privacy risks, and misuse potential alongside traditional performance metrics.  \n\nIn summary, ethical and societal concerns in CL for LLMs are multifaceted, spanning bias, privacy, misuse, and sustainability. While fairness-aware training, federated learning, and transparency frameworks offer partial solutions, their real-world efficacy is often limited by trade-offs between performance, efficiency, and ethical safeguards. A holistic approach—combining technical innovation, regulatory oversight, and interdisciplinary collaboration—will be essential to ensure the responsible advancement of CL systems. Key open questions include how to quantify ethical risks in dynamic environments and how to balance societal harms against performance gains, underscoring the need for ongoing stakeholder dialogue.\n\n### 6.4 Scalability and Real-World Deployment\n\n### 6.4 Scalability and Real-World Deployment  \n\nThe practical implementation of continual learning (CL) in large language models (LLMs) faces significant scalability challenges as these systems transition from research environments to real-world applications. Building on the ethical considerations discussed in Section 6.3, the deployment of CL-enabled LLMs must address computational, hardware, and environmental constraints while maintaining robust performance in dynamic settings. These challenges become particularly acute in high-stakes domains like healthcare and legal systems, where reliability and responsiveness are paramount—a theme that further connects to the legal and regulatory considerations explored in Section 6.5.  \n\n#### Computational Latency and Efficiency  \nA primary barrier to scalable CL deployment is the computational overhead introduced by continuous model updates. Unlike static models, CL systems must process streaming data while preserving prior knowledge, often leading to increased training and inference times. [55] demonstrates how federated CL settings exacerbate these latency issues due to synchronization delays across distributed clients. Similarly, [133] reveals inefficiencies in traditional CL approaches when applied to dynamic networks with frequent topology changes.  \n\nEdge computing has emerged as a promising solution to mitigate latency. By decentralizing computation and deploying lightweight LLM variants on edge devices, CL systems can reduce reliance on centralized infrastructure. For instance, [47] introduces a hybrid architecture where domain-specific models handle localized updates, offloading computational burden from the primary LLM. This approach not only improves responsiveness in time-sensitive applications like autonomous systems but also aligns with the ethical imperative for efficient resource use discussed earlier.  \n\n#### Hardware Constraints and Resource Optimization  \nThe resource demands of LLMs present another critical scalability challenge. CL techniques for mitigating catastrophic forgetting—such as memory replay or regularization—often require additional GPU memory and energy, straining hardware capabilities. [1] highlights how these requirements can render CL impractical for resource-constrained environments.  \n\nModular architectures offer a path forward by enabling selective updates to model components. [129] advocates for techniques like low-rank adaptation (LoRA) and mixture-of-experts, which allow domain-specific modules to be updated independently. This reduces full-model retraining needs and optimizes hardware utilization. Complementary approaches like parameter sharing and sparse updates, explored in [264], further alleviate memory constraints while maintaining model performance.  \n\n#### Adaptation to Dynamic Environments  \nReal-world deployment requires CL systems to handle evolving data distributions and heterogeneous requirements—a challenge that foreshadows the legal complexities of dynamic compliance discussed in Section 6.5. In multilingual applications, for example, models must adapt to shifting linguistic patterns without degrading performance on previously learned languages. [52] reveals inherent imbalances in LLM capabilities across languages, necessitating robust CL mechanisms to manage stability-plasticity trade-offs.  \n\nHybrid frameworks combining replay-based methods with domain adaptation have shown promise in addressing these challenges. [54] introduces cross-modal prompting to align domain-invariant features, while [265] leverages retrieval-augmented generation (RAG) to dynamically incorporate external knowledge. These approaches reduce the frequency of model updates in domains like healthcare and law, where accuracy must be maintained amid rapidly evolving knowledge bases.  \n\n#### Strategies for Scalable Deployment  \nTo bridge the gap between research and real-world application, several key strategies are emerging:  \n1. **Edge-Cloud Collaboration**: Frameworks like [53] distribute CL tasks between edge devices (for local updates) and cloud servers (for global consolidation), optimizing resource use.  \n2. **Model Compression**: Techniques such as quantization and pruning, detailed in [55], reduce model size without significant performance loss.  \n3. **Dynamic Resource Allocation**: Approaches like [266] enable LLMs to prioritize computational resources based on task criticality.  \n4. **Human-AI Collaboration**: Integrating human feedback loops, as proposed in [61], enhances adaptability in high-stakes domains while addressing accountability concerns raised in Section 6.5.  \n\n#### Open Challenges and Future Directions  \nDespite these advances, critical gaps remain. [267] underscores the need for robust security in distributed CL systems to prevent adversarial attacks during updates. Meanwhile, [232] calls for causal reasoning frameworks to ensure CL updates do not introduce spurious correlations—a challenge that intersects with the legal risks of model hallucinations discussed in the subsequent section.  \n\nIn conclusion, scaling CL for real-world LLM deployment requires solutions that balance computational efficiency, hardware constraints, and dynamic adaptability. By integrating edge computing, modular architectures, and hybrid learning paradigms, researchers can develop CL systems capable of operating effectively in complex, evolving environments. Future work must unify these approaches while addressing security, causality, and regulatory compliance—themes that further connect to the broader ethical and legal considerations surrounding CL-enabled LLMs.\n\n### 6.5 Legal and Regulatory Challenges\n\n### 6.5 Legal and Regulatory Challenges  \n\nThe deployment of continual learning (CL) in large language models (LLMs) introduces complex legal and regulatory challenges, particularly as these models are increasingly applied in high-stakes domains such as healthcare, legal services, and finance. These challenges stem from the dynamic nature of CL systems, where models evolve continuously, raising concerns about hallucinations, accountability gaps, and compliance with rapidly evolving AI regulations. Addressing these issues requires a multifaceted approach that bridges technical solutions with legal frameworks, ensuring CL systems remain trustworthy and compliant as they adapt to new data and tasks.  \n\n#### Hallucinations and Misinformation Risks  \nA critical legal risk in CL-enabled LLMs is their tendency to generate hallucinations—factually incorrect or fabricated outputs—which can have severe consequences in regulated domains. For instance, in legal applications, studies like [136] demonstrate that even state-of-the-art models struggle with accurate legal text interpretation, potentially leading to erroneous advice or judgments. Hallucinations are further exacerbated in CL settings, where incremental updates may introduce unintended biases or errors without robust safeguards.  \n\nTo mitigate these risks, recent research emphasizes the need for rigorous auditing frameworks. [208] proposes a three-layered audit system to evaluate model outputs for factual accuracy and ethical compliance. Similarly, [67] introduces dynamic benchmarking to detect and correct hallucinations in real-time by continuously evolving test cases. These approaches highlight the importance of integrating external knowledge bases, such as retrieval-augmented generation (RAG) systems, to ground model outputs in verifiable sources and reduce reliance on inherently unstable model-generated content.  \n\n#### Accountability and Traceability Gaps  \nThe continual adaptation of LLMs complicates accountability, as it becomes challenging to trace the provenance of specific model behaviors or decisions. This is particularly critical in legal and regulatory contexts, where accountability is a cornerstone of compliance. For example, [268] reveals that LLMs often lack transparency in their decision-making processes, making it difficult to assign liability for erroneous outputs. The study advocates for explainable AI (XAI) techniques to document model updates and decision pathways, ensuring traceability across the model's lifecycle.  \n\nAccountability gaps are further amplified in distributed CL settings, where multiple stakeholders contribute to model updates. [184] demonstrates that LLMs deployed as autonomous agents in multi-turn interactions often exhibit unpredictable behaviors, necessitating human oversight. To address this, [207] proposes using fine-tuned LLMs as scalable auditors to evaluate outputs against legal standards, supplemented by human reviewers to resolve ambiguous cases. This hybrid approach balances automation with accountability, ensuring compliance in high-stakes applications.  \n\n#### Compliance with Dynamic Regulatory Frameworks  \nThe regulatory landscape for AI is evolving rapidly, with jurisdictions like the EU and US introducing frameworks such as the AI Act and Algorithmic Accountability Act. CL systems must adapt to these regulations, which impose strict requirements for transparency, fairness, and data privacy. For instance, [68] highlights the risks of bias propagation in continually updated models, which can violate anti-discrimination laws. The study advocates for fairness-aware training and adversarial debiasing to ensure compliance with evolving legal standards.  \n\nData privacy remains another critical concern, particularly when CL involves processing sensitive or personal data. [233] reveals that LLMs trained on contaminated datasets may inadvertently leak private information, violating regulations like GDPR. Techniques such as differential privacy and data anonymization are essential to mitigate these risks. Additionally, [269] warns against benchmark leakage, where models overfit to evaluation data, compromising their generalizability and regulatory compliance.  \n\n#### Strategies for Mitigation and Compliance  \nTo address these challenges, several key strategies have emerged:  \n1. **Continuous Auditing and Transparency**: Frameworks like [208] enable ongoing evaluation of model outputs against legal and ethical standards. Tools such as [71] provide granular insights into model behaviors, facilitating real-time compliance monitoring.  \n2. **Human-in-the-Loop Oversight**: Integrating human reviewers in critical decision-making processes, as demonstrated in [207], ensures accountability and reduces the risk of unchecked errors in dynamic environments.  \n3. **Proactive Regulatory Alignment**: Collaboration between policymakers and AI developers is essential to align CL systems with emerging regulations, such as the AI Act, while maintaining flexibility for future legal developments.  \n\n#### Future Directions  \nFuture research must prioritize the development of standardized legal benchmarks for CL systems, akin to [270], which evaluates models against domain-specific regulations. Additionally, [271] suggests leveraging dynamic benchmarking to test compliance under evolving legal scenarios, ensuring models remain robust as regulations change.  \n\nIn conclusion, the legal and regulatory challenges of CL in LLMs demand proactive, interdisciplinary solutions to ensure accountability, transparency, and compliance. By integrating continuous auditing, human oversight, and adaptive regulatory alignment, stakeholders can mitigate risks while harnessing the transformative potential of continual learning in real-world applications. This aligns with the broader need for interdisciplinary collaboration, as explored in the following subsection, to address the societal and ethical dimensions of CL systems.\n\n### 6.6 Interdisciplinary Collaboration Needs\n\n### 6.6 Interdisciplinary Collaboration Needs  \n\nThe development and deployment of continual learning (CL) systems for large language models (LLMs) present multifaceted challenges that cannot be addressed by any single discipline alone. As highlighted in the preceding discussion on legal and regulatory challenges, the risks associated with CL—such as bias propagation, accountability gaps, and compliance with dynamic frameworks—demand coordinated efforts across technical, domain-specific, and policy-oriented fields. This subsection examines the critical role of interdisciplinary collaboration in ensuring CL systems align with societal values while mitigating risks across high-stakes applications.  \n\n#### Bridging Technical and Domain-Specific Expertise  \nThe dynamic nature of CL systems requires deep integration of domain knowledge to ensure adaptability and fairness in real-world applications. For instance, in healthcare, CL systems must incorporate medical expertise to avoid biases in clinical decision-making, as demonstrated by studies revealing demographic disparities in LLM-generated recommendations [79]. Similarly, legal applications require collaboration with jurists to align model outputs with jurisdictional norms and ethical standards, preventing harmful outcomes like misinformation or biased judgments [76].  \n\nDomain expertise also ensures contextual appropriateness, particularly in multilingual and multicultural settings. Without input from sociolinguists and regional experts, CL systems risk amplifying inequities or failing underrepresented populations, as noted in critiques of bias in pre-trained models [87]. Such collaborations enable CL systems to move beyond technical performance metrics and address real-world applicability.  \n\n#### Aligning Policy and Technical Innovation  \nThe rapid evolution of CL systems has created a governance gap, where regulatory frameworks struggle to keep pace with technological advancements. Policymakers must collaborate with AI researchers to design standards that balance innovation with accountability. For example, the ethical risks of LLMs—such as privacy violations and misuse—necessitate policies enforcing transparency and auditing mechanisms, as proposed in three-layered audit frameworks [81].  \n\nCL-specific challenges, like the plasticity-stability trade-off and catastrophic forgetting, further complicate regulatory efforts. Policymakers must understand these technical constraints to avoid unrealistic mandates, while researchers must articulate their societal implications. Studies on cognitive bias in high-stakes decision-making underscore the need for safeguards against model drift over time, particularly in finance or criminal justice applications [147]. Interdisciplinary dialogue can yield practical guidelines for monitoring and intervention, ensuring compliance with frameworks like the EU AI Act.  \n\n#### Embedding Societal and Ethical Considerations  \nTechnical solutions alone cannot resolve ethical concerns such as fairness, transparency, and accountability. CL systems trained on heterogeneous data may perpetuate stereotypes or exclude marginalized perspectives, as observed in analyses of protected group bias [73]. Addressing these issues requires input from ethicists, social scientists, and community stakeholders to define inclusive design principles and fairness metrics.  \n\nParticipatory approaches, where end-users and affected communities contribute to system design, are particularly effective. Frameworks advocating iterative feedback loops between developers and stakeholders help refine fairness trade-offs [148]. Similarly, culturally sensitive personalization in CL systems demands collaboration with anthropologists to avoid harmful homogenization [29].  \n\n#### Advancing Education and Public Engagement  \nInterdisciplinary collaboration must extend to education, ensuring practitioners and the public understand CL systems' capabilities and limitations. Studies reveal that users often lack awareness of privacy risks or ethical boundaries when interacting with AI systems [272]. Joint initiatives between AI educators, psychologists, and policymakers are needed to develop literacy programs that empower responsible use.  \n\nTraining programs must also equip future researchers with holistic problem-solving skills. Integrating ethics modules into technical curricula, as suggested by studies on LLM misuse in education [273], fosters a culture of responsible innovation. Partnerships with humanities scholars can further contextualize CL systems within historical and philosophical frameworks, countering critiques of \"semantic capital\" erosion [248].  \n\n#### Operationalizing Collaboration: Pathways Forward  \nTo translate interdisciplinary principles into practice, the following steps are critical:  \n1. **Establish Cross-Sector Consortia**: Forums like the Partnership on AI or domain-specific task forces (e.g., for healthcare or legal CL systems) can facilitate co-design of best practices.  \n2. **Develop Shared Benchmarks**: Collaborative efforts, such as EquityMedQA [144], demonstrate how interdisciplinary teams can standardize equity evaluations.  \n3. **Promote Policy-Aware Research**: Funding agencies should incentivize projects integrating technical and policy perspectives, such as those exploring CL governance [274].  \n\nIn conclusion, interdisciplinary collaboration is indispensable for addressing the complex challenges of continual learning systems. By uniting technical innovation with domain expertise, policy insights, and ethical foresight, stakeholders can ensure CL systems evolve responsibly, aligning with societal needs and values. This holistic approach sets the stage for further exploration of emerging trends and future directions in CL, as discussed in subsequent sections.\n\n## 7 Comparative Analysis of Existing Approaches\n\n### 7.1 Taxonomy of Continual Learning Methods in LLMs\n\n### 7.1 Taxonomy of Continual Learning Methods in LLMs  \n\nContinual learning (CL) methods for large language models (LLMs) can be systematically organized into three primary paradigms: **replay-based**, **regularization-based**, and **architectural adaptations**. These approaches address the fundamental challenges of catastrophic forgetting and knowledge transfer through distinct yet complementary mechanisms, each balancing plasticity (adaptation to new tasks) and stability (retention of prior knowledge). Below, we present a structured analysis of these categories, their underlying principles, strengths, limitations, and recent advancements.\n\n---\n\n#### **Replay-Based Methods**  \nReplay-based methods combat forgetting by retaining and revisiting subsets of past task data during new task training, mimicking the brain's rehearsal mechanism.  \n\n1. **Experience Replay (ER)**: This foundational approach maintains a fixed-size buffer of past task samples, which are interleaved with new task data during training [8]. Advanced variants, such as **dynamic memory replay**, optimize buffer selection by prioritizing samples with high forgetting risk or gradient diversity [167].  \n\n2. **Generative Replay**: Instead of storing raw data, generative models (e.g., GANs or VAEs) synthesize pseudo-samples of past tasks. While this reduces memory overhead, generating high-fidelity text remains challenging due to the complexity of LLMs [275].  \n\n3. **Meta-Replay**: This hybrid approach combines replay with meta-learning to optimize the balance between old and new task performance. For example, [276] employs fixed recurrent layers with replay buffers to preserve temporal patterns across tasks.  \n\n**Limitations**: Replay methods often face scalability issues due to memory constraints and may struggle with data heterogeneity or privacy concerns [18].  \n\n---\n\n#### **Regularization-Based Methods**  \nRegularization techniques preserve prior knowledge by penalizing changes to parameters critical for past tasks, offering a computationally efficient alternative to replay.  \n\n1. **Elastic Weight Consolidation (EWC)**: This method introduces a quadratic penalty on parameter updates, weighted by their importance to previous tasks [107]. While effective for smaller models, EWC struggles with the high-dimensional parameter spaces of modern LLMs.  \n\n2. **Knowledge Distillation (KD)**: KD transfers knowledge from a \"teacher\" model (trained on past tasks) to a \"student\" model (learning new tasks) via soft targets or attention maps [109].  \n\n3. **Consistency Regularization**: This approach enforces similarity between old and new model outputs or intermediate representations. For instance, [166] uses language-model-generated semantic targets to stabilize predictions across tasks.  \n\n**Limitations**: Regularization methods typically assume known task boundaries and may underperform in task-agnostic scenarios [277].  \n\n---\n\n#### **Architectural Adaptations**  \nArchitectural methods dynamically adjust the model's structure to isolate or share task-specific parameters, making them particularly scalable for LLMs.  \n\n1. **Parameter-Efficient Fine-Tuning (PEFT)**:  \n   - **LoRA (Low-Rank Adaptation)**: Freezes pretrained weights and introduces trainable low-rank matrices for task-specific adaptations [12].  \n   - **Adapters**: Lightweight task-specific modules are inserted between transformer layers, enabling incremental updates without overwriting core knowledge [278].  \n\n2. **Mixture-of-Experts (MoE)**: Dynamically allocates subsets of parameters (\"experts\") to tasks, though it may introduce inference latency.  \n\n3. **Sparse Networks**: Prunes or grows connections to allocate capacity per task. For example, [108] uses fixed-density sparse networks to stabilize old tasks while adapting to new ones.  \n\n**Limitations**: Architectural methods can increase inference complexity (e.g., MoE routing) or require careful subspace design [279].  \n\n---\n\n#### **Hybrid and Emerging Approaches**  \nRecent research explores hybrid methods that combine the strengths of multiple paradigms, as well as novel techniques leveraging external tools or pre-trained models.  \n\n1. **Replay + Regularization**: [167] integrates gradient-based replay with distillation losses to enhance stability and plasticity.  \n2. **Architectural + Replay**: [280] reduces memory overhead by replaying hidden activations within sparse networks.  \n3. **Self-Supervised CL**: [111] uses contrastive learning to align representations across tasks without labels.  \n\n**Emerging Trends**:  \n- **Tool-Augmented CL**: [21] offloads task-specific knowledge to external tools, reducing parametric memory demands.  \n- **Pre-trained Model Integration**: [7] highlights the role of foundation models as stable feature extractors for downstream CL.  \n\n---\n\n#### **Comparative Analysis and Future Directions**  \n1. **Stability-Plasticity Trade-offs**: Replay methods excel in stability but lack scalability; regularization sacrifices plasticity for stability; architectural methods offer scalability but may compromise inference efficiency.  \n2. **Memory-Compute Balance**: Replay demands memory for storage; regularization increases compute for parameter updates; architectural methods strike a balance but require careful design.  \n3. **Task Agnosticism**: Hybrid and self-supervised methods show promise for task-agnostic CL, as demonstrated in [260].  \n\n**Future Directions**: Unified frameworks like [281] aim to reconcile these paradigms, while benchmarks like [17] drive standardized evaluations.  \n\nIn summary, the taxonomy of CL methods for LLMs reflects a dynamic interplay of memory, computation, and adaptability, with each approach addressing distinct facets of the continual learning challenge. The choice of method depends on specific application requirements, balancing trade-offs to achieve optimal performance.\n\n### 7.2 Efficiency Comparison Across CL Methods\n\n---\n### 7.2 Computational Efficiency and Scalability  \n\nContinual learning (CL) methods for large language models (LLMs) must not only address catastrophic forgetting (as discussed in the taxonomy of Section 7.1) but also optimize computational resources to ensure practical deployability—a challenge that directly impacts their accuracy and retention performance (explored subsequently in Section 7.3). This subsection analyzes the efficiency-scalability trade-offs of prominent CL techniques, focusing on parameter-efficient fine-tuning (PEFT) methods like LoRA, MultiLoRA, and BBox-Adapter, while connecting their design choices to the broader stability-plasticity paradigm.  \n\n#### **Computational Costs and Parameter Efficiency**  \nThe computational overhead of CL methods is governed by their trainable parameter count and adaptation granularity:  \n- **LoRA (Low-Rank Adaptation)**: Freezes pretrained weights and injects task-specific low-rank matrices, reducing trainable parameters to 0.1%–1% of the base model size. While efficient for single-task adaptation, its computational cost scales with the rank of adaptation matrices [97].  \n- **MultiLoRA**: Extends LoRA with multiple adapters for multitasking, increasing compute proportionally to adapter count. Dynamic adapter activation during inference mitigates this cost but introduces routing latency [282].  \n- **BBox-Adapter**: Restricts updates to task-specific subspaces via bounding box mechanisms. Its compute efficiency depends on subspace identification accuracy, with coarse subdivisions risking underfitting and fine ones increasing overhead [22].  \n\n**Trade-off**: LoRA excels in single-task efficiency; MultiLoRA trades compute for flexibility; BBox-Adapter balances both but requires precise task-space alignment.  \n\n#### **Memory Overhead and Deployment Constraints**  \nMemory efficiency is critical for edge and large-scale deployments:  \n- **LoRA** minimizes memory footprint by sharing frozen base weights across tasks, making it viable for resource-constrained environments like edge devices [84].  \n- **MultiLoRA**’s memory grows linearly with active adapters, though techniques like dynamic loading (e.g., unloading inactive adapters) alleviate this [118].  \n- **BBox-Adapter** localizes memory usage to subspaces but risks redundancy if subdivisions are misaligned with task boundaries [197].  \n\n**Implication**: Memory-constrained scenarios favor LoRA, while dynamic multitasking may justify MultiLoRA’s higher overhead.  \n\n#### **Scalability in Large-Scale Systems**  \nScalability challenges emerge with model and task complexity:  \n- **LoRA**’s lightweight design enables seamless scaling to billion-parameter models and distributed training [24].  \n- **MultiLoRA** faces bottlenecks in highly dynamic environments due to adapter proliferation, prompting hybrid solutions like sparse activation [169].  \n- **BBox-Adapter** struggles with automated subspace partitioning in growing models, requiring clustering-based optimizations [120].  \n\n**Trend**: Scalability favors LoRA for homogeneous tasks and MultiLoRA for modular multitasking, with BBox-Adapter excelling in niche domains.  \n\n#### **Synthesis and Forward Pathways**  \nThe efficiency-scalability landscape reveals method-specific niches:  \n- **LoRA** dominates single-task efficiency but lacks multitasking agility.  \n- **MultiLoRA** enables flexible multitasking at higher resource costs.  \n- **BBox-Adapter** offers targeted efficiency but demands task-space expertise.  \n\n**Future Directions**: Hybridization (e.g., LoRA + BBox-Adapter subspaces) and dynamic resource allocation (e.g., [283]) could bridge these trade-offs. These advancements will inform the accuracy-retention discussions in Section 7.3, where efficiency gains must align with performance preservation.  \n\nIn summary, CL method selection hinges on application-specific resource constraints and task dynamics, with LoRA, MultiLoRA, and BBox-Adapter providing complementary tools to navigate the efficiency-scalability-accuracy trilemma.\n\n### 7.3 Accuracy and Task Retention Performance\n\n### 7.3 Accuracy and Task Retention Performance  \n\nContinual learning (CL) methods for large language models (LLMs) must balance computational efficiency (as discussed in the previous subsection) with their core objective: maintaining accuracy and retaining knowledge across sequential tasks. This subsection evaluates CL performance through three key metrics—forward/backward transfer and catastrophic forgetting—while connecting findings to real-world adaptability challenges explored in subsequent sections on domain shifts.  \n\n#### Benchmark Insights: LongICLBench and EvolvingQA  \nStandardized benchmarks like LongICLBench and EvolvingQA simulate sequential learning scenarios critical for assessing CL robustness. On LongICLBench, replay-based methods (e.g., Experience Replay) demonstrate strong backward transfer by explicitly retaining past data samples [34]. However, their memory storage demands create scalability bottlenecks, echoing the efficiency trade-offs highlighted in prior computational cost analyses.  \n\nEvolvingQA’s dynamic question-answering tasks reveal complementary strengths: parameter-efficient fine-tuning (PEFT) methods like LoRA exhibit superior forward transfer by adaptively updating subsets of parameters [121]. Yet, their backward transfer falters during significant distribution shifts—a limitation later addressed in domain adaptation strategies (see following subsection). This dichotomy underscores the plasticity-stability trade-off inherent to CL.  \n\n#### Mitigating Catastrophic Forgetting  \nThe severity of catastrophic forgetting varies across CL methodologies, with implications for deployment in resource-constrained environments:  \n- **Architectural adaptations**: Mixture-of-Experts (MoE) models reduce forgetting by isolating task-specific parameters, achieving higher retention on LongICLBench [124]. However, their expert routing complexity parallels the scalability challenges noted for MultiLoRA in earlier efficiency discussions.  \n- **Regularization techniques**: Elastic weight consolidation (EWC) stabilizes performance by penalizing changes to critical parameters [40]. Its rigidity in heterogeneous tasks mirrors the domain adaptation limitations explored later, particularly in healthcare applications.  \n- **Hybrid approaches**: Combining replay with knowledge distillation balances stability and plasticity, improving backward transfer on EvolvingQA [178]. These methods bridge the gap between computational efficiency (e.g., LoRA) and robust retention (e.g., replay).  \n\n#### Transfer Learning Dynamics  \n- **Forward transfer**: Self-supervised CL methods excel in low-data regimes by leveraging pre-trained representations [33], but plateau when task-specific fine-tuning is required—a challenge later addressed via dynamic architecture routing in multimodal scenarios.  \n- **Backward transfer**: Gradient-based replay aligns task gradients to refine prior knowledge [201], though its sensitivity to replay frequency echoes the memory overhead concerns raised earlier.  \n\n#### Task-Specific Challenges  \nPerformance variability across domains foreshadows subsequent discussions on adaptability:  \n- **Multilingual tasks**: LoRA adapters struggle with low-resource languages due to data scarcity [41], anticipating the cross-lingual adaptation challenges detailed later.  \n- **Healthcare applications**: Batch normalization modulation mitigates forgetting in medical imaging [40], yet instability persists—a theme expanded in domain-specific specialization case studies.  \n\n#### Emerging Directions  \nTwo trends align with future adaptability research:  \n1. **Meta-learning**: Dynamic plasticity-stability adjustments based on task similarity [128] prefigure hybrid domain adaptation strategies.  \n2. **Granular evaluation**: \"Stability gaps\" reveal temporary forgetting phases [284], suggesting current benchmarks may underestimate robustness—a consideration for evolving domain-shift evaluations.  \n\n#### Synthesis and Forward Look  \nNo single method universally excels; replay-based approaches dominate retention but lack scalability, while PEFT sacrifices backward transfer for efficiency. Hybrid and meta-learning solutions offer promise but require refinement—a challenge that intersects with domain adaptation’s demand for dynamic architectures (as explored next). Together, these findings underscore the need for task-aware CL frameworks that harmonize accuracy, retention, and the efficiency principles established earlier.\n\n### 7.4 Adaptability to Domain Shifts\n\n---\n### 7.4 Robustness to Domain Shifts  \n\nThe adaptability of continual learning (CL) methods for large language models (LLMs) to domain shifts serves as a critical bridge between theoretical task retention (discussed in Section 7.3) and practical integration with pre-training paradigms (explored in Section 7.5). This subsection evaluates CL robustness through three key challenges—multilingual adaptation, domain-specific specialization, and multimodal integration—while connecting findings to both upstream accuracy metrics and downstream implementation trade-offs.  \n\n#### Multilingual Adaptation  \nCL methods face unique challenges in multilingual settings, where linguistic diversity and resource disparities exacerbate catastrophic forgetting. The SERVAL framework [50] demonstrates how synergy learning between LLMs and smaller vertical models can enhance zero-shot medical QA across languages without gold annotations. However, low-resource languages remain problematic due to tokenization biases and uneven pre-training representation—a limitation partially addressed by CrossLM [53], which facilitates knowledge transfer between LLMs and client-side small language models (SLMs). Notably, [52] reveals persistent performance gaps and proposes cross-lingual-thought prompting (XLT) to stimulate reasoning skills. These studies underscore that while replay-based methods and parameter-efficient fine-tuning (e.g., LoRA) work well for high-resource languages, their efficacy diminishes in low-resource contexts due to sparse lexical overlap—a theme later echoed in domain adaptation challenges.  \n\n#### Domain-Specific Specialization  \nVertical domains like healthcare and legal demand precise adaptation to specialized knowledge while preserving general capabilities. The BLADE framework [47] exemplifies this balance by integrating domain-specific LMs with LLMs via Bayesian optimization, achieving state-of-the-art performance in legal and medical benchmarks. This hybrid approach mitigates the \"knowledge inadequacy\" problem noted in [58]. However, fine-tuning LLMs solely on domain data risks overfitting and \"dual logic ability\" degradation, as observed in [49]. The REGA strategy [56] counters this through role prompting and self-distillation, showing superior adaptability in multi-domain legal QA. Nevertheless, [46] highlights LLMs' struggles with abstract legal reasoning unless augmented with retrieval systems—foreshadowing the RAG integration discussed in Section 7.5.  \n\n#### Multimodal Integration  \nMultimodal CL introduces cross-modal alignment challenges that test the limits of current architectures. BuboGPT [134] enhances visual grounding through two-stage training and SAM-based entity extraction, while [285] unifies domain-specific visual tasks via QA formatting. However, [131] reveals a paradox: despite task-specific gains, projection networks fail to capture relevant visual attributes, indicating modality-specific forgetting. The HAMMR architecture [286] addresses this via hierarchical agent composition, outperforming monolithic models by 19.5%—a finding that aligns with the hybrid CL strategies advocated in Section 7.3.  \n\n#### Limitations and Forward Pathways  \nCase studies expose critical gaps:  \n- **Clinical LLMs**: MedAgents [59] excel in multi-agent medical reasoning but depend on expert prompts and falter in low-data regimes.  \n- **Legal benchmarks**: [51] shows LLMs inconsistently applying legal logic, excelling in complex tasks but failing basic retrieval—a challenge later addressed via RAG in Section 7.5.  \n- **Security risks**: [183] reveals vulnerabilities during cross-domain adaptation, while [55] notes prohibitive communication costs.  \n\nThree emerging directions bridge CL theory and practice:  \n1. **Cross-modal replay**: Knowledge-infused alignment ([287]).  \n2. **Dynamic routing**: Sparse module interaction for robustness ([288]).  \n3. **Ethical alignment**: Cultural adaptation in multilingual CL ([62]).  \n\nThis analysis positions domain shift robustness as a linchpin between CL's retention metrics (Section 7.3) and its integration with scalable pre-training paradigms (Section 7.5), highlighting the need for task-aware architectures that balance specialization with generalization.  \n---\n\n### 7.5 Integration with Pre-training and Fine-tuning Paradigms\n\n---\n### 7.5 Integration with Pre-training and Fine-tuning Paradigms  \n\nThe effectiveness of continual learning (CL) in large language models (LLMs) is deeply intertwined with established techniques like pre-training, fine-tuning, and retrieval-augmented generation (RAG). These paradigms collectively address key CL challenges—catastrophic forgetting, computational efficiency, and domain adaptation—while introducing unique synergies and trade-offs. This subsection systematically explores these integrations, focusing on parameter-efficient fine-tuning (PEFT), RAG, and domain-adaptive pre-training (PRE), and their implications for CL in LLMs.  \n\n#### Parameter-Efficient Fine-Tuning (PEFT) as a CL Enabler  \nPEFT methods, including Low-Rank Adaptation (LoRA) and its variants (rsLoRA, SoRA, DoRA), have become indispensable for CL by enabling incremental updates with minimal computational overhead. By freezing pre-trained weights and training small adapter modules, PEFT preserves foundational knowledge while adapting to new tasks. For example, [137] demonstrates that multi-task PEFT fine-tuning enhances performance across diverse coding tasks without catastrophic forgetting. Similarly, [138] shows that PEFT-based CL allows LLMs like GPT-4 to adapt to repository-level coding tasks efficiently, improving both task retention and forward transfer.  \n\nHowever, challenges arise when reusing PEFT modules across disparate tasks. As noted in [65], this can lead to interference, necessitating careful balancing of plasticity and stability. Future work may explore task-specific adapter routing or dynamic module allocation to mitigate this issue.  \n\n#### Retrieval-Augmented Generation (RAG): Externalizing Knowledge for CL  \nRAG complements CL by offloading part of the knowledge retention burden to external retrieval systems, reducing reliance on model memorization. This is particularly valuable in dynamic domains where up-to-date information is critical. For instance, [205] demonstrates that CL models augmented with RAG outperform traditional fine-tuned models in code-editing tasks by leveraging execution traces as external memory. Similarly, [209] highlights RAG's role in improving multilingual code generation during continual learning.  \n\nA key limitation, however, is the dependency on retrieval corpus quality. As emphasized in [67], retrieval systems must evolve alongside CL models to avoid outdated or irrelevant context—a challenge exacerbated in fast-moving domains like healthcare and law [136]. Adaptive retrieval mechanisms that align with CL's incremental learning requirements remain an open research direction.  \n\n#### Domain-Adaptive Pre-training (PRE): Laying the Foundation for CL  \nPRE provides a strong starting point for CL by embedding domain-specific priors during initial training. For example, [135] shows that chemical-domain pre-training enables more effective CL for specialized tasks like reaction prediction. This two-stage approach—PRE followed by CL—is further validated in [289], where medical LLMs pre-trained on clinical texts exhibit superior adaptability to new diagnostic tasks through continual fine-tuning.  \n\nHowever, PRE alone is insufficient for lifelong learning. As [63] reveals, models like CodeLLaMA retain better multilingual reasoning skills during CL when PRE is combined with task-specific replay buffers or regularization techniques. This underscores the need for hybrid approaches that marry PRE's domain strength with CL's adaptability.  \n\n#### Key Challenges and Trade-offs  \n1. **Computational Overhead**: While PEFT reduces memory usage, integrating it with RAG or PRE increases inference latency. [66] quantifies this trade-off, showing CL+PEFT+RAG pipelines can be 2–3× slower than standalone fine-tuning.  \n2. **Data Contamination**: Pre-trained models may inadvertently memorize benchmark data, skewing CL evaluations. Studies like [234] and [269] stress the need for contamination-free benchmarks (e.g., [64]).  \n3. **Alignment Stability**: Fine-tuning for alignment (e.g., RLHF) can conflict with CL objectives. [208] finds that aligned models often exhibit reduced plasticity, necessitating hybrid approaches to balance safety and adaptability [63].  \n\n#### Future Directions  \n1. **Unified Frameworks**: Streamlined integration of PRE, PEFT, and CL (e.g., [137]) could simplify deployment in production environments.  \n2. **Dynamic Benchmarking**: Benchmarks like [186] must evolve to test integrated paradigms under realistic, multi-task scenarios.  \n3. **Human-in-the-Loop CL**: Incorporating human feedback, as proposed in [290], could refine CL models' adaptability while preserving alignment.  \n\nIn summary, the fusion of CL with pre-training and fine-tuning paradigms offers a robust pathway to scalable and adaptable LLMs. However, realizing this potential requires addressing computational, ethical, and evaluation challenges through innovative benchmarking and interdisciplinary collaboration.  \n---\n\n### 7.6 Emerging Hybrid Approaches\n\n---\n### 7.6 Emerging Hybrid Approaches  \n\nBuilding upon the integration of continual learning (CL) with pre-training and fine-tuning paradigms discussed in Section 7.5, recent advances have introduced hybrid methodologies that combine multiple techniques to overcome the limitations of standalone approaches. These emerging frameworks synergize self-supervised learning, federated learning, and dynamic architectural adaptations to enhance plasticity, stability, and scalability in large language models (LLMs). Inspired by innovations like Adaptive-Solver and DaSLaM, these hybrids address critical CL challenges—catastrophic forgetting, task retention, and ethical alignment—while opening new research directions.  \n\n#### Self-Supervised Continual Learning  \n\nSelf-supervised learning (SSL) has become a cornerstone for CL by enabling models to learn robust representations from unlabeled data streams. This reduces dependence on annotated datasets while preserving adaptability. For instance, contrastive learning techniques, such as those explored in [291], align CL objectives with representation learning, ensuring models retain generalized knowledge across tasks. Hybrid frameworks combining SSL with replay-based methods (e.g., experience replay) further enhance stability by periodically revisiting past data distributions without violating privacy constraints [145].  \n\nA prominent application is the integration of generative pretraining with task-specific fine-tuning. Models like those in [79] use generative objectives (e.g., masked token reconstruction) during continual updates, reinforcing latent representations while adapting to new tasks. However, challenges persist in balancing generative and discriminative objectives, as noted in [188], where excessive focus on reconstruction can dilute task-specific performance.  \n\n#### Federated Learning for Decentralized CL  \n\nFederated learning (FL) extends CL capabilities by enabling decentralized training across heterogeneous data sources while preserving privacy. Hybrid FL-CL frameworks, such as those in [24], distribute model updates across edge devices, allowing LLMs to adapt to local data without centralized aggregation. This is particularly valuable for personalized applications like recommendation systems [29].  \n\nFL-CL hybrids address data heterogeneity and computational costs by leveraging parameter-efficient fine-tuning (PEFT) techniques (e.g., LoRA) to reduce communication overhead [85]. Dynamic architecture adaptations, such as task-specific sub-networks in DaSLaM, further optimize resource use [84]. However, trade-offs between convergence speed and fairness remain, as biased local updates can propagate global inequities [148].  \n\n#### Dynamic Architecture and Multi-Task Hybrids  \n\nDynamic architectures, such as Mixture-of-Experts (MoE) and modular networks, are increasingly integrated with CL to balance plasticity and stability. Systems like Adaptive-Solver employ MoE layers to route tasks to specialized experts, minimizing interference during incremental updates [292]. This approach excels in multilingual adaptation, where domain-specific experts mitigate catastrophic forgetting across languages.  \n\nMulti-task hybrids enhance CL by jointly optimizing shared and task-specific parameters. For example, [76] combines LoRA with knowledge distillation to transfer features across tasks while preserving individual performance—critical for high-stakes domains like healthcare. Scalability challenges persist, however, as excessive parameter growth can hinder deployment in resource-constrained environments [80].  \n\n#### Ethical and Robustness Considerations  \n\nHybrid CL must also address ethical risks, such as bias amplification and toxicity. Frameworks like [68] integrate bias mitigation modules into CL pipelines, using adversarial prompts to detect and correct harmful outputs. Similarly, [247] combines red teaming with CL to evaluate robustness against evolving adversarial attacks.  \n\nSelf-reflective hybrids, such as those in [83], leverage ensemble feedback for continual refinement. Human-in-the-loop validation, as advocated in [81], ensures alignment with ethical norms. However, over-constraining models for safety may suppress minority perspectives, necessitating balanced design [73].  \n\n#### Future Directions  \n\nThe future of hybrid CL lies in interdisciplinary innovation. Integrating neurosymbolic reasoning, as suggested in [212], could enhance interpretability and generalization. Meanwhile, [248] calls for frameworks that preserve semantic diversity without knowledge dilution.  \n\nIn summary, emerging hybrid approaches represent a transformative shift in CL for LLMs, combining SSL, FL, and dynamic architectures to address scalability, fairness, and robustness. As emphasized in [249], their success hinges on rigorous evaluation and ethical oversight to ensure responsible and inclusive evolution.  \n---\n\n## 8 Emerging Trends and Future Directions\n\n### 8.1 Self-Supervised Continual Learning\n\n---\n### 8.1 Self-Supervised Continual Learning  \n\nSelf-supervised learning (SSL) has emerged as a powerful paradigm for training models without explicit supervision, leveraging the inherent structure of data to learn robust representations. In the context of continual learning (CL) for large language models (LLMs), SSL techniques offer promising solutions to mitigate catastrophic forgetting and enhance adaptability to new tasks. This subsection explores the integration of SSL into CL frameworks, focusing on contrastive learning, generative pretraining, and their synergistic effects in preserving knowledge while accommodating new information.  \n\n#### **Foundations of Self-Supervised Continual Learning**  \nSelf-supervised continual learning combines the principles of SSL—where models learn from unlabeled data by predicting context or generating synthetic samples—with CL’s requirement to retain past knowledge. Unlike supervised CL, which relies on labeled task-specific data, SSL enables LLMs to extract generalized features from raw text or multimodal inputs, reducing dependency on task-specific annotations. This is particularly valuable in real-world scenarios where labeled data is scarce or expensive to acquire [15].  \n\nA key advantage of SSL in CL is its ability to decouple representation learning from task-specific fine-tuning. For instance, contrastive learning frameworks like SimCLR or MoCo learn invariant features by maximizing agreement between augmented views of the same data instance. When applied to CL, these frameworks can stabilize representations across tasks, minimizing interference between old and new knowledge [111]. Similarly, generative pretraining methods, such as masked language modeling (MLM) or autoregressive modeling, encourage LLMs to reconstruct corrupted inputs, fostering robust feature extraction that transcends task boundaries [1].  \n\n#### **Contrastive Learning for Catastrophic Forgetting Mitigation**  \nContrastive learning has shown remarkable success in reducing catastrophic forgetting by aligning representations of similar inputs while separating dissimilar ones. In CL, this is achieved through memory-augmented contrastive objectives that enforce consistency between current and past task representations. For example, [8] introduces a dynamic prototype-guided replay mechanism, where class prototypes derived from past tasks serve as anchors for contrastive loss calculations. This approach not only preserves task-specific decision boundaries but also enhances inter-task discrimination, mitigating forgetting.  \n\nAnother innovative application is found in [166], which leverages pretrained language models (PLMs) to generate semantic targets for contrastive alignment. By freezing these targets, the method ensures stable feature spaces across tasks, even when task identities are unknown. This hybrid SSL-CL framework demonstrates that semantic consistency, enforced through contrastive objectives, can significantly improve forward transfer and reduce representation drift.  \n\n#### **Generative Pretraining for Adaptability**  \nGenerative pretraining techniques, such as MLM or sequence-to-sequence modeling, offer a natural fit for CL by enabling LLMs to continually refine their understanding of language structure. In [7], MLM is adapted for domain-adaptive pretraining (DAP), where LLMs incrementally learn from unlabeled domain-specific corpora. By masking tokens and predicting them based on context, the model retains linguistic knowledge while adapting to new domains, effectively balancing stability and plasticity.  \n\nA notable extension is proposed in [293], where generative pretraining is combined with experience replay. The model periodically revisits synthetic samples generated from past task distributions, reinforcing old knowledge without explicit storage of raw data. This approach addresses computational constraints while maintaining performance, as generative replay avoids the memory overhead of traditional rehearsal-based methods [18].  \n\n#### **Hybrid Approaches and Theoretical Insights**  \nThe fusion of contrastive and generative SSL techniques has yielded hybrid frameworks that outperform standalone methods. For instance, [110] employs multi-layer contrastive distillation, where intermediate representations from a pretrained model are aligned across tasks using second-order statistics. This method capitalizes on the hierarchical nature of deep networks, ensuring that both low-level (e.g., syntactic) and high-level (e.g., semantic) features remain stable during incremental learning.  \n\nTheoretical studies, such as [16], provide insights into why SSL enhances CL. By analyzing information flow in neural networks, these works reveal that SSL objectives (e.g., contrastive or generative losses) implicitly regularize the Jacobian of feature mappings, reducing parameter sensitivity to task-specific perturbations. This regularization effect aligns with the plasticity-stability trade-off, making SSL a principled solution for CL [19].  \n\n#### **Challenges and Future Directions**  \nDespite its promise, self-supervised CL faces several challenges:  \n1. **Task Ambiguity**: SSL methods often assume task-agnostic data streams, but real-world CL scenarios may involve task boundaries or shifting objectives. Solutions like [195] propose task-aware SSL, where auxiliary heads predict task identities to guide representation learning.  \n2. **Scalability**: Large-scale SSL requires significant compute resources. Techniques like [260] explore parameter-efficient adaptations, such as random projections, to reduce overhead.  \n3. **Evaluation Metrics**: Current benchmarks lack standardized protocols for SSL-CL. [17] advocates for dynamic evaluation frameworks that measure both forgetting and adaptability in unlabeled settings.  \n\nFuture research could explore:  \n- **Cross-modal SSL**: Extending SSL-CL to multimodal data (e.g., text-image pairs) to leverage complementary information [294].  \n- **Meta-Learning SSL**: Combining meta-learning with SSL to optimize initialization for fast adaptation [294].  \n- **Neurosymbolic Integration**: Incorporating symbolic reasoning with SSL to improve interpretability and robustness [223].  \n\nIn summary, self-supervised continual learning represents a transformative direction for LLMs, offering scalable and generalizable solutions to catastrophic forgetting. By unifying contrastive and generative paradigms, future work can unlock lifelong learning systems capable of evolving alongside dynamic data streams. This foundation sets the stage for exploring hybrid models and federated learning integration in the next subsection, where decentralized and privacy-preserving adaptations of these principles are examined [1].  \n---\n\n### 8.2 Hybrid Models and Federated Learning Integration\n\n---\n### 8.2 Hybrid Models and Federated Learning Integration  \n\nThe integration of continual learning (CL) with federated learning (FL) represents a pivotal advancement for large language models (LLMs), addressing the dual challenges of lifelong adaptation and privacy-preserving decentralized training. Building on the self-supervised CL frameworks discussed in Section 8.1, this subsection explores how hybrid CL-FL systems enable LLMs to evolve dynamically across distributed data sources while mitigating catastrophic forgetting and preserving data sovereignty.  \n\n#### **Motivations for Hybrid CL-FL Systems**  \nThe fusion of CL and FL is driven by real-world demands for LLMs that can continuously adapt to non-IID data streams without centralized data access—a gap left by traditional FL frameworks that suffer from catastrophic forgetting in dynamic environments [199]. Conversely, while CL methods excel at incremental learning, their reliance on centralized data violates privacy constraints in applications like healthcare and education [118]. Hybrid CL-FL systems bridge this divide by enabling:  \n- **Personalized Adaptation**: User-specific fine-tuning without raw data sharing, as demonstrated by knowledge-augmented LLMs for contextual query suggestions [101].  \n- **Edge-Cloud Collaboration**: Distributed deployment of personalized services, exemplified by architectures like NetGPT that leverage edge resources for generative AI [262].  \n\nThese use cases align with the self-supervised principles in Section 8.1, where generalized feature learning reduces dependency on labeled data—a critical advantage in federated settings with sparse annotations.  \n\n#### **Methodologies for Combining CL and FL**  \nRecent innovations in hybrid CL-FL systems build upon three key paradigms:  \n\n1. **Memory-Augmented Federated Learning**: Clients employ local replay buffers or synthetic data generation to preserve task-specific knowledge during global aggregation. For instance, [21] introduces tool-assisted offloading to minimize memory overhead, complementing FL’s efficiency goals.  \n\n2. **Dynamic Expert Allocation**: Mixture-of-Experts (MoE) architectures, such as Lifelong-MoE, allocate domain-specific submodels to clients, balancing plasticity and stability in non-IID settings [116]. This mirrors the hierarchical representation stability achieved by self-supervised contrastive learning (Section 8.1).  \n\n3. **Parameter-Efficient Fine-Tuning (PEFT)**: Techniques like Low-Rank Adaptation (LoRA) enable lightweight client-side updates while preserving global model integrity. Studies in educational LLMs show LoRA’s efficacy for personalization in FL contexts [114], echoing the parameter regularization benefits of SSL objectives.  \n\nThese methodologies extend the self-supervised CL framework by incorporating decentralized constraints, setting the stage for lifelong and zero-shot adaptation (Section 8.3).  \n\n#### **Challenges and Open Problems**  \nKey barriers to scalable CL-FL integration include:  \n- **Non-IID Data Dynamics**: Client-specific distribution shifts exacerbate forgetting, necessitating human-in-the-loop validation for robust adaptation [115].  \n- **Privacy-Utility Trade-offs**: While FL avoids raw data sharing, model updates may leak sensitive information, requiring differential privacy or secure aggregation [29].  \n- **Benchmarking Gaps**: The lack of unified evaluation protocols for CL-FL systems hinders progress, as noted in [30].  \n\n#### **Future Directions**  \nEmerging opportunities include:  \n- **Cross-Client Knowledge Transfer**: Selective sharing of task-invariant features to enhance generalization, as proposed in [295].  \n- **Edge-Centric Hierarchies**: Lightweight CL at the edge coupled with cloud-based reasoning, aligning with 6G vision papers [84].  \n\nThis hybrid paradigm not only addresses the limitations of standalone CL and FL but also lays the groundwork for the zero-shot adaptation strategies discussed in Section 8.3, where LLMs must generalize to unseen tasks in decentralized environments.\n\n### 8.3 Lifelong Learning and Zero-Shot Adaptation\n\n### 8.3 Lifelong Learning and Zero-Shot Adaptation  \n\nBuilding upon the hybrid CL-FL integration discussed in Section 8.2, lifelong learning (LL) and zero-shot adaptation represent critical advancements in continual learning (CL) for large language models (LLMs). These paradigms enable LLMs to adapt to novel tasks and dynamic environments without extensive retraining—addressing key challenges in federated and decentralized settings where data distributions evolve unpredictably. This section examines the methodologies, challenges, and future directions of these approaches, which bridge the gap between incremental knowledge acquisition and real-world deployment needs.  \n\n#### **Lifelong Learning Strategies for LLMs**  \nLifelong learning extends traditional CL by enabling models to accumulate knowledge over extended periods while maintaining performance on prior tasks. A central challenge lies in balancing stability (retaining old knowledge) and plasticity (acquiring new knowledge), a tension exacerbated in distributed environments like those discussed in Section 8.2. Recent innovations address this through architectural and algorithmic solutions:  \n- **Memory-Decoupled Architectures**: [34] proposes a two-level framework where an inference network handles new knowledge acquisition while a generative network manages past knowledge recall, mirroring biological memory systems.  \n- **Parameter-Efficient Fine-Tuning (PEFT)**: Methods like Low-Rank Adaptation (LoRA) enable incremental updates without overwriting foundational knowledge, aligning with FL's resource constraints [39]. Hybrid approaches, such as [124], further optimize this balance by freezing feature extractors for stability while fine-tuning task-specific layers.  \n- **Dynamic Regularization**: Techniques like knowledge distillation and uncertainty-based weighting [121] mitigate interference between tasks. Intriguingly, [36] reveals that dropout mechanisms implicitly create task-specific pathways, reducing catastrophic forgetting.  \n\nThese strategies complement the FL-oriented solutions in Section 8.2, offering scalable ways to preserve knowledge across heterogeneous clients while adapting to local data shifts.  \n\n#### **Zero-Shot Adaptation in Dynamic Environments**  \nZero-shot adaptation pushes lifelong learning further by enabling LLMs to generalize to unseen tasks without explicit retraining—a capability critical for applications like personalized AI (Section 8.2) and evolving benchmarks (Section 8.4). Key advancements include:  \n- **Prompt-Based Decoupling**: [35] uses prompt-tuning to separate task-specific and task-agnostic knowledge, enabling zero-shot generalization while maintaining stability.  \n- **Self-Supervised Hybrid Learning**: Combining replay-based methods with self-supervised objectives enhances robustness to distribution shifts [125], a finding supported by analyses linking plasticity loss to curvature changes in the loss landscape.  \n- **Meta-Learning Frameworks**: Approaches like [128] optimize for both task-specific and task-agnostic performance, while [296] leverages cause-effect simulations for cross-task generalization.  \n\nThese methods align with the need for adaptive evaluation protocols highlighted in Section 8.4, as they enable models to handle unseen tasks in benchmarks like *EvolvingQA* and *TRACE*.  \n\n#### **Challenges and Future Directions**  \nDespite progress, critical gaps remain:  \n1. **Scalability**: Computational and memory constraints hinder deployment in resource-limited settings [227], echoing the FL challenges in Section 8.2.  \n2. **Evaluation Standardization**: Current metrics lack granularity for zero-shot scenarios [201], necessitating benchmarks that integrate with emerging protocols (Section 8.4).  \n3. **Cross-Modal and Ethical Considerations**: Extending these approaches to multimodal settings [42] and addressing biases [41] are urgent priorities.  \n\nFuture work should focus on:  \n- **Dynamic Architecture Adaptation**: Allocating resources based on task complexity and client capabilities in FL settings.  \n- **Unified Evaluation Frameworks**: Bridging gaps between CL, FL, and zero-shot benchmarking (Section 8.4).  \n- **Ethical Deployment**: Mitigating bias propagation across tasks in federated environments.  \n\n#### **Conclusion**  \nLifelong learning and zero-shot adaptation are transformative for LLMs, offering pathways to continuous adaptation in dynamic, privacy-sensitive environments. By integrating PEFT, meta-learning, and self-supervised techniques, these approaches address the stability-plasticity trade-off while aligning with the broader goals of federated and benchmark-driven CL. As emphasized in [44], the synergy between architectural innovation and decentralized learning frameworks will define the next generation of adaptable, ethically robust LLMs.\n\n### 8.4 Emerging Benchmarks and Evaluation Protocols\n\n---\n### 8.4 Emerging Benchmarks and Evaluation Protocols  \n\nAs discussed in Section 8.3, lifelong learning and zero-shot adaptation in large language models (LLMs) require robust evaluation frameworks to measure their ability to retain knowledge and adapt to dynamic environments. The rapid advancement of continual learning (CL) in LLMs has necessitated specialized benchmarks that address unique challenges like catastrophic forgetting, plasticity-stability trade-offs, and domain shifts—complementing the interdisciplinary applications explored in Section 8.5. This subsection examines these emerging benchmarks and evaluation protocols, highlighting their design principles, applications, and limitations in assessing LLMs' continual learning capabilities.  \n\n#### **Robustness in Dynamic Environments**  \nBuilding on the stability-plasticity balance emphasized in Section 8.3, benchmarks like *LongICLBench* and *EvolvingQA* evaluate LLMs' resilience to evolving data distributions and adversarial conditions [1]. *EvolvingQA* simulates real-world temporal shifts by incorporating time-sensitive questions, testing models' ability to integrate new information while retaining historical context—a capability critical for applications like healthcare (Section 8.5). Similarly, *RefuteBench* assesses adversarial robustness by measuring logical consistency against contradictory prompts [297], addressing ethical concerns raised in Section 8.3.  \n\n#### **Task Retention and Forward/Backward Transfer**  \nAligning with the lifelong learning strategies in Section 8.3, benchmarks such as *TRACE* and *CodeTask-CL* quantify task retention and knowledge transfer across sequential tasks. *TRACE* sequences multilingual or domain-specific tasks (e.g., legal language adaptation) to measure backward transfer—how learning new tasks affects prior performance [1]. *CodeTask-CL* reveals that parameter-efficient methods like LoRA, discussed in Section 8.3, mitigate catastrophic forgetting in programming tasks by preserving foundational knowledge [1].  \n\n#### **Domain-Specific Evaluation Frameworks**  \nSpecialized benchmarks address the domain adaptation challenges highlighted in Section 8.5. For healthcare, *MedAgents* evaluates incremental updates to clinical knowledge while penalizing hallucinations [59]. In law, *LAiW* tests temporal adaptation to evolving jurisprudence [51], mirroring the need for dynamic evaluation in robotics and education (Section 8.5). These frameworks bridge the gap between CL research and real-world deployment.  \n\n#### **Dynamic and Adaptive Benchmarking**  \nTo overcome the limitations of static benchmarks, innovations like *Benchmark Self-Evolving* and *CAT-inspired* adaptive testing enable real-time evaluation. *Benchmark Self-Evolving* uses LLMs to dynamically generate tasks, reducing overfitting risks [129], while *CAT-inspired* methods adjust task difficulty based on model performance [57]. These approaches align with the need for scalable, adaptive systems in federated and multimodal settings (Sections 8.3 and 8.5).  \n\n#### **Challenges and Future Directions**  \nCurrent benchmarks lack granularity in measuring per-task forgetting and underrepresent multimodal CL, despite its growing relevance in healthcare and robotics (Section 8.5). Future work should:  \n1. **Integrate Federated Learning Scenarios**: Assess decentralized CL performance, as noted in Section 8.3 [55].  \n2. **Develop Interdisciplinary Benchmarks**: Combine linguistic, spatial, and causal reasoning for holistic evaluation [232].  \n3. **Expand Multimodal Evaluation**: Address gaps in vision-language CL [285].  \n\n#### **Conclusion**  \nEmerging benchmarks like *TRACE* and *EvolvingQA* provide critical tools for evaluating CL in LLMs, bridging theoretical research (Section 8.3) and practical applications (Section 8.5). However, the field must evolve to address dynamic, multimodal, and federated learning challenges. By advancing adaptive and domain-specific evaluation frameworks, these benchmarks will ensure LLMs meet the demands of real-world continual learning.  \n---\n\n### 8.5 Interdisciplinary Applications\n\n---\n### 8.5 Interdisciplinary Applications of Continual Learning in LLMs  \n\nThe integration of continual learning (CL) capabilities into large language models (LLMs) has enabled transformative applications across diverse domains, where adaptability to evolving knowledge and tasks is paramount. This subsection examines how CL-enhanced LLMs are driving innovation in healthcare, robotics, and personalized education, while highlighting key challenges and synergies revealed by recent benchmarks and research.  \n\n#### **Healthcare: Adaptive Clinical Decision Support**  \nCL-equipped LLMs are revolutionizing healthcare by dynamically incorporating the latest medical research, clinical guidelines, and patient data to enhance diagnostic accuracy and treatment planning. The [298] benchmark demonstrates how domain-specific fine-tuning enables LLMs to excel in clinical tasks such as medical summarization and patient consultation. However, catastrophic forgetting—where models lose prior medical knowledge when trained on new data—remains a critical challenge. The [184] framework addresses this through multi-agent collaboration, where specialized agents retain expertise in distinct medical domains while incrementally sharing knowledge. This aligns with findings from [299], which emphasizes the risks of hallucinations in medical self-diagnosis and the need for stability in CL systems.  \n\nMultimodal CL is gaining traction in healthcare, as evidenced by [138], where LLMs integrate electronic health records (EHRs) and imaging data. Such systems must handle non-stationary data distributions, a challenge highlighted in [234], which proposes replay buffers and domain adaptation as mitigation strategies. The [298] benchmark further validates CL's potential in medical NLP, showing that multitask prompt tuning improves adaptability across diverse clinical tasks. These advances position CL-enabled LLMs as key enablers of personalized medicine, where models can continuously update patient-specific knowledge without full retraining.  \n\n#### **Robotics: Lifelong Learning for Autonomous Systems**  \nIn robotics, CL empowers LLMs to serve as adaptive \"brains\" for autonomous systems, enabling them to interpret natural language instructions, learn new tasks, and recover from errors in dynamic environments. The [184] benchmark evaluates LLMs as agents in interactive settings, revealing gaps in long-term reasoning and decision-making—critical capabilities for robotics. For instance, a robot trained for object assembly must retain procedural knowledge while acquiring new techniques, a scenario where CL methods like [251] could mitigate forgetting through meta-rationale integration.  \n\nThe [300] benchmark provides insights into CL for autonomous vehicles, where LLMs must memorize traffic rules while adapting to real-time road conditions. However, as [66] notes, deploying CL in resource-constrained robotic systems demands computational efficiency optimizations, such as quantization and parameter-efficient fine-tuning. Future research could explore hybrid architectures combining LLMs with symbolic reasoning to balance plasticity and stability in robotic tasks.  \n\n#### **Personalized Education: Adaptive Learning Systems**  \nCL enables LLMs to deliver personalized education by incrementally adapting to individual learners' progress and needs. The [301] framework demonstrates how LLMs can assess student proficiency in mathematics and adjust instruction over time, aligning with the multitask capabilities shown in [137]. The [242] benchmark underscores the importance of balanced knowledge retention across psychology subdomains for accurate tutoring.  \n\nChallenges persist in ensuring fairness and mitigating bias, as highlighted by [241], which evaluates LLMs for social bias. Educational CL systems must dynamically accommodate cultural and linguistic diversity, an area explored in [302]. Techniques like [303] could help distill inclusive knowledge while preserving core educational content. Additionally, [67] proposes dynamic evaluation frameworks to test LLMs' adaptability to evolving curricula, ensuring alignment with pedagogical goals.  \n\n#### **Cross-Domain Synergies and Future Directions**  \nThe interdisciplinary potential of CL in LLMs lies in cross-domain synergies. For example, healthcare robotics could integrate CL techniques from medical LLMs and robotic benchmarks, while educational robots might leverage [304] to refine tutoring strategies based on student feedback. The [138] benchmark, which evaluates LLMs in repository-level machine learning tasks, exemplifies how CL can bridge lab-scale benchmarks and real-world applications.  \n\nFuture research must address scalability and ethical concerns. As [299] notes, CL systems must comply with evolving regulations, especially in high-stakes domains like healthcare and education. Interdisciplinary collaboration, as advocated in [239], is essential to develop robust, equitable CL frameworks. Benchmarks like [251] and [239] provide evaluation blueprints, but standardized metrics for interdisciplinary applications remain a gap.  \n\n#### **Conclusion**  \nCL-enabled LLMs are transforming healthcare, robotics, and education by enabling adaptive, context-aware systems. However, realizing their full potential requires overcoming technical challenges—such as catastrophic forgetting and computational inefficiency—while addressing ethical and regulatory considerations. By leveraging insights from domain-specific benchmarks and fostering collaboration between AI researchers and domain experts, CL can drive responsible innovation across disciplines.  \n\n---\n\n### 8.6 Open Research Questions\n\n---\n### 8.6 Open Research Questions  \n\nThe rapid advancement of continual learning (CL) in large language models (LLMs) has opened new frontiers across applications, as highlighted in Section 8.5. However, critical gaps remain that hinder the full realization of CL's potential. This subsection systematically examines unresolved challenges—spanning lifelong generalization, computational efficiency, human feedback integration, ethical alignment, and robustness—while proposing actionable research directions to bridge these gaps.  \n\n#### **Lifelong Generalization: Beyond Short-Term Adaptation**  \nWhile current CL methods demonstrate promising results on sequential task benchmarks, their ability to sustain performance over extended periods—a requirement for real-world deployment—remains unproven. A core limitation lies in evaluation protocols: most benchmarks (e.g., [298], [184]) test CL over limited task sequences, failing to simulate lifelong scenarios like evolving medical knowledge or legal precedents. Future work should prioritize longitudinal benchmarks that mirror decade-scale learning, coupled with hybrid architectures combining neural plasticity with symbolic reasoning for stable knowledge retention.  \n\nMeta-learning presents an underexplored avenue for lifelong generalization. Frameworks like [89] suggest that models can learn to self-adapt their learning strategies, but their application to CL scenarios remains nascent. Research could investigate meta-continual algorithms where LLMs dynamically adjust update rules based on task novelty and criticality.  \n\n#### **Computational Efficiency: Scaling CL Sustainably**  \nThe resource intensity of CL methods poses a significant barrier, particularly for edge deployment. Replay buffers and dynamic parameter expansion, while effective, incur substantial memory and energy costs [24]. Key questions include:  \n- Can sparsity-aware training (e.g., [92]) reduce CL overhead while preserving plasticity?  \n- How might federated CL architectures balance communication costs with distributed learning?  \n\nEmerging techniques like self-supervised continual pre-training and quantization-aware fine-tuning offer pathways to efficiency. For instance, [66] highlights optimization opportunities in gradient computation—insights that could be adapted to CL pipelines.  \n\n#### **Human-in-the-Loop Feedback: Aligning Dynamic Learning**  \nAs CL systems increasingly influence high-stakes domains (e.g., healthcare, education), scalable mechanisms for human oversight become essential. Current approaches often rely on static alignment or ad hoc feedback, risking misalignment as models evolve [29]. Future frameworks should:  \n- Prioritize feedback based on task risk (e.g., continuous validation for medical CL vs. periodic audits for creative tasks).  \n- Integrate participatory design, building on [91] to engage stakeholders in shaping model behavior.  \n\nAdaptive HITL systems could leverage uncertainty estimation to trigger human intervention only when necessary, optimizing resource use while maintaining accountability.  \n\n#### **Ethical Alignment: Dynamic Bias Mitigation**  \nCL introduces unique ethical challenges as models absorb shifting societal norms. Static bias evaluations (e.g., [68]) fail to capture emergent harms in continual settings. Research must develop:  \n- Real-time bias monitoring tools, extending frameworks like [188].  \n- Fairness-aware CL algorithms that explicitly constrain updates to prevent discrimination, inspired by [75].  \n\nInterdisciplinary collaboration is critical to define culturally inclusive metrics and governance protocols, particularly for global deployments.  \n\n#### **Robustness and Security: Defending Evolving Models**  \nCL systems face heightened vulnerability to adversarial attacks due to their incremental nature. Proactive measures are needed to:  \n- Detect data poisoning in real-time, building on red-teaming insights from [247].  \n- Balance plasticity with security, as explored in [80].  \n\nFrameworks like [274] could inform risk-adaptive CL strategies, where update rigidity scales with threat likelihood.  \n\n#### **Conclusion and Future Directions**  \nAddressing these open questions demands coordinated progress across five axes:  \n1. **Benchmarks & Architectures**: Develop lifelong evaluation protocols and hybrid neuro-symbolic systems.  \n2. **Efficiency**: Innovate in sparse training, federated CL, and self-supervised continual pre-training.  \n3. **Human Alignment**: Design adaptive HITL frameworks with risk-proportional oversight.  \n4. **Ethics**: Establish dynamic bias monitoring and fairness-constrained learning.  \n5. **Security**: Integrate adversarial resilience into CL pipelines.  \n\nBy advancing these priorities, the field can unlock CL's potential to create LLMs that are not only adaptable and efficient but also ethically grounded and secure—fulfilling the interdisciplinary promise outlined in Section 8.5 while paving the way for the challenges discussed in subsequent sections.  \n---\n\n## 9 Conclusion and Practical Recommendations\n\n### 9.1 Summary of Key Findings\n\n---\nThis survey has systematically explored the landscape of continual learning (CL) for large language models (LLMs), highlighting both the significant advancements and persistent challenges in the field. Below, we consolidate the major insights, focusing on methodological breakthroughs, fundamental challenges, and the delicate balance between plasticity and stability in CL for LLMs—a critical foundation for the actionable recommendations discussed in the next section.\n\n### **Advancements in Continual Learning Methodologies for LLMs**  \nRecent progress in CL for LLMs has been driven by three key methodological innovations:  \n\n1. **Parameter-Efficient Fine-Tuning (PEFT)**: Techniques like Low-Rank Adaptation (LoRA) and its variants (e.g., rsLoRA, SoRA, DoRA) [12] enable efficient incremental updates with minimal computational overhead. These methods address scalability challenges while preserving prior knowledge [6].  \n\n2. **Replay-Based Optimization**: Experience Replay (ER) and dynamic memory replay mitigate catastrophic forgetting by retaining historical data. Innovations like gradient-based replay and prototype-guided memory replay [8] optimize storage by selectively preserving representative samples, balancing memory efficiency and performance [167].  \n\n3. **Dynamic Architecture Adaptation**: Modular approaches, such as Mixture-of-Experts (MoE) and task-specialized networks (e.g., MoLA, X-LoRA) [261], allow LLMs to adapt flexibly to heterogeneous tasks. These excel in both vertical (general-to-specific) and horizontal (cross-domain/temporal) CL scenarios [1], enabling applications from multilingual adaptation to personalized recommendations [223].  \n\n### **Persistent Challenges**  \nDespite these advancements, CL for LLMs faces unresolved hurdles:  \n\n- **Catastrophic Forgetting**: LLMs often lose prior knowledge when adapting to new tasks, especially with limited replay memory or high task dissimilarity [2]. Theoretical work shows perfect retention is NP-hard [19], complicating the plasticity-stability trade-off [305].  \n\n- **Resource Constraints**: Incremental training demands substantial memory and energy [6]. While quantization and distributed training help [196], performance-efficiency trade-offs persist.  \n\n- **Data Heterogeneity**: Non-stationary data streams introduce bias and instability [220]. Replay buffers and domain adaptation offer partial solutions but lack universal effectiveness [5].  \n\n### **Balancing Plasticity and Stability**  \nAchieving equilibrium between adaptation and retention remains central:  \n- **Regularization Methods**: Techniques like elastic weight consolidation (EWC) protect critical parameters but struggle in class-incremental settings with blurred task boundaries [13].  \n- **Hybrid Approaches**: Combining replay, regularization, and architectural adaptations (e.g., knowledge distillation [8]) shows promise. Self-supervised learning further enhances robustness [111].  \n\n### **Breakthroughs and Future Directions**  \nRecent innovations include:  \n- **Pre-trained Models (PTMs) in CL**: Leveraging general-purpose representations for incremental learning [7].  \n- **Refresh Learning**: Mimicking neurobiological processes to counteract negative drift [281].  \n- **Standardized Benchmarks**: Frameworks like Sequoia and CITB enable reproducible evaluation [224; 17].  \n\nFuture work must prioritize scalable, ethical CL systems, exploring lifelong learning and zero-shot adaptation [1]. Interdisciplinary collaboration—spanning ethics, policy, and domain-specific applications (e.g., healthcare, robotics) [3; 15]—will be essential to realize the potential of continually evolving LLMs.  \n\n---\n\n### 9.2 Actionable Insights for Practitioners\n\n---\n### 9.2 Actionable Insights for Practitioners  \n\nBuilding on the methodological advancements and challenges outlined in the previous section, this subsection provides concrete strategies for implementing continual learning (CL) in large language models (LLMs). We organize these insights into seven key areas, each addressing critical aspects of practical deployment while maintaining alignment with the interdisciplinary ethical considerations discussed in the subsequent section.  \n\n#### **1. Benchmark Selection and Evaluation**  \nEffective CL implementation begins with rigorous evaluation. Practitioners should prioritize benchmarks that assess long-term adaptability, such as *LongICLBench* and *EvolvingQA* [30], which measure forward/backward transfer and robustness to domain shifts. For specialized domains (e.g., healthcare or law), task-specific frameworks like *LawBench* or *MedAgents* [306; 118] provide targeted performance metrics. Dynamic benchmarks such as *EvoEval* [30] further enable real-time adaptation tracking, ensuring models remain effective in evolving environments.  \n\n#### **2. Hybrid Methodologies for Efficiency and Stability**  \nTo balance plasticity and stability—a core challenge highlighted earlier—practitioners should adopt hybrid approaches:  \n- **PEFT + Knowledge Distillation**: Low-Rank Adaptation (LoRA) [97] reduces computational costs, while knowledge distillation [101] preserves prior knowledge. Frameworks like *MultiLoRA* [116] extend this to multi-task settings.  \n- **Replay + Regularization**: Experience replay buffers [114] combined with consistency regularization mitigate forgetting and overfitting. Integrated tools like *Hydra* [116] streamline deployment.  \n- **Dynamic Architectures**: Mixture-of-Experts (MoE) [116] dynamically allocates resources, optimizing performance for heterogeneous tasks.  \n\n#### **3. Addressing Computational Constraints**  \nResource efficiency is critical for scalable CL. Practical solutions include:  \n- **Quantization and Distributed Training**: Weight quantization [24] reduces memory overhead, while federated learning [199] enables collaborative training.  \n- **Edge Deployment**: Lightweight LLMs on edge devices [84] support real-time applications like healthcare diagnostics.  \n- **Incremental Expansion**: Approaches like *Lifelong-MoE* [116] add domain-specific modules without full retraining.  \n\n#### **4. Ethical and Practical Mitigations**  \nAligning with the ethical imperatives discussed in the next section, practitioners should:  \n- **Combat Bias**: Use fairness-aware datasets [200] and auditing tools like *FUTURE-AI* [20].  \n- **Ensure Privacy**: Federated learning [199] and differential privacy [20] safeguard sensitive data.  \n- **Enhance Transparency**: Implement layered audits [20], especially in high-stakes domains like law [306].  \n\n#### **5. Domain-Specific Customization**  \nTailoring CL strategies enhances performance:  \n- **Healthcare**: Retrieval-augmented generation (RAG) [225] with EHR data improves diagnostic accuracy, as demonstrated by *MedAgents* [118].  \n- **Legal**: Combining *Lawyer LLaMA* [306] with logic-based decision trees automates legal workflows while maintaining confidentiality.  \n- **Education**: Adaptive tutoring systems leverage benchmarks like *CSEPrompts* [307].  \n\n#### **6. Human-in-the-Loop Integration**  \nHuman oversight ensures reliability:  \n- **Self-Correction**: Tools like *CLEAR* [82] enable models to detect and rectify errors.  \n- **Interactive Feedback**: Platforms such as *NetLogo Chat* [308] allow real-time model refinement.  \n\n#### **7. Future-Proofing Strategies**  \nTo anticipate emerging challenges:  \n- **Evolving Benchmarks**: Adopt self-updating benchmarks like *Benchmark Self-Evolving* [30].  \n- **Interdisciplinary Collaboration**: Partner with domain experts [306] and policymakers [20] to align CL with societal needs.  \n\n### **Summary of Recommendations**  \n| **Focus Area**       | **Actionable Insight**                                                                 | **Key Citations**                                                                 |  \n|-----------------------|---------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|  \n| Benchmarking          | Use *LongICLBench* and *EvoEval* for dynamic evaluation.                              | [30]                            |  \n| Hybrid Methods        | Combine LoRA with knowledge distillation for efficiency.                              | [97] |  \n| Resource Optimization | Quantize models and deploy at the edge.                                               | [24] |  \n| Ethical Mitigations   | Audit models with *FUTURE-AI* frameworks.                                             | [20] |  \n| Domain Adaptation     | Customize CL for healthcare using RAG.                                                | [225]                    |  \n\nThese strategies equip practitioners to deploy CL-enabled LLMs that are robust, adaptable, and ethically grounded, bridging the gap between theoretical advancements and real-world application.  \n---\n\n### 9.3 Interdisciplinary Collaboration and Ethical Considerations\n\n### 9.3 Interdisciplinary Foundations for Ethical Continual Learning  \n\nThe development of continual learning (CL) systems for large language models (LLMs) requires integrating insights from artificial intelligence, cognitive science, and ethics to address critical challenges such as bias propagation, transparency, and accountability. This interdisciplinary approach is essential for designing CL systems that are both technically robust and socially responsible.  \n\n#### **1. Bridging AI and Cognitive Science**  \nThe stability-plasticity dilemma—a core challenge in CL—mirrors biological learning systems, where the brain balances retaining old knowledge with acquiring new information [33; 36]. Cognitive science offers frameworks like synaptic metaplasticity, which dynamically adjusts learning rates based on historical activity [123]. These principles can inspire CL algorithms that mitigate catastrophic forgetting, a phenomenon where models lose previously learned knowledge when trained on new tasks [31]. For example, replay-based methods and generative adversarial meta-models emulate biological rehearsal mechanisms to stabilize knowledge retention [34].  \n\n#### **2. Ethical Imperatives in High-Stakes Domains**  \nIn domains like healthcare and law, biased or opaque CL systems risk perpetuating harm. Studies show that LLMs fine-tuned on biased medical data can propagate discriminatory practices, particularly when performance varies across institutions [40]. Frameworks like *FUTURE-AI* emphasize fairness, transparency, and robustness, which are critical for deploying CL systems in sensitive applications [178]. Ethical integration also demands addressing privacy concerns, such as those arising from replay-based methods that store past data. Federated learning offers decentralized solutions but introduces challenges in balancing global stability with local fairness [179].  \n\n#### **3. Transparency and Accountability Mechanisms**  \nCL systems often operate as black boxes, complicating audits of their decision-making processes. Techniques like attention-based plasticity [194] and explainable AI (XAI) frameworks can enhance interpretability, though their adoption requires interdisciplinary collaboration. Current benchmarks (e.g., *LongICLBench*, *EvolvingQA*) prioritize accuracy and efficiency but often neglect fairness and bias [201]. Holistic evaluation frameworks must assess both performance metrics and societal impact, particularly in healthcare, where CL systems should generalize equitably across demographic groups [122].  \n\n#### **4. Mitigating Bias Through Interdisciplinary Strategies**  \nSequential task learning can amplify biases if CL methods overlook dataset skews. For instance, spurious correlations may propagate across tasks, exacerbating discrimination [41]. Cognitive science-inspired debiasing strategies, such as attention modulation, can complement ethical guidelines for equitable data sampling. Hybrid approaches like *Group-class Balanced Greedy Sampling (BGS)* demonstrate how interdisciplinary insights reduce bias without sacrificing plasticity [41].  \n\n#### **5. Scalability and Policy Alignment**  \nSustainable CL deployment requires energy-efficient training methods [37] and adaptive regulatory frameworks. Policymakers must address dynamic data distributions and lifelong learning challenges, as exemplified by research on federated unlearning, which balances stability, fairness, and efficiency [179].  \n\n#### **Conclusion**  \nInterdisciplinary collaboration is vital to advance CL systems that are technically proficient and ethically sound. By synthesizing AI innovations, cognitive science principles, and ethical frameworks, researchers can mitigate risks like bias and opacity while ensuring societal benefit. Future work should prioritize unified benchmarks and policy guidelines to align CL advancements with real-world needs.\n\n### 9.4 Recommendations for Robust AI Systems\n\n---\n### 9.4 Recommendations for Robust AI Systems  \n\nBuilding upon the interdisciplinary foundations for ethical continual learning discussed in Section 9.3, this section presents actionable recommendations to enhance the robustness of continual learning (CL) in large language models (LLMs). These strategies address critical challenges such as hallucinations, reasoning failures, and ethical concerns while ensuring seamless integration with the scalability, equity, and human alignment pillars explored in subsequent sections.  \n\n#### **1. Multi-Agent Collaboration for Domain-Specific Robustness**  \nMulti-agent systems offer a promising paradigm to distribute tasks across specialized agents, improving accuracy and reducing errors. For instance, [59] demonstrates how LLM-based agents collaborate in medical reasoning through role-playing frameworks. This approach generalizes to high-stakes domains like law and finance, where domain-specific agents (e.g., for diagnosis, summarization, or legal analysis) cross-validate outputs to minimize hallucinations.  \n\nComplementing this, [47] highlights the synergy between general-purpose LLMs and smaller, domain-specific models. Such hybrid architectures balance broad reasoning capabilities with precision, enabling dynamic adaptation to evolving knowledge—a critical requirement for robust CL systems.  \n\n#### **2. Auditing Mechanisms for Transparency and Accountability**  \nRobust CL systems require rigorous auditing across three layers:  \n- **Data Audits**: Ensure training data diversity and fairness to prevent bias propagation. [62] underscores the need for culturally inclusive datasets to align LLMs with global ethical standards.  \n- **Model Audits**: Evaluate architectural adaptations and CL methodologies. [55] proposes federated auditing to verify model updates in distributed settings while preserving privacy.  \n- **Output Audits**: Implement post-hoc validation via human review or automated detectors. [230] introduces real-time safeguards against harmful content, aligning with the transparency principles in [61].  \n\n#### **3. Human-AI Teaming for Error Mitigation**  \nHuman oversight remains indispensable in high-risk scenarios. Two strategies stand out:  \n- **Interactive Refinement**: [297] leverages iterative human feedback to refine outputs, ensuring adherence to domain-specific constraints (e.g., medical or legal guidelines).  \n- **Role-Based Prompting**: [56] minimizes inter-domain confusion by assigning expert roles (e.g., \"legal advisor\"), reducing hallucinations in tasks like legal judgment prediction [46].  \n\nFurther, [309] demonstrates how LLMs simulate domain experts to preserve nuanced context, bridging the gap between generic outputs and specialized requirements.  \n\n#### **4. Dynamic Benchmarking and Adaptive Evaluation**  \nCL systems must be evaluated against benchmarks that simulate real-world distribution shifts. [229] and [57] advocate for multi-dimensional testing (e.g., spatial reasoning, response constraints) to measure adaptability. Frameworks like [310] further assess lateral thinking, ensuring robustness extends beyond static tasks.  \n\n#### **5. Federated and Privacy-Preserving Learning**  \nVertical federated learning (VFL) enables privacy-centric CL by training on distributed data without exposing raw inputs. While [130] outlines VFL’s potential, [183] reveals vulnerabilities, necessitating secure aggregation protocols. Hybrid approaches like [311] combine VFL with cryptographic techniques, balancing robustness with confidentiality.  \n\n#### **6. Ethical and Regulatory Alignment**  \nRobustness must align with legal and ethical standards. [51] emphasizes domain-specific fine-tuning to avoid regulatory violations, while [267] introduces hierarchical instruction-following to thwart adversarial prompts.  \n\n#### **Conclusion**  \nThese recommendations—spanning multi-agent collaboration, auditing, human oversight, dynamic evaluation, and privacy-preserving techniques—provide a roadmap for deploying robust CL-enabled LLMs. By integrating these strategies with the ethical foundations of Section 9.3 and the scalability challenges in subsequent sections, practitioners can mitigate risks while ensuring responsible, adaptive AI systems. Future work should prioritize standardized robustness metrics and interdisciplinary collaboration to address emerging CL challenges.  \n---\n\n### 9.5 Final Reflections on the Future of CL in LLMs\n\n---\nThe field of continual learning (CL) in large language models (LLMs) stands at a pivotal juncture, with transformative potential to redefine how AI systems adapt, evolve, and integrate into real-world applications. Building upon the robustness recommendations outlined in the previous section, we now examine the future trajectory of CL-enabled LLMs through three critical lenses: scalability, equity, and human alignment. These pillars not only address technical challenges but also reflect broader societal imperatives as LLMs permeate high-stakes domains like healthcare, legal practice, and multilingual communication [136].\n\n### Scalability: Beyond Computational Limits  \nCurrent CL methods face significant scalability hurdles, particularly when updating billion-parameter models, as highlighted by studies on computational overhead [66]. While parameter-efficient fine-tuning (PEFT) and dynamic architectures [137] show promise, their real-world deployment remains constrained by hardware limitations and energy inefficiencies. Modular approaches like MultiLoRA and Hydra demonstrate cost-effective retraining, yet their performance in production environments—especially on edge devices—requires rigorous validation. Emerging benchmarks such as DevEval [140] emphasize the need for CL systems that scale not just in model size but also in task complexity, aligning with the energy-conscious paradigms discussed in [268].\n\n### Equity: Bridging Domain and Linguistic Gaps  \nThe equitable deployment of CL-enabled LLMs demands urgent attention to biases and disparities in data representation. Benchmarks like TRACE [63] and CodeScope [209] reveal stark performance gaps across languages and domains, with low-resource settings disproportionately affected. For instance, while LLMs excel in English-centric tasks, their adaptability to multilingual legal or medical contexts remains inconsistent—a challenge exacerbated by the scarcity of high-quality CL benchmarks for non-Latin scripts [70]. Addressing this requires investments in culturally adaptive evaluation frameworks like Ada-LEval [186], coupled with interdisciplinary efforts to prevent CL systems from perpetuating existing inequalities.\n\n### Human Alignment: Ethical and Practical Imperatives  \nAligning CL systems with human values and safety standards presents a multifaceted challenge, particularly given risks like bias propagation and misuse in critical applications [68; 208]. Building on the auditing mechanisms proposed earlier, human-in-the-loop oversight and ethical safeguards are essential to mitigate failures observed in legal and medical LLMs [136]. Benchmarks such as CriticBench [312] further underscore the need for self-critiquing models that align outputs with ethical guidelines. Future CL systems must integrate transparency frameworks [185] to ensure traceability of incremental updates and compliance with evolving regulations like the EU AI Act.\n\n### Policy and Industry: Catalysts for Responsible Adoption  \nThe responsible advancement of CL-enabled LLMs hinges on synergistic efforts between policymakers and industry stakeholders. Standardized evaluation platforms like AgentBench [184] can assess CL capabilities alongside safety metrics, while open ecosystems such as Chakra [313] enable collaborative benchmarking. Lessons from data contamination studies [233; 269] highlight the need for stricter governance around dataset provenance—a gap addressed by adaptive benchmarking initiatives like Benchmark Self-Evolving [67]. Industry must balance innovation with ethical deployment, leveraging tools like WaterBench [314] to maintain model integrity.\n\n### A Call to Action  \nThe future of CL in LLMs transcends technical innovation, demanding collective commitment to scalable, equitable, and human-aligned systems. Researchers must bridge the lab-to-real-world gap, as evidenced by repository-level challenges in [138]. Policymakers should adopt frameworks like the maturity model in [315] to foster responsible governance. By uniting technical rigor with ethical stewardship, the CL community can realize LLMs' potential to democratize AI and drive inclusive progress—ushering in an era where adaptive intelligence serves as a force for global good.  \n---\n\n\n## References\n\n[1] Continual Learning of Large Language Models  A Comprehensive Survey\n\n[2] Beyond Supervised Continual Learning  a Review\n\n[3] Continual Learning for Smart City  A Survey\n\n[4] Continual Learning of Natural Language Processing Tasks  A Survey\n\n[5] Dissecting Continual Learning a Structural and Data Analysis\n\n[6] Computationally Budgeted Continual Learning  What Does Matter \n\n[7] Continual Learning with Pre-Trained Models  A Survey\n\n[8] Prototype-Guided Memory Replay for Continual Learning\n\n[9] BeGin  Extensive Benchmark Scenarios and An Easy-to-use Framework for  Graph Continual Learning\n\n[10] Continual Graph Learning  A Survey\n\n[11] Realistic Continual Learning Approach using Pre-trained Models\n\n[12] SAPT  A Shared Attention Framework for Parameter-Efficient Continual  Learning of Large Language Models\n\n[13] Continual Learning with Dynamic Sparse Training  Exploring Algorithms  for Effective Model Updates\n\n[14] Continual Lifelong Learning in Natural Language Processing  A Survey\n\n[15] Continual Learning for Robotics  Definition, Framework, Learning  Strategies, Opportunities and Challenges\n\n[16] Theoretical Understanding of the Information Flow on Continual Learning  Performance\n\n[17] CITB  A Benchmark for Continual Instruction Tuning\n\n[18] Memory Bounds for Continual Learning\n\n[19] Optimal Continual Learning has Perfect Memory and is NP-hard\n\n[20] The Dark Side of ChatGPT  Legal and Ethical Challenges from Stochastic  Parrots and Hallucination\n\n[21] Towards Practical Tool Usage for Continually Learning LLMs\n\n[22] Understanding LLMs  A Comprehensive Overview from Training to Inference\n\n[23] LLeMpower  Understanding Disparities in the Control and Access of Large  Language Models\n\n[24] FusionAI  Decentralized Training and Deploying LLMs with Massive  Consumer-Level GPUs\n\n[25] Large Language Models in Education  Vision and Opportunities\n\n[26] Prototyping the use of Large Language Models (LLMs) for adult learning  content creation at scale\n\n[27] CloChat  Understanding How People Customize, Interact, and Experience  Personas in Large Language Models\n\n[28] Intention and Context Elicitation with Large Language Models in the  Legal Aid Intake Process\n\n[29] Personalisation within bounds  A risk taxonomy and policy framework for  the alignment of large language models with personalised feedback\n\n[30] A Survey on Self-Evolution of Large Language Models\n\n[31] Catastrophic Forgetting in Deep Learning  A Comprehensive Taxonomy\n\n[32] An Empirical Study of Catastrophic Forgetting in Large Language Models  During Continual Fine-tuning\n\n[33] Understanding the Role of Training Regimes in Continual Learning\n\n[34] Recall-Oriented Continual Learning with Generative Adversarial  Meta-Model\n\n[35] PromptFusion  Decoupling Stability and Plasticity for Continual Learning\n\n[36] Dropout as an Implicit Gating Mechanism For Continual Learning\n\n[37] Watt For What  Rethinking Deep Learning's Energy-Performance  Relationship\n\n[38] Efficient Machine Learning for Big Data  A Review\n\n[39] Quantified Task Misalignment to Inform PEFT  An Exploration of Domain  Generalization and Catastrophic Forgetting in CLIP\n\n[40] The unreasonable effectiveness of Batch-Norm statistics in addressing  catastrophic forgetting across medical institutions\n\n[41] Continual Learning in the Presence of Spurious Correlation\n\n[42] MCF-VC  Mitigate Catastrophic Forgetting in Class-Incremental Learning  for Multimodal Video Captioning\n\n[43] Anatomy of Catastrophic Forgetting  Hidden Representations and Task  Semantics\n\n[44] Maintaining Plasticity in Deep Continual Learning\n\n[45] Keep Moving  identifying task-relevant subspaces to maximise plasticity  for newly learned tasks\n\n[46] A Comprehensive Evaluation of Large Language Models on Legal Judgment  Prediction\n\n[47] BLADE  Enhancing Black-box Large Language Models with Small  Domain-Specific Models\n\n[48] HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs\n\n[49] Don't Ignore Dual Logic Ability of LLMs while Privatizing  A  Data-Intensive Analysis in Medical Domain\n\n[50] SERVAL  Synergy Learning between Vertical Models and LLMs towards  Oracle-Level Zero-shot Medical Prediction\n\n[51] LAiW  A Chinese Legal Large Language Models Benchmark\n\n[52] Not All Languages Are Created Equal in LLMs  Improving Multilingual  Capability by Cross-Lingual-Thought Prompting\n\n[53] Mutual Enhancement of Large and Small Language Models with Cross-Silo  Knowledge Transfer\n\n[54] Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation\n\n[55] Vertical Federated Learning  Challenges, Methodologies and Experiments\n\n[56] Role Prompting Guided Domain Adaptation with General Capability Preserve  for Large Language Models\n\n[57] FollowEval  A Multi-Dimensional Benchmark for Assessing the  Instruction-Following Capability of Large Language Models\n\n[58] Knowledge Plugins  Enhancing Large Language Models for Domain-Specific  Recommendations\n\n[59] MedAgents  Large Language Models as Collaborators for Zero-shot Medical  Reasoning\n\n[60] Compositional Cyber-Physical Systems Theory\n\n[61] AI Transparency in the Age of LLMs  A Human-Centered Research Roadmap\n\n[62] CDEval  A Benchmark for Measuring the Cultural Dimensions of Large  Language Models\n\n[63] TRACE  A Comprehensive Benchmark for Continual Learning in Large  Language Models\n\n[64] LiveCodeBench  Holistic and Contamination Free Evaluation of Large  Language Models for Code\n\n[65] DevBench  A Comprehensive Benchmark for Software Development\n\n[66] Dissecting the Runtime Performance of the Training, Fine-tuning, and  Inference of Large Language Models\n\n[67] Benchmark Self-Evolving  A Multi-Agent Framework for Dynamic LLM  Evaluation\n\n[68] ROBBIE  Robust Bias Evaluation of Large Generative Language Models\n\n[69] LooGLE  Can Long-Context Language Models Understand Long Contexts \n\n[70] TransportationGames  Benchmarking Transportation Knowledge of  (Multimodal) Large Language Models\n\n[71] AgentBoard  An Analytical Evaluation Board of Multi-turn LLM Agents\n\n[72] A User-Centric Benchmark for Evaluating Large Language Models\n\n[73] Protected group bias and stereotypes in Large Language Models\n\n[74] The Unequal Opportunities of Large Language Models  Revealing  Demographic Bias through Job Recommendations\n\n[75] REQUAL-LM  Reliability and Equity through Aggregation in Large Language  Models\n\n[76] Bias patterns in the application of LLMs for clinical decision support   A comprehensive study\n\n[77] A Survey on Fairness in Large Language Models\n\n[78] A Group Fairness Lens for Large Language Models\n\n[79] The Ethics of ChatGPT in Medicine and Healthcare  A Systematic Review on  Large Language Models (LLMs)\n\n[80] Securing Large Language Models  Threats, Vulnerabilities and Responsible  Practices\n\n[81] Auditing large language models  a three-layered approach\n\n[82] Tuning-Free Accountable Intervention for LLM Deployment -- A  Metacognitive Approach\n\n[83] N-Critics  Self-Refinement of Large Language Models with Ensemble of  Critics\n\n[84] Pushing Large Language Models to the 6G Edge  Vision, Challenges, and  Opportunities\n\n[85] Challenges and Contributing Factors in the Utilization of Large Language  Models (LLMs)\n\n[86] Uncertainty-Based Abstention in LLMs Improves Safety and Reduces  Hallucinations\n\n[87] Tackling Bias in Pre-trained Language Models  Current Trends and  Under-represented Societies\n\n[88] Eagle  Ethical Dataset Given from Real Interactions\n\n[89] Self-Evaluation Improves Selective Generation in Large Language Models\n\n[90] She had Cobalt Blue Eyes  Prompt Testing to Create Aligned and  Sustainable Language Models\n\n[91] The Ethics of Interaction  Mitigating Security Threats in LLMs\n\n[92] CRISPR  Eliminating Bias Neurons from an Instruction-following Language  Model\n\n[93] LLMChain  Blockchain-based Reputation System for Sharing and Evaluating  Large Language Models\n\n[94] A Comprehensive Overview of Large Language Models\n\n[95] Efficient Large Language Models  A Survey\n\n[96] Data Management For Large Language Models  A Survey\n\n[97] Towards Efficient Generative Large Language Model Serving  A Survey from  Algorithms to Systems\n\n[98] The Efficiency Spectrum of Large Language Models  An Algorithmic Survey\n\n[99] Exploring the Nexus of Large Language Models and Legal Systems  A Short  Survey\n\n[100] Large Language Models in Biomedical and Health Informatics  A  Bibliometric Review\n\n[101] Knowledge-Augmented Large Language Models for Personalized Contextual  Query Suggestion\n\n[102] Evaluating Large Language Models  A Comprehensive Survey\n\n[103] Large Language Models for Time Series  A Survey\n\n[104] Aligning Language Models to User Opinions\n\n[105] From Instructions to Intrinsic Human Values -- A Survey of Alignment  Goals for Big Models\n\n[106] From MNIST to ImageNet and Back  Benchmarking Continual Curriculum  Learning\n\n[107] Continual Learning with Gated Incremental Memories for sequential data  processing\n\n[108] NISPA  Neuro-Inspired Stability-Plasticity Adaptation for Continual  Learning in Sparse Networks\n\n[109] InfoCL  Alleviating Catastrophic Forgetting in Continual Text  Classification from An Information Theoretic Perspective\n\n[110] Read Between the Layers  Leveraging Intra-Layer Representations for  Rehearsal-Free Continual Learning with Pre-Trained Models\n\n[111] Representational Continuity for Unsupervised Continual Learning\n\n[112] Learning to Prompt in the Classroom to Understand AI Limits  A pilot  study\n\n[113] Empowering Working Memory for Large Language Model Agents\n\n[114] Understanding the Role of Large Language Models in Personalizing and  Scaffolding Strategies to Combat Academic Procrastination\n\n[115] The Importance of Human-Labeled Data in the Era of LLMs\n\n[116] Lifelong Language Pretraining with Distribution-Specialized Experts\n\n[117] Curated LLM  Synergy of LLMs and Data Curation for tabular augmentation  in ultra low-data regimes\n\n[118] LLM-based Medical Assistant Personalization with Short- and Long-Term  Memory Coordination\n\n[119] TeaMs-RL  Teaching LLMs to Teach Themselves Better Instructions via  Reinforcement Learning\n\n[120] Exploring Autonomous Agents through the Lens of Large Language Models  A  Review\n\n[121] Adaptively Integrated Knowledge Distillation and Prediction Uncertainty  for Continual Learning\n\n[122] Addressing catastrophic forgetting for medical domain expansion\n\n[123] Synaptic metaplasticity in binarized neural networks\n\n[124] PlaStIL  Plastic and Stable Memory-Free Class-Incremental Learning\n\n[125] Understanding plasticity in neural networks\n\n[126] Entropy-based Stability-Plasticity for Lifelong Learning\n\n[127] PLASTIC  Improving Input and Label Plasticity for Sample Efficient  Reinforcement Learning\n\n[128] Online Continual Learning via the Meta-learning Update with Multi-scale  Knowledge Distillation and Data Augmentation\n\n[129] Domain Specialization as the Key to Make Large Language Models  Disruptive  A Comprehensive Survey\n\n[130] A Survey on Vertical Federated Learning  From a Layered Perspective\n\n[131] Mysterious Projections  Multimodal LLMs Gain Domain-Specific Visual  Capabilities Without Richer Cross-Modal Projections\n\n[132] Construction of a Japanese Financial Benchmark for Large Language Models\n\n[133] Fault-Tolerant Vertical Federated Learning on Dynamic Networks\n\n[134] BuboGPT  Enabling Visual Grounding in Multi-Modal LLMs\n\n[135] ChemLLM  A Chemical Large Language Model\n\n[136] BLT  Can Large Language Models Handle Basic Legal Text \n\n[137] MFTCoder  Boosting Code LLMs with Multitask Fine-Tuning\n\n[138] ML-Bench  Evaluating Large Language Models for Code Generation in  Repository-Level Machine Learning Tasks\n\n[139] LLM2LLM  Boosting LLMs with Novel Iterative Data Enhancement\n\n[140] DevEval  Evaluating Code Generation in Practical Software Projects\n\n[141] Fairness of ChatGPT\n\n[142] Measuring Implicit Bias in Explicitly Unbiased Large Language Models\n\n[143] Challenging the appearance of machine intelligence  Cognitive bias in  LLMs and Best Practices for Adoption\n\n[144] A Toolbox for Surfacing Health Equity Harms and Biases in Large Language  Models\n\n[145] Ethical Considerations and Policy Implications for Large Language  Models  Guiding Responsible Development and Deployment\n\n[146] Surveying Attitudinal Alignment Between Large Language Models Vs. Humans  Towards 17 Sustainable Development Goals\n\n[147] Cognitive Bias in High-Stakes Decision-Making with LLMs\n\n[148] Fairness  from the ethical principle to the practice of Machine Learning  development as an ongoing agreement with stakeholders\n\n[149] History, Development, and Principles of Large Language Models-An  Introductory Survey\n\n[150] Beyond Efficiency  A Systematic Survey of Resource-Efficient Large  Language Models\n\n[151] Datasets for Large Language Models  A Comprehensive Survey\n\n[152] A Survey on Dialogue Summarization  Recent Advances and New Frontiers\n\n[153] Generalization in Healthcare AI  Evaluation of a Clinical Large Language  Model\n\n[154] Retrieving Multimodal Information for Augmented Generation  A Survey\n\n[155] An Empirical Investigation of the Role of Pre-training in Lifelong  Learning\n\n[156] Scaling Laws for Forgetting When Fine-Tuning Large Language Models\n\n[157] Overcoming Catastrophic Forgetting by Incremental Moment Matching\n\n[158] Create and Find Flatness  Building Flat Training Spaces in Advance for  Continual Learning\n\n[159] Memory Aware Synapses  Learning what (not) to forget\n\n[160] Learning to Remember from a Multi-Task Teacher\n\n[161] The Joint Effect of Task Similarity and Overparameterization on  Catastrophic Forgetting -- An Analytical Model\n\n[162] On robustness of generative representations against catastrophic  forgetting\n\n[163] Provable Continual Learning via Sketched Jacobian Approximations\n\n[164] Balanced Destruction-Reconstruction Dynamics for Memory-replay Class  Incremental Learning\n\n[165] Avoiding Forgetting and Allowing Forward Transfer in Continual Learning  via Sparse Networks\n\n[166] Enhancing Visual Continual Learning with Language-Guided Supervision\n\n[167] GCR  Gradient Coreset Based Replay Buffer Selection For Continual  Learning\n\n[168] Continual World  A Robotic Benchmark For Continual Reinforcement  Learning\n\n[169] Adaptive-Solver Framework for Dynamic Strategy Selection in Large  Language Model Reasoning\n\n[170] Characterization of Large Language Model Development in the Datacenter\n\n[171] Large Language Model Can Continue Evolving From Mistakes\n\n[172] Cross-Data Knowledge Graph Construction for LLM-enabled Educational  Question-Answering System  A~Case~Study~at~HCMUT\n\n[173] Brain-Inspired Continual Learning-Robust Feature Distillation and  Re-Consolidation for Class Incremental Learning\n\n[174] Jointly Exploring Client Drift and Catastrophic Forgetting in Dynamic  Learning\n\n[175] FLAT  An Optimized Dataflow for Mitigating Attention Bottlenecks\n\n[176] EfficientMorph  Parameter-Efficient Transformer-Based Architecture for  3D Image Registration\n\n[177] Matbench Discovery -- A framework to evaluate machine learning crystal  stability predictions\n\n[178] Continual Learning in Medical Image Analysis  A Comprehensive Review of  Recent Advancements and Future Prospects\n\n[179] Federated Unlearning  a Perspective of Stability and Fairness\n\n[180] Improving Pre-trained Language Model Sensitivity via Mask Specific  losses  A case study on Biomedical NER\n\n[181] Evaluating Spatial Understanding of Large Language Models\n\n[182] Unveiling the Generalization Power of Fine-Tuned Large Language Models\n\n[183] Input Reconstruction Attack against Vertical Federated Large Language  Models\n\n[184] AgentBench  Evaluating LLMs as Agents\n\n[185] Examining the robustness of LLM evaluation to the distributional  assumptions of benchmarks\n\n[186] Ada-LEval  Evaluating long-context LLMs with length-adaptable benchmarks\n\n[187] Top Leaderboard Ranking = Top Coding Proficiency, Always  EvoEval   Evolving Coding Benchmarks via LLM\n\n[188] Bias and Fairness in Large Language Models  A Survey\n\n[189] Detectors for Safe and Reliable LLMs  Implementations, Uses, and  Limitations\n\n[190] Multi-Domain Multi-Task Rehearsal for Lifelong Learning\n\n[191] Stabilizing RLHF through Advantage Model and Selective Rehearsal\n\n[192] Model Tailor  Mitigating Catastrophic Forgetting in Multi-modal Large  Language Models\n\n[193] Deep Generative Dual Memory Network for Continual Learning\n\n[194] Attention-Based Structural-Plasticity\n\n[195] TADIL  Task-Agnostic Domain-Incremental Learning through Task-ID  Inference using Transformer Nearest-Centroid Embeddings\n\n[196] Online Continual Learning under Extreme Memory Constraints\n\n[197] Modality Plug-and-Play  Elastic Modality Adaptation in Multimodal LLMs  for Embodied AI\n\n[198] The Human Factor in Detecting Errors of Large Language Models  A  Systematic Literature Review and Future Research Directions\n\n[199] Empowering Federated Learning for Massive Models with NVIDIA FLARE\n\n[200] Use large language models to promote equity\n\n[201] New metrics for analyzing continual learners\n\n[202] Greedy Sampling of Graph Signals\n\n[203] Quantifying Relevance in Learning and Inference\n\n[204] Struc-Bench  Are Large Language Models Really Good at Generating Complex  Structured Data \n\n[205] A RAG Method for Source Code Inquiry Tailored to Long-Context LLMs\n\n[206] Enhancing Large Language Model Performance To Answer Questions and  Extract Information More Accurately\n\n[207] JudgeLM  Fine-tuned Large Language Models are Scalable Judges\n\n[208] TrustGPT  A Benchmark for Trustworthy and Responsible Large Language  Models\n\n[209] CodeScope  An Execution-based Multilingual Multitask Multidimensional  Benchmark for Evaluating LLMs on Code Understanding and Generation\n\n[210] MetaTool Benchmark for Large Language Models  Deciding Whether to Use  Tools and Which to Use\n\n[211] PiCO  Peer Review in LLMs based on the Consistency Optimization\n\n[212] Reasoning Capacity in Multi-Agent Systems  Limitations, Challenges and  Human-Centered Solutions\n\n[213] Exploring Large Language Model for Graph Data Understanding in Online  Job Recommendations\n\n[214] Characterizing Multimodal Long-form Summarization  A Case Study on  Financial Reports\n\n[215] A Simple but Effective Approach to Improve Structured Language Model  Output for Information Extraction\n\n[216] Aligning Large Language Models for Clinical Tasks\n\n[217] Fine-tuning Large Enterprise Language Models via Ontological Reasoning\n\n[218] Network Formation and Dynamics Among Multi-LLMs\n\n[219] Attribute Structuring Improves LLM-Based Evaluation of Clinical Text  Summaries\n\n[220] Continual Feature Selection  Spurious Features in Continual Learning\n\n[221] A Theoretical Study on Solving Continual Learning\n\n[222] Calibration of Continual Learning Models\n\n[223] Interactive Continual Learning  Fast and Slow Thinking\n\n[224] Sequoia  A Software Framework to Unify Continual Learning Research\n\n[225] chatClimate  Grounding Conversational AI in Climate Science\n\n[226] The Transformative Influence of Large Language Models on Software  Development\n\n[227] Challenges Towards Deploying Data Intensive Scientific Applications on  Extreme Heterogeneity Supercomputers\n\n[228] A Comprehensive Review of Machine Learning Advances on Data Change  A  Cross-Field Perspective\n\n[229] DomainVerse  A Benchmark Towards Real-World Distribution Shifts For  Tuning-Free Adaptive Domain Generalization\n\n[230] LLMGuard  Guarding Against Unsafe LLM Behavior\n\n[231] Structure Guided Prompt  Instructing Large Language Model in Multi-Step  Reasoning by Exploring Graph Structure of the Text\n\n[232] Understanding Causality with Large Language Models  Feasibility and  Opportunities\n\n[233] Investigating Data Contamination in Modern Benchmarks for Large Language  Models\n\n[234] Data Contamination Through the Lens of Time\n\n[235] Equivariant MuZero\n\n[236] Real or Not Real, that is the Question\n\n[237] Foundations\n\n[238] Are Large Language Model-based Evaluators the Solution to Scaling Up  Multilingual Evaluation \n\n[239] Coded MapReduce\n\n[240] Supernodes\n\n[241] Linear, or Non-Linear, That is the Question!\n\n[242] PsyBench  a balanced and in-depth Psychological Chinese Evaluation  Benchmark for Foundation Models\n\n[243] tinyBenchmarks  evaluating LLMs with fewer examples\n\n[244] Pachinko\n\n[245] SQT -- std $Q$-target\n\n[246] Patch Learning\n\n[247] ALERT  A Comprehensive Benchmark for Assessing Large Language Models'  Safety through Red Teaming\n\n[248] Voluminous yet Vacuous  Semantic Capital in an Age of Large Language  Models\n\n[249] FAIR Enough  How Can We Develop and Assess a FAIR-Compliant Dataset for  Large Language Models' Training \n\n[250] Learning to Evolve\n\n[251] Steps and Traces\n\n[252] Beyond Leaderboards  A survey of methods for revealing weaknesses in  Natural Language Inference data and models\n\n[253] SPEER  Sentence-Level Planning of Long Clinical Summaries via Embedded  Entity Retrieval\n\n[254] AI-Augmented Surveys  Leveraging Large Language Models and Surveys for  Opinion Prediction\n\n[255] CodeShell Technical Report\n\n[256] Lessons Learnt in Conducting Survey Research\n\n[257] A Survey for Biomedical Text Summarization  From Pre-trained to Large  Language Models\n\n[258] LLM App Store Analysis  A Vision and Roadmap\n\n[259] Ada-QPacknet -- adaptive pruning with bit width reduction as an  efficient continual learning method without forgetting\n\n[260] RanPAC  Random Projections and Pre-trained Models for Continual Learning\n\n[261] Efficient Continual Learning with Modular Networks and Task-Driven  Priors\n\n[262] NetGPT  A Native-AI Network Architecture Beyond Provisioning  Personalized Generative Services\n\n[263] Uncovering Energy-Efficient Practices in Deep Learning Training   Preliminary Steps Towards Green AI\n\n[264] Multi-User MIMO with flexible numerology for 5G\n\n[265] Retrieval-Augmented Chain-of-Thought in Semi-structured Domains\n\n[266] Dynamic Neural Network Architectural and Topological Adaptation and  Related Methods -- A Survey\n\n[267] The Instruction Hierarchy  Training LLMs to Prioritize Privileged  Instructions\n\n[268] An energy-based comparative analysis of common approaches to text  classification in the Legal domain\n\n[269] Don't Make Your LLM an Evaluation Benchmark Cheater\n\n[270] SuperCLUE  A Comprehensive Chinese Large Language Model Benchmark\n\n[271] NPHardEval  Dynamic Benchmark on Reasoning Ability of Large Language  Models via Complexity Classes\n\n[272]  It's a Fair Game , or Is It  Examining How Users Navigate Disclosure  Risks and Benefits When Using LLM-Based Conversational Agents\n\n[273]  The teachers are confused as well   A Multiple-Stakeholder Ethics  Discussion on Large Language Models in Computing Education\n\n[274] Mapping LLM Security Landscapes  A Comprehensive Stakeholder Risk  Assessment Proposal\n\n[275] Learning to Remember  A Synaptic Plasticity Driven Framework for  Continual Learning\n\n[276] Continual Learning with Echo State Networks\n\n[277] Prior-Free Continual Learning with Unlabeled Data in the Wild\n\n[278] HOP to the Next Tasks and Domains for Continual Learning in NLP\n\n[279] Convolutional Prompting meets Language Models for Continual Learning\n\n[280] SHARP  Sparsity and Hidden Activation RePlay for Neuro-Inspired  Continual Learning\n\n[281] A Unified and General Framework for Continual Learning\n\n[282] LLMs with Industrial Lens  Deciphering the Challenges and Prospects -- A  Survey\n\n[283] CHAI  Clustered Head Attention for Efficient LLM Inference\n\n[284] Continual evaluation for lifelong learning  Identifying the stability  gap\n\n[285] Visual Question Answering Instruction  Unlocking Multimodal Large  Language Model To Domain-Specific Visual Multitasks\n\n[286] HAMMR  HierArchical MultiModal React agents for generic VQA\n\n[287] Improving Contextual Congruence Across Modalities for Effective  Multimodal Marketing using Knowledge-infused Learning\n\n[288] Can Large Language Models Learn Independent Causal Mechanisms \n\n[289] Overview of the PromptCBLUE Shared Task in CHIP2023\n\n[290] Evaluating Large Language Models at Evaluating Instruction Following\n\n[291] Towards Understanding and Mitigating Social Biases in Language Models\n\n[292] MLOps  A Primer for Policymakers on a New Frontier in Machine Learning\n\n[293] Online Continual Knowledge Learning for Language Models\n\n[294] Generalized Continual Category Discovery\n\n[295] GOLF  Goal-Oriented Long-term liFe tasks supported by human-AI  collaboration\n\n[296] Short-term plasticity as cause-effect hypothesis testing in distal  reward learning\n\n[297] Chain-of-Specificity  An Iteratively Refining Method for Eliciting  Knowledge from Large Language Models\n\n[298] PromptCBLUE  A Chinese Prompt Tuning Benchmark for the Medical Domain\n\n[299] In ChatGPT We Trust  Measuring and Characterizing the Reliability of  ChatGPT\n\n[300] Privacy Games\n\n[301] PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models  A  Case Study of Mathematics Proficiency\n\n[302] Behavioral QLTL\n\n[303] The LLM Surgeon\n\n[304] AgentSims  An Open-Source Sandbox for Large Language Model Evaluation\n\n[305] The Importance of Robust Features in Mitigating Catastrophic Forgetting\n\n[306] (A)I Am Not a Lawyer, But...  Engaging Legal Experts towards Responsible  LLM Policies for Legal Advice\n\n[307] CSEPrompts  A Benchmark of Introductory Computer Science Prompts\n\n[308] Learning Agent-based Modeling with LLM Companions  Experiences of  Novices and Experts Using ChatGPT & NetLogo Chat\n\n[309] LookALike  Human Mimicry based collaborative decision making\n\n[310] LatEval  An Interactive LLMs Evaluation Benchmark with Incomplete  Information from Lateral Thinking Puzzles\n\n[311] A Vertical Federated Learning Method For Multi-Institutional Credit  Scoring  MICS\n\n[312] CriticBench  Evaluating Large Language Models as Critic\n\n[313] Chakra  Advancing Performance Benchmarking and Co-design using  Standardized Execution Traces\n\n[314] WaterBench  Towards Holistic Evaluation of Watermarks for Large Language  Models\n\n[315] An LLM Maturity Model for Reliable and Transparent Text-to-Query\n\n\n",
    "reference": {
        "1": "2404.16789v1",
        "2": "2208.14307v1",
        "3": "2404.00983v1",
        "4": "2211.12701v2",
        "5": "2301.01033v1",
        "6": "2303.11165v2",
        "7": "2401.16386v2",
        "8": "2108.12641v3",
        "9": "2211.14568v3",
        "10": "2301.12230v1",
        "11": "2404.07729v1",
        "12": "2401.08295v2",
        "13": "2308.14831v2",
        "14": "2012.09823v1",
        "15": "1907.00182v3",
        "16": "2204.12010v2",
        "17": "2310.14510v1",
        "18": "2204.10830v1",
        "19": "2006.05188v1",
        "20": "2304.14347v1",
        "21": "2404.09339v1",
        "22": "2401.02038v2",
        "23": "2404.09356v1",
        "24": "2309.01172v1",
        "25": "2311.13160v1",
        "26": "2306.01815v1",
        "27": "2402.15265v1",
        "28": "2311.13281v1",
        "29": "2303.05453v1",
        "30": "2404.14387v1",
        "31": "2312.10549v1",
        "32": "2308.08747v3",
        "33": "2006.06958v1",
        "34": "2403.03082v1",
        "35": "2303.07223v1",
        "36": "2004.11545v1",
        "37": "2310.06522v1",
        "38": "1503.05296v1",
        "39": "2402.09613v1",
        "40": "2011.08096v1",
        "41": "2303.11863v1",
        "42": "2402.17680v1",
        "43": "2007.07400v1",
        "44": "2306.13812v3",
        "45": "2310.04741v5",
        "46": "2310.11761v1",
        "47": "2403.18365v1",
        "48": "2311.09774v1",
        "49": "2309.04198v3",
        "50": "2403.01570v2",
        "51": "2310.05620v2",
        "52": "2305.07004v2",
        "53": "2312.05842v1",
        "54": "2403.02899v1",
        "55": "2202.04309v1",
        "56": "2403.02756v1",
        "57": "2311.09829v1",
        "58": "2311.10779v1",
        "59": "2311.10537v3",
        "60": "2109.04858v1",
        "61": "2306.01941v2",
        "62": "2311.16421v2",
        "63": "2310.06762v1",
        "64": "2403.07974v1",
        "65": "2403.08604v2",
        "66": "2311.03687v2",
        "67": "2402.11443v1",
        "68": "2311.18140v1",
        "69": "2311.04939v1",
        "70": "2401.04471v1",
        "71": "2401.13178v1",
        "72": "2404.13940v2",
        "73": "2403.14727v1",
        "74": "2308.02053v2",
        "75": "2404.11782v1",
        "76": "2404.15149v1",
        "77": "2308.10149v2",
        "78": "2312.15478v1",
        "79": "2403.14473v1",
        "80": "2403.12503v1",
        "81": "2302.08500v2",
        "82": "2403.05636v1",
        "83": "2310.18679v2",
        "84": "2309.16739v3",
        "85": "2310.13343v1",
        "86": "2404.10960v1",
        "87": "2312.01509v1",
        "88": "2402.14258v1",
        "89": "2312.09300v1",
        "90": "2310.18333v3",
        "91": "2401.12273v1",
        "92": "2311.09627v1",
        "93": "2404.13236v1",
        "94": "2307.06435v9",
        "95": "2312.03863v3",
        "96": "2312.01700v2",
        "97": "2312.15234v1",
        "98": "2312.00678v2",
        "99": "2404.00990v1",
        "100": "2403.16303v3",
        "101": "2311.06318v2",
        "102": "2310.19736v3",
        "103": "2402.01801v2",
        "104": "2305.14929v1",
        "105": "2308.12014v2",
        "106": "2303.11076v1",
        "107": "2004.04077v1",
        "108": "2206.09117v1",
        "109": "2310.06362v1",
        "110": "2312.08888v2",
        "111": "2110.06976v3",
        "112": "2307.01540v2",
        "113": "2312.17259v1",
        "114": "2312.13581v1",
        "115": "2306.14910v1",
        "116": "2305.12281v1",
        "117": "2312.12112v2",
        "118": "2309.11696v3",
        "119": "2403.08694v1",
        "120": "2404.04442v1",
        "121": "2301.07316v1",
        "122": "2103.13511v1",
        "123": "2101.07592v1",
        "124": "2209.06606v2",
        "125": "2303.01486v4",
        "126": "2204.09517v1",
        "127": "2306.10711v3",
        "128": "2209.06107v1",
        "129": "2305.18703v7",
        "130": "2304.01829v1",
        "131": "2402.16832v1",
        "132": "2403.15062v1",
        "133": "2312.16638v1",
        "134": "2307.08581v1",
        "135": "2402.06852v2",
        "136": "2311.09693v2",
        "137": "2311.02303v1",
        "138": "2311.09835v2",
        "139": "2403.15042v1",
        "140": "2401.06401v4",
        "141": "2305.18569v1",
        "142": "2402.04105v1",
        "143": "2304.01358v3",
        "144": "2403.12025v1",
        "145": "2308.02678v1",
        "146": "2404.13885v1",
        "147": "2403.00811v1",
        "148": "2304.06031v1",
        "149": "2402.06853v1",
        "150": "2401.00625v2",
        "151": "2402.18041v1",
        "152": "2107.03175v2",
        "153": "2402.10965v2",
        "154": "2303.10868v3",
        "155": "2112.09153v2",
        "156": "2401.05605v1",
        "157": "1703.08475v3",
        "158": "2309.11305v1",
        "159": "1711.09601v4",
        "160": "1910.04650v2",
        "161": "2401.12617v2",
        "162": "2109.01844v1",
        "163": "2112.05095v1",
        "164": "2308.01698v1",
        "165": "2110.05329v3",
        "166": "2403.16124v1",
        "167": "2111.11210v3",
        "168": "2105.10919v3",
        "169": "2310.01446v1",
        "170": "2403.07648v2",
        "171": "2404.08707v2",
        "172": "2404.09296v1",
        "173": "2404.14588v1",
        "174": "2309.00688v1",
        "175": "2107.06419v7",
        "176": "2403.11026v1",
        "177": "2308.14920v2",
        "178": "2312.17004v2",
        "179": "2402.01276v3",
        "180": "2403.18025v2",
        "181": "2310.14540v3",
        "182": "2403.09162v1",
        "183": "2311.07585v2",
        "184": "2308.03688v2",
        "185": "2404.16966v1",
        "186": "2404.06480v2",
        "187": "2403.19114v1",
        "188": "2309.00770v2",
        "189": "2403.06009v1",
        "190": "2012.07236v1",
        "191": "2309.10202v1",
        "192": "2402.12048v1",
        "193": "1710.10368v2",
        "194": "1903.06070v1",
        "195": "2306.11955v1",
        "196": "2008.01510v3",
        "197": "2312.07886v1",
        "198": "2403.09743v1",
        "199": "2402.07792v1",
        "200": "2312.14804v1",
        "201": "2309.00462v1",
        "202": "1704.01223v2",
        "203": "2202.00339v1",
        "204": "2309.08963v3",
        "205": "2404.06082v1",
        "206": "2402.01722v1",
        "207": "2310.17631v1",
        "208": "2306.11507v1",
        "209": "2311.08588v2",
        "210": "2310.03128v5",
        "211": "2402.01830v2",
        "212": "2402.01108v1",
        "213": "2307.05722v3",
        "214": "2404.06162v1",
        "215": "2402.13364v1",
        "216": "2309.02884v2",
        "217": "2306.10723v2",
        "218": "2402.10659v2",
        "219": "2403.01002v1",
        "220": "2203.01012v2",
        "221": "2211.02633v1",
        "222": "2404.07817v2",
        "223": "2403.02628v2",
        "224": "2108.01005v4",
        "225": "2304.05510v2",
        "226": "2311.16429v1",
        "227": "1804.09738v1",
        "228": "2402.12627v1",
        "229": "2403.02714v1",
        "230": "2403.00826v1",
        "231": "2402.13415v1",
        "232": "2304.05524v1",
        "233": "2311.09783v2",
        "234": "2310.10628v1",
        "235": "2302.04798v1",
        "236": "2002.05512v1",
        "237": "2009.09541v4",
        "238": "2309.07462v2",
        "239": "1512.01625v1",
        "240": "2108.10458v1",
        "241": "2111.07265v2",
        "242": "2311.09861v2",
        "243": "2402.14992v1",
        "244": "1601.05706v1",
        "245": "2402.05950v2",
        "246": "1906.00158v1",
        "247": "2404.08676v1",
        "248": "2306.01773v1",
        "249": "2401.11033v4",
        "250": "1905.03389v1",
        "251": "2004.05400v1",
        "252": "2005.14709v1",
        "253": "2401.02369v1",
        "254": "2305.09620v3",
        "255": "2403.15747v1",
        "256": "1702.05744v3",
        "257": "2304.08763v2",
        "258": "2404.12737v1",
        "259": "2308.07939v2",
        "260": "2307.02251v3",
        "261": "2012.12631v2",
        "262": "2307.06148v4",
        "263": "2303.13972v1",
        "264": "1610.03056v1",
        "265": "2310.14435v1",
        "266": "2108.10066v1",
        "267": "2404.13208v1",
        "268": "2311.01256v2",
        "269": "2311.01964v1",
        "270": "2307.15020v1",
        "271": "2312.14890v4",
        "272": "2309.11653v2",
        "273": "2401.12453v1",
        "274": "2403.13309v1",
        "275": "1904.03137v4",
        "276": "2105.07674v3",
        "277": "2310.10417v1",
        "278": "2402.18449v1",
        "279": "2403.20317v1",
        "280": "2305.18563v1",
        "281": "2403.13249v1",
        "282": "2402.14558v1",
        "283": "2403.08058v1",
        "284": "2205.13452v2",
        "285": "2402.08360v1",
        "286": "2404.05465v1",
        "287": "2402.03607v1",
        "288": "2402.02636v1",
        "289": "2312.17522v1",
        "290": "2310.07641v2",
        "291": "2106.13219v1",
        "292": "2301.05775v1",
        "293": "2311.09632v1",
        "294": "2308.12112v1",
        "295": "2403.17089v2",
        "296": "1402.0710v5",
        "297": "2402.15526v1",
        "298": "2310.14151v1",
        "299": "2304.08979v2",
        "300": "1410.1920v1",
        "301": "2404.01799v1",
        "302": "2102.11184v1",
        "303": "2312.17244v2",
        "304": "2308.04026v1",
        "305": "2306.17091v1",
        "306": "2402.01864v1",
        "307": "2404.02540v2",
        "308": "2401.17163v2",
        "309": "2403.10824v1",
        "310": "2308.10855v3",
        "311": "2111.09038v1",
        "312": "2402.13764v3",
        "313": "2305.14516v2",
        "314": "2311.07138v1",
        "315": "2402.14855v1"
    },
    "retrieveref": {
        "1": "2404.16789v1",
        "2": "2402.17400v1",
        "3": "2402.01364v2",
        "4": "2404.08707v2",
        "5": "2404.07470v1",
        "6": "2205.12393v4",
        "7": "2311.09632v1",
        "8": "2110.03215v4",
        "9": "2404.09339v1",
        "10": "2307.02435v1",
        "11": "2007.09335v2",
        "12": "2403.07356v1",
        "13": "2310.06762v1",
        "14": "2403.10894v1",
        "15": "2401.03129v1",
        "16": "2310.14152v1",
        "17": "2308.15827v1",
        "18": "2403.08763v3",
        "19": "2402.10427v1",
        "20": "2403.02628v2",
        "21": "2301.12314v1",
        "22": "2403.20317v1",
        "23": "2012.09823v1",
        "24": "2306.08200v1",
        "25": "2403.01244v1",
        "26": "2403.11549v1",
        "27": "2312.15696v1",
        "28": "2312.15918v2",
        "29": "2401.08295v2",
        "30": "2004.03794v1",
        "31": "2312.13179v1",
        "32": "2310.14510v1",
        "33": "2303.14423v1",
        "34": "2305.10626v3",
        "35": "2309.14763v1",
        "36": "2103.07492v4",
        "37": "2404.08417v1",
        "38": "2311.08545v1",
        "39": "2305.02309v2",
        "40": "2205.12186v2",
        "41": "2112.02706v1",
        "42": "2211.16234v2",
        "43": "2401.13601v4",
        "44": "2312.05934v3",
        "45": "2310.14277v1",
        "46": "2305.11206v1",
        "47": "2204.02311v5",
        "48": "2305.10998v2",
        "49": "2207.05071v1",
        "50": "2310.14248v1",
        "51": "2404.10555v1",
        "52": "2307.07164v2",
        "53": "2308.01684v2",
        "54": "2307.06018v1",
        "55": "2403.16124v1",
        "56": "2402.15818v1",
        "57": "2305.14483v1",
        "58": "2404.02204v1",
        "59": "2302.03241v4",
        "60": "2310.15777v2",
        "61": "2105.08445v2",
        "62": "2308.04014v2",
        "63": "2309.08859v1",
        "64": "2209.06767v3",
        "65": "2402.11260v1",
        "66": "2312.08888v2",
        "67": "2402.18041v1",
        "68": "1606.02355v1",
        "69": "2404.06290v1",
        "70": "2310.12321v1",
        "71": "2303.14177v1",
        "72": "2401.15098v2",
        "73": "2211.01542v2",
        "74": "2208.11857v2",
        "75": "2402.08255v1",
        "76": "2106.02232v1",
        "77": "2402.14270v2",
        "78": "2307.06435v9",
        "79": "2305.13627v2",
        "80": "2309.10305v2",
        "81": "2404.03788v1",
        "82": "2401.13303v2",
        "83": "2309.09400v1",
        "84": "2104.05489v2",
        "85": "2310.00533v4",
        "86": "2303.11165v2",
        "87": "2303.06628v2",
        "88": "2110.08534v3",
        "89": "2402.06196v2",
        "90": "2311.12351v2",
        "91": "2309.03852v2",
        "92": "2305.16339v2",
        "93": "2303.01421v1",
        "94": "2303.03378v1",
        "95": "2305.09681v1",
        "96": "2311.08106v2",
        "97": "2305.17740v1",
        "98": "2210.05549v1",
        "99": "2401.15275v1",
        "100": "2304.04309v1",
        "101": "2311.02428v1",
        "102": "2312.07622v3",
        "103": "2403.18105v2",
        "104": "2303.11076v1",
        "105": "2308.02432v1",
        "106": "2110.07055v1",
        "107": "2210.00940v1",
        "108": "2310.08172v2",
        "109": "2309.05142v1",
        "110": "2305.12766v2",
        "111": "2311.16206v1",
        "112": "2401.06466v1",
        "113": "2305.11488v2",
        "114": "2403.06414v1",
        "115": "2402.14195v1",
        "116": "1908.00355v1",
        "117": "2309.16459v1",
        "118": "2309.14771v2",
        "119": "2403.19347v1",
        "120": "2204.14211v3",
        "121": "2307.02738v3",
        "122": "2308.12097v1",
        "123": "2201.06289v3",
        "124": "2311.15786v4",
        "125": "2311.00204v1",
        "126": "2305.17256v2",
        "127": "2109.05186v2",
        "128": "2310.07984v1",
        "129": "2311.10614v1",
        "130": "2404.02893v1",
        "131": "2305.16252v1",
        "132": "2403.11439v1",
        "133": "2402.06853v1",
        "134": "2311.05584v1",
        "135": "2310.14225v1",
        "136": "2308.13542v1",
        "137": "2306.06770v4",
        "138": "2204.10830v1",
        "139": "2308.10252v1",
        "140": "2402.12151v2",
        "141": "2306.12213v1",
        "142": "2312.03863v3",
        "143": "2404.07817v2",
        "144": "2210.03114v1",
        "145": "2401.09555v1",
        "146": "2403.14221v2",
        "147": "2311.07687v1",
        "148": "2311.11293v1",
        "149": "2402.14700v1",
        "150": "2402.18590v3",
        "151": "2310.05177v1",
        "152": "2404.02060v2",
        "153": "2404.16645v1",
        "154": "2303.07971v1",
        "155": "2308.12674v1",
        "156": "2403.04260v2",
        "157": "1911.09514v2",
        "158": "2207.03509v1",
        "159": "2206.04615v3",
        "160": "2309.13205v1",
        "161": "2311.05876v2",
        "162": "2305.12281v1",
        "163": "2311.10779v1",
        "164": "2401.01286v4",
        "165": "2011.04946v1",
        "166": "2401.04507v1",
        "167": "2204.12785v1",
        "168": "2403.13164v1",
        "169": "2401.15670v1",
        "170": "2311.07418v1",
        "171": "2312.16018v3",
        "172": "2402.17396v1",
        "173": "2304.02020v1",
        "174": "2404.00983v1",
        "175": "2305.18153v2",
        "176": "2311.09825v1",
        "177": "2104.11390v1",
        "178": "2204.05928v2",
        "179": "2305.16332v1",
        "180": "2112.09427v4",
        "181": "2306.01116v1",
        "182": "2308.12261v1",
        "183": "2307.02469v2",
        "184": "2308.04477v1",
        "185": "2309.04646v1",
        "186": "2402.12170v1",
        "187": "2204.06457v2",
        "188": "2404.03558v1",
        "189": "2209.14500v2",
        "190": "2401.08092v1",
        "191": "2305.01879v4",
        "192": "2312.17055v1",
        "193": "2310.17722v2",
        "194": "2212.10511v4",
        "195": "2308.10173v1",
        "196": "2312.08027v1",
        "197": "2311.03839v3",
        "198": "2306.15895v2",
        "199": "2401.16386v2",
        "200": "2310.08754v4",
        "201": "2312.00678v2",
        "202": "2105.07627v1",
        "203": "2306.12026v1",
        "204": "1910.04732v2",
        "205": "2306.07174v1",
        "206": "2404.04900v1",
        "207": "2310.11952v2",
        "208": "2404.12843v1",
        "209": "2212.09095v2",
        "210": "2305.19249v1",
        "211": "2309.02706v5",
        "212": "2312.14862v1",
        "213": "2308.08378v1",
        "214": "2312.15472v1",
        "215": "2401.04155v1",
        "216": "2308.11131v4",
        "217": "2306.11372v1",
        "218": "2311.16822v1",
        "219": "2404.16164v1",
        "220": "2403.08350v1",
        "221": "2308.14536v1",
        "222": "2307.10188v1",
        "223": "2402.01812v1",
        "224": "2204.06514v1",
        "225": "2312.15599v1",
        "226": "2404.16478v1",
        "227": "2304.11872v2",
        "228": "2205.02014v1",
        "229": "2403.18886v1",
        "230": "2401.07324v3",
        "231": "2306.06687v3",
        "232": "2108.09020v2",
        "233": "2402.14453v1",
        "234": "2310.14542v1",
        "235": "2402.14273v1",
        "236": "2310.15389v1",
        "237": "2004.03340v2",
        "238": "2108.12641v3",
        "239": "2403.09162v1",
        "240": "2305.10645v2",
        "241": "2303.17557v1",
        "242": "2112.09175v1",
        "243": "2206.09059v2",
        "244": "2308.10410v3",
        "245": "2306.03917v1",
        "246": "2310.02238v2",
        "247": "2211.11363v2",
        "248": "2307.16184v2",
        "249": "2401.05033v1",
        "250": "2402.12691v1",
        "251": "2310.01119v2",
        "252": "2312.16374v2",
        "253": "2309.12727v1",
        "254": "2404.14294v1",
        "255": "2404.10922v1",
        "256": "2404.13028v1",
        "257": "2309.01352v1",
        "258": "2307.14430v1",
        "259": "2207.00352v1",
        "260": "2204.05185v3",
        "261": "2401.15422v2",
        "262": "2308.11432v5",
        "263": "2306.12509v2",
        "264": "1602.02410v2",
        "265": "2309.14534v3",
        "266": "2303.01081v1",
        "267": "2310.01444v3",
        "268": "1907.00182v3",
        "269": "2311.08298v2",
        "270": "2312.17259v1",
        "271": "2307.06290v2",
        "272": "2403.09522v2",
        "273": "2305.13514v2",
        "274": "2112.06511v1",
        "275": "2305.13782v1",
        "276": "2302.08917v1",
        "277": "2404.14607v1",
        "278": "2311.00915v1",
        "279": "2312.08977v2",
        "280": "2401.02909v1",
        "281": "2311.13160v1",
        "282": "2205.10770v2",
        "283": "2304.12067v1",
        "284": "2312.16409v1",
        "285": "2303.05118v4",
        "286": "2205.11152v2",
        "287": "2401.14931v1",
        "288": "2309.06384v1",
        "289": "2308.16137v6",
        "290": "2002.10957v2",
        "291": "2310.08908v1",
        "292": "2402.15833v1",
        "293": "2307.02251v3",
        "294": "2210.01911v3",
        "295": "2312.12705v2",
        "296": "2310.11374v4",
        "297": "2305.14325v1",
        "298": "2402.02558v1",
        "299": "2404.14387v1",
        "300": "2210.13617v2",
        "301": "2309.17447v1",
        "302": "2205.09357v1",
        "303": "2310.01957v2",
        "304": "2310.20046v1",
        "305": "2306.16793v1",
        "306": "2402.03175v1",
        "307": "2312.08400v1",
        "308": "2212.04842v2",
        "309": "2309.07423v1",
        "310": "2402.13598v1",
        "311": "2304.14402v3",
        "312": "2112.08786v2",
        "313": "1906.01076v3",
        "314": "2401.01055v2",
        "315": "2404.04869v1",
        "316": "2212.09097v2",
        "317": "2305.12907v1",
        "318": "2303.01580v2",
        "319": "2403.04790v1",
        "320": "2404.11973v1",
        "321": "2305.14791v2",
        "322": "2403.13325v1",
        "323": "2306.10509v2",
        "324": "2312.06323v1",
        "325": "2401.12078v1",
        "326": "2305.18703v7",
        "327": "2311.12315v1",
        "328": "2403.06018v1",
        "329": "2401.12087v1",
        "330": "2201.09227v3",
        "331": "2402.13463v2",
        "332": "2404.09296v1",
        "333": "2310.08780v1",
        "334": "2402.17944v2",
        "335": "2402.09216v3",
        "336": "2402.18225v1",
        "337": "2311.02089v1",
        "338": "2312.11336v1",
        "339": "2310.08922v1",
        "340": "2306.05696v1",
        "341": "2309.04663v2",
        "342": "2311.08105v2",
        "343": "2310.16218v3",
        "344": "2103.06799v1",
        "345": "2311.03778v1",
        "346": "2304.09871v2",
        "347": "2403.11435v1",
        "348": "2309.08637v4",
        "349": "2305.03726v1",
        "350": "2402.04588v2",
        "351": "2309.10444v4",
        "352": "2204.06283v2",
        "353": "2310.10035v1",
        "354": "2401.13870v1",
        "355": "2402.02380v3",
        "356": "2308.14831v2",
        "357": "2309.07623v1",
        "358": "2305.06555v1",
        "359": "2403.09131v3",
        "360": "2309.10400v3",
        "361": "2312.03309v1",
        "362": "2308.10462v2",
        "363": "2404.08262v2",
        "364": "1908.09355v1",
        "365": "2305.15498v1",
        "366": "2305.11130v2",
        "367": "2310.18696v1",
        "368": "2212.11456v1",
        "369": "2305.10263v2",
        "370": "2312.15922v1",
        "371": "2306.04757v3",
        "372": "2302.04931v1",
        "373": "2306.08302v3",
        "374": "2202.10203v1",
        "375": "2306.07536v1",
        "376": "2311.06720v1",
        "377": "2306.05301v2",
        "378": "2309.16609v1",
        "379": "2303.11934v1",
        "380": "2404.09220v1",
        "381": "2305.16484v1",
        "382": "2309.10706v2",
        "383": "2403.10882v2",
        "384": "2212.06833v1",
        "385": "2404.14883v1",
        "386": "2307.06530v1",
        "387": "2401.03910v1",
        "388": "2304.13276v1",
        "389": "2306.01815v1",
        "390": "2208.06458v1",
        "391": "2311.04939v1",
        "392": "2301.01033v1",
        "393": "2310.07554v2",
        "394": "2304.06975v1",
        "395": "2402.13533v1",
        "396": "2309.17122v1",
        "397": "2403.11802v2",
        "398": "2402.07827v1",
        "399": "2305.06087v1",
        "400": "2208.12097v1",
        "401": "2309.01809v1",
        "402": "2310.06362v1",
        "403": "2404.08001v1",
        "404": "2203.05692v1",
        "405": "2404.11018v1",
        "406": "2402.03182v1",
        "407": "1911.08340v1",
        "408": "2309.17167v3",
        "409": "2305.11462v1",
        "410": "2402.11700v1",
        "411": "2310.13332v1",
        "412": "2310.11158v1",
        "413": "2312.16731v2",
        "414": "2311.09758v2",
        "415": "2310.08523v1",
        "416": "2404.09356v1",
        "417": "2402.08874v1",
        "418": "1904.03137v4",
        "419": "2304.13343v2",
        "420": "2401.04482v1",
        "421": "2402.15061v1",
        "422": "2305.09731v1",
        "423": "2403.18125v1",
        "424": "2303.10868v3",
        "425": "2403.00807v1",
        "426": "2311.00217v2",
        "427": "2107.02137v1",
        "428": "2308.14034v2",
        "429": "2301.11916v4",
        "430": "2403.03866v1",
        "431": "2403.06914v2",
        "432": "2404.12526v1",
        "433": "2310.14540v3",
        "434": "2403.19135v2",
        "435": "2309.10917v1",
        "436": "2401.08429v1",
        "437": "2011.01168v1",
        "438": "2402.15613v1",
        "439": "2312.02783v2",
        "440": "2401.10580v1",
        "441": "2310.07321v2",
        "442": "2305.05576v1",
        "443": "2401.12973v2",
        "444": "2312.02445v3",
        "445": "2310.19671v2",
        "446": "2212.04088v3",
        "447": "2312.17673v2",
        "448": "2403.18365v1",
        "449": "2401.00625v2",
        "450": "2105.12374v2",
        "451": "2309.01029v3",
        "452": "2008.12579v2",
        "453": "2002.00733v1",
        "454": "2310.08923v1",
        "455": "2111.04909v3",
        "456": "2211.04486v1",
        "457": "2401.09181v2",
        "458": "2402.11565v1",
        "459": "2403.09059v1",
        "460": "2403.09085v1",
        "461": "2308.07939v2",
        "462": "2005.00785v5",
        "463": "2404.04603v1",
        "464": "2402.11187v1",
        "465": "2402.16107v3",
        "466": "2312.17276v1",
        "467": "2306.13394v4",
        "468": "2205.14288v1",
        "469": "2201.01420v1",
        "470": "2209.12153v1",
        "471": "2010.02500v1",
        "472": "2403.12675v1",
        "473": "2308.08434v2",
        "474": "2311.01468v1",
        "475": "2311.03732v2",
        "476": "2312.10908v3",
        "477": "2307.12966v1",
        "478": "2403.01031v1",
        "479": "2404.06634v1",
        "480": "2312.01188v1",
        "481": "2305.07289v1",
        "482": "2310.14403v5",
        "483": "2205.10487v1",
        "484": "2401.05778v1",
        "485": "2108.01005v4",
        "486": "2305.11627v3",
        "487": "2311.08182v1",
        "488": "2309.16575v2",
        "489": "2402.08015v4",
        "490": "2402.06126v2",
        "491": "2402.17970v2",
        "492": "2404.15458v1",
        "493": "2309.04106v2",
        "494": "2312.11420v1",
        "495": "2404.07729v1",
        "496": "2404.06349v1",
        "497": "2401.06416v1",
        "498": "2209.10372v5",
        "499": "2212.10873v3",
        "500": "2305.11991v2",
        "501": "2310.03249v2",
        "502": "2112.10668v3",
        "503": "2312.01795v1",
        "504": "2307.10549v1",
        "505": "2402.14833v1",
        "506": "2111.02080v6",
        "507": "2112.06905v2",
        "508": "2211.12701v2",
        "509": "2210.00320v1",
        "510": "2305.19270v1",
        "511": "2310.03150v1",
        "512": "2310.02995v3",
        "513": "2204.12010v2",
        "514": "2402.11192v1",
        "515": "2108.06277v1",
        "516": "2309.00862v1",
        "517": "2403.15470v1",
        "518": "2309.14321v2",
        "519": "2212.08681v1",
        "520": "1810.10612v1",
        "521": "2303.10070v2",
        "522": "2306.12619v2",
        "523": "2302.05836v1",
        "524": "2311.07387v2",
        "525": "2310.13343v1",
        "526": "2304.10464v4",
        "527": "2312.00407v1",
        "528": "2401.02954v1",
        "529": "2308.12247v1",
        "530": "2310.05499v1",
        "531": "2302.01047v3",
        "532": "2305.14782v2",
        "533": "2310.15372v2",
        "534": "2402.13449v1",
        "535": "2311.15614v1",
        "536": "2105.13880v2",
        "537": "2404.07117v1",
        "538": "2201.11990v3",
        "539": "2309.03450v1",
        "540": "2108.06552v3",
        "541": "2404.15736v2",
        "542": "2403.07648v2",
        "543": "2402.10946v1",
        "544": "2205.08184v1",
        "545": "2209.09476v1",
        "546": "2312.07887v1",
        "547": "2402.14373v1",
        "548": "2311.17041v2",
        "549": "2403.04746v1",
        "550": "2009.04891v2",
        "551": "2310.15746v1",
        "552": "2310.06846v1",
        "553": "2311.08487v1",
        "554": "2310.00898v3",
        "555": "2404.08865v1",
        "556": "2105.07674v3",
        "557": "2310.05736v2",
        "558": "2311.11551v1",
        "559": "2305.04160v3",
        "560": "2402.18449v1",
        "561": "2403.01131v2",
        "562": "2305.14552v2",
        "563": "2307.09793v1",
        "564": "2404.08885v1",
        "565": "2310.04945v1",
        "566": "2305.15066v2",
        "567": "2302.14045v2",
        "568": "2308.03228v1",
        "569": "2403.06354v1",
        "570": "2311.15766v2",
        "571": "2304.08109v2",
        "572": "2305.06474v1",
        "573": "2402.10688v2",
        "574": "2212.05339v3",
        "575": "2404.09336v1",
        "576": "2305.15255v3",
        "577": "2402.13917v2",
        "578": "2208.01448v2",
        "579": "2403.13600v1",
        "580": "2401.13227v3",
        "581": "2308.15022v2",
        "582": "2306.03268v2",
        "583": "2401.00246v1",
        "584": "2303.13217v3",
        "585": "2110.07280v2",
        "586": "2404.01856v2",
        "587": "2308.11357v1",
        "588": "1612.08083v3",
        "589": "2310.19019v2",
        "590": "2306.13275v1",
        "591": "2312.15234v1",
        "592": "2110.06976v3",
        "593": "2210.15424v2",
        "594": "2311.09619v2",
        "595": "2310.17639v3",
        "596": "2106.03027v3",
        "597": "2307.04408v3",
        "598": "2402.01722v1",
        "599": "2310.10049v1",
        "600": "2307.04964v2",
        "601": "2306.10968v2",
        "602": "2205.05055v6",
        "603": "2308.00624v1",
        "604": "2402.12465v1",
        "605": "2304.01964v2",
        "606": "2308.10390v4",
        "607": "2403.19137v1",
        "608": "2310.18581v2",
        "609": "2007.15553v1",
        "610": "2311.05374v1",
        "611": "2404.12901v1",
        "612": "2305.18098v3",
        "613": "2401.14717v1",
        "614": "2207.00349v1",
        "615": "2402.03407v1",
        "616": "2310.15793v1",
        "617": "2404.14678v1",
        "618": "2306.17089v2",
        "619": "2307.16338v1",
        "620": "2211.05110v1",
        "621": "2404.02754v1",
        "622": "2404.12766v1",
        "623": "2311.15965v1",
        "624": "2305.13230v2",
        "625": "2402.15713v1",
        "626": "2305.04400v1",
        "627": "2210.12302v1",
        "628": "2403.18140v1",
        "629": "2303.11315v2",
        "630": "2311.12699v1",
        "631": "2311.01403v1",
        "632": "2309.14316v2",
        "633": "2208.11057v3",
        "634": "2404.07143v1",
        "635": "2403.19930v1",
        "636": "2403.00810v1",
        "637": "2309.06236v1",
        "638": "2403.15042v1",
        "639": "2211.13999v1",
        "640": "2303.13375v2",
        "641": "2204.06130v2",
        "642": "2312.08618v1",
        "643": "2309.15427v2",
        "644": "2311.04900v1",
        "645": "2402.15758v2",
        "646": "2402.11997v1",
        "647": "2402.06116v1",
        "648": "2401.06785v1",
        "649": "2309.15129v1",
        "650": "2308.03638v1",
        "651": "2402.02018v3",
        "652": "2311.08572v2",
        "653": "2210.03871v1",
        "654": "2207.06543v1",
        "655": "2211.05100v4",
        "656": "2305.14622v1",
        "657": "2404.02491v3",
        "658": "2112.08654v2",
        "659": "2310.05492v3",
        "660": "2311.18041v1",
        "661": "2401.08438v1",
        "662": "2310.07343v1",
        "663": "2304.14178v3",
        "664": "2403.16378v1",
        "665": "2305.19234v3",
        "666": "2404.14285v1",
        "667": "2309.06917v1",
        "668": "1712.00409v1",
        "669": "2306.05817v5",
        "670": "2310.18356v2",
        "671": "2312.07886v1",
        "672": "2305.00660v1",
        "673": "2206.09117v1",
        "674": "2402.10891v1",
        "675": "2401.09890v1",
        "676": "2403.05434v2",
        "677": "2402.10949v2",
        "678": "2311.03498v2",
        "679": "2306.11955v1",
        "680": "2203.02026v2",
        "681": "2307.03917v3",
        "682": "2207.11005v3",
        "683": "1912.03624v2",
        "684": "2305.18365v3",
        "685": "2308.08234v1",
        "686": "2307.03170v2",
        "687": "2312.01629v2",
        "688": "2311.05640v1",
        "689": "2108.10781v2",
        "690": "2210.05751v1",
        "691": "2207.04543v2",
        "692": "2404.01869v1",
        "693": "2312.11514v2",
        "694": "2311.07978v1",
        "695": "1907.01929v1",
        "696": "2007.12865v4",
        "697": "2311.04926v1",
        "698": "2311.03319v1",
        "699": "2403.15371v1",
        "700": "2104.14690v1",
        "701": "2308.12032v5",
        "702": "2401.16186v1",
        "703": "2310.19084v1",
        "704": "2311.02105v1",
        "705": "2402.01740v2",
        "706": "2403.11430v2",
        "707": "2310.18362v1",
        "708": "2307.03109v9",
        "709": "2202.12837v2",
        "710": "1703.01024v1",
        "711": "2205.10782v1",
        "712": "2310.13712v2",
        "713": "2212.10539v1",
        "714": "2402.12080v1",
        "715": "1912.02164v4",
        "716": "2401.06603v1",
        "717": "2403.13590v1",
        "718": "2402.18086v4",
        "719": "2310.07289v1",
        "720": "2403.19443v1",
        "721": "2404.00862v1",
        "722": "2308.11224v2",
        "723": "2203.14383v1",
        "724": "2310.16937v2",
        "725": "2402.10951v1",
        "726": "2309.14504v2",
        "727": "2309.13963v2",
        "728": "2305.01550v1",
        "729": "2402.18381v1",
        "730": "2308.13566v2",
        "731": "2304.08968v1",
        "732": "2107.12708v2",
        "733": "2305.09955v3",
        "734": "2305.13829v3",
        "735": "2404.00282v1",
        "736": "2403.11399v3",
        "737": "2401.09783v1",
        "738": "2307.08393v1",
        "739": "2307.08925v1",
        "740": "2209.04212v1",
        "741": "2402.04624v1",
        "742": "2210.10209v2",
        "743": "2403.05612v1",
        "744": "2309.17446v2",
        "745": "1906.00138v1",
        "746": "2401.15496v3",
        "747": "2303.01229v2",
        "748": "2401.01335v2",
        "749": "2404.15156v1",
        "750": "2308.03279v2",
        "751": "2303.10845v1",
        "752": "2307.04094v1",
        "753": "2203.05115v2",
        "754": "2310.13596v1",
        "755": "2310.12418v1",
        "756": "2312.02179v1",
        "757": "2404.16841v1",
        "758": "2301.09790v3",
        "759": "2307.10169v1",
        "760": "2101.07295v5",
        "761": "2402.11681v1",
        "762": "1904.10584v1",
        "763": "2301.06627v3",
        "764": "2401.09615v2",
        "765": "2304.03589v1",
        "766": "2402.07862v1",
        "767": "2005.05080v3",
        "768": "2209.09839v1",
        "769": "2309.11166v2",
        "770": "2301.05272v1",
        "771": "2312.00600v2",
        "772": "2307.02690v1",
        "773": "2310.14777v1",
        "774": "2305.15076v2",
        "775": "1706.08840v6",
        "776": "2306.06264v1",
        "777": "2310.17526v2",
        "778": "2308.12030v2",
        "779": "2109.13582v2",
        "780": "2308.09138v1",
        "781": "2312.13558v1",
        "782": "2004.05569v1",
        "783": "2301.13003v2",
        "784": "2305.01610v2",
        "785": "2309.15789v1",
        "786": "2309.12307v3",
        "787": "2003.03877v1",
        "788": "2110.00908v1",
        "789": "2105.10919v3",
        "790": "2104.12369v1",
        "791": "2402.02244v1",
        "792": "2108.13161v7",
        "793": "2401.17186v1",
        "794": "2310.10417v1",
        "795": "2403.11103v1",
        "796": "1804.07827v2",
        "797": "2402.03719v1",
        "798": "2305.12392v2",
        "799": "2401.16577v1",
        "800": "2308.15047v1",
        "801": "2311.15414v2",
        "802": "2402.00795v1",
        "803": "2404.02717v1",
        "804": "2307.12981v1",
        "805": "2402.04411v1",
        "806": "2305.11159v1",
        "807": "2403.07974v1",
        "808": "2310.07338v2",
        "809": "2110.07298v3",
        "810": "2306.10512v2",
        "811": "2310.10480v1",
        "812": "2305.14105v2",
        "813": "2401.08326v2",
        "814": "2311.10791v1",
        "815": "2311.01041v2",
        "816": "2006.15720v2",
        "817": "2310.02050v1",
        "818": "2307.15411v2",
        "819": "2401.14869v1",
        "820": "2305.04369v2",
        "821": "2306.02207v3",
        "822": "2311.08596v2",
        "823": "2309.16039v3",
        "824": "2206.08446v1",
        "825": "2209.11000v1",
        "826": "2308.13207v1",
        "827": "2209.01975v1",
        "828": "2306.03604v6",
        "829": "2402.14846v1",
        "830": "2212.01393v2",
        "831": "2404.06404v1",
        "832": "2312.16337v1",
        "833": "2202.01771v4",
        "834": "2403.06870v2",
        "835": "2402.01801v2",
        "836": "2306.13304v1",
        "837": "2204.04078v2",
        "838": "2210.06101v2",
        "839": "2305.11400v3",
        "840": "2305.00316v2",
        "841": "2404.05728v3",
        "842": "2303.12767v1",
        "843": "2309.17428v2",
        "844": "2102.12459v3",
        "845": "2311.02692v1",
        "846": "2401.05667v1",
        "847": "1910.01769v2",
        "848": "2311.01677v2",
        "849": "2402.04617v1",
        "850": "2311.07434v2",
        "851": "2404.09027v1",
        "852": "2401.02731v3",
        "853": "2309.03118v1",
        "854": "2306.06545v1",
        "855": "2401.11544v1",
        "856": "2301.04589v1",
        "857": "2404.12464v1",
        "858": "2310.13132v2",
        "859": "2404.00245v1",
        "860": "2310.05707v3",
        "861": "2403.06254v1",
        "862": "2303.03457v1",
        "863": "2402.15809v1",
        "864": "2310.19736v3",
        "865": "2307.07280v2",
        "866": "2310.16450v3",
        "867": "2310.08164v4",
        "868": "2308.01776v2",
        "869": "2401.00812v2",
        "870": "2310.07849v2",
        "871": "2404.00308v1",
        "872": "2308.16361v1",
        "873": "2310.04928v2",
        "874": "2308.12014v2",
        "875": "2404.01430v1",
        "876": "2312.00763v1",
        "877": "2311.11797v1",
        "878": "2401.15328v2",
        "879": "2402.12048v1",
        "880": "2310.11604v1",
        "881": "2403.11373v1",
        "882": "2002.09571v2",
        "883": "2307.05741v1",
        "884": "2306.06794v2",
        "885": "2404.10500v1",
        "886": "2305.13016v2",
        "887": "2305.14283v3",
        "888": "2311.09533v3",
        "889": "2304.03277v1",
        "890": "2208.04227v1",
        "891": "2311.04177v1",
        "892": "2310.02527v1",
        "893": "2402.00861v2",
        "894": "2312.15033v1",
        "895": "2210.05398v2",
        "896": "2402.10835v2",
        "897": "2304.08485v2",
        "898": "2402.10671v1",
        "899": "2402.11450v1",
        "900": "2401.12863v1",
        "901": "2401.04471v1",
        "902": "2403.19913v1",
        "903": "2308.04492v1",
        "904": "2309.13345v3",
        "905": "2306.13805v2",
        "906": "2401.16265v1",
        "907": "2310.16226v3",
        "908": "2310.08309v1",
        "909": "2111.11210v3",
        "910": "2309.08958v2",
        "911": "2402.18045v2",
        "912": "2308.10755v3",
        "913": "2402.16827v2",
        "914": "2305.15062v2",
        "915": "2305.13954v3",
        "916": "2210.02615v2",
        "917": "2403.09906v1",
        "918": "2311.13784v1",
        "919": "2402.01874v1",
        "920": "2402.17733v1",
        "921": "2402.12204v1",
        "922": "2404.02422v1",
        "923": "2312.16279v1",
        "924": "2310.06003v2",
        "925": "2402.15929v1",
        "926": "2308.06013v2",
        "927": "2304.01373v2",
        "928": "2308.02151v1",
        "929": "2211.15533v1",
        "930": "2309.16145v1",
        "931": "2305.12474v3",
        "932": "2310.05163v3",
        "933": "2402.04119v1",
        "934": "2306.17091v1",
        "935": "2308.14508v1",
        "936": "2305.18582v2",
        "937": "2110.04482v2",
        "938": "2310.20051v1",
        "939": "1705.09724v1",
        "940": "2010.00352v1",
        "941": "2311.08505v2",
        "942": "2402.02420v2",
        "943": "2212.10461v1",
        "944": "2401.14656v1",
        "945": "2404.04949v1",
        "946": "2402.15526v1",
        "947": "2310.13448v1",
        "948": "2401.16640v2",
        "949": "2308.14374v1",
        "950": "2403.02715v1",
        "951": "2402.16694v2",
        "952": "2312.04333v4",
        "953": "2310.05216v2",
        "954": "2305.16130v3",
        "955": "2310.06714v2",
        "956": "2305.14919v2",
        "957": "2404.08555v2",
        "958": "2403.09362v2",
        "959": "2310.05905v2",
        "960": "2404.04748v1",
        "961": "2304.04259v1",
        "962": "2404.12253v1",
        "963": "2312.16171v2",
        "964": "2309.00986v1",
        "965": "2401.05908v1",
        "966": "2310.15113v2",
        "967": "2310.10638v5",
        "968": "2211.00635v3",
        "969": "2402.08268v2",
        "970": "2404.05868v1",
        "971": "2402.09320v1",
        "972": "2310.09454v1",
        "973": "2305.13455v3",
        "974": "2404.06209v1",
        "975": "2308.03582v2",
        "976": "2308.06077v3",
        "977": "2212.06713v1",
        "978": "2404.04286v1",
        "979": "2309.17078v2",
        "980": "2209.04840v1",
        "981": "2311.00694v2",
        "982": "2109.08270v3",
        "983": "2102.01951v2",
        "984": "2402.11684v1",
        "985": "2308.12067v2",
        "986": "2310.19531v7",
        "987": "2404.02403v1",
        "988": "2310.05149v1",
        "989": "1909.03329v2",
        "990": "2403.16843v1",
        "991": "2010.00840v1",
        "992": "2401.08940v1",
        "993": "2310.17918v2",
        "994": "2305.14982v2",
        "995": "2305.13286v1",
        "996": "2305.11778v1",
        "997": "2305.03648v1",
        "998": "2401.08664v3",
        "999": "2305.18395v2",
        "1000": "2404.10384v1"
    }
}