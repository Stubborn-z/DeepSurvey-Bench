{
  "survey": "This survey presents a comprehensive examination of continual learning in large language models (LLMs), focusing on incremental learning techniques, adaptive neural network architectures, and model scalability strategies. It underscores the necessity for LLMs to integrate new information without forgetting previously acquired knowledge, addressing challenges such as catastrophic forgetting and resource constraints. The survey systematically explores memory replay, knowledge distillation, and transfer learning as key incremental learning strategies, while highlighting adaptive architectures like modular networks and dynamic routing that enhance model flexibility and efficiency. Model scalability is addressed through efficient resource allocation, parallel processing, and distributed learning, supported by benchmarks and evaluation metrics to ensure robust performance. Applications across biomedical, healthcare, legal, financial, and educational domains illustrate the transformative potential of LLMs, while future directions emphasize advancements in model architectures, optimization of learning strategies, and interdisciplinary approaches. By leveraging these insights, the survey aims to inspire ongoing research and innovation in the field of continual learning for LLMs, ultimately enhancing their applicability and efficacy across diverse domains.\n\nIntroduction Structure of the Survey This survey systematically explores continual learning in large language models, beginning with an introduction that underscores the significance of enabling models to learn continuously while retaining previously acquired knowledge. The introduction highlights incremental learning, adaptive neural networks, and model scalability as foundational elements for advancing artificial intelligence, addressing challenges such as continual task learning, representational interference, and catastrophic forgetting. These concepts are essential for developing generalized artificial learning systems that can adapt to new tasks, domains, or classes akin to natural intelligence, as demonstrated by task-incremental, domain-incremental, and class-incremental learning scenarios, alongside the progressive networks approach that effectively leverages prior knowledge [1,2,3]. Subsequently, the survey provides a background on large language models, detailing their current capabilities and the inherent challenges of continual learning, including catastrophic forgetting and the need for scalable solutions, which are crucial for identifying impactful areas for continual learning. The core of the survey comprises three main sections. The first section, Incremental Learning Techniques, examines various methods for updating models with new data while preserving previously learned information. Techniques such as memory replay, knowledge distillation, and transfer learning are discussed, emphasizing their roles in facilitating continual learning. The second section, Adaptive Neural Network Architectures, explores architectures that dynamically adjust to new tasks or data without compromising existing knowledge, including modular networks, architecture growth, dynamic routing, resource efficiency, and neuroscience-inspired adaptations. The third section, Model Scalability, addresses strategies for enhancing the scalability of large language models within the context of continual learning. It covers efficient resource allocation, parallel processing, distributed learning, and introduces benchmarks and metrics for evaluating scalability. The survey also presents Applications and Case Studies, highlighting real-world implementations across various domains. In biomedical and healthcare, the focus is on utilizing large-scale datasets like S2ORC for text mining to enhance research capabilities. The legal and financial sectors showcase adaptations of large language models, such as Lawyer LLaMA, which incorporate domain-specific knowledge and mitigate hallucination issues through expert-driven training. Continual learning frameworks in scientific research and education tackle challenges like catastrophic forgetting and promote adaptive knowledge acquisition. Visual and multimodal applications investigate the integration of fact extraction and verification datasets, such as FEVER, to improve accuracy in claim verification tasks [4,5,6,7]. These examples illustrate the practical benefits and challenges associated with continual learning. The survey concludes with a discussion on Future Directions, identifying potential areas for further research, emerging trends, technological advancements, and unresolved challenges, aiming to inspire ongoing exploration and innovation in the field of continual learning for large language models.The following sections are organized as shown in . Background Overview of Large Language Models Large Language Models (LLMs), driven by transformer architectures, have revolutionized natural language processing (NLP) by excelling in complex linguistic tasks. Models like BERT enhance bidirectional context understanding, boosting performance across various NLP applications [8]. The scalability of LLMs reveals emergent capabilities, suggesting potential groundbreaking advancements [9]. Their transformer-based architecture efficiently processes intricate linguistic and multimodal data, crucial for applications requiring comprehensive context integration [10]. Domain-specific models, such as FinBERT for financial sentiment analysis and BioGPT for biomedical NLP, leverage specialized training for improved outcomes [11,12]. However, complex training requirements and proprietary model dominance constrain broader exploration in healthcare [13]. Optimizing LLM performance in specialized fields demands domain-specific adaptations. General-domain models often fall short in specialized tasks, necessitating pretraining on relevant data [14]. Efficient finetuning techniques like QLoRA address memory constraints, enabling large model finetuning on limited hardware resources [15]. Models like BBT-FinT5 for Chinese financial NLP underscore the importance of domain-specific pretraining [16]. Despite advancements, adapting LLMs to specific tasks remains challenging, with a need for specialized benchmarks, such as those predicting textual values from structured knowledge bases [17]. Moreover, LLM development focuses primarily on major languages, leaving gaps for underrepresented languages, particularly in South-East Asia [9]. LLMs increasingly integrate rich textual and multimodal information, adapting to specific domains through techniques like domain-specific continual pre-training, supervised fine-tuning, and input tags. These advancements help overcome domain-specific knowledge deficiencies, enhancing performance in fields like law, medicine, e-commerce, and the physical and biomedical sciences. By leveraging expert knowledge, retrieval modules, and innovative data mixing strategies, LLMs transform insights across domains, improving their generalization to unseen problems and maintaining performance across diverse data distributions [7,18,19,20,21]. However, challenges in generalization, optimization, and inclusion of underrepresented languages persist, highlighting areas for future research and development. Challenges in Continual Learning Continual learning in LLMs faces challenges like catastrophic forgetting, scalability constraints, and inefficiencies in domain-specific applications. Catastrophic forgetting, prevalent in sequential task learning, deteriorates previously acquired knowledge as models adapt to new tasks [22]. Over-parameterization exacerbates this, causing overfitting to new data while neglecting prior features [22]. The alignment-forgetting trade-off complicates task alignment with past knowledge retention, especially with Reinforcement Learning with Human Feedback (RLHF) [23]. This alignment tax underscores the difficulty of maintaining high NLP task performance while aligning outputs with user intent. Scalability issues further complicate continual learning, as growing parameter counts demand substantial computational resources [9]. Fine-tuning large models is challenging due to memory requirements, limiting accessibility for researchers [15]. Current methods inadequately manage memory, hindering fine-tuning without extensive resources [15]. Moreover, benchmarks often overlook computational budget constraints, leading to unrealistic evaluations of continual learning methods [24]. Domain-specific applications face additional hurdles. General-domain benchmarks fail to capture specialized fields' nuances, like biomedical language processing [14]. In the Chinese financial sector, lacking tailored benchmarks complicates continual learning [16]. Inefficiencies in adapting pre-trained language models (PLMs) to new information are worsened by exhaustive pre-training costs [25]. Addressing these challenges requires innovative approaches to enhance scalability, mitigate catastrophic forgetting, and refine methodologies supporting continual learning across diverse domains, ensuring LLMs adapt to new information while preserving existing knowledge. Key Concepts in Continual Learning Continual learning in LLMs centers on incremental learning, adaptive architectures, and model scalability, addressing catastrophic forgetting and distribution shifts [22]. Incremental learning integrates new data without exhaustive retraining, preserving acquired knowledge while adapting to new information. Strategies like Learning without Forgetting (LwF) and Gradient Episodic Memory (GEM) mitigate distribution shifts, ensuring models retain capabilities while learning new tasks. Decomposing continual learning into sub-problems like Within-task Prediction (WP) and Task-id Prediction (TP) refines the approach for targeted learning [26]. Adaptive architectures enable models to adjust dynamically to new tasks or data. Innovations like LoRA, introducing trainable low-rank matrices while preserving pre-trained weights, showcase adaptive strategies' efficiency in reducing catastrophic forgetting [27]. Parameter-efficient fine-tuning (PEFT) methods, including (IA)^3, facilitate adaptation with fewer parameters and lower costs, highlighting adaptive architecture designs' effectiveness [28]. Frameworks like Mod-X maintain alignment of off-diagonal information in contrastive matrices, addressing performance decline and enhancing adaptive capabilities [29]. Model scalability is crucial for managing increasing data volumes and diverse tasks. Scalable frameworks generating domain-specific instruction data through multi-agent collaboration emphasize scalable solutions for incorporating targeted knowledge [9]. Benchmarks like ETHICS and the Hippocrates framework align AI outputs with human moral values and facilitate collaborative medical domain research. Larger models exhibit greater sample efficiency, underscoring scalable practices' necessity [9]. Benchmarks like IRCoder and ESMER support LLM evaluation in continual instruction tuning, focusing on catastrophic forgetting when introducing new tasks [30]. These key concepts—incremental learning, adaptive architectures, and model scalability—form the foundation for advancing continual learning in LLMs. By addressing catastrophic forgetting and optimizing resource utilization, these strategies ensure LLMs effectively adapt to new information while preserving existing knowledge, enhancing applicability across diverse domains. The proposed benchmark BBT-CFLEB, focusing on understanding and generation tasks specific to the Chinese financial domain, exemplifies tailored incremental learning approaches, highlighting ongoing efforts to refine and evaluate continual learning methodologies [16]. Incremental Learning Techniques Incremental learning is crucial for LLMs to maintain prior knowledge while learning new information. Core strategies, such as memory and experience replay, address catastrophic forgetting and improve LLM performance. illustrates the hierarchical structure of incremental learning techniques in LLMs, encompassing memory replay, experience replay, knowledge distillation, transfer learning, and dynamic and adaptive architectures. Each category highlights specific methods and architectures that facilitate continual learning, knowledge retention, and efficient adaptation in LLMs. Table offers a detailed comparison of key incremental learning techniques, emphasizing their distinct features and domain applications relevant to continual learning in large language models. This section explores these techniques and their significance in continual learning. Memory Replay and Experience Replay Memory and experience replay are essential for retaining prior knowledge during new data assimilation in LLMs. Gradient Episodic Memory (GEM) leverages episodic storage to guide learning and manage distribution shifts [31]. Learning without Forgetting (LwF) allows models to learn new tasks using only new data, maintaining previous task performance [32]. In domains like biomedicine, models like BioGPT utilize memory replay to enhance language generation through domain-specific pre-training [12]. Techniques like QLoRA balance memory efficiency and performance via 4-bit quantization and Low Rank Adapters [15]. Experience replay extends to multimodal data scenarios, exemplified by the Lifelong-MoE architecture, which uses a Mixture-of-Experts framework to adapt to evolving data distributions while preserving knowledge [22]. Methods such as ELLE facilitate lifelong pre-training, enabling LLMs to adapt without exhaustive retraining [25]. Frameworks like BBT-FinT5 offer structured benchmarks for evaluating these techniques against knowledge retention metrics [16]. Experiments with state-of-the-art models affirm these techniques' efficacy in continual model refinement settings [33]. Knowledge Distillation and Transfer Learning Knowledge distillation and transfer learning integrate new knowledge in LLMs while retaining existing information. Distillation transfers knowledge from larger to smaller models, maintaining performance with reduced computational demands, as seen in BBT-FinT5 for financial language [16]. Transfer learning applies pre-trained model features to new tasks, demonstrated by the Lifelong-MoE architecture which dynamically includes new experts, retaining learned information [22]. Innovative strategies like QLoRA, employing 4-bit NormalFloat data types, reduce memory needs while enabling efficient finetuning [15]. Gradient alignment further enhances knowledge transfer by mitigating past learning effects. Metrics in CMR settings emphasize balancing new information integration with existing knowledge preservation [33]. These evaluations highlight domain and task-adaptive pretraining techniques' role in developing efficient models that ensure stability-plasticity trade-offs and generalizability, thriving even under resource constraints [4,34]. Dynamic and Adaptive Architectures Dynamic and adaptive architectures enhance continual learning in LLMs by optimizing model architectures to adapt to new tasks and prevent catastrophic forgetting. Inspired by synaptic consolidation, these architectures protect neural weights, facilitating sequential learning [35,36]. The Lifelong-MoE architecture demonstrates model expansion via new experts and regularization to enhance adaptability and reduce forgetting [22]. ELLE exemplifies dynamic expansion for task-specific adaptations, especially under non-stationary data [25]. LoRA employs low-rank approximations to minimize retraining, reducing computational demands while maintaining performance [27]. In specialized domains, architectures such as BioGPT effectively generate domain-specific content, showing dynamic strategies' capacity to improve outcomes [12]. Dynamic architectures facilitate efficient knowledge integration, addressing continual learning challenges across applications. The Lawyer LLaMA framework incorporates domain-specific knowledge for legal problem-solving, and TRACE exposes trade-offs between task-specific performance and overall model proficiency. Techniques like supervised fine-tuning and reasoning-augmented learning mitigate hallucination and catastrophic forgetting, ensuring adaptation without performance declines [37,7]. Adaptive Neural Network Architectures Modular Networks and Architecture Growth Modular networks and architecture growth are pivotal for enhancing the continual learning capabilities of large language models (LLMs), enabling adaptation to new tasks while preserving prior knowledge. These approaches emphasize structural flexibility, facilitating seamless integration of new information. Techniques like Incremental Classifier and Representation Learning (iCaRL) demonstrate dynamic learning of classifiers and representations, crucial for evolving tasks without static constraints [38]. Efficient methodologies outlined by [39] minimize excessive retraining, maintaining performance while incorporating new tasks without significant memory requirements. Innovative architectures such as LoRRA exemplify adaptive modular networks by integrating reading, reasoning, and answering processes within continual learning frameworks [40]. This integration supports diverse task handling and promotes efficient knowledge assimilation, significantly reducing catastrophic forgetting risks. The Instruction-based Continual Learning (InsCL) framework utilizes high-quality data guided by an Instruction Information Metric, ensuring effective replay processes that mitigate forgetting and uphold model performance across tasks [41]. Furthermore, [22] discusses a method for integrating new experts during training, enhancing adaptability to new data distributions while preserving previously learned skills. Theoretical insights from [26] decompose continual learning into subtasks like Within-task Prediction and Task-id Prediction, facilitating understanding of out-of-distribution detection challenges. Benchmarks introduced by [33] provide realistic frameworks for evaluating continual learning in NLP, addressing dynamic production environment challenges. These strategies effectively tackle domain-specific challenges by integrating specialized knowledge, as evidenced by the Lawyer LLaMA model, which incorporates legal expertise and retrieval modules to reduce hallucinations. Benchmarks like TRACE highlight the importance of balancing task-specific performance with general capabilities, underscoring the need for strategies like Reasoning-augmented Continual Learning (RCL) to decrease catastrophic forgetting and improve task convergence [37,7]. By promoting structural flexibility and efficient knowledge integration, these strategies ensure models adapt to new tasks while preserving existing expertise, enhancing performance across diverse applications. Dynamic Routing and Resource Efficiency Dynamic routing and resource efficiency are critical in designing adaptive neural network architectures, enabling efficient management of computational resources while enhancing LLM continual learning capabilities. Incorporating dynamic routing allows models to adapt to evolving content, evidenced by improved performance in tasks like Country Hashtag Prediction and OffensEval 2019 [1,35]. Resource-efficient approaches mitigate catastrophic forgetting, allowing models to retain and integrate knowledge across diverse tasks, akin to human continuous learning. Dynamic routing techniques optimize resource allocation by facilitating selective activation of pathways based on specific tasks, minimizing unnecessary computations. The Lifelong-MoE architecture exemplifies dynamic routing through a Mixture-of-Experts framework, allocating computational resources to suitable experts based on input data [22]. This enhances adaptability to new tasks and ensures efficient resource utilization, crucial for scaling LLMs to manage increasing data volumes and task diversity. Resource-efficient architectures, such as LoRA, introduce low-rank adaptations that minimize extensive parameter updates, conserving computational resources while maintaining model performance [27]. Techniques like QLoRA enhance resource efficiency through quantization strategies that significantly reduce memory requirements, enabling effective fine-tuning of large models on limited hardware [15]. Paged optimizers further improve resource efficiency, allowing scalable model adaptation in resource-constrained environments. Dynamic routing and resource-efficient strategies are particularly advantageous in specialized domains. Models like BioGPT utilize efficient architectures to manage domain-specific tasks without exhaustive retraining [12]. This highlights dynamic routing's role in facilitating efficient learning processes, ensuring models adapt to new information while retaining existing knowledge. By optimizing resource allocation and enabling adaptive learning pathways, these techniques ensure models effectively integrate new knowledge while maintaining high performance across diverse applications [37,7,42]. Neuroscience-inspired Adaptations Neuroscience-inspired adaptations offer promising avenues for improving the continual learning capabilities of large language models (LLMs) by applying insights from biological systems to model architecture and learning strategies. These adaptations aim to combat catastrophic forgetting by integrating principles from human and animal learning to enhance LLMs' ability to accumulate and retain knowledge over time [43,1,44]. Advancements like Knowledge Aware FineTuning (KAFT) bolster LLM controllability and robustness, allowing models to prioritize contextually relevant information while reducing the impact of irrelevant data. Hybrid strategies combining multiple continual learning techniques, inspired by the brain's intricate neural processes, encourage exploration of new architectures leveraging biological principles to improve learning efficacy [1]. Such models potentially incorporate mechanisms similar to synaptic plasticity, enabling dynamic parameter adjustments in response to new information. Benchmarks reflecting the dynamic nature of language are vital for evaluating the effectiveness of neuroscience-inspired adaptations, aligning model evaluation with real-world linguistic variability [45]. Methods inspired by the human ability to learn new tasks from few examples, such as LfPT5, exemplify neuroscience-inspired strategies that facilitate efficient learning processes [46]. Innovations like belief graphs and SLAG training objectives enhance logical consistency and performance in belief-updating methods, paralleling the brain's ability to maintain coherent thought processes [47]. Additionally, gradient reversal layers in domain adaptation techniques support simultaneous training of discriminative features while mitigating domain shifts [48]. Integrating explanations into reasoning processes is proposed as crucial for enhancing AI performance in complex tasks like question answering [49]. Drawing from neuroscience principles, adaptations in artificial neural networks provide significant insights into enhancing LLM continual learning abilities. These insights stem from the human brain's innate capacity to learn and retain diverse, often conflicting, information over a lifetime—an ability current AI systems strive to emulate. By addressing challenges like catastrophic forgetting, these neuroscience-inspired approaches aim to improve LLMs' capacity to accumulate and apply knowledge across various domains, such as law and medicine [7,1,43,50]. Leveraging biological principles enhances models' capacity to integrate new knowledge while preserving existing information, ultimately improving applicability across diverse linguistic and cognitive tasks. Model Scalability Enhancing model scalability requires strategic exploration of resource management and computational efficiency. The following subsections focus on optimizing large language models (LLMs) through innovative techniques that balance performance with resource utilization, crucial for continual learning and adaptation. Efficient Resource Allocation Efficient resource allocation is critical for scaling LLMs to handle increasing data volumes and diverse tasks while maintaining optimal performance and adaptability. Techniques like WISE exemplify dual management approaches that optimize resource utilization, preventing conflicts and ensuring high performance without increased resource consumption [9]. ELLE demonstrates efficient integration of new data to enhance downstream task performance without complete retraining, aligning with resource allocation principles [25]. LoRA reduces trainable parameters, lowering GPU memory requirements and eliminating additional inference latency compared to traditional methods [27]. Innovations such as MEMIT enable simultaneous handling of thousands of memory updates, significantly improving upon existing limitations and optimizing resource use in dynamic learning environments [24]. The TextVQA benchmark provides structured evaluation of model performance, crucial for enhancing scalability in processing text-related visual information [40]. Focusing on informative samples improves accuracy and efficiency, highlighting the importance of intelligent resource allocation. Efficient resource allocation advances LLM scalability in continual learning contexts by integrating adaptive techniques and strategically utilizing domain-specific datasets. For example, domain-adaptive pretraining and task-specific fine-tuning yield superior results in specialized areas such as climate change research and ethical decision-making, aligning AI outputs with human values and reducing hallucinations, as demonstrated by initiatives like ClimateGPT and the ETHICS dataset [51,52,35,34]. Parallel Processing and Distributed Learning Parallel processing and distributed learning are essential for enhancing LLM scalability, enabling efficient handling of increasing data volumes and diverse tasks. These methodologies leverage simultaneous computations and distributed learning across multiple nodes, effectively managing computational resources and alleviating bottlenecks, particularly in online continual learning settings. This approach enhances adaptability to rapidly changing data streams while maintaining consistency without forgetting prior information, leading to significant performance improvements [53,24]. Parallel processing allows LLMs to distribute tasks across multiple processors, facilitating simultaneous execution and faster processing times. Frameworks utilizing multi-agent collaboration to generate domain-specific instruction data exemplify this approach, enhancing efficiency and scalability [9]. Distributed learning decentralizes the learning process, enabling models to learn from diverse data sources concurrently, beneficial when data is geographically dispersed or resources are limited. Techniques such as distributed training across multiple GPUs or cloud-based infrastructures demonstrate scalability and performance enhancements achievable through distributed learning [9]. Innovative strategies like paged optimizers in QLoRA highlight advancements in distributed learning by enabling efficient fine-tuning of large models on limited hardware, reducing memory requirements and optimizing resource allocation [15]. The integration of parallel processing and distributed learning ensures LLMs can effectively manage increasing data volumes and diverse tasks. By optimizing resource utilization and leveraging distributed infrastructures, these methodologies significantly enhance model adaptability and performance, supporting scalable AI-driven solutions across various domains. ClimateGPT models demonstrate adaptability by synthesizing interdisciplinary climate research and employing domain-specific datasets, while domain-adaptive and task-adaptive pretraining yield substantial performance gains across diverse domains. Efforts to align AI with human values, as seen with the ETHICS dataset, further underscore the potential for AI systems to incorporate moral reasoning, although sustainability considerations prompt reflection on the balance between model updating and ecological responsibility [51,52,54,34]. Scalability Benchmarks and Evaluation Metrics Scalability benchmarks and evaluation metrics are crucial for assessing the performance and adaptability of LLMs facing increasing data volumes and diverse tasks. Table provides a comprehensive overview of the scalability benchmarks and evaluation metrics critical for assessing the performance and adaptability of large language models across diverse tasks and domains. These tools provide structured frameworks to evaluate model scalability, ensuring LLMs can efficiently integrate new information while maintaining high performance across various applications. Metrics such as accuracy and F1-score are integral for evaluating LLM effectiveness in specialized domains, offering nuanced insights into model capabilities in contexts like financial language [16] and biomedical reasoning [14]. Benchmark tests for the PaLM model, a state-of-the-art 540-billion parameter language model, underscore the importance of scaling in language modeling and the need for robust evaluation frameworks to assess performance [55]. The IRCoder benchmark evaluates the correctness of generated code and the model's ability to handle tasks across multiple programming languages, emphasizing the significance of adaptability and scalability in diverse computational contexts [56]. In specialized fields, metrics are selected based on their effectiveness in measuring accuracy and relevance, such as in geoscience, where they assess model-generated response precision [57]. Time-sensitive tasks require metrics quantifying models' abilities to remember and predict time-sensitive facts, ensuring adaptability to dynamic information [58]. Future research should explore multimodal processing and develop benchmarks encompassing a broader variety of real-world tasks to enhance model scalability and applicability [59]. Scalability benchmarks and evaluation metrics are vital for advancing LLM capabilities. By incorporating structured frameworks for evaluating performance and adaptability, these tools ensure models can efficiently scale to meet diverse application demands. This approach supports robust AI-driven solutions across multiple domains, aligning AI with shared human values, as demonstrated by the ETHICS dataset, and enhancing performance through domain and task-specific adaptive pretraining techniques [52,34]. Applications and Case Studies Biomedical and Healthcare Large language models (LLMs) are revolutionizing the biomedical and healthcare sectors by improving diagnostics and patient care. BioGPT excels in generating accurate biomedical descriptions, crucial for processing complex medical information [12]. In drug discovery, the Tag-LLM framework predicts protein and chemical properties and models drug-target interactions, streamlining drug development [20]. The Hippocrates framework enhances medical LLMs, aiding healthcare professionals in diagnostic accuracy and treatment efficacy [13]. Domain-specific adaptations, supported by comprehensive biomedical text benchmarks, ensure LLMs handle specialized medical terminology effectively [14]. QLoRA facilitates the fine-tuning of large models on limited hardware, making advanced language models accessible to healthcare professionals [15]. These innovations, including continual pre-training and reinforcement learning, enable models like the Hippo family to outperform existing models, revolutionizing medical diagnostics and research while addressing complex training requirements [52,60,13,50]. Legal and Financial Domains LLMs are transforming legal and financial processes by enhancing efficiency and decision-making. In finance, FinBERT optimizes sentiment analysis on financial texts, crucial for interpreting market sentiment [11]. The BLADE framework enhances general LLMs with domain-specific knowledge in the legal domain, reducing costs associated with continuous pre-training [61]. BBT-FinT5 underscores the importance of domain-specific pretraining, particularly in the Chinese financial sector, for accurate financial analysis [16]. Despite challenges in domain-specific knowledge, advancements like Lawyer LLaMA show promise by injecting domain knowledge during continual training [7]. Frameworks for repurposing general-purpose LLMs for specialized domains, including custom input tags, boost performance across diverse fields [20,21]. Scientific Research and Education LLMs are enhancing scientific research and education by facilitating understanding and discovery across disciplines. GeoGalactica advances geoscience research by processing complex scientific data [57]. In education, benchmarks evaluate models' academic and professional understanding, supporting AI-driven tools that enhance learning outcomes [62]. CodeT5 captures semantic nuances in code, improving educational tools and learning experiences [63]. Multimodal reasoning benchmarks guide the development of robust educational tools by integrating diverse data sources [49]. These advancements highlight AI's role in addressing challenges like climate change and improving software engineering through neural code intelligence [51,52,35,50]. Visual and Multimodal Applications LLMs are advancing visual and multimodal applications by integrating diverse data types and improving task performance. The VizWiz dataset aids visually impaired individuals through accurate visual question answering [64]. LLaVA achieves high accuracy on the Science QA benchmark, integrating linguistic and visual information for nuanced interactions [65]. The DROP benchmark emphasizes LLM versatility in reading comprehension tasks [66]. Climate research frameworks evaluate models' ability to integrate scientific perspectives, supporting advancements in climate science [51]. These innovations, including ClimateGPT's synthesis of interdisciplinary climate research, enhance accessibility and domain-specific insights [40,50,52,1,51]. The ETHICS dataset reflects progress toward AI systems that align with human values, while advancements in Neural Code Intelligence improve programming efficiency [40,50,52,1,51]. Future Directions Enhancements in Model Architectures and Scalability Future advancements in model architectures and scalability are essential for the continued evolution of large language models (LLMs), particularly in managing complex datasets and diverse applications. Research should focus on optimizing model efficiency and integrating sophisticated sampling techniques to enhance adaptability, thereby improving instruction compositionality for seamless task integration [67]. Addressing catastrophic forgetting during continual instruction tuning remains a priority, impacting future model development strategies [30]. Exploring low-rank adaptation processes, such as LoRA, across various models and tasks is crucial [27], alongside enhancing prompt learning processes and validating techniques like (IA)^3 across different tasks [28]. Transfer learning requires expanding benchmarks to encompass more complex tasks and investigating alternative architectures for improvements [68]. Enhancing sequential learning capabilities and exploring mechanisms for knowledge retention are vital [31]. Research should aim to bolster robustness in models trained on existing benchmarks and explore applications across varied domains [39]. Optimizing continual learning processes while incorporating diverse tasks and data sources is critical [9]. Enhancing benchmarks to include a broader range of relational queries and investigating the types of knowledge language models can recall are promising directions. Improvements to models like Llemma for broader mathematical problems could yield significant benefits. Innovations in models such as LoRRA show potential for improving architectures, particularly in enhancing continual learning capabilities in visual question answering [40]. Future research could refine these architectures to tackle natural language complexities, as demonstrated by the Wikireading benchmark [17]. Expanding training data for models like BioGPT to cover more biomedical topics could enhance generative abilities and applicability in the biomedical field [12]. Enhancing the Learning without Forgetting (LwF) method for greater adaptability to diverse tasks is another promising avenue [32]. Refining theoretical models and exploring additional methods to enhance Continual Incremental Learning (CIL) performance in complex environments are crucial [26]. Expanding benchmarks with diverse biomedical datasets could improve model performance in specialized fields [14]. Research should investigate optimizations in memory management and the application of QLoRA to larger models, enhancing chatbot performance evaluations [15]. Enhancing regularization strategies in Lifelong-MoE, exploring scalability to larger models, and applying these strategies beyond NLP are promising directions [22]. Refining model expansion techniques and exploring additional applications of ELLE in various domains could propel the field forward [25]. Developing efficient continual learning methods under budget constraints remains critical [24]. Expanding datasets to include a wider range of financial topics is essential for advancing model architectures and scalability [16]. Refining benchmarks and exploring metrics and adaptations to enhance performance in out-of-distribution (OOD) scenarios are crucial for future advancements [33]. Advancements in model architectures and scalability techniques are vital for LLM development, enabling models like LLaMA and DeepSeek to overcome domain-specific knowledge deficiencies and improve performance in specialized fields such as law and medicine. Integrating domain knowledge during training and employing techniques like supervised fine-tuning and retrieval modules can address challenges like hallucination, enhancing domain-related task performance. Scaling laws and innovative data strategies, exemplified by DeepSeek's utilization of a 2 trillion token dataset, facilitate the creation of more capable open-source models that excel in areas such as code, mathematics, and reasoning, outperforming existing models like LLaMA-2 70B and GPT-3.5 [7,69]. Optimizing resource allocation and leveraging innovative methodologies will support LLM application across diverse domains and tasks, ensuring sustained efficacy and adaptability. Optimization of Learning Strategies Optimizing learning strategies in continual learning for LLMs requires innovative approaches to address catastrophic forgetting, resource constraints, and enhance model adaptability and performance. Developing hybrid learning frameworks that integrate supervised, unsupervised, and reinforcement learning paradigms can leverage strengths across diverse scenarios [23]. These frameworks improve model generalization across tasks and domains, enhancing learning efficiency and reducing the risk of forgetting acquired knowledge. Adaptive learning rates and dynamic weighting schemes offer promising strategies for optimizing learning processes. Adjusting learning rates based on task complexity and model performance ensures appropriate resource allocation without compromising previously learned information [9]. Dynamic weighting schemes prioritize challenging tasks or those with higher potential for knowledge transfer, enhancing learning efficiency and adaptability [23]. Incorporating meta-learning techniques into continual learning frameworks presents another optimization avenue. Meta-learning enables rapid adaptation to new tasks by leveraging prior experiences and identifying optimal strategies [22], reducing time and resources required for model adaptation while enhancing overall learning efficiency. Memory-augmented neural networks (MANNs) offer a novel strategy for optimizing learning in LLMs. Integrating external memory modules for information storage and retrieval facilitates retention and recall of learned knowledge, mitigating catastrophic forgetting effects [31]. This memory augmentation allows models to efficiently manage information across tasks, enhancing continual learning capabilities. Developing task-specific learning modules that optimize performance for particular domains or applications is crucial for enhancing learning strategies. Tailored modules address unique challenges and requirements, ensuring high performance while adapting to new information [14]. Aligning learning strategies with task-specific needs enables LLMs to achieve greater efficiency and effectiveness in continual learning processes. Optimizing learning strategies involves integrating hybrid frameworks addressing domain-specific knowledge gaps, employing adaptive learning rates to balance stability and plasticity, utilizing meta-learning techniques to enhance task generalization, augmenting memory to mitigate catastrophic forgetting, and incorporating task-specific modules to maintain performance across diverse tasks [4,7,70,37]. These strategies collectively enhance model adaptability to new information while preserving existing knowledge, ensuring sustained performance across diverse domains and tasks. Interdisciplinary Approaches and Applications Emerging trends in continual learning for LLMs highlight the significance of interdisciplinary approaches integrating insights from cognitive science and machine learning to enhance learning methodologies in NLP [43]. These approaches leverage strengths across disciplines to tackle challenges in continual learning, such as catastrophic forgetting, distribution shifts, and resource constraints. Cognitive science provides valuable insights into human learning processes, emphasizing mechanisms like incremental knowledge acquisition, context-dependent interpretation, and balancing stability and plasticity. These insights inform effective continual learning strategies for LLMs, which face challenges like catastrophic forgetting and inter-task class separation. Leveraging cognitive principles enhances LLMs' ability to learn from continuous data streams, transfer knowledge across tasks, and maintain performance on learned tasks, facilitating adaptive language processing systems [4,43,71]. Mimicking human cognitive abilities, such as memory retention and adaptive learning, enables models to integrate new information while preserving acquired knowledge, leading to frameworks aligned with human cognitive processes and enhancing task generalization. Machine learning provides a foundation for developing advanced algorithms and architectures supporting continual learning in LLMs. Techniques like reinforcement learning, transfer learning, and meta-learning can be combined with cognitive science principles to create hybrid frameworks optimizing model adaptability and performance. These frameworks enhance the model's ability to learn continuously and efficiently from diverse data sources, utilizing methods like Model Zoo for improved accuracy through synergistic task interactions, and ScienceQA for enhanced question-answering performance through multimodal reasoning and chain-of-thought processes. Additionally, frameworks like Lawyer LLaMA adapt LLMs to specific domains by integrating domain knowledge and retrieval modules, while code-focused LLMs benefit from comment augmentation strategies aligning programming languages with natural languages to improve coding task performance [72,73,49,7]. Interdisciplinary approaches extend to domains like healthcare, finance, and education, where LLMs process complex information and generate insights. Leveraging domain-specific knowledge with cognitive and machine learning principles allows models to address distinct challenges and demands of applications. Research shows domain-adaptive pretraining significantly enhances performance across tasks, while task-adaptive pretraining boosts performance even after domain adaptation. In fields like law or medicine, integrating expert knowledge during training and utilizing retrieval modules mitigate issues like hallucination, improving domain-specific skills application. These strategies underscore the importance of multi-phase adaptive pretraining and expert-driven data augmentation for optimizing model efficacy in specialized domains [7,34]. Interdisciplinary approaches and applications, such as integrating domain-specific knowledge into LLMs like Lawyer LLaMA, hold promise for advancing continual learning. Employing methods like supervised fine-tuning and retrieval modules to mitigate catastrophic forgetting and hallucination enables LLMs to adapt effectively to domains like law and medicine. This enhances models' ability to incrementally acquire and apply knowledge over time, improving context-dependent task performance, demonstrating potential to push continual learning boundaries in LLMs [4,7,43]. Integrating insights from cognitive science and machine learning enhances model adaptability to new information while preserving existing knowledge, ensuring sustained performance across diverse domains and tasks. Conclusion This survey emphasizes the transformative progress in continual learning strategies for large language models, underscoring their critical role in enabling these models to assimilate new information while preserving existing knowledge. Despite these advancements, significant challenges persist in optimizing the balance between knowledge retention and performance across diverse tasks and domains. The exploration of models like Continual-T0 demonstrates the efficacy of fine-tuning approaches in facilitating continual learning, showcasing their ability to seamlessly integrate new tasks without compromising performance on previously learned ones. These findings highlight the promise of continual learning methodologies in advancing artificial intelligence, offering robust frameworks for adapting to ever-evolving informational landscapes. By addressing these challenges through interdisciplinary research and innovative strategies, continual learning for large language models holds the potential to enhance their utility and influence across various fields, driving forward the evolution of AI technologies.",
  "reference": {
    "1": "2112.14146v1",
    "2": "2405.08015v1",
    "3": "1606.04671v4",
    "4": "2302.00487v3",
    "5": "1911.02782v3",
    "6": "1803.05355v3",
    "7": "2305.15062v2",
    "8": "1909.01066v2",
    "9": "2001.08361v1",
    "10": "2404.07965v4",
    "11": "1908.10063v1",
    "12": "2210.10341v3",
    "13": "2404.16621v1",
    "14": "2007.15779v6",
    "15": "2305.14314v1",
    "16": "2302.09432v2",
    "17": "1608.03542v2",
    "18": "2110.08534v3",
    "19": "2302.03241v4",
    "20": "2402.05140v3",
    "21": "2312.15696v1",
    "22": "2305.12281v1",
    "23": "2309.06256v4",
    "24": "2303.11165v2",
    "25": "2203.06311v2",
    "26": "2211.02633v1",
    "27": "2106.09685v2",
    "28": "2205.05638v2",
    "29": "2305.07437v5",
    "30": "2401.03129v1",
    "31": "1706.08840v6",
    "32": "1606.09282v3",
    "33": "2205.02014v1",
    "34": "2004.10964v3",
    "35": "2106.06297v1",
    "36": "1612.00796v2",
    "37": "2310.06762v1",
    "38": "1611.07725v2",
    "39": "2112.09153v2",
    "40": "1904.08920v2",
    "41": "2403.11435v1",
    "42": "2402.14270v2",
    "43": "2012.09823v1",
    "44": "2211.05110v1",
    "45": "2102.01951v2",
    "46": "2110.07298v3",
    "47": "2111.13654v1",
    "48": "1505.07818v4",
    "49": "2209.09513v2",
    "50": "2403.14734v5",
    "51": "2401.09646v1",
    "52": "2008.02275v6",
    "53": "2305.09253v2",
    "54": "2210.07365v2",
    "55": "2204.02311v5",
    "56": "2403.03894v3",
    "57": "2401.00434v2",
    "58": "2106.15110v2",
    "59": "2303.08774v6",
    "60": "2311.00204v1",
    "61": "2403.18365v1",
    "62": "2009.03300v3",
    "63": "2109.00859v1",
    "64": "1802.08218v4",
    "65": "2304.08485v2",
    "66": "1903.00161v2",
    "67": "2110.08207v3",
    "68": "2310.14152v1",
    "69": "2401.02954v1",
    "70": "2110.03215v4",
    "71": "2211.12701v2",
    "72": "2106.03027v3",
    "73": "2402.13013v1"
  },
  "chooseref": {
    "1": "2302.00487v3",
    "2": "1907.11692v1",
    "3": "2401.00434v2",
    "4": "2403.14734v5",
    "5": "2310.19704v3",
    "6": "2211.02633v1",
    "7": "2310.12244v1",
    "8": "2112.02706v1",
    "9": "2003.09553v2",
    "10": "2211.11031v5",
    "11": "2008.02275v6",
    "12": "2112.09153v2",
    "13": "2308.08747v5",
    "14": "2010.11929v2",
    "15": "2302.09432v2",
    "16": "2210.10341v3",
    "17": "2308.09442v2",
    "18": "2403.18365v1",
    "19": "2301.12597v3",
    "20": "2210.03329v2",
    "21": "2303.01081v1",
    "22": "2309.10654v2",
    "23": "2306.12619v2",
    "24": "2308.12950v3",
    "25": "2402.13013v1",
    "26": "2404.05875v1",
    "27": "2203.13474v5",
    "28": "2305.07922v2",
    "29": "2109.00859v1",
    "30": "2102.04664v2",
    "31": "2403.08350v2",
    "32": "2303.11165v2",
    "33": "2309.14763v1",
    "34": "2311.16206v1",
    "35": "2211.12701v2",
    "36": "2311.01200v4",
    "37": "2012.09823v1",
    "38": "2404.17790v1",
    "39": "2205.09357v1",
    "40": "2302.03241v4",
    "41": "2308.04014v2",
    "42": "2210.05549v1",
    "43": "2305.07437v5",
    "44": "2311.00204v1",
    "45": "2207.06543v1",
    "46": "1909.00277v2",
    "47": "2108.05036v2",
    "48": "2004.07211v2",
    "49": "2401.02954v1",
    "50": "2401.14196v2",
    "51": "2305.18290v3",
    "52": "2111.13654v1",
    "53": "2301.04213v2",
    "54": "1505.07818v4",
    "55": "2007.15779v6",
    "56": "2403.10056v4",
    "57": "2004.10964v3",
    "58": "1903.00161v2",
    "59": "2204.04799v2",
    "60": "2106.06297v1",
    "61": "2012.15283v3",
    "62": "2203.06311v2",
    "63": "2312.15696v1",
    "64": "2004.00345v2",
    "65": "2104.08164v2",
    "66": "1812.00420v2",
    "67": "2206.07682v2",
    "68": "1907.12412v2",
    "69": "2302.11344v1",
    "70": "2401.03129v1",
    "71": "1910.10683v4",
    "72": "2203.07228v1",
    "73": "2110.11309v2",
    "74": "1803.05355v3",
    "75": "2205.05638v2",
    "76": "1908.10063v1",
    "77": "2205.12393v4",
    "78": "2109.01652v5",
    "79": "2403.05530v5",
    "80": "1511.02283v3",
    "81": "2403.18383v1",
    "82": "2103.08541v1",
    "83": "2206.15331v2",
    "84": "2303.08774v6",
    "85": "1902.09506v3",
    "86": "1706.08840v6",
    "87": "2404.16621v1",
    "88": "2311.09774v2",
    "89": "2310.02995v3",
    "90": "2002.01808v5",
    "91": "2403.11435v1",
    "92": "2402.12847v2",
    "93": "2311.16208v2",
    "94": "2403.07082v1",
    "95": "2305.05968v1",
    "96": "2403.03894v3",
    "97": "2210.07365v2",
    "98": "2306.05064v2",
    "99": "1806.03822v1",
    "100": "2005.14165v4",
    "101": "1909.01066v2",
    "102": "2211.05110v1",
    "103": "2403.11901v4",
    "104": "2305.15062v2",
    "105": "2209.09513v2",
    "106": "1810.11910v3",
    "107": "2112.08654v2",
    "108": "2103.00020v1",
    "109": "1606.09282v3",
    "110": "2110.07298v3",
    "111": "2305.12281v1",
    "112": "2110.08534v3",
    "113": "2307.09288v2",
    "114": "2310.10631v3",
    "115": "2310.14029v1",
    "116": "2106.09685v2",
    "117": "2202.05262v5",
    "118": "2106.09685v2",
    "119": "2210.07229v2",
    "120": "2009.03300v3",
    "121": "2305.17529v1",
    "122": "1711.09601v4",
    "123": "2206.06520v1",
    "124": "2102.01951v2",
    "125": "2403.01244v2",
    "126": "2309.06256v4",
    "127": "2106.03027v3",
    "128": "2110.08207v3",
    "129": "2204.05660v1",
    "130": "2310.02031v8",
    "131": "2205.02014v1",
    "132": "1312.3005v3",
    "133": "2305.09253v2",
    "134": "1805.07810v1",
    "135": "2302.13971v1",
    "136": "2310.14152v1",
    "137": "1612.00796v2",
    "138": "2305.10403v3",
    "139": "2204.02311v5",
    "140": "2310.04801v1",
    "141": "1911.11641v1",
    "142": "1810.04805v2",
    "143": "1606.04671v4",
    "144": "1707.06347v2",
    "145": "2304.01373v2",
    "146": "1910.11473v2",
    "147": "2305.14314v1",
    "148": "1908.05803v2",
    "149": "1704.04683v5",
    "150": "1908.05852v2",
    "151": "2004.12651v1",
    "152": "2305.08702v1",
    "153": "1907.03799v3",
    "154": "2107.12308v5",
    "155": "2404.07965v4",
    "156": "1911.02782v3",
    "157": "2404.03608v1",
    "158": "2403.03883v2",
    "159": "2001.08361v1",
    "160": "2403.08763v4",
    "161": "2209.09476v1",
    "162": "2402.19173v1",
    "163": "2305.06161v2",
    "164": "2401.10716v1",
    "165": "2402.05140v3",
    "166": "2402.14270v2",
    "167": "2204.14211v3",
    "168": "2001.08435v1",
    "169": "1803.05457v1",
    "170": "2405.08015v1",
    "171": "2310.16226v3",
    "172": "2110.06366v4",
    "173": "2106.15110v2",
    "174": "2202.03829v2",
    "175": "2401.09646v1",
    "176": "2110.03215v4",
    "177": "2112.14146v1",
    "178": "2309.06126v1",
    "179": "1904.08920v2",
    "180": "2310.06762v1",
    "181": "2204.05862v1",
    "182": "2203.15556v1",
    "183": "2203.02155v1",
    "184": "2301.09785v1",
    "185": "2403.01554v1",
    "186": "2305.07972v1",
    "187": "1906.02425v2",
    "188": "1602.01585v1",
    "189": "2304.08485v2",
    "190": "1802.08218v4",
    "191": "2402.01865v3",
    "192": "1608.03542v2",
    "193": "2405.14768v3",
    "194": "2306.08568v2",
    "195": "2308.09583v3",
    "196": "1706.04115v1",
    "197": "2312.11805v5",
    "198": "1611.07725v2"
  }
}