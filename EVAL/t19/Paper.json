{
  "authors": [
    "Haizhou Shi",
    "Zihao Xu",
    "Hengyi Wang",
    "Weiyi Qin",
    "Wenyuan Wang",
    "Yibin Wang",
    "Hao Wang"
  ],
  "literature_review_title": "Continual Learning of Large Language Models: A Comprehensive Survey",
  "year": "2024",
  "date": "2024-04-25",
  "category": "cs.LG",
  "abstract": "The recent success of large language models (LLMs) trained on static,\npre-collected, general datasets has sparked numerous research directions and\napplications. One such direction addresses the non-trivial challenge of\nintegrating pre-trained LLMs into dynamic data distributions, task structures,\nand user preferences. Pre-trained LLMs, when tailored for specific needs, often\nexperience significant performance degradation in previous knowledge domains --\na phenomenon known as \"catastrophic forgetting\". While extensively studied in\nthe continual learning (CL) community, it presents new manifestations in the\nrealm of LLMs. In this survey, we provide a comprehensive overview of the\ncurrent research progress on LLMs within the context of CL. This survey is\nstructured into four main sections: we first describe an overview of\ncontinually learning LLMs, consisting of two directions of continuity: vertical\ncontinuity (or vertical continual learning), i.e., continual adaptation from\ngeneral to specific capabilities, and horizontal continuity (or horizontal\ncontinual learning), i.e., continual adaptation across time and domains\n(Section 3). We then summarize three stages of learning LLMs in the context of\nmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),\nand Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of\nevaluation protocols for continual learning with LLMs, along with the current\navailable data sources (Section 5). Finally, we discuss intriguing questions\npertaining to continual learning for LLMs (Section 6). The full list of papers\nexamined in this survey is available at\nhttps://github.com/Wang-ML-Lab/llm-continual-learning-survey.",
  "structure": [
    {
      "section_title": "Introduction/Pre-section",
      "level": "1",
      "content": "rotating \\documentclass[manuscript,screen,authorversion,nonacm]{acmart} % \\providecommand\\BibTeX{{% Bib\\TeX}} none 2024 2024 XXXXXXX.XXXXXXX CSUR paralist \\usepackage[utf8]{inputenc} % allow utf-8 input \\usepackage[T1]{fontenc} % use 8-bit T1 fonts hyperref % hyperlinks url % simple URL typesetting wrapfig,lipsum,booktabs % professional-quality tables amsfonts % blackboard math symbols nicefrac % compact symbols for 1/2, etc. microtype % microtypography colortbl pifont rotating makecell bbm caption subcaption amsmath xspace multirow algorithm algpseudocode \\usepackage[titletoc]{appendix} pdflscape \\haizhou[1]{cyan{[Haizhou: #1]}} \\wenyuan[1]{magenta{[Wenyuan: #1]}} \\hao[1]{violet{[Hao: #1]}} \\zihao[1]{blue{[Zihao: #1]}} \\sayna[1]{Mahogany{[Sayna: #1]}} \\zifeng[1]{Salmon{[Zifeng: #1]}} \\todo[1]{red{[TODO: #1]}} \\tocite{Tan{[cite]}\\xspace} \\toref{OrangeRed{[ref]}\\xspace} \\newlength\\savewidth\\newcommand\\noalign{\\global\\savewidth\\arrayrulewidth \\global\\arrayrulewidth 1pt\\hline\\global\\arrayrulewidth\\savewidth} \\tablestyle[2]{\\tabcolsep{#1}\\arraystretch{#2}\\centering\\footnotesize} C{>{\\centering\\let\\newline\\\\\\arraybackslash0pt}m{2cm}} \\error[1]{\\epsilon_{\\gD_{#1}}} \\erroremp[1]{\\epsilon_{\\gD_{#1}}} \\blah{ {white{blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah}} } \\BLAH{ \\blah\\blah\\blah\\blah\\blah\\blah } \\cmark{51}% \\xmark{55}% \\club{168} gray1{HTML}{f2f2f2} gray2{HTML}{e5e5e5} gray3{HTML}{cccccc} cream{HTML}{fef9d2} green1{HTML}{e2fee2} blue1{HTML}{d7e0f9} \\usepackage[pscoord]{eso-pic}% The zero point of the coordinate systemis the lower left corner of the page (the default). \\placetextbox[3]{% <horizontal pos>{<vertical pos>}{<stuff>} \\setbox0=#3% Put <stuff> in a box \\AddToShipoutPictureFG*{% Add <stuff> to current page foreground \\put(#1\\paperwidth,#2\\paperheight){{\\null\\makebox[0pt][c]{#3}}}% }% }% amsmath,amsfonts,bm,eqnarray cancel amsthm tikz tikzmark definition{Definition}[section] theorem{Theorem}[section] corollary{Corollary}[theorem] lemma[theorem]{Lemma} \\newtheorem*{remark}{Remark} proposition{Proposition}[theorem] innercustomgeneric{\\customgenericname} \\customgenericname{} \\newcustomtheorem[2]{% #1[1] {% \\renewcommand#2% \\renewcommand##1% \\innercustomgeneric } {\\endinnercustomgeneric} } customThm{Theorem} customLemma{Lemma} customCor{Corollary} customProposition{Proposition} \\figleft{{\\em (Left)}} \\figcenter{{\\em (Center)}} \\figright{{\\em (Right)}} \\figtop{{\\em (Top)}} \\figbottom{{\\em (Bottom)}} \\captiona{{\\em (a)}} \\captionb{{\\em (b)}} \\captionc{{\\em (c)}} \\captiond{{\\em (d)}} \\newterm[1]{{\\bf #1}} \\circhat{\\circ} \\def\\Tabref#1{Table~#1} \\def\\twoTabref#1#2{Table~#1 and #2} \\def\\Corref#1{Corra~#1} \\def\\Defref#1{Definition~#1} \\def\\Lmmref#1{Lemma~#1} \\def\\Thmref#1{Theorem~#1} \\def\\figref#1{Fig.~#1} \\def\\Figref#1{Fig.~#1} \\def\\twofigref#1#2{figures #1 and #2} \\def\\quadfigref#1#2#3#4{figures #1, #2, #3 and #4} \\def\\secref#1{Sec.~#1} \\def\\appref#1{Appendix~#1} \\def\\Secref#1{Section~#1} \\def\\twosecrefs#1#2{sections #1 and #2} \\def\\secrefs#1#2#3{sections #1, #2 and #3} \\def\\eqref#1{equation~#1} \\def\\Eqref#1{Eqn.~#1} \\def\\plaineqref#1{#1} \\def\\chapref#1{chapter~#1} \\def\\Chapref#1{Chapter~#1} \\def\\rangechapref#1#2{chapters#1--#2} \\def\\algref#1{algorithm~#1} \\def\\Algref#1{Algorithm~#1} \\def\\twoalgref#1#2{algorithms #1 and #2} \\def\\Twoalgref#1#2{Algorithms #1 and #2} \\def\\partref#1{part~#1} \\def\\Partref#1{Part~#1} \\def\\twopartref#1#2{parts #1 and #2} \\def\\ceil#1{\\lceil #1 \\rceil} \\def\\floor#1{\\lfloor #1 \\rfloor} \\def\\1{1} \\train{D} \\valid{D_{\\mathrm{valid}}} \\test{D_{\\mathrm{test}}} \\def{\\epsilon} \\def{\\textnormal{$\\eta$}} \\def{\\textnormal{a}} \\def{\\textnormal{b}} \\def{\\textnormal{c}} \\def{\\textnormal{d}} \\def{\\textnormal{e}} \\def{\\textnormal{f}} \\def{\\textnormal{g}} \\def{\\textnormal{h}} \\def{\\textnormal{i}} \\def{\\textnormal{j}} \\def{\\textnormal{k}} \\def{\\textnormal{l}} \\def{\\textnormal{n}} \\def{\\textnormal{o}} \\def{\\textnormal{p}} \\def{\\textnormal{q}} \\def{\\textnormal{r}} \\def{\\textnormal{s}} \\def{\\textnormal{t}} \\def{\\textnormal{u}} \\def{\\textnormal{v}} \\def{\\textnormal{w}} \\def{\\textnormal{x}} \\def{\\textnormal{y}} \\def{\\textnormal{z}} \\def{\\mathbf{\\epsilon}} \\def{\\mathbf{\\theta}} \\def{\\mathbf{\\phi}} \\def{\\mathbf{a}} \\def{\\mathbf{b}} \\def{\\mathbf{c}} \\def{\\mathbf{d}} \\def{\\mathbf{e}} \\def{\\mathbf{f}} \\def{\\mathbf{g}} \\def{\\mathbf{h}} \\def{\\mathbf{i}} \\def{\\mathbf{j}} \\def{\\mathbf{k}} \\def{\\mathbf{l}} \\def{\\mathbf{m}} \\def{\\mathbf{n}} \\def{\\mathbf{o}} \\def{\\mathbf{p}} \\def{\\mathbf{q}} \\def{\\mathbf{r}} \\def{\\mathbf{s}} \\def{\\mathbf{t}} \\def{\\mathbf{u}} \\def{\\mathbf{v}} \\def{\\mathbf{w}} \\def{\\mathbf{x}} \\def{\\mathbf{y}} \\def{\\mathbf{z}} \\def{\\textnormal{a}} \\def{\\textnormal{b}} \\def{\\textnormal{c}} \\def{\\textnormal{d}} \\def{\\textnormal{e}} \\def{\\textnormal{f}} \\def{\\textnormal{g}} \\def{\\textnormal{h}} \\def{\\textnormal{i}} \\def{\\textnormal{j}} \\def{\\textnormal{k}} \\def{\\textnormal{l}} \\def{\\textnormal{m}} \\def{\\textnormal{n}} \\def{\\textnormal{o}} \\def{\\textnormal{p}} \\def{\\textnormal{q}} \\def{\\textnormal{r}} \\def{\\textnormal{s}} \\def{\\textnormal{t}} \\def{\\textnormal{u}} \\def{\\textnormal{v}} \\def{\\textnormal{w}} \\def{\\textnormal{x}} \\def{\\textnormal{y}} \\def{\\textnormal{z}} \\def{\\mathbf{A}} \\def{\\mathbf{B}} \\def{\\mathbf{C}} \\def{\\mathbf{D}} \\def{\\mathbf{E}} \\def{\\mathbf{F}} \\def{\\mathbf{G}} \\def{\\mathbf{H}} \\def{\\mathbf{I}} \\def{\\mathbf{J}} \\def{\\mathbf{K}} \\def{\\mathbf{L}} \\def{\\mathbf{M}} \\def{\\mathbf{N}} \\def{\\mathbf{O}} \\def{\\mathbf{P}} \\def{\\mathbf{Q}} \\def{\\mathbf{R}} \\def{\\mathbf{S}} \\def{\\mathbf{T}} \\def{\\mathbf{U}} \\def{\\mathbf{V}} \\def{\\mathbf{W}} \\def{\\mathbf{X}} \\def{\\mathbf{Y}} \\def{\\mathbf{Z}} \\def{\\textnormal{A}} \\def{\\textnormal{B}} \\def{\\textnormal{C}} \\def{\\textnormal{D}} \\def{\\textnormal{E}} \\def{\\textnormal{F}} \\def{\\textnormal{G}} \\def{\\textnormal{H}} \\def{\\textnormal{I}} \\def{\\textnormal{J}} \\def{\\textnormal{K}} \\def{\\textnormal{L}} \\def{\\textnormal{M}} \\def{\\textnormal{N}} \\def{\\textnormal{O}} \\def{\\textnormal{P}} \\def{\\textnormal{Q}} \\def{\\textnormal{R}} \\def{\\textnormal{S}} \\def{\\textnormal{T}} \\def{\\textnormal{U}} \\def{\\textnormal{V}} \\def{\\textnormal{W}} \\def{\\textnormal{X}} \\def{\\textnormal{Y}} \\def{\\textnormal{Z}} \\def{\\bm{0}} \\def{\\bm{1}} \\def{\\bm{\\mu}} \\def{\\bm{\\theta}} \\def{\\bm{a}} \\def{\\bm{b}} \\def{\\bm{c}} \\def{\\bm{d}} \\def{\\bm{e}} \\def{\\bm{f}} \\def{\\bm{g}} \\def{\\bm{h}} \\def{\\bm{i}} \\def{\\bm{j}} \\def{\\bm{k}} \\def{\\bm{l}} \\def{\\bm{m}} \\def{\\bm{n}} \\def{\\bm{o}} \\def{\\bm{p}} \\def{\\bm{q}} \\def{\\bm{r}} \\def{\\bm{s}} \\def{\\bm{t}} \\def{\\bm{u}} \\def{\\bm{v}} \\def{\\bm{w}} \\def{\\bm{x}} \\def{\\bm{y}} \\def{\\bm{z}} \\def{\\bm{\\varepsilon}} \\def{\\alpha} \\def{\\beta} \\def{\\epsilon} \\def{\\lambda} \\def{\\omega} \\def{\\mu} \\def{\\psi} \\def{\\sigma} \\def{\\theta} \\def{a} \\def{b} \\def{c} \\def{d} \\def{e} \\def{f} \\def{g} \\def{h} \\def{i} \\def{j} \\def{k} \\def{l} \\def{m} \\def{n} \\def{o} \\def{p} \\def{q} \\def{r} \\def{s} \\def{t} \\def{u} \\def{v} \\def{w} \\def{x} \\def{y} \\def{z} \\def{\\boldsymbol{\\alpha}} \\def{\\boldsymbol{\\beta}} \\def{\\boldsymbol{\\epsilon}} \\def{\\boldsymbol{\\lambda}} \\def{\\boldsymbol{\\gamma}} \\def{\\bm{A}} \\def{\\bm{B}} \\def{\\bm{C}} \\def{\\bm{D}} \\def{\\bm{E}} \\def{\\bm{F}} \\def{\\bm{G}} \\def{\\bm{H}} \\def{\\bm{I}} \\def{\\bm{J}} \\def{\\bm{K}} \\def{\\bm{L}} \\def{\\bm{M}} \\def{\\bm{N}} \\def{\\bm{O}} \\def{\\bm{P}} \\def{\\bm{Q}} \\def{\\bm{R}} \\def{\\bm{S}} \\def{\\bm{T}} \\def{\\bm{U}} \\def{\\bm{V}} \\def{\\bm{W}} \\def{\\bm{X}} \\def{\\bm{Y}} \\def{\\bm{Z}} \\def{\\bm{\\beta}} \\def{\\bm{\\Phi}} \\def{\\bm{\\Lambda}} \\def{\\bm{\\Sigma}} \\mathsfit{\\encodingdefault}{\\sfdefault}{m}{sl} \\mathsfit{bold}{\\encodingdefault}{\\sfdefault}{bx}{n} \\tens[1]{\\mathsfit{#1}} \\def{\\tens{A}} \\def{\\tens{B}} \\def{\\tens{C}} \\def{\\tens{D}} \\def{\\tens{E}} \\def{\\tens{F}} \\def{\\tens{G}} \\def{\\tens{H}} \\def{\\tens{I}} \\def{\\tens{J}} \\def{\\tens{K}} \\def{\\tens{L}} \\def{\\tens{M}} \\def{\\tens{N}} \\def{\\tens{O}} \\def{\\tens{P}} \\def{\\tens{Q}} \\def{\\tens{R}} \\def{\\tens{S}} \\def{\\tens{T}} \\def{\\tens{U}} \\def{\\tens{V}} \\def{\\tens{W}} \\def{\\tens{X}} \\def{\\tens{Y}} \\def{\\tens{Z}} \\def{\\mathcal{A}} \\def{\\mathcal{B}} \\def{\\mathcal{C}} \\def{\\mathcal{D}} \\def{\\mathcal{E}} \\def{\\mathcal{F}} \\def{\\mathcal{G}} \\def{\\mathcal{H}} \\def{\\mathcal{I}} \\def{\\mathcal{J}} \\def{\\mathcal{K}} \\def{\\mathcal{L}} \\def{\\mathcal{M}} \\def{\\mathcal{N}} \\def{\\mathcal{O}} \\def{\\mathcal{P}} \\def{\\mathcal{Q}} \\def{\\mathcal{R}} \\def{\\mathcal{S}} \\def{\\mathcal{T}} \\def{\\mathcal{U}} \\def{\\mathcal{V}} \\def{\\mathcal{W}} \\def{\\mathcal{X}} \\def{\\mathcal{Y}} \\def{\\mathcal{Z}} \\def{\\mathbb{A}} \\def{\\mathbb{B}} \\def{\\mathbb{C}} \\def{\\mathbb{D}} \\def{\\mathbb{F}} \\def{\\mathbb{G}} \\def{\\mathbb{H}} \\def{\\mathbb{I}} \\def{\\mathbb{J}} \\def{\\mathbb{K}} \\def{\\mathbb{L}} \\def{\\mathbb{M}} \\def{\\mathbb{N}} \\def{\\mathbb{O}} \\def{\\mathbb{P}} \\def{\\mathbb{Q}} \\def{\\mathbb{R}} \\def{\\mathbb{S}} \\def{\\mathbb{T}} \\def{\\mathbb{U}} \\def{\\mathbb{V}} \\def{\\mathbb{W}} \\def{\\mathbb{X}} \\def{\\mathbb{Y}} \\def{\\mathbb{Z}} \\def{\\Lambda} \\def{A} \\def{B} \\def{C} \\def{D} \\def{E} \\def{F} \\def{G} \\def{H} \\def{I} \\def{J} \\def{K} \\def{L} \\def{M} \\def{N} \\def{O} \\def{P} \\def{Q} \\def{R} \\def{S} \\def{T} \\def{U} \\def{V} \\def{W} \\def{X} \\def{Y} \\def{Z} \\def{\\Sigma} \\etens[1]{#1} \\def{\\etens{\\Lambda}} \\def{\\etens{A}} \\def{\\etens{B}} \\def{\\etens{C}} \\def{\\etens{D}} \\def{\\etens{E}} \\def{\\etens{F}} \\def{\\etens{G}} \\def{\\etens{H}} \\def{\\etens{I}} \\def{\\etens{J}} \\def{\\etens{K}} \\def{\\etens{L}} \\def{\\etens{M}} \\def{\\etens{N}} \\def{\\etens{O}} \\def{\\etens{P}} \\def{\\etens{Q}} \\def{\\etens{R}} \\def{\\etens{S}} \\def{\\etens{T}} \\def{\\etens{U}} \\def{\\etens{V}} \\def{\\etens{W}} \\def{\\etens{X}} \\def{\\etens{Y}} \\def{\\etens{Z}} \\pdata{p_{data}} \\ptrain{p_{data}} \\Ptrain{P_{data}} \\pmodel{p_{model}} \\Pmodel{P_{model}} \\ptildemodel{p_{model}} \\pencode{p_{encoder}} \\pdecode{p_{decoder}} \\precons{p_{reconstruct}} \\laplace{Laplace} % Laplace distribution \\E{E} \\Ls{L} \\R{R} \\emp{p} \\lr{\\alpha} \\reg{\\lambda} \\rect{rectifier} \\softmax{softmax} \\sigmoid{\\sigma} \\softplus{\\zeta} \\KL{D_{KL}} \\Var{Var} \\standarderror{SE} \\Cov{Cov} \\normlzero{L^0} \\normlone{L^1} \\normltwo{L^2} \\normlp{L^p} \\normmax{L^\\infty} \\parents{Pa} % See usage in notation.tex. Chosen to match Daphne's book. \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\sign{sign} \\Tr{Tr} \\let\\ab\\allowbreak \\tilde{\\widetilde} \\hat{\\widehat} \\frac{\\tfrac} \\crlref[1]{Corollary~#1} document Continual Learning of Large Language Models: A Comprehensive Survey Haizhou Shi Correspondence to: Haizhou Shi <haizhou.shi@rutgers.edu> and Hao Wang <hw488@cs.rutgers.edu>. haizhou.shi@rutgers.edu Zihao Xu Hengyi Wang Weiyi Qin Wenyuan Wang Work done as visiting students at Rutgers Machine Learning Lab. Yibin Wang \\authornotemark[2] % \\institution{Rutgers University USA } Zifeng Wang Sayna Ebrahimi % \\institution{Google Cloud AI Research Mountain View USA } Hao Wang \\authornotemark[1] hw488@cs.rutgers.edu % \\institution{Rutgers University USA } \\shortauthors{Shi et al.} \\authorsaddresses{} % suppressing the footnote. CCSXML <ccs2012> <concept> <concept_id>10010147.10010257.10010258.10010262.10010278</concept_id> <concept_desc>Computing methodologies~Lifelong machine learning</concept_desc> <concept_significance>500</concept_significance> </concept> <concept> <concept_id>10010147.10010178.10010179</concept_id> <concept_desc>Computing methodologies~Natural language processing</concept_desc> <concept_significance>500</concept_significance> </concept> <concept> <concept_id>10010147.10010257.10010293.10010294</concept_id> <concept_desc>Computing methodologies~Neural networks</concept_desc> <concept_significance>500</concept_significance> </concept> </ccs2012> CCSXML \\ccsdesc[500]{Computing methodologies~Lifelong machine learning} \\ccsdesc[500]{Computing methodologies~Natural language processing} \\ccsdesc[500]{Computing methodologies~Neural networks} Large Language Models, Continual Learning. \\maketitle",
      "origin_cites_number": 0
    },
    {
      "section_title": "Introduction",
      "level": "1",
      "content": "Recent advances in large language models~(LLMs) have demonstrated considerable potential for achieving artificial general intelligence (AGI)~radford2019language,brown2020language,achiam2022chatgpt,achiam2023gpt,chowdhery2023palm,anil2023palm,touvron2023llama,touvron2023llama2. Researchers have observed that complex abilities such as multi-step reasoning, few-shot in-context learning, and instruction following improve as the scale of parameter size increases~wei2022chain,wei2022emergent,yao2024tree,wei2021finetuned,min2022rethinking. The development of LLMs is impactful and revolutionary, prompting machine learning practitioners to reconsider traditional computational paradigms for once-challenging human-level tasks. However, LLMs are typically trained on static, pre-collected datasets encompassing general domains, leading to gradual performance degradation over time~loureiro2022timelms,jang2022towards,jin2022lifelong,jang2022temporalwiki,amba2021dynamic,dhingra2022time and across different content domains~gupta2023continual,jin2022lifelong,ke2022continual-train,sun2020ernie,cossu2022continual,gururangan2022demix,qin2023recyclable,chen2023lifelong,qin2022elle. Additionally, a single pre-trained large model cannot meet every user need and requires further fine-tuning~weyssow2023usage,winata2023overcoming,zheng2023learn,winata2023overcoming,biderman2023pythia,zheng2023learn,bai2023enhancing,ke2021achieve,wei2022circle,qin2021lfpt5,chen2024parameterizing. While one potential solution is re-collecting pre-training data and re-training models with additional specific needs, this approach is prohibitively expensive and impractical in real-world scenarios. To efficiently adapt LLMs to downstream tasks while minimizing performance degradation on previous knowledge domains, researchers employ the methodology of Continual Learning~(CL), also known as lifelong learning or incremental learning~pentina2016theoretical,chen2018lifelong,van2022three,wang2024comprehensive. Inspired by the incremental learning pattern observed in human brains~mcclelland1995there,kandel2000principles,pallier2003brain,mccaffary2021towards, CL trains machine learning models sequentially on a series of tasks with the expectation of maintaining performance across all tasks~kirkpatrick2017overcoming,li2017learning,ebrahimi2020adversarial,ebrahimi2019uncertainty. Throughout training, models have limited or no access to previous data, posing a challenge in retaining past knowledge as optimization constraints from unseen previous data are absent during current-task learning~li2017learning,lomonaco2020rehearsalfree,shi2024unified. This challenge, known as catastrophic forgetting~mccloskey1989catastrophic, has been a central focus in continual learning research since its inception. Over the years, researchers have explored various techniques to mitigate forgetting. These include replay-based methods~chaudhry2019tiny,schwarz2018progress,shi2024unified, parameter regularization~kirkpatrick2017overcoming,ritter2018online,aljundi2018memory, and model architecture expansion~ramesh2021model,wang2022coscl. Together, these techniques have significantly advanced the goal of achieving zero forgetting in continual learning across diverse tasks, model architectures, and learning paradigms. In the context of training and adapting LLMs sequentially, the significance of CL is undergoing semantic shifts of its own as well. To highlight this ongoing shift, in this paper, we provide a comprehensive overview and detailed discussion of the current research progress on continual LLMs. For the general picture of continual LLMs, we for the first time divide it into two directions of continuity that need to be addressed by practitioners~(details in sec:overview): itemize \\item Vertical continuity (or vertical continual learning), which refers to the ongoing adaptation of LLMs as they transition from large-scale general domains to smaller-scale specific domains, involving shifts in learning objectives and entities of execution. For example, healthcare institutions may develop LLMs tailored to the medical domain while retaining their general reasoning and question answering capabilities for users. \\item Horizontal continuity (or horizontal continual learning), which refers to continual adaptation across time and domains, often entails multiple training stages and increased vulnerability to forgetting. For example, social media platforms continuously update LLMs to reflect recent trends, ensuring accurate targeting of downstream services like advertising and recommendations without compromised experience for existing users. itemize Importantly, separating vertical and horizontal CL transcends mere modification of existing paradigms, like domain-incremental learning, which aligns with horizontal continuity. This distinction offers a robust framework for analyzing complex CL paradigms in language models. For instance, Recyclable Tuning preserves both vertical and horizontal continuity simultaneously~qin2023recyclable, and future designs might include zigzagging between horizontal and vertical CL. In fig:overview, following vertical continuity, we delineate three key stages of LLM learning within modern CL: Continual Pre-Training~(CPT), Domain-Adaptive Pre-training~(DAP), and Continual Fine-Tuning~(CFT)~(details in sec:stages). In CPT, existing research primarily investigates three types of distributional shifts: temporal, content-level, and language-level. Each presents distinct focuses and challenges. In DAP, CL evaluation and techniques are frequently utilized. However, there is a noticeable lack of diversity in these techniques, considering the maturity of the conventional CL community. In CFT, our focus is on the emerging field of learning LLMs, covering topics such as Continual Instruction Tuning (CIT), Continual Model Refinement (CMR), Continual Model Alignment (CMA), and Continual Multimodal LLMs (CMLLMs). Next, we present a compilation of publicly available evaluation protocols and benchmarks~(details in sec:eval-and-data). We conclude our survey with a discussion covering emergent properties of continual LLMs, changes in the roles of conventional CL types and memory constraints within the context of continual LLMs, and prospective research directions for this subject~(details in sec:discussion). In summary, this survey provides a comprehensive review of existing continual learning studies for LLMs, which significantly distinguishes itself from existing literature on related topics~biesialska2020continual,ke2023continual,wang2024comprehensive,wu2024continual,yang2024recent. Our survey highlights the underexplored research area of continually developing LLMs, especially in the field of CPT and DAP. We emphasize the needs for increased attention from the community, including the development of practical, accessible, and widely acknowledged evaluation benchmarks. Additionally, methodologies need to be tailored to address forgetting in emerging LLM learning paradigms. We hope this survey can provide a systematic and novel perspective of continual learning in the rapidly-changing field of LLMs and can help the continual learning community contribute to the challenging goals of developing LLMs in a more efficient, reliable, and sustainable manner~jang2022temporalwiki,su2023efficient,xie2023efficient,Cao2023InstructMol,attanasio2023worth.",
      "origin_cites_number": 16
    },
    {
      "section_title": "Background and Related Work",
      "level": "1",
      "content": "",
      "origin_cites_number": 0
    },
    {
      "section_title": "Large Language Models",
      "level": "2",
      "content": "Primarily built on the transformer architecture, pre-trained language models~(PLMs) have established a universal hidden embedding space through extensive pre-training on large-scale unlabeled text corpora~devlin2018bert, liu2019roberta, raffel2020exploring. By scaling parameters to billions or even hundreds of billions and training on massive text datasets~kaplan2020scaling, hoffmann2022training, PLMs not only demonstrate superior language understanding and generation capabilities but also manifest emergent abilities such as in-context learning, instruction following, and multi-step reasoning~wei2022chain,wei2022emergent,yao2024tree,wei2021finetuned,min2022rethinking. These larger models are commonly referred to as Large Language Models~(LLMs). For more detailed introduction, please refer to app:preliminary-llm.",
      "origin_cites_number": 3
    },
    {
      "section_title": "Pre-training of LLMs",
      "level": "3",
      "content": "There are two popular pre-training paradigms for LLMs. (1) Decoder-only models typically employ auto-regressive language modeling (LM) tasks during pre-training, including the GPT family~radford2019language,brown2020language,achiam2022chatgpt,achiam2023gpt, Gemini family~team2023gemini,reid2024gemini, and the open-source Llama family~touvron2023llama,touvron2023llama2. Specifically, given a sequence of tokens $\\vx = [x_1, x_2, \\cdots, x_N ]$, LM predicts the next token $x_t$ autoregressively based on all preceding tokens $\\vx_{<t} = [x_1, x_2, \\cdots, x_{t-1}]$, and trains the entire network by minimizing the negative log-likelihood $-\\sum^N_{t=1} \\log P( x_t | \\vx_{<t} )$, where $P(x_1|\\vx_{<1})\\triangleq P(x_1)$ is the unconditional probability estimation of the first token. (2) Encoder-only models, e.g., BERT~devlin2018bert,liu2019roberta, use masked language modeling (MLM) as a common pre-training objective. In MLM, for the input sequence $\\vx$, a subset of input tokens $m(\\vx)$ are masked and replaced with the special [MASK] token. The pre-training goal is to utilize the unmasked parts $\\vx_{\\backslash m(\\vx)}$ to predict the masked portions $m(\\vx)$. In summary, the overarching goal of MLM is to minimize the negative log-likelihood $-\\sum_{x \\in m(\\vx)}{\\rm log} \\, P( x|\\vx_{\\backslash m(\\vx)} )$.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Adaptation of LLMs",
      "level": "3",
      "content": "LLMs are primarily trained to generate linguistically coherent text. However, this training may not align with human values, preferences, or practical needs. Furthermore, the pre-training data can be outdated, leading to knowledge cutoffs or inaccuracies. To address these issues, various computational paradigms such as Instruction Tuning~(IT)~zhang2024instruction, Model Refinement~(MR)~de2021editing, and Model Alignment~(MA)~ouyang2022rlhf,rafailov2024dpo have been proposed. These approaches adapt LLMs to better meet diverse downstream tasks and user requirements. Numerous studies show that Instruction Tuning~(IT) can notably improve LLMs' ability to follow textual instructions~zhang2024instruction,wei2021finetuned,jiang2024instructiontuned,sanh2022multitask,ouyang2022rlhf, leveraging the pre-existing knowledge within LLMs to bridge the gap between general and task-specific performance~wei2022finetuned. Recent works like WizardLM~xu2023wizardlm and CodecLM~wang2024codeclm further tailor synthetic data to steer LLMs' behavior through IT. Additionally, IT enhances the interaction between humans and LLMs, providing a more natural interface and aligning LLM outputs more closely with human expectations and preferences~luo2023empirical. LLMs make mistakes, such as inaccurate translations or outdated information~de2021editing. Directly fine-tuning the model to correct these mistakes may disrupt its performance on previously learned tasks. To overcome these challenges, Model Refinement~(MR) is proposed to rectify the model's errors while preserving its performance on other inputs, with only moderate computing resources~sinitsin2020editable,de2021editing,fast_edit,hase2021language,huang2023transformer,mitchell2022memory,hartvigsen2023aging. Model Alignment~(MA) ensures AI systems' actions and outputs align with human values, ethics, and preferences~ouyang2022rlhf,rafailov2024dpo. MA can be broadly categorized into two types: Reinforcement Learning-based (RL-based) and Supervised Learning-based (SL-based). RL-based approaches~ouyang2022rlhf,schulman2017proximal are trained to make decisions reinforced by human feedback, using a reward system to guide them towards desirable outcomes. In contrast, SL-based approaches~hendrycks2023aligning, rafailov2024dpo,ji2024ai directly train models on datasets of human preferences, aligning their output with demonstrated human values.",
      "origin_cites_number": 13
    },
    {
      "section_title": "Continual Learning",
      "level": "2",
      "content": "Humans can accumulate knowledge and skills across tasks without significant performance decline on previous tasks mcclelland1995there,kandel2000principles,pallier2003brain,mccaffary2021towards. In contrast, machine learning models, which are typically data-centric, often experience performance degradation on old tasks when trained on new ones, a phenomenon known as ``catastrophic forgetting.'' The challenge of adapting models to a sequence of tasks without forgetting, especially when little to no past data can be preserved, is extensively studied in the continual learning community pentina2016theoretical,chen2018lifelong,van2022three,wang2024comprehensive. For formal definitions, a detailed introduction to the three CL scenarios and techniques, please refer to app:preliminary-cl.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Types of Continual Learning",
      "level": "3",
      "content": "To lay the groundwork for subsequent discussions (as illustrated in tab:cft and sec:discussion-xil), we follow the conceptual framework proposed by van2022three,kim2022theoretical,wang2024comprehensive. There are three primary types of continual learning scenarios: (i) Task-Incremental Learning~(TIL), where task indices are available to the model during inference~li2017learning,kirkpatrick2017overcoming; (ii) Domain-Incremental Learning~(DIL), where the model learns a sequence of tasks with the same formulation but without task indices during inference~shi2024unified; and (iii) Class-Incremental Learning~(CIL), where the model learns new classes of data during training~rebuffi2017icarl,kim2022theoretical.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Techniques of Continual Learning",
      "level": "3",
      "content": "Existing CL techniques can be roughly categorized into five groups~wang2024comprehensive: (i)~replay-based, (ii)~regularization-based, (iii)~architecture-based, (iv)~optimization-based, and (v)~representation-based. Here, we provide a concise yet comprehensive introduction to the first three categories of continual learning techniques, as they are extensively applied in continual LLMs. Replay-based methods adopt the relaxed memory constraint by keeping a small buffer of observed data and retraining the model on it when learning new tasks. Although replay-based methods may theoretically lead to loose generalization bounds~shi2024unified, they are valued for their simplicity, stability, and high performance, even with a small episodic memory~chaudhry2019tiny,riemer2018learning,buzzega2020dark,rebuffi2017icarl. Regularization-based methods adopt a regularization term $\\lambda \\left\\| \\vtheta - \\vtheta_{t-1}\\right\\|_\\mSigma$ that penalizes large deviation from the history model in the parameter space, where $\\|\\vv\\|_\\mSigma = \\vv^\\top \\mSigma \\vv$ is the vector norm evaluated on a positive-semi-definite matrix $\\mSigma$, and $\\lambda$ is the regularization coefficient, a hyper-parameter introduced to balance the past knowledge retention and current knowledge learning. The matrix $\\mSigma$ introduced is to measure the different level of importance of each parameters and their correlations in retaining the past knowledge. In practice, to reduce computational overhead, diagonal matrices are often designed to encode only the importance of each parameter~kirkpatrick2017overcoming,aljundi2018memory,rongali2021continual. Architecture-based methods, especially expanding the network architecture dynamically to assimilate new knowledge, is considered the most efficient form of CL~wang2022learning,wang2022dualprompt. This method primarily tackles adaptation challenges and can achieve zero-forgetting when task IDs are available during inference or can be correctly inferred~gururangan2022demix,wistuba2023. However, due to the difficulty of task ID inference, architecture expansion is predominantly utilized in TIL but is scarcely explored in DIL or CIL. In conjunction with pre-trained backbone large models like ViT~dosovitskiy2020image, CoLoR~wistuba2023 trains various low-rank adaptation (LoRA)~hu2021lora modules for different tasks. It estimates and stores prototypes for each task and utilizes the natural clustering ability of the pre-trained model during testing to infer task IDs, selecting the corresponding LoRA component for prediction generation. In the domain of continual LLMs, architecture expansion has resurged in popularity following the rise of parameter-efficient fine-tuning (PEFT)~shazeer2017outrageously,hu2021lora,dettmers2023qlora, a topic we will delve into shortly~yang2024moral,wang2023orthogonal,li2024examining,jang2022towards,jin2022lifelong,paul2024ircoder,yan2023af,wu2024llama.",
      "origin_cites_number": 11
    },
    {
      "section_title": "Evaluation Metrics of Continual Learning",
      "level": "3",
      "content": "There are four evaluation protocols primarily designed for continual learning. Overall Performance~(OP)~ke2021achieve,zhang2022continual,zhang2023copf calculates the average performance up until the current training stage, measuring the overall ability of a model balancing the performance of each task. As noted in shi2024unified, OP corresponds to the primary optimization objective of continual learning, and hence receives the most attention. Forgetting~(F) represents the largest performance drop observed of each task throughout the training process, averaged over all training stages. It quantifies the negative impact of learning new tasks brought to previously acquired knowledge. Ideally, a robust continual learning framework should achieve Backward Transfer~(BWT), where learning new tasks enhances performance on prior tasks. BWT is measured by negating the forgetting, and hence a negative forgetting indicates a an improvement in performance on earlier tasks. Forward Transfer (FWT) measures the generalization ability of the continual learning algorithms to unseen tasks. It is defined as the difference between the current model's performance evaluated on the future tasks and the randomly initialized model. Refer to app:eval for more details.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Continual Learning Meets Large Language Models: An Overview",
      "level": "1",
      "content": "Large language models (LLMs) are extensive in various dimensions, including the size of model parameters, pre-training datasets, computational resources, project teams, and development cycles~radford2019language,brown2020language,achiam2022chatgpt,achiam2023gpt,chowdhery2023palm,anil2023palm,touvron2023llama,touvron2023llama2. The substantial scale of LLMs presents notable challenges for development teams, particularly in keeping them updated amidst rapid environmental changes~amba2021dynamic,jin2022lifelong,dhingra2022time,jang2022towards,jang2022temporalwiki. To illustrate, in 2023, the average daily influx of new tweets exceeds 500 millionSource: \\href{https://www.omnicoreagency.com/twitter-statistics{https://www.omnicoreagency.com/twitter-statistics} }, and training on even a subset of this large volume of data is unaffordable. Recyclable Tuning~qin2023recyclable is the first work to explicitly outline the supplier-consumer structure in the modern LLM production pipeline. On the supplier side, the model is continually pre-trained over a sequence of large-scale unlabeled datasets. After every release of the pre-trained model, the consumer utilizes the stronger and more up-to-date upstream model for downstream tasks. Compared to the upstream supplier, downstream users often lack capacity of collecting and storing large-scale data, maintaining large-scale hardware systems, and training LLMs themselves. In this survey, we extend this framework and further present a comprehensive modern production pipeline encompassing various studies on continual LLM pre-training, adaptation, and deployment~(fig:overview). What sets our framework apart from existing studies~wu2024continual is the incorporation of two directions of continuity: \\textbf{Vertical Continuity and Horizontal Continuity}.",
      "origin_cites_number": 4
    },
    {
      "section_title": "Vertical Continuity (Vertical Continual Learning)",
      "level": "2",
      "content": "Definition.\\quad Vertical continuity (or vertical continual learning) has long been studied, either implicitly or explicitly, in existing literature. Vertical continuity is characterized by a hierarchical structure encompassing data inclusiveness, task scope, and computational resources. Specifically, the training task transitions gradually from general pre-training to downstream tasks, typically undertaken by distinct entities within the production pipeline~qin2023recyclable,gururangan2022demix,rongali2021continual,guo2023continuous,yan2023af,xie2023efficient. fig:overview shows a typical pipeline for vertical continuity in LLMs, i.e., ``pre-training'' $\\rightarrow$ ``domain-adaptive training'' $\\rightarrow$ ``downstream fine-tuning''~luo2023biomedgpt,li2023cfgpt,deng2023learning,han2021econet,zhou2020pre,guo2023continuous,gururangan2020dont,colombo2024saullm7b,wu2023pmc,wu2024llama,yan2023af,rongali2021continual,ma2023ecomgptct,huang2023lawyer: % pipeline illustrates this concept. itemize \\item Pre-training. During the pre-training stage, a substantial amount of data from diverse domains is required to develop a general-purpose LLM. This phase demands a sizable research and development team dedicated to training and benchmarking the model, along with considerable computational resources. \\item Domain-Adaptive Pre-training. Subsequently, downstream institutions may opt for domain-adaptive pre-training to tailor the model for specific tasks using domain-specific data unavailable to the upstream supplier. \\item Finetuning. Finally, the LLM undergoes fine-tuning on annotated data for downstream tasks before deployment. itemize 0.44{0.818}{\\cite{loureiro2022timelms,jang2022towards,jin2022lifelong,jang2022temporalwiki,amba2021dynamic,dhingra2022time,yildiz2024investigating}} % Temporal Shift 0.596{0.818}{\\cite{gupta2023continual,ke2022continual-train,jin2022lifelong,sun2020ernie,cossu2022continual,gururangan2022demix,qin2023recyclable,chen2023lifelong,qin2022elle,ibrahim2024simple}} % Content Shift 0.73{0.818}{\\cite{gogoulou2024continual,li2024examining,ibrahim2024simple}} % Language 0.55{0.7893}{\\cite{Lu2023BBTFin,Nguyen2023AstroLLaMA,xie2023efficient,li2023starcoder,guo2024deepseekcoder,Xue2023WeaverBird,paul2024ircoder,cheng2024adapting,dou2024sailor,fujii2024continual,takahashi2024pretraining}} % DAP 0.56{0.7694}{\\cite{li2023cfgpt,deng2023learning,han2021econet,zhou2020pre,guo2023continuous,gururangan2020dont,yan2023af,rongali2021continual,ma2023ecomgptct}} % DAP -> SFT 0.76{0.7694}{\\cite{luo2023biomedgpt}} % DAP -> MM-SFT 0.852{0.7694}{\\cite{wu2024llama}} % DAP -> U-SFT 0.535{0.7486}{\\cite{Bi2023OCEANGPT,wu2023pmc,Yang2023PLLaMa,li2024blade,xie2024me,nakamura2024aurora}} % DAP -> IT 0.68{0.7486}{\\cite{Zheng2023MarineGPT}} % DAP -> MM-IT 0.77{0.7486}{\\cite{colombo2024saullm7b}} % DAP -> U-IT 0.535{0.7284}{\\cite{song2024code,lin2023geogalactica,rozière2024code,Chen2023HuatuoGPTII,Zhang2023xuanyuan,huang2023lawyer,thulke2024climategpt,acikgoz2024hippocrates}} % Others 0.51{0.7077}{\\cite{nijkamp2022codegen}} % DAP -> DAP 0.63{0.7077}{\\cite{ke2022continual-pre}} % (DAP)_n 0.476{0.661}{\\cite{scialom2022fine,wang2023trace,mok2023large,zhang2023citb,huang2024mitigating}} % CIT 0.476{0.668}{\\cite{he2024dont,yin2022contintin,wang2023orthogonal,zhao2024sapt}} % CIT 0.575{0.661}{\\cite{lin2022continual,hartvigsen2023aging,hu2024wilke,wang2024wise}} % CMR 0.575{0.668}{\\cite{das2024larimar,yu2023melo,li2023continual}} % CMR 0.647{0.661}{\\cite{zhang2023copf,zhangcppo}} % CMA 0.647{0.668}{\\cite{lin2024mitigating}} % CMA 0.713{0.661}{\\cite{he2023continual,zheng2024antiforgetting,chen2024coin}} % CMLLMs 0.713{0.668}{\\cite{zhu2024model,zhao2024reconstruct}} % CMLLMs Throughout the process, the unlabeled domain-specific dataset is smaller in scale than the upstream pre-training phase but larger than the final downstream task fine-tuning phase. This pattern extends to computational resources, team size, and other factors. It is important to note that vertical continuity can involve more than three stages~nijkamp2022codegen,lin2023geogalactica,rozière2024code,huang2023lawyer. In real-world applications, during domain-adaptive pre-training, additional ``layers'' can be added to accommodate multiple entities, such as various departments with distinct objectives but operating within the same domain. figure*[t] center \\includegraphics[width=1\\textwidth]{figures/overview.v6.pdf} center A high-level overview of the modern pipeline for continually pre-training and fine-tuning LLMs, where two dimensions of continuity are described. \\textbf{Vertical Continuity~(or Vertical Continual Learning): LLM training can be vertically divided into three stages: (i)~Continual Pre-Training~(CPT), (ii)~Domain-Adaptive Pre-training~(DAP), and (iii)~Continual Fine-Tuning~(CFT). The main focus is the retention of the LLM's general knowledge~(prevention of vertical forgetting). Horizontal Continuity~(or Horizontal Continual Learning): After the LLMs are deployed, the models are continually updated when a new set of data becomes available. The primary goal is to prevent horizontal forgetting in a long sequence of tasks. } -1em figure* Vertical Forgetting.\\quad We term the performance degradation (in terms of general knowledge) due to vertical continual learning ``vertical forgetting''. As shown in fig:continuity, for vertical continual learning, the data distribution of upstream tasks partially covers the downstream, meaning the model might start off at a decent initialization for the subsequent stage of training. Two significant challenges must be addressed to prevent vertical forgetting: itemize \\item Task Heterogeneity. Stemming from the inherent disparity between the formulation of upstream tasks and downstream tasks, task heterogeneity can lead to differences in model structures and training schemes, which has long been recognized as a major hurdle~rebuffi2017icarl,li2017learning,wu2019large,ni2021revisiting,kim2022theoretical. To mitigate this issue, practitioners often employ methodologies such as freezing shared parameters during downstream phases or reformulating downstream tasks to match the structure of pre-training tasks~yang2024moral,wang2023orthogonal,li2024examining,paul2024ircoder,yan2023af,wu2024llama. \\item Inaccessible Upstream Data. This challenge arises primarily from varying levels of confidentiality across entities undertaking vertical continual learning. Data collected and curated under different protocols may not be accessible to some downstream entities. This scenario is even more challenging than the strict memory constraint presented in conventional CL~(def:memory), as algorithms for latter case rely on access to previous data at specific points for parameter importance measurement~kirkpatrick2017overcoming,aljundi2018memory or for replay~riemer2018learning,chaudhry2019tiny,buzzega2020dark,shi2024unified. To address the challenge of inaccessible upstream data, existing methods either use public datasets or generate pseudo-examples to create proxy pre-training datasets~qin2021lfpt5. itemize",
      "origin_cites_number": 29
    },
    {
      "section_title": "Horizontal Continuity (Horizontal Continual Learning)",
      "level": "2",
      "content": "Definition.\\quad Horizontal continuity (or horizontal continual learning) refers to continual adaptation across time and domains, a topic extensively explored within the continual learning community. The primary rationale for preserving horizontal continuity lies in the dynamic nature of data distribution over time. To stay updated with these content shifts, an LLM must incrementally learn newly-emerged data. Otherwise, the cost of re-training will become prohibitively expensive and impractical~chaudhry2019efficient,amba2021dynamic,su2023efficient,xie2023efficient. Empirical evidence has consistently shown that despite their impressive capabilities, LLMs struggle to generalize effectively to future unseen data, particularly in the face of temporal or domain shifts~amba2021dynamic,jang2022towards,jang2022temporalwiki,dhingra2022time. Additionally, they struggle to retain complete knowledge of past experiences when adapting to new temporal domains, although they do demonstrate a higher level of robustness against catastrophic forgetting~tao2022can,luo2023investigating,zheng2023learn,mehta2023empirical. The necessity of employing complex CL algorithms to address challenges in LLMs remains an open question. For instance, during large-scale continual pre-training, large institutions can typically afford the storage costs of retaining all historical data, rendering memory constraints meaningless. %not suitable. Several studies have demonstrated that with full access to historical data, simple sparse replay techniques can effectively mitigate forgetting~tao2022can,scialom2022fine,prabhu2023online,garg2024tic. In contrast, numerous continual learning studies have showcased superior performance compared to naive solutions, suggesting the importance of continual learning techniques in LLM training~jang2022temporalwiki,jin2022lifelong,qin2022elle,chen2023lifelong. figure*[t] center \\includegraphics[width=0.9\\textwidth]{figures/continuity.v0.pdf} center A diagram showing two different directions of continual learning of LLMs. \\textbf{(a) Vertical Continual Learning of LLMs: in this case, the upstream data distribution usually partially covers the subsequent tasks' data distribution. (b) Horizontal Continual Learning of LLMs: No constraints on the data distributions are present on horizontal continual learning. The continual LLMs need to handle the challenge of abrupt distributional shifts and a longer sequence of training. } -1em figure* Horizontal Forgetting.\\quad We informally define ``horizontal forgetting'' as the performance degradation on the previous tasks when model is undergoing horizontal continual learning. As illustrated in fig:continuity, horizontal continual learning typically involves training stages of similar scales, with potential distributional overlap among their data. In summary, two main challenges need to be addressed for horizontal continual learning of LLMs: itemize \\item Long Task Sequences. Horizontal continual learning ideally involves numerous incremental phases, particularly to accommodate temporal shifts in data distribution. A longer task sequence entails more update steps of the model, leading to inevitable forgetting of previously learned tasks. To address this challenge, researchers employ established continual learning techniques with stronger constraints, such as continual model ensemble~ramesh2021model. \\item Abrupt Distributional Shift. In contrast to vertical continuity, where distributional shifts are often predictable, horizontal continual learning does not impose constraints on task properties. Evidence suggests that abrupt changes in task distributions can result in significant horizontal forgetting of the model~sarfraz2023error. itemize",
      "origin_cites_number": 7
    },
    {
      "section_title": "Learning Stages of Continual Large Language Models",
      "level": "1",
      "content": "fig:overview provides an overview of continually learning LLMs. Along the axis of vertical continuity, three main ``layers'' of modern continual learning emerge. The top layer, Continual Pre-Training~(CPT), involves continuous pre-training of LLMs by the supplier on newly-collected data alongside existing data (sec:cpt). The middle layer, Domain-Adaptive Pre-training~(DAP), prepares LLMs for domain-specific applications through additional pre-training on domain-specific unlabeled data (sec:dap). The bottom layer, Continual Fine-Tuning~(CFT), targets models for final downstream tasks on the consumer side (sec:cft), where the model needs to be updated after deployment for the specified task.",
      "origin_cites_number": 0
    },
    {
      "section_title": "Continual Pre-Training~(CPT)",
      "level": "2",
      "content": "",
      "origin_cites_number": 0
    },
    {
      "section_title": "CPT: Effectiveness and Efficiency",
      "level": "3",
      "content": "Before delving into the details of continual pre-training (CPT), it is important to address two fundamental questions: Firstly, regarding effectiveness, can CPT enhance performance on downstream tasks beyond that of the initial training on a wide range of data domains? Extensive studies have not only demonstrated the necessity of CPT for improved downstream performance~qin2022elle,gururangan2022demix,jang2022towards,jang2022temporalwiki,jin2022lifelong,chen2023lifelong, but also shown that when distributional shifts are gradual~jang2022temporalwiki,yildiz2024investigating or somewhat correlated~gururangan2022demix, CPT can effectively help model generalize to unseen data. The second question is about efficiency: given the large size of an LLM' parameters and data, both old and new, can we achieve adaptation and knowledge retention in a computationally efficient way? Concerning efficiency, most studies focus on techniques for efficient knowledge retention~jin2022lifelong,jang2022towards,jang2022temporalwiki,li2024examining, which significantly overlap with the CL literature addressing catastrophic forgetting~schwarz2018progress,riemer2018learning,buzzega2020dark,shi2024unified,rebuffi2017icarl,ritter2018online,aljundi2018memory,rusu2016progressive,ramesh2021model,wang2022coscl. In contrast to prior approaches that fully utilize emergent data, some studies recognize the impracticality of this approach in real production environments. Instead, they concentrate on further improving the efficiency of adaptation. For instance, ELLE~qin2022elle employs a function-preserved model expansion to facilitate efficient knowledge growth; amba2021dynamic and xie2023efficient sub-sample training data based on novelty and diversity to enhance training efficiency, achieving superior performance compared to full-data training. Though currently underexplored, efficient adaptation in continual pre-training is poised to become significant, given recent findings emphasizing data quality over quantity for LLM generalization~xie2024data,soldaini2024dolma. table*[t] \\textbf{Summary of existing studies on Continual Pre-training of LLMs. The papers are organized based on their relation to CL: (i)~no CL techniques are studied, (ii)~CL techniques are studied as solely baselines, and (iii)~new CL approaches are proposed. In the table, Dist.~Shift denotes what type(s) of distributional shifts this particular study considers and is dedicated to solve. In the section of Continual Learning Tech., we mainly categorize three types of continual learning techniques that are studied in the paper: rehearsal~(Rehearsal), parameter regularization~(Param.~Reg.), and architecture expansion~(Arch. Exp.). We use ``\\cmark'', ``\\xmark'', and ``\\club'' to denote ``deployed in the proposed method'', ``not studied in the paper'', and ``studied as a baseline method'', respectively. Note that we do not include naive sequential fine-tuning in this table, as it is universally studied as the important baseline method in all of the papers in the table. The papers with only ``\\club''~jin2022lifelong,jang2022temporalwiki,jang2022towards means that only existing CL techniques are studied, without proposing new ones, and the papers with only ``\\xmark''~gupta2023continual,gogoulou2024continual means that special aspects of fine-tuning are studied, without using CL techniques. } -0.5em center 1\\linewidth{!}{% \\tabcolsep{9pt} tabular{cCCCC CCcc} \\toprule[0.12em] 2{*}{Method} & 2{c}{Scenario} & 3{c}{{Continual Learning Tech.}} & 2{*}{{LLM Arch.}} & 2{c}{{Evaluation}} \\\\ \\cmidrule(lr){2-3}\\cmidrule(lr){4-6}\\cmidrule(lr){8-9} & Dist. Shift & \\#Domains & Rehearsal & Param. Reg. & Arch. Exp. & & Pre-Training & Downstream \\\\ \\cmidrule[0.12em]{1-9} TimeLMs~loureiro2022timelms & Temporal & 8 & \\xmark & \\xmark & \\xmark & RoBERTa & \\cmark & \\cmark \\\\ \\hline yildiz2024investigating & Content & 159 & \\xmark & \\xmark & \\xmark & RoBERTa GPT-2 & \\cmark & \\xmark \\\\ \\hline gupta2023continual & Content & 1 & \\xmark & \\xmark & \\xmark & Pythia & \\cmark & \\xmark \\\\ \\hline gogoulou2024continual & Language & 3 & \\xmark & \\xmark & \\xmark & GPT & \\cmark & \\xmark \\\\ \\hline RHO-1~lin2024rho & Other & 1 & \\xmark & \\xmark & \\xmark & TinyLlama Mistral & \\cmark & \\cmark \\\\ \\midrule \\midrule li2024examining & Language & 1 & gray2\\xmark & gray2P-Freeze$^\\club$ & gray2Adapter$^\\club$ LoRA$^\\club$ & Llama2 & \\cmark & \\cmark \\\\ \\hline CKL~jang2022towards & Temporal & 1 & gray2 Mix-Review$^\\club$ & gray2 P-Freeze$^\\club$ RecAdam$^\\club$ & gray2 LoRA$^\\club$ K-Adapter$^\\club$ & T5 & \\xmark & \\cmark \\\\ \\hline LLPT~jin2022lifelong & Temporal \\\\ \\rule[0.5ex]{1\\linewidth{0.4pt} \\\\ Content} & 4 \\\\ \\rule[0.5ex]{1\\linewidth{0.4pt} \\\\ 8} & gray2 ER$^\\club$ Logit-KD$^\\club$ Rep-KD$^\\club$ Contrast-KD$^\\club$ SEED-KD$^\\club$ & gray2 oEWC$^\\club$ & gray2 Adapter$^\\club$ Layer~Exp.$^\\club$ & RoBERTa & \\cmark & \\cmark \\\\ \\hline TemporalWiki~jang2022temporalwiki & Temporal & 5 & gray2 Mix-Review$^\\club$ & gray2 P-Freeze$^\\club$ RecAdam$^\\club$ & gray2 LoRA$^\\club$ K-Adapter$^\\club$ & GPT-2 & \\cmark & \\cmark\\\\ \\midrule \\midrule CPT$^*$~ke2022continual-train & Content & 4 & gray3 DER++$^\\club$\\qquad KD$^\\club$ & gray3CPT$^\\cmark$\\qquad EWC$^\\club$\\qquad HAT$^\\club$ & gray3Adapter$^\\club$ DEMix$^\\club$ & RoBERTa & \\cmark & \\xmark \\\\ \\hline ERNIE~2.0~sun2020ernie & Content & 4 & gray3 ER$^\\cmark\\club$ & gray3 \\xmark & gray3 \\xmark & ERNIE & \\xmark & \\cmark \\\\ \\hline amba2021dynamic & Temporal & 7 & gray3 \\xmark & gray3 P-Freeze$^\\cmark$ & gray3 Vocab. Exp.$^\\cmark$ & BERT & \\xmark & \\cmark \\\\ \\hline cossu2022continual & Content & 5 & gray3 \\xmark & gray3 \\xmark & gray3 Vocab. Exp.$^\\cmark$ & BERT\\qquad RoBERTa & \\xmark & \\cmark \\\\ \\hline DEMix~gururangan2022demix & Content & 8 & gray3 \\xmark & gray3 \\xmark & gray3 MoE$^\\cmark$ & GPT-3 & \\cmark & \\cmark \\\\ \\hline TempoT5~dhingra2022time & Temporal & 1 & gray3 \\xmark & gray3 \\xmark & gray3 Vocab. Exp.$^\\cmark$ Prompt$^\\cmark$ & T5 & \\xmark & \\cmark \\\\ \\hline RecTuning~qin2023recyclable & Content & 4 & gray3 ER$^\\cmark$\\qquad \\qquad KD$^\\cmark$ & gray3 \\xmark & gray3 Adapter$^\\cmark$ & RoBERTa & \\xmark & \\cmark\\\\ \\hline Lifelong-MoE~chen2023lifelong & Content & 3 & gray3 ER$^\\club$\\qquad \\qquad KD$^\\cmark$ & gray3 P-Freeze$^\\cmark$\\qquad L2$^\\club$ & gray3 MoE$^\\cmark$ & GLaM & \\cmark & \\cmark\\\\ \\hline ELLE~qin2022elle & Content & 5 & gray3 ER$^\\cmark\\club$\\qquad KD$^\\club$ & gray3 P-Freeze$^\\cmark$ & gray3 Prompt$^\\cmark$\\qquad Layer~Exp.$^\\cmark$ Adapter$^\\club$ & BERT\\qquad \\qquad GPT & \\cmark & \\cmark \\\\ \\hline ibrahim2024simple & Content Language & 2 & gray3 ER$^\\cmark$ & gray3 \\xmark & gray3 \\xmark & GPT-NeoX & \\cmark & \\cmark\\\\ \\hline CEM~zhao2024large & Other & 1 & gray3 ER$^\\cmark$ & gray3 \\xmark & gray3 \\xmark & CuteGPT ChatGLM Qwen-Chat & \\xmark & \\cmark \\\\ \\hline IR-DRO~chen2024take & Other & 1 & gray3 ER$^\\cmark$ & gray3 \\xmark & gray3 \\xmark & OPT & \\xmark & \\cmark \\\\ \\bottomrule[0.12em] tabular } center -1.5em table*",
      "origin_cites_number": 32
    },
    {
      "section_title": "General Observations on CPT",
      "level": "3",
      "content": "tab:cpt-small summarizes the existing studies on continual pre-training~(CPT), and here are some key observations we make about CPT. itemize \\item OBS-1: The development of advanced techniques tailored specifically for CPT is at the starting stage and warrants further exploration. Only about half of the examined papers propose novel techniques for CPT~ke2022continual-train,sun2020ernie,amba2021dynamic,cossu2022continual,gururangan2022demix,dhingra2022time,qin2023recyclable,chen2023lifelong,qin2022elle, while the remaining half either focus solely on the effects of pure adaptation without considering CL techniques~loureiro2022timelms,gupta2023continual,gogoulou2024continual, or conduct empirical studies on the straightforward application of existing CL techniques~li2024examining,jin2022lifelong,jang2022towards,jang2022temporalwiki. \\item OBS-2: The diversity of CL techniques incorporated in CPT remains limited. Most practical implementations of CL techniques for CPT primarily focus on architecture expansion of LLMs~amba2021dynamic,cossu2022continual,gururangan2022demix,dhingra2022time,qin2023recyclable,chen2023lifelong, with only a few explicitly utilizing replay~qin2023recyclable,chen2023lifelong and parameter regularization~amba2021dynamic,chen2023lifelong. \\item OBS-3: There is an apparent gap between the existing studies and the real production environment of CPT. Except for the recent study~yildiz2024investigating which conducts CPT over 159 domains, the longest sequence of pre-training stages explored is 8~jin2022lifelong,gururangan2022demix. However, this falls short of real-world scenarios where continual pre-training occurs more frequently and persists for months or years. The efficacy of CPT methods in such prolonged scenarios remains uncertain. Additionally, investigating CPT in a task-boundary-free data stream setting is an important avenue for research to be explored in the future as well. itemize",
      "origin_cites_number": 8
    },
    {
      "section_title": "Distributional Shifts in CPT",
      "level": "3",
      "content": "This survey categorizes distributional shifts of CPT into three main types: (i)~Language Shift: LLMs sequentially learn different language corpora, e.g., English $\\rightarrow$ Chinese~gogoulou2024continual,li2024examining. (ii)~Content Shift: LLMs sequentially learn corpora from different fields, e.g., chemistry $\\rightarrow$ biology~gururangan2022demix,cossu2022continual,jin2022lifelong,qin2023recyclable,chen2023lifelong,gupta2023continual. (iii)~Temporal Shift: Distributional shifts occur over time, e.g., news in 2021 $\\rightarrow$ news in 2022, with a major focus on timestamp-sensitive knowledge retention and update~amba2021dynamic,jin2022lifelong,dhingra2022time,jang2022towards,jang2022temporalwiki. Language Shift.\\quad gogoulou2024continual focuses on assessing LLMs' natural ability to learn new languages sequentially. With no explicit CL techniques employed, the study observes consistent positive forward transfer of the knowledge, facilitating new language acquisition regardless of the learning order. Forgetting, on the other hand, emerges as a significant challenge that cannot be mitigated by the increasing size of LLMs. In li2024examining, the degree of forgetting of previously learned language when adapting LLMs to a new language is investigated. Various CL techniques, including parameter freezing, LoRA~hu2021lora, and (IA)$^3$~liu2022few, are evaluated across multiple dimensions. Preliminary experimental results highlight the non-trivial nature of addressing horizontal forgetting for CPT under the language shift as well. Content Shift.\\quad yildiz2024investigating explores the large-scale CPT over 159 content domains, and shows that CPT on various domains can effectively improve models' adaptation ability compared to DAP on single domain. Similarly, gupta2023continual continues the pre-training phase of Pythia~biderman2023pythia with no complex CL techniques and discovers that learning rate re-warming consistently improves models trained from scratch. Built upon this simple observation, ibrahim2024simple further shows that proper combination of learning rate re-warming and re-decay, and replay of the previous data is sufficient to achieve a comparable performance to full re-training. LLPT~jin2022lifelong establishes a comprehensive training and evaluation protocol for a series of content-level distributional shifts. They assess multiple CL methods and, similar to gogoulou2024continual, find consistent forward knowledge transfer, yet horizontal forgetting remains significant. Besides, contrary to the common understanding that experience replay~chaudhry2019tiny is the most efficient approach to preventing forgetting, the authors find it ineffective in the case of CPT, due to the potential overfitting issue. Recyclable Tuning~qin2023recyclable shows that if the upstream supplier continually pre-trains LLMs, with or without replay, consumer-side efficiency can be boosted by recycling previously learned update components when proper CL techniques are applied. DEMix~gururangan2022demix incrementally trains and integrates new experts (DEMix layer) for new domains during CPT. To ensure reasonable inference performance during testing when no domain information is available, it proposes a parameter-free probabilistic approach to dynamically estimate a weighted mixture of domains. DEMix's modularization has been shown to facilitate efficient domain-adaptive pre-training, promote relevant knowledge during inference, and allow for removable components. Lifelong-MoE~chen2023lifelong, similar to DEMix~gururangan2022demix, incrementally trains domain experts for new domains. However, Lifelong-MoE differs from DEMix in utilizing a token-level gating function to activate multiple experts for intermediate embedding calculation. During training, previous experts' parameters and gating functions remain frozen, and knowledge distillation loss is employed to regulate parameter updates, which thereby makes Lifelong-MoE robust against the issue of horizontal forgetting. It is noteworthy that some papers draw almost opposite conclusions regarding the significance of CPT for content shifts. For instance, cossu2022continual continually pre-trains BERT-based models~devlin2018bert,liu2019roberta on five scientific domains and evaluates performance on downstream sentiment analysis. They observe that even the trivial sequential pre-training does not exhibit severe forgetting, prompting reasonable questions about the necessity of CPT. Temporal Shift.\\quad In the context of CPT amid content shifts, Multi-Task Learning~(MTL) is often regarded as the upper bound achievable~pentina2016theoretical, wang2024comprehensive, shi2024unified. However, this belief does not fully hold when considering CL under temporal shifts~jang2022towards,jang2022temporalwiki,dhingra2022time, as temporal shifts can introduce conflicting information, posing challenges for LLMs. For instance, the statement ``Lionel Messi plays for team Barcelona'' remains accurate from 2004 to 2021 but becomes false by 2024, as ``Lionel Messi plays for team Inter Miami'' becomes the correct statement. Hence, as advocated by CKL~jang2022towards and TemporalWiki~jang2022temporalwiki, LLMs undergoing continual adaptation to temporal shifts must simultaneously achieve three objectives: (i)~retention of old knowledge, (ii)~acquisition of new knowledge, and (iii)~update of the outdated knowledge. They evaluate the same set of continual learning baseline methods~chen2020recall,he2021analyzing,hu2022lora,wang2021kadapter, each highlighting distinct aspects of their impact. CKL~jang2022towards observes that parameter expansion consistently exhibits robust performance across all experimental conditions. In contrast, replay-based methods struggle to efficiently adapt to new knowledge acquisition and outdated knowledge update, leading to rapid forgetting of newly learned information during training. TemporalWiki~jang2022temporalwiki constructs a series of temporal corpora and their differential sets from sequential snapshots of Wikipedia, revealing that updating LLMs on these differential sets substantially enhances new knowledge acquisition and updates, requiring significantly less computational resources, and various CL techniques prove effective in mitigating horizontal forgetting during this process. LLPT~jin2022lifelong introduces temporal generalization evaluation for LLMs pre-trained on sequential corpora. Through experiments on a large-scale chronologically-ordered Tweet Stream, the authors demonstrate the superiority of CPT combined with CL techniques to task-specific LMs, in terms of both knowledge acquisition and temporal generalization. Nonetheless, these preliminary experiments do not conclusively determine which specific CL method is more preferable than the others. Another line of work, Temporal Language Models (TLMs), takes a different approach to address knowledge retention, acquisition, and update under temporal shifts by integrating temporal information into the model~rosin2022time,dhingra2022time,su2023efficient. During training, they inject temporal information into training examples as prefixes of prompts, using special tokens~rosin2022time, explicit year information~dhingra2022time, or syntax-guided structural information~su2023efficient. In sequential training experiments conducted by TempoT5~dhingra2022time, comparison between continually and jointly pre-trained LMs demonstrates that CPT better balances adaptation and forgetting when the replay rate of past data is appropriately set. Others.\\quad CPT as a technique to progressively attain novel knowledge, can be used to refine LLMs' behavior. CEM~zhao2024large collects examples where the model's response is incorrect and continually trains the model on these examples, along with a supplemental dataset. RHO-1~lin2024rho proposes Selective Language Modeling (SLM), which employs a reference model to evaluate the perplexity of each token in the training corpus, and continually pre-trains the model on high-perplexity tokens. Similarly, IR-DRO~chen2024take re-trains the model on re-weighted examples from the original pre-training dataset, focusing more on higher-loss sequences. The significance of addressing temporal shifts through CPT is underscored by several industrial studies. For instance, amba2021dynamic employs a dynamic vocabulary expansion algorithm and an efficient sub-sampling procedure to conduct CPT on large-scale emerging tweet data. Conversely, loureiro2022timelms adopts CPT without explicit measures to constrain model updates, releasing a series of BERT-based LMs incrementally trained on new tweet data every three months. Preliminary experimental results demonstrate substantial improvements of continually pre-trained LMs over the base BERT model across downstream tasks. While some studies question the necessity of continually adapting LLMs along the temporal axis for environmental reasons, such as reducing CO$_2$ emissions~attanasio2023worth, the community commonly embraces CPT as a more efficient learning paradigm compared to the traditional ``combine-and-retrain'' approach.",
      "origin_cites_number": 39
    },
    {
      "section_title": "Domain-Adaptive Pre-training~(DAP)",
      "level": "2",
      "content": "Background of DAP.\\quad Institutions, regardless of size, often possess significant amounts of unlabeled, domain-specific data. This data bridges the gap between general-purpose LLMs trained on diverse corpora and fine-tuned LLMs designed for specific downstream tasks. Leveraging this data as a preparatory stage can facilitate effective adaptation of LLMs to downstream tasks. Such process of ``continued/continual/continuous pre-training''~yan2023af,guo2023continuous,ma2023ecomgptct,han2021econet,xie2023efficient,xie2023quert,huang2023lawyer,Lu2023BBTFin,Xie2023PIXIU,Azerbayev2023LLEMMA,yue2023mammoth,colombo2024saullm7b,Zhang2024SciGLM,shen2024tag, ``further pre-training''~song2024code,lin2023geogalactica,deng2023learning,Rubungo2023LLM-Prop,agarwal2024structured, ``domain tuning''~rongali2021continual, ``knowledge enhancement pre-training''~Lu2023BBTFin, and ``knowledge injection training''~wu2023pmc is unified and termed ``\\emph{Domain Adaptive Pre-training~(DAP)}''~gururangan2020dont for clarity and consistency throughout this survey. In the pioneering work of domain-adaptive pre-training~(DAPT)~gururangan2020dont, the authors continuously pre-train the language models on a larger domain-specific dataset before fine-tuning them to the downstream tasks, resulting in universally improved performance aross various tasks. As the observation above has been validated on multiple domains in parallel, including BioMed, CS, News, and Reviews~gururangan2020dont, practitioners commonly accept that employing DAP on additional unlabeled domain-specific data benefits downstream tasks. Consequently, this technique has become widely deployed in many modern LLMs. Summary of LLMs with DAP.\\quad We provide a summary of the existing 41 studies utilizing DAP for LLMs in tab:dap. Each entry is characterized by three main features: (i)~training process specifications, encompassing the vertical domain for which LLMs are trained, the training pipeline preceding release, and the LLM architecture employed; (ii)~adopted continual learning techniques, including rehearsal, parameter regularization, and architecture expansion; and (iii) evaluation metrics for CL, such as backward transfer (forgetting) and forward transfer (adaptation to downstream data).",
      "origin_cites_number": 8
    },
    {
      "section_title": "General Observation on DAP",
      "level": "3",
      "content": "Several key observations emerge regarding the research landscape of DAP~(tab:dap). itemize \\item OBS-1: DAP predominantly occurs in a single stage. Continual DAP which involves more than one stage is seldom explored: among all papers listed in tab:dap, only one employs two stages of DAP~(``PT $\\rightarrow$ DAP $\\rightarrow$ DAP $\\rightarrow$ FT'' in Code Llama rozière2024code). It is arguably reasonable to categorize studies that conduct only one stage of DAP and nothing more~Lu2023BBTFin,Nguyen2023AstroLLaMA,song2024code,xie2023efficient,li2023starcoder,guo2024deepseekcoder,Xue2023WeaverBird,paul2024ircoder,Azerbayev2023LLEMMA,cheng2024adapting into CPT rather than DAP. Nevertheless, considering that they aim to adapt a general-purpose LLM to a specific domain, we include them in this section. \\item OBS-2: The notion of interpreting DAP through the lens of CL, whether intentional or not, is widely embraced. As shown in~tab:dap, except for the first section~(white, 13/41), where papers overlook any potential side effects of DAP leading to vertical forgetting, the remaining sections~(all gray, 28/41) either evaluate the potential negative impacts of DAP or proactively employ CL techniques to mitigate the risk of vertical forgetting. \\item OBS-3: Further research of more sophisticated CL techniques for not just DAP, but general vertical continual learning is much needed. It is supported by the widespread adoption of CL techniques (22/41) for training domain-specific LLMs. However, the diversity of these techniques is limited, with only replay~colombo2024saullm7b,wu2023pmc,Azerbayev2023LLEMMA,rongali2021continual,Chen2023HuatuoGPTII,Zhang2023xuanyuan,Yang2023PLLaMa,ma2023ecomgptct,huang2023lawyer,cheng2024adapting and parameter expansion (LoRA~Xue2023WeaverBird,paul2024ircoder,wu2024llama,yan2023af) or Layer/Block expansion~wu2024llama,yan2023af utilized. In fact, it appears that individuals may not explicitly recognize that DAP should be viewed from the perspective of vertical continuity, as they often employ CL techniques unknowingly, e.g., studies deploying replay terming the technique as ``data combination''~wu2023pmc or ``data mixing/mixture''~Azerbayev2023LLEMMA,Yang2023PLLaMa,ma2023ecomgptct,cheng2024adapting, without recognizing it as a typical CL solution to vertical continual learning. itemize",
      "origin_cites_number": 7
    },
    {
      "section_title": "Different Domains of DAP",
      "level": "3",
      "content": "We include work aimed at establishing vertical LLMs across various domains, including legal, medical, financial, scientific, and code. Additionally, we cover other domains such as language and e-commerce. Legal Domain.\\quad In Layer Llama~huang2023lawyer, the authors gathered publicly available legal texts from China Courts websites, totaling approximately 10 billion tokens as noted in a GitHub issue. In SaulLM~colombo2024saullm7b, the authors collected the DAP corpus from various jurisdictions in different countries, resulting in a corpus of 30 billion tokens to cover diverse aspects of legal texts. When combined with previously available datasets, the total number of tokens used for legal-domain DAP reaches 94 billion. The substantial volume of DAP data, while offering valuable insights into specific domains, increases the risk of vertical forgetting of the general knowledge due to the large number of update steps involved. To mitigate this issue, SaulLM incorporates general data from Wikipedia, StackExchange, and GitHub into the DAP data, constituting about 2\\% of the final dataset~colombo2024saullm7b. Similarly, Lawyer Llama incorporates replaying general-domain data during DAP, but the replay rate is not disclosed~huang2023lawyer. takahashi2024pretraining also replays of non-latest business documents during DAP when building a Japanese business-specific LLM. Medical Domain.\\quad Efforts have been made to develop medical specialists by either training an LLM from scratch~gu2021domain,luo2022biogpt or fine-tuning publicly-available LLMs to meet specific medical needs~luo2023biomedgpt,wu2023pmc,Chen2023HuatuoGPTII. Among these approaches, DAP techniques have been extensively utilized to preserve the communication and instruction-following abilities of a general LLM, preparing it for subsequent medical applications~luo2023biomedgpt,wu2023pmc,Chen2023HuatuoGPTII. BioMedGPT~luo2023biomedgpt is a multi-modal biomedical language model that integrates representations of human language and the language of life (molecules, proteins, cells, genes, etc.). Prior to final multi-modal supervised fine-tuning, the authors initialize the model from Llama2-Chat~touvron2023llama2 and conduct DAP using extensive biomedical documents from S2ORC~lo2020s2orc, without considering any CL techniques or evaluations. In guo2023continuous, DAP is performed using Chinese medical encyclopedias and online expert articles, with next-token prediction as the training objective. During DAP, the performance gradually deteriorates on general-domain datasets as the training step increases, but improves on the downstream medical examination tasks~hendryckstest2021. PMC-LLama~wu2023pmc gathers biomedical papers from S2ORC~lo2020s2orc and medical textbooks for ``knowledge injection training.'' During this phase, a general language corpus from RedPajama-Data~together2023redpajama is replayed at a 5\\% rate within a training batch. However, the paper does not analyze the effectiveness of this operation of mixing in general-domain data for DAP. To mitigate vertical forgetting, AF Adapter~yan2023af proposes an adapter structure extending the width of Attention layers and FFNs for acquiring domain knowledge and only the adapters are tuned during DAP. Similarly, Hippocrates~acikgoz2024hippocrates deploys LoRA during DAP to both have medical-specific knowledge injected and general ability preserved. Me-Llama~xie2024me mixes in about 25\\% of the general-domain data for DAP on the clinical notes and biomedical articles, which achieves even positive backward transfer on MMLU~hendryckstest2021. HuatuoGPT-II~Chen2023HuatuoGPTII proposes to fuse the DAP into the final SFT, unifying the two stages into one single process. The challenge of such process mainly comes from the data heterogeneity of DAP's unlabeled corpus. The authors address this challenge by reformulating paragraphs of data into (instruction, output) format using existing large language models. They further employ a priority sampling strategy to avoid compromising downstream ability, a pitfall observed in the fixed-rate data mixing strategy~touvron2023llama2. This paper empirically demonstrates the superiority of unified one-stage SFT over two-stage training, questioning the reasonability of the current DAP. On medical-domain data, rongali2021continual finds that LMs constrained by CL techniques on source domains exhibit greater robustness to future domain shifts. Specifically, they identify that parameter regularization techniques like EWC~kirkpatrick2017overcoming, despite slightly higher cost, can facilitate positive forward and backward transfer. table*[htbp] \\centering \\textbf{Summary of the existing studies that leverage Domain-Adaptive Pre-training of LLMs, where the papers are organized in four main categories based on whether they (i) adopt the continual learning techniques and (ii) perform the evaluation for backward transfer~(forgetting). In the column of Train Proc.~(Training Process), we omit the phase of general Pre-Training. DAP represents Domain-Adaptive Pre-Training; SFT represents Supervised Fine-Tuning; IT represents Instruction Tuning. The prefix G- and D- represent General and Domain-Specific training process~lin2023geogalactica,huang2023lawyer, and the prefix U- represents them unified~wu2024llama,Chen2023HuatuoGPTII. The prefix MM- and LC- represents Multi-Modal and Long-Context training phases~luo2023biomedgpt,Zheng2023MarineGPT,rozière2024code. In the column of Continual Learning Eval., we consider two criteria: (i)~Backward Transfer, i.e., performance degradation on the previous tasks, which is also known as catastrophic forgetting, (ii)~Forward Transfer, i.e., the performance gained by DAP while transferring the LLMs to the downstream tasks. We use L and Perp. to denote Loss and Perplexity, FT to denote Fine-Tuning, ZS and FS to denote Zero-Shot and Few-Shot Accuracy, HE and LLM to denote the Human Evaluation and LLM Evaluation for generative tasks. } 1\\linewidth{!}{% \\tabcolsep{5pt} tabular{ccccC CCcc} \\toprule[0.15em] 2{*}[-0.25em]{Domain} & 2{*}[-0.25em]{Method} & 2{*}[-0.25em]{Train Proc.} & 2{*}[-0.25em]{LLM Arch.} & 3{c}{{Continual Learning Tech.}} & 2{c}{{Continual Learning Eval.}} \\\\ \\cmidrule(lr){5-7}\\cmidrule(lr){8-9} & & & & Rehearsal & Param. Reg. & Arch. Exp. & Backward Transfer & Forward Transfer \\\\ \\midrule \\midrule Medical & BioMedGPT~luo2023biomedgpt & DAP $\\rightarrow$ MM-SFT & Llama2 & \\xmark & \\xmark & \\xmark & \\xmark & FT \\\\ \\hline Financial & BBT-Fin~Lu2023BBTFin & DAP & T5 & \\xmark & \\xmark & \\xmark & \\xmark & FT \\\\ \\hline Financial & CFGPT~li2023cfgpt & DAP $\\rightarrow$ SFT & InternLM & \\xmark & \\xmark & Q-LoRA$_{(SFT)}$ & \\xmark & HE$^1$ \\\\ \\hline Scientific & AstroLlama~Nguyen2023AstroLLaMA & DAP & LlaVa & \\xmark & \\xmark & \\xmark & \\xmark & Perp. \\\\ \\hline Scientific & OceanGPT~Bi2023OCEANGPT & DAP $\\rightarrow$ IT & Vicuna \\\\ Llama2-chat \\\\ ChatGLM2 & \\xmark & \\xmark & LoRA$_{(IT)}$ & \\xmark & HE\\\\ \\hline Scientific & K2~deng2023learning & DAP $\\rightarrow$ SFT & Llama & \\xmark & \\xmark & LoRA$_{(SFT)}$ & \\xmark & Perp. | ZS | LLM \\\\ \\hline Scientific & MarineGPT~Zheng2023MarineGPT & MM-DAP $\\rightarrow$ MM-IT & Llama & \\xmark & \\xmark & \\xmark & \\xmark & HE \\\\ \\hline Code & CodeGen~nijkamp2022codegen & DAP $\\rightarrow$ DAP & CodeGen & \\xmark & \\xmark & \\xmark & \\xmark & Perp. | ZS\\\\ \\hline Code & Comment-Aug~song2024code & IT $\\rightarrow$ DAP & Llama2 \\\\ Code~Llama \\\\ InternLM2 & \\xmark & \\xmark & \\xmark & \\xmark & ZS \\\\ \\hline EventTemporal & EcoNet~han2021econet$^1$ & DAP $\\rightarrow$ FT & BERT \\\\ RoBERTa & \\xmark & \\xmark & \\xmark & \\xmark & FT \\\\ \\hline CommonSense & CALM~zhou2020pre & DAP $\\rightarrow$ FT & T5 & \\xmark & \\xmark & \\xmark & \\xmark & FT \\\\ \\hline Multi-Domain & BLADE~li2024blade & DAP $\\rightarrow$ IT & BLOOMZ & \\xmark & \\xmark & \\xmark & \\xmark & ZS \\\\ \\hline Scientific & ClimateGPT~thulke2024climategpt & DAP $\\rightarrow$ IT $\\rightarrow$ RAG & Llama2 & \\xmark & \\xmark & \\xmark & \\xmark & FS | Ret. \\\\ \\midrule \\midrule Medical & guo2023continuous & DAP $\\rightarrow$ FT & Llama2 & \\xmark & \\xmark & \\xmark & gray1 FS | FT & FS | FT\\\\ \\hline Financial & xie2023efficient & DAP & Pythia & \\xmark & \\xmark & \\xmark & gray1 L | FS & L | FS \\\\ \\hline Scientific & GeoGalactica~lin2023geogalactica & DAP $\\rightarrow$ G-SFT $\\rightarrow$ D-SFT & GAL & \\xmark & \\xmark & \\xmark & gray1 ZS & Perp. | ZS | LLM \\\\ \\hline Code & StarCoder~li2023starcoder & DAP & StarCoder & \\xmark & \\xmark & \\xmark & gray1 Perp. | ZS | FS & Perp. | ZS | FS \\\\ \\hline Code & DeepSeek-Coder~guo2024deepseekcoder & DAP & DeepSeek-LLM & \\xmark & \\xmark & \\xmark & gray1 ZS | FS & ZS \\\\ \\hline Multi-Domain & DAPT~gururangan2020dont & DAP $\\rightarrow$ FT & RoBERTa & \\xmark & \\xmark & \\xmark & gray1 Loss & L | FT \\\\ \\hline \\hline Financial & WeaverBird~Xue2023WeaverBird & DAP & GLM2 & gray2 \\xmark & gray2 \\xmark & gray2 LoRA & \\xmark & HE \\\\ \\hline Code & IRCoder~paul2024ircoder & DAP & StarCoder \\\\ DeepSeek-Coder \\\\ Code~Llama & gray2 \\xmark & gray2 \\xmark & gray2 LoRA & \\xmark & ZS \\\\ \\hline Code & Code~Llama~rozière2024code & \\small{DAP $\\rightarrow$ LC-FT $\\rightarrow$ IT \\\\ DAP $\\rightarrow$ DAP $\\rightarrow$ LC-FT} & Llama2 & gray2 Replay & gray2 \\xmark & gray2 \\xmark & \\xmark & Perp. | ZS \\\\ \\hline Legal & SaulLM~colombo2024saullm7b & DAP $\\rightarrow$ U-IT & Mistral & gray2 Replay & gray2 \\xmark & gray2 \\xmark & \\xmark & Perp. | ZS \\\\ \\hline Medical & PMC-Llama~wu2023pmc & DAP $\\rightarrow$ IT & Llama & gray2 Replay & gray2 \\xmark & gray2 \\xmark & \\xmark & ZS | FT \\\\ \\hline Scientific & Llema~Azerbayev2023LLEMMA & DAP & Code~Llama & gray2 Replay & gray2 \\xmark & gray2 \\xmark & \\xmark & Perp. | FS \\\\ \\hline Multi-Domain & DAS~ke2022continual-pre & [DAP]$_n$ & RoBERTa & gray2 DER++$^\\club$ & gray2 EWC$^\\club$\\qquad HAT$^\\club$\\qquad Soft-Masking & gray2 Adapter$^\\club$\\qquad DEMix$^\\club$ & \\xmark & FT \\\\ \\hline Medical & Hippocrates~acikgoz2024hippocrates & DAP $\\rightarrow$ IT $\\rightarrow$ MA & Llama2 \\\\ Mistral & gray2 \\xmark & gray2 \\xmark & gray2 LoRA & \\xmark & ZS | FS \\\\ \\hline Language & Sailor~dou2024sailor & DAP & Qwen1.5 & gray2 Replay & gray2 \\xmark & gray2 \\xmark & \\xmark & ZS \\\\ \\midrule \\midrule Code \\& Math & Llama Pro~wu2024llama & DAP $\\rightarrow$ U-SFT & Llama2 & gray3 \\xmark & gray3 \\xmark & gray3 Block~Exp. LoRA$^{\\club}$ & gray3 ZS | FS & Perp. | ZS | FS \\\\ \\hline Medical & AF Adapter~yan2023af & DAP $\\rightarrow$ FT & RoBERTa & gray3 \\xmark & gray3 \\xmark & gray3 Layer~Exp. LoRA$^{\\club}$ & gray3 Acc. & L | FT\\\\ \\hline Medical & rongali2021continual & DAP $\\rightarrow$ FT & BERT \\\\ RoBERTa \\\\ DistilBERT & gray3 Replay$^{\\club}$ GEM$^{\\club}$ & gray3 L2~Reg.$^{\\club}$ EWC$^{\\club}$ & gray3 \\xmark & gray3 L | FT & L | FT\\\\ \\hline Medical & HuatuoGPT-II~Chen2023HuatuoGPTII & DAP + U-SFT & Baichuan2 & gray3 Replay & gray3 \\xmark & gray3 \\xmark & gray3 ZS & ZS | HE\\\\ \\hline Financial & XuanYuan 2.0~Zhang2023xuanyuan & DAP + SFT & BLOOM & gray3 Replay & gray3 \\xmark & gray3 \\xmark & gray3 HE & HE\\\\ \\hline Scientific & PLlama~Yang2023PLLaMa & DAP $\\rightarrow$ IT & GAL & gray3 Replay & gray3 \\xmark & gray3 \\xmark & gray3 L & L | ZS \\\\ \\hline E-Commerce & EcomGPT-CT~ma2023ecomgptct & DAP $\\rightarrow$ SFT & BLOOM & gray3 Replay & gray3 \\xmark & gray3 \\xmark & gray3 ZS | FS & ZS | FS \\\\ \\hline Legal & Layer~Llama~huang2023lawyer & DAP $\\rightarrow$ G-IT $\\rightarrow$ D-IT & Llama & gray3 Replay & gray3 \\xmark & gray3 \\xmark & gray3 ZS & ZS\\\\ \\hline Multi-Domain & AdaptLLM~cheng2024adapting & DAP & Llama & gray3 Replay & gray3 \\xmark & gray3 \\xmark & gray3 ZS & ZS | FT\\\\ \\hline Language & Swallow~fujii2024continual & DAP & Llama2 & gray3 Replay & gray3 \\xmark & gray3 \\xmark & gray3 FS & FS \\\\ \\hline Financial & takahashi2024pretraining & DAP & Llama2 & gray3 Replay & gray3 \\xmark & gray3 \\xmark & gray3 Loss | ZS & Loss | ZS | FS | RAG \\\\ \\hline Medical & Me-Llama~xie2024me & DAP $\\rightarrow$ IT & Llama2 & gray3 Replay & gray3 \\xmark & gray3 \\xmark & gray3 ZS | FS & ZS | FS | FT \\\\ \\hline Language & Aurora-M~nakamura2024aurora & DAP $\\rightarrow$ IT & StarCoder & gray3 Replay & gray3 \\xmark & gray3 \\xmark & gray3 ZS & ZS | FS | HE \\\\ \\hline \\bottomrule[0.15em] tabular } table* Financial Domain.\\quad A gap persists between general-purpose LLMs and existing domain-specific smaller-scale LLMs~araci2019finbert,Wu2023BloombergGPT, underscoring the urgent need for more powerful financial-domain experts through the integration of LLMs. Notably, DAP techniques have emerged as crucial tools for tailoring LLMs to the intricacies of the financial domain while mitigating the negative effects of abrupt domain shifts from general to finance~Lu2023BBTFin,li2023cfgpt,xie2023efficient,Xue2023WeaverBird,Zhang2023xuanyuan. BBT-Fin~Lu2023BBTFin collects a Chinese financial DAP dataset comprising 80 billion tokens sourced from corporate reports, analyst reports, social media, and financial news. In addition to the conventional masked language modeling (MLM) training objective, BBT-Fin further incorporates triplet masking and span masking techniques during DAP. CFGPT~li2023cfgpt creates CFData, a financial dataset for DAP and SFT, comprising 141 billion tokens. During DAP, CFGPT does not employ CL techniques but utilizes QLoRA~dettmers2023qlora for preventing overfitting to downstream data and balancing general response ability and domain-specific ability during SFT. These two methods are typical domain-specific LLMs focusing solely on adaptation to target domains without explicit CL measures or evaluation of vertical forgetting. In xie2023efficient, the authors aim to enhance the data efficiency of DAP. When the downstream tasks' data distribution $\\gT$ are known, based on the generalization bound~ben2010theory,ganin2016domain,shi2024unified, the authors propose to sample the subset of DAP data whose distribution $\\gD$ is similar to the downstream task's data, i.e., $d_{\\gH\\Delta \\gH}(\\gD, \\gT)$ is low. When the downstream data distribution is unknown, the authors suggest ensuring novelty and diversity in the sampled corpus for DAP. This approach significantly enhances DAP efficiency: it utilizes only 10\\% of the originally collected data yet outperforms models trained on the entire DAP dataset, underscoring the importance of data quality over quantity. WeaverBird~Xue2023WeaverBird introduces an intelligent finance dialogue system, where the encoder is trained on Chinese and English financial documents, alongside expert-annotated financial query-response pairs, using LoRA~hu2022lora. Xuanyuan 2.0~Zhang2023xuanyuan, akin to HuatuoGPT-II~Chen2023HuatuoGPTII, proposes the technique of hybrid-tuning, which fuses the stages of DAP and SFT into one, general-domain data and financial-domain data into one. Notably, the distribution of data in hybrid-tuning is unconventional: financial DAP data comprises only a small portion of 13\\%. This prompts a pertinent question in line with the investigation on efficient DAP in xie2023efficient: Is a large DAP dataset necessary for developing a domain-specific LLM? Scientific Domain.\\quad Vertical scientific LLMs span many subjects~Zhang2024SciGLM,Nguyen2023AstroLLaMA,Azerbayev2023LLEMMA,luo2023wizardmath,lin2023geogalactica,Bi2023OCEANGPT,Zheng2023MarineGPT. However, among all the studies listed above, only a small fraction of them adopt the technique of DAP. OceanGPT~Bi2023OCEANGPT is the first LLM tailored specifically for the ocean domain. It performs DAP on a raw corpus of ocean science literature, prioritizing recent research and historically significant works. K2~deng2023learning pioneers the development of a foundational language model tailored specifically for geoscience. It aggregates geoscience open access literature and Earth science-related Wikipedia pages for DAP. Following this, it undergoes multi-task instruction tuning utilizing LoRA~hu2022lora on both a general instruction tuning dataset and the GeoSignal benchmark introduced within the K2 framework. AstroLlama~Nguyen2023AstroLLaMA gathers abstracts solely from astronomy papers on arXiv and proceeds pre-training. It observes an improved perplexity on the domain of scholarly astronomy, without providing more quantitative evaluation. MarineGPT~Zheng2023MarineGPT is a multi-modal LLM designed specifically for the marine domain. During DAP, MarineGPT incorporates 5 million marine image-text pairs to imbue domain knowledge. This involves training a Q-Former~li2023blip2 between the frozen visual and text decoder~dosovitskiy2020image,touvron2023llama. Another branch of methods proactively integrate in the replay of the general-domain data to mitigate vertical forgetting. GeoGalactica~lin2023geogalactica introduces a series of LLMs tailored for geoscience. In the DAP phase, besides the 52-billion-token geoscience corpus, Arxiv papers and Codedata are incorporated, with a mixing ratio of 8:1:1. The authors believe that the inclusion of the Codedata during the model's pre-training can significantly boost the reasoning ability of the LLMs. Although GeoGalactica pinpoints challenges of DAP, including overfitting, catastrophic forgetting, maintaining the training stability, and convergence speed, it does not further provide empirical evidence supporting the inclusion of the Codedata, or deploying specific measures to address the challenges proposed above. Llemma~Azerbayev2023LLEMMA focuses on mathematics, initialized from Code~Llama~rozière2024code, and undergoes DAP on a blend of the 55-billion-token mathematical pre-training dataset and general domain data at the ratio of 19:1. In contrast, PLlama~Yang2023PLLaMa, designed for plant science, mixes domain-specific and general-domain data at the ratio of 9:1. Code Domain.\\quad The development of LLMs for automatic code filling, debugging, and generation holds significant practical importance~moradidakhel2023github,sun2024survey. These advancements cover various frameworks, including encoder-only~moradidakhel2023github, encoder-decoder~wang2021codet5,wang2023codet5plus, and decoder-only~nijkamp2022codegen,lozhkov2024starcoder,guo2024deepseekcoder. There is a growing trend towards decoder-only architectures~sun2024survey, leveraging models pre-trained on general natural language like Llama~touvron2023llama,touvron2023llama2. Consequently, there is a shift in the training objective from utilizing code structures to simpler tasks like next token prediction and infilling. From the perspective of CL, the code domain presents unique advantages and challenges for DAP, compared to other domains. On one hand, its hierarchical structure (general domain corpus $\\rightarrow$ multi-language code $\\rightarrow$ specific programming language) provides an ideal training pipeline for DAPs~rozière2024code, offering potential for more efficient training strategies. On the other hand, programming languages adhere to strict grammars, unlike the fuzzy and context-dependent natural language. Consequently, language models should ideally leverage these structures through tailored designs, and adopting the same training objectives as for natural languages may yield sub-optimal results. Therefore, many existing studies omit DAP~wang2021codet5,wang2023codet5plus,luo2023wizardcoder. In the following section, we will introduce existing code LLMs that employ DAP before the final downstream tasks, discussing both their common attributes and unique characteristics. Representing a series of notable works that focus solely on adaptation to target domains, CodeGen~nijkamp2022codegen comprises a suite of LLMs designed for natural language~(CodeGen-NL), multi-lingual programming languages~(CodeGen-Multi), and mono-lingual programming languages~(CodeGen-Mono). These models are trained sequentially, with each subsequent model initialized from the previous one trained on more general-domain data. Comment-Aug~song2024code addresses the challenge of aligning programming languages with natural languages (PL-NL alignment) by performing DAP on the code augmented with generated additional comments. StarCoder~li2023starcoder introduces two models: StarCoderBase and StarCoder. StarCoderBase is initially trained on a mixed dataset comprising various programming languages without significant reweighting on the data. Subsequently, StarCoderBase undergoes further fine-tuning on additional 35 billion tokens of Python code, resulting in the development of StarCoder. DeepSeek-Coder-v1.5~guo2024deepseekcoder originates from DeepSeek-LLM~deepseekai2024deepseek and undergoes pre-training on 2 trillion tokens, comprising 87\\% source code, 10\\% English code-related natural language, and 3\\% Chinese natural language corpus. Initialization from a general-domain LLM results in improved performance across various tasks, including natural language and mathematical reasoning, with minimal performance degradation on coding tasks, which underscores the efficacy of DAP. As the only work that utilizes the general data replay to mitigate vertical forgetting in the code domain, Code~Llama~rozière2024code introduces a sophisticated training framework tailored for various coding tasks and model sizes. Initialized from Llama~2 weights, these models undergo DAP on a dataset composed of deduplicated public code, discussions about code, and a subset of natural language data. This mix of natural language data serves as a form of pseudo-replay to maintain the models' proficiency in understanding natural language. Besides replay, architecture expansion has proven effective in acquiring robust coding abilities and preventing vertical forgetting simultaneously. IRCoder~paul2024ircoder utilizes compiler intermediate representations to enhance the multilingual transferability of Code LLMs. By conducting DAP on code grounded in intermediate representations with LoRA~hu2021lora, IRCoder achieves superior multilingual programming instruction following, enhanced multilingual code understanding, and increased robustness to prompt perturbations. Llama~Pro~wu2024llama undergoes DAP on a combination of code and math data. It expands the original Llama2 architecture by dynamically adding multiple identity copies of the transformer blocks. These added blocks initially preserves the original functionality, and will be tuned for DAP. The proposed expansion method is shown to be more resilient against vertical forgetting compared to other parameter-efficient tuning methods like LoRA. The three aforementioned studies highlight the importance of DAP for code LLMs. However, it is crucial to note that the problem definition and conventional architectures of existing Code LLMs may present challenges of compatibility for DAP deployment, and need to be addressed in the future. Other Domains.\\quad ECONET~han2021econet enhances the model's ability to reason about event temporal relations through a dedicated DAP phase. Temporal and event indicators are masked out, and a contrastive loss is applied to the recovered masked tokens. Results demonstrate that incorporating this DAP stage significantly improves performance on final tasks compared to direct fine-tuning. Concept-Aware Language Model~(CALM)~zhou2020pre introduces a data-efficient DAP approach for enhancing the concept-centric commonsense reasoning ability of LLMs. It incorporates both generative and discriminative commonsense reasoning tasks specifically tailored for concept-centric reasoning tasks. Consequently, even a small number of data examples for DAP can lead to notable improvements for downstream tasks. Aurora-M~nakamura2024aurora and Swallow~fujii2024continual adopt the simple replay strategy that mixes in a small portion of general data during DAP for their multi-lingual ability. Furthermore, Sailor~dou2024sailor studies the optimal strategy of data mixing for DAP, balancing the general knowledge and capacity of different languages. EcomGPT-CT~ma2023ecomgptct employs a data mixing strategy for DAP which transforms semi-structured E-commerce data into a set of nodes and edges, samples a cluster of nodes, and then extracts and concatenates them into a training example. It combines the general-domain corpus with E-commerce data at a ratio of 2:1, which is significantly lower than the common setting adopted by other works. Notably, there are some papers studying other effective ways of DAP. AdaptLLM~cheng2024adapting transforms raw corpora into (raw text, question, answer) format, creating intrinsic reading comprehension tasks. AdaptLLM demonstrates superior domain-specific knowledge adaptation and minimal vertical forgetting, thereby challenging the data efficiency of conventional DAP. Tag-LLM~shen2024tag re-purposes the general-domain LLM into domain-specific one by multi-stage training of domain tags and function tags, without modifying the base LLM's weights and thereby mitigates forgetting.",
      "origin_cites_number": 117
    },
    {
      "section_title": "Continual Fine-Tuning~(CFT)",
      "level": "2",
      "content": "Background of Continual Fine-Tuning~(CFT).\\quad Continual Fine-Tuning~(CFT) lies at the bottom layer of the vertical continuity, where models are trained on successive homogeneous tasks drawn from an evolving data distribution. As the service-oriented layer of LLM, it does not require consideration of further adaptation to another downstream tasks, simplifying optimization objectives to a great extent: better adaptation and less forgettingWe direct interested readers to additional survey literature on the topic of general CFT~\\cite{biesialska2020continual,ke2023continual.}. In the era of LLMs, new computational paradigms in CFT have emerged and attracted significant attention within the research community. These topics include (i)~Continual Instruction Tuning~(CIT)~zhang2023citb, (ii)~Continual Model Refinement~(CMR)~hartvigsen2023aging, (iii)~Continual Model Alignment~(CMA)~lin2024mitigating,zhangcppo, and (iv)~Continual Learning for Multimodal Language Models~(CMLLMs)~he2023continual,ni2023continual. We summarize existing studies on CFT in tab:cft, categorizing studies into sub-categories as listed above. The table includes details on incremental learning types (X-IL), LLM architecture, and employed CL techniques and evaluation metrics. After discussing general observations on CFT in sec:cft-obs, we will delve into each sub-category in detail. \\aboverulesep{0pt} \\belowrulesep{0pt} table*[htbp] \\centering \\textbf{Summary of the existing studies on Continual Fine-Tuning LLMs, where the papers are organized in five main categories based on what downstream tasks they are designed to tackle, including (i)~General Continual Fine-Tuning~(CFT); (ii)~Continual Instruction Tuning~(CIT); (iii)~Continual Model Refinement~(CMR); (iv)~Continual Model Alignment~(CMA); (v)~Continual Multimodal LLMs~(CMLLMs), which is shown in the column of CFT Type. The column of X-IL shows what continual learning paradigm the study includes~van2022three, where TIL represents task-incremental learning, meaning task ID/information is provided during inference; DIL represents domain-incremental learning, meaning the tasks are defined in the same format, and no task ID/information is available during inference; CIL represents class-incremental learning, meaning the task ID needs to be further inferred when testing. } 1\\linewidth{!}{% \\tabcolsep{2pt} tabular{ccccc ccccc c} \\toprule[0.15em] 2{*}[-0.6em]{CFT Type} & 2{*}[-0.6em]{Method} & 2{*}[-0.6em]{X-IL} & 2{*}[-0.6em]{LLM Arch.} & 4{c}{{Continual Learning Tech.}} & 3{c}{{Continual Learning Eval.}} \\\\ \\cmidrule(lr){5-8}\\cmidrule(lr){9-11} & & & & Rehearsal & Param. Reg. & Arch. Exp. & Others & \\makecell{Avg. \\\\Acc.} & \\makecell{Bwd. \\\\Trans.} & \\makecell{Fwd. \\\\Trans.} \\\\ \\midrule \\midrule 11{*}[0em]{General} & CTR~ke2021achieve & DIL | CIL & BERT & \\xmark & \\xmark & Adapter & \\xmark & \\cmark & \\cmark & \\cmark \\\\ 2-11 & tao2022can & TIL & BERT & S-Replay & \\xmark & \\xmark & \\xmark & \\club & \\club & \\club \\\\ 2-11 & CIRCLE~wei2022circle & DIL & T5 & Replay & EWC & Prompt & \\xmark & \\cmark & \\cmark & \\cmark \\\\ 2-11 & ConPET~song2023conpet & DIL & Llama & Replay & \\xmark & LoRA & \\xmark & \\cmark & \\cmark & \\cmark \\\\ 2-11 & bai2023enhancing & DIL | CIL & BERT & \\xmark & \\xmark & \\xmark & G-Prompt & \\cmark & \\cmark & \\xmark \\\\ 2-11 & luo2023investigating & TIL & DistilBERT \\\\ ALBERT | RoBERTa & ER | DER | LwF & \\xmark & \\xmark & \\xmark & \\club & \\club & \\xmark \\\\ 2-11 & SEQ$^*$~zheng2023learn & TIL | CIL & Pythia | BERT | GPT2 & \\xmark & P-Freeze & \\xmark & Tricks for Classifiers & \\xmark & \\cmark & \\xmark \\\\ 2-11 & LFPT5~qin2021lfpt5 & DIL & T5 & P-Replay & \\xmark & \\xmark & \\xmark & \\cmark & \\cmark & \\xmark\\\\ 2-11 & weyssow2023usage & DIL & RoBERTa | GPT2 & Replay & EWC | SI | RWalk & \\xmark & \\xmark & \\cmark & \\cmark & \\xmark \\\\ 2-11 & LR~ADJUST~winata2023overcoming & DIL & XLM-R & \\xmark & \\xmark & \\xmark & LR Scheduling & \\cmark & \\cmark & \\cmark \\\\ 2-11 & C3~chen2024parameterizing & TIL & T5 & KD & \\xmark & Prompt Tuning & \\xmark &\\cmark & \\cmark & \\xmark \\\\ \\midrule \\midrule & CT0~scialom2022fine & TIL & T0 & S-Replay & \\xmark & \\xmark & \\xmark & \\cmark & \\cmark & \\cmark \\\\ 2-11 & RCL~wang2023trace & TIL & LLaMA \\\\ Vicuna | Baichuan & Replay & \\xmark & \\xmark & \\xmark & \\cmark & \\cmark & \\cmark \\\\ 2-11 & DynaInst~mok2023large & TIL & BART & Replay & \\xmark & \\xmark & \\xmark & \\cmark & \\cmark & \\cmark \\\\ 2-11 & CITB~zhang2023citb & TIL & T5 & Replay | AGEM & L2 | EWC & AdapterCL & \\xmark & \\cmark & \\cmark & \\cmark \\\\ 2-11 & SSR~huang2024mitigating & TIL & LLaMA | Alpaca & RandSel | KMeansSel & \\xmark & \\xmark & \\xmark & \\cmark & \\cmark & \\cmark \\\\ 2-11 & KPIG~he2024dont & DIL | TIL & LLaMA | Baichuan & DynaInst | PCLL | DCL & L2 \\\\ EWC & DARE \\\\ LM-Cocktail & KPIG & \\cmark & \\cmark & \\cmark \\\\ 2-11 & ConTinTin~yin2022contintin & TIL & BART & Replay & \\xmark & \\xmark & InstructionSpeak & \\cmark & \\cmark & \\cmark \\\\ 2-11 & O-LoRA~wang2023orthogonal & TIL & LLaMA | Alpaca & \\xmark & \\xmark & O-LoRA & \\xmark & \\cmark & \\cmark & \\cmark \\\\ 2-11 -9{*}[1em]{{CIT}} & SAPT~zhao2024sapt & TIL & T5 | LLaMA & \\xmark & \\xmark & \\xmark & SAPT & \\cmark & \\cmark & \\cmark \\\\ 2-11 & InsCL~wang2024inscl & TIL & LLaMA & Replay & \\xmark & \\xmark & InsCL & \\cmark & \\cmark & \\cmark \\\\ \\midrule \\midrule & CMR~lin2022continual & DIL & BART & ER | MIR | MLR & L2 | EWC & \\xmark & \\xmark & \\cmark & \\cmark & \\cmark \\\\ 2-11 & GRACE~hartvigsen2023aging & DIL & T5 | BERT | GPT2 & \\xmark & \\xmark & Adapter & \\xmark & \\cmark & \\cmark & \\xmark \\\\ 2-11 & WilKE~hu2024wilke & DIL & GPT2 | GPT-J & \\xmark & \\xmark & Adaptor & \\xmark & \\cmark & \\cmark & \\cmark \\\\ 2-11 & Larimar~das2024larimar & DIL & BERT | GPT-J & \\xmark & \\xmark & \\xmark & Kanerva Memory & \\cmark & \\cmark & \\cmark \\\\ 2-11 & MELO~yu2023melo & DIL & BERT | GPT2 | T5 & \\xmark & \\xmark & LoRA & \\xmark & \\cmark & \\cmark & \\cmark \\\\ 2-11 & CME~li2023continual & DIL & BERT & Replay & \\xmark & \\xmark & Inner-Prod. Reg. & \\cmark & \\cmark & \\cmark \\\\ 2-11 -7{*}[0em]{{CMR}} & WISE~wang2024wise & DIL & GPT-J | Llama2 | Mistral & \\xmark & \\xmark & \\xmark & Side Memory & \\cmark & \\cmark & \\cmark \\\\ \\midrule \\midrule & COPF~zhang2023copf & TIL | DIL & Llama & Replay & Function Reg. & Prompt & \\xmark & \\checkmark & \\xmark & \\checkmark \\\\ 2-11 & AMA~lin2024mitigating & DIL & OpenLLaMA | Mistral & Replay & L1 | L2 & LoRA & Adaptive Model Avg. &\\club &\\club &\\club \\\\ 2-11 -3{*}[0em]{{CMA}} & CPPO~zhangcppo & TIL & GPT2 & \\xmark & Weighting & Prompt & \\xmark & \\checkmark & \\checkmark & \\checkmark \\\\ \\midrule \\midrule & EProj~he2023continual & TIL & InstructBLIP & \\xmark & TSIR & Projector Exp. & \\xmark & \\cmark & \\xmark & \\cmark\\\\ 2-11 & Fwd-Prompt~zheng2024antiforgetting & TIL & InstructBLIP | BLIP2 & \\xmark & \\xmark & Projector Exp. & \\xmark & \\cmark & \\cmark & \\cmark\\\\ 2-11 & CoIN~chen2024coin & TIL & LLaVA & \\xmark & \\xmark & MoE | LoRA & \\xmark & \\cmark & \\xmark & \\cmark \\\\ 2-11 & Model~Tailor~zhu2024model & TIL & InstructBLIP | LLaVA & \\xmark & Model~Tailor & \\xmark & \\xmark & \\cmark & \\cmark & \\cmark \\\\ 2-11 -5{*}[0em]{{CMLLMs}} & RebQ~zhao2024reconstruct & TIL & ViLT & \\xmark & \\xmark & Prompt Tuning & \\xmark & \\cmark & \\xmark & \\cmark \\\\ \\bottomrule[0.15em] tabular } table*",
      "origin_cites_number": 42
    },
    {
      "section_title": "General Observations on CFT",
      "level": "3",
      "content": "Examining the landscape of continual learning in the context of LLMs, and combined with the results shown in tab:cft, we make several key observations about CFT. itemize \\item OBS-1: There has been a noticeable transition in focus from CIL to TIL and DIL. It has been a longstanding common sense in the CL community that CIL, as it requires the model to predict the context label and within-context label at the same time~van2022three,wang2024comprehensive,kim2022theoretical, is the most challenging CL scenario and hence receives most of the attention from the community. However, among all 35 papers presented in tab:cft, only 3 papers study CFT of CIL. The transition of the research focus demonstrates the importance of TIL and DIL in the real-world applications of continual LLMs. More detailed discussion of this transition is included in sec:discussion-xil. \\item OBS-2: In CFT, CL techniques enjoy broader adoption and explicit exploration compared to CPT and DAP. In tab:cft, all 35 papers explicitly deploy the CL techniques, 50\\% of which develop new techniques that cannot be easily interpreted as trivial combination of existing classic CL techniques, e.g., shared attentive learning framework in SAPT~zhao2024sapt, external memory deployed in Larimar~das2024larimar, and adaptive model averaging method to achieve Pareto-optimal in AMA~lin2024mitigating, etc. This underscores the recognition of continual learning as a pivotal component in the development of resilient and adaptive LLMs. itemize \\aboverulesep{2pt} \\belowrulesep{2pt}",
      "origin_cites_number": 4
    },
    {
      "section_title": "General Continual Fine-Tuning~(General CFT)",
      "level": "3",
      "content": "Researchers have long investigated the phenomenon of forgetting resilience in pre-trained LLMs when fine-tuned for downstream tasks ke2021achieve,tao2022can,luo2023investigating,zheng2023learn,mehta2023empirical, despite some discover the opposite~luo2023investigating. Although the pre-trained weights initially position the model in a flat-loss basin, aiding adaptation to future tasks without severely impacting previous ones mehta2023empirical, zero or near-zero forgetting is only observed at the representation level. This implies that while the model retains its ability to distinguish between task-specific representations, it may still forget specific task details wu2021pretrained,tao2022can,luo2023investigating,zheng2023learn. Therefore, additional measures are necessary when deploying these models in real-world applications ke2021achieve,wei2022circle,bai2023enhancing,qin2021lfpt5,weyssow2023usage,chen2024parameterizing. Many studies advance beyond naive sequential fine-tuning, leveraging the inherent anti-forgetting nature of LLMs while avoiding the adoption of overly complex CL techniques~winata2023overcoming,zheng2023learn. For instance, LR~ADJUST~winata2023overcoming proposes a straightforward yet effective method of dynamically adjusting the learning rate to mitigate the overwriting of knowledge from new languages onto old ones. Building on the innate anti-forgetting ability of large language models like Pythia biderman2023pythia, SEQ$^*$ zheng2023learn introduces several strategies for fine-tuning LLMs on a sequence of downstream classification tasks, such as freezing the LLM and old classifier's parameters after warm-up, and pre-allocating future classifiers, etc. Given the minimal forgetting observed at the representation level in CL, some studies aim to tackle the misalignment between the representation space and the decision-making layers by introducing representation-level constraints during CFT. NeiAttn~bai2023enhancing exemplifies this approach by formulating classification tasks as masked language modeling and proposing a neighboring attention mechanism to counteract negative representation drift. Another line of approaches refines the input/output format and network architectures of pre-trained LLMs to be better suited for CFT. For instance, CTR~ke2021achieve incorporates two CL-plugin modules, i.e., a task-specific module~(TSM) for acquiring task-specific knowledge and a knowledge-sharing module~(KSM) for selectively transferring previously learned similar knowledge. CIRCLE~wei2022circle manually designs diverse prompt templates for various types of buggy code, unifying them as the cloze task and employs difficulty-based replay to enhance continual program repair. LFPT5~qin2021lfpt5 addresses lifelong few-shot language learning by consolidating sequence labeling, text classification, and text generation into a text-to-text generation task. It undergoes prompt tuning on generated pseudo-examples from previous domains when adapting to new tasks. In zhang2022continual, the authors propose a method for adaptively adding compositional adapters during continual sequence generation tasks. Before training on new domains, a decision stage determines which trained module can be reused. During training, this module also regenerates examples of the past for replay. C3~chen2024parameterizing merges PEFT and in-context learning (ICL) in a teacher-student framework. The teacher model undergoes in-context tuning focused solely on the current domain, while the student model, together with tunable prompts, minimizes the KL-divergence between the output distribution and the ground truth and teacher model simultaneously.",
      "origin_cites_number": 15
    },
    {
      "section_title": "Continual Instruction Tuning~(CIT)",
      "level": "3",
      "content": "When the instruction tuning data comes in as a stream, forgetting of the previously learned instructions should be addressed. CT0~scialom2022fine represents the inaugural study on Continual Instruction Tuning~(CIT) of LLMs, applying the replay method on the base T0 model throughout the process. Many subsequent studies focus on enhancing the replay method used during CIT. For instance, he2024dont improve replay efficiency by computing Key-Part Information Gain~(KPIG) on masked parts to dynamically select replay data, addressing the ``half-listening'' issue in instruction following. Similarly, SSR~huang2024mitigating uses the LLM to generate synthetic instances for replay, achieving superior or comparable performance to traditional methods at a lower cost. Other approaches introduce multiple CL techniques during CIT. DynaInst~mok2023large merges parameter regularization with dynamic replay, selectively storing and replaying instances and tasks to enhance outcomes. InstructionSpeak~yin2022contintin employs negative training and replay instructions to improve both forward transfer and backward transfer. Some methods incorporate PEFT. Orthogonal Low-Rank Adaptation~(O-LoRA) learns new tasks within an orthogonal subspace while preserving LoRA parameters for previous tasks~wang2023orthogonal to minimize the interference among different tasks. Shared Attention Framework~(SAPT) combines a PET block with a selection module via a Shared Attentive Learning \\& Selection module, tackling catastrophic forgetting and knowledge transfer concurrently~zhao2024sapt. While regularization-based and architectural-based methods require additional parameter storage and GPU memory, together with replay-based methods they remain for CIT due to the simplicity and effectiveness~wang2024inscl.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Continual Model Refinement~(CMR)",
      "level": "3",
      "content": "The concept of model editing was initially explored in sinitsin2020editable, which introduced a ``reliability-locality-efficiency'' principle and proposed a gradient descent editor to address it efficiently. Subsequent research, such as de2021editing and fast_edit, extended this principle to edit factual knowledge in BERT-based language models and larger models like GPT-J-6B~gpt-j and T5-XXL~raffel2020exploring, respectively, using gradient decomposition. These approaches typically update a subset of model parameters to alter the labels of specific inputs. Additionally, memory-based models, as discussed in mitchell2022memory and hartvigsen2023aging, incorporate editing through retrieval mechanisms. Continual Model Refinement~(CMR) extends model refinement horizontally, presenting updated sample pairs ${(\\vx_e, y_e, y_e)}^{e=1}_N$ sequentially as a stream. lin2022continual initially introduces this idea, evaluating various CL methods with a dynamic sampling algorithm. Many CMR methods employ a retrieval mechanism. For instance, hartvigsen2023aging uses hidden activations of the language model as a ``key'' to activate updated parameters only when input $x_0$ resembles updated sample pairs; yu2023melo improves this approach's efficiency by integrating LoRA hu2021lora; das2024larimar augments the LLM with an external episodic memory, modeling CMR as an ongoing memory refresh. Meanwhile, some methods focus solely on updating a subset of model parameters. For example, hu2024wilke addresses the issue of ``toxicity buildup and flash'' in single-editing methods like ROME meng2022locating, adapting it to the CL context with a knowledge-aware layer selection algorithm. WISE~wang2024wise addresses the ``impossible triangle'' of reliability, locality, and generalization in existing lifelong model refinement methods. It introduces a side memory system that enables knowledge sharding and merging, successfully achieving all three objectives simultaneously. While all these works pioneer research in CMR, the exploration of CMR of LLMs remains open. hase2023does highlights a potential problem: the location for storing the fact may not coincide with the best place for editing it. This challenges the classical ``locate and edit'' paradigm used by several existing methods~meng2022locating, meng2022mass, and could become a significent concern for CMR~hu2024wilke. Other questions, including whether such problem setting fits LLMs and whether more memory/computationally efficient methods of CMR could be developed for LLMs, are yet to be answered.",
      "origin_cites_number": 18
    },
    {
      "section_title": "Continual Model Alignment~(CMA)",
      "level": "3",
      "content": "When LLMs undergo the phase of MA, vertical forgetting of previous knowledge usually occurs. In lin2024mitigating, the authors refer to this phenomenon of catastrophic forgetting induced caused by MA as the ``Alignment Tax.'' Notably, even a single stage of MA can diminish the model's performance capabilities, as it restricts the model's responses to a narrower subset of the training distribution. Continual Model Alignment~(CMA) aims to continuously refine LLMs to align with evolving human values, ethics, and data. The static nature of LLM training on historical data sets can lead to discrepancies between the models' outputs and current factual accuracies, societal norms, and standards, making CMA a crucial process for maintaining their adaptability and alignment with contemporary contexts. Likewise, there are two types of CMA frameworks: RL-based and SL-based. In the realm of RL-based CMA, two significant contributions have been noted. lin2024mitigating identifies the conflicts between the existing CL techniques and RLHF, and proposes Adaptive Model Averaging~(AMA), adaptively finding appropriate ratios for the combination of model layers to gain maximal rewards with minimal tax; Continual Proximal Policy Optimization~(CPPO)~zhangcppo proposes a weighting strategy for different examples deciding its usage of policy enhancement or knowledge retention, mitigating the alignment tax over time. For SL-based CMA, Continual Optimal Policy Fitting~(COPF)~zhang2023copf presents a solution adapted from the Direct Policy Optimization~(DPO)~rafailov2024direct, solving its potential risks of sub-optimal policy fitting and over-optimization in the context of CMA.",
      "origin_cites_number": 5
    },
    {
      "section_title": "Continual Multimodal Large Language Models~(CMLLMs)",
      "level": "3",
      "content": "Continually training multi-modal models like CLIP~radford2021learning has been long studied~zheng2023preventing,ni2023continual, while the problem of continually training MLLMs still remains underexplored. Several existing studies have investigated the causes of catastrophic forgetting when continually training MLLMs. zheng2024antiforgetting performs singular value decomposition on input embeddings, revealing a significant disparity among different input embeddings. This discrepancy causes the model to learn irrelevant information for previously trained tasks, resulting in catastrophic forgetting and negative forward transfer. zhai2023investigating observes that minority collapse may lead to catastrophic forgetting, when the imbalance ratio between majority and minority classes approaches infinity during fine-tuning. It further identifies hallucination as a contributing factor to performance degradation in MLLMs. Continual Fine-Tuning MLLMs.\\quad In contrast to traditional continual learning methods that involve full-model fine-tuning for new tasks, continual fine-tuning for MLLMs focuses on refining specific layers when adapting to new tasks~zhai2023investigating,he2023continual,zheng2024antiforgetting,chen2024coin,zhu2024model. Given the strong capabilities of pre-trained models, training specific layers suffices, and can simultaneously reduce computational demands. zhao2024reconstruct additionally considers an continual learning scenario, Continual Missing Modality Learning~(CMML), where different modalities are emerging throughout the incremental learning stages. All the aforementioned studies collectively indicate that MLLMs still suffer from catastrophic forgetting, which manifests in two ways: along the direction of vertical continuity, a performance decline on pre-trained tasks following fine-tuning for downstream tasks; and along the axis of horizontal continuity, a performance degrade on previously fine-tuned tasks after fine-tuning for new tasks. zheng2024antiforgetting also observes negative forward transfer, where the performance of unseen tasks degrades when learning new tasks, indicating a decline in model generalization capability. While traditional CL methods are applicable, some may not yield optimal results, as evidenced by various experiments~he2023continual,zheng2024antiforgetting. For instance, he2023continual observes a consistent efficacy of replay-based and model expansion strategies across diverse scenarios of continual fine-tuning MLLMs, but regularization-based methods only perform well on models that have been jointly instruction-tuned on multiple tasks. Other works seek to develop ad-hoc solutions for continual learning MLLMs. he2023continual proposes EProj to expand the projection layer in MLLMs for each new task and utilizes task-similarity-informed regularization~(TIR) to enhance performance. zheng2024antiforgetting introduces Fwd-Prompt, a prompt tuning method that projects prompt gradient to both the residual space and the pre-trained subspace to minimize the interference between tasks and reuse pre-trained knowledge respectively, fostering positive forward transfer without relying on previous samples. zhu2024model focuses on the forgetting of the pre-trained MLLMs after fine-tuned on specific tasks and proposes model tailor to compensate the selected subset that are critical for enhancing target task performance. zhao2024reconstruct presents a novel method named Reconstruct before Query~(RebQ), leveraging the multi-modal knowledge from a pre-trained model to reconstruct the absent information for the missing modality. Recently, MoE~(Mixture-of-Experts) framework has gained attention which resembles the architecture-based methods in CL. It provides the model with the ability to learn different intentions from distinct experts, e.g., chen2024coin first introduces MoELoRA to fine-tune LLaVA, effectively mitigate the catastrophic forgetting of MLLMs in CoIN and the results demonstrate the effectiveness.",
      "origin_cites_number": 14
    },
    {
      "section_title": "Evaluation Protocols and Datasets",
      "level": "1",
      "content": "Continual LLMs' Evaluation Protocols.\\quad LAnguage Model Analysis~(LAMA) is an evaluation framework designed to probe the world knowledge embedded in language models~petroni2019language. LAMA converts each world fact into a cloze statement, which is then input into the language models to predict the correct answer. It has been extensively utilized in work on CPT under the temporal shifts~jang2022temporalwiki,jang2022towards. {FUAR~(Forgotten / (Updated + Acquired) Ratio)} is proposed for CPT to address the OP's drawback of not able to accurately reflect the model's behavior. A FUAR value of 1 represents an equal trade-off between the knowledge forgetting and knowledge learning, while a FUAR less than 1 suggests high learning efficacy. In TRACE~wang2023trace, the authors propose a set of ``X-Delta'' metrics for continual instruction tuning, quantifying the forward transfer on specific abilities of LLMs, which is a straightforward extension of FWT. Specifically, the authors construct three sets of evaluation tasks to benchmark the ability of LLMs, including general ability, instruction following, and safety. For more detailed introduction to these evaluation protocols, please refer to app:eval-llm. Datasets.\\quad In this section, we provide a comprehensive review of the datasets available for benchmarking continual LLMs, as illustrated in tab:datasets. We provide information about these datasets' types, what distributional shifts and semantic domains they include, and their sources and applications. We intentionally exclude datasets used for domain-adaptive pre-training LLMs in vertical domains such as legal, medical, and financial, unless they are specifically designed for continual domain-adaptive pre-training. Furthermore, we omit datasets used in general continual fine-tuning, as they have already been extensively studied in existing works~biesialska2020continual,ke2023continual. For details, please refer to app:data. sidewaystable \\centering \\textbf{Summary of the existing benchmarks publicly available for Continual Learning LLMs. In the column of Name, we use the superscript ``$^*$'' to denote the lack of the dataset name and the name shown is that of the original paper. In this table, we deliberately omit the datasets used for domain-adaptive pre-training the vertical LLMs, as their main focus of development is not on continual learning. We also omit the datasets used for general continual fine-tuning, as they are extensively discussed in other existing surveys~biesialska2020continual,ke2023continual. } 1\\linewidth{!}{ tabular{ccccc ccccc } \\toprule[0.15em] Name & Type & Shift & Domain & \\#Stages & Scale & Sources & Applications & Comment \\\\ \\midrule \\midrule $^*$TimeLMs~loureiro2022timelms & CPT & Temporal & Social Media & 8 & \\#Examples: 123.86M & Tweets & loureiro2022timelms & https://github.com/cardiffnlp/timelms{code}\\\\ \\midrule CC-RecentNews~jang2022towards & CPT & Temporal & News & 1 & \\#Tokens: $\\sim$168M & Web & jang2022towards & https://github.com/joeljang/continual-knowledge-learning{code}\\\\ \\midrule TWiki~jang2022temporalwiki & CPT & Temporal & General Knowledge & 5 & \\#Tokens: 4.7B & Wikipedia & jang2022temporalwiki & https://github.com/joeljang/temporalwiki{code} \\\\ \\midrule $^*$DAPT~gururangan2020dont & CPT \\\\ DAP & Content & Multi-Domain & 4 & Size: 160GB & BioMed~\\cite{lo2020s2orc, CS~lo2020s2orc, News~zellers2019defending, Reviews~he2016ups} & \\cite{gururangan2020dont \\\\ qin2023recyclable \\\\ qin2022elle} & https://github.com/allenai/dont-stop-pretraining{code} \\\\ \\midrule $^*$CPT~ke2022continual-train & CPT & Content & Multi-Domain & 4 & \\#Examples: 3.12M & Yelp~\\cite{xu2019bert, S2ORC~lo2020s2orc, AG-News~zhang2015character} & ke2022continual-train & https://github.com/UIC-Liu-Lab/CPT{code} \\\\ \\midrule $^*$DEMix~gururangan2022demix & CPT & Content & Multi-Domain & 8 & \\#Tokens: 73.8B & 1B~\\cite{chelba2014billion, CS~lo2020s2orc, Legal~caselaw2018, Med~lo2020s2orc\\\\ WebText~gokaslan2019OpenWeb, RealNews~zellers2019defending, Reddit~baumgartner2020pushshift, Reviews~ni2019justifying} & gururangan2022demix & https://github.com/kernelmachine/demix{code} \\\\ \\midrule $^*$DAS~ke2022continual-pre & CPT \\\\ DAP & Content & Multi-Domain & 6 & Size: 4.16GB & Yelp~\\cite{xu2019bert, Reviews~ni2019justifying, Papers~lo2020s2orc, PubMed} & ke2022continual-pre & https://github.com/UIC-Liu-Lab/ContinualLM{code} \\\\ \\midrule SuperNI~wang2022supernaturalinstructions & CIT & Content & Mutli-Domain & 16 & \\#Tasks: 1616 \\\\ \\#Examples: $\\sim$5M & GitHub & zhang2023citb,wang2024inscl & https://github.com/allenai/natural-instructions{code} \\\\ \\midrule CITB~zhang2023citb & CIT & Content & Mutli-Domain & 19 & \\#Tasks: 38 & SuperNI~wang2022supernaturalinstructions & zhang2023citb & https://github.com/hyintell/CITB{code} \\\\ \\midrule CoIN~chen2024coin & CIT & Content & Multi-Domain & 8 &\\#Examples: $\\sim$1.14M & RefCOCO~\\cite{kazemzadeh-etal-2014-referitgame,RefCOCO+~mao2016generation,RefCOCOg~mao2016generation \\\\ ImageNet~imagenet_cvpr09, VQAv2~goyal2017making, ScienceQA~lu2022learn \\\\ TextVQA ~singh2019vqa, GQA~hudson2019gqa, VizWiz ~gurari2018vizwiz, OCR-VQA~mishraICDAR19} & chen2024coin & https://github.com/zackschen/CoIN{code} \\\\ \\midrule TRACE~wang2023trace & CIT & Content & Mutli-Domain & 8 & \\#Examples: 56,000 & ScienceQA~\\cite{lu2022learn, FOMC~shah2023trillion, MeetingBank~hu2023meetingbank\\\\ C-STANCE~zhao-etal-2023-c, 20Minuten~kew-etal-2023-20, CodeXGLUE~lu2021codexglue, NumGLUEmishra2022numglue} & wang2023trace & https://github.com/BeyonderXX/TRACE{code} \\\\ \\midrule NATURAL-INSTRUCTION~mishra2021natural & CIT & Content & Mutli-Domain & 6 & \\#Examples: 193k & CosmosQA~\\cite{huang2019cosmos, DROP~dua2019drop, Essential-Terms~khashabi-etal-2017-learning \\\\ MCTACO~zhou2019goingvacationtakeslonger, MultiRC~khashabi-etal-2018-looking, QASC~khot2020qasc \\\\ Quorefdasigi-etal-2019-quoref~, ROPES~lin2019reasoning , Winogrande~sakaguchi2019winogrande} & mishra2021natural & https://github.com/allenai/natural-instructions-v1{code} \\\\ \\midrule IMDB~maas2011learning & CMA & Content &Social Media&1&Size: 217.35 MB& IMDB& zhang2023copf & https://huggingface.co/datasets/stanfordnlp/imdb{code}\\\\ \\midrule HH-RLHF~bai2022training & CMA & Content &General Knowledge&1&Size: 28.1 MB& Human Feedback& zhang2023copf & https://github.com/anthropics/hh-rlhf{code}\\\\ \\midrule Reddit TL;DR~volske2017tl & CMA & Content&Social Media&2&Size: 19.6 GB&Reddit & zhang2023copf,zhangcppo &https://zenodo.org/records/1043504{code}\\\\ \\midrule Common Sense QA~\\cite{lin2024mitigating \\\\ Reading Comprehension~lin2024mitigating\\\\ Translation~lin2024mitigating} & CMA & Content &Multi-Domain&6& \\#Examples: $\\sim$ 41.16M& ARC Easy and Challenge~\\cite{clark2018think, Race~lai2017race, PIQA~bisk2020piqa \\\\ SQuAD~rajpurkar2018know, DROP~dua2019drop \\\\ WMT 2014 French to English~bojar2014findings }& lin2024mitigating & see sources \\\\ \\midrule FEVER~fever & CMR & Content & General Knowledge & 1 & \\#Examples: 420k & Wikipedia & de2021editing, hase2021language & https://fever.ai/resources.html{code} \\\\ \\midrule VitaminC~vitaminC & CMR & Content & General Knowledge & 1 & \\#Examples: 450k & Wikipedia & mitchell2022memory & https://github.com/TalSchuster/VitaminC{code} \\\\ \\midrule zsRE~zsRE & CMR & Content & General Knowledge & 1 & \\#Examples: 120M & Wikireading~Wikireading & hase2021language, meng2022locating, meng2022mass, hase2023does, hartvigsen2023aging, das2024larimar & - \\\\ \\midrule T-rex~T-rex & CMR & Content & General Knowledge & 1 & \\#Examples: 11M & Dbpedia abstracts~Dbpedia & li2022large, dong2022calibrating & https://hadyelsahar.github.io/t-rex/{code} \\\\ \\midrule NQ~nq & CMR & Content & General Knowledge & 1 & \\#Examples: 320k & Google queries, Wikipedia & hartvigsen2023aging & https://ai.google.com/research/NaturalQuestions{code} \\\\ \\midrule CounterFact~meng2022locating & CMR & Content & General Knowledge & 1 & \\#Examples: 22k & zsRE zsRE & meng2022locating, yu2023melo, hu2024wilke, das2024larimar & https://github.com/kmeng01/rome{code} \\\\ \\midrule SCOTUS~scotus & CMR & Temporal & Law & 1 & \\#Examples: 9.2k & Supreme Court Database & hartvigsen2023aging & https://github.com/coastalcph/fairlex{code} \\\\ \\bottomrule[0.15em] tabular } sidewaystable",
      "origin_cites_number": 109
    },
    {
      "section_title": "Discussion",
      "level": "1",
      "content": "",
      "origin_cites_number": 0
    },
    {
      "section_title": "Intriguing Properties Emergent in Continual LLMs",
      "level": "2",
      "content": "Beyond the well-established resilience of pre-trained large language models~(LLMs) against catastrophic forgetting compared to downstream-specific models~ke2021achieve,tao2022can,luo2023investigating,zheng2023learn,mehta2023empirical, there is a notable lack of exploration into other intriguing properties of LLMs when trained continually. In yang2024reawakening, it is observed that when fine-tuned sequentially and cyclically on a series of documents, large models exhibit a phenomenon known as ``anticipatory recovering.'' This refers to the LLMs' ability to recover forgotten information on documents even before encountering them again. This suggests that LLMs may possess the capability of sequential memorization, which could pave the way for research into more complex structured learning environments as model parameters scale up.",
      "origin_cites_number": 2
    },
    {
      "section_title": "Conventional Types of Incremental Learning",
      "level": "2",
      "content": "As mentioned in sec:background-cl-types, three types of incremental learning are prevalent~van2022three. Among them, class-incremental learning~(CIL) has historically attracted significant attention from the community~rebuffi2017icarl,wu2019large. However, in the context of continually pre-training and adapting large language models~(LLMs), we observe a decreased interest in CIL but an increased focus on task-incremental learning~(TIL) and domain-incremental learning~(DIL). Given that language models are inherently designed for content generation and are pre-trained with the pretext generative task of next-word prediction, it is natural to emphasize the patterns of generative tasks and integrate the traditional CIL paradigm into the broader framework of language modeling, discarding the incremental classification head~shao2023class,cao2024generative. However, the declining attention to CIL does not suggest that it is not impactful in the field of continual learning for LLMs. Techniques such as vocabulary expansion~amba2021dynamic,cossu2022continual and learning routing function in the MoE system~chen2023lifelong can be seen as an extension of expanding the classification head in CIL, and previously validated techniques of CIL can be directly applied. The importance of DIL is self-evident, given the shared task definition and input-output format in continual pre-training~(CPT) and domain-adaptive pre-training~(DAP). On the other hand, TIL attracts significant interest as it plays a crucial role in instruction tuning, where instructions can be seen as natural-language-encoded task indices~scialom2022fine,huang2024mitigating,mok2023large,he2024dont,yin2022contintin,wang2023orthogonal,zhao2024sapt,wang2024inscl. It is worth noting that the boundary between TIL and DIL becomes somewhat blurred in continual instruction tuning. Language models demonstrate the capability to infer domain information for unseen instructions, suggesting a convergence of TIL and DIL in certain contexts.",
      "origin_cites_number": 6
    },
    {
      "section_title": "Roles of Memory in Continual LLMs",
      "level": "2",
      "content": "Previous continual learning research, drawing inspiration from human learning patterns, primarily emphasizes the storage efficiency of past data. However, this focus may no longer hold true in the context of continual LLMs. In the direction of relaxing memory constraints, institutions with access to training data may opt to retain full access without restricting memory size, given that the cost of memory storage is more than affordable. In such scenarios, as highlighted in verwimp2024continual, the challenge shifts from storage efficiency to computational efficiency. To achieve continual learning goals, models must efficiently adapt to new data (efficient adaptation) and select key experiences for replay (efficient replay)~xie2023efficient,jin2024model. Therefore, it is essential to reassess the existing memory constraint and prioritize optimizing computational efficiency for continual learning of LLMs by restricting the number of updates and FLOPs~prabhu2023computationally,wang2022sparcl. On the other end of the spectrum, studies with tightened memory constraints remain vital in modern continual learning of LLMs. As shown in fig:overview, upstream suppliers of LLMs typically do not provide training data with the released model weights. Consequently, consumers must adapt these models to downstream data without access to the actual replay data. Various rehearsal-free continual strategies are applied in this scenario, such as collecting data examples from alternate sources~rozière2024code,colombo2024saullm7b,wu2023pmc,Azerbayev2023LLEMMA, leveraging the generative capabilities of LLMs to produce pseudo-examples for replay~qin2021lfpt5, and implementing regularization techniques in the parameter space~ke2022continual-pre,rongali2021continual. Continual learning under the strict memory constraint is also driven by data privacy concerns, where preserving data on the server side is prohibited. In these scenarios, researchers must rely on online continual learning methods~mai2022online,prabhu2023online, where data examples are only utilized for training as they arrive in a stream, and numerous efforts are already underway to develop LLMs capable of operating under these constraints~bornschein2024transformers.",
      "origin_cites_number": 8
    },
    {
      "section_title": "Prospective Directions",
      "level": "2",
      "content": "Theories of Continual LLMs.\\quad It is widely recognized that the continual learning community tends to prioritize empirical research over theoretical exploration. Nevertheless, there are efforts to establish theoretical foundations for CL. In wang2024comprehensive, the authors utilize second-order Taylor expansions around optimal parameters to derive an inter-task generalization error bound based on the maximum eigenvalue and $l_2$-norm of parameter differences. Another line of approaches leverages task/domain discrepancies to construct a multi-task generalization bound. For instance, Unified Domain Incremental Learning~(UDIL) in shi2024unified proposes upper bounds for intra-domain and cross-domain distillation losses, unifying various replay-based DIL techniques under a single adaptive generalization bound. However, applying these existing theories directly to continual LLMs can be imprudent, given their pre-trained, large-scale nature. Consequently, there is a notable gap in research focusing on continually learning LLMs with robust theoretical guarantees and understanding the forgetting behaviors of LLMs from a theoretical perspective. Efficient Replay for Knowledge Retention for Continual LLMs.\\quad While the storage budget can theoretically be infinite (sec:discussion-mem), replaying past experiences without specific design can lead to inefficient updates in current domain learning, resulting in slow convergence. Beyond sparse replay solutions that control data mixture ratios lin2023geogalactica,rozière2024code,Yang2023PLLaMa, there is ongoing exploration of efficient replay for continual LLMs. For example, KPIG~he2024dont enhances replay efficiency by calculating Key-Part Information Gain~(KPIG) on masked segments, enabling the dynamic selection of replay data. jin2024model introduces a forgetting forecasting mechanism based on output changes during adaptation, later used for selective replay in continual model refinement~(CMR). More sophisticated and accurate data mixing strategies and efficient replay sample selection mechanisms are needed and hence we mark it as a significant research focus in the future. Continual LLMs with Controllable Memory.\\quad The long-term memory inherent in the whole set of parameters of LLMs often lacks interpretability and explicit manipulability, which is crucial in certain application areas such as machine unlearning~bourtoule2020machine, where the continually pre-trained models need to constantly roll back to a previous version predating the inclusion of the revoked data and retrain the model from that point onward. This example illustrates the benefits of equipping LLMs with an external, controllable memory. As part of continual model refinement~(CMR), memory systems for continual learning have been explored in several studies. Larimar~das2024larimar suggests integrating the Kanerva Machine~wu2018kanerva as an episodic memory for multi-fact model editing. This memory system supports basic operations like writing, reading, and generating, as well as advanced operations such as sequential writing and forgetting. It enables one-shot knowledge updates without costly retraining or fine-tuning. Other memory systems like Hopfield Networks~ramsauer2021hopfield hold promise for future investigation as well. Continual LLMs with Custom Preferences.\\quad In service-oriented contexts, users often require different trade-offs between domain expertise, ethics, values, or tones of expression. Efficiently building customized LLMs for individual users and offering flexible adjustment options is a challenging task. Early attempts in this direction include Imprecise Bayesian Continual Learning~(IBCL), which, under certain assumptions, guarantees the generation of Pareto-optimal models based on user preferences by combining two model posteriors in the parameter space~lu2023ibcl. While empirical validation is limited in scale, this approach paves the way for future research in this area.",
      "origin_cites_number": 10
    },
    {
      "section_title": "Conclusion",
      "level": "1",
      "content": "In this work, we offer a comprehensive survey on continual LLMs, summarizing recent advancements in their training and deployment from a continual learning standpoint. We categorize the problems and tasks based on their positions within our proposed broader framework of modern stratified continual learning of LLMs. While there is a widespread and growing interest in this area across the community, we also note several missing cornerstones, including algorithmic diversity and a fundamental understanding of large models' behaviors such as knowledge forgetting, transfer, and acquisition. With a holistic yet detailed approach, we aim for this survey to inspire more practitioners to explore continual learning techniques, ultimately contributing to the development of robust and self-evolving AI systems. { abbrv",
      "origin_cites_number": 0
    }
  ],
  "literature_review_id": 269362836,
  "meta_info": {
    "cite_counts": 304,
    "Conference_journal_name": "ACM Computing Surveys",
    "influentialcitationcount": 10,
    "Author_info": {
      "Publicationsh": 10,
      "h_index": 5,
      "Citations": 241,
      "Highly Influential Citations": 0
    },
    "all_cites_title": [
      "Gpt-4 technical report",
      "Hippocrates: An open-source framework for advancing large language models in healthcare",
      "Structured code representations enable data-efficient adaptation of code language models",
      "Memory aware synapses: Learning what (not) to forget",
      "Dynamic language models for continuously evolving content",
      "Palm 2 technical report",
      "Finbert: Financial sentiment analysis with pre-trained language models",
      "Is it worth the (environmental) cost? limited evidence for temporal adaptation via continuous training",
      "Llemma: An open language model for mathematics",
      "Enhancing continual learning with global prototypes: Counteracting negative representation drift",
      "Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
      "The pushshift reddit dataset",
      "A theory of learning from different domains",
      "Oceangpt: A large language model for ocean science tasks",
      "Pythia: A suite for analyzing large language models across training and scaling",
      "Continual lifelong learning in natural language processing: A survey",
      "Piqa: Reasoning about physical commonsense in natural language",
      "Findings of the 2014 workshop on statistical machine translation",
      "Transformers for supervised online continual learning",
      "L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie, and N. Papernot. Machine unlearning, 2020.",
      "Language models are few-shot learners",
      "Dbpedia abstracts: A large-scale, open, multilingual nlp training corpus",
      "Dark experience for general continual learning: a strong, simple baseline",
      "Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
      "Generative multi-modal models are good class incremental learners",
      "Caselaw Access Project. Caselaw access project, 2018.",
      "Fairlex: A multilingual benchmark for evaluating fairness in legal text processing",
      "Efficient lifelong learning with a-gem",
      "On tiny episodic memories in continual learning",
      "One billion word benchmark for measuring progress in statistical language modeling",
      "Coin: A benchmark of continual instruction tuning for multimodel large language model",
      "Huatuogpt-ii, one-stage training for medical adaption of llms",
      "Recall and learn: Fine-tuning deep pretrained language models with less forgetting",
      "Lifelong language pretraining with distribution-specialized experts",
      "Take the bull by the horns: Hard sample-reweighted continual training improves llm generalization",
      "Parameterizing context: Unleashing the power of parameter-efficient fine-tuning and in-context tuning for continual table semantic parsing",
      "Lifelong machine learning",
      "Adapting large language models via reading comprehension",
      "Palm: Scaling language modeling with pathways",
      "Think you have solved question answering? try arc, the ai2 reasoning challenge",
      "Saullm-7b: A pioneering large language model for law",
      "Redpajama: an open dataset for training large language models",
      "Continual pre-training mitigates forgetting in language and vision",
      "Large language models with episodic memory control",
      "Quoref: A reading comprehension dataset with questions requiring coreferential reasoning",
      "Editing factual knowledge in language models",
      "K2: A foundation language model for geoscience knowledge understanding and utilization",
      "ImageNet: A Large-Scale Hierarchical Image Database",
      "Qlora: Efficient finetuning of quantized llms",
      "Pre-training of deep bidirectional transformers for language understanding",
      "Time-aware language models as temporal knowledge bases",
      "Calibrating factual knowledge in pretrained language models",
      "An image is worth 16x16 words: Transformers for image recognition at scale",
      "Sailor: Open language models for south-east asia",
      "Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "Uncertainty-guided continual learning with bayesian neural networks",
      "Adversarial continual learning",
      "T-rex: A large scale alignment of natural language with knowledge base triples",
      "Continual pre-training for cross-lingual llm adaptation: Enhancing japanese language capabilities",
      "Domain-adversarial training of neural networks",
      "Tic-clip: Continual training of clip models",
      "Continual learning under language shift",
      "Openwebtext corpus",
      "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "Domain-specific language model pretraining for biomedical natural language processing",
      "Deepseek-coder: When the large language model meets programming -the rise of code intelligence",
      "Continuous training and fine-tuning for domain-specific language models in medical question answering",
      "Continual pre-training of large language models: How to (re)warm your model?",
      "Vizwiz grand challenge: Answering visual questions from blind people",
      "DEMix layers: Disentangling domains for modular language modeling",
      "Don't stop pretraining: Adapt language models to domains and tasks",
      "ECONET: Effective continual pretraining of language models for event temporal reasoning",
      "Aging with grace: Lifelong model editing with discrete key-value adaptors",
      "Does localization inform editing? surprising differences in causality-based localization vs",
      "Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs",
      "Continual instruction tuning for large multimodal models",
      "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
      "Analyzing the forgetting problem in pretrain-finetuning of open-domain dialogue response models",
      "Don't half-listen: Capturing key-part information in continual instruction tuning",
      "Aligning ai with shared human values",
      "Measuring massive multitask language understanding",
      "Wikireading: A novel large-scale language understanding task over wikipedia",
      "Training compute-optimal large language models",
      "C. Hu, P. Cao, Y. Chen, K. Liu, and J. Zhao. Wilke: Wise-layer knowledge editor for lifelong knowledge editing, 2024.",
      "Lora: Low-rank adaptation of large language models",
      "LoRA: Low-rank adaptation of large language models",
      "Meetingbank: A benchmark dataset for meeting summarization",
      "Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal",
      "Cosmos qa: Machine reading comprehension with contextual commonsense reasoning",
      "Lawyer llama technical report",
      "Transformer-patcher: One mistake worth one neuron",
      "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
      "Simple and scalable strategies to continually pre-train large language models",
      "Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models",
      "Towards continual knowledge learning of language models",
      "J. Ji, T. Qiu, B. Chen, B. Zhang, H. Lou, K. Wang, Y. Duan, Z. He, J. Zhou, Z. Zhang, F. Zeng, K. Y. Ng, J. Dai, X. Pan, A. O'Gara, Y. Lei, H. Xu, B. Tse, J. Fu, S. McAleer, Y. Yang, Y. Wang, S.-C. Zhu, Y. Guo, and W. Gao. Ai alignment: A comprehensive survey, 2024.",
      "Instruction-tuned language models are better knowledge learners",
      "What will my model forget? forecasting forgotten examples in language model refinement",
      "Lifelong pretraining: Continually adapting language models to emerging corpora",
      "Principles of neural science",
      "Scaling laws for neural language models",
      "ReferItGame: Referring to objects in photographs of natural scenes",
      "Continual training of language models for few-shot learning",
      "Continual learning of natural language processing tasks: A survey",
      "Achieving forgetting prevention and knowledge transfer in continual learning",
      "Continual pre-training of language models",
      "20 minuten: A multi-task news summarisation dataset for German",
      "Looking beyond the surface: A challenge set for reading comprehension over multiple sentences",
      "Learning what is essential in questions",
      "Qasc: A dataset for question answering via sentence composition",
      "A theoretical study on solving continual learning",
      "Overcoming catastrophic forgetting in neural networks",
      "Natural questions: a benchmark for question answering research",
      "Race: Large-scale reading comprehension dataset from examinations",
      "Mind the gap: Assessing temporal generalization in neural language models",
      "Zero-shot relation extraction via reading comprehension",
      "Examining forgetting in continual pre-training of aligned large language models",
      "Large language models with controllable working memory",
      "Blade: Enhancing black-box large language models with small domain-specific models",
      "Cfgpt: Chinese financial assistant with large language model",
      "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "CONTINUAL MODEL EVOLVEMENT WITH INNER-PRODUCT RESTRICTION",
      "Learning without forgetting",
      "On continual model refinement in out-of-distribution data streams",
      "Rouge: A package for automatic evaluation of summaries",
      "Reasoning over paragraph effects in situations",
      "Mitigating the alignment tax of rlhf",
      "A scientific large language model in geoscience",
      "Rho-1: Not all tokens are what you need",
      "Visual instruction tuning",
      "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
      "A robustly optimized bert pretraining approach",
      "S2ORC: The semantic scholar open research corpus",
      "Rehearsal-free continual learning over small non-i.i.d. batches",
      "Gradient episodic memory for continual learning",
      "TimeLMs: Diachronic language models from Twitter",
      "Bbt-fin: Comprehensive construction of chinese financial domain pre-trained language model, corpus and benchmark",
      "Ibcl: Zero-shot model generation for task trade-offs in continual learning",
      "Learn to explain: Multimodal reasoning via thought chains for science question answering",
      "Codexglue: A machine learning benchmark dataset for code understanding and generation",
      "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
      "Biogpt: generative pre-trained transformer for biomedical text generation and mining",
      "Investigating forgetting in pre-trained representations through continual learning",
      "An empirical study of catastrophic forgetting in large language models during continual fine-tuning",
      "Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine",
      "Wizardcoder: Empowering code large language models with evol-instruct",
      "Ecomgpt-ct: Continual pre-training of e-commerce large language models with semi-structured data",
      "Learning word vectors for sentiment analysis",
      "Online continual learning in image classification: An empirical survey",
      "Generation and comprehension of unambiguous object descriptions",
      "A survey on knowledge editing of neural networks",
      "Towards continual task learning in artificial neural networks: current approaches and insights from neuroscience",
      "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory",
      "Catastrophic interference in connectionist networks: The sequential learning problem",
      "An empirical investigation of the role of pre-training in lifelong learning",
      "Locating and editing factual associations in gpt",
      "Mass-editing memory in a transformer",
      "Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint",
      "Ocr-vqa: Visual question answering by reading text in images",
      "Natural instructions: Benchmarking generalization to new tasks from natural language instructions",
      "Numglue: A suite of fundamental yet challenging mathematical reasoning tasks",
      "Fast model editing at scale",
      "Memory-based model editing at scale",
      "Large-scale lifelong learning of in-context instructions and how to tackle it",
      "Github copilot ai pair programmer: Asset or liability",
      "Aurora-m: The first open source multilingual language model red-teamed according to the us executive order",
      "Towards specialized foundation models in astronomy",
      "Justifying recommendations using distantly-labeled reviews and fine-grained aspects",
      "Revisiting catastrophic forgetting in class incremental learning",
      "Continual vision-language representation learning with off-diagonal information",
      "Codegen: An open large language model for code with multi-turn program synthesis",
      "Introducing chatgpt",
      "Training language models to follow instructions with human feedback",
      "Brain imaging of language plasticity in adopted adults: Can a second language replace the first?",
      "Bleu: a method for automatic evaluation of machine translation",
      "Ircoder: Intermediate representations make language models robust multilingual code generators",
      "Theoretical foundations of multi-task lifelong learning",
      "Language models as knowledge bases",
      "Computationally budgeted continual learning: What does matter?",
      "Online continual learning without the storage constraint",
      "Lfpt5: A unified framework for lifelong few-shot language learning based on prompt tuning of t5",
      "Recyclable tuning for continual pre-training",
      "ELLE: Efficient lifelong pre-training for emerging data",
      "Learning transferable visual models from natural language supervision",
      "Language models are unsupervised multitask learners",
      "Direct preference optimization: Your language model is secretly a reward model",
      "Direct preference optimization: Your language model is secretly a reward model",
      "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "Know what you don't know: Unanswerable questions for squad",
      "Model zoo: A growing",
      "H. Ramsauer, B. Schäfl, J. Lehner, P. Seidl, M. Widrich, T. Adler, L. Gruber, M. Holzleitner, M. Pavlović, G. K. Sandve, V. Greiff, D. Kreil, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. Hopfield networks is all you need, 2021.",
      "icarl: Incremental classifier and representation learning",
      "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "Learning to learn without forgetting by maximizing transfer and minimizing interference",
      "Online structured laplace approximations for overcoming catastrophic forgetting",
      "Continual domain-tuning for pretrained language models",
      "Time masking for temporal language models",
      "Code llama: Open foundation models for code",
      "Llm-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions",
      "Progressive neural networks",
      "Winogrande: An adversarial winograd schema challenge at scale",
      "Multitask prompted training enables zero-shot task generalization",
      "Error sensitivity modulation based experience replay: Mitigating abrupt representation drift in continual learning",
      "Proximal policy optimization algorithms",
      "Get your vitamin c! robust fact verification with contrastive evidence",
      "Progress & compress: A scalable framework for continual learning",
      "Fine-tuned language models are continual learners",
      "Trillion dollar words: A new financial dataset, task & market analysis",
      "Class-incremental learning based on label generation",
      "Outrageously large neural networks: The sparsely-gated mixture-ofexperts layer",
      "Tag-llm: Repurposing general-purpose llms for specialized domains",
      "A unified approach to domain incremental learning with memory: Theory and algorithm",
      "Towards vqa models that can read",
      "Editable neural networks",
      "L. Soldaini, R. Kinney, A. Bhagia, D. Schwenk, D. Atkinson, R. Authur, B. Bogin, K. Chandu, J. Dumas, Y. Elazar, V. Hofmann, A. H. Jha, S. Kumar, L. Lucy, X. Lyu, N. Lambert, I. Magnusson, J. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, A. Ravichander, K. Richardson, Z. Shen, E. Strubell, N. Subramani, O. Tafjord, P. Walsh, L. Zettlemoyer, N. A. Smith, H. Hajishirzi, I. Beltagy, D. Groeneveld, J. Dodge, and K. Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research, 2024.",
      "Conpet: Continual parameter-efficient tuning for large language models",
      "Code needs comments: Enhancing code llms with comment augmentation",
      "Efficient continue training of temporal language model with structural information",
      "A survey of neural code intelligence: Paradigms, advances and beyond",
      "Ernie 2.0: A continual pre-training framework for language understanding",
      "Pretraining and updating language-and domain-specific large language model: A case study in japanese business domain",
      "Can bert refrain from forgetting on sequential tasks? a probing study",
      "Deepseek llm: Scaling open-source language models with longtermism",
      "a family of highly capable multimodal models",
      "Starcoder: may the source be with you!",
      "Starcoder 2 and the stack v2: The next generation",
      "Fever: a large-scale dataset for fact extraction and verification",
      "Towards ai synthesizing interdisciplinary research on climate change",
      "Open and efficient foundation language models",
      "Llama 2: Open foundation and fine-tuned chat models",
      "Three types of incremental learning",
      "E. Verwimp, R. Aljundi, S. Ben-David, M. Bethge, A. Cossu, A. Gepperth, T. L. Hayes, E. Hüllermeier, C. Kanan, D. Kudithipudi, C. H. Lampert, M. Mundt, R. Pascanu, A. Popescu, A. S. Tolias, J. van de Weijer, B. Liu, V. Lomonaco, T. Tuytelaars, and G. M. van de Ven. Continual learning: Applications and the road forward, 2024.",
      "Tl; dr: Mining reddit to learn automatic summarization",
      "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
      "Coscl: Cooperation of small continual learners is stronger than a big one",
      "A comprehensive survey of continual learning: Theory, method and application",
      "Wise: Rethinking the knowledge memory for lifelong model editing of large language models",
      "Infusing Knowledge into Pre-Trained Models with Adapters",
      "Orthogonal subspace learning for language model continual learning",
      "Trace: A comprehensive benchmark for continual learning in large language models",
      "Codet5+: Open code large language models for code understanding and generation",
      "Inscl: A data-efficient continual learning paradigm for fine-tuning large language models with instructions",
      "Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, E. Pathak, G. Karamanolakis, H. G. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia, K. Doshi, M. Patel, K. K. Pal, M. Moradshahi, M. Parmar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma, R. S. Puri, R. Karia, S. K. Sampat, S. Doshi, S. Mishra, S. Reddy, S. Patro, T. Dixit, X. Shen, C. Baral, Y. Choi, N. A. Smith, H. Hajishirzi, and D. Khashabi. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks, 2022.",
      "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
      "Codeclm: Aligning language models with tailored synthetic data",
      "Sparcl: Sparse continual learning on the edge",
      "Dualprompt: Complementary prompting for rehearsal-free continual learning",
      "Learning to prompt for continual learning",
      "Finetuned language models are zero-shot learners",
      "Finetuned language models are zero-shot learners",
      "Emergent abilities of large language models",
      "Chain-of-thought prompting elicits reasoning in large language models",
      "On the usage of continual learning for out-of-distribution generalization in pre-trained language models of code",
      "Overcoming catastrophic forgetting in massively multilingual continual learning",
      "Continual learning with low rank adaptation",
      "Llama pro: Progressive llama with block expansion",
      "Pmc-llama: Towards building open-source language models for medicine",
      "Bloomberggpt: A large language model for finance",
      "Pretrained language model in continual learning: A comparative study",
      "Continual learning for large language models: A survey",
      "Large scale incremental learning",
      "The kanerva machine: A generative distributed memory",
      "Quert: Continual pre-training of language model for query understanding in travel domain search",
      "Me llama: Foundation large language models for medical applications",
      "PIXIU: A large language model, instruction data and evaluation benchmark for finance",
      "Data selection for language models via importance resampling",
      "Efficient continual pre-training for building domain specific large language models",
      "Wizardlm: Empowering large language models to follow complex instructions",
      "Bert post-training for review reading comprehension and aspect-based sentiment analysis",
      "Weaverbird: Empowering financial decision-making with large language model, knowledge base, and search engine",
      "Af adapter: Continual pretraining for building chinese biomedical language model",
      "Moral: Moe augmented lora for llms' lifelong learning",
      "Pllama: An open-source large language model for plant science",
      "Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training",
      "Recent advances of foundation language models-based continual learning: A survey",
      "Tree of thoughts: Deliberate problem solving with large language models",
      "Investigating continual pretraining in large language models: Insights and implications",
      "ConTinTin: Continual learning from task instructions",
      "Enhancing model editing with neuron-indexed dynamic lora",
      "Circle: continual repair across programming languages",
      "Mammoth: Building math generalist models through hybrid instruction tuning",
      "Defending against neural fake news",
      "Investigating the catastrophic forgetting in multimodal large language models",
      "Sciglm: Training scientific language models with self-reflective instruction annotation and tuning",
      "Copf: Continual learning human preference through optimal policy fitting",
      "Cppo: Continual learning for reinforcement learning with human feedback",
      "Instruction tuning for large language models: A survey",
      "Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters",
      "Character-level convolutional networks for text classification",
      "Continual sequence generation with adaptive compositional modules",
      "CITB: A benchmark for continual instruction tuning",
      "C-STANCE: A large dataset for Chinese zero-shot stance detection",
      "Large language model can continue evolving from mistakes",
      "Memory-efficient class-incremental learning for image classification",
      "Reconstruct before query: Continual missing modality learning with decomposed prompt collaboration",
      "Sapt: A shared attention framework for parameter-efficient continual learning of large language models",
      "Beyond anti-forgetting: Multimodal continual instruction tuning with positive forward transfer",
      "Learn or recall? revisiting incremental learning with pre-trained language models",
      "Preventing zero-shot transfer degradation in continual learning of vision-language models",
      "Marinegpt: Unlocking secrets of ocean to the public",
      "going on a vacation\" takes longer than \"going for a walk",
      "Pre-training text-to-text transformers for concept-centric common sense",
      "Model tailor: Mitigating catastrophic forgetting in multi-modal large language models"
    ]
  }
}