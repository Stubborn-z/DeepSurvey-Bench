{
  "survey": "Continual learning in large language models (LLMs) represents a pivotal advancement in artificial intelligence, enabling models to adapt dynamically to non-stationary data distributions while retaining previously acquired knowledge. This survey comprehensively explores methodologies such as incremental learning, transfer learning, and model adaptation. Incremental learning techniques, like Continual PostTraining, allow LLMs to integrate new domain-specific knowledge without catastrophic forgetting. Transfer learning enhances model efficiency by leveraging knowledge from related tasks, facilitating learning in data-scarce scenarios. Model adaptation strategies ensure robust performance across diverse tasks, exemplified by domain-specific applications in fields like E-commerce and biomedical NLP. Challenges such as catastrophic forgetting, distribution shifts, and resource constraints underscore the need for efficient continual learning strategies. The survey also highlights frameworks and taxonomies that structure methodologies for dynamic adaptation, alongside evaluation metrics crucial for assessing model performance. Future directions emphasize expanding benchmarks, optimizing learning strategies, enhancing knowledge transfer, and improving evaluation metrics. These insights underline the transformative impact of continual learning on LLMs, advancing their capabilities and applicability in real-world scenarios.\n\nIntroduction Importance of Continual Learning in AI Continual learning represents a significant advancement in artificial intelligence, particularly for large language models (LLMs), enabling them to adapt to non-stationary data distributions without extensive retraining [1]. This adaptability enhances AI models' efficiency, allowing them to update their knowledge in response to evolving information [2]. By integrating continual learning techniques, LLMs can assimilate new data while retaining previously acquired knowledge, thus preventing performance degradation over time [3]. Such adaptability is crucial for maintaining robust performance across diverse tasks and domains, as demonstrated in specialized applications like financial sentiment analysis [4]. The importance of continual learning extends to domain-specific pretraining, particularly in biomedical NLP, where models must adapt to emerging data [5]. Efficiently fine-tuning LLMs involves optimizing learning strategies to minimize memory and computational resource demands, thereby enhancing model efficiency [6]. Current pre-trained language models often face challenges in adapting to continuously emerging data from various sources, highlighting the necessity of continual learning for improved responsiveness and efficiency [7]. In computationally constrained environments, continual learning is vital for advancing AI capabilities, enabling models to handle out-of-distribution (OOD) data streams without catastrophic forgetting [8]. The AI2 Reasoning Challenge (ARC) exemplifies the need for benchmarks that require advanced knowledge and reasoning capabilities, underscoring continual learning's role in enhancing model adaptability and efficiency [9]. Furthermore, LLMs' ability to perform zero-shot generalization is a key aspect of continual learning, contributing to the advancement of AI capabilities [10]. By optimizing learning strategies and model architectures, continual learning ensures that language models remain responsive to evolving data landscapes, enhancing their effectiveness in real-world applications [11]. Scope of the Survey This survey delineates the boundaries of continual learning methodologies within large language models (LLMs), focusing on incremental learning, transfer learning, and model adaptation. Incremental learning is examined through techniques such as Continual PostTraining (CPT), which allows LLMs to assimilate new domain-specific knowledge while preserving previously learned information [12]. This approach is crucial for models operating in environments characterized by sequential task presentations, effectively addressing catastrophic forgetting [11]. Moreover, the necessity for LLMs to adapt to new data without complete retraining is emphasized, facilitating more efficient knowledge acquisition [7]. Transfer learning is highlighted as a means to leverage knowledge from related tasks, promoting efficient learning in scenarios with limited labeled data. The survey explores techniques that significantly reduce memory usage for fine-tuning LLMs, enhancing accessibility for researchers and practitioners [6]. The integration of multimodal models in resource-constrained environments is also discussed, emphasizing the need for parameter-efficient architectures. Model adaptation strategies are investigated, focusing on enhancing domain-specific applications, such as adapting LLMs for E-commerce tasks by injecting domain knowledge during continual training and supervised fine-tuning, and improving code generation through structured representations like parse trees. These approaches address challenges such as domain knowledge deficiency and hallucination in LLMs, leveraging structured code representations for data-efficient adaptation, resulting in significant performance gains in specialized tasks, even with limited training examples [13,14,15]. Techniques like continual pre-training and vocabulary expansion are essential for adapting models to specific languages. Furthermore, the survey addresses the underexplored integration of NLP models in museum education, highlighting the potential for enhancing interactive learning experiences. Structure of the Survey This survey is systematically organized to provide a comprehensive exploration of continual learning in large language models (LLMs), structured across several key sections. The introduction highlights the importance and scope of continual learning, focusing on incremental learning, transfer learning, and model adaptation. Following this, the background and definitions section elucidates core concepts essential for understanding continual learning, including incremental learning techniques, transfer learning approaches, and model adaptation strategies. The methodologies section delves into various frameworks and taxonomies developed to enable continual learning in LLMs, discussing evaluation metrics and benchmarking methods crucial for assessing these models. The challenges section identifies significant hurdles such as catastrophic forgetting, distribution shifts, and resource constraints that impede the effective implementation of continual learning. Subsequent sections examine model adaptation strategies, presenting techniques to mitigate catastrophic forgetting, enhance knowledge transfer, and adapt dynamically to domain-specific needs. This is followed by a discussion on applications and case studies that showcase the practical impact of continual learning across various domains, including programming, natural language processing, and multimodal tasks. The survey concludes with a forward-looking perspective on future directions, emphasizing the need for expanding benchmarks, optimizing learning strategies, and addressing theoretical challenges. Each section is meticulously crafted to build upon the previous, providing a coherent narrative that advances the understanding of continual learning in LLMs.The following sections are organized as shown in . Background and Definitions Continual Learning Continual learning is crucial for large language models (LLMs) as it allows adaptation to new data distributions while preserving existing knowledge [16]. This capability is vital for tasks such as coreference resolution in reading comprehension, where models must assimilate new information without losing prior competencies [17]. It ensures that model outputs meet user expectations, maintaining previously learned skills during adaptations [11]. In domain-specific applications like biomedical text processing, continual learning is key to keeping models relevant [5]. Techniques like QLoRA enable efficient fine-tuning, enhancing LLMs' ability to manage computational resources while integrating streaming data [6,7]. Continual learning also addresses prediction errors in out-of-distribution data streams, minimizing catastrophic forgetting [8]. This is essential for robust performance in specialized fields, such as language understanding and generation within the Chinese financial sector [18]. By fostering dynamic adaptation and preventing knowledge erosion, continual learning bolsters the robustness and efficacy of LLMs in evolving data landscapes [19]. Incremental Learning Techniques Incremental learning is vital for LLMs, facilitating the integration of new data while preserving existing knowledge and combating catastrophic forgetting [20]. Techniques like Lifelong Pretraining (LPT) ensure sustained performance across sequential tasks in non-stationary environments by updating pre-trained language models (PTLMs) incrementally [21]. Representational interference, where new task training can overwrite previous knowledge, poses a challenge [22]. Hybrid frameworks like Cooperation of Small Continual Learners (CoSCL) employ fixed sub-networks to learn tasks in parallel, enhancing retention [23]. These frameworks often combine architecture growth with experience replay to reinforce past experiences [24]. Incremental Classifier and Representation Learning (iCaRL) exemplifies incremental learning by progressively adding new classes while maintaining performance on prior ones [25]. Instruction tuning, involving fine-tuning on diverse tasks described through natural language, demonstrates LLMs' adaptability in incremental learning [26]. Memory replay mechanisms, critical for retaining prior knowledge, face challenges in effectiveness [27]. Addressing inefficiencies in lifelong learning methods is essential for enhancing learning efficiency [28]. The computational cost of exhaustive pre-training limits new information integration [7]. Benchmarks addressing boundary-agnostic shifts and error-centric streams are crucial for evaluating incremental learning robustness [8]. Incremental learning techniques enable LLMs to adapt to new information dynamically, ensuring efficacy in real-world scenarios [20]. Transfer Learning Approaches Transfer learning enhances LLMs' adaptability and performance by facilitating knowledge transfer across domains and tasks [29]. It is particularly beneficial in scenarios with limited labeled data, allowing models to generalize from existing knowledge to novel tasks [30]. Transfer learning enables synthesis of programs from multi-turn prompts, as shown by the DeepSeek benchmark, which evaluates open-source code models' accuracy in code generation and infilling [31]. The ScienceQA benchmark illustrates transfer learning's role in improving efficiency in question-answering tasks by leveraging multimodal information from diverse science topics [29]. In programming, the Llemma model combines pretraining on a curated mathematics dataset, demonstrating transfer learning principles [32]. The GitHub Copilot dataset exemplifies transfer learning's application in algorithmic tasks, boosting LLM performance in programming scenarios [31]. Multitask learning enhances zero-shot performance by leveraging knowledge from related tasks [33]. Parameter-efficient fine-tuning methods like (IA)$^3$ scale activations using learned vectors, showcasing transfer learning's efficiency [34]. TextVQA introduces a dataset focused on text-related questions in images, improving learning efficiency through task-related knowledge [35]. Adaptive systems capable of learning from new data streams without exhaustive retraining are necessary, as prior methods rely on fixed data representations [5]. Transfer learning enhances adaptability by addressing model size and training data relationships, improving learning efficiency [30]. Metrics in the BBT-CFLEB framework evaluate model responses in financial language tasks, highlighting transfer learning's impact on performance [18]. Transfer learning enables LLMs to achieve improved performance across diverse tasks and domains, advancing AI by meeting evolving challenges and opportunities [29]. Model Adaptation Strategies Model adaptation strategies are crucial for ensuring LLMs maintain robust performance across diverse tasks and rapidly evolving environments. Low-Rank Adaptation (LoRA) exemplifies a method that adapts models to new tasks by injecting trainable rank decomposition matrices into each layer of a Transformer architecture, preserving task performance while accommodating new information [36]. This technique is particularly valuable for integrating domain-specific knowledge without extensive retraining. FinBERT, tailored for financial NLP tasks, illustrates specialized adaptation strategies that enhance model efficacy within the financial domain [37]. The FOMC communications dataset is pertinent for analyzing monetary policy effects, showcasing strategies for adapting models to new environments [38]. Temporal knowledge adaptation is addressed through benchmarks that jointly model text with timestamps, ensuring LLMs remain responsive to temporal shifts [39]. The ETHICS dataset provides a comprehensive evaluation of moral concepts, enhancing ethical decision-making in AI systems [40]. Adaptation strategies include breaking down complex tasks into manageable segments, as demonstrated by the MeetingBank's divide-and-conquer approach [41]. The Lifelong-MoE architecture represents an extensible Mixture-of-Experts framework, dynamically increasing model capacity through expert addition and regularized pretraining to mitigate catastrophic forgetting [16]. Addressing performance degeneration caused by Spatial Disorder (SD), including Intra-modal Rotation and Inter-modal Deviation, highlights the need for robust adaptation strategies [42]. By employing these strategies, LLMs can dynamically integrate new information while preserving performance across tasks, advancing their applicability in real-world scenarios. In recent years, the field of machine learning has seen significant advancements driven by the need for models that can adapt and learn continuously. As depicted in , this figure illustrates the methodologies for continual learning, highlighting various frameworks and taxonomies, as well as evaluation and benchmarking strategies. The first category focuses on efficient lifelong learning, encompassing domain-specific frameworks and the handling of dynamic data through benchmarks such as ELLE, BBT-CFLEB, and CMR. In contrast, the second category emphasizes the use of metrics for performance assessment, including accuracy and F1-score, alongside methods for knowledge retention and adaptation, such as QLoRA and MER. This comprehensive overview not only elucidates the complexities of continual learning but also serves as a reference point for evaluating the effectiveness of different approaches within the field. Methodologies for Continual Learning Frameworks and Taxonomies Frameworks and taxonomies are crucial for organizing continual learning methodologies in large language models (LLMs), enabling them to adapt dynamically to evolving data environments. The ELLE method exemplifies efficient lifelong pre-training, integrating new data while minimizing computational expenses typical of traditional approaches [7]. Such frameworks are vital in scenarios with continuous data influx, ensuring robust model adaptability. The BBT-CFLEB framework plays a significant role in evaluating NLP models within the Chinese financial sector, underscoring the importance of domain-specific benchmarks for assessing model performance [18]. This framework highlights the need for specialized methodologies tailored to the unique challenges of financial language tasks. The CMR benchmark introduces a sampling algorithm for dynamic out-of-distribution (OOD) data streams, providing a realistic framework for evaluating continual learning in NLP [8]. This benchmark is essential for addressing challenges associated with non-stationary data distributions, offering a structured approach to assess model adaptability in real-world contexts. These frameworks and taxonomies enhance the understanding of continual learning in LLMs by systematically organizing methodologies for evaluating and improving model adaptability. Researchers can leverage incremental training techniques to address semantic shifts and emerging entities, as demonstrated by BERT-like models on Twitter data, and employ ensemble methods like the Model Zoo to manage multiple tasks effectively [43,13,44,45]. While continuous adaptation provides significant advantages, it is crucial to consider environmental costs and the utility of domain- and task-specific pretraining phases to optimize model performance across diverse applications. Evaluation and Benchmarking Evaluation metrics and benchmarking methods are critical for assessing the efficacy of continual learning in LLMs, offering insights into model performance, adaptability, and knowledge retention. Metrics such as accuracy and F1-score ensure consistency and comparability across tasks, facilitating systematic comparisons of model performance on benchmark tasks [46]. These metrics are vital for evaluating the impact of continual learning methodologies, particularly in addressing catastrophic forgetting during continual pre-training [46]. Table provides a detailed overview of the benchmarks employed in the evaluation and benchmarking of large language models, highlighting their size, domain, task format, and the metrics used for performance assessment. The effectiveness of methods like QLoRA is demonstrated through performance assessments using both human and GPT-4 evaluations, focusing on instruction-following and chatbot performance metrics [6]. These evaluations highlight QLoRA's capacity to enhance model efficiency and effectiveness in practical applications. Extensive experiments using streaming data from multiple domains on models such as BERT and GPT compare the ELLE method against various lifelong learning baselines, illustrating its ability to efficiently integrate emerging data without high computational costs [7]. Performance assessments often measure the quality of representations generated by probe networks, analyzing their ability to retain knowledge over time. Methods like MER, which combine experience replay with optimization-based meta-learning, enhance continual learning by aligning gradients across examples, improving knowledge retention and adaptation to new tasks [7]. Evaluating instruction-tuned models on unseen tasks offers insights into their adaptability and performance relative to unmodified and few-shot versions of models like GPT-3 [6]. By employing diverse metrics and benchmarks, researchers can thoroughly assess LLM capabilities, ensuring their ongoing development and applicability across varied domains. These evaluation strategies are crucial for enhancing the understanding and application of continual learning methodologies within LLMs. Addressing catastrophic forgetting and ensuring a stability-plasticity trade-off enables LLMs to adapt dynamically to evolving data landscapes, achieved through integrating task-specific cues and meta-rationales, as evidenced by benchmarks like TRACE [47,48]. Challenges in Continual Learning Understanding the complexities of continual learning involves addressing its inherent challenges, such as catastrophic forgetting, distribution shifts, and resource constraints. These issues are pivotal in developing robust strategies to maintain model performance and integrate new information effectively. Catastrophic Forgetting Catastrophic forgetting poses a significant challenge in continual learning for LLMs, leading to performance degradation on previously learned tasks when new tasks are introduced [16]. Over-parameterized models are particularly susceptible, often overfitting to new data and compromising retention of essential features from pretraining [8]. The dynamic nature of OOD data streams further exacerbates this issue, as models strive to maintain efficacy across diverse domains [7]. In financial NLP applications, as highlighted by the BBT-CFLEB framework, models often struggle to generalize findings, underscoring the need for high-quality data and methodologies that mitigate performance loss [18]. The ELLE method provides an effective solution, enhancing downstream task performance by integrating new data while preserving existing knowledge [7]. Addressing catastrophic forgetting requires advanced learning methodologies, such as synaptic consolidation and knowledge transfer mechanisms, to balance stability and plasticity, ensuring consistent performance across tasks and domains [48,49,50,51,52]. Distribution Shifts Distribution shifts significantly challenge LLMs in continual learning due to semantic changes encountered with new data [43]. Such shifts can deteriorate model performance, as seen in BERT models applied to newer datasets after training on historical data. Effective domain adaptation is necessary to maintain task-specific performance while generalizing across contexts [53]. Distinguishing between domain-specific and task-relevant features is crucial for enhancing model reliability. Adaptive strategies, including domain adversarial training, facilitate domain adaptation by aligning feature representations across domains, leveraging labeled source domain data and unlabeled target domain data [53,13,54]. This approach, incorporating a gradient reversal layer, proves effective in classification tasks like sentiment analysis, enhancing the robustness and reliability of LLMs in dynamic data contexts. Resource Constraints Resource constraints impact continual learning implementation in LLMs, manifesting as computational demands, memory requirements, and the need for efficient algorithms. Pre-training models like BERT is resource-intensive, limiting continual learning feasibility in constrained environments [55]. High computational costs associated with domain-specific tasks further exacerbate these constraints, as continual learning systems must adapt without incurring prohibitive resource expenditures [56]. Techniques such as LoRA reduce trainable parameters and GPU memory requirements, alleviating some resource burdens, though challenges remain in utilizing outdated adapted weights during PLM tuning [57]. Experience replay introduces computational overhead and necessitates careful parameter tuning, further straining resources [22]. Benchmarks like WIKIREADING require substantial computational resources, limiting accessibility for some researchers and highlighting broader implications of resource constraints in the field [58]. Model Adaptation Strategies Mitigating Catastrophic Forgetting Addressing catastrophic forgetting in large language models (LLMs) is vital for preserving previously learned information while acquiring new knowledge. Strategies like Learning without Forgetting (LwF) allow models to learn new tasks without access to original task data, thus safeguarding task-specific knowledge [2]. QLoRA enhances this by enabling efficient fine-tuning on limited hardware, incorporating new information while minimizing performance loss [6]. The Lifelong-MoE framework expands model capacity through function-preserved expansions, ensuring retention from earlier tasks alongside new data integration [16]. Similarly, the ELLE method uses pre-trained domain prompts to optimize knowledge acquisition, maintaining adaptability in dynamic environments [7]. A theoretical link between task prediction (TP) and out-of-distribution (OOD) detection offers insights into continual incremental learning (CIL), highlighting the importance of robust methodologies to combat forgetting [10]. Budget constraints in traditional methods have led to innovative strategies that optimize computational resources while addressing catastrophic forgetting [19]. These advancements aim to enhance LLM adaptability and robustness, ensuring consistent performance across varied tasks and domains. Knowledge Transfer and Instruction Fine-Tuning Effective knowledge transfer and instruction fine-tuning are crucial for improving LLM performance across tasks and domains. ERNIE 2.0's continual pre-training framework exemplifies this by incrementally capturing lexical, syntactic, and semantic information, enhancing knowledge transfer [59]. Training neural networks with labeled source and unlabeled target data further boosts knowledge transfer and instruction fine-tuning [53]. The FLAN framework demonstrates the advantages of instruction tuning, achieving superior performance on unseen tasks and showcasing greater versatility [26]. Fine-tuning GPT-3 with human feedback underscores the importance of refining model outputs through enhanced knowledge transfer [11]. Future research, as suggested by TowardsVQA, aims to explore improvements in model architectures to further advance knowledge transfer and instruction fine-tuning [35]. Collectively, these methodologies emphasize the role of knowledge transfer and instruction fine-tuning in advancing LLM capabilities across applications and domains. Dynamic and Domain-Specific Adaptation Dynamic and domain-specific adaptation strategies are crucial for optimizing LLM performance in diverse environments. The DEMix method exemplifies robust adaptation by allowing expert networks to be added, removed, or mixed post-training, facilitating dynamic and domain-specific tuning [60]. SparCL enhances training efficiency and preserves accuracy, making it suitable for resource-constrained settings [55]. These strategies underscore the importance of dynamic adaptation in maintaining LLM relevance and efficacy across applications. DEMix layers enable modular domain-specific processing, improving generalization across diverse domains while maintaining low test-time perplexity. SparCL utilizes sparsity principles to optimize continual learning on edge devices, mitigating catastrophic forgetting and reducing training costs while preserving accuracy. Together, these methods ensure robust performance in dynamic environments with evolving data and varied domain requirements [43,55,60]. Memory and Replay Mechanisms Memory and replay mechanisms are essential for supporting continual learning in LLMs, facilitating the retention and integration of past knowledge while adapting to new tasks. These mechanisms counter catastrophic forgetting by reintroducing previously learned information during training on new data, preserving task-specific knowledge and enhancing adaptability [8]. Experience replay, where past experiences are stored and revisited, exemplifies a strategy that mitigates erosion of competencies [24]. Hybrid frameworks like Cooperation of Small Continual Learners (CoSCL) utilize memory to balance learning new tasks with retaining prior knowledge [23]. CoSCL employs narrower sub-networks for parallel learning of incremental tasks, ensuring robust knowledge retention. Incremental Classifier and Representation Learning (iCaRL) uses memory to add new classes while maintaining performance on previously learned ones [25]. Integrating memory replay with optimization-based meta-learning, as seen in MER, enhances continual learning by aligning gradients across examples, improving retention and adaptation [7]. However, memory replay introduces computational overhead, necessitating efficient parameter tuning [22]. These mechanisms ensure LLMs remain effective across evolving data landscapes, enhancing applicability in real-world scenarios [20]. Applications and Case Studies Exploring applications and case studies of large language models (LLMs) reveals their diverse capabilities across domains, particularly in programming and code generation. Continual learning methodologies enhance LLMs' adaptability to evolving programming challenges, as discussed in the following subsection. Programming and Code Generation Continual learning enhances LLM capabilities in programming tasks, enabling adaptation to evolving coding environments. Integrating program structures into pre-trained models improves performance, especially in data-scarce contexts, facilitating accurate code generation from limited data [15]. This structural integration is essential for developing efficient language models that meet real-world programming demands. The Gemini 1.5 framework exemplifies continual learning's efficiency, demonstrating time savings of 26 to 75\\ Moreover, the SSR methodology achieves superior or comparable performance to conventional methods while maintaining data efficiency, essential for handling diverse programming tasks [61]. The IBCL framework further illustrates continual learning's effectiveness, improving classification accuracy by an average of 44\\ The Dark Experience Replay method effectively utilizes limited resources, establishing a strong baseline for continual learning applications [62]. This approach enables LLMs to adapt to new information while retaining prior knowledge, ensuring sustained performance across various coding challenges. Benchmarks like WIKIREADING and TextVQA demonstrate LLM capabilities in integrating structured knowledge and addressing critical needs, such as visually impaired users [58,35]. Incorporating continual learning strategies addresses challenges like catastrophic forgetting and scarcity of code-comment aligned data. Techniques such as comment generation for existing code and Reasoning-augmented Continual Learning (RCL) improve the stability-plasticity trade-off and enhance intra/inter-task generalizability of LLMs, ensuring sustained performance across diverse coding challenges and real-world applications, including domain-specific tasks, multilingual capabilities, and mathematical reasoning, as evidenced by benchmarks like TRACE [47,63,48]. Natural Language Processing and Text Mining Continual learning methodologies enhance LLM adaptability and performance in natural language processing (NLP) and text mining tasks. The Lifelong-MoE framework demonstrates superior few-shot performance across various NLP tasks, underscoring its effectiveness in dynamic linguistic environments [16]. The CMR benchmark dataset provides a robust framework for evaluating continual learning challenges in NLP, offering insights into model performance amidst non-stationary data distributions [8]. Applications like TimeLMs showcase LLM capabilities in processing diachronic data from platforms like Twitter, effectively managing evolving language trends and concept drift. This adaptability enables efficient updating without full retraining, illustrating LLMs' ability to handle temporal knowledge while integrating new data seamlessly [64,39,65,66]. Furthermore, CaliNet enhances model calibration in closed-book question answering tasks, improving accuracy in generating contextually appropriate responses. In mathematical language processing, Llemma exemplifies effective application by outperforming existing models on the MATH benchmark, developed through continued pretraining on the Proof-Pile-2 dataset. This underscores the importance of tailored methodologies for specialized domains, as Llemma excels in tool use and formal theorem proving without additional fine-tuning [67,32,14]. The ERNIE 2.0 framework demonstrates the benefits of continual pre-training, significantly outperforming models like BERT and XLNet in NLP tasks. BioGPT showcases continual learning's effectiveness in generating coherent biomedical text, enhancing LLM performance in specialized fields like biomedical research. Targeted strategies, including domain-specific pretraining and continuous training, enable LLMs to excel in complex tasks such as predicting protein properties or modeling drug-target interactions, outperforming expert models tailored for these tasks [67,14,56,5,63]. The integration of continual learning in NLP and text mining significantly enhances LLM adaptability and efficiency, ensuring high performance across diverse linguistic and application-specific challenges. By mimicking human learning, continual learning addresses natural language's ambiguity and context-dependence, employing techniques to prevent catastrophic forgetting and facilitate knowledge transfer. Distillation-based approaches effectively retain performance across earlier domains while improving knowledge transfer for new tasks. This ongoing evolution through continual learning mitigates the limitations of static pretrained models, paving the way for resilient and versatile applications in real-world scenarios [68,65,69]. Domain-Specific Applications Domain-specific applications of continual learning in LLMs highlight their adaptability and effectiveness across specialized fields. In the legal domain, the SaulLM-7B model exemplifies continual learning's impact, showcasing significant advancements in legal technology [70]. Similarly, Lawyer LLaMA emphasizes the importance of domain-specific knowledge, outperforming existing models in legal tasks [14]. ClimateGPT illustrates the synthesis of interdisciplinary climate studies, demonstrating continual learning's potential in real-world applications [71]. The Swallow model showcases continual learning's effectiveness in Japanese language processing, addressing language-specific challenges [72]. In astronomy, AstroLLaMA effectively generates astronomy-related content, with applications in various research tasks [73]. The Hippocrates dataset exemplifies continual learning's role in medical diagnostics, enhancing model adaptability in health-related applications [74]. GeoGalactica demonstrates effectiveness in geoscience research tasks, highlighting continual learning's potential in understanding complex data [75]. Additionally, experiments in document sentiment analysis and image classification illustrate continual learning's adaptability in new environments [53]. MeetingBank serves as a case study in meeting summarization, demonstrating continual learning's effectiveness in summarizing complex interactions [41]. These case studies underscore the transformative potential of continual learning in enhancing LLM capabilities for real-world applications. They highlight the critical role of domain-specific adaptation strategies, such as integrating domain knowledge during continual training and employing supervised fine-tuning tasks. Frameworks like input tags and soft-masking mechanisms facilitate zero-shot generalization and mitigate catastrophic forgetting, enabling LLMs to outperform expert models in specialized fields, including law, medicine, and e-commerce [76,67,14,77]. Multimodal and Vision-Language Tasks Continual learning enhances LLM capabilities in multimodal and vision-language tasks, enabling effective integration of diverse data modalities. Models like LLaVA demonstrate impressive multimodal chat abilities, achieving 92.53\\ The development of GPT-4 exemplifies advancements in multimodal tasks, showcasing capabilities in processing and generating content from both text and images [78]. This dual-modality processing represents a significant leap in AI capabilities, allowing models to understand and generate complex content with increased accuracy. By leveraging continual learning, GPT-4 can dynamically adapt to new data streams, maintaining efficacy across evolving multimodal landscapes. These advancements underscore the importance of continual learning in enhancing LLM performance in multimodal tasks. By integrating visual and textual data, models achieve comprehensive understanding, enabling complex task performance that requires synthesizing diverse data types. For instance, LoRRA addresses visual question answering (VQA) by interpreting text within images, crucial for visually impaired users. The WikiReading task exemplifies models' potential to extract and predict textual values from structured data, showcasing deep neural networks' effectiveness in handling complex language understanding tasks [58,35]. This capability is vital for advancing LLM applicability in real-world scenarios, where multimodal data integration is often necessary for accurate outputs. Educational and Interactive Systems Continual learning methodologies significantly impact educational and interactive systems, enhancing LLM adaptability and efficacy. These techniques enable educational systems to dynamically adjust to evolving curricula, ensuring relevance in delivering personalized learning experiences. This adaptability facilitates continuous updates to incorporate new information while considering training efficiency and sustainability [43,79,44]. Interactive systems, including intelligent tutoring systems and virtual assistants, benefit from continual learning by incrementally acquiring and updating knowledge, enhancing their ability to provide contextually appropriate feedback. This adaptability is crucial in natural language processing, where context-dependence requires handling ambiguity effectively. By integrating continual learning strategies, these systems align better with user intent, improving response truthfulness and relevance, as demonstrated in models fine-tuned with human feedback [11,69,48]. Moreover, continual learning supports adaptive learning platforms that integrate new educational resources without extensive retraining, ensuring learners access the most current information. This approach enhances educational outcomes by facilitating dynamic and resource-efficient learning processes, helping learners retain knowledge and apply it to new tasks [68,69,48,52]. Future Directions Advancing large language models (LLMs) through continual learning requires strategies to enhance performance and applicability. This section discusses methodologies to overcome limitations and drive innovation, starting with the expansion of benchmarks and datasets. Improving the quality and diversity of these datasets is crucial for refining LLM evaluations and adaptability, enabling better assessments of model performance in continual learning contexts. Expanding Benchmarks and Datasets Expanding benchmarks and datasets is essential for advancing LLM capabilities in continual learning. Future research should optimize task construction within frameworks like ERNIE 2.0 to enhance language understanding [59]. Developing benchmarks that integrate user feedback, akin to DeepSeek in programming tasks, can improve model performance across diverse coding environments [31]. In biomedical NLP, refining benchmarks such as BLURB is crucial for creating comprehensive datasets that support robust model evaluations [5]. Exploring new datasets in financial NLP, highlighted by the BBT-CFLEB framework, can provide nuanced evaluation metrics [18]. Optimizing memory usage and episodic storage in frameworks like GEM can bolster performance in complex tasks beyond image classification [1]. Integrating theoretical insights with practical applications in NLP ensures models remain relevant in real-world scenarios [30]. Optimizing continual training processes to minimize resource use while enhancing task retention is a critical area for exploration [16]. Research should also investigate optimizations like QLoRA for larger models and explore applications across various domains [6]. Future work could delve into optimizing gradient alignment in MER for broader applicability [22] and refining layer averaging in model architectures to enhance adaptability [4]. By expanding benchmarks and datasets, researchers can advance AI and continual learning. Optimizing Learning Strategies and Architectures Optimizing learning strategies and model architectures is crucial for improving LLM performance in continual learning. Future research should refine warm-up strategies to align learning processes with evolving data landscapes, enhancing adaptability and efficiency [80]. Exploring continual learning strategies across diverse language tasks is vital for developing robust models capable of maintaining performance across varied domains [65]. Integrating diverse datasets and evaluating additional hyperparameters can enhance model training, ensuring responsiveness to dynamic data environments [81]. Refining evaluation metrics and benchmarks to include evolving knowledge sources is crucial for advancing understanding of model performance in real-world applications [64]. Optimizing pre-training processes in models like BERT is another focus area, leading to improved learning strategies and architectures [82]. Expanding datasets to include diverse meeting types and refining summarization techniques can optimize learning strategies and architectures [41]. Applying scaling laws to guide computational resource allocation presents a promising approach to optimizing learning strategies, ensuring efficient resource utilization and improved performance [9]. Concentrating on these areas can advance LLM capabilities, ensuring sustained relevance and effectiveness in evolving data landscapes. Enhancing Knowledge Transfer and Adaptation Enhancing knowledge transfer and model adaptation for LLMs involves optimizing architecture growth and memory management systems to improve performance in complex continual learning environments. Prioritizing improvements in model architectures is essential for facilitating knowledge retention and acquisition, advancing continual learning settings [83]. Exploring emergent abilities in model design offers opportunities to harness capabilities for optimized knowledge transfer, while enhancing memory management systems like MEMIT could bolster adaptation across domains [84]. Dynamic training methods could enhance adaptability and performance across tasks [85]. Refinements in IR-DRO's reweighting mechanism and application to other models and datasets could improve generalization [20]. Enhancements to the PEFT framework indicate a direction for advancing continual learning strategies [86]. Improving data mixing strategies is essential for enhancing model adaptation across specialized fields [77]. Enhancing robustness to varying data distributions may optimize learning strategies and architectures [24]. Focusing on models' abilities to comprehend coreferential relationships is vital for improving knowledge transfer [17]. Enhancing Learning without Forgetting for better performance across tasks is promising [2]. Future work for BioGPT should include enhancing knowledge transfer strategies to improve generative capabilities in biomedical contexts [12]. Developing new tasks within WIKIREADING and optimizing model architectures can enhance knowledge transfer [58]. Refining feedback mechanisms and exploring additional training data sources can bolster knowledge transfer [11]. Developing efficient continual learning methods under computational constraints is crucial [19]. Exploring model adaptations and new metrics to enhance the CMR benchmark is valuable [8]. Enhancing models' abilities to generalize from fewer examples and integrating diverse data sources are essential for advancing knowledge transfer strategies [7]. Focusing on domain-specific adaptation and innovative tagging methods can achieve significant advancements in knowledge transfer and model adaptation, ensuring effectiveness across diverse applications and challenges. Frameworks like Lawyer LLaMA inject domain knowledge during training and incorporate retrieval modules to enhance legal domain proficiency, while model-agnostic input tags allow LLMs to excel in fields such as physical and biomedical sciences, facilitating zero-shot generalization and outperforming expert models in predicting chemical properties and modeling drug-target interactions [67,14]. Improving Evaluation Metrics Improving evaluation metrics for continual learning in LLMs is crucial for accurately assessing model performance and adaptability across dynamic data environments. Current frameworks often rely on metrics like accuracy and F1-score, which may not fully capture the complexities of continual learning scenarios [46]. Future research should develop comprehensive metrics that address challenges such as catastrophic forgetting and distribution shifts [8]. Enhancing evaluation metrics by integrating task-specific benchmarks reflecting diverse LLM applications is promising. Benchmarks like CMR, assessing model performance in dynamic OOD data streams, provide structured approaches for evaluating continual learning methodologies [8]. Incorporating such benchmarks enables nuanced understanding of model adaptability across domains. Refining metrics to include knowledge retention and transfer measures is crucial. Techniques like MER, aligning gradients to improve retention, underscore the need for metrics capturing methodology effectiveness [7]. Evaluating instruction-tuned models on unseen tasks provides insights into adaptability, highlighting metrics assessing performance in novel scenarios [6]. Future work should explore metrics evaluating learning strategy efficiency and model architectures, particularly in resource-constrained environments. Optimizing computational resource utilization ensures evaluation metrics reflect practical LLM applicability [55]. Integrating user feedback into evaluation, as demonstrated by programming tasks with DeepSeek, can refine performance and adaptability across coding environments [31]. Addressing Theoretical Challenges Addressing theoretical challenges in continual learning research is essential for enhancing LLM capabilities and ensuring adaptability across applications. Refining contrastive learning approaches, pivotal for optimizing feature representations and improving knowledge retention, is a key focus [51]. Future research could explore further refinements, investigating strategies complementing existing methodologies like C4IL to enhance robustness. Exploring theoretical insights into continual incremental learning (CIL) offers opportunities to refine methods and expand applicability across domains [10]. Examining CIL's theoretical foundations can develop comprehensive strategies addressing dynamic data environments, ensuring LLM responsiveness and effectiveness. Integrating theoretical models tackling catastrophic forgetting and distribution shifts in continual learning scenarios is crucial. Current methods often struggle to balance forgetting prevention and knowledge transfer, particularly when tasks lack shared knowledge. Although pre-trained models can enhance end-task performance, they may exacerbate forgetting. Addressing these challenges can improve models' ability to maintain expertise across sequential tasks, as demonstrated in studies exploring forgetting causes and proposing frameworks to mitigate it [51,87,50,52]. Developing robust theoretical frameworks to guide learning strategy and architecture optimization can enhance LLM adaptability and efficiency, ensuring sustained performance across tasks and domains. Conclusion Continual learning plays a pivotal role in advancing the capabilities of large language models (LLMs), as evidenced by various experimental results. The methodologies explored demonstrate significant progress in adapting LLMs to specialized domains, such as the Chinese medical sector, achieving competitive performance while optimizing computational resources. Despite these advancements, challenges remain, particularly in preserving syntactic and semantic knowledge, which can deteriorate during continual learning processes. The KAFT framework exemplifies the potential for enhancing the controllability and robustness of LLMs, indicating promising directions for future research. Furthermore, Static ConPET efficiently reduces tunable parameters while maintaining high performance across diverse scenarios. These findings highlight the importance of scaling language models to improve few-shot learning capabilities, which has profound implications for the future of language modeling. Integration of frameworks like Tag-LLM results in notable performance gains in specialized domains, surpassing traditional expert models. Additionally, incorporating temporal context into language models improves their capacity to handle evolving knowledge, emphasizing the importance of time-aware methodologies. Aligning AI with human values remains a critical challenge, as current models show promise in understanding ethical knowledge but require further refinement. The success of InstructGPT in aligning model outputs with user intent and reducing toxicity underscores the potential of continual learning in enhancing model alignment and truthfulness. Domain-specific pretraining, especially in biomedical NLP tasks, highlights the need for benchmarks like BLURB to facilitate substantial improvements in model performance. Insights from this survey reveal the transformative impact of continual learning on the adaptability and efficiency of LLMs, paving the way for further innovations in AI and machine learning.",
  "reference": {
    "1": "1706.08840v6",
    "2": "1606.09282v3",
    "3": "2112.09153v2",
    "4": "2309.06256v4",
    "5": "2007.15779v6",
    "6": "2305.14314v1",
    "7": "2203.06311v2",
    "8": "2205.02014v1",
    "9": "2001.08361v1",
    "10": "2211.02633v1",
    "11": "2203.02155v1",
    "12": "2210.10341v3",
    "13": "2004.10964v3",
    "14": "2305.15062v2",
    "15": "2401.10716v1",
    "16": "2305.12281v1",
    "17": "1908.05803v2",
    "18": "2302.09432v2",
    "19": "2303.11165v2",
    "20": "2402.14270v2",
    "21": "2405.08015v1",
    "22": "1810.11910v3",
    "23": "2309.14763v1",
    "24": "1907.03799v3",
    "25": "1611.07725v2",
    "26": "2109.01652v5",
    "27": "2303.01081v1",
    "28": "1606.04671v4",
    "29": "2209.09513v2",
    "30": "2204.02311v5",
    "31": "2401.14196v2",
    "32": "2310.10631v3",
    "33": "2110.08207v3",
    "34": "2205.05638v2",
    "35": "1904.08920v2",
    "36": "2106.09685v2",
    "37": "1908.10063v1",
    "38": "2305.07972v1",
    "39": "2106.15110v2",
    "40": "2008.02275v6",
    "41": "2305.17529v1",
    "42": "2305.07437v5",
    "43": "2106.06297v1",
    "44": "2210.07365v2",
    "45": "2106.03027v3",
    "46": "2401.03129v1",
    "47": "2310.06762v1",
    "48": "2302.00487v3",
    "49": "2308.08747v5",
    "50": "1612.00796v2",
    "51": "2107.12308v5",
    "52": "2112.02706v1",
    "53": "1505.07818v4",
    "54": "2003.09553v2",
    "55": "2209.09476v1",
    "56": "2311.00204v1",
    "57": "2305.08702v1",
    "58": "1608.03542v2",
    "59": "1907.12412v2",
    "60": "2108.05036v2",
    "61": "2403.05530v5",
    "62": "2302.11344v1",
    "63": "2403.01244v2",
    "64": "2310.02995v3",
    "65": "2004.07211v2",
    "66": "2402.13013v1",
    "67": "2204.14211v3",
    "68": "2110.08534v3",
    "69": "2202.03829v2",
    "70": "2402.05140v3",
    "71": "2211.12701v2",
    "72": "2012.09823v1",
    "73": "2403.03883v2",
    "74": "2401.09646v1",
    "75": "2404.17790v1",
    "76": "2309.06126v1",
    "77": "2404.16621v1",
    "78": "2401.00434v2",
    "79": "2302.03241v4",
    "80": "2312.15696v1",
    "81": "2304.08485v2",
    "82": "2303.08774v6",
    "83": "2210.03329v2",
    "84": "2308.04014v2",
    "85": "1907.11692v1",
    "86": "1810.04805v2",
    "87": "2203.15556v1",
    "88": "2210.07229v2",
    "89": "2403.18365v1",
    "90": "2310.04801v1",
    "91": "2305.05968v1"
  },
  "chooseref": {
    "1": "2302.00487v3",
    "2": "1907.11692v1",
    "3": "2401.00434v2",
    "4": "2403.14734v5",
    "5": "2310.19704v3",
    "6": "2211.02633v1",
    "7": "2310.12244v1",
    "8": "2112.02706v1",
    "9": "2003.09553v2",
    "10": "2211.11031v5",
    "11": "2008.02275v6",
    "12": "2112.09153v2",
    "13": "2308.08747v5",
    "14": "2010.11929v2",
    "15": "2302.09432v2",
    "16": "2210.10341v3",
    "17": "2308.09442v2",
    "18": "2403.18365v1",
    "19": "2301.12597v3",
    "20": "2210.03329v2",
    "21": "2303.01081v1",
    "22": "2309.10654v2",
    "23": "2306.12619v2",
    "24": "2308.12950v3",
    "25": "2402.13013v1",
    "26": "2404.05875v1",
    "27": "2203.13474v5",
    "28": "2305.07922v2",
    "29": "2109.00859v1",
    "30": "2102.04664v2",
    "31": "2403.08350v2",
    "32": "2303.11165v2",
    "33": "2309.14763v1",
    "34": "2311.16206v1",
    "35": "2211.12701v2",
    "36": "2311.01200v4",
    "37": "2012.09823v1",
    "38": "2404.17790v1",
    "39": "2205.09357v1",
    "40": "2302.03241v4",
    "41": "2308.04014v2",
    "42": "2210.05549v1",
    "43": "2305.07437v5",
    "44": "2311.00204v1",
    "45": "2207.06543v1",
    "46": "1909.00277v2",
    "47": "2108.05036v2",
    "48": "2004.07211v2",
    "49": "2401.02954v1",
    "50": "2401.14196v2",
    "51": "2305.18290v3",
    "52": "2111.13654v1",
    "53": "2301.04213v2",
    "54": "1505.07818v4",
    "55": "2007.15779v6",
    "56": "2403.10056v4",
    "57": "2004.10964v3",
    "58": "1903.00161v2",
    "59": "2204.04799v2",
    "60": "2106.06297v1",
    "61": "2012.15283v3",
    "62": "2203.06311v2",
    "63": "2312.15696v1",
    "64": "2004.00345v2",
    "65": "2104.08164v2",
    "66": "1812.00420v2",
    "67": "2206.07682v2",
    "68": "1907.12412v2",
    "69": "2302.11344v1",
    "70": "2401.03129v1",
    "71": "1910.10683v4",
    "72": "2203.07228v1",
    "73": "2110.11309v2",
    "74": "1803.05355v3",
    "75": "2205.05638v2",
    "76": "1908.10063v1",
    "77": "2205.12393v4",
    "78": "2109.01652v5",
    "79": "2403.05530v5",
    "80": "1511.02283v3",
    "81": "2403.18383v1",
    "82": "2103.08541v1",
    "83": "2206.15331v2",
    "84": "2303.08774v6",
    "85": "1902.09506v3",
    "86": "1706.08840v6",
    "87": "2404.16621v1",
    "88": "2311.09774v2",
    "89": "2310.02995v3",
    "90": "2002.01808v5",
    "91": "2403.11435v1",
    "92": "2402.12847v2",
    "93": "2311.16208v2",
    "94": "2403.07082v1",
    "95": "2305.05968v1",
    "96": "2403.03894v3",
    "97": "2210.07365v2",
    "98": "2306.05064v2",
    "99": "1806.03822v1",
    "100": "2005.14165v4",
    "101": "1909.01066v2",
    "102": "2211.05110v1",
    "103": "2403.11901v4",
    "104": "2305.15062v2",
    "105": "2209.09513v2",
    "106": "1810.11910v3",
    "107": "2112.08654v2",
    "108": "2103.00020v1",
    "109": "1606.09282v3",
    "110": "2110.07298v3",
    "111": "2305.12281v1",
    "112": "2110.08534v3",
    "113": "2307.09288v2",
    "114": "2310.10631v3",
    "115": "2310.14029v1",
    "116": "2106.09685v2",
    "117": "2202.05262v5",
    "118": "2106.09685v2",
    "119": "2210.07229v2",
    "120": "2009.03300v3",
    "121": "2305.17529v1",
    "122": "1711.09601v4",
    "123": "2206.06520v1",
    "124": "2102.01951v2",
    "125": "2403.01244v2",
    "126": "2309.06256v4",
    "127": "2106.03027v3",
    "128": "2110.08207v3",
    "129": "2204.05660v1",
    "130": "2310.02031v8",
    "131": "2205.02014v1",
    "132": "1312.3005v3",
    "133": "2305.09253v2",
    "134": "1805.07810v1",
    "135": "2302.13971v1",
    "136": "2310.14152v1",
    "137": "1612.00796v2",
    "138": "2305.10403v3",
    "139": "2204.02311v5",
    "140": "2310.04801v1",
    "141": "1911.11641v1",
    "142": "1810.04805v2",
    "143": "1606.04671v4",
    "144": "1707.06347v2",
    "145": "2304.01373v2",
    "146": "1910.11473v2",
    "147": "2305.14314v1",
    "148": "1908.05803v2",
    "149": "1704.04683v5",
    "150": "1908.05852v2",
    "151": "2004.12651v1",
    "152": "2305.08702v1",
    "153": "1907.03799v3",
    "154": "2107.12308v5",
    "155": "2404.07965v4",
    "156": "1911.02782v3",
    "157": "2404.03608v1",
    "158": "2403.03883v2",
    "159": "2001.08361v1",
    "160": "2403.08763v4",
    "161": "2209.09476v1",
    "162": "2402.19173v1",
    "163": "2305.06161v2",
    "164": "2401.10716v1",
    "165": "2402.05140v3",
    "166": "2402.14270v2",
    "167": "2204.14211v3",
    "168": "2001.08435v1",
    "169": "1803.05457v1",
    "170": "2405.08015v1",
    "171": "2310.16226v3",
    "172": "2110.06366v4",
    "173": "2106.15110v2",
    "174": "2202.03829v2",
    "175": "2401.09646v1",
    "176": "2110.03215v4",
    "177": "2112.14146v1",
    "178": "2309.06126v1",
    "179": "1904.08920v2",
    "180": "2310.06762v1",
    "181": "2204.05862v1",
    "182": "2203.15556v1",
    "183": "2203.02155v1",
    "184": "2301.09785v1",
    "185": "2403.01554v1",
    "186": "2305.07972v1",
    "187": "1906.02425v2",
    "188": "1602.01585v1",
    "189": "2304.08485v2",
    "190": "1802.08218v4",
    "191": "2402.01865v3",
    "192": "1608.03542v2",
    "193": "2405.14768v3",
    "194": "2306.08568v2",
    "195": "2308.09583v3",
    "196": "1706.04115v1",
    "197": "2312.11805v5",
    "198": "1611.07725v2"
  }
}